{"input": "Parity-L is the set of languages recognized by a non-deterministic Turing machine which can only distinguish between an even number or odd number of \"acceptance\" paths (rather than a zero or non-zero number of acceptance paths), and which is further restricted to work in logarithmic space. Solving a linear system of equations over \u21242 is a complete problem for Parity-L, and so Parity-L is contained in P.\nWhat other complexity class relations would be known, if Parity-L and P were equal?", "output": "parity-$L$ is in $NC^2$ and parity-$L=P$ would mean that $P$ can be simulated in parallel $\\log^2$ time or in $\\log^2$ space (since $NC^2$ is in $DSPACE(log^2 n)$)", "meta": {"post_id": 175, "input_score": 29, "output_score": 29, "post_title": "What are the consequences of Parity-L = P?"}}
{"input": "A problem P is said to be in APX if there exists some constant c > 0 such that a polynomial-time approximation algorithm exists for P with approximation factor 1 + c.\nAPX contains PTAS (seen by simply picking any constant c > 0) and P.\nIs APX in NP? In particular, does the existence of a polynomial-time approximation algorithm for some approximation factor imply that the problem is in NP?", "output": "APX is defined as a subset of NPO, so yes, if an optimization problem is in APX then the corresponding decision problem is in NP.\nHowever, if what you're asking is whether an arbitrary problem must be in NP (or NPO) if there is a poly time O(1)-approximation, then the answer is no.  I don't know of any natural problems that serve as a counter-example, but one could define an artificial maximization problem where the objective is the sum of two terms, a large term that is easily optimized in P, and a much smaller term that adds a small amount if part of the solution encodes an answer to some hard problem (outside of NP).  Then you could find, say, a 2-approximation in poly time by concentrating on the easy term, but finding an optimal solution would require solving the hard problem.", "meta": {"post_id": 284, "input_score": 16, "output_score": 22, "post_title": "Is APX contained in NP?"}}
{"input": "Scott Aaronson proposed an interesting challange: can we use supercomputers today to help solve CS problems in the same way that physicists use large particle colliders?\n\nMore concretely, my proposal is to\n  devote some of the world\u2019s computing\n  power to an all-out attempt to answer\n  questions like the following: does\n  computing the permanent of a 4-by-4\n  matrix require more arithmetic\n  operations than computing its\n  determinant?\n\nHe concludes that this would require ~$10^{123}$ floating point operations, which is beyond our current means.  The slides are available and are also worth reading.  \nIs there any precedence for solving open TCS problems through brute force experimentation?", "output": "In \"Finding Efficient Circuits Using SAT-solvers\", Kojevnikov, Kulikov, and Yaroslavtsev have used SAT solvers to find better circuits for computing $MOD_k$ function. \nI have used computers to find proofs of time-space lower bounds, as described here. But that was only feasible because I was working with an extremely restrictive proof system.\nMaverick Woo and I have been working for some time to find the \"right\" domain for proving circuit upper/lower bounds using computers. We had hoped that we may resolve $CC^0$ vs $ACC^0$ (or a very weak version of it) using SAT solvers, but this is looking more and more unlikely. (I hope Maverick doesn't mind me saying this...)\nThe first generic problem with using brute-force search to prove nontrivial lower bounds is that it just takes too damn long, even on a very fast computer. The alternative is to try to use SAT solvers, QBF solvers, or other sophisticated optimization tools, but they do not seem to be enough to offset the enormity of the search space. Circuit synthesis problems are among the hardest practical instances one can come by.\nThe second generic problem is that the \"proof\" of the resulting lower bound (obtained by running brute-force search and finding nothing) would be insanely long and apparently yield no insight (other than the fact that the lower bound holds). So a big challenge to \"experimental complexity theory\" is to find interesting lower bound questions for which the eventual \"proof\" of the lower bound is short enough to be verifiable, and interesting enough to lead to further insights.", "meta": {"post_id": 524, "input_score": 25, "output_score": 24, "post_title": "Is \"Experimental Complexity Theory\" being used to solve open problems?"}}
{"input": "As far as I understand, the geometric complexity theory program attempts to separate $VP \\neq VNP$ by proving that the permament of a complex-valued matrix is much harder to compute than the determinant.\nThe question I had after skimming through the GCT Papers: Would this immediately imply $P \\neq NP$, or is it merely a major step towards this goal?", "output": "Assuming the generalized Riemann hypothesis (GRH), the following quite strong connections are known between $ VP= VNP $ and the collapse of the polynomial hierarchy ($ {\\rm PH}$):\n\n\nIf $ VP= VNP\\,$ (over any field) then the polynomial hierarchy collapses to the second level;\nIf $ VP=VNP\\,$ over a field of characteristic $ 0 $, then $ \\rm{NC}^3/{\\rm poly}={\\rm P}/{\\rm poly} = {\\rm PH}/{\\rm poly} $;\nIf $ VP=VNP\\,$ over a field of finite characteristic $ p $, then $ \\rm{NC}^2/{\\rm poly}={\\rm P}/{\\rm poly} = {\\rm PH}/{\\rm poly} $. \n\n\nThese are results from: Peter Burgisser, \"Cook\u2019s versus Valiant\u2019s hypothesis\",  Theor. Comp. Sci., 235:71\u201388,\n2000.\nSee also: Burgisser, \"Completeness and Reduction in Algebraic Complexity Theory\", 1998.", "meta": {"post_id": 529, "input_score": 38, "output_score": 25, "post_title": "Does $VP \\neq VNP$ imply $P \\neq NP$?"}}
{"input": "Let class A denote all the graphs of size $n$ which have a Hamiltonian cycle. It is easy to produce a random graph from this class--take $n$ isolated nodes, add a random Hamiltonian cycle and then add edges randomly.\nLet class B denote all the graphs of size $n$ which do not have a Hamiltonian cycle. How can we pick a random graph from this class? (or do something close to that)", "output": "This is impossible (unless NP=coNP) since in particular that implies a poly-time function whose range is the non-Hamiltonian graphs (the function goes from the random string to the output graph), which in turn will imply an NP-proof of non-Hamiltonianicity (to prove G doesn't have an Hamiltonian circuit, show x that maps to it.)", "meta": {"post_id": 562, "input_score": 28, "output_score": 34, "post_title": "How to produce a random graph that does not have a Hamiltonian cycle?"}}
{"input": "Ladner's Theorem states that if P \u2260 NP, then there is an infinite hierarchy of complexity classes strictly containing P and strictly contained in NP.  The proof uses the completeness of SAT under many-one reductions in NP.  The hierarchy contains complexity classes constructed by a kind of diagonalization, each containing some language to which the languages in the lower classes are not many-one reducible.\nThis motivates my question:\n\nLet C be a complexity class, and let D be a complexity class that strictly contains C.  If D contains languages that are complete for some notion of reduction, does there exist an infinite hierarchy of complexity classes between C and D, with respect to the reduction?\n\nMore specifically, I would like to know if there are results known for D = P and C = LOGCFL or C = NC, for an appropriate notion of reduction.\n\nLadner's paper already includes Theorem 7 for space-bounded classes C, as Kaveh pointed out in an answer.  In its strongest form this says: if NL \u2260 NP then there is an infinite sequence of languages between NL and NP, of strictly increasing hardness.  This is slightly more general than the usual version (Theorem 1), which is conditional on P \u2260 NP.  However, Ladner's paper only considers D = NP.", "output": "The answer to your question is \"yes\" for a wide variety of classes and reductions, including logspace reductions and the classes you mentioned, as is proved in these papers:\n\nH. Vollmer.  The gap-language technique revisited.  Computer Science Logic, Lecture Notes in Computer Science Vol. 533, pages 389-399, 1990.\nK. Regan and H. Vollmer.  Gap-languages and log-time complexity classes.  Theoretical Computer Science, 188(1-2):101-116, 1997.\n\n(You can download gzipped postscript files of these papers here.)\nThe proofs follow the basic principle of Uwe Sch\u00f6ning's extension of Ladner's theorem:\n\nUwe Sch\u00f6ning.  A uniform approach to obtain diagonal sets in complexity classes.  Theoretical Computer Science 18(1):95-103, 1982.\n\nSch\u00f6ning's proof has always been my favorite proof of Ladner's theorem -- it's both simple and general.", "meta": {"post_id": 799, "input_score": 48, "output_score": 36, "post_title": "Generalized Ladner's Theorem"}}
{"input": "This question most likely has a simple answer; however, I do not see it.\nLet $g:\\mathbb{N} \\rightarrow \\mathbb{N}$ be an uncomputable function and $c$ a positive real number. Can there be a computable function $f : \\mathbb{N} \\rightarrow \\mathbb{N}$ such that, for all $n$ large enough: $g(n) \\leq f(n) \\leq c \\cdot g(n)$ (that is $f(n) = \\Theta(g(n)$)?", "output": "Sure: just take g(n) = n + halt(n) (where halt(n)=1 if TM number n halts, and 0 ow).", "meta": {"post_id": 914, "input_score": 6, "output_score": 24, "post_title": "Computable function $f = \\Theta(g)$ with $g$ uncomputable"}}
{"input": "Is BQP equal to BPP with access to an Abelian hidden subgroup oracle?", "output": "Like many complexity-class separations, our best guess is that the answer is that BPP^{HSP} != BQP, but we can only prove this rigorously relative to oracles.  This separation was observed by Scott Aaronson in this blog post\nwhere he observed that the welded-tree speedup of Childs, Cleve, Deotto, Farhi, Gutmann and Spielman was not contained in SZK.\nOn the other hand, BPP^{HSP} is contained in SZK, at least if the goal is to determine the size of the hidden subgroup.  This includes even the abelian HSP, although I'm not sure how exactly to find the generators of an arbitrary hidden subgroup in SZK.  The reason we can decide the size of the hidden subgroup is that if f:G->S has hidden subgroup H, and we choose g uniformly at random from G, then f(g) is uniformly random over a set of size |G|/|H|. In particular, f(g) has entropy log|G| - log|H|.  And entropy estimation is in SZK.", "meta": {"post_id": 1298, "input_score": 23, "output_score": 26, "post_title": "Is BQP equal to BPP with access to an Abelian hidden subgroup oracle?"}}
{"input": "Coppersmith\u2013Winograd algorithm is the asymptotically fastest known algorithm for multiplying two $n \\times n$ square matrices. The running time of their algorithm is $O(n^{2.376})$  which is the best known so far. What is the space complexity of this algorithm ? Is it in $\\Theta(n^2)$ ?", "output": "Yes, all algorithms which stem from Strassen's original algorithm (this includes most known $n^{3-\\varepsilon}$ algorithms for matrix multiplication, but not all -- see the comments) have space complexity $\\Theta(n^2)$. If you could find a $n^{3-\\varepsilon}$ time algorithm with $poly(\\log n)$ space complexity, this would be a great advance. One application would be a $2^{(1-\\varepsilon)n}$ time, $poly(n)$ space algorithm for the Subset-Sum problem. \nHowever there are some obstacles to such a result. For some computational models, there are fairly strong lower bounds for the time-space product of matrix multiplication. References like Yesha and Abrahamson will give you more information.", "meta": {"post_id": 1313, "input_score": 24, "output_score": 31, "post_title": "Space complexity of Coppersmith\u2013Winograd algorithm"}}
{"input": "In a lot of domains, there are canonical techniques which everybody working in the field should master. For example, for logspace reductions, the \"bit trick\" for composition consisting of not constructing the full output of the composed function, but always asking to recompute the result for every bit of output, permitting to keep logspace constraints.\nMy question is about non-relativizing techniques. Do theoricians have outlined some fundamental non-relativizing operations, or is there a different trick for each known non-relativizing proof ?", "output": "There's really only one \"flagship\" non-relativizing technique: namely, arithmetization (the technique used in the proofs of IP=PSPACE, MIP=NEXP, PP\u2284SIZE(nk), MAEXP\u2284P/poly, and several other results).\nHowever, the proof that all NP languages have computational zero-knowledge proofs (assuming one-way functions exist), due to Goldreich, Micali, and Wigderson, used a different non-relativizing technique (namely, the symmetries of the 3-COLORING problem).\nArora, Impagliazzo, and Vazirani argued that even \"local checkability,\" the property of NP-complete problems used in the proof of the original Cook-Levin Theorem (as well as the PCP Theorem), should count as a non-relativizing technique (though Lance Fortnow wrote a paper arguing the opposite).  The sticking point is whether it makes sense to relativize the complexity class of \"locally checkable problems.\"\nThe pebbling arguments used in results from the 1970s such as TIME(n)\u2260NTIME(n) have been put forward as another example of a non-relativizing technique.\nFor more, you might want to check out my algebrization paper with Wigderson, and especially the references therein.  We had to pretty much catalogue the existing non-relativizing techniques in order to figure out which ones were and weren't encompassed by the algebrization barrier.\nAddendum: I just realized that I forgot to mention measurement-based quantum computing (MBQC), which was recently used to great effect by Broadbent, Fitzsimons, and Kashefi to obtain quantum complexity theorems (such as QMIP = MIP*, and BQP = MIP with entangled BQP provers and BPP verifier) that most likely fail to relativize.", "meta": {"post_id": 2045, "input_score": 30, "output_score": 41, "post_title": "Are there canonical non-relativizing techniques?"}}
{"input": "This question is inspired by the Georgia Tech Algorithms and Randomness Center's t-shirt, which asks \"Randomize or not?!\"\nThere are many examples where randomizing helps, especially when operating in adversarial environments.  There are also some settings where randomizing doesn't help or hurt.  My question is:\n\nWhat are some settings when randomizing (in some seemingly reasonable way) actually hurts?\n\nFeel free to define \"settings\" and \"hurts\" broadly, whether in terms of problem complexity, provable guarantees, approximation ratios, or running time (I expect running time is where the more obvious answers will lie).  The more interesting the example, the better!", "output": "Here is a simple example from game theory. In games in which both pure and mixed Nash equilibria exist, the mixed ones are often much less natural, and much \"worse\".\nFor example, consider a simple balls and bins game: there are n bins, and n balls (players). Each player gets to pick a bin, and incurs a cost equal to the number of people in his bin. The pure Nash equilibrium has everyone each picking a unique bin, and nobody incurs cost more than 1. However, there is a mixed Nash equilibrium in which everyone randomly chooses a bin, and then with high probability, there will be one bin with ~ $\\log(n)/\\log\\log(n)$ people. Since OPT is 1, that means that (if what we care about is max player cost), if randomization is not allowed, then the price of anarchy is 1. But if randomization is allowed, it grows unboundedly with the number of players in the game.\nThe takeaway message: randomization can harm coordination.", "meta": {"post_id": 2229, "input_score": 17, "output_score": 25, "post_title": "Randomize or Not?"}}
{"input": "I recently taught expanders, and introduced the notion of Ramanujan graphs.\nMichael Forbes asked why they are called this way, and I had to admit I don't know. \nAnyone?", "output": "To add some content to the answers here, I'll explain briefly what Ramanujan's conjecture is.\nFirst of all, Ramanujan's conjecture is actually a theorem, proved by Eichler and Igusa.  Here is one way to state it.  Let $r_m(n)$ denote the number of integral solutions to the quadratic equation $x_1^2 + m^2 x_2^2 + m^2 x_3^2 + m^2 x_4^2 = n$.  If $m=1$, that $r_m(n) > 0$ was of course proved by Legendre, but Jacobi gave the exact count: $r_1(n) = 8 \\sum_{d \\mid n, 4 \\not \\mid d} d$.  Nothing similarly exact is known for larger $m$ but Ramanujan conjectured the bound: $r_m(n) = c_m \\sum_{d \\mid n} d + O(n^{1/2 + \\epsilon})$ for every $\\epsilon > 0$, where $c_m$ is a constant dependent only on $m$.  \nLubtozky, Phillips and Sarnak constructed their expanders based on this result.  I'm not familiar with the details of their analysis but the basic idea, I believe, is to construct a Cayley graph of $PSL(2,Z_q)$ for a prime $q$ that $1 \\bmod 4$, using generators determined by every sum-of-four-squares decomposition of $p$, where $p$ is a quadratic residue modulo $q$.  Then, they relate the eigenvalues of this Cayley graph to $r_{2q}(p^k)$ for integer powers $k$.  \nA reference, other than the Lubotzky-Phillips-Sarnak paper itself, is Noga Alon's brief description in Tools from Higher Algebra.", "meta": {"post_id": 2315, "input_score": 27, "output_score": 36, "post_title": "Why are Ramanujan graphs named after Ramanujan?"}}
{"input": "Matrix multiplication using regular (row - column inner product) technique takes $O(n^{3})$ multiplucations and $O(n^{3})$ additions. However assuming equal sized entries (number of bits in each entry of both matrices being multiplied) of size $m$ bits, the addition operation actually happens on $O(n^{3}nm) = O(n^{4}m)$ bits.\nSo it seems that the true complexity of matrix multiplication if measured via bit complexity should be $O(n^{4})$. \n$(1)$Is this correct?\nSupposing if one creates an algorithm which reduces the bit complexity to $O(n^{3+\\epsilon})$ rather than total multiplications and additions, this might be a sounder approach than say reducing the total multiplications and additions to $O(n^{2+\\epsilon})$ as attempted by researchers such as Coppersmith and Cohn.\n$(2)$ Is this a valid argument?", "output": "No, the bit complexity of matrix multiplication on $M$-bit entries is $n^{\\omega} (\\log n)^{O(1)} \\cdot M (\\log M)^{O(1)}$, where $\\omega < 2.4$ is the best known matrix multiplication exponent. Multiplication and addition of $M$-bit numbers can be done in $M (\\log M)^2$ time. Multiplying two $M$-bit numbers yields a number which has no more than $2M$ bits. Adding $n$ numbers of $M$ bits each, yields a number which has no more than $M+\\log n+O(1)$ bits. (Think about it: the sum is at most $n 2^M$, so the bit representation takes no more than $\\log (n 2^M)+O(1)$ bits.)\nReferences to fast integer multiplication algorithms can be found with a web search or wikipedia.", "meta": {"post_id": 2349, "input_score": 9, "output_score": 31, "post_title": "True Bit Complexity of matrix multiplication is $O(n^{4})$"}}
{"input": "I've been thinking of a variant of hex, where instead of the two players making moves alternately, each turn a player picked at random makes a move. How hard is it to determine the chances for each player winning? This problem is obviously in PSPACE, but can't it to be NP-hard, much less PSPACE-complete. The difficulties come from how the randomness makes it impossible for a player to be forced into making a choice among options; if that player is lucky he gets enough moves two take both options, and if the player is unlucky the opponent gets enough moves to block both options. On the other hand, I can't think of any polynomial-time algorithms for this.", "output": "You might want to look at the paper \"Random-Turn Hex and Other Selection Games,\" by Yuval Peres, Oded Schramm, Scott Sheffield, and David Wilson. From the introduction: \n\n\"Random-Turn Hex is the same as\n  ordinary Hex, except that instead of\n  alternating turns, players toss a coin\n  before each turn to decide who gets to\n  place the next stone. Although\n  ordinary Hex is famously difficult to\n  analyze, the optimal strategy for\n  Random-Turn Hex turns out to be very\n  simple.\"\n\nSo indeed, your intuition was right: this will be in BPP (or maybe P).", "meta": {"post_id": 2353, "input_score": 17, "output_score": 23, "post_title": "Complexity of hex with random turn order."}}
{"input": "First of all, I apologize in advance for any stupidity. I am by no means an expert on complexity theory (far from it! I am an undergraduate taking my first class in complexity theory) Here's my question. Now Savitch's Theorem states that \n$$\\text{NSPACE}\\left(f\\left(n\\right)\\right) \\subseteq \\text{DSPACE}\\left(\\left(f\\left(n\\right)\\right)^2\\right)$$\nNow I'm curious if if this lower bound was tight, i.e that is something along the lines of \n$\\text{NSPACE}\\left(f\\left(n\\right)\\right) \\subseteq \\text{DSPACE}\\left(\\left(f\\left(n\\right)\\right)^{1.9}\\right)$ is not achievable. \nIt seems like something there should be a straightforward combinatorial argument to be made here - each node in the configuration graph for a Deterministic Turing machine has only one outgoing edge, while each node in the configuration graph of a Non-Deterministic Turing machine can have more than one outgoing edge. What Savitch's algorithm is doing is converting configuration graphs with any number outgoing edge to configuration graphs with $<2$ outgoing edges.\nSince the configuration graph defines a unique TM (not sure about this), the combinatorial size of the latter is almost certainly larger than the former. This \"difference\" is perhaps a factor of $n^2$, perhaps less - I don't know. Of course, there are lots of little technical issues to be worked out, like how you need to make sure there are no loops and so forth, but my question is if this is a reasonable way to begun proving a thing like this.", "output": "This is a well known open question. You will see in complexity theory many open questions for which you'd wonder how come no one managed to solve them. Part of the reason is that we need new people like you to help us solve them :)\nFor the latest result in this area, showing that Savitch's algorithm is optimal in some restricted model, see Aaron Potechin's FOCS paper.\nSpecifically, he starts from the nice observation that because the configuration graph of a deterministic TM has only one outgoing edge (after fixing the input), one can think of it as an undirected graph, and so the question becomes something like the following: given a directed graph $G$ of $n$ vertices with two special vertices $s,t$, if we map it to an $N$ vertex undirected graph $G'$ (also with special vertices $s',t'$) such that the existence of each edge in $G'$ depends on one edge in $G$ and there is a path from $s$ to $t$ in $G$ iff there's a path between $s'$ and $t'$ in $G'$, how much bigger $N$ has to be from $n$.\nTo show that Savitch's algorithm is optimal, one needs to show that $N$ has to be at least $2^{\\Omega(\\log^2 n)}  = n^{\\Omega(\\log n)}$. To show $L\\neq NL$, it suffices to show the weaker bound that $N > n^c$ for every constant $c$.  I'm pretty sure that even $N > n^{10}$ is not known, though perhaps something like $N \\geq n^2$ is known for some not so interesting reasons.", "meta": {"post_id": 2426, "input_score": 29, "output_score": 31, "post_title": "Tight Lower bounds on Savitch's theorem"}}
{"input": "1) What, if any, is the relationship between static typing and formal grammars?\n2) In particular, would it be possible for a linear bounded automaton to check whether, say, a C++ or SML program was well typed? A nested stack automaton?\n3) Is there a natural way to express static typing rules in formal grammar terms?", "output": "It is not possible for linear bounded automata to check whether C++ programs, and unlikely to be possible for and LBA to check whether SML programs are well-typed. C++ has a Turing-complete type system, since you can code arbitrary programs as template metaprograms. \nSML is more interesting. It does have decidable type checking, but the problem is EXPTIME-complete. Hence it is unlikely an LBA can check it, unless there is a very surprising collapse in the complexity hierarchy. The reason for this is that SML requires type inference, and there are families of programs the size of whose type grows much faster than the size of the program. As an example, consider the following program:\nfun delta x = (x, x)        (* this has type 'a -> ('a * 'a), so its return value\n                               has a type double the size of its argument *)\n\nfun f1 x = delta (delta x)  (* Now we use functions to iterate this process *)\nfun f2 x = f1 (f1 x)        \nfun f3 x = f2 (f2 x)        (* This function has a HUGE type *)\n\nFor simpler type systems, such as C or Pascal's, I believe it is possible for an LBA to check it. \nIn the early days of programming languages research, people sometimes used van Wingaarden grammars (aka two-level grammars) to specify type systems for programming languages. I believe Algol 68 was specified in this way. However, I am told this technique was abandoned for essentially pragmatic reasons: it turned out to be quite difficult for people to write grammars that specified what they thought they were specifying! (Typically, the grammars people wrote generated larger languages than they intended.) \nThese days people use schematic inference rules to specify type systems, which is essentially a way of specifying predicates as the least fixed point of a collections of Horn clauses. Satisfiability for first-order Horn theories is undecidable in general, so if you want to capture everything type theorists do, then whatever grammatical formalism you choose will be stronger than is really convenient.\nI know there has been some work on using attribute grammars to implement type systems. They claim there are some software engineering benefits for this choice: namely, attribute grammars control information flow very strictly, and I am told this makes program understanding  easier.", "meta": {"post_id": 2428, "input_score": 26, "output_score": 21, "post_title": "Context Sensitive Grammars and Types"}}
{"input": "Hey Guys, I understand that the padding trick allows us to translate complexity classes upwards - for example $P=NP \\rightarrow EXP=NEXP$. Padding works by \"inflating\" the input, running the conversion (say from say $NP$ to $P$), which yields a \"magic\" algorithm which you can run on the padded input. While this makes technical sense, I can't get a good intuition of how this works. What exactly is going on here? Is there a simple analogy for what padding is? \nCan provide a common sense reason why this is the case?", "output": "I think the best way to get intuition for this issue is to think of what the complete problems for exponential time classes are.  For example, the complete problems for NE are the standard NP-complete problems on succinctly describable inputs, e.g., given a circuit that describes the adjacency matrix of a graph, is the graph 3-colorable?  Then the problem of\nwhether E=NE becomes equivalent to whether NP problems are solvable in polynomial time on the\nsuccinctly describable inputs, e.g., those with small effective Kolmogorov complexity.   This is obviously no stronger than whether they are solvable on all inputs.  The larger the time bound, the smaller the Kolmogorov complexity of the relevant inputs, so collapses for larger time bounds are in effect algorithms that work on smaller subsets of inputs.\nRussell Impagliazzo", "meta": {"post_id": 2434, "input_score": 25, "output_score": 30, "post_title": "Why do equalities between complexity classes translate upwards and not downwards?"}}
{"input": "Does anyone know about an NP-completeness result for the DOMINATING SET problem in graphs, restricted to the class of planar bipartite graphs of maximum degree 3?\nI know it is NP-complete for the class of planar graphs of maximum degree 3 (see the Garey and Johnson book), as well as for bipartite graphs of maximum degree 3 (see M. Chleb\u00edk and J. Chleb\u00edkov\u00e1, \"Approximation hardness of dominating set problems in bounded degree graphs\"), but could not find the combination of the two in the literature.", "output": "What if you simply do the following: Given a graph $G = (V,E)$, construct another graph $G' = (V \\cup U, E')$ by subdividing each edge of $G$ in 4 parts; here $U$ is the set of new nodes that we introduced, and $|U| = 3|E|$.\nThe graph $G'$ is bipartite. Moreover, if $G$ is planar and has max. degree 3, then $G'$ is also planar and has max. degree 3.\nLet $D'$ be a (minimum) dominating set for $G'$. Consider an edge $(x,y) \\in E$ that was subdivided to form a path $(x,a,b,c,y)$ in $G'$. Now clearly at least one of $a,b,c$ is in $D'$. Moreover, if we have more than one of $a,b,c$ in $D'$, we can modify $D'$ so that it remains a valid dominating set and its size does not increase. For example, if we have $a \\in D'$ and $c \\in D'$, we can equally well remove $c$ from $D'$ and add $y$ to $D'$. Hence w.l.o.g. we have $|D' \\cap U| = |E|$.\nThen consider $D = D' \\cap V$. Assume that $x \\in V$ and $x \\notin D'$. Then we must have a node $a \\in D'$ such that $(x,a) \\in E'$. Hence there is an edge $(x,y) \\in E$ such that we have a path $(x,a,b,c,y)$ in $G'$. Since $a,b,c \\in U$ and $a \\in D'$, we have $b, c \\notin D'$, and to dominate $c$ we must have $y \\in D'$. Hence in $G$ node $y$ is a neighbour of $x$ with $y \\in D$. That is, $D$ is a dominating set for $G$.\nConversely, consider a (minimum) dominating set $D$ for $G$. Construct a dominating set $D'$ for $G'$ so that $|D'| = |D| + |E|$ as follows: For an edge $(x,y) \\in E$ that was subdivided to form a path $(x,a,b,c,y)$ in $G'$, we add $a$ to $D'$ if $x \\notin D$ and $y \\in D$; we add $c$ to $D'$ if $x \\in D$ and $y \\notin D$; and otherwise we add $b$ to $D'$. Now it can be checked that $D'$ is a dominating set for $G'$: By construction, all nodes in $U$ are dominated. Now let $x \\in V \\setminus D'$. Then there is a $y \\in V$ such that $(x,y) \\in E$, and hence along the path $(x,a,b,c,y)$ we have $a \\in D'$, which dominates $x$.\nIn summary, if $G$ has a dominating set of size $k$, then $G'$ has a dominating set of size at most $k + |E|$, and if $G'$ has a dominating set of size $k + |E|$, then $G$ has a dominating set of size at most $k$.\nEdit: Added an illustration. Top: the original graph $G$; middle: graph $G'$ with a \"normalised\" dominating set; bottom: graph $G'$ with an arbitrary dominating set.", "meta": {"post_id": 2505, "input_score": 19, "output_score": 25, "post_title": "Is the dominating set problem restricted to planar bipartite graphs of maximum degree 3 NP-complete?"}}
{"input": "Does it make sense to consider a category of all NP-complete problems, with morphisms as poly-time reductions between different instances? Has anyone ever published a paper about this, and if so, where can I find it?", "output": "The area you want to look at is called \"implicit complexity theory\". A random and incomplete fistful of names to Google for are Martin Hofmann, Patrick Baillot, Ugo Dal Lago, Simona Ronchi Della Rocca, and Kazushige Terui. \nThe basic technique is to relate complexity classes to subsystems of linear logic (the so-called \"light linear logics\"), with the idea that the cut-elimination for the logical system should be complete for the given complexity class (such as LOGSPACE, PTIME, etc). Then via Curry-Howard you get out a programming language in which precisely the programs in the given class are expressible. As you might expect from the mention of linear logic, these all these systems then give rise to monoidal closed categories of various flavors, which leaves you with a purely algebraic and machine-independent characterization of various complexity classes. \nOne of the things that make this area interesting is that neither traditional complexity nor logical/PL methods are entirely appropriate. \nSince the categories involved typically have closed structure, the combinatoric methods favored by complexity theorists often break down (since higher-order programs tend to resist combinatorial characterizations). A typical example of this is the failure of syntactic methods to handle contextual equivalence. Similarly, the methods of semantics also have trouble, since they are often too extensional (since traditionally semanticists have wanted to hide the internal structure of functions). The simplest example I know here is the closure of LOGSPACE under composition: this is AFAIK only possible due to dovetailing and selective recomputation, and you can't treat the problems as pure black boxes.\nYou will likely also want to have some familiarity with game semantics and Girard's Geometry of Interaction (and their precursor, Kahn-Plotkin-Berry's concrete data structures) if you get seriously into this area -- the ideas of token-passing representations of higher-order computations used in this work supply a lot of the intuitions for ICC.\nSince I've pointed out the central role of monoidal categories in this work, you might reasonably wonder about the connections to Mulmuley's GCT. Unfortunately, I can't help you here, since I simply don't know enough. Paul-Andr\u00e9 Melli\u00e8s might be a good person to ask, though.", "meta": {"post_id": 3074, "input_score": 28, "output_score": 21, "post_title": "A category of NP-complete problems?"}}
{"input": "The central problem of complexity theory is arguably $P$ vs $NP$.\nHowever, since Nature is quantum, it would seem more natural to consider the classes $BQP$ (ie decision problems solvable by a quantum computer in polynomial time, with an error probability of at most 1/3 for all instances) ans $QMA$ (the quantum equivalent of $NP$) instead.\nMy questions:\n1) Would a solution to the $P$ vs $NP$ problem give a solution to $BQP$ vs $QMA$?\n2) Do the three barriers of relativization, natural proofs and algebrization also apply to the $BQP$ vs $QMA$ problem?", "output": "1) No implication is known in either direction.  We know that P=NP implies P=PH.  But we don't know if BQP and QMA are in PH, so maybe P could equal NP yet BQP and QMA still wouldn't collapse.  (On the other hand, note that QMA\u2286PP\u2286P#P, so certainly P=P#P would imply BQP=QMA.)  To show that BQP=QMA implies P=NP seems even more hopeless in the present state of knowledge.\n2) Absolutely, all three barriers apply with full force to BQP vs. QMA (and even to the \"easier\" problem of proving P\u2260PSPACE).  First, relative to a PSPACE oracle (or even the low-degree extension of a PSPACE oracle), we have\nP = NP = BQP = QMA = PSPACE,\nso certainly nonrelativizing and non-algebrizing techniques will be needed to separate any of these classes.  Second, to get a natural proofs barrier for putting stuff outside BQP, all you need is a pseudorandom function family that's computable in BQP, which is a formally weaker requirement than a pseudorandom function family computable in P.\nAddendum: Let me say something about a \"metaquestion\" which you didn't ask but hinted at, of why people still focus on P vs. NP even though we believe Nature is quantum.  Personally, I've always seen P vs. NP as nothing more than the \"flagship\" for a whole bunch of barrier questions in complexity theory (P vs. PSPACE, P vs. BQP, NP vs. coNP, NP vs. BQP, the existence of one-way functions, etc), none of which we know how to answer, and all of which are related in the sense that any breakthrough with one would very likely lead to breakthroughs with the others (even where we don't have formal implications between the questions, which in many cases we do).  P vs. NP isn't inherently more fundamental than any of the others -- but if we have to pick one question to serve as the poster child for complexity, then it's a fine choice.", "meta": {"post_id": 3304, "input_score": 35, "output_score": 34, "post_title": "$BQP$ vs $QMA$?"}}
{"input": "I was wondering if the JSON spec defined a regular language.  It seems simple enough, but I'm not sure how to prove it myself.\nThe reason I ask, is because I was wondering if one could use regular expressions to effectively pars JSON.\nCould someone with enough rep please create the tags json and regular-language for me?", "output": "Since $a^n b^n$ is not a regular language, neither is JSON, since $[^n 5 ]^n$ is valid input for any $n$.  Likewise, your regular expression parser would have to properly reject any input $[^m 4 ]^n$ where $m \\ne n$ which you cannot do with regular expressions.\nHence, JSON is not regular.", "meta": {"post_id": 3987, "input_score": 21, "output_score": 30, "post_title": "Is JSON a Regular Language?"}}
{"input": "In trying to devise my own sorting algorithm, I'm looking for the optimal benchmark to which I can compare it.  For an unsorted ordering of elements A and a sorted ordering B, what is an efficient way to calculate the optimal number of transpositions to get from A to B ?\nA transposition is defined as switching the position of 2 elements in the list, so for instance\n1 2 4 3\n\nhas one transposition (transposition 4 and 3) to make it\n1 2 3 4\n\nSomething like\n1 7 2 5 9 6\n\nrequires 4 transpositions (7, 2), (7, 6), (6,5), (9, 7)\nUpdate (9/7/11): question changed to use \"transposition\" instead of \"swaps\" to refer to non-adjacent exchanges.", "output": "If you're only dealing with permutations of $n$ elements, then you will need exactly $n-c(\\pi)$ swaps, where $c(\\pi)$ is the number of cycles in the disjoint cycle decomposition of $\\pi$. Since this distance is bi-invariant, transforming $\\pi$ into $\\sigma$ (or $A$ into $B$, or conversely) requires $n-c(\\sigma^{-1}\\circ\\pi)$ such moves.", "meta": {"post_id": 4096, "input_score": 16, "output_score": 23, "post_title": "Minimum number of transpositions to sort a list"}}
{"input": "2 questions for the computational geometers or algebraists:\nI am just beginning to dive into computational geometry and I am loving it =)\nI am attempting to read the famous article by Guibas and Stolfi called \"Primitives for the manipulation of general subdivisions and the computation of Voronoi Diagrams\" in order to implement a Delaunay triangulation algorithm. I am tempted to skip all the theoretical stuff and just read the description of their quad-edge data structure to save time. However, I think it may be worth it to understand all the math in the article if the structure is widely used, or just because it may be beautiful.\nThe math is a little to dense for me. I'm not completely ignorant on topology, but the description of their edge algebra requires knowledge of abstract algebra that I don't have.\nMy two questions are: What other applications of the quad-edge structure are there besides computing Delaunay/Voronoi? It seems like an extremely powerful tool.\nThe second question; What is an abstract algebra? It would be great if you could give me a reference to an introduction to abstract algebra, just enough so I can understand the section on their edge algebra. \nThank you!", "output": "I think Guibas and Stolfi's \u201cedge algebra\u201d formalism is a bit unnecessary.\nAll that's really necessary is to remember the distinction between primal and dual graphs.  Each face $f$ of the primal graph has a corresponding dual vertex $f^*$;  each edge $e$ of the primal graph has a corresponding dual edge $e^*$; and each vertex $v$ of the primal graph has a corresponding dual face $v^*$.  Primal edges connect primal vertices and separate primal faces; dual edges connect dual vertices and separate dual faces.  The dual of the dual of anything is the original thing.  See Figure 4 in Guibas and Stolfi's paper:\n\nGuibas and Stolfi propose thinking about each edge (either primal or dual) as a collection of four directed, oriented edges; for simplicity, I'll call these darts.  Each dart $\\vec{e}$ points from one endpoint $\\text{tail}(\\vec{e})$ to the other endpoint $\\text{head}(\\vec{e})$, and locally separates two faces $\\text{left}(\\vec{e})$ and $\\text{right}(\\vec{e})$.  The choice of which endpoint to call $\\text{tail}(\\vec{e})$ is the dart's direction, and the choice of which face to call $\\text{left}(\\vec{e})$ is its orientation.  (Guibas and Stolfi use \u201cOrg\u201d and \u201cDest\u201d instead of \u201ctail\u201d and \u201chead\u201d, but I prefer the shorter labels, because Unnecessary Abbreviations Are Evil.)\nFor any dart $\\vec{e}$, Guibas and Stolfi associate three related darts:\n\n$\\text{tailNext}(\\vec{e})$: The dart leaving $\\text{tail}(\\vec{e})$ next in counterclockwise order after $\\vec{e}$.\n$\\text{flip}(\\vec{e})$: The \u201csame\u201d dart as $\\vec{e}$, but with $\\text{left}(\\vec{e})$ and $\\text{right}(\\vec{e})$ swapped.\n$\\text{rotate}(\\vec{e})$: The dual dart obtained by giving $\\vec{e}$ a quarter turn counterclockwise around its midpoint. \n\n\nThese three functions satisfy all sorts of wonderful identities, like the following:\n\n$\\text{right}(\\text{tailNext}(\\vec{e})) = \\text{left}(\\vec{e})$\n$\\text{right}(\\text{flip}(\\vec{e})) = \\text{left}(\\vec{e})$\n$\\text{right}(\\text{rotate}(\\vec{e})) = \\text{head}(\\vec{e})^*$\n$\\text{flip}(\\text{flip}(\\vec{e})) = \\vec{e}$\n$\\text{rotate}(\\text{rotate}(\\text{rotate}(\\text{rotate}(\\vec{e})))) = \\vec{e}$\n$\\text{tailNext}(\\text{rotate}(\\text{tailNext}(\\text{rotate}(\\vec{e})))) = \\vec{e}$\n\nFor a complete list, see page 83 of the paper (but beware that the authors use postfix notation $e~Flip$, presumably because it's closer to the declarative code e.Flip).  Guibas and Stolfi call any triple of functions satisfying all these identities an edge algebra.\nMoreover, given these three functions, one can define several other useful functions like\n\n$\\text{reverse}(\\vec{e}) = \\text{rotate}(\\text{flip}(\\text{rotate}(\\vec{e})))$ \u2014 swap head and tail vertices\n$\\text{leftNext}(\\vec{e}) = \\text{rotate}(\\text{tailNext}(\\text{rotate}(\\text{rotate}(\\text{rotate}(\\vec{e})))))$ \u2014 the next dart after $\\vec{e}$ in counterclockwise order around the face $\\text{left}(\\vec{e})$\n\nFinally, knowing these functions tell you absolutely everything about the topology of the subdivision, and any polygonal subdivision of any surface (orientable or not) can be encoded using these three functions.\nThe quad-edge data structure is a particularly convenient representation of a surface graph that provides access to all these functions, along with several other constant-time operations like inserting, deleting, contracting, expanding, and flipping edges; splitting or merging vertices or faces; and adding or deleting handles or cross-caps.\nHave fun!", "meta": {"post_id": 4746, "input_score": 18, "output_score": 31, "post_title": "The quad-edge data structure (Delaunay/Voronoi)"}}
{"input": "This question is inspired by a similar question about applied mathematics on mathoverflow, and that nagging thought that important questions of TCS such as P vs. NP might be independent of ZFC (or other systems). As a little background, reverse mathematics is the project of finding the axioms necessary to prove certain important theorems. In other words, we start at a set of theorems we expect to be true and try to derive the minimal set of 'natural' axioms that make them so. \nI was wondering if the reverse mathematics approach has been applied to any important theorems of TCS. In particular to complexity theory. With deadlock on many open questions in TCS it seems natural to ask \"what axioms have we not tried using?\". Alternatively, have any important questions in TCS been shown to be independent of certain simple subsystems of second-order arithmetic?", "output": "Yes, the topic has been studied in proof complexity. It is called Bounded Reverse Mathematics. You can find a table containing some reverse mathematics results on page 8 of Cook and Nguyen's book, \"Logical Foundations of Proof Complexity\",  2010. Some of Steve Cook's previous students have worked on similar topics, e.g. Nguyen's thesis, \"Bounded Reverse Mathematics\", University of Toronto, 2008.\nAlexander Razborov (also other proof complexity theorists) has some results on the weak theories needed to formalize the circuit complexity techniques and prove circuit complexity lowerbounds. He obtains some unprovability results for weak theories, but the theories are considered too weak.\nAll of these results are provable in $RCA_0$ (Simpson's base theory for Reverse Mathematics), so AFAIK we don't have independence results from strong theories (and in fact such independence results would have strong consequences as Neel has mentioned, see Ben-David's work (and related results) on independence of $\\mathbf{P} vs. \\mathbf{NP}$ from $PA_1$ where $PA_1$ is an extension of $PA$).", "meta": {"post_id": 4816, "input_score": 38, "output_score": 24, "post_title": "Axioms necessary for theoretical computer science"}}
{"input": "I think that a size hierarchy theorem for circuit complexity can be a major breakthrough in the area.\nIs it an interesting approach to class separation?\nThe motivation for the question is that we have to say\n\nthere is some function that cannot be computed by size $f(n)$ circuits and can be computed by a size $g(n)$ circuit where $f(n)<o(g(n))$. (and possibly something regarding the depth)\n\nso, if $f(m)g(n) \\leq n^{O(1)}$, the property seem to be unnatural (it violates the largeness condition). Clearly we can't use diagonalization, because we aren't in a uniform setting. \nIs there a result in this direction?", "output": "In fact it is possible to show that, for every $f$ sufficiently small (less than $2^n/n$), there are functions computable by circuits of size $f(n)$ but not by circuits of size $f(n)-O(1)$, or even $f(n)-1$, depending on the type of gates that you allow.\nHere is a simple argument that shows that there are functions computable in size $f(n)$ but not size$ f(n)-O(n)$.\nWe know that:\n\nthere is a function $g$ that requires circuit complexity at least $2^n/O(n)$, and, in particular, circuit complexity more than $f(n)$.\nthe function $z$ such that $z(x)=0$ for every input $x$ is computable by a constant-size circuit.\nif two functions $g_1$ and $g_2$ differ only in one input, then their circuit complexity differs by at most $O(n)$\n\nSuppose that $g$ is nonzero on $N$ inputs. Call such inputs $x_1,\\ldots,x_N$. We can consider, for each $i$, the function $g_i(x)$ which is the indicator function of the set $\\{ x_1,\\ldots,x_i \\}$; thus $g_0=0$ and $g_N=g$.\nClearly there is some $i$ such that $g_{i+1}$ has circuit complexity more than $f(n)$ and $g_i$ has circuit complexity less than $f(n)$. But then $g_{i}$ has circuit complexity less than $f(n)$ but more than $f(n) - O(n)$.", "meta": {"post_id": 5110, "input_score": 18, "output_score": 33, "post_title": "Hierarchy theorem for circuit size"}}
{"input": "I understand that Turing completeness requires unbounded memory and unbounded time.\nHowever there is a finite amount of atoms in this universe, thus making memory bounded. For example even though $\\pi$ is irrational there is no way to store more than a certain number of digits even if all the atoms in the universe were used for this purpose.\nWhat then are the limits of computability of an implemented Turing machine (which could use all the resources of the universe but no more) based on the limits of the universe? What is the maximum number of digits of $\\pi$? Are there any papers on this subject that might be interesting to read?", "output": "Seth Lloyd has a paper on the subject. You need energy to compute, but if you put too much energy into a small region, it forms a black hole. This slows down time (making the time it takes for the computation to complete relatively longer), and any computation done in the interior of a black hole is wasted, as the results cannot be extracted from the black hole and used. Seth calculates the limits on the amount of computation possible, and shows that for some measures of computation, the most computationally intensive environment possible in the universe would be that surrounding a black hole.", "meta": {"post_id": 5157, "input_score": 19, "output_score": 36, "post_title": "What are the limits of computation in this universe?"}}
{"input": "Is there a way to prove the following theorem in Coq?\nTheorem bool_pirrel : forall (b : bool) (p1 p2 : b = true), p1 = p2.\nEDIT: An attempt to give a brief explanation for \"what proof irrelevance is\" (correct me someone if I am wrong or inaccurate)\nThe basic idea is that in the proposition world (or the Prop sort in Coq), what you (and you should) really care about is the provability of a proposition, not the proofs of it, there may be many (or none). In case you have multiple proofs, from the provability point of view, they are equal in the sense that they prove the same proposition. So their distinction is just irrelevant. This differs from the computational point of view where you really care about the distinction of two terms, e.g., basically, you don't want the two inhabitants of the bool type (or Set in Coq's words), namely true and false to be equal. But if you put them in Prop, they are treated equal.", "output": "Proof irrelevance in general is not implied by the theory behind Coq. Even proof irrelevance for equality is not implied; it is equivalent to Streicher's axiom K. Both can be added as axioms.\nThere are developments where it's useful to reason about proof objects, and proof irrelevance makes this nigh-impossible. Arguably these developments should have all the objects whose structure matters recast in Set, but with the basic Coq theory the possibility is there.\nThere is an important subcase of proof irrelevance that always holds. Streicher's axiom K always holds on decidable domains, i.e. equality proofs on decidable sets are unique. The general proof is in the Eqdep_dec module in the Coq standard library. Here's your theorem as a corollary (my proof here is not necessarily the most elegant):\nRequire Bool.\nRequire Eqdep_dec.\nTheorem bool_pirrel : forall (b : bool) (p1 p2 : b = true), p1 = p2.\nProof.\n  intros; apply Eqdep_dec.eq_proofs_unicity; intros.\n  destruct (Bool.bool_dec x y); tauto.\nQed.\n\nFor this special case, here's a direct proof (inspired by the general proof in Eqdep_dec.v). First, define we define a canonical proof of true=b (as usual in Coq, it's easier to have the constant first). Then we show that any proof of true=b has to be refl_equal true.\nLet nu b (p:true = b) : true = b :=\n  match Bool.bool_dec true b with\n    | left eqxy => eqxy\n    | right neqxy => False_ind _ (neqxy p)\n  end.\nLemma bool_pcanonical : forall (b : bool) (p : true = b), p = nu b p.\nProof.\n  intros. case p. destruct b.\n  unfold nu; simpl. reflexivity.\n  discriminate p.\nQed.\n\nIf you add classical logic to Coq, you get proof irrelevance. Intuitively speaking, classical logic gives you a decision oracle for propositions, and that's good enough for axiom K. There is a proof in the Coq standard library module Classical_Prop.", "meta": {"post_id": 5158, "input_score": 23, "output_score": 31, "post_title": "Prove proof irrelevance in Coq?"}}
{"input": "This is a follow up question to \nWhat is the difference between proofs and programs (or between propositions and types)?\nWhat program would correspond to a non-constructive (classical) proof of the form $\\forall k \\ T(e,k) \\lor \\lnot \\forall k \\ T(e,k)$? (Assume that $T$ is some interesting decidable relation e.g. $e$-th TM does not halt in $k$ steps.)\n(ps: I am posting this question partly because I am interested in learning more about what Neel means by \"the Godel-Gentzen translation is a continuation-passing transformation\" in his comment.)", "output": "This an interesting question. Obviously one can't expect to have a program that decides for each $e$ whether $\\forall k T(e, k)$ holds or not, as this would decide the Halting Problem. As mentioned already, there are several ways of interpreting proofs computationally: extensions of Curry-Howard, realizability, dialectica, and so on. But they would all computationally interpret the theorem you mentioned more or less in the following way.\nFor simplicity consider the equivalent classical theorem\n(1) $\\exists i \\forall j (\\neg T(e, j) \\to \\neg T(e, i))$\nThis is (constructively) equivalent to the one mentioned because given $i$ we can decide whether $\\forall k T(e, k)$ holds or not by simply checking the value of $\\neg T(e, i)$. If $\\neg T(e, i)$ holds then $\\exists i \\neg T(e, i)$ and hence $\\neg \\forall i T(e, i)$. If on the other hand $\\neg T(e, i)$ does not hold then by (1) we have $\\forall j (\\neg T(e, j) \\to \\bot)$ which implies $\\forall j T(e, j)$. \nNow, again we can't compute $i$ in (1) for each given $e$ because we would again solve the Halting Problem. What all interpretations mentioned above would do is to look at the equivalent theorem\n(2) $\\forall f \\exists i' (\\neg T(e, f(i')) \\to \\neg T(e, i'))$\nThe function $f$ is called the Herbrand function. It tries to compute a counter example $j$ for each given potential witness $i$. It is clear that (1) and (2) are equivalent. From left to right this is constructive, simply take $i' = i$ in (2), where $i$ is the assumed witness of (1). From right to left one has to reason classically. Assume (1) was not true. Then, \n(3) $\\forall i \\exists j \\neg (\\neg T(e, j) \\to \\neg T(e, i))$ \nLet $f'$ be a function witnessing this, i.e. \n(4) $\\forall i \\neg (\\neg T(e, f'(i)) \\to \\neg T(e, i))$ \nNow, take $f = f'$ in (2) and we have $(\\neg T(e, f'(i')) \\to \\neg T(e, i'))$, for some $i'$. But taking $i = i'$ in (4) we obtain the negation of that, contradiction. Hence (2) implies (1).\nSo, we have that (1) and (2) are classically equivalent. But the interesting thing is that (2) has now a very simple constructive witness. Simply take $i' = f(0)$ if $T(e, f(0))$ does not hold, because then the conclusion of (2) is true; or else take $i' = 0$ if $T(e, f(0))$ holds, because then $\\neg T(e, f(0))$ does not hold and the premise of (2) is false, making it again true.\nHence, the way to computationally interpret a classical theorem like (1) is to look at a (classically) equivalent formulation which can be proven constructively, in our case (2). \nThe different interpretations mentioned above only diverge on the way the function $f$ pops up. In the case of realizability and the dialectica interpretation this is explicitly given by the interpretation, when combined with some form of negative translation (like Goedel-Gentzen's). In the case of Curry-Howard extensions with call-cc and continuation operators the function $f$ arises from the fact that the program is allowed to \"know\" how a certain value (in our case $i$) will be used, so $f$ is the continuation of the program around the point where $i$ is computed.\nAnother important point is that you want the passage from (1) to (2) to be \"modular\", i.e. if (1) is used to prove (1'), then its interpretation (2) should be used used in similar way to prove the interpretation of (1'), say (2'). All the interpretations mentioned above do that, including the Goedel-Gentzen negative translation.", "meta": {"post_id": 5245, "input_score": 30, "output_score": 25, "post_title": "Curry-Howard and programs from non-constructive proofs"}}
{"input": "If a problem is NP-hard (using polynomial time reductions), does that imply that it is P-hard (using log space or NC reductions)?  It seems intuitive that if it is as hard as any problem in NP that it should be as hard as any problem in P, but I don't see how to chain the reductions and get a log space (or NC) reduction.", "output": "No such implication is known.  In particular it may be that $L \\ne P = NP$ in which case all problems (including trivial ones) are NP-hard under poly-time reductions (as the reduction can just solve the problem), but trivial ones (in particular ones that lie in L) are surely not P-hard under logspace reductions (as otherwise L=P).  \nThe same goes for NC instead of L.", "meta": {"post_id": 5395, "input_score": 22, "output_score": 32, "post_title": "Does NP-hardness imply P-hardness?"}}
{"input": "Banach's fixed point theorem says that if we have a nonempty complete metric space $A$, then any uniformly contractive function $f : A \\to A$ it has a unique fixed point $\\mu(f)$. However, the proof of this theorem requires the axiom of choice -- we need to choose an arbitrary element $a \\in A$ to start iterating $f$ from, to get the Cauchy sequence $a, f(a), f^2(a), f^3(a), \\ldots$.   \n\nHow are fixed point theorems stated in constructive analysis? \nAlso, are there any concise references to constructive metric spaces?\n\nThe reason I ask is that I want to construct a model of System F in which the types additionally carry metric structure (among other things). It's rather useful that in constructive set theory, we can cook up a family of sets $U$, such that $U$ is closed under products, exponentials, and $U$-indexed families, which makes it easy to give a model of System F. \nIt would be very nice if I could cook up a similar family of constructive ultrametric spaces. But since adding choice to constructive set theory makes it classical, obviously I need to be more careful about fixed point theorems, and probably other stuff too.", "output": "The axiom of choice is used when there is a collection of \"things\" and you choose one element for each \"thing\". If there is just one thing in the collection, that's not the axiom of choice. In our case we only have one metric space and we are \"choosing\" a point in it. So that's not the axiom of choice but elimination of existential quantifiers, i.e., we have a hypothesis $\\exists x \\in A . \\phi(x)$ and we say \"let $x \\in A$ be such that $\\phi(x)$\". Unfortunately, people often say \"choose $x \\in A$ such that $\\phi(x)$\", which then looks like application of the axiom of choice.\nFor reference, here is a constructive proof of Banach's fixed point theorem.\nTheorem: A contraction on an inhabited complete metric space has a unique fixed point.\nProof. Suppose $(M,d)$ is an inhabited complete metric space and $f : M \\to M$ is a contraction. Because $f$ is a contraction there exists $\\alpha$ such that $0 < \\alpha < 1$ and $d(f(x), f(y)) \\leq \\alpha \\cdot d(x,y)$ for all $x, y \\in M$.\nSuppose $u$ and $v$ are fixed point of $f$. Then we have $$d(u,v) = d(f(u), f(v)) \\leq \\alpha d(u,v)$$ from which it follows that $0 \\leq d(u,v) \\leq (\\alpha - 1) d(u,v) \\leq 0$, hence $d(u,v) = 0$ and $u = v$. This proves that $f$ has at most one fixed point.\nIt remains to prove the existence of a fixed point. Because $M$ is inhabited there exists $x_0 \\in M$. Define the sequence $(x_i)$ recursively by $$x_{i+1} = f(x_i).$$ We can prove by induction that $d(x_i, x_{i+1}) \\leq \\alpha^i \\cdot d(x_0, x_1)$. From this it follows that $(x_i)$ is a Cauchy sequence. Because $M$ is complete, the sequence has a limit $y = \\lim_i x_i$. Since $f$ is a contraction, it is uniformly continuous and so it commutes with limits of sequences: $$f(y) = f(\\lim_i x_i) = \\lim_i f(x_i) = \\lim_i x_{i+1} = \\lim_i x_i = y.$$\nThus $y$ is a fixed point of $f$. QED\nRemarks:\n\nI was careful not to say \"choose $\\alpha$\" and \"choose $x_0$\". It is common to say such things, and they just add to the confusion that prevents ordinary mathematicians from being able to tell what is and isn't the axiom of choice.\nIn the uniqeness part of the proof people often assume unnecessarily that there are two different fixed points and derive a contradiction. This way they have only managed to prove that if $u$ and $v$ are fixed points of $f$ then $\\lnot\\lnot (u = v)$. So now they need excluded middle to get to $u = v$. Even for classical mathematics this is suboptimal and just shows that the author of the proof does not exercise good logical hygiene.\nIn the existence part of the proof, the sequence $(x_i)$ depends on the existential witness $x_0$ we get from eliminating the assumption $\\exists x \\in M . \\top$. There is nothing wrong with that. We do that sort of thing all the time. We did not choose anything. Think of it this way: someone else gave us a witness $x_0$ for inhabitedness of $M$, and we are free to do something with it.\nClassically, \"$M$ is inhabited\" ($\\exists x \\in M . \\top$) and \"$M$ is non-empty\" ($\\lnot\\forall x \\in M . \\bot$) are equivalent. Constructively, the former makes more sense and is useful.\nBecause we showed uniqueness of fixed points we actually get a fixed-point operator $\\mathrm{fix}_M$ from contractions on $M$ to points of $M$, rather than just a $\\forall\\exists$ statement.\nFinally, the following fixed-point theorems have constructive versions:\n\nKnaster-Tarski fixed-point theorem for monotone maps on complete lattices\nBanach's fixed-point theorem for contractions on a complete metric space\nKnaster-Tarski fixed-point theorem for monotone maps on dcpos (proved by Pataraia)\nVarious fixed-point theorems in domain theory usually have constructive proofs\nRecursion theorem is a form of fixed-point theorem and it has a constructive proof\nI proved that Knaster-Tarski fixed-point theorem for monotone maps on chain-complete posets does not have a constructive proof. Similarly, the Bourbaki-Witt fixed-point theorem for progressive maps on chain-complete posets fails constructively. The counter-example for the later one comes from the effective topos: in the effective topos ordinals (suitably defined) form a set and the successor maps is progressive and has no fixed points. By the way, the successor map on the ordinals is not monotone in the effective topos.\n\n\nNow that's rather more information than you asked for.", "meta": {"post_id": 5922, "input_score": 15, "output_score": 22, "post_title": "Fixed point theorems for constructive metric spaces?"}}
{"input": "I'm having a little trouble fully understanding the final steps of Shor's factoring algorithm.\nGiven an $N$ we want to factor, we choose a random $x$ which has order $r$.\nThe first step involves setting up the registers and applying the Hadamard operator. The second step a linear operator is applied. The third step the second register is measured (I believe this step can be performed later instead). The fourth step the discrete Fourier transform is applied to the first register. Then we measure the first register.\nHere's where I'm a little hazy:\nWe get a measurement in the form $\\mid j , x^k \\textrm{mod} N \\rangle $.\nFrom this we can find the convergents of the fraction $ \\frac{j}{2^q} $ , the convergents are possible values of the order $ r $. Here do we just try all the convergents $ < N $ and if we don't find $ r $ as one of the convergents do we just start again?\nAlso how does the probability for possible values $ j $ differ? They way I see it they should all have the same probability but Shor's paper says this isn't the case?\nJust a little confused as some papers seem to say different things.\nThanks.", "output": "From this we can find the convergents\n  of the fraction $j/2^q$ , the convergents\n  are possible values of the order $r.$\n  Here do we just try all the\n  convergents $<N$ and if we don't find $r$\n  as one of the convergents do we just\n  start again?\n\nYou could; the algorithm works fairly fast if you do. If you want to reduce the expected number of quantum steps, you could also do some other tests; for example you should check whether $r$ is a small multiple of one of the convergents. But if you don't find $r$ after these extended tests, you need to start again.\n\nAlso how does the probability for\n  possible values $j$ differ? They way I\n  see it they should all have the same\n  probability but Shor's paper says this\n  isn't the case?\n\nI don't know whether I can help you more with this, because you haven't given me enough information for me to tell why you're confused. The probability for each value of $k$ in the fraction $k/r$ is (very nearly) the same. However, depending on exactly where $k/r$ falls between the adjacent values of $j/2^q$ and $(j+1)/2^q$, the probabilities of the specific values of $j$ differ.", "meta": {"post_id": 6085, "input_score": 28, "output_score": 47, "post_title": "Shor's factoring algorithm help"}}
{"input": "Greg Egan in his fiction \"Dark Integers\" (story about two universes with two different mathematics communicating by means of proving theorems around of inconsistence in arithmetic) claims that it is possible to build general purpose computer solely on existing internet routers using only its basic functionality of packet switching (and checksum correction, to be precise).\nIs this possible, in principle?\nUpdate.\nTo make the question more precise:\nWhat is an absolutely minimal set(s) of properties the router network must have that it will be possible to build general purpose computer on top of it?", "output": "This can be helpful:\nParasitic computing is an example of a potential technology that could be viewed simultaneously as a threat or healthy addition to the online universe. On the Internet, reliable communication is guaranteed by a standard set of protocols, used by all computers. These protocols can be exploited to compute with the communication infrastructure, transforming the Internet into a distributed computer in which servers unwittingly perform computation on behalf of a remote node. In this model, one machine forces target computers to solve a piece of a complex computational problem merely by engaging them in standard communication.\nIn the parasitic computing site you can detailed information on how you can solve a 3-SAT problem using the checksum of TCP packets.\nOther useful links:\n\nSeminar report on parasitic computing by K.K.Maharana\nNature's article on parasitic computing (Aug 2001)", "meta": {"post_id": 6713, "input_score": 14, "output_score": 21, "post_title": "Dark Integers: General Purpose Computations on Internet Routers"}}
{"input": "In one sentence: would the existence of a hierarchy for $\\mathsf{BPTIME}$ imply any derandomization results? \nA related but vaguer question is: does the existence of a hierarchy for $\\mathsf{BPTIME}$ imply any difficult lower bounds? Does the resolution of this problem hit against a known barrier in complexity theory?\nMy motivation for this question is to understand the relative difficulty (with respect to other major open problems in complexity theory) of showing a hierarchy for $\\mathsf{BPTIME}$. I am assuming that everyone believes that such a hierarchy exists, but please correct me if you think otherwise.\nSome background: $\\mathsf{BPTIME}(f(n))$ contains those languages whose membership can be decided by a probabilistic Turning machine in time $f(n)$ with bounded probability of error. More precisely, a language $L \\in \\mathsf{BPTIME}(f(n))$ if there exists a probabilistic Turing machine $T$ such that for any $x \\in L$ the machine $T$ runs in time $O(f(|x|))$ and accepts with probability at least $2/3$, and for any $x \\not \\in L$, $T$ runs in time $O(f(|x|))$ and rejects with probability at least $2/3$. \nUnconditionally, it is open whether $\\mathsf{BPTIME}(n^c) \\subseteq \\mathsf{BPTIME}(n)$ for all $c > 1$. Barak showed that there exists a strict hierarchy for $\\mathsf{BPTIME}$ for machines with $O(\\log n)$ advice. Fortnow and Santhanam improved this to 1 bit of advice. This leads me to think that a proving the existence of a probabilistic time hierarchy is not that far off. On the other hand, the result is still open and I cannot find any progress after 2004. References, as usual, can be found in the Zoo.\nThe relation to derandomization comes from Impagliazzo and Wigderson's results: they showed that under a plausible complexity assumption, $\\mathsf{BPTIME}(n^d) \\subseteq \\mathsf{DTIME}(n^c)$ for any constant $d$ and some constant $c$. By the classical time-hierarchy theorems for deterministic time, this implies a time hierarchy for probabilistic time. I am asking the converse question: does a probabilistic hiearchy hit against a barrier related to proving derandomization results?\n\nEDIT: I am accepting Ryan's answer as a more complete solution.\nIf anyone has observations about what stands between us and proving the existence of a hierarchy for probabilistic time, feel free to answer/comment. Of course, the obvious answer is that $\\mathsf{BPTIME}$ has a semantic definition that defies classical techniques. I am interested in less obvious observations.", "output": "Let PTH be the hypothesis that there exists a probabilistic time hierarchy. Suppose the answer to your question is true, i.e., \"PTH implies $BPP \\subseteq TIME[2^{n^{c}}]$\" for some fixed $c$. Then, $EXP \\neq BPP$ would be unconditionally true. Consider two cases:\n\nIf PTH is false, then $EXP \\neq BPP$. This is the contrapositive of what Lance noted.\nIf PTH is true, then \"PTH implies $BPP \\subseteq TIME[2^{n^{c}}]$\" so again $EXP \\neq BPP$.\n\nIn fact, even an infinitely-often derandomization of BPP under PTH would entail $EXP \\neq BPP$ unconditionally. So whatever barriers apply to proving $EXP \\neq BPP$, they apply to proving statements of the kind \"PTH implies derandomization\".", "meta": {"post_id": 6748, "input_score": 31, "output_score": 24, "post_title": "Hierarchy for BPP vs derandomization"}}
{"input": "I wonder why computer scientist have chosen recursor instead of iterator (or tail recursor if you like) in primitive recursion, given that function defined in terms of iteration behaves more efficiently than that in terms of recursion.\nEDIT:\nLet us go further, do you think iteration would be better than recursion to complexity?", "output": "Iterators of any kind are just a form of recursion, i.e., a thing \"using itself\". For example, the basic equation governing the while loop is\n\n(while b do C)  =  (if b then (C; while b do C))\n\nYou may think I am doing something exotic, but as soon as you try to explain what recursion and iteration mean, you will end up writing such equations. For example, people tend to explain the while loop by saying things like \"and so on\" (imprecise) or \"do C while b\" (not explaining anything), or \"keep doing\" (circular explanation). The above equation tells you everything there is to know about the while loop. Similar arguments can be made about other forms of iteration.\nThe above equation is recursive, as the defined term appears on both sides. The general form is $$x = \\phi(x),$$ which of course is just a general recursive definition of $x$. In this sense iteration is just a special form of recursion. So as far as theory is concerned, there is pratically no difference between recursion and iteration, except that the latter is a special case of the former. And both concepts are special cases of fixed point equations, a very important and general mathematical concept. This answers why \"they chose recursor instead of iterator\".\nYour claim that iteration is more efficient than recursion is bogus. Modern programming languages are quite efficient at both of these tasks. In any case, it is not the job of humans to think about low-level details of execution of programs (maybe that was the case in the 1960's when the total amount of CPU power and memory on the planet was comparable to what kids carry in their pockets these days). We have better things to do as programmers, such as how to write programs most efficiently and how to make sure they are actually correct.", "meta": {"post_id": 7029, "input_score": 5, "output_score": 31, "post_title": "Why have computer scientists chosen recursor instead of iterator in primitive recursion?"}}
{"input": "This may be considered a stupid question. I am not a computer science major (and I'm not a mathematics major yet, either), so please excuse me if you think that the following questions display some major erroneous assumptions.\nWhile there are plans to formalize Fermat's Last Theorem (see this presentation), I have never read or heard that a computer can prove even a \"simple\" theorem like Pythagoras'. \nWhy not? What is (/are) the main difficulty(/ies) behind establishing a fully autonomous proof by a computer, aided only by some \"built-in axioms\"? \nA second question I would like to ask is the following: Why are we able to formalize many proofs, while it is currently impossible for a computer to prove a theorem on its own? Why is that \"harder\" ?", "output": "While there are plans to formalize Fermat's Last Theorem (see this presentation), I have never read or heard that a computer can prove even a \"simple\" theorem like Pythagoras'. \n\nIn 1949 Tarski proved that almost everything in The Elements lies within a decidable fragment of logic, when he showed the decidability of the first-order theory of real closed fields. So the Pythagorean theorem in particular is not talked about much because it's not especially hard. \nIn general, the thing that makes theorem proving hard is induction. First-order logic without induction has a very useful property called the subformula property: true formulas $A$ have proofs involving only the subterms of $A$. This means that it's possible to build theorem provers which can decide what to prove next based on an analysis of the theorem they are instructed to prove. (Quantifier instantiation can make the right notion of subformula a bit more subtle, but we have reasonable techniques to cope with this.) \nHowever, the addition of the induction schema to the axioms breaks this property. The only proof of a true formula $A$ may require doing a proof $B$ which is not syntactically a subformula of $A$. When we run into this in a paper proof, we say we have to \"strengthen the induction hypothesis\". This is quite hard for a computer to do, because the appropriate strengthening can require both significant domain-specific information, and an understanding of why you're proving a particular theorem. Without this information, truly relevant generalizations can get lost in a forest of irrelevant ones.", "meta": {"post_id": 7129, "input_score": 18, "output_score": 23, "post_title": "Why is it so difficult for a computer to prove something?"}}
{"input": "What is the average number of publications (including conference proceedings), per year, for CS postdoctoral researchers in the US? What are the ways to find out or estimate this number?", "output": "The main motive behind this question is to understand where am I standing with my own performance and productivity.\nThen you're asking the wrong question.  The right metric to examine isn't the number of publication, but rather your visibility and reputation (and ultimately impact) within the research community.  If the intellectual leaders of your target subcommunity are eager to write you good recommendation letters, it doesn't matter whether you publish one paper a year or ten.  Similarly, if the intellectual leaders in your target subcommunity are not eager to write you good recommendation letters, it doesn't matter whether you publish one paper a year or ten.", "meta": {"post_id": 7559, "input_score": 9, "output_score": 23, "post_title": "The average number of annual publications for CS postdocs"}}
{"input": "As stated in the title, I wonder any relation and difference between CIC and ITT. Could someone explain or point to me some literature that compares these two systems? Thanks.", "output": "I've already answered somewhat, but I'll try to give a more detailed overview of the type theoretical horizon, if you will.\nI'm a bit fuzzy on the historical specifics, so more informed readers will have to forgive me (and correct me!). The basic story is that Curry had uncovered the basic correspondence between simply-typed combinators (or $\\lambda$-terms) and propositional logic, which was extended by Howard to cover first-order logic, and IIRC independently discovered by de Bruijn in investigations around the hugely influential Automath system.\nThe Automath system was a refinement of Church's simple type theory which itself was a dramatic simplification of Russel and Whitehead's type theory with universes and the axiom of reducibility. This was relatively well-known logical terrain by the 1960s.\nHowever, giving a coherent, simple, foundational system that encompassed both proof and term systems was still a very open question by 1970, and a first answer was given by Per Martin-L\u00f6f. He gives a very philosophical overview On the Meaning of the Logical Constants and the Justification of Logical Laws. He reasons that both in logic and mathematics, the meaning of constructions can be given by examining the introduction rules which allow the formation of those constructions as judgements, e.g. for conjunction\n$$ \\frac{\\vdash A\\quad \\vdash B}{\\vdash A\\wedge B}$$\nDetermines the corresponding elimination rule. He then gave a very powerful foundational system based on such judgements, allowing him to give a foundational system similar to Automath using very few syntactic constructions. Girard found that this system was contradictory, prompting Martin-L\u00f6f to adopt \"Russel-style\" predicative universes, severely limiting the expressiveness of the theory (by effectively removing the axiom of reducibility) and making it slightly more complex (but had the advantage of making it consistent).\nThe elegant constructions allowing for the definition of the logical symbols didn't work anymore though, which prompted M-L to introduce them in a different form, as inductively defined families. This is a very powerful idea, as it allows defining everything from judgmental equality and logical operators to natural numbers and functional data-types as they appear in computer science. Note that each family we add is akin to adding a number of axioms, which need to be justified as consistent in each instance. This system (dependent types + universes + inductive families) is usually what is referred to as ITT.\nHowever there was some lingering frustration, as the powerful but simple foundational system was inconsistent, and the resulting system was more complex, and somewhat weak (in the sense that it was difficult to develop much of the modern mathematical framework in it). Enter Thierry Coquand, who with his supervisor Gerard Huet, introduced the Calculus of Constructions (CoC), which mostly solved these issues: a unified approach to proofs and data-types, a powerful (impredicative) foundational system and the ability to define \"constructions\" of the logical or mathematical variety. This eventually matured into an actual implementation of a system designed as a modern alternative to Automath, culminating in the Coq system we know and love.\nI highly suggest this foundational paper on CoC, as Thierry knows a ridiculous amount about the historical development of type theory, and probably explains this much better than I. You might also want to check out his article on type theory, though it doesn't explain the C-H correspondence in much detail.", "meta": {"post_id": 7561, "input_score": 37, "output_score": 29, "post_title": "What's the relation and difference between Calculus of Inductive Constructions and Intuitionistic Type Theory?"}}
{"input": "I haven't managed to find this data structure, but I'm not an expert in the field.\nThe structure implements a set, and is basically an array of comparable elements with an invariant. The invariant is the following (defined recursively):\nAn array of length 1 is a merge-array.\nAn array of length 2^n (for n > 0) is a merge-array iff:\n\nthe first half is a merge-array and the second half is empty\nor\nthe first array is full and sorted, and the second half is a merge-array.\n\nNote that if the array is full, it is sorted.\nTo insert an element, we have two cases:\n\nIf the first half is not full, insert recursively in the first half.\nIf the first half is full, insert recursively in the second half.\nAfter the recursive step, if the whole array is full, merge the\nhalves (which are sorted), and resize it to the double of its original\nlength.\n\nTo find an element, recurse in both halves, using binary search\nwhen the array is full. (This should be efficient since there are\nat most $O(\\log(n))$ ascending fragments).\nThe structure can be thought as a static version of mergesort.\nIt's not clear what one should do to erase an element.\nEdit: after improving my understanding of the structure.", "output": "You're describing the classical Bentley-Saxe logarithmic method, applied to static sorted arrays.  The same idea can be used to add support for insertions to any static data structure (no insertions or deletions) for any decomposable searching problem.  (A search problem is decomposable if the answer for any union $A\\cup B$ can be computed easily from the answers for the sets $A$ and $B$.)  The transformation increases the amortized query time by a factor of $O(\\log n)$ (unless it was already bigger than some polynomial in $n$), but increases the space by only a constant factor.  Yes, it can be deamortized, thanks to Overmars and van Leeuwen, but you really don't want to do that if you don't have to.\nThese notes cover the basics.\nCache-oblivious lookahead arrays are the mutant offspring of Bentley-Saxe and van Emde Boas trees on steroids.", "meta": {"post_id": 7642, "input_score": 27, "output_score": 31, "post_title": "I dreamt of a data structure, does it exist?"}}
{"input": "It is known via the universal approximation theorem that a neural network with even a single hidden layer and an arbitrary activation function can approximate any continuous function.\nWhat other models are there that are also universal function approximators", "output": "This is treated extensively in the statistics literature, under the topic of regression.  Two standard references here are Wasserman's book \"all of nonparametric statistics\" and Tsybakov's \"introduction to nonparametric estimation\".  I'll talk briefly about some of the standard stuff, and try to give pointers outside of statistics (this is a common topic and different fields have different cultures: prove different kinds of theorems, make different assumptions).\n\n(Kernel regressors, sometimes called the Nadaraya-Watson Estimator.)  Here you write the function at any point as a weighted combination of nearby values. More concretely, \nsince this is in the statistics literature, you typically suppose you have some examples $((x_i,f(x_i)))_{i=1}^n$ drawn from some distribution,\nand fix some kernel $K$ (can think of this as a gaussian, but zero mean is what matters most), and write\n$$\r\n\\hat f(x) := \\sum_i f(x_i) \\left(\\frac{ K(c_n(x-x_i)) }{ \\sum_j K(c_n(x-x_j))}\\right),\r\n$$\nwhere $c_n\\to\\infty$ (you are more sensitive to small distances as $n$ increases).\nThe guarantee is that, as $n\\to\\infty$, a probilistic criterion of distortion\n(expectation of sup-norm, high probability, whatever) goes to zero.\n(It hardly matters what $K$ looks like---it matters more how you choose $c_n$.)\n(Basis methods.)  A similar thing is to choose some family of \"basis functions\", things like Wavelets or piecewise linear functions, but really anything that forms a (possibly overcomplete) basis for the vector space $L^2$, and determine a weighted linear combination of scaled and translated elements. \nThe techniques here differ drastically from (1.); rather than plopping down basis functions centered at data points, you carefully compute the weight and location of each in order to minimize some distortion criterion.  (Typically, their quantity is fixed a priori.) One approach is \"basis pursuit\", where you greedily add in new functions while trying to minimize some approximation error between $\\hat f$ and $f$.  To get a sense of the diversity of approaches here, a neat paper is Rahimi & Recht's \"uniform approximation of functions with random bases\".  Perhaps I should say that the grand-daddy of all of these is the Fourier expansion; there's a lot of good material on this in Mallat's book on Wavelets.\n(Tree methods.)  Another way is to look at a function as a tree; at each level, you are working with some partition of the domain, and return, for instance, the average point.  (Each pruning of the tree also gives a partition.)  In the limit, the fineness of this partition will no longer discretize the function, and you have reconstructed it exactly.  How best to choose this partition is a tough problem. (You can google this under \"regression tree\".)\n(Polynomial methods; see also splines and other interpolating techniques.)  By Taylor's theorem, you know that you can get arbitrarily close to well behaved functions.  This may seem like a very basic approach (i.e., just use the Lagrange interpolating polynomial), but where things get interesting is in deciding which points to interpolate.  This was investigated extensively in the context of numerical integration; you can find some amazing math under the topics of \"clenshaw-curtis quadrature\" and \"gaussian quadrature\".  I'm throwing this in here because the types of assumptions and guarantees here are so drastically different than what appears above.  I like this field but these methods suffer really badly from the curse of dimension, at least I think this is why they are less discussed than they used to be (if you do numeric integration with mathematica, I think it does quadrature for univariate domains, but sampling techniques for multivariate domains).\n\nConsidering various restrictions to your function class, you can instantiate the above to get all sorts of other widely-used scenarios.  For instance, with boolean valued functions, thresholding (1.) will look a lot like a nearest-neighbor estimator, or an SVM with some local kernel (gaussian).  A lot of the above stuff suffers from curse of dimension (bounds exhibit exponential dependence on the dimension).  In machine learning you get around this either by explicitly constraining your class to some family (i.e., \"parametric methods), or by an implicit constraint, usually something relating the quality of the approximants to the target function complexity (i.e., an analog of the weak learning assumption in boosting).\nBy the way, my favorite theorem related to neural net approximation  is Kolmogorov's superposition theorem (from 1957!).  It says that any multivariate continuous function $f:\\mathbb{R}^d \\to \\mathbb{R}$ has the form\n$$\r\nf(x) = \\sum_{j=0}^{2d}h_j\\left(\\sum_{i=1}^d g_{j,i}(x_i)\\right),\r\n$$\nwhere each $g_{j,i} : \\mathbb{R}\\to\\mathbb{R}$ and $h_j:\\mathbb{R}\\to\\mathbb{R}$ is (univariate) continuous.  Note that unlike neural nets, the $g$'s and $h$'s may all differ.  But even so, given that there are only $\\Theta(d^2)$ functions floating around, I find this totally amazing.\n(You only asked about function classes, but I figured you'd be interested in methods as well.. if not.. oops)", "meta": {"post_id": 7894, "input_score": 18, "output_score": 27, "post_title": "Universal Function approximation"}}
{"input": "Let PRIMES (a.k.a. primality testing) be the problem: \n\nGiven a natural number $n$, is $n$ a prime number?\n\nLet FACTORING be the problem: \n\nGiven natural numbers $n$, $m$ with $1 \\leq m \\leq n$, does $n$ have a factor $d$ with $1 < d < m$?\n\nIs it known whether PRIMES is P-hard? How about FACTORING? \nWhat are the best known lowerbounds for these problems?", "output": "Factoring can be achieved by using a polylog $n$ depth quantum circuit, and ZPP pre- and post-processing; see this paper. If it were P-hard, any algorithm in P could be done with polylog $n$ depth quantum circuit and the same pre- and post-processing steps. I believe these steps are modular exponentiation and continued fractions, which to me seem unlikely to be powerful enough to solve P-complete problems, even with the addition of a polylog $n$ depth quantum circuit.", "meta": {"post_id": 8000, "input_score": 42, "output_score": 30, "post_title": "Are the problems PRIMES, FACTORING known to be P-hard?"}}
{"input": "The Satisfiability problem is, of course, a fundamental problem in theoretical CS. I was playing with one version of the problem with infinitely many variables. $\\newcommand{\\sat}{\\mathrm{sat}} \\newcommand{\\unsat}{\\mathrm{unsat}}$\nBasic Setup. Let $X$ be a nonempty and possibly infinite set of variables. A literal is either a variable $x \\in X$ or its negation $\\neg x$. A clause $c$ is a disjunction of finite number of literals. Finally, we define a formula $F$ as a set of clauses. \nAn assignment of $X$ is a function $\\sigma : X \\to \\{0,1\\}$. I will not explicitly define the condition for when an assignment $\\sigma$ satisfies a clause; it is slightly cumbersome, and is the same as in standard SAT. Finally, an assignment satisfies a formula if it satisfies every constituent clause. Let $\\sat(F)$ be the set of satisfying assignments for $F$, and let $\\unsat(F)$ be the complement of $\\sat(F)$. \nA topological space. \nOur goal is to endow the space of all assignments of $X$, call this $\\Sigma$, with a topological structure. Our closed sets are of the form $\\sat(F)$ where $F$ is a formula. We can verify that this is indeed a topology:\n\nThe empty formula $\\emptyset$ containing no clauses is satisfied by all assignments; so $\\Sigma$ is closed.\nThe formula $\\{ x, \\neg x \\}$ for any $x \\in X$ is a contradiction. So $\\emptyset$ is closed.\nClosure under arbitrary intersection. Suppose $F_{i}$ is a formula for each $i \\in I$. Then $\\sat \\left(\\bigcup_{i \\in I} F_i\\right) = \\bigcap_{i \\in I} \\sat(F_i)$.\nClosure under finite union. Suppose $F$ and $G$ are two formulas, and define\n$$\r\nF \\vee G := \\{ c \\vee d \\,:\\, c \\in F, d \\in G \\}.\r\n$$ \nThen $\\sat(F \\vee G) = \\sat(F) \\cup \\sat(G)$.This needs an argument, but I'll skip this.\n\nCall this topology $\\mathcal T$, the \"satisfiability topology\"(!) on $\\Sigma$. Of course, the opens sets of this topology are of the form $\\unsat(F)$. Moreover, I observed that the collection of open sets\n$$\r\n\\{ \\unsat(c) \\,:\\, c \\text{ is a clause} \\}\r\n$$\nforms a basis for $\\mathcal T$. (Exercise!) \nCompact? I feel that this is an interesting, if not terribly useful, way to look at things. I want to understand whether this topological space possesses traditional interesting properties like compactness, connectedness etc. In this post, we will restrict ourselves to compactness:\n\nLet $X$ be a countably infinite collection of variables.1 Is $\\Sigma$ compact under $\\mathcal T$? \n\nOne can prove the following\n\nProposition. $\\mathcal T$ is compact if and only for all unsatisfiable formulas $F$, there exists a finite unsatisfiable subformula $\\{ c_1, c_2, \\ldots, c_m \\} \\subseteq F$.\n\n(Not-so-hard exercise!) After several days of thinking, I do not have much progress in answering this question. I also do not have strong evidence for or against compactness. Can you suggest some approach? \nFinally, as a bonus question: \n\nHas such a structure been studied before? \n\n1The restriction to countable $X$ is just for simplicity; it also feels like the next natural step from finite number of variables.", "output": "What you are doing is deriving a topological representation of a Boolean algebra. The study of representations of Boolean algebras goes back at least to Lindenbaum and Tarski who proved (in 1925, I think) that the complete, atomic Boolean algebras are isomorphic to powerset lattices. \nThere are however, Boolean algebras that are not complete and atomic. For example, the sequence $x_1, x_1 \\land x_2, \\ldots$, is a descending chain that has no limit in the Boolean algebra defined over formulas. The question of whether arbitrary Boolean algebras, such as the one you mention, also had set-based representations was solved by Marshall Stone, who put forth the maxim \"always topologize\" (Marshall H. Stone. The representation of Boolean algebras, 1938).\n\nStone's Representation Theorem for Boolean Algebras Every Boolean algebra is isomorphic to the lattice of clopen subsets of a topological space.\n\nThe main idea is to consider what in your case are the satisfying assignments to a formula. In the general case, you consider  homomorphisms from a Boolean algebra into the two element Boolean algebra (the truth values). The inverse of $\\mathit{true}$ gives you the sets of satisfying assignments, or what are called ultrafilters of the Boolean algebra. From these, one can obtain a topology called the spectrum or Stone space of a Boolean algebra. Stone provide the answer to your question too.\n\nThe Stone space of a  Boolean algebra is a compact, totally disconnected Hausdorff space.  \n\nThere have been several results that extend and generalise Stone's representation in various directions. A natural question is to ask if other families of lattices have such representations. Stone's results also apply to distributive lattices. Topological representations for arbitrary lattices were given by Alasdair Urquhart in 1978. Distributive lattices enjoy greater diversity in structure, compared to Boolean algebras and are of great interest. A different  representation for the distributive case was given by Hilary Priestley in 1970, using the idea of an ordered topological space. Instead of set-based representations, we can find poset-based representations and topologies.\nThe constructions in these papers have one remarkable property. Stone's construction maps not just Boolean algebras to topological spaces: structural relationships relating Boolean algebras translate into structural properties between the resulting topologies. It is a duality between categories. The entire gamut of such results is called Stone Duality. Informally, dualities give us precise translations between mathematical universes: the combinatorial world of sets, the algebraic world of lattices, the spatial world of topology and the deductive world of logic. Here are a few starting points that may help.\n\nChapter 11 of Introduction to Lattices and Order, by Davey and Priestley covers Stone's theorem.\nMatthew Gwynne's slides  cover the theorem and give a proof of compactness. Matthew (in the comments) also suggests Introduction to Boolean Algebras by Paul Halmos.\nIn moving from propositional logic to modal logic, the Boolean algebra is extended with a join-preserving operator and topology with an interior.  J\u00f3nsson and Tarski's 1952 paper, Boolean Algebras with Operators is extremely readable and consistent with modern notation. \nChapter 5 of Modal Logic by Blackburn, de Rijke and Venema covers Stone's theorem and its extension to Boolean algebras with operators.\nStone Spaces by Peter Johnstone reviews such results for various other kinds of algebras.", "meta": {"post_id": 8095, "input_score": 19, "output_score": 22, "post_title": "A topological space related to SAT: is it compact?"}}
{"input": "Why are regular languages (and from that regular expressions) called \"regular\"? There is lot of regularity also in context-free languages other types of languages.\nI suppose that, in the beginning, the adjective \"regular\" has been used to differentiate that type of languages from other \"non-regular\" or somehow abnormal languages. If so, what where these other types, and what was their non-regularity?", "output": "A quick check of the sources reveals that Chomsky called the levels of his hierarchy just \u201ctype 0, type 1, type 2, type 3\u201d. He mentions in a footnote that his type 3 corresponds to \u201cregular events\u201d of Kleene. Kleene wrote there: We shall presently describe a class of events which we will call \"regular events.\" (We would welcome any suggestions as to a more descriptive term.)\nIt would thus appear that the term is a historical accident, and in any case has no bearing on the relation of regular languages to context-free languages.", "meta": {"post_id": 8739, "input_score": 29, "output_score": 40, "post_title": "Why are regular languages called \"regular\"?"}}
{"input": "During my work i came up with the following problem:\nI am trying to find an $n \\times n$ $(0,1)$-matrix $M$, for any $n > 3$, with the following properties:\n\nThe determinant of $M$ is even.\nFor any non-empty subsets $I,J\\subseteq\\{1,2,3\\}$ with $|I| = |J|$, the submatrix $M^I_J$ has odd determinant if and only if $I=J$.  \n\nHere $M^I_J$ denotes the submatrix of $M$ created by removing the rows with indices in $I$ and the columns with indices in $J$.\nSo far, I tried to find such a matrix via random sampling but I am only able to find a matrix that has all properties except the first one, i.e., the matrix always has an odd determinant. I tried various dimensions and different input/output sets without any success. So this makes me think: \nIs that there is a dependency among the requirements, which prevents them from being simultaneously true?\nor\nIs it possible that such a matrix exists and can someone give me an example?\nThanks,\nEtsch", "output": "No such matrix exists.\nThe Desnanot-Jacobi identity says that for $i \\neq j$, \n$$\r\n\\det M_{ij}^{ij} \\det M = \\det M_i^i \\det M_j^j -\\det M_i^j \\det M_j^i\r\n$$\nso using this, we get\n$$\r\n\\det M_{12}^{12} \\det M = \\det M_{1}^{1} \\det M_{2}^{2} - \\det M_{1}^{2} \\det M_{2}^{1}\r\n$$\nBut your requirements force the left-hand-side to be 0 (mod 2) and the right-hand-side to be 1 (mod 2), showing they are incompatible.", "meta": {"post_id": 8883, "input_score": 10, "output_score": 22, "post_title": "Can such a matrix exist?"}}
{"input": "Are there any toy examples that provide 'essential' insights into understanding the three known barriers to $P = NP$ problem - relativization, natural proofs and algebrization?", "output": "Let me give a toy example of the relativization barrier. The canonical example is the time hierarchy theorem that ${\\bf TIME}[t(n)] \\subsetneq {\\bf TIME}[t(n)^2]$. The proof (by diagonalization) is only a little more involved than the proof that the halting problem is undecidable: we define an algorithm $A(x)$ which simulates the $x$th algorithm $A_x$ on input $x$ directly step-for-step for $t(|x|)$ steps, then outputs the opposite value. Then we argue that $A$ can be implemented to run in $t(|x|)^2$ time. \nThe argument works equally well if we equip all algorithms with access to an arbitrary oracle set $O$, which we assume we can ask membership queries to, in one step of computation. A step-for-step simulation of $A_x^O$ can also be carried out by $A$, as long as $A$ has access to the oracle $O$ too. In notation, we have ${\\bf TIME}^O[t(n)] \\subsetneq {\\bf TIME}^O[t(n)^2]$ for all oracles $O$. In other words, the time hierarchy relativizes.\nWe can define oracles for nondeterministic machines in a natural way, so it makes sense to define classes $P^O$ and $NP^O$ with respect to oracles. But there are oracles $O$ and $O'$ relative to which $P^O = NP^O$ and $P^{O'} \\neq NP^{O'}$, so this kind of direct simulation argument in the time hierarchy theorem won't work for resolving $P$ versus $NP$. Relativizing arguments are powerful in that they are widely applicable and have led to many great insights; but this same power makes them \"weak\" with respect to questions like $P$ versus $NP$. \nThe above is, of course, a toy example -- there are many other more complicated examples of arguments in complexity which still relativize (i.e., hold up when arbitrary oracles are introduced).", "meta": {"post_id": 8902, "input_score": 29, "output_score": 26, "post_title": "Toy examples for barriers to $P \\ne NP$"}}
{"input": "Shiva Kintali has just announced a (cool!) result that graph isomorphism for bounded treewidth graphs of width $\\geq 4$ is $\\oplus L$-hard.  Informally, my question is, \"How hard is that?\"\nWe know that nonuniformly $NL \\subseteq \\oplus L$, see the answers to this question.  We also know that it is unlikely that $\\oplus L = P$, see the answers to this question.  How surprising would it be if $L=\\oplus L$?  I have heard many people say that $L=NL$ would not be shocking the way $P=NP$ would.\n\nWhat are the consequences of $L=\\oplus L$?\n\nDefinition: $\\oplus L$ is the set of languages recognized by a non-deterministic Turing machine which can only distinguish between an even number or odd number of \"acceptance\" paths (rather than a zero or non-zero number of acceptance paths), and which is further restricted to work in logarithmic space.", "output": "Wigderson proved that $NL/poly \\subseteq \\oplus L/poly$. By standard arguments, $L = \\oplus L$ would imply $L/poly = NL/poly$. (Take a machine $M$ in $NL/poly$. It has an equivalent machine $M'$ in $\\oplus L/poly$. Take the $\\oplus L$ language of instance-advice pairs $S = \\{(x,a)~|~M'(x,a)~\\textrm{accepts}\\}$. If this language is in $L$, then by hardcoding the appropriate advice $a$ we get an $L/poly$ machine equivalent to $M$.) \nI think that would be surprising: nondeterministic branching programs would be equivalent to deterministic branching programs (up to polynomial factors).", "meta": {"post_id": 8991, "input_score": 26, "output_score": 23, "post_title": "What are the consequences of $L = \\oplus L$?"}}
{"input": "I am interested in a SAT variation where the CNF formula is monotone (no variables are negated). Such a formula is obviously satisfiable.\nBut say the number of true variables is a measure of how good our solution is.  So we have the following problem:\nMINIMUM TRUE MONOTONE 3SAT\nINSTANCE: Set U of variables, collection C of disjunctive clauses of 3 literals, where a literal is a variable (not negated).\nSOLUTION: A truth assignment for U that satisfies C.\nMEASURE:  The number of variable that are true.\nCould someone give me some helpful remarks on this problem?", "output": "This problem is the same as the Vertex Cover problem for $3$-uniform hypergraphs: \ngiven a collection $H$ of subsets of $V$ of size $3$ each, find a minimal subset $U\\subseteq V$ that intersects each set in $H$.\nIt is therefore NP-hard, but fixed parameter tractable. \nIt is also NP-hard to approximate to within a factor of $2-\\epsilon$ for every $\\epsilon>0$. This was shown in the following paper:\nIrit Dinur, Venkatesan Guruswami, Subhash Khot and Oded Regev.\nA New Multilayered PCP and the Hardness of Hypergraph Vertex Cover,\nSIAM Journal on Computing, 34(5):1129\u20131146, 2005.", "meta": {"post_id": 9084, "input_score": 12, "output_score": 21, "post_title": "Minimum True Monotone 3SAT"}}
{"input": "I will be attending my first computer science conference and after reading the advice for how to improve conferences I noticed the several suggestions were about grad students attending their first conference.\nWhat advice do you have for a grad student attending his first conference and what should his focus be.", "output": "Talk to people, even if they are scary big names. \nAttend all the keynote/invited presentations.\nAttend the talks most relevant to you.\nDon't be afraid to ask questions.\nAttend the social events, meet other graduate students, have fun.\nTalk enthusiastically about your research. \nMake sure you have a 1 minute pitch describing your work, plus a 5 minute description, and also be prepared to enter into a more detailed discussion.\nAsk people about their research.   Simply asking what are you working on? will get the conversation started.\nBe open to possible collaborations, and follow up after the conference.", "meta": {"post_id": 9091, "input_score": 25, "output_score": 29, "post_title": "Advice for attending my first TCS conference"}}
{"input": "This question may not be technical. As a non-native speaker and a TA for algorithm class, I always wondered what gadget means in 'clause gadget' or 'variable gadget'. The dictionary says a gadget is a machine or a device, but I'm not sure what colloquial meaning it has in the context of NP-complete proof.", "output": "A \"gadget\" is a small specialized device for some particular task. In NP-hardness proofs, when doing a reduction from problem A to problem B, the colloquial term \"gadget\" refers to small (partial) instances of problem B that are used to \"simulate\" certain objects in problem A. For example, when reducing 3SAT to 3-COLORING, clause gadgets are small graphs that are used to represent the clauses of the original formula and variable gadgets are small graphs that are used to represent the variables of the original formula. In order to ensure that the reduction is correct, the gadgets have to be graphs that can be 3-colored in very specific ways. Hence we think of these small graphs as devices that perform a specialized task.\nIn many cases, the main difficulty of proving NP-hardness is constructing appropriate gadgets. Sometimes these gadgets are complicated and moderately large. The creative process of creating such gadgets is sometimes called \"gadgeteering.\"", "meta": {"post_id": 9173, "input_score": 15, "output_score": 27, "post_title": "What does 'gadget' mean in NP-hard reduction?"}}
{"input": "The definition of Ramsey numbers is the following: \nLet $R(a,b)$ be a positive number such that every graph of order at least $R(a,b)$ contains either a clique on $a$ vertices or a stable set on $b$ vertices.\nI am working on some extension of Ramsey Numbers. While the study has some theoretical interest, it would be important to know the motivation of these numbers. More specifically I am wondering the (theoretical or practical) applications of Ramsey numbers. For instance, are there any solution methodology for a real life problem that uses Ramsey numbers? Or similarly, are there any proofs of some theorems based on Ramsey numbers?", "output": "Applications of Ramsey theory to CS, Gasarch", "meta": {"post_id": 9500, "input_score": 14, "output_score": 22, "post_title": "Application of Ramsey Numbers"}}
{"input": "Since it does not allow nonterminating computation, Coq is necessarily not Turing-complete. What is the class of functions that Coq can compute? (is there an interesting characterization thereof?)", "output": "Benjamin Werner has proved the mutual interpretability of ZFC with countably many inaccessibles and the Calculus of Inductive Constructions, in his paper Sets in Types, Types in Sets.  \nThis means, roughly, that any function which can be shown to be total in ZFC with countably many inaccessibles can be defined in Coq. So unless you are a set theorist working on large cardinals, it is unlikely that any computable function you have ever wanted cannot be defined in Coq.", "meta": {"post_id": 9775, "input_score": 25, "output_score": 21, "post_title": "Class of functions computable by Coq"}}
{"input": "What is the possibility for a computer scientist to change his field into a pure mathematician ? and what's the most smooth way to do it ? any examples for people who could make it ?", "output": "Theoretical computer scientists are already mathematicians.  So you don't need to do anything!\nYour later comment suggests that you're really asking about changing from one area of mathematics (like complexity theory) to another (like functional analysis).  The only way to do it is to just do it.  You want to be an analyst?  Great!  Just start doing analysis!  Read analysis books and papers, solve analysis problems, talk to lots of analyst faculty and students, attend analysis seminars, go to analysis conferences, ask analysis questions on MathOverflow, and so on.  Act like an analyst long enough (and well enough) and you'll become one.\nIn that respect, becoming a functional analyst is not so different from becoming anything else.  Want to be a programmer?  Program!  Want to be a writer?  Write!  Want to be a painter?  Paint!  Want to be a functional analyst?  Analyze functions!  (Meanwhile, don't forget to earn enough money to eat, and do not blow off the stupid administrative hurdles.)\nAnd yes, lots of people successfully change fields/careers.  For example, Joan Birman started her academic career in physics, worked for several years as a systems analyst in the aircraft industry, spent several more years at home raising kids, went back to grad school in mathematics in her 40s, and became one of the most influential researchers in modern low-dimensional topology and geometric group theory.  For other examples, see these questions on MathOverflow.\nOne last suggestion: You might find the transition somewhat smoother (possibly even real-analytic) if you drop your prejudices about \"pure\" versus \"applied\" mathematics.  Forget about the labels; just do the work.", "meta": {"post_id": 10170, "input_score": 9, "output_score": 21, "post_title": "About switching from a computer scientist to a mathematician"}}
{"input": "Are there any (functional?) programming languages where all functions have a canonical form? That is, any two functions that return the same values for all set of input is represented in the same way, e.g. if f(x) returned x + 1, and g(x) returned x + 2, then f(f(x)) and g(x) would generate indistinguishable executables when the program is compiled.\nPerhaps more importantly, where/how might I find more information on canonical representation of programs (Googling \"canonical representation programs\" has been less than fruitful)? It seems like a natural question to ask, and I'm afraid that I just don't know the proper term for what I am looking for. I'm curious as to whether it is possible for such a language to be Turing complete, and if not, how expressive a programming language you can have, while still retaining such a property.\nMy background is rather limited, so I would prefer sources with fewer prerequisites, but references to more advanced sources may be cool too, as that way I'll know what I want to work towards.", "output": "The extent to which this is possible is actually a major open question in the theory of the lambda calculus. Here's a quick summary of what's known:\n\nThe simply-typed lambda calculus with unit, products, and function space does have a simple canonical forms property. Two terms are equal if and only if they have the same beta-normal, eta-long form. Computing these normal forms is also quite straightforward.\nThe addition of sum types greatly complicates matters. The equality problem is still decidable (the keyword to search for is \"coproduct equality\"), but the known algorithms work for extremely tricky reasons and to my knowledge there is no totally satisfying normal form theorem. Here are the four approaches I know of:\n\nNeil Ghani, Beta-Eta Equality for Coproducts, TLCA 1995.\nVincent Balat, Roberto di Cosimo, Marcelo Fiore, Extensional normalisation and type-directed partial evaluation for typed lambda calculus with sums, POPL 2004.\nSam Lindley, Extensional Rewriting with Sums, TLCA 2007.\nArbob Ahmad, Daniel R. Licata, and Robert Harper, A Proof-Theoretic Decision Procedure for the Finitary Lambda-Calculus. WMM 2007.\n\nThe addition of unbounded types, such as natural numbers, makes the problem undecidable. Basically, you can now encode Hilbert's tenth problem.\nThe addition of recursion makes the problem undecidable, because having normal forms makes equality decidable, and that would let you solve the halting problem.", "meta": {"post_id": 10608, "input_score": 31, "output_score": 41, "post_title": "Programming languages with canonical functions"}}
{"input": "It is known that with a countable set of algorithms (characterised by a G\u00f6del number), we cannot compute (build a binary algorithm which checks belonging) all subsets of N.\nA proof could be summarized as: if we could, then the set of all subsets of N would be countable (we could associate to each subset the G\u00f6del number of the algorithm which computes it). As this is false, it proves the result.\nThis is a proof I like as it shows that the problem is equivalent to the subsets of N not being countable.\nNow I'd like to prove that the halting problem is not solvable using only this same result (uncountability of N subsets), because I guess those are very close problem. Is it possible to prove it this way ?", "output": "The halting theorem, Cantor's theorem (the non-isomorphism of a set and its powerset), and Goedel's incompleteness theorem are all instances of the Lawvere fixed point theorem, which says that for any cartesian closed category, if there is an epimorphic map $e : A \\to (A \\Rightarrow B)$ then every $f : B \\to B$ has a fixed point.\n\nLawvere, F. William. Diagonal Arguments and Cartesian Closed Categories. Lecture Notes in Mathematics, 92 (1969), 134-145.\n\nFor a nice introduction to these ideas, see this blog post of Andrej Bauer.", "meta": {"post_id": 10635, "input_score": 30, "output_score": 42, "post_title": "Halting problem, uncomputable sets: common mathematical proof?"}}
{"input": "I wonder how to find the girth of a sparse undirected graph. By sparse I mean $|E|=O(|V|)$. By optimum I mean the lowest time complexity.\nI thought about some modification on Tarjan's algorithm for undirected graphs, but I didn't find good results. Actually I thought that if I could find a 2-connected components in $O(|V|)$, then I can find the girth, by some sort of induction which can be achieved from the first part. I may be on the wrong track, though. Any algorithm asymptotically better than $\\Theta(|V|^2)$ (i.e. $o(|V|^2)$) is welcome.", "output": "Here's what I know about the girth problem in undirected unweighted graphs.\nFirst of all, if the girth is even, you can determine it in $O(n^2)$ time- this is an old result of Itai and Rodeh (A. Itai and M. Rodeh. Finding a minimum circuit in a graph. SIAM J. Computing, 7(4):413\u2013423, 1978.). The idea there is: for each vertex in the graph, start a BFS until the first cycle is closed (then stop and move on to the next vertex); return the shortest cycle found. If the girth is even the shortest cycle found will be the shortest cycle. In particular if your graph is bipartite this will always compute the girth. If the girth $g$ is odd, however, you'll find a cycle of length $g$ or $g+1$, so you may be off by $1$.\nNow, the real problem with odd girth is that inevitably your algorithm would have to be able to detect if the graph has a triangle. The best algorithms for that use matrix multiplication: $O($ min{$n^{2.38}, m^{1.41})$ time for graphs on $n$ nodes and $m$ edges.\nItai and Rodeh also showed that any algorithm that can find a triangle in dense graphs can also compute the girth, so we have an $O(n^{2.38})$ time girth algorithm. However, the runtime for the girth in sparse graphs is not as good as that for finding triangles. The best we know in general is $O(mn)$. In particular, what seems to be the hardest is to find a $o(n^2)$ time algorithm for graphs with $m=O(n)$.\nIf you happen to care about approximation algorithms, Liam Roditty and I have a recent paper in SODA'12 on that: Liam Roditty, V. Vassilevska Williams: Subquadratic time approximation algorithms for the girth. SODA 2012: 833-845.\nThere we show that a $2$-approximation can be found in subquadratic time, and some other results concerning additive approximations and extensions. Generally speaking, because of a theorem of Bondy and Simonovits, when you have densish graphs, say on $n^{1+1/k}$ edges, they already contain short even cycles, say roughly $2k$. So the denser the graph is, the easier it is to find a good approximation to the girth. When the graph is very sparse, the girth can be essentially arbitrarily large.", "meta": {"post_id": 10983, "input_score": 14, "output_score": 28, "post_title": "Optimal algorithm for finding the girth of a sparse graph?"}}
{"input": "Recently I stumbled upon quite an interesting theoretical construct. A so called  G\u00f6del machine\nIt's a general problem solver which is capable of self-optimization. It's suitable for reactive environments.\nAs I understand, it can be implemented as a program for universal Turing machine, although it's requirements go far beyond hardware currently available. I couldn't find many details, though. \nCan such machines be built in practice? Are they at least feasible in our Universe?", "output": "Can such machines be built in practice? \nYes. By \"machine\", Schmidhuber just means \"computer program\".\nAre they at least feasible in our Universe?\nNot in their current form -- the algorithms are too inefficient. \n\nFrom a ten thousand meter perspective, J\u00fcrgen Schmidhuber (and former students, like Marcus Hutter) have been investigating the idea of combining Levin search with Bayesian reasoning to work out algorithms for general problem-solving. \nThe basic idea behind Levin search is that it's possible to use dovetailing and Goedel codes to give a single algorithm which is, up to constant factors, optimal. Loosely, you fix a Godel encoding of programs, and then run a Turing machine that runs the $n$-th program once every $2^{n}$ steps. This means that if the $n$-th program is optimal for some problem, then Levin search will \"only\" be a constant factor of $2^n$ times slower. \nThey have done a fair amount of work on making the constant factors less stupendously, horrifically awful, and are optimistic that this kind of scheme can work in practice. I am (based on my experience in automated theorem proving) very skeptical, since good data structures are critical to theorem proving, and Goedel encodings are terrible data structures. \nBut you don't know it can't work until you try to make it work! After all, we already live in a world where people solve problems by reduction to SAT.", "meta": {"post_id": 11927, "input_score": 18, "output_score": 22, "post_title": "Feasibility of G\u00f6del machines"}}
{"input": "I am looking for sources about formalized notion of programs. This seems to be closely related to Curry-Howard correspondence, but one could also track this back to Universal Turing Machines and its ability to read description and input of any TM.\nWhen reading about Curry-Howard correspondece I feel that the primordiality of UTM-s can harm research on programs with the unique conclusion that any program can be reduced to symbols, states and rules. Does there exist the opposite approach, where high-level computation systems are defined and examined? What are good resources about it?", "output": "What you want exists, and is an enormous area of research: it's the entire theory of programming languages. \nLoosely speaking, you can view computation in two ways. You can think of machines, or you can think of languages. \nA machine is basically some kind of finite control augmented with some (possibly unbounded) memory. This is why introductory TOC classes go from finite automata to pushdown automata to Turing machines --- each class takes a finite control and adds some more memory to it. (Nowadays, the finite control is often  limited even more, as in circuit models.)  The essential thing is that the finite control is given up front, and all at once. \nA language is a way of specifying a whole family of controls, in a compositional way. You have primitive forms for basic controls, and operators for building up larger controls from smaller ones. The primordial language, the lambda-calculus, in fact specifies nothing but control -- the only things you can define are function abstractions, applications, and variable references. \nYou can go back and forth between these two views: the $snm$-theorem is essentially a proof that Turing machines can implement function abstraction and application, and Church encodings demonstrate that the lambda-calculus can encode data. But there's nontrivial content in both of these theorems, and so you should not make the mistake of thinking that the two ways of understanding computation are the same.\nResearchers in complexity and algorithms typically take machines as fundamental, because they are interested in costs and in feasibility results. To exaggerate a bit, the basic research question they have is: \n\nWhat is the least powerful machine that can solve a certain kind of problem? \n\nLanguage researchers take languages as fundamental, because we are interested in expressiveness and impossibility results. With a similar exaggeration, our basic research question is: \n\nWhat is the most expressive language that rules out a certain kind of bad behavior?\n\nAs an aside, note how the two goods each kind of theoretician values are directly in conflict! Good work in algorithms and complexity lets you solve a harder problem, using less resources. Good work in languages lets programmers do more things, while forbidding more bad behaviors. (This conflict is basically why research is hard.)\nNow, you might ask why more Theory A types don't use languages, or why more Theory B researchers don't use machines. The reason arises from the shape of the basic research question. \nNote that the stylized basic research question in algorithms/complexity is a lower bound question -- you want to know that you have the best solution, and that there is no possible way to do better, no matter how clever you are. A language definition fixes the means of program composition, and so if you prove that a lower bound with a language model, then you might be left with the question of whether it might not somehow be possible to do better if you extended your language with some new feature. A machine model gives you the whole control in one go, and so you know everything the machine can possibly do right from the outset. \nBut machine specifications are exactly the wrong thing for saying interesting things about blocking bad behavior. A machine gives you a whole control up-front, but knowing that one particular program is okay or bad doesn't help you when you want to extend it or use it as a subroutine -- as Perlis's epigram states, \"Every program is a part of some other program and rarely fits.\" Since language researchers are interested in saying things about whole classes of programs, languages are much better suited for purpose.", "meta": {"post_id": 12255, "input_score": 8, "output_score": 21, "post_title": "Proofs techniques related to Curry\u2013Howard correspondence"}}
{"input": "The best upper bound known on the time complexity of multiplication is Martin F\u00fcrer's bound $n\\log n2^{O(\\log^* n)}$, which is more than linear time complexity of addition.  Do we have a proof that addition is inherently easier than multiplication?", "output": "No.\nNo unconditional better lower bound than the trivial $\\Omega(n)$ is currently known for integer multiplication. There are some conditional lower bounds though. For more on this, you can have a look at Martin F\u00fcrer's paper Faster Integer Multiplication.\nEdit following Andrej's comment: Addition can be done in time $\\mathcal O(n)$. In comparison, the best known upper bound for multiplication is (approximately) $\\mathcal O(n\\log n)$. On the other hand, no non trivial lower bound is known for multiplication, thus there is no proof that addition is faster than multiplication yet. As (too) often in complexity theory, we just don't know!", "meta": {"post_id": 12671, "input_score": 22, "output_score": 31, "post_title": "Is there a proof that addition is faster than multiplication?"}}
{"input": "Can we compute an $n$-bit threshold gate by polynomial size (unbounded fan-in) circuits of depth $\\frac{\\lg n}{\\lg \\lg n}$? Alternatively, can we count the number of 1s in the input bits using these circuits?\nIs $\\mathsf{TC^0} \\subseteq \\mathsf{AltTime}(O(\\frac{\\lg n}{\\lg  \\lg n}), O(\\lg n))$?\n\nNote that $\\mathsf{TC^0} \\subseteq \\mathsf{NC^1} = \\mathsf{ALogTime} = \\mathsf{AltTime}(O(\\lg n), O(\\lg n))$. So the question is essentially asking if we can save a $\\lg \\lg n$ factor in the depth of circuits when computing threshold gates.\n\nEdit:\nAs Kristoffer wrote in his answer we can save a $\\lg \\lg n$ factor. But can we save a little bit more? Can we replace $O(\\frac{\\lg n}{\\lg \\lg n})$ with $o(\\frac{\\lg n}{\\lg \\lg n})$?\nIt seems to me that the layered brute-force trick doesn't work for saving even $2 \\lg \\lg n$ (more generally any function in $\\lg \\lg n + \\omega(1)$).", "output": "Consider a fanin 2 circuit of $C$ depth $O(\\log n)$. Divide the layers of $C$ into $O(\\log n/\\log\\log n)$ blocks each of $\\log\\log n$ consecutive layers. We now wish to replace each block by a depth 2 circuit. Namely, each gate in the last layer of a block depends on at most $2^{\\log\\log n} = \\log n$ gates of the last layer in the block below. We can thus replace each gate in the last layer by a DNF of polynomial size with inputs being the gates in the last layer of the block below. Doing this for all gates in the last layers for all blocks and connecting these should yield the desired circuit.\nLet me note this is essentially the best one can obtain: the switching lemma allows for lower bounds all the way to depth $\\log n/ \\log\\log n$.", "meta": {"post_id": 12865, "input_score": 19, "output_score": 22, "post_title": "Can we count in depth $\\frac{\\lg n}{\\lg \\lg n}$?"}}
{"input": "I am looking for the method / correct way to approach to reduce the traveling salesman problem to an instance of traveling salesman problem which satisfies the triangle inequality, ie:\n$D(a, b) \\leq D(a, c) + D(c, b)$\nI am not sure how to attack this kind of problem, so any pointers / explanations regarding this would be helpful. Thank you.", "output": "Here is a simple reduction for the TSP problem to the metric TSP problem:\nFor the given TSP instance with $n$ cities, let $D(i,j) \\geq 0$ denote the distance between $i$ and $j$. Now let $M = \\max_{i,j} D(i,j)$. Define the metric TSP instance by the distances $D'(i,j) := D(i,j)+M$. To see that this gives a metric TSP instance, let $i,j,k$ be arbitrary. Then $D'(i,j) + D'(j,k) = D(i,j) + D(j,k) + 2M \\geq 2M \\geq D(i,k) + M = D'(i,k)$. Since any tour uses exactly $n$ edges, the transformation adds exactly $nM$ to any tour, which shows the correctness of the reduction.\nRemark: We can of course also allow for negative distances in the original TSP instance if you prefer by changing the reduction slightly.", "meta": {"post_id": 12885, "input_score": 6, "output_score": 21, "post_title": "Guidelines to reduce general TSP to Triangle TSP"}}
{"input": "Cubic graphs are graphs where every vertex has degree 3. They have been extensively studied and I'm aware that several NP-hard problems remain NP-hard even restricted to subclasses of cubic graphs, but some others get easier. A superclass of cubic graphs is the class of graphs with maximum degree $\\Delta \\leq 3$.\n\nIs there any problem that can be solve in polynomial time for cubic graphs but that is NP-hard for graphs with maximum degree $\\Delta \\leq 3$?", "output": "Here's a reasonably natural one: on an input $(G,k)$, determine whether $G$ has a connected regular subgraph with at least $k$ edges. For 3-regular graphs this is trivial, but if max degree is 3 and the input is connected, not a tree, and not regular, then the largest such subgraph is the longest cycle, so the problem is NP-complete.", "meta": {"post_id": 14457, "input_score": 27, "output_score": 27, "post_title": "Is there a problem that is easy for cubic graphs but hard for graphs with maximum degree 3?"}}
{"input": "Many believe that $\\mathsf{BPP} = \\mathsf{P} \\subseteq \\mathsf{NP}$. However we only know that $\\mathsf{BPP}$ is in the second level of polynomial hierarchy, i.e. $\\mathsf{BPP}\\subseteq \\Sigma^ \\mathsf{P}_2 \\cap \\Pi^ \\mathsf{P}_2$. A step towards showing $\\mathsf{BPP} = \\mathsf{P}$ is to first bring it down to the first level of the polynomial hierarchy, i.e. $\\mathsf{BPP} \\subseteq \\mathsf{NP}$.\nThe containment would mean that nondeterminism is at least as powerful as randomness for polynomial time.\nIt also means that if for a problem we can find the answers using efficient (polynomial time) randomized algorithms then we can verify the answers efficiently (in polynomial time) .\n\nAre there any known interesting consequences for $\\mathsf{BPP} \\subseteq \\mathsf{NP}$?\nAre there any reasons to believe that proving $\\mathsf{BPP} \\subseteq \\mathsf{NP}$ is out of reach right now (e.g. barriers or other arguments)?", "output": "For one, proving $BPP \\subseteq NP$ would easily imply that $NEXP \\neq BPP$, which already means that your proof can't relativize.\nBut let's look at something even weaker: $coRP \\subseteq NTIME[2^{n^{o(1)}}]$. If that is true, then polynomial identity testing for arithmetic circuits is in nondeterministic subexponential time. By Impagliazzo-Kabanets'04, such an algorithm implies circuit lower bounds: either the Permanent does not have poly-size arithmetic circuits, or $NEXP \\not\\subset P/poly$. \nI personally don't know why it looks \"out of reach\" but it does seem hard to prove. Certainly some genuinely new tricks will be needed to prove it.", "meta": {"post_id": 14994, "input_score": 36, "output_score": 39, "post_title": "Consequences of $\\mathsf{NP}$ containing $\\mathsf{BPP}$"}}
{"input": "Does every Turing-recognizable undecidable language have a NP-complete subset?\nThe question could be seen as a stronger version of the fact that every infinite Turing-recognizable language has an infinite decidable subset.", "output": "No.\nTuring-recognizable undecidable languages can be unary (define $x \\not\\in L$  unless $x = 0000\\ldots 0$, so the only difficult strings are composed solely of 0's). Mahaney's theorem says that no unary language can be NP-complete unless P=NP.", "meta": {"post_id": 16182, "input_score": 10, "output_score": 22, "post_title": "Does every Turing-recognizable undecidable language have a NP-complete subset?"}}
{"input": "For the longest time I have thought that a problem was NP-complete if it is both (1) NP-hard and (2) is in NP. \nHowever, in the famous paper \"The ellipsoid method and its consequences in combinatorial optimization\", the authors claim that the fractional chromatic number problem belongs to NP and is NP-hard, yet is not known to be NP-complete. On the third page of the paper, the authors write:\n\n... we note that the vertex-packing problem of a graph is in a sense equivalent to the fractional chromatic number problem, and comment on the phenomenon that this latter problem is an example of a problem in $\\mathsf{NP}$ which is $\\mathsf{NP}$-hard but (as for now) not known to be $\\mathsf{NP}$-complete.\n\nHow is this possible? Am I missing a subtle detail in the definition of NP-complete?", "output": "It seems the issue is the kind of reductions used for each of them, and they are using different ones: they probably mean \"$\\mathsf{NP}$-hard w.r.t. Cook reductions\" and \"$\\mathsf{NP}$-complete w.r.t. Karp reductions\". \nSometimes people use the Cook reduction version of $\\mathsf{NP}$-hardness because it is applicable to more general computational problems (not just decision problems). Although the original definition of both $\\mathsf{NP}$-hardness and $\\mathsf{NP}$-completeness used Cook reductions (polynomial-time Turing reductions) it has become uncommon to use Cook reductions for $\\mathsf{NP}$-completeness (unless it is stated explicitly). I don't recall any recent paper that has used $\\mathsf{NP}$-complete to mean $\\mathsf{NP}$-complete w.r.t. Cook reductions. (As a side note the first problem that to be proven to $\\mathsf{NP}$-hard was TAUT not SAT and the completeness for SAT is implicit in that proof.)\nNow if you look at section 7 of the paper, bottom of page 195, you will see that they mean $\\mathsf{NP}$-hardness w.r.t. Turing reductions.\nSo what they mean here is that the problem is in $\\mathsf{NP}$, is hard for $\\mathsf{NP}$ w.r.t. Cook reductions, but it is unknown to be hard for $\\mathsf{NP}$ w.r.t. Karp reductions (polynomial-time many-one reductions).", "meta": {"post_id": 16983, "input_score": 15, "output_score": 28, "post_title": "How can a problem be in NP, be NP-hard and not NP-complete?"}}
{"input": "In a talk by Razborov, a curious little statement is posted.\n\nIf FACTORING is hard, then Fermat\u2019s little theorem is not provable in $S_{2}^{1}$.\n\nWhat is $S_{2}^{1}$ and why are current proofs not in $S_{2}^{1}$?", "output": "$S^1_2$ is a theory of bounded arithmetic, that is, a weak axiomatic theory obtained by severely restricting the schema of induction of Peano arithmetic. It is one of the theories defined by Sam Buss in his thesis, other general references include Chapter V of H\u00e1jek and Pudl\u00e1k\u2019s Metamathematics of first-order arithmetic, Kraj\u00ed\u010dek\u2019s \u201cBounded arithmetic, propositional logic, and complexity theory\u201d, Buss\u2019s Chapter II of the Handbook of proof theory, and Cook and Nguyen\u2019s Logical foundations of proof complexity.\nYou can think of $S^1_2$ as a theory of arithmetic which has induction only for polynomial-time predicates. In particular, the theory does not prove that exponentiation is a total function, the theory can prove to exist only objects of polynomial size (loosely speaking).\nAll known proofs of the Fermat Little Theorem utilize either exponential-size objects, or they rely on exact counting of sizes of bounded sets (which is probably not definable by a bounded formula, i.e., in the polynomial hierarchy, because of Toda\u2019s theorem).\nThe result on FLT, $S^1_2$, and factoring stems from Kraj\u00ed\u010dek and Pudl\u00e1k\u2019s paper Some consequences of cryptographical conjectures for $S^1_2$ and EF, and in my opinion it is fairly misleading. What Kraj\u00ed\u010dek and Pudl\u00e1k prove is that if factoring (actually, IIRC they state it for RSA instead of factoring, but it\u2019s known that a similar argument works for factoring too) is hard for randomized polynomial time, then $S^1_2$ cannot prove the statement that every number $a$ coprime to a prime number $p$ has finite exponent modulo $p$, that is, there exists $k$ such that $a^k\\equiv1\\pmod p$.\nIt\u2019s true that this is a consequence of FLT, but in fact it is a much, much weaker statement than FLT. In particular, this statement follows from the weak pigeonhole principle, which is known to be provable in a subsystem of bounded arithmetic (albeit a stronger one than $S^1_2$). Thus, Kraj\u00ed\u010dek and Pudl\u00e1k\u2019s argument shows that $S^1_2$ does not prove the weak pigeonhole principle unless factoring is easy, and as such provides a conditional separation of $S^1_2$ from another level of the bounded arithmetical hierarchy, say $T^2_2$.\nIn contrast, the actual FLT does not even seem to be provable in full bounded arithmetic $S_2=T_2$, but this is not related to cryptography. You can find some relevant discussion in my paper Abelian groups and quadratic residues in weak arithmetic.", "meta": {"post_id": 17031, "input_score": 10, "output_score": 21, "post_title": "Proofs in $S_{2}^{1}$"}}
{"input": "In a Cartesian Closed Category (CCC), there exist the so-called exponential objects, written $B^A$.  When a CCC is considered as a model of the simply-typed $\\lambda$-calculus, an exponential object like $B^A$ characterizes the function space from type $A$ to type $B$.  An exponential object is introduced by an arrow called $curry : (A \\times B \\rightarrow C) \\rightarrow (A \\rightarrow C^B)$ and eliminated by an arrow called $apply : C^B \\times B \\rightarrow C$ (which unfortunately called $eval$ in most texts on category theory).  My questions here is: is there any difference between the exponential object $C^B$, and the arrow $B \\rightarrow C$?", "output": "One is internal and the other is external.\nA category $\\mathcal{C}$ consists of objects and morphisms. When we write $f : A \\to B$ we mean that $f$ is a morphism from object $A$ to object $B$. We may collect all morphisms from $A$ to $B$ into a set of morphisms $\\mathrm{Hom}_{\\mathcal{C}}(A,B)$, called the \"hom-set\". This set is not an object of $\\mathcal{C}$, but rather an object of the category of sets.\nIn contrast, an exponential $B^A$ is an object in $\\mathcal{C}$. It is how \"$\\mathcal{C}$ thinks of its hom-sets\". Thus, $B^A$ must be equipped with whatever structure the objects of $\\mathcal{C}$ have.\nAs an example, let us consider the category of topological spaces. Then $f : X \\to Y$ is a continuous map from $X$ to $Y$, and $\\mathrm{Hom}_{\\mathsf{Top}}(X,Y)$ is the set of all such continuous maps. But $Y^X$, if it exists, is a topological space! You can prove that the points of $Y^X$ are (in bijective correspondence with) the continuous maps from $X$ to $Y$. In fact, this holds in general: the morphisms $1 \\to B^A$ (which are \"the global points of $B^A$\") are in bijective correspondence with morphisms $A \\to B$, because\n$$\\mathrm{Hom}(1, B^A) \\cong\n\\mathrm{Hom}(1 \\times A, B) \\cong\n\\mathrm{Hom}(A, B).\n$$\nSometimes we get sloppy about writing $B^A$ as opposed to $A \\to B$. In fact, often these two are synonyms, with the understanding that $f : A \\to B$ might mean \"oh by the way here I meant the other notation, so this means $f$ is a morphism from $A$ to $B$.\" For example, when you wrote down the currying morphism\n$$\\textit{curry}: (A \\times B \\to C) \\to (A \\to C^B)$$\nyou really should have written\n$$\\textit{curry}: C^{A \\times B} \\to {(C^B)}^A.$$\nSo we cannot really blame anyone for getting confused here. The inner $\\to$ is used in the internal sense, and the outer in the external.\nIf we work in simply typed $\\lambda$-calculus then everything is internal, so to speak. We have just a basic typing judgment \"$t$ has type $B$\", written as $t : B$. Because here $B$ is a type, and types correspond to objects, then we clearly have to interpet any exponentials and arrows in $B$ in the internal sense. So then, if we understand\n$$\\textit{curry}: (A \\times B \\to C) \\to (A \\to C^B)$$\nas a typing judgment in the $\\lambda$-calculus, all arrows are internal, so this is the same as\n$$\\textit{curry}: ((C^B)^A)^{C^{A \\times B}}.$$\nI hope by now it is clear why people use $B^A$ and $A \\to B$ as synonyms.", "meta": {"post_id": 17292, "input_score": 22, "output_score": 34, "post_title": "What is the difference between arrows and exponential objects in a cartesian closed category?"}}
{"input": "Given a directed cyclic graph where the weight of each edge may be negative the concept of a \"shortest path\" only makes sense if there are no negative cycles, and in that case you can apply the Bellman-Ford algorithm.\nHowever, I'm interested in finding the shortest-path between two vertices that doesn't involve cycling (ie. under the constraint that you may not visit the same vertex twice). Is this problem well studied? Can a variant of the Bellman-Ford algorithm be employed, and if not is there another solution?\nI'm also interested in the equivalent all-pairs problem, for which I might otherwise apply Floyd\u2013Warshall.", "output": "Paths with no repeated vertices are called simple-paths, so you are looking for the shortest simple-path in a graph with negative-cycles.\nThis can be reduced from the longest-path problem.  If there were a fast solver for your problem, then given a graph with only positive edge-weights, negating all the edge-weights and running your solver would give the longest path in the original graph.\nThus your problem is NP-Hard.", "meta": {"post_id": 17462, "input_score": 15, "output_score": 28, "post_title": "Finding the shortest path in the presence of negative cycles"}}
{"input": "First of all, my understanding on G\u00f6del's incompleteness theorem (and formal logic in general) is very naive, also is my knowledge on theoretical computer science (meaning only one graduate course taken while I'm still an undergraduate), so this question may be very naive.\nAs far as I could find, the provability of P versus NP is an open problem.\nNow:\n\nG\u00f6del's first incompleness theorem states that there may be statements that are true but not provable nor disprovable.\nIf a polynomial solution is found for an NP-complete problem, it proves that P = NP.\n\nSo, suppose that P=NP is not provable:\nThis means that no example of a polynomial solution for an NP-complete problem can be found (otherwise, this would be a proof).\nBut if no example of a polynomial solution for an NP-complete problem can be found, this means that P=NP is false (proving it, meaning the statement is provable), which leads to a contradiction, therefore P=NP should be provable.\nThis sounds like a proof of the provability of P=NP to me, but I think it's extremely likely that it is due to my lack of understanding of the logic topics involved. Could anyone please help me understand what is wrong with this?", "output": "If P=NP, there must be polynomial-time algorithms for NP-complete problems.  However, there might not be any algorithm that provably solves an NP-complete problem and provably runs in polynomial time.", "meta": {"post_id": 18791, "input_score": 11, "output_score": 21, "post_title": "On the provability of P versus NP"}}
{"input": "What exactly is theoretical computer science? Is it learning to code in various language and making apps in platforms? Or is it just thinking about faster and faster algorithms so that you can achieve a task more efficiently by the computers? Or is it programming and thinking of new life situations which can be simulated on a computer? What exactly are we trying to do here ?\nFor example, physics is trying to find all the laws of nature that govern it; mathematics is a tautology based to model reality and used as a very precise language by other subjects.\nWhat exactly is theoretical computer science? When computers were designed by us humans for application purposes, hence it must all drop down to mathematics and physics in the end? Then where is the \"theory\" in computer science itself.\nSorry for being too naive but I want to know what does a theoretical computer scientist do ?", "output": "Your question itself is not naive but the type of answer you ask for is. It is rare for any line of work or intellectual enquiry to have an elevator pitch explanation. Not all would agree with your characterizations of mathematics and physics because they ignore the depth and nuances of those fields. \nTheoretical computer scientists are concerned with studying and applying computation. The computational perspective is a deep and all encompassing one so the study of computation is also deep and has a bearing on many other areas of study. Every single process, whether arising in nature or synthetic, manipulates information. They compute. As in mathematics, there are different languages and types of structures involved in computation, as in physics, there are fundamental laws about computation that we are trying to discover, as in chemistry, fundamental elements of computation can be classified. Theoretical computer science is broad and robust enough to be amenable to any perspective you bring to it. Some of the questions studied are:\n\nWhat is computation and how can it be characterized? (Turing machines, lambda calculi, tiling systems, register machines, DNA computers etc.)\nWhat is the computational model underlying a process? (Biological, chemical, economic, sociological processes, etc.)\nWhat is efficient computation? (complexity via time, space, communication, amortized, smoothed, etc.)\nWhat are characterizations of efficient computation? (Turing machines, algebraic notions, logic, type systems, etc.)\nWhat is the most efficient way to  compute a solution to a problem? (algorithms)\nHow efficient are existing algorithmic processes (analysis of algorithms, statistical phenomena, market equillibria, etc.)\n\nThese are a small and non-representative sample of the questions that one may ask. As in any field, answers to some questions generate new questions and drive enquiry about other questions. You can find a rather dated view of the field by browsing the articles in the Handbook of Theoretical Computer Science. \n\nHandbook of Theoretical Computer Science, Volume A: Algorithms and Complexity, 1990\nHandbook of Theoretical Computer Science, Volume B: Formal Models and Sematics, 1990", "meta": {"post_id": 19636, "input_score": 14, "output_score": 21, "post_title": "What is theoretical computer science?"}}
{"input": "The only definition of \"calculus\" I'm aware of is the study of limits, derivatives, integrals, etc. in analysis. In what sense is lambda calculus (or things like mu calculus) a \"calculus\"? How does it relate to calculus in analysis?", "output": "A calculus is just a system of reasoning. One particular calculus (well, actually two closely related calculi: the differential calculus and the integral calculus) has become so widespread that it is just known as \"calculus\", as if it were the only one. But, as you have observed, there are other calculi, such as the lambda calculus, mu calculus, pi calculus, propositional calculus, predicate calculus, sequent calculus and Professor Calculus.", "meta": {"post_id": 19675, "input_score": 15, "output_score": 23, "post_title": "Why is lambda calculus a \"calculus\"?"}}
{"input": "(This question is a bit of a \"survey\".)\nI'm currently working on a problem where I'm trying to partition the edges of a tournament into two sets, both of which are required to fulfill some structural properties. The problem \"feels\" quite hard, and I fully expect it to be $\\mathcal{NP}$-complete. For some reason I'm having a hard time even finding similar problems in literature.\nAn example of a problem that I would consider comparable to the one I'm dealing with:\n\nGiven a weighted tournament $G = (V,E,w)$, is there a feedback arc set in $G$ the edges of which fulfill the triangle inequality?\n\nNote the difference to the traditional feedback arc set (DFAS) problem: I don't care about the size of the set, but I do care whether the set itself has a certain structural property.\nHave you encountered any decision problems that feel similar to this? Do you remember whether they were $\\mathcal{NP}$-complete or in $\\mathcal{P}$? Any and all help appreciated.", "output": "I think there are lot of similar problems. Here are two in vertex version and one in edge version:\n1) Does a given graph have an independent feedback vertex set? (we don't care about the size of the set). \nThis problem is NP-complete; the proof can be derived from the proof of Theorem 2.1 in \nGarey, Johnson & Stockmeyer.\n2) Does a given graph have a vertex cover that induces a tree? (we don't care about the size of the set). \nThis paper \ngives an NP-completeness proof for this problem (Theorem 2); even for bipartite graphs.\n3) Does a given graph have a dominating edge set the edges of which form an induced $1$-regular subgraph? \n(also known as dominating induced matching or efficient edge dominating; the vertex version is given in the \nsecond answer by Mohammad. \nAgain, we don't care about the size of the set). \nThis problem is NP-complete (well-known, first proved here), even for planar bipartite graphs. \nThe first two problems are particular examples of the problem class called stable-$\\pi$: \nLet $\\pi$ be a graph property. Does a given graph have a vertex cover satisfying $\\pi$? \nMore NP-complete cases as well as polynomially solvable cases can be found in \nthis \nand in this paper \n(and the refs given there) .", "meta": {"post_id": 19845, "input_score": 24, "output_score": 21, "post_title": "NP complete graph problems about structural properties"}}
{"input": "I'm designing a simple statically typed functional programming language as a learning experience.\nIt appears that the type system I have implemented so far could (with a little extra work) incorporate intersection and union types, e.g. you could have:\n\n<Union String Integer>\n<Union Integer Foo>\nThe intersection of the two types above would be a plain Integer\nThe union of the two types would be <Union String Integer Foo>\n\nThe fact that this is possible, of course, doesn't necessary mean it is a good design idea. In particular, I'm a somewhat concerned about the implementation difficulties of keeping the types disjoint and/or handling overlaps.\nWhat are the pros/cons of incorporating such features in the type system?", "output": "Here are a few things to keep in mind:\n\nAlthough we generally think we know what we mean by set-theoretic intersection and union, there have been several different takes on what exactly intersection and union types are.  So, it's worth pinning this down before you embark on an implementation.\nOne element which I think is awfully important for understanding intersections and unions is the concept of type refinement, essentially the idea that a program has a certain intrinsic \"archetype\" (e.g., \"foo is a function from integers to integers\"), which can then be refined to express more precise properties (e.g., \"foo takes even integers to even integers and odd integers to odd integers\"). With the concept of refinement in hand, the key property which distinguishes intersections and unions from products and sums is that the intersection/union of two types can be formed only if they refine the same archetype.  In other words, the type formation rules for intersections and unions may be expressed like so (read \"$S \\sqsubset A$\" as \"$S$ refines $A$\")\n$$\\frac{S\\sqsubset A \\quad T \\sqsubset A}{S\\cap T\\sqsubset A}\\qquad\n\\frac{S\\sqsubset A \\quad T \\sqsubset A}{S\\cup T\\sqsubset A}$$\nwhereas the formation rules for ordinary products and sums are \n$$\\frac{S\\sqsubset A \\quad T \\sqsubset B}{S* T\\sqsubset A*B}\\qquad\n\\frac{S\\sqsubset A \\quad T \\sqsubset B}{S+T\\sqsubset A+B}$$\nSince intersections and unions can be used to make more precise assertions about the run-time behavior of a program, it is natural that typing becomes sensitive to evaluation order. For example, papers (2) and (4) below explained why  the \"obvious\" (and fairly standard) typing and subtyping rules for intersections and unions are actually unsound for ML-like languages (due to the presence of side effects and non-termination). You have been warned!\nFor similar reasons, global type inference generally becomes impractical or undecidable. Indeed, the whole concept of \"principal type\" is arguably a red-herring, since a function may satisfy many different properties which are irrelevant to its intended use (e.g., \"foo takes prime integers to integers greater than 7\"). Instead, practical approaches to intersections and unions (see (3), (4)) are generally based on a combination of inference and checking.\n\nI suppose some of the above points might sound negative, though I wouldn't call them \"cons\" but merely \"realities\" of intersection and union types. On the other hand, from a language design perspective, one reason for making the effort of supporting intersections and unions (and for getting them right!) is that they allow more precise properties of programs to be expressed in a fairly incremental way, requiring a much less drastic transformation than, say, dependent type theory.\nA brief reading list:\n\nDesign of the Programming Language Forsythe by John C. Reynolds\nIntersection Types and Computational Effects by Rowan Davies and Frank Pfenning\nPractical Refinement-Type Checking by Rowan Davies (dissertation)\nTridirectional Typechecking by Joshua Dunfield and Frank Pfenning", "meta": {"post_id": 20536, "input_score": 22, "output_score": 27, "post_title": "What are the practical issues with intersection and union types?"}}
{"input": "Suppose there is a graph $G=(V,E)$. I want to test if $V$ can be partitioned into two disjoint sets $V_1$ and $V_2$ such that the subgraphs induced by $V_1$ and $V_2$ are unit interval graphs.\nI know about the NP-completeness of determining interval numbers but the above problem is different.\nNow, in the literature I found this work by A. Gy\u00e1rf\u00e1s and D. West on multitrack interval graphs but I'm not sure if it is relevant to above problem.\nAny citation to existing literature on the above or similar problem would be helpful. Also please let me know if there is a formal name for the above problem.", "output": "I think, your problem is NP-complete. It is a special case of a theorem \nby Farrugia, \nstating that it is NP-hard to test if the vertex set a graph can be partitioned into \ntwo subsets $V_1,$ and $V_2$ such that $G(V_1)$ belongs to the graph class $\\mathcal{P}$ and \n$G(V_2)$ belongs to the graph class $\\mathcal{Q}$, provided $\\mathcal{P}$ and $\\mathcal{Q}$ \nare closed under taking vertex-disjoint unions and talking induced subgraphs, and at least \none of $\\mathcal{P}$ and $\\mathcal{Q}$ is non-trivial (meaning not all graphs in the class are \nedgeless).", "meta": {"post_id": 20843, "input_score": 13, "output_score": 21, "post_title": "Partition into interval graphs"}}
{"input": "What would be the nasty consequences of NP=PSPACE? I am surprised I did not found anything on this, given that these classes are among the most famous ones.\nIn particular, would it have any consequences on the lower classes?", "output": "One point which has been implicitly but not explicitly mentioned yet is that we would get $\\mathsf{NP} = \\mathsf{coNP}$. Although this is equivalent to $\\mathsf{PH}$ collapsing to $\\mathsf{NP}$, it follows directly from the fact that $\\mathsf{PSPACE}$ is closed under complement, which is trivial to prove. \nI think $\\mathsf{NP} = \\mathsf{coNP}$ is worth pointing out on its own because of the large number of surprising consequences it has: there are short proofs witnessing when a graph is not 3-colorable, *non-*Hamiltonian, when two graphs are *non-*isomorphic, ..., and (in some sense more generally) that there is some Cook-Reckhow proof system in which every propositional tautology has a polynomial-sized proof.", "meta": {"post_id": 21026, "input_score": 31, "output_score": 22, "post_title": "Consequences of NP=PSPACE"}}
{"input": "Kurt G\u00f6del's incompleteness theorems  establish the \"inherent limitations of all but the most trivial axiomatic systems capable of doing arithmetic\". \nHomotopy Type Theory provides an alternative foundation for mathematics, a univalent foundation based on higher inductive types and the univalence axiom. \nThe HoTT book explains that types are higher groupoids, functions are functors, type families are \ufb01brations, etc. \nThe recent article \"Formally Verified Mathematics\" in CACM \nby Jeremy Avigad and John Harrison \ndiscusses HoTT with respect to \nformally verified mathematics and automatic theorem proving. \n\nDo G\u00f6del's incompleteness theorems apply to HoTT?\n\nAnd if they do,\n\nis homotopy type theory impaired by G\u00f6del's incompleteness theorem \n  (within the context of formally verified mathematics)?", "output": "HoTT \"suffers\" from G\u00f6del incompleteness, of course, since it has a computably enumerable language and rules of inference, and we can formalize arithmetic in it. The authors of the HoTT book were perfectly aware of its incompletness. (In fact, this is quite obvious, especially when half of the authors are logicians of some sort).\nBut does incompleteness \"impair\" HoTT? No more than it does any other formal system, and I think the whole issue is a bit misguided. Let me try an analogy. Suppose you have a car which can't take you everywhere on the planet. For instance, it can't climb vertically up a wall. Is the car \"impaired\"? Of course, it can't get you to the top of the Empire State building. Is the car useless? Far from it, it can take you too many other interesting places. Not to mention that the Empire State building has elevators.", "meta": {"post_id": 22130, "input_score": 15, "output_score": 33, "post_title": "Homotopy type theory and G\u00f6del's incompleteness theorems"}}
{"input": "Any language which is not Turing complete can not write an interpreter for it self. I have no clue where I read that but I have seen it used a number of times. It seems like this gives rise to a kind of \"ultimate\" non Turing complete language; the one(s) that can only be interpreted by a Turing machine. These languages would not necessarily be able to compute all total functions from naturals to naturals nor would they necessarily be isomorphic (that is maybe ultimate languages A and B exists such that there exists a function F that A can compute but B cannot). Agda can interpret Godel's system T and Agda is total so that such an ultimate language should be strictly more powerful that Godel's system T it would seem. It seems to me that such a language would be at least as powerful as agda too (though I have no evidence, just a hunch).\nHas any research been done in this vein? What results are known (namely is such an \"ultimate\" language known)?\nBonus: I am worried that there exists a pathological case that can not compute functions that Godel's System T could yet can still only be interpreted by a Turing machine because it allows some really odd functions to be computed. Is this the case or can we know that such a language would be able to compute anything Godel's System T could compute?", "output": "Any language which is not Turing complete can not write an interpreter for it self.\n\nThis statement is incorrect. Consider the programming language in which the semantics of every string is \"Ignore your input and halt immediately\".  This programming language is not Turing complete but every program is an interpreter for the language.", "meta": {"post_id": 24986, "input_score": 20, "output_score": 21, "post_title": "A total language that only a Turing complete language can interpret"}}
{"input": "An important 2003 paper by Childs et al. introduced the \"conjoined trees problem\": a problem admitting an exponential quantum speedup that's unlike just about any other such problem that we know of.  In this problem, we're given an exponentially-large graph like the one pictured below, which consists of two complete binary trees of depth n, whose leaves are connected to each other by a random cycle.  We're supplied with the label of the ENTRANCE vertex.  We're also supplied with an oracle that, given as input the label of any vertex, tells us the labels of its neighbors.  Our goal is to find the EXIT vertex (which can easily be recognized, as the only degree-2 vertex in the graph other than the ENTRANCE vertex).  We can assume that the labels are long random strings, so that with overwhelming probability, the only way to learn the label of any vertex other than the ENTRANCE vertex is to be given it by the oracle.\nChilds et al. showed that a quantum walk algorithm is able simply to barrel through this graph, and find the EXIT vertex after poly(n) steps.  By contrast, they also showed that any classical randomized algorithm requires exp(n) steps to find the EXIT vertex with high probability.  They stated their lower bound as \u03a9(2n/6), but I believe that a closer examination of their proof yields \u03a9(2n/2).  Intuitively, this is because with overwhelming probability, a random walk on the graph (even a self-avoiding walk, etc.) will get stuck in the vast middle region for an exponential amount of time: any time a walker starts heading toward the EXIT, the much larger number of edges pointing away from the EXIT will act as a \"repulsive force\" that pushes it back toward the middle.\nThe way they formalized the argument was to show that, until it's visited ~2n/2 vertices, a randomized algorithm hasn't even found any cycles in the graph: the induced subgraph that it's seen so far is just a tree, providing no information whatsoever about where the EXIT vertex might be.\nI'm interested in pinning down the randomized query complexity of this problem more precisely.  My question is this:\nCan anyone come up with a classical algorithm that finds the EXIT vertex in fewer than ~2n steps---say, in O(2n/2), or O(22n/3)?  Alternatively, can anyone give a lower bound better than \u03a9(2n/2)?\n(Note that, by the birthday paradox, it's not hard to find cycles in the graph after O(2n/2) steps.  The question is whether one can use the cycles to get any clues about where the EXIT vertex is.)\nIf anyone can improve the lower bound past \u03a9(2n/2), then to my knowledge, this would provide the very first provable example of a black-box problem with an exponential quantum speedup, whose randomized query complexity is greater than \u221aN.  (Where N~2n is the problem size.)\nUpdate: I've learned from Andrew Childs that, in this note, Fenner and Zhang explicitly improve the randomized lower bound for conjoined trees to \u03a9(2n/3).  If they were willing to accept constant (rather than exponentially-small) success probability, I believe they could improve the bound further, to \u03a9(2n/2).", "output": "I think I have a deterministic algorithm that finds the exit in $O(n2^{n/2})$ oracle calls.\nFirst, find the labels for all the vertices of distance $n/2$ from the entrance. This takes $O(2^{n/2})$ queries. Then, start from the entrance and walk $n+1$ steps to get to a node $X$ of distance $n+1$ from the entrance. We will try to reach the exit from this node.\nWe have two options of where to go from $X$, and we wish to choose the one closer to the exit. To do this, pick one of the options arbitrarily, arriving at a node $Y$. Then explore all the $O(2^{n/2})$ ways of walking $n/2$ steps from $Y$. If one of them gives a label of distance $n/2$ from the entrance, we know that going from $X$ to $Y$ was the wrong choice. Otherwise, $Y$ was the right choice. Thus in $O(2^{n/2})$ we found a node $X_2$ of distance $n+2$ from the entrance.\nWe can keep proceeding this way. To find a node of distance $n+3$ from the entrance, we start at $X_2$ and walk two arbitrary steps. We then explore all options of walking $n/2$ additional steps (while never walking backwards), and check if any of them have distance $n/2$ from the entrance. This will happen if and only if the first step we walked from $X_2$ was wrong.\nGetting to the exit requires doing this $n$ times, for a total of $O(n2^{n/2})$ oracle calls. In addition, perhaps surprisingly, this algorithm is deterministic.\nEdit: Just to clarify, to get from $X_t$ to $X_{t+1}$, we walk $t$ arbitrary steps, then run a search of depth $n/2$, for a total of $t+2^{n/2}$ steps. If the first step lead away from the exit, then all the first $t$ steps did, and we therefore find a label of distance $n/2$ from the entrance. This means $t+2^{n/2}$ steps suffice for determining the next single step to take from $X_t$.", "meta": {"post_id": 25279, "input_score": 23, "output_score": 22, "post_title": "The randomized query complexity of the conjoined trees problem"}}
{"input": "Given two directed acyclic graphs $G_1$ and $G_2$, is it NP-Complete to find a one-to-one mapping $f:V(G_1) \\rightarrow V(G_2)$ such that $(v_i,v_j) \\in D(G_1)$ if and only if $(f(v_i),f(v_j)) \\in D(G_2)$?\n$D(G)$ is defined as the set of arcs in $G$.", "output": "It's GI-complete which means it's probably not NP-complete.\nReduction from undirected graph isomorphism to DAG isomorphism: given an undirected graph $(V,E)$, make a DAG whose vertices are $V\\cup E$, with an edge from $x$ to $y$ whenever $x\\in V$, $y\\in E$, and $x$ is an endpoint of $y$. (i.e. replace every undirected edge with a node and two ingoing edges)\nReduction from DAG isomorphism to undirected graph isomorphism: replace each arc $x\\rightarrow y$ in the DAG with a four-edge subgraph consisting of a triangle having $x$ as one endpoint, and a dangling edge on the triangle connected to $y$. The reduction is only ambiguous in the case of a component that's a directed cycle, and that's not a DAG.", "meta": {"post_id": 25975, "input_score": 9, "output_score": 27, "post_title": "Is DAG isomorphism NP-C"}}
{"input": "Let's suppose that $P\\ne NP$. Is that possible to solve all the instances of size $n$ of an NP-complete problem in polynomial time using some \"universal magic constant\" $C_n$ that has a polynomial length $P(n)$?\nClearly, if $P\\ne NP$ this constant can be only calculated in exponential time and the calculation must be done for each $n$.\nThanks", "output": "Probably not. What you are asking is whether NP $\\subset$ P/poly. If this were true, then the polynomial hierarchy would collapse (this is the Karp\u2013Lipton theorem), something that is widely believed not to happen.", "meta": {"post_id": 26012, "input_score": 5, "output_score": 34, "post_title": "Magic constant to solve NP-complete problem in polynomial time"}}
{"input": "Looking at the homotopy type theory blog one can easily find a lot of library formalizing most of Homotopy Type Theory in Agda and Coq. \nIs there anyone aware if there is any similar attempt to formalize HoTT in Idris?", "output": "Here is a small, incomplete, and inconsistent formalization of HoTT in Idris. It shows that you can derive a contradiction in Idris just by postulating univalence. There are two barriers to formalizing HoTT in Idris at the moment.\nBarrier 1:\nIdris has heterogeneous equality and heterogeneous equality rewriting. From the HoTT perspective this means we have access to the following rewriting principle, which is inconsistent with univalence:\n$$\n \\prod_{P \\,:\\, X \\to \\mathsf{Type}}\\ \\prod_{x\\,:\\,X}\\ \\prod_{p \\,:\\, x = x}\\ \\prod_{a,\\,b \\,:\\, P\\, x} (\\mathsf{transport}\\ P\\ p\\ a = b) \\to (a = b)  \n$$\nWith this principle, we can easily prove True = False.\nBarrier 2: \nThe pattern matching in Idris is too strong for HoTT, as Neel Krishnaswami suspected in a comment above. We can derive Streicher's K. This leads to uniqueness of identity proofs, and is therefore incompatible with univalence. We can once again show True = False.", "meta": {"post_id": 27979, "input_score": 17, "output_score": 21, "post_title": "Formalizing Homotopy Type theory in Idris"}}
{"input": "I've been trying to wrap my head around the what, why and how of $\\lambda$-calculus but I'm unable to come to grips with \"why does it work\"?\n\"Intuitively\" I get the computability model of Turing Machines (TM). But this $\\lambda$-abstraction just leaves me confounded. \nLet's assume, TMs don't exist - then how can one be \"intuitively\" convinced about $\\lambda$-calculus's ability to capture this notion of computability. How does having a bunch of functions for everything and their composobility imply computability? What am I missing here? I read Alonzo Church's paper on that but I'm still confused and looking for a more \"dummed down\" understanding of the same.", "output": "You're in good company. Kurt G\u00f6del criticized $\\lambda$-calculus (as well as his own theory of general recursive functions) as not being a satisfactory notion of computability on the grounds that it is not intuitive, or that it does not sufficiently explain what is going on. In contrast, he found Turing's analysis of computability and the ensuing notion of machine totally convincing. So, don't worry.\nOn the other hand, to get some idea on how a model of computability works, it's best to write some programs in it. But you do not have to do it in pure $\\lambda$-calculus, although it's fun (in the same sort of way that firewalking is). You can use a modern descendant of $\\lambda$-calculus, such as Haskell.", "meta": {"post_id": 29458, "input_score": 13, "output_score": 22, "post_title": "How exactly does lambda calculus capture the intuitive notion of computability?"}}
{"input": "I've been reading up on Intuitionistic Type Theory (ITT) and it does make sense. But what I'm struggling to understand is \"why\" was it created in the first place?\nIntuitionistic Logic (IL) and Simply-typed $\\lambda$-calculus (STLC) and type theory in general predates the very existence of Martin-L\u00f6f himself! It seems that one can do everything in STLC that is doable in ITT (I may be wrong, but at least it feels that way).  \nSo what was \"novel\" about ITT and how exactly did (or does) it advance the theory of computation? From what I understand, he introduced the notion of \"dependent types\", but it seems they were already there in STLC, in a way. Was his ITT an attempt at abstraction to understand the underlying principles of STLC and IL together? But didn't STLC already held do that? So, just why was ITT created in the first place? What is/was the point?\nHere's an excerpt from Wikipedia: But I still don't get the reason behind its creation that didn't already exist before.\n\nMartin-L\u00f6f's first draft article on type theory dates back to 1971.\n  This impredicative theory generalized Girard's System F. However, this\n  system turned out to be inconsistent due to Girard's paradox which was\n  discovered by Girard when studying System U, an inconsistent extension\n  of System F. This experience led Per Martin-L\u00f6f to develop the\n  philosophical foundations of type theory, his meaning explanation, a\n  form of proof-theoretic semantics, which justifies predicative type\n  theory as presented in his 1984 Bibliopolis book...\n\nIt seems from the excerpt that the reason was to develop the \"philosophical foundations of type theory\" - I thought this foundation already existed (or maybe I assumed it did). Was this the main reason then?", "output": "Very briefly: the simply-typed $\\lambda$-calculus does not have  dependent types. Dependent types were proposed by de Bruijn and Howard who wanted to extend the Curry-Howard correspondence from propositional to first-order logic. The key contribution of Martin-L\u00f6f's is a novel analysis of equality.\nThere are two main ways of giving Curry-Howard style account of equality. \n\nUsing Leibniz's rule of the identity of indiscernibles to encode propositional equality . This approach is used in the calculus of constructions, but it requires impredicative universes which were rejected by Martin-L\u00f6f for philosophical reasons.\nA direct constructive characterisation of equality. Giving such a characterisation using identity types might be the main novelty of Martin-L\u00f6f's intuitionistic type theory.\n\nIdentity types appear deceptively simple today, but they refocussed the understanding of type-theory partly because they gave rise to intriguing semantical questions such as: are identity proofs unique? In some sense this question lead to homotopy type theory and the univalence axiom (which is incompatible with the uniqueness of identities). That the uniqueness of identity proofs is not derivable in Martin-L\u00f6f's intuitionistic type theory was shown \nby Hofmann and Streicher in: \"The groupoid interpretation of type theory\". Incidentally, this result also shows that pattern matching is not a conservative extension of traditional type theory.", "meta": {"post_id": 30651, "input_score": 14, "output_score": 24, "post_title": "Why was there a need for Martin-L\u00f6f to create intuitionistic type theory?"}}
{"input": "After reading the literature on type theory (especially the constructive kind - CTT) I'm left wondering \"why\" should one study type theory, specifically within the confines of \"computing\" in general?\nI understand how type systems (loosely speaking) were created to avoid various paradoxes and the correspondence between philosophy, logic, lambda-calculus and the how it comes together for CTT to serve as a foundation of mathematics. Fair enough.\nNow, functional programming (FP) languages like Haskell, Scala that can be used in large projects are based on an inconsistent logic - making any kind of (automated) formal reasoning about them nearly impossible - but that seems to be the very need/power of TT! E.g., theorem proving and proof assistants and the notion of programs as proofs. But none of this carries over to FP languages.\nSo my question is trying to understand the bigger role i.e., interplay of TT and computing taken together. Most FP languages have just \"good enough\" type systems (e.g., Haskell > Java). The problem of \"type inference\" is in some way similar to \"logical inference\" and doesn't seem all that complicated for simple types. I'm guessing things become undecidable after a particular threshold. Fair enough. I understand its need till this level. But is that it? It seems one can understand type systems/inference without really diving into the details of TT per se. \nSince FP languages don't really borrow much from TT other than \"type systems/inference\", why bother studying the theory in depth especially within computation theory? It seems that after studying a good deal of TT, for fun, I'm still left wondering \"what did I gain\" - both as a theoretician and a software practitioner? What \"is\" the aha understanding that one gains at a deeper level - since very little of TT's power/awesomeness actually gets carried over to commercially viable FP languages (and not Agda, Epigram etc.,)?\n(PS: Here's a similar question on Math.SE- but that's from a mathematical POV and I get that from that perspective. I'm struggling to see TT's importance when concerned about computing and software engineering in general)", "output": "Type theories in which every type is inhabited are far from being useless. True enough, through the eyes of logic they are inconsistent, but there are other things in life apart from logic.\nA general purpose programming language has general recursion. This allows it to populate every type, but we would not conclude from this fact that programming is a useless exercise, would we?\nIn the theory of programming languages types are used as a safety feature (\"A well typed program does not go wrong\" sad a famous man), as an organizational device, and a tool for program analysis. None of these purposes requires that there be an empty type.\nType inference is only one aspect in which type theory relates to programming languages. Some other uses of types in programming languages are:\n\nSpecification: before the programmer starts writing code they writes down the type of the program he is after. They specify (although usually not fully) what they wants. This is also useful for communication between programmers.\n\nModularization: when several pieces of software need to be assembled together we have to make sure they fit. One way of doing this is to speficy the interfaces through which the pieces connect. This can be done with types.\n\n\nDependent types appear in programming languages, but in limited form (because general dependent types cannot be handled automatically by the compiler). For instance, an ML-style module is a dependent sum, while a polymorphic type can be seen as a dependent product.\nYou ask what is gained by studying type theory? Clarity of mind where there was only Visual Basic before. Ability to write 30000 lines of code without making it look like the Flyng Spaghetti Monster. Inner peace and feeling of superiority over the unfortunate users of Lisp.", "meta": {"post_id": 30769, "input_score": 11, "output_score": 21, "post_title": "Why study type theory?"}}
{"input": "Counting triangles in general graphs can be done trivially in $O(n^3)$ time and I think that doing much faster is hard (references welcome). What about planar graphs? The following straightforward procedure shows that it can be done in $O(n\\log{n})$ time. My question is two-fold:\n\nWhat is a reference for this procedure?\nCan the time be made linear?\n\nFrom the algorithmic proof of Lipton-Tarjan's planar separator theorem we can, in time linear in the size of the graph, find a partition of vertices of the graph into three sets $A,B,S$ such that there are no edges with one endpoint in $A$ and the other in $B$, $S$ has size bounded by $O(\\sqrt{n})$ and both $A,B$ have sizes upper bounded by $\\frac{2}{3}$ of the number of vertices. Notice that any triangle in the graph either lies entirely inside $A$ or entirely inside $B$ or uses at least one vertex of $S$ with the other two vertices from $A \\cup S$ or both from $B \\cup S$. Thus it suffices to count the number of triangles in the graph on $S$ and the neighbours of $S$ in $A$ (and similarly for $B$). Notice that $S$ and its $A$-neighbours induce a $k$-outer planar graph (the said graph is a subgraph of a planar graph of diameter $4$). Thus counting the number of triangles in such a graph can be done directly by dynamic programming or by an application of Courcelle's theorem (I know for sure that such a counting version exists in the Logspace world by Elberfeld et al and am guessing that it also exists in the linear time world) since forming an undirected triangle is an $\\mathsf{MSO}_1$ property and since a bounded width tree decomposition is easy to obtain from an embedded $k$-outer planar graph.\nThus we have reduced the problem to a pair of problems which are each a constant fraction smaller at the expense of a linear time procedure. \nNotice that the procedure can be extended to find the count of the number of instances of any fixed connected graph inside an input graph in $O(n\\log{n})$ time.", "output": "The number of occurrences of any fixed subgraph H in a planar graph G can be counted in O(n) time, even if H is disconnected. This, and several related results, are described in the paper Subgraph Isomorphism in Planar Graphs and Related Problems by David Eppstein of 1999; see Theorem 1. The paper indeed uses treewidth techniques.", "meta": {"post_id": 30820, "input_score": 18, "output_score": 22, "post_title": "Time complexity of counting triangles in planar graphs"}}
{"input": "Imagine, we defined natural numbers in dependently typed lambda calculus as Church numerals. They might be defined in the following way:\nSimpleNat = (R : Set) \u2192 R \u2192 (R \u2192 R) \u2192 R\n\nzero : SimpleNat\nzero = \u03bb R z _ \u2192 z\n\nsuc : SimpleNat \u2192 SimpleNat\nsuc sn = \u03bb R z s \u2192 s (sn R z s)\n\nSimpleNatRec : (R : Set) \u2192 R \u2192 (R \u2192 R) \u2192 SimpleNat \u2192 R\nSimpleNatRec R z s sn = sn R z s\n\nHowever, it seems that we can't define Church numerals with the following type of Induction principle:\nNatInd : (C : Nat -> Set) -> (C zero) -> ((n : Nat) -> C n -> C (suc n)) -> (n : Nat) -> (C n)\n\nWhy is it so? How can I prove this? It seems that the problem is with defining a type for Nat which becomes recursive. Is it possible to amend lambda calculus to allow this?", "output": "The question you are asking is interesting and known. You are using the so-called impredicative encoding of the natural numbers. Let me explain a bit of the background.\nGiven a type constructor $T : \\mathsf{Type} \\to \\mathsf{Type}$, we might be interested in the \"minimal\" type $A$ satisfying $A \\cong T(A)$. In terms of category theory $T$ is a functor and $A$ is the initial $T$-algebra. For example, if $T(X) = 1 + X$ then $A$ corresponds to the natural numbers. If $T(X) = 1 + X \\times X$ then $A$ is the type of finite binary trees.\nAn idea with long history is that the initial $T$-algebra is the type\n$$A \\mathrel{{:}{=}} \\prod_{X : \\mathsf{Type}} (T(X) \\to X) \\to X.$$\n(You are using Agda notation for dependent products, but I am using a more traditional mathematical notation.) Why should this be? Well, $A$ essentially encodes the recursion principle for the initial $T$-algebra: given any $T$-algebra $Y$ with a structure morphism $f : T(Y) \\to Y$, we get an algebra homomorphism $\\phi : A \\to Y$ by\n$$\\phi(a) = a \\, Y \\, f.$$\nSo we see that $A$ is weakly initial for sure. For it to be initial we would have to know that $\\phi$ is unique as well. This is not true without further assumptions, but the details are technical and nasty and require reading some background material. For instance, if we can show a satisfactory parametricty theorem then we win, but there are also other methods (such as massaging the definition of $A$ and assuming the $K$-axiom and function extensionality).\nLet us apply the above to $T(X) = 1 + X$:\n$$\\mathsf{Nat} =\n  \\prod_{X : \\mathsf{Type}} ((1 + X) \\to X) \\to X =\n  \\prod_{X : \\mathsf{Type}} (X \\times (X \\to X)) \\to X = \\\\\n  \\prod_{X : \\mathsf{Type}} X \\to (X \\to X) \\to X.\n$$\nWe got Church numerals! And we also understand now that we will get a recursion principle for free, because the Church numerals are the recursion principle for numbers, but we will not get induction without parametricity or a similar device.\nThe tehcnical answer to your question is this: there exist models of type theory in which the type SimpleNat contains exotic elements that do not correspond to numerals, and moreover, these elements break the induction principle. The type SimpleNat in these models is too big and is only a weak initial algebra.", "meta": {"post_id": 30923, "input_score": 21, "output_score": 24, "post_title": "Why it's impossible to declare an induction principle for Church numerals"}}
{"input": "This question was previously posted to Computer Science Stack Exchange here.\n\nImagine you're a very successful travelling salesman with clients all over the country. To speed up shipping, you've developed a fleet of disposable delivery drones, each with an effective range of 50 kilometers. With this innovation, instead of travelling to each city to deliver your goods, you only need to fly your helicopter within 50km and let the drones finish the job.\nProblem: How should your fly your helicopter to minimize travel distance?\nMore precisely, given a real number $R>0$ and $N$ distinct points $\\{p_1, p_2, \\ldots, p_N\\}$ in the Euclidean plane, which path intersecting a closed disk of radius $R$ about each point minimizes total arc length? The path need not be closed and may intersect the disks in any order.\nClearly this problem reduces to TSP as $R \\to 0$, so I don't expect to find an efficient exact algorithm. I would be satisfied to know what this problem is called in the literature and if efficient approximation algorithms are known.", "output": "This is a special case of the Travelling Salesman with Neighborhoods (TSPN) problem. In the general version, the neighborhoods need not all be the same.\nA paper by Dumitrescu and Mitchell, Approximation algorithms for TSP with neighborhoods in the plane, addresses your question. They give a constant factor approximation algorithm for a slightly more general problem (case 1), and a PTAS when the neighborhoods are disjoint balls of the same size (case 2).\nAs a side comment, I think Mitchell has done a lot of work on geometric TSP variants, so you might want to look at his other papers.", "meta": {"post_id": 31511, "input_score": 15, "output_score": 22, "post_title": "What is known about this TSP variant?"}}
{"input": "Is Martin-L\u00f6f type theory basically the predicative Calculus of inductive Constructions without impredicative $\\mathtt{Prop}$?\nIf they're closely related but with more differences than just $\\mathtt{Prop}$, what are those differences?", "output": "The short answer is yes, MLTT can reasonably be equated with CIC without impredicative Prop.\nThe main technical issue is that there are dozens of variants when one talks about Martin-L\u00f6f Type Theory and, perhaps more surprisingly, when one talks about CIC. For example, taking the version of CIC defined in Benjamin Werner's thesis, it doesn't even make sense to remove Prop, as one doesn't have either Set or universes of Type.\nThe main variations one can consider in either of these theories are:\n\nUniverses: how many, and how are they defined (Palmgren, On Universes in Type Theory, discusses many inequivalent variations), and whether or not universe polymorphism is admitted.\nWhich inductive types/families: Agda admits Inductive-Recursive types, but there are many more mundane variations depending on how \"large\" the types in the constructors and eliminators are allowed, handling parameters vs indices, etc.\nInjectivity of type constructors. This leads to a system inconsistent with EM in Agda. Of course Epigram has a more extreme \"Observational Type Theory\", but this can be considered something different altogether.\nAxiom K: this comes for free with certain versions of dependent pattern matching.\nIntentional vs Extensional: this is a huge difference, where essentially a new conversion rule is added in the extensional systems\n$$ \\frac{\\Gamma\\vdash t:\\mathrm{Id}_{\\mathrm{Type}}\\ A\\ B }{\\Gamma\\vdash A\\ =\\ B} $$\nWhich makes type-checking undecidable (but much more powerful!). Martin-L\u00f6f himself seems to have considered both types of systems.\nThe presence of coinductive types and associated elimination principles.\n\nAll of the above variations (except OTT) have been considered in the literature and associated with the name \"Martin-L\u00f6f Type Theory\" or \"Calculus of Inductive Constructions\", mostly because of their association with the Agda and Coq systems, respectively.\nSo the long answer is that there is no consensus about what the exact definition of either of these systems is.", "meta": {"post_id": 32071, "input_score": 15, "output_score": 23, "post_title": "Is MLTT effectively pCiC without Prop?"}}
{"input": "I have two historical questions:\n\nWho first described nondeterministic computation?\n\nI know that Cook described NP-complete problems, and \nthat Edmonds proposed that P algorithms are \"efficient\" or \"good\" algorithms.  \nI searched this Wikipedia article and \nskimmed \"On the Computational Complexity of Algorithms,\" \nbut couldn't find any reference to when nondeterministic computation was first discussed.\n\nWhat was the first reference to the class NP? Was it Cook's 1971 paper?", "output": "I have always seen the notion of nondeterminism in computation attributed to Michael Rabin and Dana Scott. They defined nondeterministic finite automata in their famous paper Finite Automata and Their Decision Problems, 1959. Rabin's Turing Award citation also suggests that Rabin and Scott introduced nondeterministic machines.", "meta": {"post_id": 32403, "input_score": 20, "output_score": 30, "post_title": "Who introduced nondeterministic computation?"}}
{"input": "Is there any natural problem in P for which the best known running time bound is of the form $O(n^\\alpha)$, where $\\alpha$ is an irrational constant?", "output": "While admittedly I haven't done the analysis, and this is not strictly a decision problem, I am willing to wager the best known matrix multiplication algorithms (by Coppersmith, Winograd, Stothers, Williams, et al) have irrational exponent. \nThis can be seen more clearly in the simple case of Strassen's algorithm, which has running time $O(n^{\\log_2 7})$.\nAnd, this is not precisely what you asked, but Ryan Williams has shown that all algorithms that solve SAT in space $n^{o(1)}$ require time $n^{2 \\cos(\\pi/7) - o(1)}$, which is another interesting and unusual appearance of an irrational constant in TCS.", "meta": {"post_id": 32632, "input_score": 19, "output_score": 21, "post_title": "Time complexity with irrational exponent?"}}
{"input": "I'm a fourth year PhD student in theoretical computer science. I'd like to stay in academia, so I'm thinking about how best to advance my career. Obviously the best way to do that is write lots of good papers, but another question is whether I should be trying to have more of those papers be single author.\nSo far I only have one single author paper (out of six). It's neither my best work, nor very recent. Is that a red flag when applying for postdoc or faculty positions? Should I try to have more single author papers?\nThis comes down to my approach to research. I like talking to people. I like telling people what I'm working on and, if they are interested, I'm keen to work with them. In other words, I'm very open to collaboration even when it isn't strictly necessary. Should I change that? That is, should I try to keep projects to myself more so that I end up with some single author papers?\nApologies if this question is off topic. I want to ask this question to people in my area, rather than on a general forum like academia.se. In particular, on all my papers, authors are ordered alphabetically. This makes single author papers a more important signal in TCS than in other areas, where author ordering conveys this information.", "output": "In some fields (like e.g. Economics and Mathematics) single authored papers -are- a good thing to have when you go on the job market. In theoretical computer science, collaboration is much more common, and it is not unusual for even relatively senior researchers to have very few single authored papers. It is not at all suspicious if a student on the market has only papers with co-authors. This is partly because we publish more frequently, so it is ok that each individual paper has less signal about your specific contribution. This is not to say that single authored papers are not impressive, but credit is super-additive across authors, and all things being equal, you should prefer to have a better paper with more coauthors than a worse paper with fewer. This should push you towards collaboration, since collaborators make your research both more fun and generally stronger. \nOf course in hiring decisions, committees will attempt to figure out whether you were a significant contributor to the papers you worked on or not -- but this will be done largely through the letters you obtain. If you have several good papers with the same senior co-author, that coauthor should write a letter for you, since they can explain in detail your contribution. It also helps if you have a clear research agenda of your own. Many papers on -your- topic with a rotating cast of coauthors conveys that you are leading the research direction, in contrast to a series of papers on disparate topics, each in the research agenda of your various coauthors.", "meta": {"post_id": 33074, "input_score": 40, "output_score": 23, "post_title": "Importance of single author papers?"}}
{"input": "The CoC is said to be the culmination of all three dimensions of the Lambda Cube. This isn't apparent to me at all. I think I understand the individual dimensions, and the combination of any two seems to result in a relatively straightforward union (maybe I'm missing something?). But when I look at the CoC, instead of looking like a combination of all three, it looks like a completely different thing. Which dimension do Type, Prop, and small/large types come from? Where did dependent products disappear to? And why is there a focus on propositions and proofs instead of types and programs? Is there something equivalent that does focus on types and programs?\nEdit: In case it isn't clear, I'm asking for an explanation of how the CoC is equivalent to the straightforward union of the Lambda Cube dimensions. And is there an actual union of all three out there somewhere I can study (that is in terms of programs and types, not proofs and propositions)? This is in response to comments on the question, not to any current answers.", "output": "First, to reiterate one of cody's points, the Calculus of Inductive Constructions (which Coq's kernel is\nbased on) is very different from the Calculus of Constructions. It is\nbest thought of as starting at Martin-Lo\u0308f type theory with universes,\nand then adding a sort Prop at the bottom of the type hierarchy.  This \nis a very different beast than the original CoC, which is\nbest thought of as a dependent version of F-omega. (For instance, CiC \nhas set-theoretic models and the CoC doesn't.) \nThat said, the lambda-cube (of which the CoC is a member) is typically presented as a pure type system for reasons of economy in the number of typing rules. By treating sorts, types, and terms as elements of the same syntactic category, you can write down many fewer rules and your proofs get quite a bit less redundant as well. \nHowever, for understanding, it can be helpful to separate out the different categories explicitly. We can introduce three syntactic categories, kinds (ranged over by the metavariable k), types (ranged over by the metavariable A), and terms (ranged over by the metavariable e). Then all eight systems can be understood as variations on what is permitted at each of the three levels.\n\u03bb\u2192 (Simply-typed lambda calculus)\n k ::= \u2217\n A ::= p | A \u2192 B\n e ::= x | \u03bbx:A.e | e e\n\nThis is the basic typed lambda calculus. There is a single kind \u2217, which is the kind of types. The types themselves are atomic types p and function types A \u2192 B. Terms are variables, abstractions or applications.\n\u03bb\u03c9_ (STLC + higher-kinded type operators)\n k ::= \u2217 | k \u2192 k\n A ::= a | p | A \u2192 B | \u03bba:k.A | A B\n e ::= x | \u03bbx:A.e | e e\n\nThe STLC only permits abstraction at the level of terms. If we add it at the level of types, then we add a new kind k \u2192 k which is the type of type-level functions, and abstraction \u03bba:k.A and application A B at the type level as well. So now we don't have polymorphism, but we do have type operators.\nIf memory serves, this system does not have any more computational power than the STLC; it just gives you the ability to abbreviate types.\n\u03bb2 (System F)\n k ::= \u2217\n A ::= a | p | A \u2192 B  | \u2200a:k. A \n e ::= x | \u03bbx:A.e | e e | \u039ba:k. e | e [A]\n\nInstead of adding type operators, we could have added polymorphism. At the type level, we add \u2200a:k. A which is a polymorphic type former, and at the term level, we add abstraction over types \u039ba:k. e and type application e [A].\nThis system is much more powerful than the STLC -- it is as strong as second-order arithmetic.\n\u03bb\u03c9 (System F-omega)\n k ::= \u2217 | k \u2192 k \n A ::= a | p | A \u2192 B  | \u2200a:k. A | \u03bba:k.A | A B\n e ::= x | \u03bbx:A.e | e e | \u039ba:k. e | e [A]\n\nIf we have both type operators and polymorphism, we get F-omega. This system is more or less the kernel type theory of most modern functional languages (like ML and Haskell). It is also vastly more powerful than System F -- it is equivalent in strength to higher order arithmetic.\n\u03bbP (LF)\n k ::= \u2217 | \u03a0x:A. k \n A ::= a | p | \u03a0x:A. B | \u039bx:A.B | A [e]\n e ::= x | \u03bbx:A.e | e e\n\nInstead of polymorphism, we could have gone in the direction of dependency from simply-typed lambda calculus. If you permitted the function type to let its argument be used in the return type (ie, write \u03a0x:A. B(x) instead of A \u2192 B), then you get \u03bbP. To make this really useful, we have to extend the set of kinds with a kind of type operators which take terms as arguments \u03a0x:A. k , and so we have to add a corresponding abstraction \u039bx:A.B and application A [e] at the type level as well. \nThis system is sometimes called LF, or the Edinburgh Logical Framework.\nIt has the same computational strength as the simply-typed lambda calculus.\n\u03bbP2 (no special name)\n k ::= \u2217 | \u03a0x:A. k \n A ::= a | p | \u03a0x:A. B | \u2200a:k.A | \u039bx:A.B | A [e]\n e ::= x | \u03bbx:A.e | e e | \u039ba:k. e | e [A]\n\nWe can also add polymorphism to \u03bbP, to get \u03bbP2. This system is not often used, so it doesn't have a particular name. (The one paper I've read which used it is Herman Geuvers' Induction is Not Derivable in Second Order Dependent Type Theory.)\nThis system has the same strength as System F.\n\u03bbP\u03c9_ (no special name)\n k ::= \u2217 | \u03a0x:A. k | \u03a0a:k. k'\n A ::= a | p | \u03a0x:A. B | \u039bx:A.B | A [e] | \u03bba:k.A | A B \n e ::= x | \u03bbx:A.e | e e \n\nWe could also add type operators to \u03bbP, to get \u03bbP\u03c9_. This involves adding a kind \u03a0a:k. k' for type operators, and corresponding type-level abstraction \u039bx:A.B and application A [e]. \nSince there's again no jump in computational strength over the STLC, this system should also make a fine basis for a logical framework, but no one has done it. \n\u03bbP\u03c9 (the Calculus of Constructions)\n k ::= \u2217 | \u03a0x:A. k | \u03a0a:k. k'\n A ::= a | p | \u03a0x:A. B | \u2200a:k.A | \u039bx:A.B | A [e] | \u03bba:k.A | A B \n e ::= x | \u03bbx:A.e | e e | \u039ba:k. e | e [A]\n\nFinally, we get to \u03bbP\u03c9, the Calculus of Constructions, by taking \u03bbP\u03c9_ and adding a polymorphic type former \u2200a:k.A and term-level abstraction \u039ba:k. e and application e [A] for it. \nThe types of this system are much more expressive than in F-omega, but it has the same computational strength.", "meta": {"post_id": 36054, "input_score": 25, "output_score": 35, "post_title": "How do you get the Calculus of Constructions from the other points in the Lambda Cube?"}}
{"input": "I have a (hopefully simple, maybe dumb) question on Babai's landmark paper showing that $\\mathsf{GI}$ is quasipolynomial.\nBabai showed how to produce a certificate that two graphs $G_i=(V_i,E_i)$ for $i\\in\\{1,2\\}$ are isomorphic, in time quasipolynomial in $v=|V_i|$.\n\nDid Babai actually show how to find an element $\\pi\\in S_v$ that permutes the vertices of $G_1$ to $G_2$, or is the certificate merely an existence-statement?\n\nIf an oracle tells me that $G_1$ and $G_2$ are isomorphic, do I still need to look through all $v!$ permutations of the vertices?\nI ask because I also think about knot equivalence.  As far as I know, it's not known to be, but say detecting the unknot were in $\\mathsf{P}$.  Actually finding a sequence of Reidemeister moves that untie the knot might still take exponential time...", "output": "These problems are polynomially equivalent.\nIndeed, suppose that you have an algorithm that can decide whether two graphs are isomorphic or not, and it claims that they are.\nAttach a clique of size $n+1$ to an arbitrary vertex of each graph.\nTest whether the resulting graphs are isomorphic or not.\nIf they are, then we can conclude that there's an isomorphism that maps the respective vertices to each other, thus we can delete them.\nBy repeating this test $n$ times, we can find (a possible) image for any vertex.\nAfter this, we attach another clique, this time of size $n+2$ to a (different) arbitrary vertex of each original graph, and proceed as before, etc.\nEventually, we'll end up with two graphs that are isomorphic, with cliques of size $n+1,\\ldots n+n$ hanging from their vertices, which makes the isomorphism unique.", "meta": {"post_id": 39844, "input_score": 13, "output_score": 28, "post_title": "Does Babai's quasipolynomial time $\\mathsf{GI}$ algorithm actually generate the isomorphism?"}}
{"input": "Pierce (2002) introduces the typing relation on page 92 by writing:\n\nThe typing relation for arithmetic expressions, written \"t : T\", is defined by\n  a set of inference rules assigning types to terms\n\nand the footnote says The symbol $\\in$ is often used instead of :. My question is simply why type theorists prefer to use : over $\\in$? If a type $T$ is a set of values then it makes perfect sense to write $t \\in T$, no new notation needed. \nIs this similar to how some cs writers prefer $3n^2 = O(n^2)$ even thought it is abuse of notation and should be written $3n^2 \\in O(n^2)$?", "output": "The main reason to prefer the colon notation $t : T$ to the membership relation $t \\in T$ is that the membership relation can be misleading because types are not (just) collections.\n[Supplemental: I should note that historically type theory was written using $\\in$. Martin-L\u00f6f's conception of type was meant to capture sets constructively, and already Russell and Whitehead used $\\epsilon$ for the class memebrship. It would be interesting to track down the moment when $:$ became more prevalent than $\\in$.]\nA type describes a certain kind of construction, i.e., how to make objects with a certain structure, how to use them, and what equations holds about them.\nFor instance a product type $A \\times B$ has introduction rules that explain how to make ordered pairs, and elimination rules explaining that we can project the first and the second components from any element of $A \\times B$. The definition of $A \\times B$ does not start with the words \"the collection of all ...\" and neither does it say anywhere anything like \"all elements of $A \\times B$ are pairs\" (but it follows from the definition that every element of $A \\times B$ is propositionally equal to a pair). In constrast, the set-theoretic definition of $X \\times Y$ is stated as \"the set of all ordered pairs ...\".\nThe notation $t : T$ signifies the fact that $t$ has the structure described by $T$.\nA type $T$ is not to be confused with its extension, which is the collection of all objects of type $T$.  A type is not determined by its extension, just like a group is not determined by its carrier set. Furthermore, it may happen that two types have the same extension, but are different, for instance:\n\nThe type of all even primes larger than two: $\\Sigma (n : \\mathbb{N}) . \\mathtt{isprime}(n) \\times \\mathtt{iseven}(n) \\times (n > 2)$. \nThe type of all odd primes smaller than two: $\\Sigma (n : \\mathbb{N}) . \\mathtt{isprime}(n) \\times \\mathtt{isodd}(n) \\times (n < 2)$.\n\nThe extension of both is empty, but they are not the same type.\nThere are further differences between the type-theoretic $:$ and the set-theoretic $\\in$. An object $a$ in set theory exists independently of what sets it belongs to, and it may belong to several sets. In contrast, most type theories satisfy uniqueness of typing: if $t : T$ and $t : U$ then $T \\equiv U$. Or to put it differently, a type-theoretic construction $t$ has precisely one type $T$, and in fact there is no way to have just an object $t$ without its (uniquely determined) type.\nAnother difference is that in set theory we can deny the fact that $a \\in A$ by writing $\\lnot (a \\in A)$ or $a \\not\\in A$. This is not possible in type theory, because $t : T$ is a judgement which can be derived using the rules of type theory, but there is nothing in type theory that would allow us to state that something has not been derived. When a child makes something from LEGO blocks they proudly run to their parents to show them the construction, but they never run to their parents to show them what they didn't make.", "meta": {"post_id": 43971, "input_score": 21, "output_score": 39, "post_title": "Why colon to denote that a value belongs to a type?"}}
{"input": "It is commonly believed that for all $\\epsilon > 0$, it is possible to multiply two $n \\times n$ matrices in $O(n^{2 + \\epsilon})$ time. Some discussion is here.\nI have asked some people who are more familiar with the research whether they think that there is a $k>0$ independent of $n$ such that there exists an $O(n^2 \\log^k n)$ algorithm for matrix multiplication and they overwhelmingly seemed to have intuition that the answer is \"no\" but could not explain why. That is, they believe that we can do it in $O(n^{2.001})$ time, but not $O(n^2 \\log^{100} n)$ time.\nWhat reasons are there to believe that there is no $O(n^2 \\log^k n)$ algorithm at a fixed $k>0$?", "output": "There's an algorithm for multiplying an $N \\times N^{0.172}$ matrix with an $N^{0.172} \\times N$ matrix in $N^2 \\operatorname{polylog}\\left(N\\right)$ arithmetic operations. The main identity used for it comes from Coppersmith's paper \"Rapid multiplication of rectangular matrices\", but the explanation for why it leads to $N^2 \\operatorname{polylog}\\left(N\\right)$ instead of $N^{2 + \\epsilon}$ is in the appendix of Williams' paper, \"New algorithms and lower bounds for circuits with linear threshold gates\".\nThis only works because Coppersmith's identity has some additional structure you can take advantage of, and the more recent MM algorithms don't seem to have this structure. That said, I'm not sure why one can't hope to extend this approach to $N \\times N \\times N$ matrix multiplication.", "meta": {"post_id": 44509, "input_score": 44, "output_score": 32, "post_title": "Evidence that matrix multiplication is not in $O(n^2\\log^kn)$ time"}}
{"input": "Given: An undirected, unweighted graph\nLooking for: A disjoint vertex cycle cover where every cycle has at least 3 edges\nIs there any algorithm that solves this problem, possibly with some heuristics? Can the bipartite representation of the graph used for finding perfect matching be leveraged here?", "output": "The cycle cover problem (CC) is the problem of finding a spanning set of cycles in a given directed or undirected input graph. \nIf all the cycles in the cover must consist of at least $k$ edges/arcs, the resulting restriction of the problem is denoted $k$-UCC (in undirected graphs) and $k$-DCC (in directed graphs).\nThe complexity of the directed version is fully understood:\n\nMarkus Bl\u00e4ser and Bodo Siebert (\"Computing Cycle Covers without Short Cycles\", in Proceedings of ESA 2001, LNCS 2161, pp 368--379) have proved that $k$-DCC is NP-complete for any fixed $k\\ge3$.\n\nThe complexity landscape of the undirected version is more diverse, and there are some open questions: \n\n$3$-UCC is polynomially solvable. This is a folklore result that follows from a reduction of Tutte (\"A short proof of the factor theorem for finite graphs\", Canadian Journal of Mathematics 6, pp 347\u2013352, 1954) to the classical unrestricted matching problem.\nDavid Hartvigsen (in his PhD thesis \"An Extension of Matching Theory\", Carnegie-Mellon University, 1984) has shown that $4$-UCC is polynomially solvable. \nThe complexity status of $5$-UCC is open. David Hartvigsen has some positive results on special cases of this problem (\"The square-free 2-factor problem in bipartite graphs\", in Proceedings of IPCO 1999, LNCS 1610, pp 234\u2013241). \nPapadimitriou has proved that $k$-UCC is NP-complete for any fixed $k\\ge6$. His proof is sketched in the 1980 paper \"A matching problem with side conditions\" by Gerard Cornuejols and Bill Pulleyblank (Discrete Mathematics 29, pp 135--159).", "meta": {"post_id": 46819, "input_score": 7, "output_score": 22, "post_title": "Algorithm for finding a 3-cycle cover"}}
{"input": "I want to use use Agda to help me write proofs, but I am getting contradictory feedback about the value of proof relevance.\nJacques Carette wrote a Proof-relevant Category Theory in Agda library. But some seem to think (perhaps here, but I was told elsewhere) that proof relevance can be problematic. 1-Category Theory is supposed to be proof irrelevant (and I guess above two categories, this is no longer the case?) I even heard that one may not get the same results if one uses proof relevant category theory.\nAt the same time I believe the Category Theory in the HoTT book and the implementation in Cubical Agda are proof irrelevant (as the HomSets are Sets, i.e., have only one way of being equal).\nWhen should I be happy to have proof relevance? When should I rather choose a proof irrelevant library or proof assistant? What are the advantages of each? Would proof irrelevance be problematic as I move to two categories?", "output": "There are several possible notions of proof relevance. Let us consider three similar situations:\n\nAn element of a sum $\\Sigma (x : A) . P(x)$ is a pair $(a, p)$ where $a : A$ and $p$ is a proof of $P(a)$.\n\nAn element of $\\Sigma (x : A) . \\|P(x)\\|$, where $\\|{-}\\|$ is propositional truncation, is a pair $(a, q)$ where $a : A$ and $q$ is an equivalence class of proofs of $P(a)$ (where any two proofs of $P(a)$ are considered equivalent).\n\nIn set theory, an element of the subset $\\{x \\in A \\mid \\phi(x)\\}$, where $\\phi(x)$ is a logical formula, is just $a \\in A$ such that $\\phi(a)$ holds.\n\n\nThe first situation is proof relevant because we get full access to the proof $p$, and in particular we may analyze $p$.\nThe third situation is proof irrelevant because we get access just to $a \\in A$ but have no further information as to why $\\phi(a)$ holds, just that it does.\nThe second situation looks like proof irrelevance, but is actually a form of restricted proof relevance: we do not delete the proof of $P(a)$ but just control its uses with truncation. That is, from $q$ we may extract a representative proof $p$ of $P(a)$, so long as the choice of $p$ is irrelevant.\nThere is a cruicial difference between the third and the second situation, for having restricted access to $p$ is not at all the same as not having access at all. Here is a concrete example. Given $f : \\mathbb{N} \\to \\mathbb{N}$, define\n$$\nZ(f) = \\Sigma(x : \\mathbb{N}) . \\Pi (y : \\mathbb{N}) . \\mathrm{Id}(f(x + y), 0)\n$$\nAn element of $Z(f)$ is a pair $(m, p)$ witnessing the fact that $f(n)$ is zero for all $n \\geq m$. Given $f$ with this property, we want to define the sum $S(f) = f(0) + f(1) + f(2) + \\cdots$, which of course should be a natural number since eventually the terms are all zero. But proof relevance matters:\n\nWe may define $S : (\\Sigma (f : \\mathbb{N} \\to \\mathbb{N}) . Z(f)) \\to \\mathbb{N}$ by\n$$S(f, (m, p)) = f(0) + \\cdots + f(m)$$\n\nWe may define $S : (\\Sigma (f : \\mathbb{N} \\to \\mathbb{N}) . \\|Z(f)\\|) \\to \\mathbb{N}$ by\n$$S(f, |(m,p)|) = f(0) + \\cdots + f(m),$$\nwhere $|(m,p)|$ is the truncated witness of $Z(f)$. This is a valid definition because using a different representative $(m',p')$ leads to the same value (as we just end up adding fewer or more zeroes).\n\nImagining that in type theory we had proof irrelevant subset types\n$$\\frac{\\vdash a : A \\qquad \\vdash p : P(a)}{\\vdash a : \\{x : A \\mid P(x)\\}}$$\nwe cannot define $S : \\{f : \\mathbb{N} \\to \\mathbb{N} \\mid Z(f)\\} \\to \\mathbb{N}$ because we have no information that would allow us to limit the number of terms $f(0), f(1), f(2), \\ldots$ that need to be added. (There are other things we can do, but that is beside the point here.)\n\n\nAs long as one works in type theory, the only truly proof irrelevant judgements are judgemental equalities. We never define subset types, such as the one above, because that ruins many good properties of type theory (although it would be interesting to investigate this direction).\nIn the old days type theory did not have propositional truncation or any other form of quotienting, and so one was forced to work in a completely proof relevant way all the time. This is unsatisfactory because it fails to capture properly a great deal of mathematical reasoning. People invented setoids to deal with the problem, and later on introduced propositional truncation (and other forms of quotienting).\nYou ask wheter 1-categories are \"proof relevant\". Well, everything in type theory is proof relevant, the only question is how do we deal with having too much proof relevance. Concretely, in a 1-category $\\mathcal{C}$, equality of morphisms $f, g : A \\to B$ should be \"irrelevant\" in the sense that it never matter how $f$ and $g$ are equal, only that they are. In HoTT this is expressed by requiring that $\\mathrm{Id}(f,g)$ have at most one element, which amounts to\n$\\mathrm{Hom}_\\mathcal{C}(A,B)$ being an h-set.\nIn setoid-based formulations of category theory, one needs to account for this phenomenon in some other way, or else one is secretly doing something other than 1-category theory. But I never liked the setoid approach (or Bishop's notion of sets, for that matter), so I will let someone else explain why and how it all makes sense.", "meta": {"post_id": 48112, "input_score": 17, "output_score": 21, "post_title": "Proof relevance vs. proof irrelevance"}}
{"input": "Recently, I came across the problem of figuring out whether a given binary function $f(x)$ is constant (0 for all values of $x$ or 1 for all values of $x$) or balanced (0 for half of values and 1 for the other half), for $x \\leq N$.\nClearly, the complexity of a bounded error probabilistic algorithm is $O(1)$, because the probability of the function being constant if you get k random elements with the same value is less than $\\frac{1}{2^k}$.\nHowever, the deterministic algorithm is to check $f(x)$ for the possible values of $x$ until you find different values or $\\frac{N}{2} + 1$ equal values. The time complexity for the worst case is $O(2^n)$, with $n$ being the number of bits of $N$, which is exponential.\nTo me it seems trivial that there is no better solution than this one, because you cannot be sure that two values inside a black box are different until you find them or get the maximum amount of equal numbers, so this problem is in $BPP$, but not in $P$.\nWhat is wrong in my line of thought? Also, is this a \"clue\" that P is probably different from BPP?", "output": "It is true that if the function $f$ is given by an oracle, then a randomized algorithm is exponentially faster than any deterministic algorithm. With an oracle function, however, this is not a $BPP$ problem! It becomes a $BPP$ problem only if the function $f$ is given by a polynomial time algorithm, so that the whole task can be defined via a polynomial time Turing machine.\nIn that case, however, it is not clear that you indeed have to check exponentially many values in the deterministic case. You might be able to bypass it via capitalizing on knowing the special algorithm that computes $f$. (Note that  knowing the algorithm is essential to make it a $BPP$ problem.)\nOnce you are in this setting, it might be possible  to use the knowledge of the algorithm   to devise a deterministic polynomial time solution. For example, by an appropriate pseudo random number generator you might be able to  simulate the randomness well enough for the specific function $f$, so that the same solution is obtained as by the randomized algorithm. Whether this is indeed possible in every case is a major open question, it is the $P=?BPP$ problem.\nYour line of thought can used, however, to prove that there is no universal deterministic pseudo random number generator, which works for every function $f$  in this setting.", "meta": {"post_id": 52885, "input_score": 10, "output_score": 22, "post_title": "Why is the \"balanced vs constant function\" problem not a proof that P \u2260 BPP?"}}
