{"input": "I know that the harmonic series $1 + \\frac12 + \\frac13 + \\frac14 + \\cdots$ diverges. I also know that the sum of the inverse of prime numbers $\\frac12 + \\frac13 + \\frac15 + \\frac17 + \\frac1{11} + \\cdots$ diverges too, even if really slowly since it's $O(\\log \\log n)$.\nBut I think I read that if we consider the numbers whose decimal representation does not have a certain digit (say, 7) and sum the inverse of these numbers, the sum is finite (usually between 19 and 20, it depends from the missing digit). Does anybody know the result, and some way to prove that the sum is finite?", "output": "It is not very surprising that the sum is finite, since numbers without a 7 (or any other digit) get rarer and rarer as the number of digits increases.\nHere's a proof.\nLet $S$ be the harmonic series with all terms whose denominator contains the digit $k$ removed. We can write $S =S_1 + S_2 + S_3 + \\ldots$, where $S_i$ is the sum of all terms whose denominator contains exactly $i$ digits, all different from $k$.\nNow, the number of $i$-digit numbers that do not contain the digit $k$ is $8\\cdot9^{i-1}$ (there are $8$ choices for the first digit, excluding $0$ and $k$, and $9$ choices for the other digits). [Well, if $k=0$ there are $9$ choices for the first digit, but the proof still works.] So there are $8\\cdot9^{i-1}$ numbers in the sum $S_i$.\nNow each number in $S_i$ is of the form $\\frac1a$, where $a$ is an $i$-digit number. So $a \\geq 10^{i-1}$, which implies $\\frac1a \\leq \\frac1{10^{i-1}}$.\nTherefore $S_i \\leq 8\\cdot\\dfrac{9^{i-1} }{10^{i-1}} = 8\\cdot\\left(\\frac9{10}\\right)^{i-1}$.\nSo $S= \\sum S_i \\leq \\sum 8\\cdot\\left(\\frac9{10}\\right)^{i-1}$\nwhich is a  geometric series of ratio $\\frac9{10} < 1$, which converges. Since $S$ is a positive series bounded above by a converging series, $S$ converges.", "meta": {"post_id": 387, "input_score": 46, "output_score": 63, "post_title": "Sum of reciprocals of numbers with certain terms omitted"}}
{"input": "I know a little bit of the theory of compact Riemann surfaces, wherein there is a very nice divisor -- line bundle correspondence.\nBut when I take up the book of Hartshorne, the notion of Cartier divisor there is very confusing. It is certainly not a direct sum of points; perhaps it is better to understand it in terms of line bundles. But Cartier divisors do not seem to be quite the same thing as line bundles. The definition is hard to figure out. Can someone clear the misunderstanding for me and explain to me how best to understand Cartier divisors?", "output": "When discussing divisors, a helpful distinction to make at the beginning is effective divisors vs. all divisors.  Normally effective divisors have a more geometric description; all divisors can then be obtained from the effective ones by allowing some minus signs to come into the picture.\nAn irreducible effective Weil divisor on a variety $X$ is the same thing as an irreducible codimension one subvariety, which in turn is the same thing as a height one point $\\eta$ of $X$.  (We get $\\eta$ as the generic point of the irred. codim'n one subvariety, and we recover the subvariety as the closure of $\\eta$.)\n An effective Weil divisor is a non-negative integral linear combination of irreducible ones, so you can think of it as a non-negative integral linear combination of height one points $\\eta$.\nTypically, one restricts to normal varieties, so that all the local rings at height one points  are DVRs.    Then, given any pure codimension one subscheme $Z$ of $X$, you can attach a Weil\ndivisor to $Z$, in the following way:\nbecause the local rings at height one points are DVRs, if $Z$ is any codimension one subscheme of $X$, cut out by an ideal sheaf $\\mathcal I_Z$,  and $\\eta$ is a height one point, then the stalk $\\mathcal I_{Z,\\eta}$ is\nan ideal in the DVR $\\mathcal O_{X,\\eta}$, thus is just some power of the maximal ideal\n$\\mathfrak m_{\\eta}$ (using the DVR property), say $\\mathcal I_{Z,\\eta} = \\mathfrak m_{\\eta}^{m_{Z,\\eta}},$ and so the multiplicity $m$ of $Z$ at $\\eta$ is well-defined.\nThus the effective Weil divisor $$div(Z) := \\sum_{\\eta \\text{ of height one}} m_{Z,\\eta}\\cdot  \\eta$$\nis well-defined.\nNote that this recipe only goes one way: starting with the Weil divisor, we can't recover $Z$, because the Weil divisor does not remember all the scheme structure (i.e. the whole\nstructure sheaf, or equivalently, the whole ideal sheaf) of $Z$, but only its behaviour at its generic points (which amounts to the same thing as remembering the irreducible components and their multiplicities). \nAn effective Cartier divisor is actually a more directly geometric object, namely, \nit is a locally principal pure codimension one subscheme, that is,\na subscheme, each component of which is codimension one, and which, locally around each point, is the zero locus of a section of the structure sheaf.   Now in order to cut out\na pure codimension one subscheme as its zero locus, a section of the structure\nsheaf should be regular (in the commutative algebra sense), i.e. a non-zero divisor.\nAlso, two regular sections will cut out the same zero locus if their ratio is a unit\nin the structure sheaf.  So if we let $\\mathcal O_X^{reg}$ denote the subsheaf of\n$\\mathcal O_X$ whose sections are regular elements (i.e. non-zero divisors in each stalk),\nthen the equation of a Cartier divisor is a well-defined global section of the quotient\nsheaf\n$\\mathcal O_X^{reg}/\\mathcal O_X^{\\times}$.\nNow suppose that we are on a smooth variety.  Then any irreducible codimension one subvariety\nis in fact locally principal, and so given a Weil divisor \n$$D = \\sum_{\\eta \\text{ of height one}} m_{\\eta} \\cdot\\eta,$$\nwe can actually canonically attach a Cartier divisor to it, in the following way:\nin a n.h. of some point $x$, let $f_{\\eta}$ be a local equation for the Zariski closure\nof $\\eta$; then if $Z(D)$ is cut out locally by $\\prod_{\\eta} f_{\\eta}^{m_{\\eta}} = 0,$\nthen $Z(D)$ is locally principal by construction, and, again by construction,\n$div(Z(D)) = D.$\nSo in the smooth setting,\nwe see that $Z \\mapsto div(Z)$ and $D \\mapsto Z(D)$ establish a bijection between\neffective Cartier divisors and effective Weil divisors.\nOn the other hand, on a singular variety, it can happen that an irreducible codimension one subvariety need not be locally principal in the neighbourhood of a singular point (e.g. a generating line on the cone $x^2 +y^2 + z^2 = 0$\nin $\\mathbb A^3$ is not locally\nprincipal in any neighbourhood of the cone point).  Thus there can be Weil divisors that\nare not of the form $div(Z)$ for any Cartier divisor $Z$.\nTo go from effective Weil divisor to all Weil divisors, you just allow negative coefficients.\nTo go from effective Cartier divisors to all Cartier divisors, you have to allow yourself \nto invert the functions $f$ that cut out the effective Cartier divisors, or equivalently, to go from the sheaf of monoids $\\mathcal O_X^{reg}/\\mathcal O_X^{\\times}$ to the associated sheaf of groups, which is $\\mathcal K_X^{reg}/\\mathcal O_X^{\\times}.$\n(Here, it helps to remember that $\\mathcal K_X$ is obtained from $\\mathcal O_X$\nby inverting non-zero divisors.)\nFinally, for the connection with line bundles: if $\\mathcal L$ is a line bundle,\nand $s$ is a regular section (i.e. a section whose zero locus is pure codimension one,\nor equivalently, a section which, when we choose a local isomorphism $\\mathcal L_{| U}\n\\cong \\mathcal O_U$, is not a zero divisor), then the zero locus $Z(s)$ of $s$\nis an effective Cartier divisor, essentially by definition.\nSo we have a map $(\\mathcal L,s) \\mapsto Z(s)$ which sends line bundles with regular sections to effective Cartier divisors.  This is in fact an isomorphism of monoids\n(where on the left we consider pairs $(\\mathcal L,s)$ up to isomorphism of pairs):\ngiven an effective Cartier divisor $D$, we can define $\\mathcal O(D)$ to be the\nsubsheaf of $\\mathcal K_X$ consisting (locally) of sections $f$ such that the locus of\npoles of $f$ (a well-defined Cartier divisor) is contained (as a subscheme)in the Cartier\ndivisor $D$ (perhaps less intuitively, but more concretely: if $D$ is locally cut out\nby the equation $g = 0$, then $\\mathcal O(D)$ consists (locally) of sections $f$\nof $\\mathcal K_X$ such that $fg$ is in fact a section of $\\mathcal O_X$).\nThe constant function $1$ certainly lies in $\\mathcal O(D)$, and (thought of as a section\nof $\\mathcal O(D)$ -- not as a function!) its zero locus is exactly $D$.\nThus $D \\mapsto (\\mathcal O(D), 1)$ is an inverse to the above map $(\\mathcal L,s) \\mapsto\nZ(s)$.\nFinally, if we choose two different regular sections of the same line bundle,\nthe corresponding Cartier divisors are linearly equivalent.  Thus we are led to the\nisomorphism \"line bundles up to isomorphism = Cartier divisors up to linear equivalence\".\nBut, just to emphasize, to understand this it is best to restrict first to line bundles\nwhich admit a regular section, and then think of the corresponding Cartier divisor as being\nthe zero locus of that section.   This brings out the geometric nature of the Cartier divisor quite clearly.", "meta": {"post_id": 1926, "input_score": 73, "output_score": 110, "post_title": "Divisor -- line bundle correspondence in algebraic geometry"}}
{"input": "Why don't $3$-cycles generate the symmetric group? was asked earlier today. The proof is essentially that $3$-cycles are even permutations, and products of even permutations are even.\nSo: do the $3$-cycles generate the alternating group? Similarly, do the $k$-cycles generate the alternating group when $k$ is odd?\nAnd do the $k$-cycles generate the symmetric group when $k$ is even? I know that transpositions ($2$-cycles) generate the symmetric group.", "output": "If $n\\geq5$, then the only normal subgroups of the symmetric group $S_n$ are the trivial group, the alternating group and the symmetric group itself. Since the $k$-cycles form a full conjugacy class, it follows that the subgroup they generate is normal. This determines everything if $n \\geq 5$.\nMore specifically: the $k$-cycles in $S_n$ generate the alternating group if $k$ is odd and $k \\ne 1$; they generate the full symmetric group if $k$ is even.", "meta": {"post_id": 3667, "input_score": 20, "output_score": 36, "post_title": "What do all the $k$-cycles in $S_n$ generate?"}}
{"input": "If memory serves, ten years ago to the week (or so), I taught first semester freshman calculus for the first time.  As many calculus instructors do, I decided I should ask some extra credit questions to get students to think more deeply about the material.  The first one I asked was this:\n1) Recall that a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is said to have a removable discontinuity at a point $x_0 \\in \\mathbb{R}$ if $\\lim_{x \\rightarrow x_0} f(x)$ exists but not does not equal $f(x_0)$.  Does there exist a function $f$ which has a removable discontinuity at $x_0$ for every $x_0 \\in \\mathbb{R}$?\nCommentary: if so, we could define a new function $\\tilde{f}(x_0) = \\lim_{x \\rightarrow x_0} f(x)$ and it seems at least that $\\tilde{f}$ has a fighting chance to be continuous on $\\mathbb{R}$.  Thus we have successfully \"removed the discontinuities\" of $f$, but in so doing we have changed the value at every point!  \nRemark: Lest you think this is too silly to even seriously contemplate, consider the function $f: \\mathbb{Q} \\rightarrow \\mathbb{Q}$ given by $f(0) = 1$ and for a nonzero \nrational number $\\frac{p}{q}$, $f(\\frac{p}{q}) = \\frac{1}{q}$.  It is easy to see that this function has limit $0$ at every (rational) point!\nSo I mentioned this problem to my students.  A week later, the only person who asked me about it at all was my Teaching Assistant, who was an older undergraduate, not even a math major, I think.  (I hasten to add that this was not in any sense an honors calculus class, i.e., I was pretty clueless back then.)  Thinking about it a bit, I asked him if he knew about uncountable sets, and he said that he didn't.  At that point I realized that I didn't have a solution in mind that he would understand (so still less so for the freshman calculus students) and I advised him to forget all about it.\n\nSo my actual question is: can you solve this problem using only the concepts in a non-honors freshman calculus textbook?  (In particular, without using notions of un/countability?)\n[Addendum: Let me say explicitly that I would welcome an answer that proceeds directly in terms of the least upper bound axiom.  Most freshman calculus books do include this, albeit somewhere hidden from view of the casual readers, i.e., actual freshman calculus students.]\n\nIf you can't figure out how to answer the question at all, I think the following related question helps.\n2) Define a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ to be precontinuous if the limit exists at every point.  For such a function, we can define $\\tilde{f}$ as above.  Prove/disprove that, as suggested above, $\\tilde{f}$ is indeed continuous.  [Then think about $f - \\tilde{f}$.]\nNow that I think about it, there is an entire little area here that I don't know anything about, e.g.\n3) The set of discontinuities of an arbitrary function is known -- any $F_{\\sigma}$ set inside $\\mathbb{R}$ can serve.  What can we say about the set of discontinuities of a \"precontinuous function\"?  [Edit: from the link provided in Chandru1's answer, we see that it is countable.  What else can we say?  Note that taking the above example and extending by $0$ to the irrationals, we see that the set of points of discontinuity of a precontinuous function can be dense.]", "output": "I think the following works:\nHere is a sketch, I will fill in the details later if required.\nLet $g(x) = \\lim_{t\\rightarrow x} f(t)$. Then we can show that $g(x)$ is continuous.\nLet $h(x) = f(x) - g(x)$. Then $\\lim_{t \\rightarrow x} h(t)$ exists and is $0$ everywhere.\nWe will now show that $h(c) = 0$ for some $c$. \nThis will imply that $f(x)$ is continuous at $c$ as then we will have $f(c) = g(c) = \\lim_{t->c} f(t)$.\nConsider any point $x_0$.\nBy limit of $h$ at $x_0$ being $0$, there is a closed interval $I_0$ (of length > 0) such that $|h(x)| < 1$ for all $x \\in I_0$.\nThis is because, given an $\\epsilon > 0$ there is a $\\delta > 0$ such that $|h(x)| < \\epsilon$ for all $x$ such that $0 < |x - x_{0}| < \\delta$. Pick $\\epsilon = 1$ and pick $I_{0}$ to be any closed interval of non-zero length in $(x_{0}, x_{0} + \\delta)$.\nNow pick any point $x_1$ in $I_0$.\nBy limit of $h$ at $x_1$ being $0$, there is a closed interval $I_1 \\subset I_0$ (of length > 0) such that $|h(x)| < 1/2$ for all $x \\in I_1$, by argument similar to above.\nContinuing this way, we get a sequence of closed intervals $I_n$ such that\n$|h(x)| < \\frac{1}{n+1}$ for all $x \\in I_n$. We also have that $I_{n+1} \\subset I_n$ for each $n$, and that length $I_n$ > 0. We could also arrange so that length $I_n \\rightarrow 0$.\nNow there is a point $c$ (by completeness of $\\mathbb{R}$) such that $c \\in \\bigcap_{n=0}^{\\infty}I_{n}$.\nThus we have that $|h(c)| < \\frac{1}{n+1}$ for all $n$ and so $h(c) = 0$ and $f(c) = g(c)$.", "meta": {"post_id": 3777, "input_score": 83, "output_score": 41, "post_title": "Is there a function with a removable discontinuity at every point?"}}
{"input": "Given $f(x) = \\frac{1}{2}x^TAx + b^Tx + \\alpha $ \nwhere A is an nxn symmetric matrix, b is an n-dimensional vector, and alpha a scalar. Show that\n$\\bigtriangledown _{x}f(x) = Ax + b$ \nand\n$H = \\bigtriangledown ^{2}_{x}f(x) = A$ \nIs this simply a matter of taking a derivative with respect to X, how would you attack this one?", "output": "$\\nabla f = (\\partial f/\\partial x_1, \\ldots, \\partial f/\\partial x_n)^t$ denotes the vector of partial derivatives of $f$ and is a completely standard notation.\nOn the other hand, $\\nabla^2 f$ seems to be used here in an unusual way, namely to denote the Hessian (the matrix of all second order partial derivatives), $(\\partial^2 f/\\partial x_i \\partial x_j)_{i,j=1}^n$.\n(The usual meaning of $\\nabla^2 f$ is the Laplacian, $\\partial^2 f/\\partial x_1^2 + \\ldots + \\partial^2 f/\\partial x_n^2$.)", "meta": {"post_id": 5076, "input_score": 39, "output_score": 42, "post_title": "what does \u2207 (upside down triangle) symbol mean in this problem"}}
{"input": "What are the benefits of using a conjugate linear inner product in a complex vector space vs a simple linear inner product?  That is, why do we demand that $(y,x) = \\overline{(x,y)}$ as opposed to $(y,x)=(x,y)$?  Of course, this ensures that $(x,x)$ is real and thus makes an easy definition of norm, but is that necessary?", "output": "It is in fact necessary. The inner product axioms without the conjugation are inconsistent:\n(Here $u$, $v$, $w$ are vectors and $c$ is a scalar)\n\n$\\langle cu, v\\rangle = c\\langle u, v\\rangle$\n$\\langle u,v\\rangle = \\langle v,u\\rangle$\nIf $u \\neq 0$, then $\\langle u,u\\rangle$ is a positive real number\n$\\langle u+v,w\\rangle = \\langle u,w\\rangle + \\langle v,w\\rangle$\n\nIn fact, 1-3 alone are inconsistent. Indeed, let $u$ be any nonzero vector, so $\\langle u,u\\rangle > 0$ by condition 3. But if $i = \\sqrt{-1}$, then $\\langle iu, iu\\rangle = i\\langle u, iu\\rangle$ (by 1) $ = i\\langle iu, u\\rangle$ (by 2) $ = i^2\\langle u, u\\rangle$ (by 1) $ = -\\langle u, u\\rangle < 0$, contradicting condition 3. \nThe upshot is that you can choose: either conjugate one side of condition 2, giving you the axioms for an inner product, or get rid of condition 3, giving you the axioms for a symmetric bilinear form. You could also consider a weaker version of 3, like requiring that if $u\\neq 0$, then $\\langle u, v\\rangle \\neq 0$ for some $v$. That gives you nondegenerate symmetric bilinear forms.\nNote that there's nothing wrong with bilinear forms on complex vector spaces; they're just not inner products. They're disjoint concepts, unlike in real vector spaces, where inner products are just special symmetric bilinear forms. In some ways, bilinear forms are nicer than inner products, since you don't have to worry about complex conjugation. However, bilinear forms over the complex numbers do not give rise to norms, which means they don't endow vector spaces with good geometry. Inner products do, hence their ubiquity.", "meta": {"post_id": 5179, "input_score": 26, "output_score": 38, "post_title": "Is complex conjugation needed for valid inner product?"}}
{"input": "So, from what I understand, the axiom of choice is equivalent to the claim that every set can be well ordered. A set is well ordered by a relation, $R$ , if every subset has a least element. My question is: Has anyone constructed a well ordering on the reals?\nFirst, I was going to ask this question about the rationals, but then I realised that if you pick your favourite bijection between rationals and the integers, this determines a well ordering on the rationals through the natural well order on $\\mathbb{Z}$ . So it's not the denseness of the reals that makes it hard to well order them. So is it just the size of $\\mathbb{R}$ that makes it difficult to find a well order for it? Why should that be?\nTo reiterate:\n\nIs there a known well order on the Reals?\nIf there is, does a similar construction work for larger cardinalities?\nIs there a largest cardinality for which the construction works?", "output": "I assume you know the general theorem that, using the axiom of choice, every set can be well ordered. Given that, I think you're asking how hard it is to actually define the well ordering. This is a natural question but it turns out that the answer may be unsatisfying.\nFirst, of course, without the axiom of choice it's consistent with ZF set theory that there is no well ordering of the reals. So you can't just write down a formula of set theory akin to the quadratic formula that will \"obviously\" define a well ordering. Any formula that does define a well-ordering of the reals is going to require a nontrivial proof to verify that it's correct.\nHowever, there is not even a formula that unequivocally defines a well ordering of the reals in ZFC.\n\nThe theorem of \"Borel determinacy\" implies that there is no well ordering of the reals whose graph is a Borel set. This is provable in ZFC.  The stronger hypothesis of \"projective determinacy\" implies there is no well ordering of the reals definable by a formula in the projective hierarchy. This is consistent with ZFC but not provable in ZFC. \nWorse, it's even consistent with ZFC that no formula in the language of set theory defines a well ordering of the reals (even though one exists). That is, there is a model of ZFC in which no formula defines a well ordering of the reals. \n\nA set theorist could tell you more about these results.  They are in the set theoretic literature but not in the undergraduate literature. \nHere is a positive result. If you work in $L$ (that is, you assume the axiom of constructibility) then a specific formula is known that defines a well ordering of the reals in that context.  However, the axiom of constructibility is not provable in ZFC (although it is consistent with ZFC), and the formula in question does not define a well ordering of the reals in arbitrary models of ZFC. \nA second positive result, for relative definability. By looking at the standard proof of the well ordering principle (Zermelo's proof), we see that there is a single formula $\\phi(x,y,z)$ in the language of set theory such that if we have any choice function $F$ on the powerset of the reals then the formula $\\psi(x,y) = \\phi(x,y,F)$ defines a well ordering of the reals, in any model of ZF that happens to have such a choice function. Informally, this says that the reason the usual proof can't explicitly construct a well ordering is because we can't explicitly construct the choice function that the proof takes as an input.", "meta": {"post_id": 6501, "input_score": 142, "output_score": 134, "post_title": "Is there a known well ordering of the reals?"}}
{"input": "I know it is possible for a group $G$ to have normal subgroups $H, K$, such that $H\\cong K$ but $G/H\\not\\cong G/K$, but I couldn't think of any examples with $G$ finite. What is an illustrative example?", "output": "Take $G = \\mathbb{Z}_4 \\times \\mathbb{Z}_2$, $H$ generated by $(0,1)$, $K$ generated by $(2,0)$.  Then $H \\cong K \\cong \\mathbb{Z}_2$ but $G/H \\cong \\mathbb{Z}_4$ while $G/K \\cong \\mathbb{Z}_2 \\times \\mathbb{Z}_2$.", "meta": {"post_id": 7720, "input_score": 30, "output_score": 39, "post_title": "Finite group with isomorphic normal subgroups and non-isomorphic quotients?"}}
{"input": "Ok. This question may sound very easy, but actually I am in great need of an answer. I have been facing trouble in constructing functions, which are only continuous at some particular sets. \nFor example, the standard example of a function which is only continuous at one point, is the function,  $f(x) = x, \\ x \\in \\mathbb{Q}$ and $f(x) = -x, x  \\in \\mathbb{R} \\setminus \\mathbb{Q}$. Similarly, I would like to know how to construct a function which is:\n\nContinuous at exactly $2,3,4$ points.\nContinuous exactly at integers\nContinuous exactly at Natural numbers\nContinuous exactly at Rationals. \n\nI would like to see many examples (with proof!), so that I won't struggle when somebody asks me to construct such functions.", "output": "One simple way of constructing a function which is continuous only at a finite number of points, $x=a_1,\\ldots,a_n$, is to do a slight modification to the function you give: take a polynomial $p(x)$ that has roots exactly at $x=a_1,\\ldots,a_n$ (e.g., $p(x) = (x-a_1)\\cdots(x-a_n)$) , and then define\n$$ g(x) = \\left\\{\\begin{array}{ll}\np(x) & \\text{if $x\\in\\mathbb{Q}$;}\\\\\n0 & \\text{if $x\\notin\\mathbb{Q}$.}\n\\end{array}\\right.$$\nThe function is continuous at $a_1,\\ldots,a_n$, and since $p(x)\\neq 0$ for any $x\\notin\\{a_1,\\ldots,a_n\\}$ then $g(x)$ is not continuous at any point other than $a_1,\\ldots,a_n$. Other possibilities should suggest themselves easily enough.\nA function that is continuous exactly at the integers: a similar idea will work: find a function that has zeros exactly at the integers, for example $f(x)=\\sin(\\pi x)$, and then take\n$$g(x) = \\left\\{\\begin{array}{ll}\n\\sin(\\pi x) & \\text{if $x\\in\\mathbb{Q}$;}\\\\\n0 & \\text{if $x\\notin\\mathbb{Q}$.}\n\\end{array}\\right.$$\nA function continuous exactly in the natural numbers: take a function that is continuous at the integers, and redefine it as the characteristic function of the rationals in appropriate places(what happens at $0$ depends on whether you believe $0$ is in the natural numbers or not). Assuming that $0\\in\\mathbb{N}$, one possibility is:\n$$g(x) = \\left\\{\\begin{array}{ll}\n\\sin(\\pi x)&\\text{if $x\\in\\mathbb{Q}$ and $x\\geq 0$;}\\\\\nx & \\text{if $x\\in\\mathbb{Q}$ and $-\\frac{1}{2}\\lt x\\leq 0$;}\\\\\n1 & \\text{if $x\\in\\mathbb{Q}$ and $x\\leq -\\frac{1}{2}$;}\\\\\n0 & \\text{if $x\\notin\\mathbb{Q}$.}\n\\end{array}\\right.$$\nA function continuous exactly on the rationals. This one is a bit trickier. There is no such function. This follows because the set of discontinuities of a real valued function must be a countable union of closed sets. \nPerhaps then, we might anticipate the next question:\nA function that is continuous exactly on the irrationals. An example is the following: let $s\\colon\\mathbb{N}\\to\\mathbb{Q}$ be an enumeration of the rationals (that is, a bijection from $\\mathbb{N}$ to $\\mathbb{Q}$. Define $f(x)$ as follows:\n$$f(x) = \\sum_{\\stackrel{n\\in\\mathbb{N}}{s_n\\leq x}} \\frac{1}{2^n}.$$\nThe function has a jump at every rational, so it is not continuous at any rational. However, if $x$ is irrational, let $\\epsilon\\gt 0$. Then there exists $N$ such that $\\sum_{k\\geq N}\\frac{1}{2^k}\\lt \\epsilon$. Find a neighborhood of $x$ which excludes every $q_m$ with $m\\leq N$, and conclude that the difference between the value of $f$ at $x$ and at any point in the neighborhood is at most $\\sum_{k\\geq N}\\frac{1}{2^k}$. \nEdit: As I was reminded in the comments by jake, in fact the \"standard example\" of a function that is continuous at every rational and discontinuous at every rational is Thomae's function. The example I give is a monotone function, and although it is discontinuous at every rational, it is continuous from the right at every number.", "meta": {"post_id": 7821, "input_score": 25, "output_score": 34, "post_title": "Constructing Continuous functions at given points"}}
{"input": "Is the localization of a reduced ring (no nilpotents) still reduced?", "output": "Let $A$ be a ring, $S\\subset A$ a multiplicatively closed subset, and suppose that $0\\neq a/b\\in A_S$ is nilpotent. Then there exists $n$ such that $(a/b)^n=0$, i.e., such that there exists $t\\in S$ with $ta^n=0$. But then $ta$ is nilpotent in $A$. If it is zero, then $a/b=0$ in $A_S$, which it isn't.", "meta": {"post_id": 8150, "input_score": 19, "output_score": 38, "post_title": "Does localization preserve reducedness?"}}
{"input": "Why is every group the quotient of a free group by a normal subgroup?", "output": "This is one of the most intuitive observations in all of group theory, and it illustrates the quotient operation in the most fundamental way.\nI'll provide two separate answers. The first is fully intuitive; the second is a formalized version of the first.\nFirst answer: Take a group $G$. A relation on $G$ is an equation satisfied by some of the elements. For instance, $eg = g$ where $e$ is the identity is a relation satisfied by all group elements $g \\in G$. Because we can always multiply by inverses in a group, we can rewrite this relation as $egg^{-1} = gg^{-1} = e$, i.e., $e = e$. This can be applied to any relation. If $G$ is abelian, then $ab = ba$ for all $a,b \\in G$, and we can rewrite this as $aba^{-1}b^{-1} = e$.\nIn other words, a relation asserts that some product of group elements coincides with the identity, so the only information we need to understand the relation is the product which occurs on the left side of the equals sign.\nNow every group has a few relations which are implied directly by the group axioms. $aa^{-1} = e$ is one of them. We can ask whether the group has any extra relations which are not implied by the group axioms. If no such relations exist, i.e., if the only relations which hold are those which must hold by virtue of the group axioms, then the group is said to be free; the group is \"free of additional relations.\"\nIf you have a group $G$, one natural thing to do is to introduce new relations into it and to thereby create new groups. But you can't just introduce completely random relations because (a) the relations can't contradict each other or pre-exising relations and (b) the resulting structure must again be a group. Now we saw earlier that a relation can be specified as a product of group elements. In order that the relations satisfy (a) and (b), it turns out it is necessary and sufficient that the corresponding products form a normal subgroup $N$. The result of introducing the collection of relations $N$ into the group $G$ is the quotient $G/N$.\nAny group $G$ can be obtained in this manner. You start with the free group $F$ whose generators are elements of $G$ considered as a set. And then you look at all the additional relations satisfied by elements of $G$ and assemble them into a normal subgroup $N$. Then $G = F/N$ by the above.\nSecond answer: Given any set $S$, the free group on $S$ is that group $F(S)$ for which every function $f : S \\rightarrow G$ from $S$ to an arbitrary group $G$ extends to a unique homomorphism $\\tilde{f} : F(S) \\rightarrow G$. There are various ways of constructing $F(S)$ explicitly. For instance, you may take $F(S)$ to consist of words over the alphabet whose letters are elements of $S$ and $S'$, where $S'$ has the letter $s^{-1}$ (a symbol at the moment) for each symbol $s \\in S$. It's important to notice that $F(S)$ actually contains equivalence classes of words, because we introduce the obvious cancellation rules; e.g., $abb^{-1}c$ can be reduced via cancellation to $ac$. It must be proved that all possible algorithms for reduction yield the same reduced word; I'll omit that step.\nYou also have to prove that this group $F(S)$ satisfies the stated universal property. I won't prove this in detail, but it is more or less intuitive. Since $\\tilde{f}$ has to be a homomorphism, we find, for instance, that $\\tilde{f}(ab) = \\tilde{f}(a) \\tilde{f}(b) = f(a)f(b)$. In general, since $f$ is defined for all elements of $S$, $\\tilde{f}$ is thereby defined uniquely for all elements of $F(S)$. [It is via similar reasoning that you may determine that it is sufficient to know the values of a linear operator on the elements of a basis of a vector space.]\nSo we start with our group $G$ which we would like to write as a quotient of a free group. Which free group? That free group whose generators are the symbols from $G$. So we pick $F(G)$. Now we need to introduce the needed relations in order to collapse $F(G)$ into $G$. How do we carry it out? By the first answer, we could easily accomplish this if only we knew the normal subgroup $N$ of relations, but it seems that in this general case we don't really know $N$ concretely.\nIn fact, we can figure out $N$ as follows. We can take the identity map $f : G \\rightarrow G$ and extend it to a homomorphism $\\tilde{f} : F(G) \\rightarrow G$. The extension $\\tilde{f}$ is in general not injective, and its kernel is precisely the group of relations $N$! (Formally this is an application of one of the standard theorems on homomorphisms.) Then $G = F(G)/N$ as before.", "meta": {"post_id": 9446, "input_score": 23, "output_score": 39, "post_title": "Every group is the quotient of a free group by a normal subgroup"}}
{"input": "I'm just reviewing for my exam tomorow looking at old exams, unfortunately I don't have solutions.  Here is a question I found : determine if the series converges or diverges.  If it converges find it's limit.  \n$$\\displaystyle \\sum\\limits_{n=1}^{\\infty}\\dfrac{\\sin(n-\\sqrt{n^2+n})}{n}$$\r\nI've ruled down possible tests to the limit comparison test, but I feel like I've made a mistake somewhere.\ndivergence test - limit is 0 by the squeeze theorem\nintegral test - who knows how to solve this\ncomparison test - series is not positive\nratio root tests - on the absolute value of the series, this wouldn't work out\nalternating series test - would not work, the series is not decreasing or alternating  \nAny ideas what to compare this series here with or where my mistake is on my reasoning above?", "output": "The key here is that $n - \\sqrt{n^2 + n}$ converges to $-{1 \\over 2}$ as $n$ goes to infinity:\n$$n - \\sqrt{n^2 + n}=  (n - \\sqrt{n^2 + n}) \\times  {n + \\sqrt{n^2 + n} \\over n + \\sqrt{n^2 + n}}$$\r\n$$= {n^2 - (n^2 + n) \\over n + \\sqrt{n^2 + n}} = -{n \\over  n + \\sqrt{n^2 + n}}$$\r\n$$= -{1 \\over 1 + \\sqrt{1 + {1 \\over n}}}$$\r\nTake limits as $n$ goes to infinity to get $-{1 \\over 2}$.\nHence $\\sin(n - \\sqrt{n^2 + n})$ converges to $\\sin(-{1 \\over 2})$, and the series diverges similarly to ${1 \\over n}$, using the limit comparison test for example.", "meta": {"post_id": 10264, "input_score": 32, "output_score": 39, "post_title": "Does the series $\\sum\\limits_{n=1}^{\\infty}\\frac{\\sin(n-\\sqrt{n^2+n})}{n}$ converge?"}}
{"input": "What is the limit of the series $1 \\over (2n)!$ for n in $[0, \\infty)$ ?  \n$$ \\sum_{n = 0}^{\\infty}{1 \\over (2n)!}$$\nI've ground out the sum of the 1st 1000 terms to 1000 digits using Python,\n(see here ), but how would a mathematician calculate the limit? And what is it? \nNo, this isn't homework. I'm 73. Just curious. \nThanks", "output": "It's half the sum of $e^1=\\sum 1/n!$ and $e^{-1}=\\sum (-1)^{n}/n!$ (or $\\cosh 1$, in other words).", "meta": {"post_id": 12340, "input_score": 25, "output_score": 36, "post_title": "What is limit of $\\sum \\limits_{n=0}^{\\infty}\\frac{1}{(2n)!} $?"}}
{"input": "I heard that using some relatively basic differential geometry, you can show that the only spheres which are Lie groups are $S^0$, $S^1$, and $S^3$.  My friend who told me this thought that it involved de Rham cohomology, but I don't really know anything about the cohomology of Lie groups so this doesn't help me much.  Presumably there are some pretty strict conditions we can get from talking about invariant differential forms -- if you can tell me anything about this it will be a much-appreciated bonus :)\n(A necessary condition for a manifold to be a Lie group is that is must be parallelizable, since any Lie group is parallelized (?) by the left-invariant vector fields generated by a basis of the Lie algebra.  Which happens to mean, by some pretty fancy tricks, that the only spheres that even have a chance are the ones listed above plus $S^7$.  The usual parallelization of this last one comes from viewing it as the set of unit octonions, which don't form a group since their multiplication isn't associative; of course this doesn't immediately preclude $S^7$ from admitting the structure of a Lie group.  Whatever.  I'd like to avoid having to appeal to this whole parallelizability business, if possible.)", "output": "Here is the sketch of the proof.\nStart with a compact connected Lie group G.  Let's break into 2 cases - either $G$ is abelian or not.\nIf $G$ is abelian, then one can easily show the Lie algebra is abelian, i.e., $[x,y]=0$ for any $x$ and $y$ in $\\mathfrak{g}$.  Since $\\mathbb{R}^n$ is simply connected and has the same Lie algebra as $G$, it must be the universal cover of $G$.\nSo, if $G$ is a sphere, it's $S^1$, since all the others are simply connected, and hence are their own universal covers.\nNext, we move onto the case where $G$ is nonabelian.  For $x,y,$ and $z$ in the Lie algebra, consider the map $t(x,y,z) = \\langle [x,y], z\\rangle$.  This map is clearly multilinear.  It obviously changes sign if we swap $x$ and $y$.  What's a bit more surprising is that it changes sign if we swap $y$ and $z$ or $x$ and $z$.  Said another way, $t$ is a 3 form!  I believe $t$ is called the Cartan 3-form.  Since $G$ is nonabelian, there are some $x$ and $y$ with $[x,y]\\neq 0$.  Then $t(x,y,[x,y]) = ||[x,y]||^2 \\neq 0$ so $t$ is not the 0 form.\nNext, use left translation on $G$ to move $t$ around: define $t$ at the point $g\\in G$ to be $L_{g^{-1}}^*t$, where $L_{g^{-1}}:G\\rightarrow G$ is given by $L_{g^{-1}}(h) = g^{-1}h$.\nThis differential 3-form is automatically left invariant from the way you've defined it everywhere.  It takes a bit more work (but is not too hard) to show that it's also right invariant as well.\nNext one argues that a biinvariant form is automatically closed.  This means $t$ defines an element in the 3rd de Rham cohomology of $G$.  It must be nonzero, for if $ds = t$, then we may assume wlog that $s$ is biinvariant in which case $ds = 0 = t$, but $t$ is not $0$ as we argued above.\nThus, for a nonabelian Lie group, $H^3_{\\text{de Rham}}(G)\\neq 0$.  But this is isomorphic to singular homology.  Hence, for a sphere to have a nonabelian Lie group structure, it must satisfy $H^3(S^n)\\neq 0$.  This tells you $n=3$.", "meta": {"post_id": 12453, "input_score": 87, "output_score": 136, "post_title": "Is there an easy way to show which spheres can be Lie groups?"}}
{"input": "in functional analysis, you encounter the terms 'adjoint' and 'formal adjoint'.\nWhat does 'formal' in that case mean? It Sounds like a hint that 'formal adjoints' lack a certain property to make them a 'true' adjoint.\nI have nowhere found a definition, and would be eager to know.", "output": "Are you talking about differential operators on functions over a domain in $\\mathbb{R}^d$? (This is the context in which the phrase \"formal adjoint\" usually comes up.)\nThe idea is that working with, say, smooth functions with compact support, we have the integration by parts formula\n$$ \\int Du\\cdot v ~dx + \\int u\\cdot Dv~ dx = 0 $$\nSo if a linear partial differential operator is defined as $P = \\sum A_\\alpha D^\\alpha$ where $\\alpha$ are multi-indices, you can write $P'$ as a linear partial differential operator $P'\\phi = \\sum (-1)^{|\\alpha|} D^\\alpha(A_\\alpha \\phi)$ and generalize the integration by parts formula\n$$ \\int Pu \\cdot v~dx = \\int u \\cdot P'v~ dx $$\nwhich looks, in form, suspiciously like the adjoint with respect to the $L^2$ inner product. That is, writing $\\langle,\\rangle$ for the $L^2$ inner product of real valued functions, \n$$ \\langle Pu,v\\rangle = \\langle u, P'v\\rangle $$\nThe reason that we call this a formal adjoint is because, technically, to take an adjoint (in the Hilbert space sense, there is also a different notion for Banach spaces) of an operator, you need to specify which Hilbert space you are working over. In the case of the formal adjoint, it is left unspecified: indeed, the formula only really hold for sufficiently smooth function decaying sufficiently fast at infinity, and not in general for arbitrary functions $u,v\\in L^2$. \nIn general for differential operators, the operator itself will not be bounded on an $L^2$ Hilbert space, and so the operator is only densely defined on your Hilbert space. Therefore the adjoint can only be defined on another subset of the Hilbert space, the domain of the adjoint. (In the most general cases, the domain of the adjoint can be a much, much smaller set [even finite dimensional], so does not make much sense as an operator on the original Hilbert space. For differential operators, the adjoint is still densely defined using the density of $C^\\infty_0$ in $L^2$.) (Note that also if the spatial domain has a boundary, the integration by parts formula picks up a boundary term in general, so you pick up a further problem with the notion of adjoints, related to the fact that $C^\\infty_0(\\Omega)$ is not dense in the Sobolev space $W^{1,2}(\\Omega)$ when $\\Omega$ has boundary.)\nWhile the word \"formal\" is, I think, not mentioned explicitly, a lot of the problems that can arise when you deal with unbounded operators are discussed in chapter 8 of Reed-Simon, \"Methods of mathematical physics\".", "meta": {"post_id": 12894, "input_score": 29, "output_score": 48, "post_title": "Distinction between 'adjoint' and 'formal adjoint'"}}
{"input": "My undergraduate number theory class decided to dip into a bit of algebraic geometry to finish up the semester. I'm having trouble understanding this bit of information that the instructor presented in his notes. \nHere it is in paraphrase (assume we are over an abstract field k)\nWe take a polynomial in k, $f  =Y^2 - X^3 -aX -b$ and homogenize the polynomial to $F = Y^2Z - X^3 -aXZ^2 - bZ^3$. Note that the points at infinity of V(F) consist of triples $[\\alpha : \\beta : 0]$ s.t $ -\\alpha^3 = 0$, hence the only point at infinity is $[0 : 1 :0]$\r\nThe part I'm confused about is in italics. He introduces the terms \"points at infinity\" without defining it. After some google time, I understand what a point at infinity means in the context of a projective space/projective line but am having trouble understanding how the professor came to his conclusion about the point at infinity in this particular example \nHere is my question. In general, are all points in the locus of vanishing points for a homogeneous polynomials considered points at infinity? If not, is there a general procedure for calculating these point if we are given an arbitrary polynomial?\nMore abstractly, How do I understand that a finite point in the projective space is a \"point at infinity\" for this polynomial.", "output": "Here's another way to think about the \"line at infinity\" and the \"points at infinity\"...\nThink of the usual $XY$-plane as sitting inside of $3$-space, but instead of it sitting in its usual place, $\\{(x,y,0) : x,y\\in\\mathbb{R}\\}$, shift it up by $1$ so that it sits as the $z=1$ plane.\nNow, you are sitting at the origin with a very powerful laser pointer. Whenever you want a point on the $XY$-plane, you shine your laser pointer at that point. So, if you want the point $(x,y)$, you are actually pointing your laser pointer at the point $(x,y,1)$; since you are sitting at the origin, the laser beam describes a (half)-line, joining $(0,0,0)$ to $(x,y,1)$. \nNow, for example, look at the point $(x,0,1)$, and imagine $x$ getting larger. The angle your laser pointer makes with the $z=0$ plane gets smaller and smaller, until \"as $x$ goes to infinity\", your laser pointer is just pointing along the line $x$ axis (at the point $(1,0,0)$), and the same thing happens if you let $x$ go to $-\\infty$. More generally, if you start pointing to points that are further and further away from the \"origin\" in your plane (away from $(0,0,1)$), the laser beam's angle with $Z=0$ gets smaller and smaller, until, \"at the limit\" as $||(x,y)||\\to\\infty$, you end up with the laser beam pointing along the $z=0$ plane in some direction. We can represent the direction with the slope of the line, so that we are pointing at $(1,m,0)$ for some $m$ (or perhaps to $(-1,-m,0)$, but that's the same direction), or perhaps to the point $(0,1,0)$. So we \"add\" these \"points at infinity\" (so called because we get them by letting the point we are shining the laser beam on \"go to infinity\"), one for each direction away from the \"origin\": $(1,m,0)$ for arbitrary $m$ for nonvertical lines, and $(0,1,0)$ corresponding to the direction of $x=0$, $y\\to\\pm\\infty$. \nSo: the \"usual\", affine points, are the ones in the $z=1$ plane, and they correspond to laser beams coming from the origin; they are each of the form $(x,y,1)$ for some $x,y$ in $\\mathbb{R}$. In addition, for each \"direction\" we want to include that limiting laser beam which does not intersect the plane $z=1$; those correspond to points $(1,m,0)$, or the point $(0,1,0)$ when you do it with the line $x=0$. So we get one point for every real $m$, $(1,m,0)$, and another for $(0,1,0)$. You are adding one point for every direction of lines through the origin; these points are the \"points at infinity\", and together they make the \"line at infinity\".\nNow, put your elliptic curve/polynomial $F=Y^2 - X^3 - aX-b$, and draw the points that correspond to it on the $z=1$ plane; that's the \"affine piece\" of the curve. But do you also get any of those \"points at infinity\"?\nWell, even though we are thinking of the points as being on the $XY$-plane, they \"really\" are in the $Z=1$ plane; so our equation actually has a \"hidden\" $Z$ that we lost sight of when we evaluated at $Z=1$. We use the homogenization $f = Y^2Z - X^3 - aXZ^2 - bZ^3$ to find it. Why that? Well, for any fixed point $(x,y,1)$ in our \"$XY$-plane\", the laser pointer points to all points of the form $(\\alpha x,\\alpha y,\\alpha)$. If we were to shift up our copy of the plane from $Z=1$ to $Z=\\alpha$, we'll want to scale everything so that it still corresponds to what I'm tracing from the origin; this requires that every monomial have the same total degree, which is why we put in factors of $Z$ to complete them to degree $3$, the smallest we can (making it bigger would give you the point $(0,0,0)$ as a solution, and we do need to stay away from that because we cannot point the laser pointer in our eye). \nOnce we do that, we find the \"directions\" that also correspond to our curve by setting $Z=0$ and solving, to find those points $(1,m,0)$ and $(0,1,0)$ that may also lie in our curve. But the only one that works is $(0,1,0)$, which is why the elliptic curve $F$ only has one \"point at infinity\".", "meta": {"post_id": 13763, "input_score": 29, "output_score": 50, "post_title": "Elliptic Curves and Points at Infinity"}}
{"input": "The Collatz Conjecture is a famous conjecture in mathematics that has lasted for over 70 years. It goes as follows:\nDefine $f(n)$ to be as a function on the natural numbers by:\n$f(n) = n/2$ if $n$ is even and\n$f(n) = 3n+1$ if $n$ is odd\nThe conjecture is that for all $n \\in \\mathbb{N}$, $n$ eventually converges under iteration by $f$ to $1$.\nI was wondering if the \"5n+1\" problem has been solved. This problem is the same as the Collatz problem except that in the above one replaces $3n+1$ with $5n+1$.", "output": "You shouldn't expect this to be true. Here is a nonrigorous argument. Let $n_k$ be the sequence of odd numbers you obtain. So (heuristically), with probability $1/2$, we have $n_{k+1} = (5n_k+1)/2$, with probability $1/4$, we have $n_{k+1} = (5 n_k+1)/4$, with probability $1/8$, we have $n_{k+1} = (5 n_k+1)/8$ and so forth. Setting $x_k = \\log n_k$, we approximately have $x_{k+1} \\approx x_k + \\log 5 - \\log 2$ with probability $1/2$, $x_{k+1} \\approx x_k + \\log 5 - 2 \\log 2$ with probability $1/4$, $x_{k+1} \\approx x_k + \\log 5 - 3 \\log 2$ with probability $1/8$ and so forth.\nSo the expected change from $x_{k}$ to $x_{k+1}$ is \n$$\\sum_{j=1}^{\\infty} \\frac{ \\log 5 - j \\log 2}{2^j} = \\log 5 - 2 \\log 2.$$\nThis is positive! So, heurisitically, I expect this sequence to run off to $\\infty$. This is different from the $3n+1$ problem, where $\\log 3 - 2 \\log 2 <0$, and so you heurisitically expect the sequence to decrease over time. \nHere is a numerical example. I started with $n=25$ and generated $25$ odd numbers. Here is a plot of $(k, \\log n_k)$, versus the linear growth predicted by my heuristic. Notice that we are up to 4 digit numbers and show no signs of dropping down.", "meta": {"post_id": 14569, "input_score": 31, "output_score": 49, "post_title": "The $5n+1$ Problem"}}
{"input": "I have an HP 50g graphing calculator and I am using it to calculate the standard deviation of some data. In the statistics calculation there is a type which can have two values:\nSample\nPopulation\nI didn't change it, but I kept getting the wrong results for the standard deviation. When I changed it to \"Population\" type, I started getting correct results!\nWhy is that? As far as I know, there is only one type of standard deviation which is to calculate the root-mean-square of the values!\nDid I miss something?", "output": "There are, in fact, two different formulas for standard deviation here: The population standard deviation $\\sigma$ and the sample standard deviation $s$.\nIf $x_1, x_2, \\ldots, x_N$ denote all $N$ values from a population, then the (population) standard deviation is \n$$\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2},$$\nwhere $\\mu$ is the mean of the population.  \nIf $x_1, x_2, \\ldots, x_N$ denote $N$ values from a sample, however, then the (sample) standard deviation is \n$$s = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\bar{x})^2},$$\nwhere $\\bar{x}$ is the mean of the sample.\nThe reason for the change in formula with the sample is this: When you're calculating $s$ you are normally using $s^2$ (the sample variance) to estimate $\\sigma^2$ (the population variance).  The problem, though, is that if you don't know $\\sigma$ you generally don't know the population mean $\\mu$, either, and so you have to use $\\bar{x}$ in the place in the formula where you normally would use $\\mu$.  Doing so introduces a slight bias into the calculation: Since $\\bar{x}$ is calculated from the sample, the values of $x_i$ are on average closer to $\\bar{x}$ than they would be to $\\mu$, and so the sum of squares $\\sum_{i=1}^N (x_i - \\bar{x})^2$ turns out to be smaller on average than $\\sum_{i=1}^N (x_i - \\mu)^2$.  It just so happens that that bias can be corrected by dividing by $N-1$ instead of $N$.  (Proving this is a standard exercise in an advanced undergraduate or beginning graduate course in statistical theory.)  The technical term here is that $s^2$ (because of the division by $N-1$) is an unbiased estimator of $\\sigma^2$. \nAnother way to think about it is that with a sample you have $N$ independent pieces of information.  However, since $\\bar{x}$ is the average of those $N$ pieces, if you know $x_1 - \\bar{x}, x_2 - \\bar{x}, \\ldots, x_{N-1} - \\bar{x}$, you can figure out what $x_N - \\bar{x}$ is.  So when you're squaring and adding up the residuals $x_i - \\bar{x}$, there are really only $N-1$ independent pieces of information there.  So in that sense perhaps dividing by $N-1$ rather than $N$ makes sense.  The technical term here is that there are $N-1$ degrees of freedom in the residuals $x_i - \\bar{x}$.\nFor more information, see Wikipedia's article on the sample standard deviation.", "meta": {"post_id": 15098, "input_score": 58, "output_score": 105, "post_title": "Sample Standard Deviation vs. Population Standard Deviation"}}
{"input": "Many courses and books assume that rings have an identity. They say there is not much loss in generality in doing so as rings studied usually have an identity or can be embedded in a ring with an identity. What then are the major applications of rings without an identity occurring naturally in mathematics?", "output": "The most common example of rings without identity occurs in functional analysis, when one considers rings of functions. A typical example is to consider the ring of all functions of compact support on a non-compact space. Obviously, as these rings of functions are very important in $C^*$-algebras and in studying the properties of the space, knowledge about rings without identity is very important for studying these spaces.\nArbitrary direct sums of rings with unity are not rings with unity, which can also be fairly annoying. \nIt is true that one can always embed a ring (as an ideal, even) into a ring with identity. The most common such embedding is the Dorroh embedding, in which we start with a ring $R$, and consider the ring with underlying set $\\mathbb{Z}\\times R$ and operations given by $(n,a)+(m,b) = (n+m,a+b)$ and $(n,a)(m,b) = (nm, nb+ma+ab)$. It is not hard to verify that $r\\mapsto (0,r)$  embeds $R$ into the Dorroh extension as an ideal. You can preserve the characteristic of $R$ if necessary: if $R$ is of characteristic $n$, then replace $\\mathbb{Z}$ with $\\mathbb{Z}/n\\mathbb{Z}$ in the construction. The extension has other nice properties (ideals of $R$ remain ideals of the extension, for example).\n(Luckily, I am currently going over a thesis about embedding rings as ideals into rings with identity, so I can give you some other classical results.)\nHowever, the Dorroh extension does not preserve all ring properties that may be of interest in $R$. For example, a ring is entire if it has no nonzero zero divisors; a ring is prime if whenever $A$ and $B$ are ideals and $AB=0$, then either $A=0$ or $B=0$ (that is, \"prime\" is the ideal version of \"entire\"; an entire ring is necessarily prime). For example, if you perform the Dorroh extension on $\\mathbb{Z}$ itself (perhaps not realizing it already had a $1$) then $(1,-1)(0,r)=(0,0)$ even though $\\mathbb{Z}$ is entire. There are nontrivial examples of this situation as well. Another property not necessarily preserved by the Dorroh extension is being semiprime.\nThere are other standard embeddings of rings into rings with identity, such as the Szendrei extension (a quotient of the Dorroh extension). But even so there are ring-theoretic properties that may be very hard to maintain in these kinds of embeddings. Among the more difficult ones are simplicity (if $R$ is simple, can we embed $R$ into a simple ring with identity? Yes; Anne Vakarietis, a student of a colleague, just finished putting together the pieces for this in her dissertation). It's known that every commutative $n$-root ring (rings in which every element has an $n$th root) can be embedded in a commutative $n$-root ring with identity, but it is not known if this is possible for noncommutative rings. Likewise, it is not known if every semiprimary ring can be embedded in a semiprimary ring with identity.\nAnd worse, there are some properties that we know cannot be respected by such embeddings. For example, Fuchs and Rangaswamy proved that not every $\\pi$-regular ring can be embedded as an ideal in a $\\pi$-regular ring with identity (a ring is $\\pi$-regular if every element is $n$-regular for some natural number $n$; an element $x$ is $n$-regular if there exists some $y$ such that $x^nyx^n=x^n$; this is a generalization of von Neumann regularity).\nSo, in summary: yes, rings without identity arise very naturally, and as such they show up when investigating other mathematical objects. And while it is true that one can always embed a ring without identity as an ideal into a ring with identity, this may not be a good thing from the point of view of studying some ring-theoretic properties of these rings.", "meta": {"post_id": 16168, "input_score": 28, "output_score": 38, "post_title": "Applications of rings without identity"}}
{"input": "I'm doing the exercises in \"Introduction to commutive algebra\" by Atiyah&MacDonald. In chapter two, exercises 24-26 assume knowledge of the Tor functor.\nI have tried Googling the term, but I don't find any readable sources. Wikipedia's explanation use the the term \"take the homology\", which I don't understand (yet).\nAre there any good explanations of what the Tor functor is available online not assuming any knowledge about homology?\nThe first exercise:\n\"If $M$ is an $A$-module, TFAE:\n\n1) $M$ is flat\n2) $\\operatorname{Tor}_n^A (M,N)=0$ for all $n>0$ and all $A$-modules $N$.\n3) $\\operatorname{Tor}_1^A (M,N)=0$ for all $A$-modules $N$.\"\n\nThanks in advance.", "output": "You will be a lot more motivated to learn about Tor once you observe closely how horribly tensor product behaves. \nLet us look at the simplest example possible. Consider the ring $R=\\mathbb C[x,y]$ and the ideal $I=(x,y)$. These are about the most well-understood objects, right? What is the tensor product $I\\otimes_RI$? This is quite nasty, it has torsions: the element $u = x\\otimes y - y\\otimes x$ is non-zero, but $xu=yu=0$!\nTor gives you a black box to understand this kind of things. Take the short exact sequence $0 \\to I \\to R \\to R/I \\to 0$ and tensor with $I$ we get:\n$$0 \\to \\text{Tor}_1(R/I,I) \\to I\\otimes I \\to I \\to I/I^2 \\to 0$$ \nfrom which you can extract:\n$$0 \\to \\text{Tor}_1(R/I,I) \\to  I\\otimes I \\to I^2 \\to 0$$\nBut $\\text{Tor}_1(R/I,I) = \\text{Tor}_2(R/I, R/I) = \\mathbb C$ by standard homological algebra.  So now everything fits nicely: the map from $I\\otimes I \\to I^2$ takes $f\\otimes g$ to $fg$, and the kernel is generated by the element $u$, which is killed by $I$, so it is isomorphic to $R/I \\cong \\mathbb C$. \nTo summarize: tensor product, despite being a fundamental operation, is actually quite bad, and Tor helps you to understand it.", "meta": {"post_id": 16310, "input_score": 85, "output_score": 45, "post_title": "What is the Tor functor?"}}
{"input": "Let $G$ be a group where every non-identity element has order 2.   \nIf |G| is finite then $G$ is isomorphic to the direct product $\\mathbb{Z}_{2} \\times \\mathbb{Z}_{2} \\times \\ldots  \\times \\mathbb{Z}_{2}$.\nIs the analogous result \n $G= \\mathbb{Z}_{2} \\times \\mathbb{Z}_{2} \\times \\ldots $.\ntrue for the case |G| is infinite?", "output": "Perhaps the best way to look at the problem is to establish the following more precise result:\nFor a group $G$, the following are equivalent:\n(i) Every non-identity element of $G$ has order $2$.\n(ii) $G$ is commutative, and there is a unique $\\mathbb{Z}/2\\mathbb{Z}$-vector space structure on $G$ with the group operation as addition.\nI guess you probably already know how to show that if every nonidentity element has order $2$, $G$ is commutative: for all $x,y \\in G$, $e = (xy)^2 = xyxy$.  Multiplying on the left by $x$ and on the right by $y$ gives $xy = yx$. \nHaving established the commutativity, it is convenient to write the group law additively.  Then there is only one possible $\\mathbb{Z}/2\\mathbb{Z}$-vector space structure on $G$, since it remains to define a scalar multiplication and of course we need $0 \\cdot x = 0, \\ 1 \\cdot x = x$ for all $x \\in G$.  But you should check that this actually works: i.e., defines an $\\mathbb{Z}/2\\mathbb{Z}$-vector space structure, just by checking the axioms: the key point is that for all $x \\in G$, $(1+1)x = x + x = 0 = 0x$.\nSo now your question is equivalent to: is every $\\mathbb{Z}/2\\mathbb{Z}$ vector space isomorphic to a product of copies of $\\mathbb{Z}/2\\mathbb{Z}$?  Well, the only invariant of a vector space is its dimension.  It is clear that every finite-dimensional vector space is of this form.  Every infinite dimensional space is isomorphic to a direct sum $\\bigoplus_{i \\in I} \\mathbb{Z}/2\\mathbb{Z}$, the distinction being that in a direct sum, every element has only finitely many nonzero entries.  (In other words, the allowable linear combinations of basis elements are finite linear combinations.)  Moreover, for any infinite index set $I$, the direct sum $\\bigoplus_{i \\in I} \\mathbb{Z}/2\\mathbb{Z}$ has dimension $I$ and also cardinality $I$.  \nFinally, it is not possible for a direct product of two element sets to have countably infinite cardinality: if $I$ is infinite, it is at least countable, and then the infinite direct product has the same cardinality of the real numbers (think of binary expansions).  So the answer to your question is \"yes\" for direct sums, but \"no\" for direct products.", "meta": {"post_id": 17054, "input_score": 44, "output_score": 43, "post_title": "Group where every element is order 2"}}
{"input": "I'm studying graphs in algorithm and complexity,\n(but I'm not very good at math) as in title:\n\nWhy a complete graph has $\\frac{n(n-1)}{2}$ edges?\n\nAnd how this is related with combinatorics?", "output": "A simpler answer without binomials: A complete graph means that every vertex is connected with every other vertex. If you take one vertex of your graph, you therefore have $n-1$ outgoing edges from that particular vertex. \nNow, you have $n$ vertices in total, so you might be tempted to say that there are $n(n-1)$ edges in total, $n-1$ for every vertex in your graph. But this method counts every edge twice, because every edge going out from one vertex is an edge going into another vertex. Hence, you have to divide your result by 2. This leaves you with $n(n-1)/2$.", "meta": {"post_id": 17747, "input_score": 59, "output_score": 94, "post_title": "Why a complete graph has $\\frac{n(n-1)}{2}$ edges?"}}
{"input": "I am trying to prove a result, for which I have got one part, but I am not able to get the converse part.\nTheorem. Let $R$ be a commutative ring with $1$. Then $f(X)=a_{0}+a_{1}X+a_{2}X^{2} + \\cdots + a_{n}X^{n}$ is a unit in $R[X]$ if and only if $a_{0}$ is a unit in $R$ and $a_{1},a_{2},\\dots,a_{n}$ are all nilpotent in $R$.\nProof. Suppose $f(X)=a_{0}+a_{1}X+\\cdots +a_{n}X^{n}$ is such that $a_{0}$ is a unit in $R$ and $a_{1},a_{2}, \\dots,a_{r}$ are all nilpotent in $R$. Since $R$ is commutative, we get that $a_{1}X,a_{2}X^{2},\\cdots,a_{n}X^{n}$ are all nilpotent and hence also their sum is nilpotent. Let $z = \\sum a_{i}X^{i}$ then $a_{0}^{-1}z$ is nilpotent and so $1+a_{0}^{-1}z$ is a unit. Thus $f(X)=a_{0}+z=a_{0} \\cdot (1+a_{0}^{-1}z)$ is a unit since product of two units in $R[X]$ is a unit. \nI have not been able to get the converse part and would like to see the proof for the converse part.", "output": "Let $f=\\sum_{k=0}^n a_kX^k$ and $g= \\sum_{k=0}^m b_kX^k$. If $f g=1$, then clearly $a_0,b_0$ are units and:\n$$a_nb_m=0 \\tag1$$\n$$a_{n-1}b_m+a_nb_{m-1}=0$$\n(on multiplying both sides by $a_n$)\n$$\\Rightarrow (a_n)^2b_{m-1}=0 \\tag2$$\n$$a_{n-2}b_m+a_{n-1}b_{m-1}+a_nb_{m-2}=0$$\n(on multiplying both sides by $(a_n)^2$)\n$$\\Rightarrow (a_n)^3b_{m-2}=0 \\tag3$$\n$$.....$$\n$$.....+a_{n-2}b_2+a_{n-1}b_1+a_nb_0=0$$\n(on multiplying both sides by $(a_n)^m$)\n$$\\Rightarrow (a_n)^{m+1}b_{0}=0 \\tag{m+1}$$\nSince $b_0$ is an unit, it follows that $(a_n)^{m+1}=0$.\nHence, we proved that $a_n$ is nilpotent, but this is enough. Indeed, since $f$ is invertible, $a_nx^n$ being nilpotent implies that $f-a_nX^n$ is unit and we can repeat (or more rigorously, perform induction on $\\deg(f)$).", "meta": {"post_id": 19132, "input_score": 69, "output_score": 76, "post_title": "Characterizing units in polynomial rings"}}
{"input": "Consider the following diagram:\n\nWhat does it mean precisely to say \"$f$ factors through $G/\\text{ker}(f)$\"?\nDoes it mean $f = \\tilde{f} \\circ \\pi$, for some $\\tilde{f}$?\nI've seen texts use the phrase, but never a definition of this notion.", "output": "It means exactly what you write: that you can express $f$ as  \"product\" (composition) of two functions, with the first function going through $G/\\mathrm{ker}(f)$; by implication, that map will be the \"natural\" map into the quotient, i.e., $\\pi$. Under more general circumstances, you would also indicate the map in question.\nThe reason for the term \"factors\" is that if you write composition of functions by juxtaposition, which is fairly common, then the equation looks exactly as if you \"factored\" $f$: $f=\\tilde{f}\\pi$.", "meta": {"post_id": 21932, "input_score": 56, "output_score": 48, "post_title": "What does it mean to say a map \"factors through\" a set?"}}
{"input": "So the question states that the intersection of two prime ideals is always a prime ideal. Well this is false but I need an example to counter it. I looked online and found one \"For example, inside $\\mathbb Z, 2 \\mathbb Z$ and $3\\mathbb Z$ are prime, but there intersection, $6\\mathbb Z$ is not prime\"\nso I just need some explanation to what a prime ideal is and how you can determine that an ideal is prime. The definition I know of is\n\nLet $R$ be a comm. ring with identity. An ideal P is prime iff $R \\neq P$ and whenever $bc \\in P$ then $b \\in P$ or $c \\in P$\n\nI dont know how to apply this definition to the example above.", "output": "I'm going to concentrate on commutative rings, because those are the ones closest to what you might be familiar with. To deal with the notions in noncommutative rings takes a bit more work, but is certainly doable.\nIdeals, at least at first, are meant to generalize the notion of \"is a multiple of\" (turns out that there is a different motivation for singling out ideals among the subrings, which is essentially the same reason for singling out normal subgroups among all subgroups of a group; this is not relevant right now, but you might be interested in taking a look at this answer about normal subgroups later). \nIf you consider the integers, you can characterize a number $n$ (up to sign) by describing all elements that are multiples of $n$. If you know exactly who the multiples of $n$ are, then you know exactly who $n$ is (except that you might confuse it with $-n$). So, instead of looking at the number $n$, we can look at the collection of all its multiples, \n$$n\\mathbb{Z} = (n) = \\{a\\in\\mathbb{Z}\\mid a\\text{ is a multiples of }n\\}.$$\nWhat properties do the collections of \"all multiples of a given number\" have? Well:\n\nThe collection always contains $0$.\nIf $a$ and $b$ are in the collection, so are $a+b$ and $a-b$.\nIf $a$ is in the collection, and $r$ is any element of the ring, then $ra$ is also in the collection.\n\nIn the integers, and also in many other rings (for example, $\\mathbb{R}[x]$, the polynomials over $\\mathbb{R}$), every collection that satisfies these three properties is in fact the collection of all multiples of some $a\\in R$. But there are other rings where this does not happen. For example, if you consider the ring $\\mathbb{Z}[x]$ of all polynomials with integer coefficients, you can take\n$$I = \\{ p(x)\\in\\mathbb{Z}[x]\\mid p(0)\\text{ is even}\\}.$$\nThis collection satisfies all three properties: $0$ is in $I$; if $p(x)$ and $q(x)$ are in $I$, then so is $p(x)+q(x)$, because $(p+q)(0) = p(0)+q(0)$ is a sum of two even numbers, hence even; and if $p(x)$ is in the collection and $q(x)$ is any polynomial with integer coefficients, when $pq(0) = p(0)q(0)$ is even, because $p(0)$ is even and $q(0)$ is an integer. So $I$ is an ideal. \nIs this $I$ the collection of \"all multiples of $a$\" for some $a\\in\\mathbb{Z}[x]$? No. If there were such an $a$, then since $2\\in I$, then $2$ would have to be a multiple of $a$. That means that $a$ must be a constant polynomial, and must be either $\\pm 1$ or $\\pm 2$ (the only elements of $\\mathbb{Z}[x]$ that divide $2$). It can't be either $1$ or $-1$, because \"multiples of $\\pm 1$\" is everything, and not everything is in our $I$. But $x\\in I$ as well, since evaluating $x$ at $0$ is even; and neither $2$ nor $-2$ divide $x$ in $\\mathbb{Z}[x]$. So even though $I$ is an ideal, it is not \"all multiples of\" someone. So the notion of \"ideal\", even though it starts up as \"all multiples of\" someone, is actually more general. This is the distinction between principal ideals (ideals which are \"all multiples of $a$\" for some $a$), and more general ideals (which need not be made up of \"all multiples of $a$\" for some $a$).\nNonetheless, ideals are closely connected to the notions of divisibility; as Dedekind noted when he introduced them in the 19th century, if you want to try to do \"modulo arithmetic\" as in the integers (working modulo $n$ is \"really\" working in $\\mathbb{Z}/(n)$) then the conditions you need on a collection are precisely the conditions that are needed to have ideals. That is, ideals are exactly the things for which you can do \"modulo arithmetic\". And modulo arithmetic is all about divisibility (after all, $a\\equiv b\\pmod{n}$ means that $n$ divides $a-b$). \nSo we want to also keep track of a few of the other special properties that some numbers have, and \"translate\" them into what they mean for ideals. \nPrime numbers play a major role in divisibility issues in the integers. How does the \"prime\" property translate into the setting of ideals? A prime is a number $p$ such that:\n\n$p\\neq \\pm 1$; and\nIf $p$ divides a product, then it divides at least one of the factors.\n\nOkay, how does that translate into ideals? If you think of an ideal $I$ as \"the collection of all multiples of some number\" (again, not really that in the general setting, but that's where the intuition and some of the definitions come from), then when do the multiples correspond to a prime? We need the prime not to divide everything; so we require the ideal to not be the entire ring, $I\\neq R$. And the second condition: if $ab$ is a multiple, then either $a$ or $b$ is a multiple. In other words: if $ab\\in I$, then either $a\\in I$ or $b\\in I$. So we define:\n\nAn ideal $I$ is a prime ideal if and only if $I\\neq R$, and whenever $ab\\in I$, either $a\\in I$ or $b\\in I$.\n\nGoing back to the intuition for ideals: what does the intersection of ideals correspond to? If $I$ is sort of like \"all multiples of $a$\", and $J$ is sort of like \"all multiples of $b$\", then what is $I\\cap J$? All things that are multiples of both $a$ and $b$!\nSo, if $P$ and $Q$ are both prime ideals, would $P\\cap Q$ be a prime ideal? Generally no: in general, you don't expect things that are multiples of two different primes to be themselves prime. And so you get to your example. $(2)$ is a prime ideal in $\\mathbb{Z}$, precisely because $2$ is a prime number: if $ab\\in(2)$, then $ab$ is a multiple of $2$, so either $a$ is a multiple of $2$ or $b$ is a multiple of $2$ (because $2$ is a prime number), so either $a\\in(2)$ or $b\\in(2)$. Similarly with $(3)$. But $(2)\\cap(3)$ will be all numbers that are multiples of both $2$ and $3$; this corresponds to \"all multiples of $6$\", as we know from elementary number theory: $(2)\\cap(3)=(6)$. But $6$ is not a prime number, so there is no reason to expect $(6)$ to be a prime ideal. In fact, a witness to the fact that $6$ is not a prime number should also work as a witness to the fact that $(6)$ is not a prime ideal. And indeed it does.\nCaveat. The analogy of ideals as \"set of all multiples of something\" works reasonably well in very familiar settings, but breaks down very quickly once you get beyond the most basic of rings. For instance, in the integers, you cannot have two nonzero prime ideals $(p)$ and $(q)$ with $p\\neq 0$, $q\\neq 0$, $p\\neq \\pm q$, and $(p)\\subseteq (q)$: that would mean that $p$ is a multiple of $q$, and with prime numbers that can only happen if $p=\\pm q$. But in other rings it is certainly possible for it to happen. For instance, in $R=\\mathbb{R}[x,y]$, the ring of polynomials in two variables, both\n\\begin{align*}\n(x) &= \\{ p(x,y)\\in R\\mid p(0,y) = 0\\text{ for all }y\\};\\\\\n(x,y) &= \\{ p(x,y)\\in R\\mid p(0,0) = 0\\}\n\\end{align*}\nare ideals; clearly $(x)\\subseteq (x,y)$, $(x)\\neq (0)$, $(x,y)\\neq (0)$, and $(x)\\neq (x,y)$. Yet both $(x)$ and $(x,y)$ are prime ideals. \nSo the analogy can only take you so far, and it can be misleading if you try to take it all the way. But at least at first you might find it a useful hook for thinking about possible examples and possible counterexamples.", "meta": {"post_id": 23790, "input_score": 8, "output_score": 34, "post_title": "Prime ideals: definition, verification, and examples"}}
{"input": "Could give some examples of nonnegative measurable function $f:\\mathbb{R}\\to[0,\\infty)$, such that its integral over any bounded interval is infinite?", "output": "The easiest example I know is constructed as follows. Let $q_{n}$ be an enumeration of the rational numbers in $[0,1]$. Consider $$g(x) = \\sum_{n=1}^{\\infty} 2^{-n} \\frac{1}{|x-q_{n}|^{1/2}}.$$\nSince each function  $\\dfrac{1}{|x-q_{n}|^{1/2}}$ is integrable on $[0,1]$, so is $g(x)$ [verify this!]. Therefore $g(x) < \\infty$ almost everywhere, so we can simply set $g(x) = 0$ in the points where the sum is infinite.\nOn the other hand, $f = g^{2}$ has infinite integral over each interval in $[0,1]$. Indeed, if $0 \\leq a \\lt b \\leq 1$ then $(a,b)$ contains a number $q_{n}$, so $$\\int_{a}^{b} f(x)\\,dx \\geq \\int_{a}^{b} \\frac{1}{|x-q_{n}|}\\,dx = \\infty.$$ Now in order to get the function $f$ defined at every point of $\\mathbb{R}$, simply define $f(n + x) = f(x)$ for $0 \\leq x \\lt 1$.", "meta": {"post_id": 24413, "input_score": 40, "output_score": 47, "post_title": "Is there a function with infinite integral on every interval?"}}
{"input": "I've just sat through several lectures that proved most of the results in Tate's thesis: the self-duality of the adeles, the construction of \"zeta functions\" by integration, and the proof of the functional equation. However, while I was able to follow at least some of the arguments in the individual steps,  I understand almost nothing about the big picture. My impression so far is that Tate invented a new and fancier way of proving the functional equation that the Hecke analytic approach. But is there more to the story than \"this is a neat way of proving something already known\"?\nI'm under the impression that Tate's thesis laid the foundations for the Langlands program, but I don't understand this properly yet. \nCan someone explain to me what's the real significance and meaning of Tate's thesis?", "output": "Tate's thesis introduces the concept, ubiquitous now, of doing analysis, and especially Fourier analysis, on the locally compact ring of adeles.    In this setting, the discrete subgroup $\\mathbb Z \\subset \\mathbb R$ is replaced\nby the discrete subgroup $\\mathbb Q \\subset \\mathbb A$.\nThis has a number of implications, some of which are:\n\n$\\mathbb Q$ is a field, and $\\mathbb A$ is essentially a product of fields.\nIt is technically almost always easier to work with fields rather than more general rings (such as $\\mathbb Z$).  The adelic formalism allows one to have one's cake and eat it too (in some sense): one is working with the field $\\mathbb Q$, not the ring $\\mathbb Z$, but the primes are still present, in the factorization of $\\mathbb A$ as a product.  (And this product structure of \n$\\mathbb A$, which is formally very simple, captures in some subtle way the\ndeeper sense of \"product\" in the statement of the fundamental theorem of arithmetic, i.e. that any natural number is a unique product of prime powers.)\nTate writes zeta-functions, or more generally, Hecke $L$-series, as integrals over $\\mathbb A^{\\times}$.  The Euler product structure of the $L$-series then becomes simply a factorization of this integral according to the product structure of $\\mathbb A^{\\times}$.  (This is a manifestation of the parenthetical remark at the end of point (1).)\nThe proof of the functional equation becomes (more-or-less) just an application of Poisson summation (in the adelic context).  \nIt is worth comparing this with the classical proof (which one can read in Lang's book, among other places, if memory serves).   Classically, one takes\nthe sum over ideals representation of the $L$-function, and decomposes it first\ninto a finite collection of sums, indexed by the ideal class group, each sum taking place over all the integral ideals in a given ideal class.  These individual series are then described as Mellin transforms of theta series, \nand the functional equation is derived from the transformation properties of the\ntheta series, the latter being proved by an application of Poisson summation in the classical setting.   \nOnce one unpacks all the details, Tate's proof and Hecke's proof don't look so different; but the difference in packaging is enormous!  In Tate's approach there is no need to unpack everything (for example, the ideal class group is just lurking around in the background implicitly, and there is no need to bring it out explicitly), while in the classical arguments such unpacking is key to the whole thing.   \nAs another example of the conceptual clarity and simplification that Tate's approach gives, you might consider the way he derives the formula for the residue at $s = 1$ of the zeta function of a number field (i.e. the general class number formula) and compare it with the classical derivation.\nWorking in the case of a function field over a finite field, Tate derives the Riemann--Roch formula (in the form $\\dim H^0(C,\\mathcal O(D)) - \\dim\nH^0(C,\\mathcal O(K - D)) = 1 + \\deg D - g$) as a straightforward consequence of Poisson summation.  Among other things, this provides a rather striking unification of (what we now call) Serre duality and Fourier duality.  (Although I don't know the precise history, this probably has antecedents in the literature: the original proof of the functional equation of the $\\zeta$-function for a curve over a finite field, by  Schmidt, proceeded by applying Riemann--Roch; so Tate is essentially reversing this argument.)\nTate's explication of the functional equation of $L$-series in terms of local functional equations shows that the global root number --- i.e. the constant that appears in the functional equation --- is a product of local numbers.  As far as I understand, this wasn't known (and perhaps not even suspected) prior to Tate's proof.  \nThis may seem slightly esoteric, but experience shows that one should regard global root numbers, and their factorization into a product of local root numbers (or $\\epsilon$-factors), to be of essentially equal importance to global $L$-series, and their (Euler product) factorization into local $L$-factors.\n\nSummary/Conclusion: The aim of the above list is just to highlight some of the points to watch out for while studying Tate's thesis.  Let me now make some remarks at a more general level.\nIn the classical theory of zeta and $L$-functions, there is a tension between the analytic tools, which are essentially additive Fourier theory (e.g. Poissson summation) and the multiplicative aspects of the theory (exemplified by the Euler product).  Tate's thesis resolves these tensions by moving to the adelic context.\nIn the general theory of automorphic forms (say on a quotient $\\Gamma\n\\backslash G(\\mathbb R)$) for some congruence subgroup $\\Gamma$ of the integral points $G(\\mathbb Z)$ of a semi-simple or reductive Lie group $G(\\mathbb R)$) there is the same tension between the harmonic analysis and Lie theory (which $\\Gamma \\backslash G(\\mathbb R)$ is well set-up to accommodate) and the theory of Hecke operators (which pertain to the finite primes, which are not particular visible in this classical description), which is resolved by moving to the adelic picture $G(\\mathbb Q)\\backslash G(\\mathbb A)$.\nAnother thing to bear in mind is that the theories of $L$-series and of automorphic forms are quite technical in nature, and so conceptual and aesthetic simplifications (as in Tate's thesis) go hand in hand with technical simplifications.  (See e.g. points (1) and (3) above.)  One instance of this in the automorphic forms context is that conjugacy classes in $G(\\mathbb Q)$ are much easier to understand than in a congruence subgroup $\\Gamma$ of $G(\\mathbb Z)$.  (Another instance of the technical superiority of fields over more general rings.)  One might also consider the Tamagawa number one theorem, which gives an elegant reformulation and generalization of a myriad of classical results.\nSo, to finish, Tate's thesis is significant because it improves the classical point of view in a number of ways, achieving conceptual, technical, and aesthetic simplifications.  At the same time, it suggests a way of unifying harmonic analytic and arithmetic considerations in the general context of automorphic forms, by working in the adelic context.\nFinally, I strongly suggest working through the details of Tate's thesis in the particular case of the Riemann zeta function, and seeing how his arguments and construction compare with the classical ones.  If you haven't already done this, it should be quite enlightening.  (In particular, it will illuminate points (1), (2), and (3) above.)", "meta": {"post_id": 25090, "input_score": 162, "output_score": 199, "post_title": "What's the significance of Tate's thesis?"}}
{"input": "My son was busily memorizing digits of $\\pi$ when he asked if any power of $\\pi$ was an integer.  I told him: $\\pi$ is transcendental, so no non-zero integer power can be an integer.\nAfter tiring of memorizing $\\pi$, he resolved to discover a new irrational whose expansion is easier to memorize.  He invented (probably re-invented) the number $J$:\n$$J = 6.12345678910111213141516171819202122\\ldots$$\nwhich clearly lets you name as many digits as you like pretty easily.  He asked me if $J$ is transcendental just like $\\pi$, and I said it must be but I didn't know for sure. Is there an easy way to determine this?  \nI can show that $\\pi$ is transcendental (using Lindemann-Weierstrass) but it doesn't work for arbitrary numbers like $J$, I don't think.", "output": "This is a transcendental number, in fact one of the best known ones, it is $6+$ Champernowne's number.\nKurt Mahler was first to show that the number is transcendental, a proof can be found on his \"Lectures on Diophantine approximations\", available through Project Euclid. The argument (as typical in this area) consists in analyzing the rate at which rational numbers can approximate the constant (see the section on \"Approximation by rational numbers: Liouville to Roth\" in the Wikipedia entry for Transcendence theory).\nAn excellent book to learn about proofs of transcendence is \"Making transcendence transparent: an intuitive approach to classical transcendental number theory\", by Edward Burger and Robert Tubbs.", "meta": {"post_id": 25205, "input_score": 58, "output_score": 47, "post_title": "Is $6.12345678910111213141516171819202122\\ldots$ transcendental?"}}
{"input": "$X \\sim \\mathcal{N}(0,1)$, then to show that for $x > 0$,\n$$\r\n\\mathbb{P}(X>x) \\leq \\frac{\\exp(-x^2/2)}{x \\sqrt{2 \\pi}} \\>.\r\n$$", "output": "Integrating by parts,\n$$\\begin{align*}\nQ(x) &= \\int_x^{\\infty} \\phi(t)\\mathrm dt = \\int_x^{\\infty} \\frac{1}{\\sqrt{2\\pi}}\\exp(-t^2/2) \\mathrm dt\\\\\n&= \\int_x^{\\infty} \\frac{1}{t} \\frac{1}{\\sqrt{2\\pi}}t\\cdot\\exp(-t^2/2) \\mathrm dt\\\\\n&= - \\frac{1}{t}\\frac{1}{\\sqrt{2\\pi}}\\exp(-t^2/2)\\biggr\\vert_x^\\infty \n- \\int_x^{\\infty} \\left( - \\frac{1}{t^2} \\right ) \\left ( - \\frac{1}{\\sqrt{2\\pi}} \\exp(-t^2/2) \\right )\\mathrm dt\\\\\n&= \\frac{\\phi(x)}{x} - \\int_x^{\\infty} \\frac{\\phi(t)}{t^2} \\mathrm dt.\n\\end{align*}\n$$\nThe integral on the last line above has a positive integrand and so \nmust have positive value.  Therefore we have that \n$$\nQ(x) < \\frac{\\phi(x)}{x} = \\frac{\\exp(-x^2/2)}{x\\sqrt{2\\pi}}~~ \\text{for}~~ x > 0.\n$$\nThis argument is more complicated than @cardinal's elegant proof of the\nsame result.  However, note that by repeating the above trick of \nintegrating by parts and the \nargument about the value of an integral with positive integrand, we get that\n$$\nQ(x) > \\phi(x) \\left (\\frac{1}{x} - \\frac{1}{x^3}\\right ) = \\frac{\\exp(-x^2/2)}{\\sqrt{2\\pi}}\\left (\\frac{1}{x} - \\frac{1}{x^3}\\right )~~ \\text{for}~~ x > 0.\n$$\nIn fact, for large values of $x$, a sequence of increasingly tighter upper and lower bounds can be developed via this argument. Unfortunately all the bounds diverge to $\\pm \\infty$ as $x \\to 0$.", "meta": {"post_id": 28751, "input_score": 44, "output_score": 41, "post_title": "Proof of upper-tail inequality for standard normal distribution"}}
{"input": "How I can compute cohomology de Rham of the projective plane $P^{2}(\\mathbb{R})$ using Mayer vietoris or any other methods?", "output": "If you remove a point from $P^2$ you are left with something which looks like a Moebius band. You can use this to compute $H^\\bullet(P^2)$.\nLet $p\\in P^2$, let $U$ be a small open neighborhood of $p$ in $P^2$ diffeomorphic to an open disc centered at $p$, and let $V=P^2\\setminus\\{p\\}$. Now use Mayer-Vietoris.\nThe cohomology of $U$ you know. The open set $V$ is diffeomorphic to an open moebious band, so that tells you the cohomology; alternatively, you can check that it deformation-retracts to the $P^1\\subseteq P^2$ consiting of all lines orthogonal to the line corresponding to $p$ (with respect to any inner product in the vector space $\\mathbb R^3$ you used to construct $P^2$), and the intersection $U\\cap V$ has also the homotopy type of a circle. The maps in the M-V long exact sequence are not hard to make explicit; it does help to keep in mind the geometric interpretation of $U$ and $V$.\nLater: alternatively, one can do a bit of magic. Since there is a covering $S^2\\to P^2$ with $2$ sheets, we know that the Euler characteristics of $S^2$ and $P^2$ are related by $\\chi(S^2)=2\\chi(P^2)$. Since $\\chi(S^2)=2$, we conclude that $\\chi(P^2)=1$. Since $P^2$ is of dimension $2$, we have $\\dim H^p(P^2)=0$ if $p>2$; since $P^2$ is non-orientable, $H^2(P^2)=0$; finally, since $P^2$ is connected, $H^0(P^2)\\cong\\mathbb R$. It follows that $1=\\chi(P^2)=\\dim H^0(P^2)-\\dim H^1(P^2)=1-\\dim H^1(P^2)$, so that $H^1(P^2)=0$. \nEven later: if one is willing to use magic, there is lot of fun one can have. For example: if a finite group $G$ acts properly discontinuously on a manifold $M$, then the cohomology of the quotient $M/G$ is the subset $H^\\bullet(M)^G$ of the cohomology $H^\\bullet(M)$ fixed by the natural action of $G$. In this case, if we set $M=S^2$, $G=\\mathbb Z_2$ acting on $M$ so that the non-identity element is the antipodal map, so that $M/G=P^2$: we get that $H^\\bullet(P^2)=H^\\bullet(S^2)^G$.\nWe have to compute the fixed spaces: \n\n$H^0(S^2)$ is one dimensional, spanned by the constant function $1$, which is obviously fixed by $G$, so $H^0(P^2)\\cong H^0(S^2)^G=H^0(S^2)=\\mathbb R$. \nOn the other hand, $H^2(S^2)\\cong\\mathbb R$, spanned by any volume form on the sphere; since the action of the non-trivial element of $G$ reverses the orientation, we see that it acts as multiplication by $-1$ on $H^2(S^2)$ and therefore $H^2(P^2)\\cong H^2(S^2)^G=0$. \nFinally, if $p\\not\\in\\{0,2\\}$, then $H^p(S^2)=0$, so that obviously $H^p(P^2)\\cong H^p(S^2)^G=0$.\n\nLuckily, this agrees with the previous two computations.", "meta": {"post_id": 29404, "input_score": 30, "output_score": 43, "post_title": "Cohomology of projective plane"}}
{"input": "I came across the following definition:\nGiven a ring $A$, with a unit $1 \\in A$, and $A$-modules $M$ and $N$, we denote by $Hom(M, N)$ or $Hom_A(M, N)$ the space of $A$-linear maps from $M$ to $N$.\nMy question is: what exactly is the difference between homomorphism and a linear map? I can see that linearity is defined in terms of a vector space or module and homomorphism in terms of groups.\nBut every linear map is a homomorphism and when treating a group as a one dimensional vector space over itself, every homo. is also a linear map. This makes me think they are kind of the same.\nIs it ok to think of it that way? Or am I confused? Because I feel confused. Thanks once again for your help!", "output": "\"Homomorphism\" comes from the greek homo (same) and morphus (form or shape).\nSo a \"homomorphism\" is a map that \"preserves the shape\" or \"preserves the structure.\"\n\nIf you are working with groups, you want $f\\colon G\\to H$ to preserve the group structure: identity, inverses, and products. So a homomorphism is a map $f$ such that $f(1)=1$, $f(a^{-1}) = (f(a))^{-1}$, and $f(ab) = f(a)f(b)$ (though it turns out that the latter is enough to guarantee all of them, so we only check the latter). \nIf you are working with rings, you want $f\\colon R\\to S$ to preserve the ring structure (addition and multiplication; if the rings have unity, then you want it to preserve unity). So you want $f(a+b) = f(a)+f(b)$, $f(ab)=f(a)f(b)$ (and if both rings have unity, you often want $f(1_R) = 1_S$).\nIf you are working with partially ordered sets, you want $f$ to preserve the order structure. So you want that if $a\\leq b$, then $f(a)\\leq f(b)$.\nIf you are working with graphs, you want the homomorphisms to preserve the graph structure, which is adjacency: if $v$ is adjacent to $w$, you want $f(v)$ to be adjacent to $f(w)$.\nIf you are working with topological spaces, you want homomorphisms to preserve the topological space structure; it turns out that the way to do this is to ask that the inverse image of an open set be open.\nIf you are working with \"pointed sets\" (sets with a distinguished object), then you want a homomorphism $f\\colon S\\to T$ to \"preserve the structure\", so you require it to map the distinguished object of $S$ to the distinguished object of $T$.\nAnd if you are working with vector spaces over a field $F$, you want a homomorphism $f\\colon V\\to W$ to \"preserve the vector space structure\"; so you want it to preserve the additive structure, $f(x+y) = f(x)+f(y)$; and the scalar multiplication structure, $f(av) = af(v)$. \nSimilarly, if you are working with $R$-modules, a homomorphism will be a map $f\\colon M\\to N$ that preserves \"the $R$-module structure\", $f(m+m') = f(m)+f(m')$ and $f(rm) = rf(m)$.\n\nSo the meaning of \"homomorphism\" will depend on the context. It is often clear. If I say \"Let $G$ and $H$ be groups, and let $f\\colon G\\to H$ be a homomorphism\", then it's pretty clear I'm talking about a group homomorphism. \nBut sometimes it isn't clear. What if I say \"Let $f\\colon\\mathbb{Z}\\to\\mathbb{R}$ be a homomorphism\"? Am I talking about a homomorphism of additive groups, or a homomorphism of rings? How about \"$f\\colon\\mathbb{R}\\to\\mathbb{C}$\"? Am I talking about additive groups, rings, topological spaces,  $\\mathbb{R}$-vector spaces, $\\mathbb{Q}$-vector spaces, inner product spaces? Which?\nSo we often specify what kind of homomorphism we mean. This is especially important when a particular set has many different structures (such as $\\mathbb{R}$, which is an additive group, a field, a vector space over $\\mathbb{Q}$, a vector space over $\\mathbb{R}$, etc). So we will say things like \"let $f\\colon M\\to N$ be an $R$-module homomorphism\", or \"let $f\\colon\\mathbb{R}\\to\\mathbb{C}$ be an additive homomorphism\" to specify which kind we are thinking about.\nAnd, historically, some terminology precedes the generic \"homomorphism.\" Homomorphisms of vector spaces have long been called \"linear transformations\", so we often call them that instead of \"vector space homomorphism\". When a vector space has several structures as a vector space (e.g., $\\mathbb{C}^2$ can be thought of as a complex vector space or as a real vector space), we often specify the field, so we may say things like \"let $f\\colon\\mathbb{C}^2\\to\\mathbb{C}$ be an $\\mathbb{R}$-linear transformation\" or just \"$\\mathbb{R}$-linear\", to specify we are looking at the structure as a real vector space.\nBecause modules are a direct generalization of vector spaces, we often say \"$R$-linear function\" or \"$R$-linear\" to refer to homomorphisms of $R$-modules, by analogy to $\\mathbb{R}$-linear or $\\mathbb{C}$-linear for homomorphisms of real or complex vector spaces. Note that a module over a field is the same thing as a vector space.", "meta": {"post_id": 29944, "input_score": 28, "output_score": 72, "post_title": "Difference between linear map and homomorphism"}}
{"input": "I know that different people follow different conventions, but whenever I see $\\arcsin(x)$ written as $\\sin^{-1}(x)$, I find myself thinking it wrong, since $\\sin^{-1}(x)$ should be $\\csc(x)$, and not possibly confused with another function.\nDoes anyone say it's bad practice to write $\\sin^{-1}(x)$ for $\\arcsin(x)$?", "output": "The notation for trigonometric functions is \"traditional\", which is to say that it is not the way we would invent notation today.\n\n$\\sin^{-1}(x)$ means the inverse sine, as you mentioned, rather than a reciprocal.  So $\\sin^{-1}(x)$ is not an abbreviation for $(\\sin(x))^{-1}$. Instead it's notation for $(\\sin^{-1})(x)$, in the same way that $f^{-1}(x)$ means the inverse function of $f$, applied to $x$. \nBut $\\sin^2(x)$ means $(\\sin(x))^2$, rather than $\\sin(\\sin(x))$. In other contexts, like dynamical systems, if I have a function $f$, the notation $f^2$ means $f \\circ f$. This is compatible with the $f^{-1}$ notation, if we take juxtaposition of functions to mean composition: $f^{-1}f^{3}$ will be $f^{2}$ as desired. \n\nSo the traditional notation for sine is actually a mixture of two different systems: $-1$ denotes an inverse, not a power, while positive integer exponents denote powers, not iterated compositions. \nThis is simply a fact of life, like an irregular conjugation of a verb. As with other languages, the things that we use most often are the ones that are likely to remain irregular. That doesn't mean that they are incorrect, however, as long as other speakers of the language know what they mean. \nMoreover, if you wanted to reform the system, there would be an equally strong argument for changing $\\sin^2$ to mean $\\sin \\circ \\sin$. This is already slowly happening with $\\log$; I think that the usage of $\\log^2(x)$ to mean $(\\log(x))^2$ is slowly decreasing, because people tend to confuse it with $\\log(\\log(x))$. That confusion is less likely with $\\sin$ because $\\sin(\\sin(x))$ arises so rarely in practice, unlike $\\log(\\log(x))$.", "meta": {"post_id": 30317, "input_score": 21, "output_score": 39, "post_title": "$\\arcsin$ written as $\\sin^{-1}(x)$"}}
{"input": "I need to find a way of proving that the square roots of a finite set \n  of different primes are linearly independent over the field of \n  rationals. \n\nI've tried to solve the problem using elementary algebra \nand also using the theory of field extensions, without success. To \nprove linear independence of two primes is easy but then my problems \narise. I would be very thankful for an answer to this question.", "output": "Assume that there was some linear dependence relation of the form\n$$ \\sum_{k=1}^n c_k \\sqrt{p_k} + c_0 = 0 $$\nwhere $ c_k \\in \\mathbb{Q} $ and the $ p_k $ are distinct prime numbers. Let $ L $ be the smallest extension of $ \\mathbb{Q} $ containing all of the $ \\sqrt{p_k} $. We argue using the field trace $ T =  T_{L/\\mathbb{Q}} $. First, note that if $ d \\in \\mathbb{N} $ is not a perfect square, we have that $ T(\\sqrt{d}) = 0 $. This is because $ L/\\mathbb{Q} $ is Galois, and $ \\sqrt{d} $ cannot be a fixed point of the action of the Galois group as it is not rational. This means that half of the Galois group maps it to its other conjugate $ -\\sqrt{d} $, and therefore the sum of all conjugates cancel out. Furthermore, note that we have $ T(q) = 0 $ iff $ q = 0 $ for rational $ q $.\nTaking traces on both sides we immediately find that $ c_0 = 0 $. Let $ 1 \\leq j \\leq n $ and multiply both sides by $ \\sqrt{p_j} $ to get\n$$ c_j p_j + \\sum_{1 \\leq k \\leq n, k\\neq j} c_k \\sqrt{p_k p_j} = 0$$\nNow, taking traces annihilates the second term entirely and we are left with $ T(c_j p_j) = 0 $, which implies $ c_j = 0 $. Since $ j $ was arbitrary, we conclude that all coefficients are zero, proving linear independence.", "meta": {"post_id": 30687, "input_score": 163, "output_score": 40, "post_title": "The square roots of different primes are linearly independent over the field of rationals"}}
{"input": "Let $f(\\bar x)$ be a multivariable polynomial with integer coefficients.\nThe zeros of that polynomial are in bijection with the homomorphisms $\\mathbb Z[\\bar x] \\rightarrow \\mathbb Z$ that factor through $\\mathbb{Z}[\\bar x]/(f)$.\nAs I understand it this viewpoint leads to the contrafunctor $\\text{Spec}$ and schemes and such.\nCan you show any concrete examples of Diophantine equations that we can solve using this viewpoint?", "output": "As far as I know, the first Diophantine problem (over a number field) that was solved using Spec and other tools of algebraic geometry was the following result (proved by Mazur and Tate in a paper from Inventiones in the early 1970s):\n\nIf $E$ is an elliptic curve over $\\mathbb Q$, then $E$ has no rational point of order 13.\n\nThe proof as it's written uses quite a bit more than you can learn just from reading Hartshorne; I don't know if there is any way to significantly simplify it.  [Added: Rereading the first page of the Mazur--Tate paper, I see that they\nrefer to another proof of this fact by Blass, which I've never read, but which\nseems likely to be of a more classical nature.]\nThere is another result, which goes back to Billing and Mahler, of the same nature:\n\nIf $E$ is an elliptic curve over $\\mathbb Q$, then $E$ has no rational point\nof order $11$.\n\nThis was proved by elementary (if somewhat complicated) arguments.  An analogous\nresult with $11$ replaced by $17$ was\nproved by Ogg again\nusing elementary arguments.\nThese results were all generalized by Mazur (in the mid 1970s) as follows:\n\nIf $E$ is an elliptic curve over $\\mathbb Q$, then $E$ has no rational point of any order other than $2,\\ldots,10$, or $12$.\n\nMazur's paper doing this (the famous Eisenstein ideal paper) was the one which\nreally established the effectiveness of Grothendieck's algebro-geometric tools for solving classical number theory problems.  For example, Wiles's work on Fermat's Last Theorem fits squarely in the tradition established by Mazur's paper.\nAs far as I know, no-one has found an elementary proof of Mazur's theorem; the\nelementary techniques of Billing--Mahler and Ogg don't seem to be extendable to the general case.  So this is an interesting Diophantine problem which seems to require modern algebraic geometry to solve.\n\nOften when a Diophantine problem is solved by algebro-geometric methods, it is not as simple as the way you suggest in your question.\nFor example, in the results described above, one does not work with one particular elliptic curve at a time.  Rather, for each $N \\geq 1$, there is a Diophantine equation, whose solutions over $\\mathbb Q$ correspond to elliptic\ncurves over $\\mathbb Q$ with a rational solution of order $N$.\nThis is the so-called modular curve $Y_1(N)$; although it was in some sense known to Jacobi, Kronecker, and the other 19th century developers of the theory of elliptic and automorphic functions, its precise interpretation as a Diophantine equation over $\\mathbb Q$ is hard to make precise without modern\ntechniques of algebraic geometry.  (As its name suggests, it is a certain moduli space.)\nAn even more important contribution of modern theory is that this Diophantine equation even has a canonical model over $\\mathbb Z$, which continues to have\na moduli-space interpretation.  (Concretely, this means that one starts with\nsome Diophantine equation --- or better, system of Diophantine equations --- over $\\mathbb Q$, and then clears the denominators in a canonical fashion,\nto get a particular system of Diophantine equations with integral coefficients\nwhose solutions have a conceptual interpretation in terms of certain data related\nto elliptic curves.)\nThe curve $Y_1(N)$ is affine, not projective, and it is more natural to study projective curves.  One can naturally complete it to a projective curve,\ncalled $X_1(N)$.  It turns out that $X_1(N)$ can have rational solutions --- some of the extra points we added in going from $Y_1(N)$ to $X_1(N)$\nmight be rational --- and so we can rephrase Mazur's theorem as saying that\nthe only rational points of $X_1(N)$ (for any $N \\neq 2,\\ldots,10,12$) lie in\nthe complement of $Y_1(N)$.\nIn fact, there are related curves $X_0(N)$, and what he proves is that $X_0(N)$ has only finitely many rational points for each $N$.  He is then able to deduce the result about $Y_1(N)$ and $X_1(N)$ by further arguments.\n\nThe reason for giving the preceding somewhat technical details is that I want\nto say something about how Mazur's proof works in the particular case $N = 11$\n(recovering the theorem of Billing and Mahler).\nThe curve $X_0(11)$ is an elliptic curve.  One can write down its\nexplicit equation easily enough; it is (the projectivization of)\n$$y^2 +y = x^3 - x^2 - 10 x - 20.$$\n(There is one point at infinity, which serves as the origin of the group law.)\nMazur wants to show it has only finitely many solutions.   It's not clear how the explicit equation will help.  (In the sense that if you begin with this equation, it's not clear how to directly show that it has only finitely many solutions over $\\mathbb Q$.)\nInstead, he first notes that it has a subgroup of rational points of order $5$:\n$$\\{\\text{ the point at infinity}, (5,5), (16,-61), (16,60), (5,-6) \\}.$$\nOne knows from the general theory of elliptic curves that the full $5$-torsion subgroup of $X_0(11)$ is of order $25$, a product of two cyclic groups of order $5$.\nWe have one of them above, while the other factor is not given by\nrational points.\nIn fact, the other $5$-torsion points have coordinates in the field $\\mathbb Z[\\zeta_5]$.  (I don't know their explicit coordinates, unfortunately.)\nMazur doesn't need to know their exact values; instead, what is important for\nhim is that he is able to show (by conceptual, not computational, arguments)\nthat the full $5$-torsion subgroup of $X_0(11)$, now thought of not just as a Diophantine over $\\mathbb Q$ but as a scheme over Spec $\\mathbb Z$,\nis a product of two group schemes of order $5$: namely\n$$\\mathbb Z/ 5\\mathbb Z \\times \\mu_5.$$\nThe first factor is the subgroup of order $5$ determined by the points with\ninteger coordinates; the second factor is a subgroup of order $5$ generated by\na $5$-torsion point with coefficients in Spec $\\mathbb Z[\\zeta_5]$.\nWhat does it mean that this second factor is $\\mu_5$?\nWell, $X^5 - 1$ is a Diophantine equation, whose solutions are defined over\n$\\mathbb Z[\\zeta_5]$, and  have a natural (multiplicative) group structure, and this is what $\\mu_5$ is.\nWhat Mazur says is that an isomorphic copy of this \"Diophantine group\" (more precisely, this group scheme) lives inside $X_0(11)$.\nNote that the classical theory of Diophantine equations is not very well set up\nto deal with concepts like \"isomorphisms of Diophantine equations whose solutions admits a natural group structure\".  (One already sees this if one tries to develop the theory of elliptic curves, including the group structure, in an elementary way.)   So this is already a place where scheme theory provides new and important expressive power.\nIn any event, once Mazur has this formula for the $5$-torsion, he can make an infinite descent to prove that there are no other rational points besides the $5$ that we already wrote down.   He doesn't phrase this infinite descent in\nthe naive way, with equations, as Fermat did with his descents (although it\nis the same underlying idea): rather, he argues as follows:\nThe curve $X_0(11)$ stays non-singular modulo every prime except $11$ (as you can check directly from the above equation).  Modulo $11$ it becomes singular:\nyou can check directly that reduced modulo $11$, the above equation becomes\n$$(y-5)^2 = (x-2)(x-5)^2,$$\nwhich has a singular point (a node) at $(5,5)$.\nNote now that all our rational solutions $(5,5), (16,-61),$ etc. (other than\nthe point at infinity) reduce\nto the node when you reduce them modulo $11$.\nUsing this (plus a little more argument) what you can show is that if\n$(x,y)$ is any rational point of $X_0(11)$, then after subtracting off\n(in the group law) a suitable choice of one of our $5$ known points, you obtain\na point which does not reduce to the node upon reduction modulo $11$.\nSo what we have to show is that if $(x,y)$ is any rational solution on $X_0(11)$\nwhich does not map to the node mod $11$, it is trivial (i.e. the point at\ninfinity).\nSuppose it is not: then Mazur considers a point $(x',y')$ (no longer necessarily rational,\njust defined over some number field) which maps to $(x,y)$ under multiplication\nby $5$ (in the group law).  (This is the descent argument.)\nNow this point is not uniquely determined, but it is determined up to\naddition (in the group law) of a $5$-torsion point.  Because we know the precise\nstructure of the $5$-torsion (even over Spec $\\mathbb Z$) we see that this\npoint would have to have coordinates in some compositum of fields of the following type: (a) an everywhere unramified cyclic degree $5$ extension of $\\mathbb Q$ (this relates to the $\\mathbb Z/5\\mathbb Z$ factor); and (b) an everywhere\nunramifed extension of $\\mathbb Q$ obtained by extracting the $5$th root of\nsome number (this relates to the $\\mu_5$ factor). Now no such extension of $\\mathbb Q$ exist (e.g. because\n$\\mathbb Q$ admits no non-trivial everywhere unramified extension), and hence\n$(x',y')$ again has to be defined over $\\mathbb Q$.  Now we repeat the above\nprocedure ad infitum, to get a contradiction (via infinite descent).\n\nI hope that the above sketch gives some idea of how more sophisticated methods\ncan help with the solution of Diophantine equations.  It is not just that one writes down Spec and magically gets new information.  Rather, the introduction of a more conceptual way of thinking gives whole new ways of transferring information around and making computations which are not accessible when working in a naive manner.\nA good high-level comparison would be the theory of solutions of algebraic equations before and after Galois's contributions.\nA more specific analogy would be the difference between studying surfaces in space (say) with the tools of an undergraduate multi-variable calculus class,\ncompared to the tools of manifold theory.  In undergraduate calculus, one has to\nat all times remember the equation for the surface, work with explicit coordinates, make explicit coordinate changes to reduce computations from the curved surface to the plane, and so on.  In manifold theory, one has a conceptual apparatus which lets one speak of the surface as an object independent of the equation cutting it out; one can say \"consider a chart\nin the neighbourhood of the point $p$\" without having to explicitly write\ndown the functions giving rise to the chart.  (The implicit function theorem\nsupplies them, and that is often enough; you don't have to concretely determine the output\nof that theorem every time you want to apply it.)\nSo it goes with the scheme-theoretic point of view.  One can use the modular\ninterpretation to write down points of $X_0(11)$ without having to give their\ncoordinates.  In fact, one can show that it has a node when reduced modulo $11$\nwithout ever having to write down an equation.  The determination of the $5$-torsion group is again made by conceptual arguments, without having to write down the actual solutions in coordinates.  And as the above sketch of the infinite descent (hopefully) makes clear, it is any case the abstract nature\nof the $5$-torsion points (the fact that they are isomorphic to\n$\\mathbb Z/5\\mathbb Z \\times \\mu_5$) which is important for the descent, not any information about their explicit coordinates.\nI hope this answer, as long and technical as it is, gives some hint as to the utility of the scheme-theoretic viewpoint.\n\nReferences: A nice introduction to $X_0(11)$ is given in this expository article of Tom Weston.\nAs for Mazur's theorem, I don't know of any expositions which are not at a\nmuch higher level of sophistication.  (There are simpler proofs of his main technical results now, e.g. here,\nbut these are simpler only in a relative sense; they are still not accessible\nto non-experts in this style of number theory.)", "meta": {"post_id": 30866, "input_score": 39, "output_score": 71, "post_title": "Diophantine applications of Spec?"}}
{"input": "This is a simple question I came across in reviewing.  I am wondering if I got the correct answer.  \nThe question is simple.  You have $n$ balls and $m$ bins.  Each ball has an equal probability of landing in any bin.  I want to know what the probability that exactly $1$ bin is empty.\nMy answer seems simple enough, but I don't think it's sufficient.  It is $(\\frac{m-1}{m})^n$ since for each ball, it can go in any of the other bins.  I think, however, that this is just the probability that some arbitrary bin $A$ is empty, not exactly one bin.  What else should I consider?", "output": "Let's count configurations, and then divide by $m^n$. \nThere are $m$ choices for the empty bin. Then the other bins are occupied. We can count the ways to place $n$ balls in $m-1$ bins so that no bin is empty by inclusion-exclusion: It is \n$$\\sum_{k ~\\text {bins known to be empty}} (-1)^k {m-1 \\choose k} (m-1-k)^n.$$\nAnother way to get this is to label the parts of a set partition of size $n$ with $m-1$ parts. The number of set partitions with a given number of parts is a Stirling number of the second kind, and we want $(m-1)! S(n,m-1)$.\nMultiply this by $m$ and then divide by $m^n$ to get the probability exactly $1$ bin is empty. \nWe can use the same techniques to compute the probability exactly $e$ bins are empty for other values of $e$. For example, suppose there are $4$ bins and $6$ balls. Then there are $1560$ ways for there to be $0$ empty bins, $2160$ ways for there to be exactly $1$ empty bin, $372$ ways for there to be exactly $2$ empty bins, and $4$ ways for there to be exactly $3$ empty bins. The total is $4096 = 4^6$. Dividing by this gives a probability of $\\frac{135}{256} = 0.52734375$ that exactly $1$ bin is empty.", "meta": {"post_id": 32444, "input_score": 5, "output_score": 35, "post_title": "Simple probability question, balls and bins"}}
{"input": "Given a matrix, is the Frobenius norm of that matrix always equal to the 2-norm of it, or are there certain matrices where these two norm methods would produce different results?\nIf they are identical, then I suppose the only difference between them is the method of calculation, eh?", "output": "The 2-norm (spectral norm) of a matrix is the greatest distortion of the unit circle/sphere/hyper-sphere. It corresponds to the largest singular value (or |eigenvalue| if the matrix is symmetric/hermitian). \nThe Forbenius norm is the \"diagonal\" between all the singular values.\ni.e. $$||A||_2 = s_1 \\;\\;,\\;\\;||A||_F = \\sqrt{s_1^2 +s_2^2 + ... + s_r^2}$$ \n(r being the rank of A).  \nHere's a 2D version of it: $x$ is any vector on the unit circle. $Ax$ is the deformation of all those vectors. The length of the red line is the 2-norm (biggest singular value). And the length of the green line is the Forbenius norm (diagonal).", "meta": {"post_id": 33083, "input_score": 77, "output_score": 41, "post_title": "What is the difference between the Frobenius norm and the 2-norm of a matrix?"}}
{"input": "There is a huge debate on the internet on the value of $48\\div2(9+3)$.\nI believe the answer $2$ as I believe it is part of the bracket operation in BEDMAS. Mathway yields the same answer. I also believe that if $48\\div2\\times(9+3)$ was asked it would be $288$ which Mathway agrees with as well.\nHowever, WolframAlpha says it is $288$ either way.\nA friend of mine (who is better at math) told me that there is no such thing as 'implicit multiplication', only shorthand so that is in fact done after the division (going left to right, not necessarily because division occurs before multiplication. But he didn't explicitly give a reason)\nWhat is the answer and why?", "output": "There is no Supreme Court for mathematical notation; there were no commandments handed down on Sinai concerning operational precedence; all there is, is convention, and different people are free to adhere to different conventions. Wise people will stick in enough parentheses to make it impossible for anyone to mistake the meaning. If they mean, $(48\\div2)(9+3)$, they'll write it that way; if they mean $48\\div\\bigl(2(9+3)\\bigr)$, they'll write it that way.", "meta": {"post_id": 33215, "input_score": 46, "output_score": 179, "post_title": "What is $48\\div2(9+3)$?"}}
{"input": "From a bank of previous masters exams:\n\nLet $G$ be a finite group such that its automorphism group $\\operatorname{Aut}(G)$ is cyclic. Prove that $G$ is abelian.\n\nHere's what I was thinking. Let $\\phi:G \\to G$ be the generator of $\\operatorname{Aut}(G)$, with order $n$. Assume that $G$ is not abelian. Then there is a nontrivial inner automorphism $\\psi_g(x) = g^{-1}xg$. Since $\\phi$ generates all automorphisms, then $\\psi_g = \\phi^k$ for some $k$. This also implies that $\\psi_g^n(x) = g^{-n}xg^n$ is the identity map.\nAfter that, no luck. Any ideas?", "output": "If the automorphism group is cyclic, then the Inner automorphism group is cyclic. But the inner automorphism group is isomorphic to $G/Z(G)$, and if $G/Z(G)$ is cyclic, then it is trivial. Therefore, $G=Z(G)$ so $G$ is abelian. (The argument does not require $G$ to be finite, by the by.)\nFor the latter:\nProp. If $H\\subseteq Z(G)$ and $G/H$ is cyclic, then $G$ is abelian.\nSuppose $G/H$ is cyclic, with $H\\subseteq Z(G)$. Let $g\\in G$ be such that $gH$ generates $G/H$. Then every $x\\in G$ can be written as $x=g^kh$ for some integer $k$ and some $h\\in H$. Given $x,y\\in G$, we have $x=g^kh$ and $y=g^{\\ell}h'$, so\n$$\\begin{align*}\nxy &= (g^kh)(g^{\\ell}h')\\\\\n   &= g^kg^{\\ell}hh' &\\quad&\\text{since }h\\in Z(G)\\\\\n   &= g^{\\ell}g^kh'h\\\\\n   &= g^{\\ell}h'g^kh &&\\text{since }h'\\in Z(G)\\\\\n   &= (g^{\\ell}h')(g^kh)\\\\ \n   &= yx,\n\\end{align*}$$\nhence $G$ is abelian. QED\nFor more on what groups can occur as central quotients, see this previous question.\nAdded. Since you mention you did not know that $\\mathrm{Inn}(G)\\cong G/Z(G)$, let's do that too:\nDefine a map $G\\to \\mathrm{Aut}(G)$ by mapping $g\\mapsto \\varphi_g$, where $\\varphi_g$ is \"conjugation by $g$\". That is, for all $x\\in G$,\n$\\varphi_g(x) = gxg^{-1}$. \nThis map is a group homomorphism: if $g,h\\in G$, then we want to show that $\\varphi_{gh} = \\varphi_g\\circ\\varphi_h$. To that end, let $x\\in G$ be any element, and we show that $\\varphi_{gh}(x) = \\varphi_g(\\varphi_h(x))$.\n$$\\varphi_{gh}(x) = (gh)x(gh)^{-1} = ghxh^{-1}g^{-1}= g(hxh^{-1})g^{-1} = \\varphi_g(hxh^{-1}) = \\varphi_g(\\varphi_h(x)).$$\nTherefore, the map $g\\mapsto\\varphi_g$ is a homomorphism from $G$ onto $\\mathrm{Inn}(G)$. By the Isomorphism Theorem, $\\mathrm{Inn}(G)$ is isomorphic to $G/N$, where $N$ is the kernel of this homomorphism.\nWhat is $N$? $g\\in N$ if and only if $\\varphi_g$ is the identity element of $\\mathrm{Aut}(G)$, which is the identity; that is, if and only if $\\varphi_g(x)=x$ for all $x\\in G$. But $\\varphi_g(x)=x$ if and only if $gxg^{-1}=x$, if and only if $gx = xg$. So $\\varphi_g(x) = x$ if and only if $g$ commutes with $x$. Thus, $\\varphi_g(x)=x$ for all $x$ if and only if $g$ commutes with all $x$, if and only if $g\\in Z(G)$. Thus, $N=Z(G)$, so $\\mathrm{Inn}(G)\\cong G/Z(G)$, as claimed.", "meta": {"post_id": 33254, "input_score": 19, "output_score": 35, "post_title": "Showing that a cyclic automorphism group makes a finite group abelian"}}
{"input": "I'm a bachelor student of Math major. The question is about the better way to select and read maths text- or non-text- books, without considering the course and exam. (Because we follow the lecture notes during the course, and the exam is totally covered in lecture notes, so it's not an issue, but just about how to really learn)\nQuestion: \n(1) Reading the book one by one, or just focus and re-read lots of times on one certain book, and then continue with little others ? Because recently I've tried the first way, for example, to learn Calculus, I took one book, the read it from first page to the last and finish some exercises, and then immediately read another calculus book, doing the same thing, and then the third one, and so forth. After this, I realize it is really bad result, it seems that I don't master the calculus pretty well, just memorize lots of things. So, I'm thinking about whether the second way is better. Because some friends told me before, said that there're lots of good maths textbooks, I should read as much as possible. \n(2): If the second way is better, could I just use the UTM and GTM series(published by Springer, Undergraduate Texts in Mathematics) for all the maths courses ? Because there're too many, tons of books in the market, I'm not sure how to select the perfect books. But lots of people said that the UTM & GTM are two perfect series in Maths. And my teacher told me that it is better to spend more time on the books written by the famous mathematicians who are the masters in their areas. Less time on the so-called popular textbooks, like thousand-page widely-used textbooks. He's not saying that the latter one is bad, it just means that the latter one is too specified for the general course in university, passing exam , or something like that. Although it's organized well, ton's of exercises, examples and so on,  the former one is better to grab the deep insight of the field and focus more on thinking, not just knowledge. So, briefly, is it good to just use to the whole UTM series for every course in math bachelor program?", "output": "In my opinion it's much better to pick some book and read it in depth, solving many of the exercises while reading. If you read a 100 books without actually concentrating on what you're doing, it will be of no help. Of course, reading a book thoroughly and attentively is slow, so don't expect it to be quick.\nYou seem to be somewhat biased by your experience so far. Apart from the first few courses, there are no expensive, shiny, popular textbooks - only the dense, terse, substantial ones. If you really want to understand math (rather than to be just able to apply it), then the shiny books don't help you at all.\nFinally, I have no idea whether UTM/GTM books are \"enough\", but if for some reason they're all that you have access to, you're probably fine. But why limit yourself? It might happen that in a particular subject, none of the \"standard\" books are UTM/GTM, e.g. Kunen's Set Theory is Elsevier and Jech's is Springer but neither UTM/GTM. Instead of committing to UTM/GTM, just pick whatever friends or professors recommend and is actually available.", "meta": {"post_id": 35105, "input_score": 47, "output_score": 35, "post_title": "Math Major: How to read textbooks in better style or method ? And how to select best books?"}}
{"input": "Let $f:R^n \\rightarrow R^m$ be any function. Will the graph of $f$ always have Lebesgue measure zero?\n$(1)$ I could prove that this is true if $f$ is continuous.\n$(2)$ I suspect it is true if $f$ is measurable, but I'm not sure. (My idea was to use Fubini's Theorem to integrate the indicator function of the graph, but I don't know if I'm using the Theorem properly).\nIf $(2)$ is incorrect, what would be a counterexample where the graph of $f$ has positive measure?\nIf $(2)$ is correct, can we prove the existence of a non-measurable function whose graph has positive outer measure?", "output": "No function can have a graph with  positive measure or even positive inner measure, since every function graph has uncountably many disjoint vertical translations, which cover the plane. \nMeanwhile, using the axiom of choice, there is a function whose graph has positive outer measure. The construction is easiest to see if one assumes that the Continuum Hypothesis is true, so let me assume that.\nTo begin, note first that there are only continuum many open sets in\nthe plane, since every such set is determined by a\ncountable union of basic open balls with rational center\nand rational radius. Next, it follows that the number of\n$G_\\delta$ sets is also continuum, since any such set is\ndetermined by a countable sequence of open sets, and\n$(2^{\\aleph_0})^{\\aleph_0}=2^{\\aleph_0}$.\nThus, we may enumerate the $G_\\delta$ sets in the plane as\n$A_\\alpha$ for $\\alpha\\lt \\aleph_1$ (using CH). Build a\nfunction $f:\\mathbb{R}\\to\\mathbb{R}$ by transfinite\ninduction. At any stage $\\alpha\\lt \\aleph_1$, we have\nthe approximation $f_\\alpha$ to $f$, and we assume that it\nhas been defined on only $\\alpha$ many points. Given\n$f_\\alpha$, consider the $G_\\delta$ set $A_\\alpha$. If we\ncan extend $f_\\alpha$ to a function $f_{\\alpha+1}$ by\ndefining it on one more point $x$, so that\n$(x,f_{\\alpha+1}(x))$ is outside $A_\\alpha$, then do so.\nOtherwise, $A_\\alpha$ contains the complement of\ncountably many vertical lines in the plane, and thus has\nfull measure.\nAfter this construction, extend the resulting function if\nnecessary to a total function $f:\\mathbb{R}\\to\\mathbb{R}$.\nIt now follows that the graph of $f$ is not contained in\nany $G_\\delta$ set with less than full measure. Thus, the\ngraph has full outer measure.\nNow, finally, the same construction works without CH, once you realize that any $G_\\delta$ set containing the complement of fewer than continuum many vertical lines has full measure.", "meta": {"post_id": 35606, "input_score": 42, "output_score": 51, "post_title": "Lebesgue Measure of the Graph of a Function"}}
{"input": "While going through Probability: Theory and Examples by Rick Durrett (4th edition, p.9), I came across the familiar definition of $\\sigma$-algebras where, if $A_i \\in \\mathcal{F}$ is a countable sequence of sets for some $\\sigma$-algebra $\\mathcal{F}$ and $\\cup_i A_i \\in \\mathcal{F}$ by definition, then it follows that $\\cap_i A_i^C \\in \\mathcal{F}$ by de Morgan's law.\nThat's when it occurred to me that I had never seen a proof that de Morgan's law holds over a countably infinite number of sets. I don't have my measure theory/probably theory books with me right now, but I'm quite sure that I've never seen any of them prove this before extending $\\sigma$-algebras to countable union or intersection, depending on which definition it started with.\nOn the one hand, it seems obvious that it would hold. On the other hand, seeming obvious is not a proof, especially when it comes to something involving infinity.\nI can imagine an inductive proof where I\n\nassume de Morgan's law holds for an index set of size $n$\nThen prove that it holds for an index set of size $n+1$\n\nand wrap it up by $n \\rightarrow \\infty$ but I'm not convinced that's right. For example, an argument like that doesn't work for countable intersection being closed on a collection of open sets.\nSo what's a good proof that can extend de Morgan's law to an infinite collection of sets.", "output": "The result holds for every family, countable or not, of sets $A(i)$ and it is a simple matter of logic.\nTo wit, the assertion \"$x$ belongs to the union\" means \"There exists $i$ such that $x$ belongs to $A(i)$\" hence its negation \"$x$ belongs to the complement of the union\" is also \"For all $i$, $x$ does not belong to $A(i)$\", that is, \"For all $i$, $x$ belongs to the complement of $A(i)$\". We are done.", "meta": {"post_id": 37045, "input_score": 43, "output_score": 51, "post_title": "De Morgan's law on infinite unions and intersections"}}
{"input": "Here it is :\n$$\r\n\\frac{\\mathrm d}{\\mathrm dx}\\left( \\int_{\\cos x}^{\\sin x}{\\sin \\left( t^3 \\right)\\mathrm dt} \\right)\r\n$$\nI've got the answer but I don't know how to start , what to do ?\nHere is the answer : \n$\r\n\\sin \\left( \\sin^3 x \\right)\\cos x + \\sin \\left( \\cos ^{3}x \\right)\\sin x\r\n$\nSo first I calculate the primitive and then I derivate it. But I don't know how to integrate. Should I use 'substitution' method ? I tried but then i was blocked...", "output": "I understand from the comments that you are not completely pleased with the answers so far. That's why I try it (with a bit delay). Note that there is nothing new in this answer ...\nAll you need to know is the fundamental theorem of calculus\n$$f(x) = \\frac{d}{dx} F(x)$$\nwith\n$$F(x) = \\int^x_a f(t) dt$$\nand the chain rule\n$$\\frac{d}{dx} f[g(x)] = f'[g(x)] g'(x).$$\nYour integral is given by\n$$ \\int_{\\cos x}^{\\sin x}{\\sin ( t^3) \\,dt} =F(\\sin x) - F(\\cos x)$$\nwith $$F(x) = \\int_a^x f(t) dt$$\nand $f(t)=\\sin(t^3)$.\nTherefore,\n$$ \\frac{d}{dx}\\left[ \\int_{\\cos x}^{\\sin x}{\\sin ( t^3 ) dt} \\right]\n = \\frac{d}{dx} [F(\\sin x) - F(\\cos x)]\n = F'(\\sin x) \\sin' x - F'(\\cos x) \\cos' x$$\n$$ = f(\\sin x) \\cos x + f(\\cos x) \\sin x = \\sin ( \\sin^3 x) \\cos x + \\sin (\\cos^3 x) \\sin x.$$", "meta": {"post_id": 37656, "input_score": 23, "output_score": 38, "post_title": "How to calculate the derivative of this integral?"}}
{"input": "What does it mean to talk about the \"irreducible representatives of SO(3)\"?  I'm struggling to understand the concept of irreducible representations.  Could someone give a concrete example for someone unfamiliar with group theory or representation theory?", "output": "A representation of the group $G$ means a homomorphism from $G$ into the group of automorphisms of a vector space $\\mathbf{V}$. Essentially, you are trying to interpret each element of $G$ as an invertible linear transformation $\\mathbf{V}\\to\\mathbf{V}$, in order to try to understand the group $G$ by how it \"acts on $\\mathbf{V}$.\" \nIf you have an action $\\rho_1$ of $G$ on a vector space $\\mathbf{W}$ (that is, one representation), and you have some other action $\\rho_2$ of $G$ on another vector space $\\mathbf{Z}$ (another representation), then you can use these two actions to construct an action of $G$ on the vector space $\\mathbf{W}\\oplus\\mathbf{Z}$: just let $G$ act on the first coordinate using the old action on $\\mathbf{W}$, and let it act on the second coordinate using the old action on $\\mathbf{Z}$. \nThe point to observe, however, is that the action of $G$ on $\\mathbf{W}\\oplus \\mathbf{Z}$ defined this way does not give you any new insights into the structure of $G$: anything you can glean about $G$ from this action, you can learn about $G$ by considering the original actions $\\rho_1$ and $\\rho_2$. So this new action does not give us anything new.\nConversely, suppose you have one representation $\\rho$, with $G$ acting on $\\mathbf{V}$, and that there are proper subspaces $\\mathbf{W}$ and $\\mathbf{Z}$ of $\\mathbf{V}$ that satisfy the following properties:\n\n$\\mathbf{V}=\\mathbf{W}\\oplus\\mathbf{Z}$; and\nThe action of every $g\\in G$ on $\\mathbf{V}$ maps $\\mathbf{W}$ to itself; and\nThe action of every $g\\in G$ on $\\mathbf{V}$ maps $\\mathbf{Z}$ to itself.\n\nThen you can look at the restriction of the action of $G$ on $\\mathbf{W}$ to get a representation, and the restriction on $\\mathbf{Z}$ to get another representation; and these two representations will give you all the information from the original representation, the same way we had before. The advantage being that since $\\mathbf{W}$ and $\\mathbf{Z}$ are proper subspaces of $\\mathbf{V}$, they have smaller dimension and, presumably, it's easier to understand a subgroup of linear automorphisms for them than for $\\mathbf{V}$.\nSo the moral is that we want to find representations that cannot be \"broken up\" into smaller ones, because there's no point in trying to understand ones that do break up, we can focus our attention on those that don't, because all the other representations can be built up in terms of the ones that cannot be broken up.\nThe irreducible representations are precisely the ones that cannot be broken up into smaller pieces (at least for finite groups). Maschke's Theorem says that if you have a representation $\\rho$ of a finite group $G$ acting on $\\mathbf{V}$, and $\\mathbf{W}$ is a subspace of $\\mathbf{V}$ such that for all $g\\in G$, the image of $\\mathbf{W}$ under the action of $g$ is $\\mathbf{W}$ itself, then you can find a subspace $\\mathbf{Z}$ of $\\mathbf{V}$ such that $\\mathbf{V}=\\mathbf{W}\\oplus\\mathbf{Z}$ and every $g\\in G$ maps $\\mathbf{Z}$ to itself (that is, in order to break up $\\rho$ into two smaller pieces, it is enough to find a single proper piece on which $\\rho$ acts; then you can find a complement for it). \nWith this in mind, we say:\n\nLet $\\rho\\colon G\\to \\mathrm{Aut}(\\mathbf{V})$ be a representation of $G$. We say that $\\rho$ is irreducible if and only if $\\mathbf{V}$ is not the zero vector space, and the only subspaces of $\\mathbf{V}$ that are mapped to themselves under the action of every $g\\in G$ are $\\{\\mathbf{0}\\}$ and $\\mathbf{V}$ itself.\n\nAn irreducible representation of $SO(3)$ will be a representation of $SO(3)$ that is irreducible. $SO(3)$ acts naturally on the vector space $\\mathbb{R}^3$: it consists of all automorphisms of $\\mathbb{R}^3$ that respect the inner product, so this is itself a representation of $SO(3)$ (which is irreducible, because no proper subspace of $\\mathbb{R}^3$ is sent to itself by all elements of $SO(3)$). \n(Maschke's Theorem holds if the vector space is over a field of characteristic $0$, or if the characteristic does not divide the order of the group; there are similar theorems for certain kinds of infinite groups, but it does not hold for arbitrary infinite groups in general.)", "meta": {"post_id": 38958, "input_score": 37, "output_score": 85, "post_title": "What is the meaning of an \"irreducible representation\"?"}}
{"input": "Let's say I have one point that will be taken randomly from a normal distribution with mean $\\mu_1$ and standard deviation $\\sigma_1$.  Let's say I have another point that is taken much in the same way from another normal distribution with mean $\\mu_2$ and standard deviation $\\sigma_2$;.\nHow can I compute the probability, given $\\mu_1$, $\\mu_2$, $\\sigma_1$, and $\\sigma_2$, that my first point will be larger than the second?\nI am sort of interested in the reasoning behind an \"analytic\" answer (or as analytic as you can possibly get with the normal distribution, which isn't that much), but I am more importantly looking for an algorithm of computing this probability, as it will be used in a simulation/model.\nDoes anyone know where I could get started on reasoning through this?\nNote: For actual computation, having a table of values of the % of the curve within a given multiple of the standard deviation is feasible in my situation.", "output": "Suppose that $X_1 \\sim {\\rm N}(\\mu_1,\\sigma_1^2)$ and $X_2 \\sim {\\rm N}(\\mu_2,\\sigma_2^2)$ are independent. Then,\n$$\r\n{\\rm P}(X_1  > X_2 ) = {\\rm P}(X_1  - X_2  > 0) = 1 - {\\rm P}(X_1  - X_2  \\le 0).\r\n$$\nNow, by independence, $X_1 - X_2$ is normally distributed with mean\n$$\r\n\\mu := {\\rm E}(X_1 - X_2) = \\mu_1 - \\mu_2\r\n$$\nand variance\n$$\r\n\\sigma^2 := {\\rm Var}(X_1 - X_2) = \\sigma_1^2 + \\sigma_2^2.\r\n$$\nHence, \n$$\r\n\\frac{{X_1  - X_2  - \\mu}}{{\\sigma}} \\sim {\\rm N}(0,1),\r\n$$\nand so\n$$\r\n{\\rm P}(X_1  - X_2  \\le 0) = {\\rm P}\\bigg(\\frac{{X_1  - X_2  - \\mu }}{\\sigma } \\le \\frac{{0 - \\mu }}{\\sigma }\\bigg) = \\Phi \\Big(  \\frac{-\\mu }{\\sigma }\\Big),\r\n$$\nwhere $\\Phi$ is the distribution function of the ${\\rm N}(0,1)$ distribution. Thus,\n$$\r\n{\\rm P}(X_1  > X_2 )  = 1 - {\\rm P}(X_1  - X_2  \\le 0) = 1 - \\Phi \\Big(  \\frac{-\\mu }{\\sigma }\\Big).\r\n$$", "meta": {"post_id": 40224, "input_score": 32, "output_score": 42, "post_title": "Probability of a point taken from a certain normal distribution will be greater than a point taken from another?"}}
{"input": "Suppose $F$ is a field s.t $\\left|F\\right|=q$. Take $p$ to be some prime. How many monic irreducible polynomials of degree $p$ do exist over $F$?\n\nThanks!", "output": "The number of such polynomials is exactly $\\displaystyle \\frac{q^{p}-q}{p}$ and this is the proof:\nThe two main facts which we use (and which I will not prove here) are that $\\mathbb{F}_{q^{p}}$ is the splitting field of the polynomial $g\\left(x\\right)=x^{q^{p}}-x$, and that every monic irreducible polynomial of degree $p$ divides $g$.\nNow: $\\left|\\mathbb{F}_{q^{p}}:\\mathbb{F}_{q}\\right|=p$ and therefore there could be no sub-extensions. Therefore, every irreducible polynomial that divides $g$ must be of degree $p$ or 1. \nSince each linear polynomial over $\\mathbb{F}_{q}$ divides $g$ (since for each $a\\in \\mathbb{F}_{q}$, $g(a)=0$), and from the fact that $g$ has distinct roots, we have exactly $q$ different linear polynomials that divide $g$. \nMultiplying all the irreducible monic polynomials that divide $g$ will give us $g$, and therefore summing up their degrees will give us $q^{p}$.\nSo, if we denote the number of monic irreducible polynomials of degree $p$ by $k$  (which is the number we want), we get that $kp+q=q^{p}$, i.e $\\displaystyle k=\\frac{q^{p}-q}{p}$.", "meta": {"post_id": 40811, "input_score": 65, "output_score": 76, "post_title": "Number of monic irreducible polynomials of prime degree $p$ over finite fields"}}
{"input": "I'm working on an exercise from Atiyah and MacDonald's Commutative Algebra, and have hit a bump on Exercise 14 of Chapter 1. \n\nIn a ring $A$, let $\\Sigma$ be the set of all ideals in which every element is a zero-divisor. Show that set $\\Sigma$ has maximal elements and that every maximal element of $\\Sigma$ is a prime ideal. Hence the set of zero-divisors in $A$ is a union of prime ideals.\n\nI see by an application of Zorn's Lemma that $\\Sigma$ has maximal elements. I take $\\mathfrak{m}$ to be maximal in $\\Sigma$, with $xy\\in\\mathfrak{m}$. Since $xy$ is a zero divisor, $xyz=0$ for some $z\\neq 0$. If $yz=0$, then $y$ is a zero divisor, otherwise $x$ is a zero divisor. So I guess I then want to show $x\\in\\Sigma$ or $y\\in\\Sigma$. If neither is, then $\\mathfrak{m}$ is properly contained in both $(\\mathfrak{m},x)$ and $(\\mathfrak{m},y)$. However, I'm not sure how to show either of these ideals is again in $\\Sigma$. \nIf my understanding is correct, elements of $(\\mathfrak{m},x)$ are finite sums of the form $\\sum_ia_im_i+bx$ for $a_i\\in A$, $m_i\\in\\mathfrak{m}$ and $b\\in A$. To show this sum is a zero divisor, my hunch is that if $c_im_i=0$ for $c_i\\neq 0$ and $dx=0$ for $d\\neq 0$, then \n$$\r\n\\left(\\sum_ia_im_i+bx\\right)(d\\prod_ic_i)=0.\r\n$$\nMy concern is that perhaps $d\\prod_ic_i=0$, so the above wouldn't show that $(\\mathfrak{m},x)$ consists of only zero divisors. How can I get around this? Or is there perhaps a better approach? Thank you for your help.", "output": "Let $\\frak{m}$ be a maximal element in $\\Sigma$. We want to show it is prime, i.e. that if $x\\notin\\frak{m}$ and $y\\notin\\frak{m}$, then $xy\\notin\\frak{m}$. \nIf $x\\notin\\frak{m}$ and $y\\notin\\frak{m}$, then ${\\frak{m}}+(x)$ and ${\\frak{m}}+(y)$ are both ideals of $A$ that strictly contain $\\frak{m}$, and therefore each must contain non-zero-divisors ($\\frak{m}$ is maximal among ideals consisting only of zero-divisors, so any ideal strictly containing $\\frak{m}$ cannot consist only of zero-divisors). Thus the ideal $({\\frak{m}}+(x))({\\frak{m}}+(y))\\subseteq{\\frak{m}}+(xy)$ contains non-zero-divisors (because there is at least one non-zero-divisor in each of ${\\frak{m}}+(x)$ and ${\\frak{m}}+(y)$, and the product of two non-zero-divisors is a non-zero-divisor). But the fact that ${\\frak{m}}+(xy)$ contains non-zero-divisors implies that ${\\frak{m}}+(xy)$ strictly contains $\\frak{m}$, hence $xy\\notin\\frak{m}$. Thus $\\frak{m}$ is prime.", "meta": {"post_id": 44481, "input_score": 45, "output_score": 47, "post_title": "Showing the set of zero-divisors is a union of prime ideals"}}
{"input": "I need your help with evaluating this limit:\n$$ \\lim_{n \\to \\infty }\\underbrace{\\sin \\sin \\dots\\sin}_{\\text{$n$ compositions}}\\,n,$$\ni.e. we apply the $\\sin$ function $n$ times.\nThank you.", "output": "The first sine is in $I_1=[-1,1]$ hence the $n$th term of the sequence is in the interval $I_n$ defined recursively by $I_1=[-1,1]$ and $I_{n+1}=\\sin(I_n)$. One sees that $I_n=[-x_n,x_n]$ where $x_1=1$ and $x_{n+1}=\\sin(x_n)$. The sine function is such that $0\\le\\sin(x)\\le x$ for every nonnegative $x$ hence $(x_n)$ is nonincreasing and bounded below by zero hence it converges to a limit $\\ell$. The sine function is continuous hence $\\ell=\\sin(\\ell)$. The only fixed point of the sine function is zero hence $\\ell=0$. This proves that $x_n\\to0$, that the sequence $(I_n)$ is nonincreasing and that its intersection is reduced to the point zero and finally, that the sequence considered in the post converges to zero.\nEdit: The argument above shows that for every sequence $(z_n)$, the sequence $(s_n)$ defined by $s_n=\\sin\\sin\\cdots\\sin(z_n)$ (the sine function being iterated $n$ times to define $s_n$) converges to zero. In other words, there is nothing particular about the choice $z_n=n$.", "meta": {"post_id": 45283, "input_score": 81, "output_score": 164, "post_title": "Compute $ \\lim\\limits_{n \\to \\infty }\\sin \\sin \\dots\\sin n$"}}
{"input": "A function by definition is a set of ordered pairs, and also according the Kuratowski, an ordered pair $(x,y)$ is defined to be $$\\{\\{x\\}, \\{x,y\\}\\}.$$\nGiven $A\\neq \\varnothing$, and $\\varnothing\\colon \\varnothing \\rightarrow A$. I know $\\varnothing \\subseteq \\varnothing \\times A$, but still an empty set is not an ordered pair. How do you explain that an empty function is a function?", "output": "I believe that the answer lies hidden within the depth of \"vacuously true argument\".\nAn argument of the form $\\forall x\\varphi$ is true if and only if there is no $x$ such that $\\lnot\\varphi(x)$.\nFor example if our universe is the natural numbers with the usual $\\ge$ order, then $\\forall x(x\\ge 0)$ is true because there are no negative numbers.\nOn the other hand, $\\forall x(x\\ge 0 \\land x\\neq 0)$ is false, simply because setting $x=0$ is a counterexample.\nMore generally, a sentence \"If $p$ then $q$\" ($p\\implies q$, or $p\\rightarrow q$) is true whenever the assumption is false, i.e. $p$ never occurs. \nAn example I often use is \"If I am standing upside down from the ceiling right now, then you are all unicorns\". It does not matter that I am talking to people, and not to unicorns, because I never stand upside down from the ceiling (it gives me a huge headache, you see).\nThe next point in our journey towards the empty function, is the bounded quantification. When we write $\\forall x\\in A(P(x))$ we actually write $\\forall x(x\\in A\\rightarrow P(x))$, this means that we quantify over all the possible $x$, but if $x\\notin A$ then we do not care about it anymore (the proposition is true since the assumption is false).\nAnd lastly, the definition of a function $F$ is this: \n$$\\begin{align}\r\n\\forall z & (z\\in F\\rightarrow\\exists x\\exists y(z=\\{\\{x\\},\\{x,y\\}\\})\\land\\\\ & \\forall x(\\exists z\\exists y(z\\in F\\land z=\\{\\{x\\},\\{x,y\\}\\})\\rightarrow \\\\ &\\qquad(\\forall u\\forall v(\\exists z\\exists w((z\\in F\\land w\\in F\\land z= \\{\\{x\\},\\{x,v\\}\\}\\land w=\\{\\{x\\},\\{x,u\\}\\})\\rightarrow u=v)\r\n\\end{align}$$\nLet's read this long formula. It says that $F$ is such that every element of $F$ is an ordered pair, and for every $x$, if there is an ordered pair $z$ with $x$ for left coordinate, then there is only one such pair (i.e. given two pairs, if their right coordinate is equal then they are equal).\nInformally, $F$ is a function if it is a set of ordered pairs, that for every $x\\in Dom(f)$ there is a unique $y$ such that $\\langle x,y\\rangle\\in F$.\nAn example is $F=\\{\\langle 1,2\\rangle\\}$ is a function, all its members are ordered pairs, and since there is only one member it automatically satisfies the requirement that the left-coordinate determines the pair.\nOn the other hand $R=\\{\\langle 1,2\\rangle,\\langle 1,3\\rangle\\}$ is a set that indeed all its members are ordered pairs, but there are two distinct ordered pairs with $1$ in the left coordinate, so it is not a function.\nAnd even more $A=\\{3,\\langle 1,2\\rangle\\}$ is clearly not a function, since $3$ is not an ordered pair!\nThe definition of a function is vacuously true when it is applied to the empty set, let us see why:\n\nFor all $z$ if $z\\in\\emptyset$ then $z$ is an ordered pair is vacuously true, as no $z$ is a member of the empty set. \nNext we have, that for all $x$, if there is some ordered pair in the empty set, with $x$ as left coordinate then the right coordinate is unique, this is also vacuously true since there is no ordered pair in the empty set with $x$ in the left coordinate. This is exactly the case $p\\rightarrow q$ and $p$ is false.\n\nThe conjunction of two true statement is true as well, therefore the empty set satisfies the requirement that every element of it is an ordered pair, and if two ordered pairs have the same left coordinate then they are equal. Therefore, $\\emptyset$ is a function.", "meta": {"post_id": 45625, "input_score": 52, "output_score": 41, "post_title": "Why is an empty function considered a function?"}}
{"input": "I have two questions, actually. The first is as the title says: how do we know there exists an infinite cardinal such that there exists no other cardinals between it and $ \\aleph_0 $? (We would have to assume or derive the existence of such an object before we label it something like $ \\aleph_1 $.)\nMy second question is, can we say for certain if there's any limit to the number of cardinals existing between $ \\aleph_0 $ and continuum (i.e. $ 2^{\\aleph_0} $)? I mean, how could we know that there's not an infinite number of cardinals between the two - perhaps even more than $ \\aleph_0 $?", "output": "To prove the existence of $\\aleph_1$ we use the concept of Hartogs number. The question asks, really, why are there uncountable ordinals, since $\\aleph_1$ is by definition the least ordinal which is not countable.\nTake a set of cardinality $\\aleph_0$, say $\\omega$. Now consider all the orders on $\\omega$ which are well-orders, and consider the order isomorphism as an equivalence relation. The collection of all equivalence classes is a set.\n\nFact: If $(A,R)$ is a well-ordered set, then there exists a unique ordinal $\\alpha$ such that $(A,R)\\cong(\\alpha,\\in)$.\n\nMap every equivalence class to the unique ordinal which is order isomorphic to the members of the class. We now have a set and all its members are ordinals which correspond to possible well-ordering of $\\omega$.\n\nFact: The union of a set of ordinals is an ordinal, it is in fact the supremum of the elements in the union.\n\nLet $\\alpha$ be the union of the set defined above. We have that $\\alpha$ is an ordinal, and that every ordinal below $\\alpha$ is a possible well-ordering of $\\omega$ (and therefore countable). \nSuppose $\\alpha$ was countable too, then $\\alpha+1$ was also countable (since $\\alpha+1=\\alpha\\cup\\{\\alpha\\}$), and therefore a possible well ordering of $\\omega$. This would contradict the above fact that $\\alpha$ is greater or equal to all the ordinals which correspond to well-orderings of $\\omega$, since $\\alpha<\\alpha+1$.\nThis means that $\\alpha$ is uncountable, and that it is the first uncountable ordinal, since if $\\beta<\\alpha$ then $\\beta$ can be injected into $\\omega$, and so it is countable. Therefore we have that $\\alpha=\\omega_1=\\aleph_1$. \nNote that the above does not require the axiom of choice and holds in $\\sf ZF$. The collection of all well-orders is a set by power set and replacement, so is the set of equivalence classes, from this we have that the collection of ordinals defined is also a set (replacement again), and finally $\\alpha$ exists by the axiom of union. There was also no use of the axiom of choice because the only choice we had to do was of \"a unique ordinal\" which is a definable map (we can say when two orders are isomorphic, and when a set is an ordinal - without the axiom of choice).\nWith the axiom of choice this can be even easier:\nFrom the axiom of choice we know that the continuum is bijectible with some ordinal. Let this order type be $\\alpha$, now since the ordinals are well-ordered there exists some $\\beta\\le\\alpha$ which is the least ordinal which cannot be injected into $\\omega$ (that is there is no function whose domain is $\\beta$, its range is $\\omega$ and this function is injective).\nFrom here the same argument as before, since $\\gamma<\\beta$ implies $\\gamma$ is countable, $\\beta$ is the first uncountable ordinal, that is $\\omega_1$.\nAs to why there is no cardinals strictly between $\\aleph_0$ and $\\aleph_1$ (and between any two consecutive $\\aleph$-numbers) also stems from this definition.\n\n$\\aleph_0 = |\\omega|$, the cardinality of the natural numbers,\n$\\aleph_{\\alpha+1} = |\\omega_{\\alpha+1}|$, the cardinality of the least ordinal number which cannot bijected with $\\omega_\\alpha$,\n$\\aleph_{\\beta} = \\bigcup_{\\alpha<\\beta}\\aleph_\\alpha$, at limit points just take the supremum.\n\nThis is a function from the ordinals to the cardinals, and this function is strictly increasing and continuous. Its result is well-ordered, i.e. linearly ordered, and every subset has a minimal element.\nThis implies that $\\aleph_1$ is the first $\\aleph$ cardinal above $\\aleph_0$, i.e. there are no others between them.\nWithout the axiom of choice, however, there are cardinals which are not $\\aleph$-numbers, and it is consistent with $\\sf ZF$ that $2^{\\aleph_0}$ is not an $\\aleph$ number at all, and yet there are not cardinals strictly between $\\aleph_0$ and $2^{\\aleph_0}$ - that is $\\aleph_0$ has two distinct immediate successor cardinals.\n\nFor the second question, there is no actual limit. Within the confines of a specific model, the continuum is a constant, however using forcing we can blow up the continuum to be as big as we want.\nThis is the work of Paul Cohen. He showed that you can add $\\omega_2$ many subsets of $\\omega$ (that is $\\aleph_2\\le 2^{\\aleph_0}$), and the proof is very simple to generalize to any higher cardinal.\nIn fact Easton's theorem shows that if $F$ is a function defined on regular cardinals, which has a very limited set of constraints, then there is a forcing extension where $F(\\kappa) = 2^\\kappa$, so we do not only violate $\\sf CH$ but we violate $\\sf GCH$ ($2^{\\aleph_\\alpha}=\\aleph_{\\alpha+1}$) in a very acute manner.", "meta": {"post_id": 46833, "input_score": 118, "output_score": 102, "post_title": "How do we know an $ \\aleph_1 $ exists at all?"}}
{"input": "Let $1\\leq p < \\infty$. Suppose that\n\n$\\{f_k, f\\} \\subset L^p$ (the domain here does not necessarily have to be finite),\n$f_k \\to f$ almost everywhere, and\n$\\|f_k\\|_{L^p} \\to \\|f\\|_{L^p}$.\n\nWhy is it the case that $$\\|f_k - f\\|_{L^p} \\to 0?$$\nA statement in the other direction (i.e. $\\|f_k - f\\|_{L^p} \\to 0 \\Rightarrow \\|f_k\\|_{L^p} \\to \\|f\\|_{L^p}$ ) follows pretty easily and is the one that I've seen most of the time. I'm not how to show the result above though.", "output": "This is a theorem by Riesz.\nObserve that\n$$|f_k - f|^p \\leq 2^p (|f_k|^p + |f|^p),$$\nNow we can apply Fatou's lemma to\n$$2^p (|f_k|^p + |f|^p) - |f_k - f|^p \\geq 0.$$\nIf you look well enough you will notice that this implies that\n$$\\limsup_{k \\to \\infty} \\int |f_k - f|^p \\, d\\mu = 0.$$\nHence you can conclude the same for the normal limit.", "meta": {"post_id": 51502, "input_score": 104, "output_score": 86, "post_title": "If $f_k \\to f$ a.e. and the $L^p$ norms converge, then $f_k \\to f$ in $L^p$"}}
{"input": "I am struggling to understand why it should be that the $\\sigma$-algebra of subsets of $X$ generated by $\\mathcal{A}$ should be the smallest $\\sigma$-algebra of subsets of $X$ including $\\mathcal{A}$.\n\nLet me try to elucidate my understanding of the topic, in the hope that somebody patient and kind might be able to fill in the gaps.\nIf $X$ is a set, and $\\mathcal{G}$ is any non-empty family of $\\sigma$-algebras of subsets of $X$, then I am very happy that \n$$ \\bigcap \\mathcal{G} := \\left\\{ E : E \\in \\Sigma, \\forall \\Sigma \\in \\mathcal{G} \\right\\},$$\nthe intersection of all the $\\sigma$ algebras belonging to $\\mathcal{G}$ is a $\\sigma$-algebra of subsets of $X$.  \nNow, if $\\mathcal{A}$ is any any of subsets of $X$, then defining\n$$ \\mathcal{G} := \\left\\{ \\Sigma : \\Sigma \\ \\textrm{is a } \\sigma \\textrm{-algebra of subsets of } X, \\mathcal{A} \\subseteq \\Sigma \\right\\},$$\nthen we have by definition that $\\mathcal{G}$ is a family of $\\sigma$-algebras of subsets of $X$; also, since $\\mathcal{P} X \\in \\mathcal{G}$ we have that it is non-empty. So $\\Sigma_{\\mathcal{A}} := \\bigcap \\mathcal{G}$, called the $\\sigma$-algebra of subsets of $X$ generated by $\\mathcal{A}$, is a $\\sigma$-algebra of subsets of $X$. Because $\\mathcal{A} \\subseteq \\Sigma$ for every $\\Sigma \\in \\mathcal{G}$, we have $\\mathcal{A} \\subseteq \\Sigma_{\\mathcal{A}}$; thus $\\Sigma_{\\mathcal{A}}$ itself belongs to $\\mathcal{G}$.\nHowever, I cannot get my head around why it should be that $\\Sigma_{\\mathcal{A}}$ should be the smallest $\\sigma$-algebra of subsets of $X$ including $\\mathcal{A}$, perhaps because I am not entirely sure what this statement means explicitly (namely, I have problems interpreting 'smallest' and 'including')! I'd be very relieved if someone could try to explain this to me as it has been bugging me for a week now; I have a feeling that it might rely heavily on the $\\bigcap$, but I'm not sure exactly how...", "output": "Let me make a general comment rather than a specific one, because the construction that you are having trouble with is one that is very common and very useful (though it does have its limitations; see below) so it is important and good to have it \"down\" properly.\nYou have the following situation: you are considering a certain type of object of interest. For simplicity, let's look at the earliest example that most students encounter, which is vector spaces. So, you are looking at vector spaces. Specifically, you are looking at a particular vector space $\\mathbf{V}$.\nThe objects have sub-objects (subspaces). These are subsets of your original $\\mathbf{V}$, which are also objects (vector spaces) in their own right. Not every subset is a subobject, but every subobject is a subset.\nIn this situation, it is often fruitful to consider the following problem:\n\nGiven a subset $S$ of $\\mathbf{V}$, what is the smallest subspace of $\\mathbf{V}$ that contains $S$? \n\nThat is, we want to find a $\\mathbf{W}$ with the following properties:\n\n$\\mathbf{W}$ is a subspace of $\\mathbf{V}$;\n$S$ is contained in $\\mathbf{W}$ (\"...that contains $S$\");\nIf $\\mathbf{Z}$ is any subspace of $\\mathbf{V}$ that contains $S$, then $\\mathbf{W}\\subseteq\\mathbf{Z}$ (\"... smallest ...\")\n\nThis is the situation you have at hand, and it's also a very common situation that we encounter over and over again. Some examples:\n\nGiven a group $G$ and a subset $S$, to find the smallest subgroup of $G$ that contains $S$ (the \"subgroup generated by $S$\");\nGiven a group $G$ and a subset $S$, to find the smallest normal subgroup of $G$ that contains $S$;\nGiven a subset $S$ of the plane $\\mathbb{R}^2$, to find the smallest convex set that contains $S$ (the \"convex hull of $S$\");\nGiven a set $X$ and a collection of subsets $\\mathcal{S}\\subseteq \\mathcal{P}(X)$, find the smallest $\\sigma$-algebra on $X$ that contains $\\mathcal{S}$ (the case you have);\nGiven a set $X$ and a relation $R$ on $X$, find the smallest transitive relation on $X$ that extends $R$ (the \"transitive closure\");\nGiven a topological space $X$ and a subset $S$, find the smallest closed subset of $X$ that contains $S$ (the \"closure of $S$\").\n\nand so on and so forth.\nNow, in general, such a thing may not exist; or there may be minimal objects but no minimum object. For example, if in the last example above you replace \"closed\" with \"open\", there may be no such object: if $X=\\mathbb{R}$ and $S=[0,1]$, there is no \"smallest open set that contains $S$\". \nBut in many situations, there is one single observation that lets you conclude that such as \"smallest subobject\" must exist. Namely, if you can show that the intersection of any collection of \"subobjects\" is again a \"subobject\". For the example with vector spaces: is the intersection of an arbitrary family of subspaces of $\\mathbf{V}$ itself a subspace of $\\mathbf{V}$? For the above examples:\n\nIs the intersection of an arbitrary family of subgroups of $G$, itself a subgroup of $G$?\nIs the intersection of an arbitrary family of normal subgroups of $G$ itself a normal subgroup of $G$?\nIs the intersection of an arbitrary family of convex subsets of $\\mathbb{R}^2$ itself a convex subset of $\\mathbb{R}^2$?\nIs the intersection of an arbitrary family of $\\sigma$-algebras on $X$ itself a $\\sigma$-algebra on $X$?\nIs the intersection of an arbitrary family of transitive relations on $X$ itself a transitive relation on $X$?\nIs the intersection of an arbitrary family of closed subsets of $X$ itself a closed subset of $X$?\n\nWhen the answer is \"yes\", then the following construction will always show that there is such a thing as the \"smallest subobject that contains $S$\":\n\nTake the family of all subobjects that contain $S$; then take the intersection of the family. That's the smallest subobject that contains $S$.\n\nWhy does this work?\nBecause:\n(i) There is at least one subobject that contains $S$, (namely the original object itself; for $\\sigma$-algebras, this would be $\\mathcal{P}(X)$; for the transitive closure example, you would take the \"total relation\" $X\\times X$). \n(ii) Since the intersection of an arbitrary family of subobjects is a subobject (this is our assumption), then this intersection is a subobject.\n(iii) Since each thing being intersected contains $S$, the intersection contains $S$.\nThis means that the intersection is indeed a subobject that contains $S$. Finally:\n(iv) The intersection is always contained in each and every element of the family being intersected. So if $\\mathbf{Z}$ is any subobject that contains $S$, then it is a member of the family being intersected, so the intersection is contained in $\\mathbf{Z}$. This shows the intersection is indeed the \"smallest subobject\" with the desired properties.\nSo:\n\nTo find the smallest subspace of $\\mathbf{V}$ that contains $S$, intersect all subspaces that contain $S$.\nTo find the smallest subgroup of $G$ that contains $S$, intersect all subgroups that contain $S$.\nTo find the smallest normal subgroup of $G$ that contains $S$, intersect all normal subgroups that contain $S$.\nTo find the smallest convex set that contains $S$, intersect all convex subsets of $\\mathbb{R}^2$ that contain $S$.\nTo find the smallest $\\sigma$-algebra that contains $S$, intersect all $\\sigma$-algebras that contain $S$.\nTo find the smallest transitive relation that contains $R$, intersect all transitive relations that contain $R$.\nTo find the smallest closed subset that contains $S$, intersect all closed subsets that contain $S$.\n\nAnd this works like magic. Voil\u00e1! You have shown that this object exists. It necessarily has the properties you want.\nThis is a \"top-down\" approach. Imagine yourself looking at the \"big object\", and you are \"paring it down\" until you get \"just enough\" for the object you want (intersections make things smaller; you are paring down stuff that may not be needed).\nThe problem? Like most magic spells, it doesn't really tell you much about the end product. The fact that the end product appeared \"as if by magic\" means that you are likely to be as clueless about the actual nature of the \"smallest object\" in question as you were when you started. You now know that there is such a thing, but you don't really know what it \"looks like\".\nThat is why in almost every situation like this, you also want a \"bottom-up\" description of this \"smallest subobject that contains $S$\". You want an explicit description of what it actually looks like. For the examples above:\n\nThe smallest subspace of $\\mathbf{V}$ that contains $S$ is the set of all linear combinations of vectors in $S$.\nThe smallest subgroup of $G$ that contains $S$ is the set of all finite products of elements of $S$ and their inverses.\nThe smallest normal subgroup of $G$ that contains $S$ is the set of all finite products of conjugates of elements of $S$ and their inverses.\nThe smallest convex subset of $\\mathbb{R}^2$ that contains $S$ is the set of all convex combinations of elements of $S$.\nAsaf gives an explicit description of the smallest $\\sigma$-algebra on $X$ that contains $S$ in his answer, described by starting from $S$.\nThe smallest transitive relation on a set $X$ that contains a given relation $R$ is the set of all pairs $(a,b)$ such that there exist a finite sequence  $x_0,x_1,\\ldots,x_n$ of elements of $X$ such that $x_0=a$, $x_n=b$, and $(x_i,x_{i+1})\\in R$ for $i=0,\\ldots,n-1$.\nThe smallest closed subset of a topological space $X$ that contains a given set $S$ is equal to $S\\cup\\partial S$ or to $S\\cup S'$.\n\nIn each of these cases, one would have to show that the given description actually has the desired properties. This is a \"bottom-up\" approach.\nThe \"top-down\" description has the benefit of simplicity, that the \"universal properties\" that define the object are very clearly satisfied, and that they make proving results about how the \"smallest object\" relates to other objects easy. However, the \"top-down\" description is usually very hard to actually use to prove things about the specific smallest object. The \"bottom-up\" construction has the benefit of (usually) being a very concrete way of getting your hands on the object itself, making it easy to prove things about the object itself, but proving the universal properties is usually difficult. Thus, for example, the top-down definition of \"subspace $\\mathbf{V}$ generated by $S$\" in the linear algebra setting makes it very hard to figure out things like the dimension of the subspace, or a basis, while the \"bottom-up\" approach makes that very easy, but then proving that the collection of all linear combinations forms a subspace is more difficult than simply taking an intersection of subspaces.\nIn most books or presentations, when discussing \"the smallest X that contains S\", you will see one of two approaches:\n\nDefine it as a big intersection, then prove a theorem that gives the \"bottom-up\" description; or\nGive a \"bottom-up\" description; then prove the object described has the desired properties of being a subobject, containing S, and being the \"smallest\".\n\nWhenever possible, you want both descriptions because they have complementary strengths and weaknesses.", "meta": {"post_id": 54172, "input_score": 66, "output_score": 118, "post_title": "The $\\sigma$-algebra of subsets of $X$ generated by a set $\\mathcal{A}$ is the smallest sigma algebra including $\\mathcal{A}$"}}
{"input": "What is the relation between normal extension and separable extension?\nLet F be the algebraic extension of K, if F is a separable extension of K,if and only if F is a normal extension of K? is this correct?", "output": "Neither implies the other. So, there exist separable extensions that are not normal, and normal extensions that are not separable.\n$\\mathbb{Q}(\\sqrt[3]{2})/\\mathbb{Q}$ is a separable extension that is not normal.\nIt is separable because any extension of characteristic zero fields is separable, but it is not normal because (for example) not all algebraic conjugates of $\\sqrt[3]{2}$ lie in the field $\\mathbb{Q}(\\sqrt[3]{2})$ (its conjugates are $\\zeta_3\\sqrt[3]{2}$ and $\\zeta_3^2\\sqrt[3]{2}$ where $\\zeta_3$ is a cube root of unity; these are complex numbers, while $\\mathbb{Q}(\\sqrt[3]{2})\\subset\\mathbb{R}$).\n$\\mathbb{F}_p(\\sqrt[p]{T})/\\mathbb{F}_p(T)$ is a normal extension that is not separable.\nIt is normal because $\\mathbb{F}_p(\\sqrt[p]{T})$ is the splitting field of $x^p-T\\in \\mathbb{F}_p(T)[x]$, but it is not separable because the minimal polynomial of $\\sqrt[p]{T}$ is $x^p-T=(x-\\sqrt[p]{T})^p$ which does not have distinct roots.\nA field extension $L/K$ that is both normal and separable is called a Galois extension.", "meta": {"post_id": 54565, "input_score": 15, "output_score": 37, "post_title": "What is the relation between normal extension and separable extension?"}}
{"input": "In http://en.wikipedia.org/wiki/Manifold_(mathematics)#Construction, it says that 6 charts can be used to make an atlas for a sphere. But the text shows that you have a chart for the northern hemisphere, and you can make a similar chart for the southern hemisphere. Hence, these two charts cover the entire sphere.\nWhat am I doing wrong?", "output": "The northern hemisphere and southern hemisphere don't cover the entire sphere. The sphere is \n$$\\mathbb{S}^2=\\{(x,y,z)\\in\\mathbb{R}^3\\mid x^2+y^2+z^2=1\\}.$$\nThe northern and southern hemispheres are, respectively,\n$$\\mathbb{S}_N^2=\\{(x,y,z)\\in\\mathbb{S}^2\\mid z>0\\},\\qquad\\mathbb{S}_S^2=\\{(x,y,z)\\in\\mathbb{S}^2\\mid z<0\\}.$$\nThese miss the equator $\\{(x,y,z)\\in\\mathbb{S}^2\\mid z=0\\}$. Adding \"east\" and \"west\" hemispheres \n$$\\mathbb{S}_W^2=\\{(x,y,z)\\in\\mathbb{S}^2\\mid x>0\\},\\qquad\\mathbb{S}_E^2=\\{(x,y,z)\\in\\mathbb{S}^2\\mid x<0\\}$$\nstill doesn't get everything: we are missing the points on the equator $(0,1,0)$ and $(0,-1,0)$. Finally, adding the last two hemispheres (east and west, only rotated 90 degrees) covers the entire sphere.\n\nThis raises the question, why are we defining our hemispheres with $>$ and $<$? Perhaps we could instead use $\\leq $ and $\\geq$, and this would let us cover the sphere with two hemispheres?\nThe answer is that a chart of a manifold needs to be a homeomorphism between an open subset of the manifold with an open subset of $\\mathbb{R}^n$. The sets\n$$\\{(x,y,z)\\in\\mathbb{S}^2\\mid z\\geq 0\\},\\qquad\\{(x,y,z)\\in\\mathbb{S}^2\\mid z\\leq 0\\}$$\nare not open in the topology of $\\mathbb{S}^2$ (which is the subspace topology inherited from $\\mathbb{R}^3$). So we can't use them as coordinate neighborhoods in the manifold structure of $\\mathbb{S}^2$.\n\nIt does warrant mentioning, however, that we can cover the sphere using only two charts, via stereographic projection. The two open subsets of $\\mathbb{S}^2$ acting as our coordinate domains are \n$$\\mathbb{S}^2-\\{(0,0,1)\\},\\qquad\\mathbb{S}^2-\\{(0,0,-1)\\}$$\nand for each, we project a line from the removed point to the plane, which one can check gives a continuous map. It is a tedious (but important) exercise to demonstrate that the smooth structure determined by stereographic projection is the same as that of the hemispheres (i.e., they are compatible atlases).", "meta": {"post_id": 54643, "input_score": 24, "output_score": 43, "post_title": "why not just 2 charts to make atlas for sphere?"}}
{"input": "As I understand it (If I'm imprecise, as I will likely be, please correct me), Langlands says roughly as follows:\nFor every representation $Gal(\\mathbb{Q}) \\rightarrow GL_n(\\mathbb{C})$ we can form a function, called the associated $L$-function.\nThere is such a thing called an automorphic form, which in the $n=2$ case is just a modular form (is this right? I have a feeling that there's a subtlety missing here, but I will continue, as this is not yet the gist of my question). For each automorphic form we may associate a function, confusingly also called the associated $L$-function (except now it's associated to an automorphic form rather than a Galois representation).\nLanglands says that we get the same set of functions from both of these constructions. (although, as I understand it, it doesn't really say much about what this bijection is)\nHowever, Taniyama-Shimura is phrased in terms of elliptic curves! I'm trying to relate it to something I would recognize as being my rough sketch for Langlands in $n=2$ case.\nIt is true if an elliptic curve is defined over $\\mathbb{Q}$ then one would get a representation of $Gal(\\mathbb{Q})$ by its action on the vector space $H^1(E,\\mathbb{Q}_l)$ (in the etale sense, for various prime $l$). But this is confusing to me: are all $2$-dimensional representations of this form? And what about this weird varying $l$ (shouldn't the representations ultimately be over $\\mathbb{C}$ as in my statement of Langlands, or was that wrong?)?\nAnd how does any of this relate to the phrasing of Taniyama-Shimura that says that any elliptic curve over $\\mathbb{Q}$ has a rational map into it from some $X_0(N)$ with integer coefficients?\nI hope you can help my confusion...\nEdit:\nLet me focus my question: in what sense does an elliptic curve give a 2-dimensional representation? (do you look at the first etale cohomology with coefficients in $\\mathbb{Q}_l$ and then basechange to $\\mathbb{C}$ and it turns out to be indep. of both $l$ and the embedding into $\\mathbb{C}$?)\nAnd secondly, do all two dimension (do I need the word \"irreducible\" here?) representations of the Galois group of the rationals arise from an elliptic curve in this way?", "output": "Here are some answers to your various questions:\nFor the group $GL_2$, every cuspidal automorphic representation of $GL_2(\\mathbb A)$ (here $\\mathbb A$ is the adele ring of $\\mathbb Q$) is generated by a uniquely determined newform, which (when normalized in a suitable fashion) is either a classical newform in the sense of Atkin and Lehner, i.e. a holomorphic cuspform of some weight $k \\geq 1$ which is an eigenform for all the Hecke operators, and of minimal possible level for its associated system of Hecke eigenvalues, or else is a Maass newform (same as before but replace holomorphic cuspform of weight $k \\geq 1$ by Maass form, which is an eigenform for the Laplacian with some eigenvalue $\\lambda$).  \nThere is a conjecture of Selberg that in the Maass form case, $\\lambda \\geq 1/4$.  This is still open (although there are known lower bounds that are close); it is an analogue \"at infinity\" of the Ramanujan--Petersson conjecture.  (More precisely, Langlands explained how to unify Selberg's conjecture with the Ramanujan--Petersson conjecture using the language of automorphic representations and the concept of \"tempered\" local factors. The local factor at infinity for a holomorphic newform is discrete series (or limit of discrete series in the weight one case), hence automatically tempered, but for Maass forms the local factor at infinity is unitary principal series, and then temperedness is a non-trivial additional condition.) \nIt has been proved (essentially by Eichler--Shimura when $k = 2$, by Deligne for the general case of $k \\geq 2$, and by Deligne and Serre when $k = 1$) that holomorphic newforms give rise to two-dimensional Galois representations, and it is conjectured (but not proved in general) that $\\lambda = 1/4$ Maass forms also give rise to two-dimensional Galois representations.  The Maass forms with $\\lambda \\neq 1/4$ (i.e. $\\lambda > 1/4$ if Selberg's conjecture is true) are not expected to correspond to Galois representations.\nWhat does \"give rise to two-dimensional Galois representations\" mean exactly? Let me explain.  (Note: from now on, to avoid circumlocutions, I will write affirmative statements, but in the Maass form $\\lambda = 1/4$ case, these remain conjectural in general; in the holomorphic case they are all proved theorems.)\nActually what will happen is that the newform will correspond to a two-dimensional motive.  This motive has $\\ell$-adic cohomology, for each $\\ell$, and these will form a compatible system of two-dimensional $\\ell$-adic representations of $G_{\\mathbb Q}$.  \nThis motive also has a Hodge structure, which has Hodge numbers $(0,0), (0,0)$ in the $\\lambda = 1/4$ case, and $(k-1,0), (0,k-1)$ in the holomorphic case.\nIn the $\\lambda = 1/4$ case, or in the holomorphic weight $1$ case, since the Hodge numbers are $(0,0), (0,0)$, the motive will come from a zero-dimensional variety, which is to say it will be a two-dimensional Artin motive, and so the\n$\\ell$-adic Galois representations can be consolidated into a single Artin\nrepresentation $G_{\\mathbb Q} \\to GL_2(\\mathbb C)$.  However, in the holomorphic case when $k \\geq 2$, the motive will come from a positive-dimensional variety, and so the $\\ell$-adic representations will not be able to be simplified into a single Artin representation (e.g. because they will have infinite image, not finite image).\nE.g. in the case $k = 2$, the Hodge numbers are $(1,0), (0,1)$, which says that the motive comes from a certain abelian variety.  The $\\ell$-adic representations are then the $\\ell$-adic Tate modules (or rather, their duals, if you want to be careful about the distinction betweeh cohomology and homology) of this abelian variety.\nI now should say something about coefficients.  The Hecke eigenvalues of the newform will be algebraic numbers, and collectively they will generate a finite extension $E$ of $\\mathbb Q$.  The motive attached to the newform will have $E$ acting as endomorphisms, and when I say it it two-dimensional, I mean it is two-dimensional over $E$.  Thus the $\\ell$-adic representations will actually be two\ndimensional $\\lambda$-adic representations for each prime $\\lambda$ of $E$, i.e. they will be representations $G_{\\mathbb Q} \\to GL_2(E_{\\lambda})$.\nE.g. if $k = 2$ and the newform $f$ has level $N$, then we can form the Jacobian\n$J_1(N)$ of $X_1(N)$, and this has an action of $\\mathbb T$, the Hecke algebra at level $N$.  The action of $\\mathbb T$ on $f$ gives rise to a homomorphism\n$\\mathbb T \\to E$ (sending $T_p$ to the $p$th Hecke eigenvalue of $f$), and\nwe may form $A_f:= E\\otimes_{\\mathbb T} J_1(N)$, which is an abelian variety well-defined up to isogeny.  It has dimension $[E:\\mathbb Q]$ and endomorphisms by $E$, and so for each $\\lambda$ we form its $\\lambda$-adic Tate module,\nand (the dual of) this is two-dimensional over $E_{\\lambda}$, and is the $\\lambda$-adic Galois representation attached to $f$.\nIn particular, if $k = 2$ and $E = \\mathbb Q$ (i.e. all the Hecke eigenvalues of $f$ are rational numbers) then $A_f$ is an elliptic curve, and this is the association of elliptic curves to weight two newforms with rational Hecke eigenvalues.  (Note that in this case $f$ automatically has level $\\Gamma_0(N)$ rather than $\\Gamma_1(N)$, and so in the above we replace $X_1(N)$ and $J_1(N)$ by $X_0(N)$ and $J_0(N)$, which may be a little more familiar.)\nNow suppose conversely that $M$ is a motive with endomorphisms by a number field $E$, irreducible and of dimension two over $E$.  It is conjectured that $M$ is attached to a newform.  In fact, we can determine the newform precisely (assuming it exists):\nthe ramification of $M$ will determine the level, and counting points on $M$ mod primes will determine the Hecke eigenvalues of the newform.   The Hodge numbers of $M$ (possibly after a Tate twist) will be $(i,0),(0,i)$ for some $i \\geq 0$.\nIf $i > 0$, then the newform will have to be a weight $i + 1$ holomorphic newform.  If $i = 0$, then the newform will be either holomorphic weight one,\nor a $\\lambda = 1/4$ Maass form; to tell which, you have to look at the action of complex conjugation on the Hodge strucuture: if it is non-scalar (this is usually called \"odd\"), you are in the holomorphic case, while if it is scalar\n(this is usually called \"even\") you are in the Maass case.  (The reason for \"odd\" and \"even\" is that in these case the determinant of complex conjugation on the $\\ell$-adic cohomology is $-1$ resp. $+1$.)\nLet's suppose that the motive $M$ comes from an elliptic curve $C$.  (I use $C$ rather than $E$ because $E$ was my notation for the field of coefficients.)\nThen the Hodge numbers are $(1,0),(0,1)$, hence we expect there to be weight two newform $f$ such that $C = A_f$.  (Since $A_f$ is only defined up to isogeny, this equality should be understood as an isogeny.)  But if $C$ is isogenous to a factor of the Jacobian of $X_0(N)$ (which is what we are saying), then this implies (indeed is equivalent to) $C$ being a quotient of $X_0(N)$.\nSometimes things are phrased in terms of $L$-functions rather than motives (because motives are subtle to think about, and parts of the theory of motives remain conjectural).  In the elliptic curve case, though, everything is known, because Faltings proved the Tate conjecture, namely that two elliptic curves with the same $L$-function are isogenous.  Since you can read off the $L$-function from the $\\ell$-adic Tate module, this says that even knowing that the $\\ell$-adic Tate module of $C$ coincides with the $\\ell$-adic Galois representation attached to $f$ (even for a single $\\ell$) is enough to get\n$C = A_f$.  (This is is why things are sometimes formulated in terms of $L$-functions, sometimes in terms of Galois reps., and sometimes in terms of motives --- the formulations are conjecturally all equivalent, and are known to be equivalent in the elliptic curve case.)\nThe preceding discussion shows that the general conjecture about going from two-dimensional motives to newforms is a generalization of Shimura--Taniyama.  It is known in the odd case (it follows from Serre's conjecture, proved by Khare, Wintenberger, and Kisin; see Cor. 0.5 of Kisin's paper).  It is open in general in the even case (just as the construction of motives, or Galois representations, from $\\lambda = 1/4$ Maass forms is wide open).  (But note that it is proved in the case of solvable image, by Langlands--Tunnel --- see below.)\n\nAs I already noted: one can avoid talking about motives, and instead talk about compatible families of $\\ell$-adic Galois reps. (more precisely, $\\lambda$-adic Galois reps., if we fix our coefficient field $E$), or even $\\ell$-adic (or $\\lambda$-adic) reps. for a single choice of $\\ell$ or $\\lambda$. \nThere are arrows\n$$\\text{motives } \\to \\text{ compatible families of $\\ell$-adic reps. } \\to \\text{ $\\ell$-adic rep. for a fixed $\\ell$} $$\ngiven by passing to $\\ell$-adic cohomology for all $\\ell$, and then by picking out a particular $\\ell$.  These maps are injective (the first conjecturally in general --- this is the Tate conjecture), but not surjective.  Not all compatible families come from a motive, and not every individual $\\ell$-adic rep. sits in a compatible family.  \nThere is a conjecture of Fontaine and Mazur which (conjecturally!) describes those $\\ell$-adic reps. which come from motives. (Here $\\ell$ is a fixed prime, and I could just as well be talking about $\\lambda$-adic reps. here, but it is more traditional to say $\\ell$-adic rather than $\\lambda$-adic, even when the latter is what is meant.) In the two-dimensional case, for representations which are supposed to come from holomorphic newforms of weight $k \\geq 2$, it is largely proved: see e.g. Thm. 1.2.4 (2), and also the discussion in Section 1.4, of this paper, as well as this paper of Kisin, and these papers\nof Calegari.  (These papers all build on the work of Wiles and Taylor--Wiles.)\nIn the case of representations which are supposed to come from either weight one holomorphic forms or Maass forms with $\\lambda = 1/4$, the situation is more complicated.  If you assume that the Galois rep. is odd and has finite image, then it is known to come from a weight one form; this follows from Serre's conjecture, as noted above.  However, it is supposed to be enough (according to Fontaine and Mazur) to assume that the $\\ell$-adic representation has finite image on inertia at $\\ell$ --- this should then imply that the whole image is finite.  However, this is not proved in general; I believe this result of Buzzard, which handles certain odd cases, is the best general result currently.\nIn the even case, if you assume that the image is finite and solvable, then Langlands--Tunnel give a $\\lambda = 1/4$ Maass form giving rise to your Galois rep., but (still in the even case now), if the image is finite but non-solvable, or (even worse) if you just assume that the image of inertia at $\\ell$ is finite, then the conjecture is wide-open.\n\nNote that $2$-dimensional Artin (i.e. finite image) reps. are just a special case of all two-dimensional $\\ell$-adic reps. (corresponding to $\\lambda = 1/4$ Maass form or weight one forms --- at least conjecturally, in the even case), and the $\\ell$-adic Tate modules of elliptic curves are just another special case of such reps.  Even if you write down all the $\\ell$-adic reps. coming from all newforms, there are uncountably manner other irreducible two-dimensional $\\ell$-adic reps. that don't come from motives at all.", "meta": {"post_id": 55449, "input_score": 41, "output_score": 50, "post_title": "In what sense is Taniyama-Shimura the $n=2$ case of Langlands?"}}
{"input": "Let $\\mathcal{A}$ be an abelian category. \nWe say that $\\mathcal{A}$ satisfies (AB5) if $\\mathcal{A}$ is cocomplete and filtered colimits are exact.\nIn Weibel's Introduction to homological algebra, he states (without proof) that $\\mathcal{A}$ satisfies axiom (AB5) iff $\\mathcal{A}$ is cocomplete and for all lattices $\\{ A_i \\}$ of subobjects of $A \\in \\mathcal{A}$ and all subobjects $B$ of $A$, we have $$ \\sum (A_i \\cap B) = B \\cap \\sum A_i.$$\nI have been thinking about this for a few days but have been unable to come up with a proof. In the forward direction I can't seem to relate the sum of subobjects and filtered colimits. I have no idea about the backward direction. Could anyone give me a hint?\nNote: This is not actually an exercise in Weibel's book, he states it in the appendix on category theory when he is defining axiom (AB5). It is stated without proof in Grothendieck's T\u014dhoku paper. Also, It is an exercise in Freyd's abelian categories.", "output": "I borrowed the ideas from the following books.\nAbelian categories with application to rings and modules by Popescu, 1973.\nTheory of categories by Mitchell, 1964.\nNotations and Conventions\nWe fix a Grothendieck universe $\\mathcal{U}$.\nWe consider only categories which belong to $\\mathcal{U}$.\nLet $\\mathcal{C}$ be a category.\nWe denote by Ob($\\mathcal{C}$) the set of objects of $\\mathcal{C}$.\nOften, by abuse of notation, we use $\\mathcal{C}$ instead of Ob($\\mathcal{C}$). \nWe denote by Mor($\\mathcal{C}$) the set of morphisms of $\\mathcal{C}$.\nLet $f:X \\rightarrow Y$ be a morphism of $\\mathcal{C}$.\nWe denote by dom($f$) the domain of $f$, i.e. $X$ = dom($f$).\nWe denote by codom($f$) the codomain of $f$, i.e. $Y$ = codom($f$).\nDefinition\nLet $\\mathcal{C}$ be a category.\nLet $X$ be an object of $\\mathcal{C}$.\nLet $I$ be a small set.\nLet $(X_i)_I$ be a family of subobjects of $X$.\nIf $(X_i)_I$ satisfies the following condition, $(X_i)_I$ is called a directed family of subobjects of $X$.\nFor any $i, j \\in I$, there exists $k \\in I$ such that $X_i \\subset X_k$ and $X_j \\subset X_k$.\nLemma 1\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nLet $I$ be a small category.\nLet $F: I \\rightarrow \\mathcal{A}$ be a functor.\nLet $A$ = colim $F$.\nFor each $i \\in I$, let $f_i:F(i) \\rightarrow A$ be the canonical morphism.\nFor each $i \\in I$, let $A_i$ = Im($f_i$).\nSince $\\mathcal{A}$ is cocomplete, $\\sum A_i$ exists.\nThen $A = \\sum A_i$.\nProof:\nLet $B = \\sum A_i$.\nLet $m:B \\rightarrow A$ be the canonical monomorphism.\nSince $A_i$ = Im($f_i$) for each $i \\in I$,\nthere exists $g_i:F(i) \\rightarrow B$ such that $f_i = mg_i$.\nLet $u: i \\rightarrow j$ be a morphism of I.\nSince $f_i = f_jF(u)$, $mg_i = mg_jF(u)$.\nSince $m$ is a monomorphism, $g_i = g_jF(u)$.\nHence there exists $g:A \\rightarrow B$ such that $g_i = gf_i$ for each $i$.\nHence $mgf_i = mg_i = f_i$ for each $i$.\nHence $mg = 1_A$.\nHence $A \\subset B$.\nHence $A = B$.\nQED\nLemma 2\nLet $\\mathcal{C}$ be a cocomplete category.\nLet I be a small category.\nLet $\\mathcal{C}^I$ be the category of functors: $I \\rightarrow \\mathcal{C}$.\nThen colim$: \\mathcal{C}^I \\rightarrow \\mathcal{C}$ preserves colimits.\nProof:\nLet $\\Delta: \\mathcal{C} \u2192 \\mathcal{C}^I$ be the diagonal functor,\ni.e. for each $X \\in \\mathcal{C}$ and for each $i \\in I$, $\\Delta(X)(i) = X$.\nSince colim is a left adjoint functor of $\\Delta$, it preserves colimits(MacLane: Categories for the working mathematician, Chapter V, Section 5, Theorem 1, p.114).\nQED\nLemma 3\nLet $\\mathcal{A}$ be a cocomplete abelian category which satisfies (AB5).\nLet $X$ be an object of $\\mathcal{A}$.\nLet I be a small filtered category.\nLet Sub($X$) be the category of subobjects of $X$.\nLet $F: I \\rightarrow$ Sub($X$) be a functor.\nThen $\\sum F(i)$ = colim $F$.\nProof:\nFor each $i \\in I$, Let $u_i\uff1aF(i) \\rightarrow$ colim $F$ be the canonical morphism.\nFor each $i \\in I$, Let $m_i\uff1aF(i) \\rightarrow X$ be the canonical monomorphism.\nSince $(m_i)_I$ is a cocone, it induces a morphism $f$: colim $F \\rightarrow X$.\nBy (AB5), $f$ is mono.\nHence we can regard colim $F$ as a subobject of $X$.\nSince $fu_i = m_i$ for each $i$, $F(i) \\subset$ colim $F$.\nLet $Z$ be a subobject of $X$.\nLet $r: Z \\rightarrow X$ be the canonical monomorphism.\nSuppose $F(i) \\subset Z$ for each $i$.\nLet $k_i: F(i) \\rightarrow Z$ be the canonical monomorphism.\nSince $(k_i)_I$ is a cocone, it induces a morphism $g$: colim $F \\rightarrow Z$.\nFor each $i \\in I$, $rgu_i = rk_i = m_i$.\nHence $f = rg$.\nHence colim $F \\subset Z$.\nQED\nLemma 4\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nLet $f:X \\rightarrow Y$ be a morphism of $\\mathcal{A}$.\nLet $I$ be a small set.\nLet $(X_i)_I$ be a family of subobjects of $X$.\nThen $\\sum f(X_i) = f(\\sum X_i)$.\nProof:\nFor each $i \\in I$, $X_i \\subset \\sum X_i$.\nHence $f(X_i) \\subset f(\\sum X_i)$.\nLet $Z$ be a subobject of $X$.\nSuppose $f(X_i) \\subset Z$ for each $i$.\nThen $f^{-1}(f(X_i)) \\subset f^{-1}(Z)$.\nSince $X_i \\subset f^{-1}(f(X_i))$, $X_i \\subset f^{-1}(Z)$.\nHence $\\sum X_i \\subset f^{-1}(Z)$.\nHence $f(\\sum X_i) \\subset f(f^{-1}(Z)) \\subset Z$.\nQED\nLemma 4.5\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nLet $I$ be a small category.\nLet $F: I \\rightarrow \\mathcal{A}$ be a functor.\nLet $X$ = colim $F$.\nLet $(s_i: F(i) \\rightarrow Y)_I$ be a cocone.\nLet $f:X \\rightarrow Y$ be the morphism induced by the cocone.\nThen $f(X) = \\sum s_i(F(i))$.\nProof:\nFor each $i$, let $u_i: F(i) \\rightarrow X$ be the canonical morphism.\nFor each $i$, $fu_i = s_i$.\nHence $f(u_i(F(i)) = s_i(F(i)) \\subset f(X)$.\nLet $Z$ be a subobject of $Y$.\nSuppose $s_i(F(i)) \\subset Z$ for each $i$.\nFor each $i$, $s_i$ induces $t_i: F(i) \\rightarrow Z$.\nSince $(t_i: F(i) \\rightarrow Z)_I$ is a cocone,\nit induces $g:X \\rightarrow Z$.\nLet $m: Z \\rightarrow Y$ be the canonical monomorphism.\n$mgu_i = mt_i = s_i$ for each i.\nHence $f = mg$.\nHence $f(X) \\subset Z$.\nQED\nLemma 5\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nLet $X$ be an object of $\\mathcal{A}$.\nLet $I$ be a small set.\nLet $(X_i)_I$ be a family of subobjects of $X$.\nThen $\\bigoplus X/X_i$ = $X/(\\sum X_i)$.\nProof:\nFor each $i \\in I$, the following sequence is exact.\n$0 \\rightarrow X_i \\rightarrow X \\rightarrow X/X_i \\rightarrow 0$.\nBy Lemma 2, colim preserves cokernels.\nHence, colim $X_i \\rightarrow X \\rightarrow$ colim $X/X_i \\rightarrow 0$ is exact.\nBy Lemma 4.5, Im(colim $X_i \\rightarrow X$) = $\\sum X_i$.\nHence colim $X/X_i$ = $X/(\\sum X_i)$.\nQED\nLemma 5.4\nSuppose the following is a pullback diagaram in an abelian category.\n$$\\begin{matrix}\nA&\\stackrel{f}{\\rightarrow}&B\\\\\n\\downarrow&&\\downarrow\\\\\nC&\\stackrel{h}{\\rightarrow}&D\n\\end{matrix}\n$$\nSuppose the following sequence is exact.\n$0 \\rightarrow C \\stackrel{h}{\\rightarrow}D \\rightarrow E$\nThen $0 \\rightarrow A \\stackrel{f}{\\rightarrow}B \\rightarrow E$ is exact.\nProof: Left to the readers.\nLemma 5.5\nConsider the following commutative diagram with two horizontal exact sequences in an abelian category.\n$X \\rightarrow Y \\rightarrow Z \\rightarrow 0$\n$0 \\rightarrow X' \\rightarrow Y' \\rightarrow Z' \\rightarrow 0$\nSuppose the left square is a pullback.\nThen $Z \\rightarrow Z'$ is mono.\nProof:\nWe call s the above morphism $Z \\rightarrow Z'$.\nLet $r:T \\rightarrow Z$ be a morphism such that sr = 0.\nThere exists the following pullback diagaram.\n$$\\begin{matrix}\nP&\\stackrel{u}{\\rightarrow}&T\\\\\n\\downarrow&&\\downarrow{r}\\\\\nY&\\stackrel{}{\\rightarrow}&Z\n\\end{matrix}\n$$\nBy Lemma 5.4,\n$0 \\rightarrow X \\rightarrow Y \\rightarrow Z\u2019$ is exact.\nHence there exists $P \\rightarrow X$ such that $P \\rightarrow Y = P \\rightarrow  X \\rightarrow Y$.\nHence $ru$ = 0.\nOn the other hand, since a pullback of an epimorphism in an abelian category is epi(MacLane Proposition 2, p.199), $u$ is epi.\nHence $r$ = 0.\nQED\nLemma 6\nLet $\\mathcal{A}$ be an abelian category.\nLet $f:X \\rightarrow Y$ be a morphism of $\\mathcal{A}$.\nLet $Z \\subset Y$.\nThen $X/f^{-1}(Z)$ is canonically isomorphic to $f(X)/(f(X) \\cap Z)$.\nProof:\nConsider the following commutative diagram with two horizontal exact sequences.\n\nBy Lemma 5.5, $X/f^{-1}(Z) \\rightarrow f(X)/(f(X) \\cap Z)$ is mono.\nSince $X \\rightarrow f(X)$ is epi,\n$X \\rightarrow X/f^{-1}(Z) \\rightarrow f(X)/(f(X) \u2229 Z)$ is epi.\nHence $X/f^{-1}(Z) \\rightarrow f(X)/(f(X) \\cap Z)$ is epi.\nHence $X/f^{-1}(Z) \\rightarrow f(X)/(f(X) \\cap Z)$ is an isomorphism.\nQED\nNote\nIf you are willing to accept Mitchell's embedding theorem, Lemma 6 will be trivial.\nLemma 7\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nSuppose $\\mathcal{A}$ has the following property.\n\nLet $A$ be an object of $\\mathcal{A}$.\n  Let $(A_i)_I$ be a directed family of subobjects of $A$.\n  Then, for every subobject $B$ of $A$, $(\\sum A_i) \\cap B = \\sum (A_i \\cap B)$.\n\nLet $f:Y \\rightarrow X$ be a morphism of $\\mathcal{A}$.\nLet $(X_i)_I$ be a directed family of subobjects of $X$.\nThen,\n$f^{-1}(\\sum X_i) = \\sum f^{-1}(X_i)$.\nProof:\nBy Lemma 6, for each $i$, $Y/f^{-1}(X_i)$ is canonically isomorphic to $f(Y)/(f(Y) \\cap X_i)$.\nHence $\\bigoplus Y/f^{-1}(X_i)$ is canonically isomorphic to $\\bigoplus f(Y)/(f(Y) \\cap  X_i)$.\nBy Lemma 5, $Y/\\sum f^{-1}(X_i)$ = $\\bigoplus Y/f^{-1}(X_i)$.\nHence $Y/\\sum f^{-1}(X_i)$ = $\\bigoplus f(Y)/(f(Y) \\cap X_i)$.\nBy Lemma 5, $\\bigoplus f(Y)/(f(Y) \\cap X_i)$ = $f(Y)/\\sum (f(Y) \\cap X_i)$.\nBy the assumption, $f(Y)/\\sum (f(Y) \\cap  X_i)$ = $f(Y)/((\\sum X_i) \\cap f(Y))$.\nBy Lemma 6, $Y/f^{-1}(\\sum X_i)$ is canonically isomorphic to $f(Y)/((\\sum X_i) \\cap  f(Y))$.\nHence $Y/\\sum f^{-1}(X_i)$ is canonically isomorphic to $Y/f^{-1}(\\sum X_i)$.\nHence $f^{-1}(\\sum X_i)$ = $\\sum f^{-1}(X_i)$.\nQED\nLemma 7.3\nLet $\\mathcal{C}$ be a category.\nLet $X$ be an object of $\\mathcal{C}$.\nLet Sub($X$) be the category of subobjects of $X$.\nLet $I$ be a small set.\nLet $(X_i)_I$ be a directed family of subobjects of $X$.\nThen there exists a preorder on $I$ making $I$ a filtered category and a functor $F: I \\rightarrow$ Sub($X$) such that\n$F(i) = X_i$ for each $i \\in I$.\nProof: Define $i \\leq j$ if and only if $X_i \\subset X_j$.\nQED \nLemma 7.5\nLet $\\mathcal{A}$ be an abelian category.\nLet $I$ be a small category.\nLet $F: I \\rightarrow \\mathcal{A}$ be a functor.\nLet $i \\in I$.\nLet $(i\\downarrow I)$ be the coslice category under i.\nLet Sub($F(i)$) be the category of subobjects of $F(i)$.\nThen there exists a functor $G$: $(i\\downarrow I) \\rightarrow$ Sub($F(i)$)\nsuch that $G(u)$ = Ker($F(u)$) for each $u \\in (i\\downarrow I)$.\nProof:Clear.\nLemma 8\nLet $\\mathcal{A}$ be an abelian category.\nLet $I$ be a small filtered category.\nLet $F: I \\rightarrow \\mathcal{A}$ be a functor.\nLet $i \\in I$.\nLet $J$ = {$u \\in$ Mor($I$); $i$ = dom($u$)}.\nThen (Ker($F(u))$)$_J$ is a directed family of subobjects of $F(i)$.\nProof:\nLet $(i\\downarrow I)$ be the coslice category under i.\n$(i\\downarrow I)$ is clearly a filtered category.\nSince $J$ = Ob($(i\\downarrow I)$), the assertion follows immediately from Lemma 7.5.\nQED\nLemma 8.5\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nLet $I$ be a small category.\nLet $F: I \\rightarrow \\mathcal{A}$ be a functor.\nLet $S = \\bigoplus_i F(i)$, where $i$ runs over every object of $I$.\nLet $m_i: F(i) \\rightarrow S$ be the canonical monomorphism for each $i \\in I$.\nLet $M$ = $\\sum_u$ Im($m_i - m_jF(u)$), where $u$ runs over every morphism of $I$ and $i$ = dom($u$), $j$ = codom($u$).\nLet $\\pi:S \\rightarrow S/M$ be the canonical epimorphism.\nLet $f_i = \\pi m_i$ for each $i \\in I$.\nThen $S/M$ = colim $F$ with canonical morphisms $f_i: F(i) \\rightarrow S/M$ for each $i \\in I$.\nProof:Left to the readers.\nLemma 8.6\nLet $I$ be a filtered category.\nLet $V$ be a non-empty finite subset of Ob($I$).\nLet $T$ be a finite subset of Mor($I$) such that dom($u$) $\\in V$ and codom($u$) $\\in V$ whenever $u \\in T$.\nThen there exists $p \\in$ Ob($I$) and a morphism $f_i: i \\rightarrow p$ for each $i \\in V$ with the following property.\n\nFor each $u:i \\rightarrow j$ in $T$, $f_i = f_ju$.\n\nProof:\nThere exists $q \\in I$ such that there exists a morphism $g_i:i \\rightarrow q$ for each $i \\in V$.\nLet $u:i \\rightarrow j$ in $T$.\nThere exists $r_u \\in I$ and a morphism $h_u:q \\rightarrow r_u$ such that\n$h_ug_i = h_ug_ju$.\nThere exists $r \\in I$ such that there exists a morphism $r_u \\rightarrow r$ for each $u \\in T$.\nHence, for each $u:i \\rightarrow j$ in $T$ there exist a morphism $g_{u, i}: i \\rightarrow r$ and a morphism $h_{u, j}: j \\rightarrow r$ such that $g_{u, i} = h_{u, j}u$.\nFor each $i \\in V$, let $G_i$ be the set {$g_{u, i}: i$ = dom($u$), $u \\in T$},\nand let $H_i$ be the set {$h_{u, i}: i$ = codom($u$), $u \\in T$}.\nLet $S_i = G_i \\cup H_i$ for each $i \\in V$.\nBy the properties of a filtered category, we can assume that $S_i$ consists of one morphism $f_i$ with a common codomain $p$ for each $i \\in V$.\nIf $S_i$ is empty, we can assume that there exists a morphism $f_i:i \\rightarrow p$ which has no condition.\nQED\nLemma 9\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nSuppose $\\mathcal{A}$ has the following property.\n\nLet $A$ be an object of $\\mathcal{A}$.\n  Let $(A_i)_I$ be a directed family of subobjects of $A$.\n  Then, for every subobject $B$ of $A$, $(\\sum A_i) \\cap B = \\sum (A_i \\cap B)$.\n\nLet I be a small filtered category.\nLet $F: I \\rightarrow \\mathcal{A}$ be a functor.\nFor each $i$, let $f_i: F(i) \\rightarrow$ colim($F$) be the canonical morphism.\nThen, for each $i$, Ker($f_i$) = $\\sum$ Ker($F(u)$), where $u$ runs over every morphism such that $i$ = dom($u$).\nProof:\nWe use the notations of Lemma 8.5.\nLet $T$ be a subset of Mor($I$).\nLet $M_T$ = $\\sum_{u \\in T}$ Im($m_i - m_jF(u)$), where $i$ = dom($u$), $j$ = codom($u$).\nThen $M = \\sum_T M_T$, where T runs through all finite subsets of Mor($I$).\nHence, by Lemma 8.5 and Lemma 7, Ker($f_i$) = $m_i^{-1}(M)$ = $\\sum_T m_i^{-1}(M_T)$,\nwhere T runs through all finite subsets of Mor($I$).\nIt suffices to prove:\nFor each finite subset $T$ of Mor($I$), $m_i^{-1}(M_T) \\subset$ Ker($F(u)$) for some $u \\in$ Mor($I$) such that  $i$ = dom($u$).\nLet $V$ be the set of $k \\in I$ such that $k$ = $i$ or $k$ = dom($u$) or $k$ = codom($u$) for some $u \\in T$.\nSince $I$ is filtered, by Lemma 8.6, there exists $p \\in$ Ob($I$) and a morphism $v_k: k \\rightarrow p$ for each $k \\in V$ with the following property.\n\nFor each $u \\in$ Mor($I$) such that k = dom($u$) $\\in V$ and $j$ = codom($u$) $\\in V$, $v_k = v_ju$.\n\nWe define $f:S \\rightarrow F(p)$ as follows.\nLet $k$ be any object of $I$.\nIf $k \\in V$, $fm_k = F(v_k)$, otherwise $fm_k = 0$.\nFor each $u \\in T$, let $k$ = dom($u$), $j$ = codom($u$).\nThen $f(m_k - m_jF(u))$ = $F(v_k) - F(v_j)F(u)$ = $0$.\nHence, by Lemma 4, $f(M_T)$ = $0$.\nSince $m_i(m_i^{-1}(M_T)) \\subset M_T$, $0$ = $f(m_i(m_i^{-1}(M_T)))$ = $F(v_i)(m_i^{-1}(M_T))$.\nHence $m_i^{-1}(M_T) \\subset$ Ker($F(v_i)$) as required.\nQED\nProposition 1\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nSuppose $\\mathcal{A}$ has the following property.\n\nLet $A$ be an object of $\\mathcal{A}$.\n  Let $(A_i)_I$ be a directed family of subobjects of $A$.\n  Then, for every subobject $B$ of $A$, $(\\sum A_i) \\cap B = \\sum (A_i \\cap B)$.\n\nThen $\\mathcal{A}$ satisfies (AB5).\nProof:\nLet $I$ be a small filtered category.\nBy Lemma 2, colim$: \\mathcal{A}^I \\rightarrow \\mathcal{A}$ preserves colimits.\nIn particular, it preserves cokernels.\nHence it is right exact.\nIt suffices to prove that it preserves monomorphisms.\nLet $f: F \\rightarrow G$ be a monomorphism of $\\mathcal{A}^I$.\nLet $K$ = Ker(colim($f$)).\nFor each i, let $u_i: F(i) \\rightarrow$ colim $F$ be the canonical morphism.\nLet $A_i$ = $u_i(F(i))$ for each i.\nSince $I$ is a filtered category, $(A_i)_I$ is a directed family of subobjects of colim $F$.\nBy Lemma 1, colim $F$ = $\\sum A_i$.\nBy the assumption, $K$ = $(\\sum A_i) \\cap K = \\sum (A_i \\cap K)$.\nSuppose $K \\neq 0$.\nThere exists $k \\in I$ such that $A_k \\cap K \\neq 0$.\nSince $A_k$ = Im($u_k$), $u_k^{-1}(A_k \\cap K) \\neq 0$.\nLet $M = u_k^{-1}(A_k \\cap K)$.\nThen $u_k(M) \\neq 0$.\nFor each i, let $v_i: G(i) \\rightarrow$ colim $G$ be the canonical morphism.\n$v_k(f_k(M))$ = (colim $f$)($u_k(M)$) = (colim $f$)($A_k \\cap K$) $\\subset$ (colim f)($K$) = $0$.\nHence $f_k(M) \\subset$ Ker($v_k$).\nBy Lemma 9, $f_k(M) \\subset \\sum$ Ker($G(t)$), where $t$ runs over every morphism such that $k$ = dom($t$).\nBy Lemma 8 and the assumption, $f_k(M)$ = $\\sum$ (Ker($G(t)$) $\\cap f_k(M)$).\nSince $f_k$ is mono,  $M$ = $f_k^{-1}(f_k(M))$.\nBy Lemma 7, $M$ = $f_k^{-1}(f_k(M))$ = $f_k^{-1}(\\sum$ (Ker($G(t)$) $\\cap f_k(M)))$ = $\\sum f_k^{-1}$(Ker($G(t)$ $\\cap f_k(M))$.\nFor each morphism $t: k \\rightarrow j$, Let $N_t = f_k^{-1}$(Ker($G(t)$) $\\cap f_k(M))$.\nThen $G(t)f_k(N_t) = 0$.\nSince $G(t)f_k = f_jF(t)$, $f_jF(t)(N_t) = G(t)f_k(N_t) = 0$.\nSince $f_j$ is mono, $F(t)(N_t) = 0$.\nHence, by Lemma 9, $u_k(N_t) = 0$.\nHence, by Lemma 4, $u_k(M) = u_k(\\sum N_t) = \\sum u_k(N_t) = 0$.\nThis is a contradiction.\nQED\nProposition 2\nLet $\\mathcal{A}$ be a cocomplete abelian category satisfying (AB5).\nLet $A$ be an object of $\\mathcal{A}$. \nLet $(A_i)_I$ be a directed family of subobjects of $A$.\nThen, for every subobject $B$ of $A$, $(\\sum A_i) \\cap B = \\sum (A_i \\cap B)$.\nProof:\nLet $C = \\sum A_i$.\nFor each i, we have the following exact sequence.\n$0 \\rightarrow A_i \\cap B \\rightarrow A_i \\rightarrow C/(C \\cap B)$\nBy (AB5), Lemma 7.3 and Lemma 3, we get the following exact sequence.\n$0 \\rightarrow \\sum (A_i \\cap B) \\rightarrow C \\rightarrow C/(C \\cap B)$\nHence $(\\sum A_i) \\cap B = \\sum (A_i \\cap B)$.\nQED", "meta": {"post_id": 56454, "input_score": 31, "output_score": 61, "post_title": "Abelian categories and axiom (AB5)"}}
{"input": "Let's say I have two simple vectors: $[0, 1]$ and $[1, 0]$.\nTheir Kronecker product would be $[0, 0, 1, 0]$.\nLet's say I have only the Kronecker product. How can I find the two initial vectors back?\nIf my two vectors are written as : $[a, b]$ and $[c, d]$, the (given) Kronecker product is:\n$$[ac, ad, bc, bd] = [k_0, k_1, k_2, k_3]$$\nSo I have a system of four non linear equations that I wish to solve:\n$$\\begin{align*}\r\nac &= k_0\\\\\r\nad&= k_1\\\\\r\nbc&= k_2\\\\\r\nbd &=k_3.\r\n\\end{align*}$$\nI am looking for a general way to solve this problem for any number of initial vectors in $\\mathbb{C}^2$ (leading my number of variables to $2n$ and my equations to $2^n$ if I have $n$ vectors).\nSo here are a few specific questions:\nWhat is the common name of this problem? \nIf a general solution is known, what is its complexity class?\nDoes the fact that I have more and more equations when $n$ goes up compared to the number of variables help?\n(Note: I really didn't know what to put as a tag.)", "output": "This problem (Reverse kronecker product) has a known solution called \"Nearest Kronecker Product\" and it is generalized to matrices as well.\nGiven $A\\in  \\mathbb R^{m\\times n} $ with $m = m_1m_2$ and $n = n_1n_2$, find $B\\in  \\mathbb R^{m_1\\times n_1}$ and $C\\in \\mathbb R^{m_2\\times n_2}$ so\n$\\phi(B,C)$ = min $|| A- B\\otimes C||_F$, where $F$ denotes Frobenius norm. \nThis is reformulated as:\n$\\phi(B,C)$ = min $|| R- vec(B)\\otimes vec(C)'||_F$\n$vec$ is the vectorization operator which stacks columns of a matrix on top of each other. A is rearranged into $R \\in  \\mathbb R^{m_1n_1\\times m_2n_2}$ such that the sum of squares in $|| A- B\\otimes C||_F$ is exactly the same as $|| R- vec(B)\\otimes vec(C)'||_F$.\nExample for arrangement where $m_1=3,n_1=m_2=n_2=2$: \n$$ \n\\phi(B,C)  =  \\left|\n\\left[\n\\begin{array}{cc|cc}\na_{11}& a_{12}  & a_{13} & a_{14} \\\\\na_{21}& a_{22} & a_{23} & a_{24} \\\\ \\hline\na_{31}& a_{32} & a_{33} & a_{34} \\\\\na_{41}& a_{42} & a_{43} & a_{44} \\\\  \\hline\na_{51}& a_{52} & a_{53} & a_{54} \\\\\na_{11}& a_{62} & a_{63} & a_{64} \n\\end{array}\n\\right]\n - \n\\begin{bmatrix}\nb_{11}& b_{12}  \\\\\nb_{21}& b_{22}  \\\\\nb_{31}& b_{32}  \n\\end{bmatrix}\n\\otimes\n\\begin{bmatrix}\nc_{11}& c_{12}  \\\\\nc_{21}& c_{22}  \n\\end{bmatrix} \\right|_F \\\\\n\\phi(B,C) =\n\\left|\n\\begin{bmatrix}\na_{11}& a_{21} & a_{12} & a_{22} \\\\ \\hline\na_{31}& a_{41} & a_{32} & a_{42} \\\\ \\hline\na_{51}& a_{61} & a_{52} & a_{62} \\\\ \\hline\na_{13}& a_{23} & a_{14} & a_{24} \\\\ \\hline \na_{33}& a_{43} & a_{34} & a_{44} \\\\ \\hline\na_{53}& a_{63} & a_{54} & a_{64} \n\\end{bmatrix}\n - \n\\begin{bmatrix}\nb_{11}  \\\\\nb_{21}  \\\\\nb_{31}  \\\\\nb_{12} \\\\\nb_{22} \\\\\nb_{32}\n\\end{bmatrix}\n\\begin{bmatrix}\nc_{11}&c_{21} & c_{12} & c_{22}\n\\end{bmatrix} \\right|_F\n$$\nNow the problem has turned into rank 1 approximation for a rectangular matrix. The solution is given by the singular value decomposition of $R  = USV^T$ in [1,2]. \n$$\nvec(B) = \\sqrt{\\sigma_1}u_1, \\quad vec(C) = \\sqrt{\\sigma_1}v_1\n$$   \nIf $R$ is a rank 1 matrix solution will be exact i.e. $A$ is full seperable.[3]\n[1] Golub G, Van Loan C. Matrix Computations, The John Hopkins University Pres. 1996\n[2] Van Loan C., Pitsianis N., Approximation with Kronecker Products, Cornell University, Ithaca, NY, 1992 \n[3] Genton MG. Separable approximations of space\u2013time covariance matrices. Environmetrics 2007; 18:681\u2013695.", "meta": {"post_id": 60399, "input_score": 22, "output_score": 41, "post_title": "Method to reverse a Kronecker product"}}
{"input": "This is a question that has been bothering me for quite a while. Let me put between quotation marks the terms that are used informally.\n\"Quotient objects\" are always the same. Take groups, abelian groups, rings, topological vector spaces for example. Inside every object there are certain \"subobjects\" that we can divide by: normal subgroups, subgroups, ideals and subspaces with the subspace topology, respectively.\nI (think I) know from an universal algebra viewpoint (of which I know nothing) that in the case of \"algebraic\" objects, the \"subobjects\" we take the quotient by are determined by congruences: equivalence relations on the cartesian product that respect the operation(s).\nThe first problem is:\n1) How to define categorically the \"subobjects\" by which we take quotients?\nNow, the resulting \"quotient object\" $G/N$ always satisfies the same universal property: it comes with a morphism $\\pi$ such that $\\pi:G\\to G/N$ and $N\\subset ker \\,\\pi$, which is universal with respect to this: any other such morphism factors through $\\pi$.\nThe second (intimately linked) problem is:\n2) How to define categorically these \"quotient objects\" and express in fancy terms the universal property they satisfy?\nNow, in \"algebraic\" categories there are well-known isomorphism theorems. In the Wikipedia link, it is explained that they are all a special case of the universal algebra statement.\n3) How can we express them in categorical terms?\nAny insightful answer, even if partial, will be very welcome.", "output": "First of all, let me say that there already well-defined notions of subobjects and quotient objects in category theory. I'll state them here for reference:\nDefinition. A subobject of an object $A$ in a category $\\mathbf{C}$ is an equivalence class of monomorphisms with codomain $A$, where we identify two monomorphisms $m : B \\to A$, $m' : B' \\to A$ as equivalent just if there is an isomorphism $f : B \\to B'$ such that $m = m' \\circ f$.\nDefinition. A quotient object of an object $A$ in a category $\\mathbf{C}$ is a subobject of $A$ in the opposite category $\\mathbf{C}^{\\textrm{op}}$. Explicitly, it is an equivalence class of epimorphisms with domain $A$, where we identify two epimorphisms $e : A \\to B$ and $e' : A \\to B'$ as equivalent just if there is an isomorphism $f : B' \\to B$ such that $e = f \\circ e'$.\nBut we shall see by means of examples that these aren't necessarily what we want them to be.\nExample. In the category of topological spaces, subspaces are subobjects. But so are other things. Indeed, if $B$ has a finer topology than $A$, then $B$ is also a subobject of $A$.\nExample. In the category of monoids, $\\mathbb{Z}$ is a quotient of $\\mathbb{N}$, because the natural inclusion map $\\mathbb{N} \\hookrightarrow \\mathbb{Z}$ is epic.\nThe problem is that the notions of monomorphism and epimorphism are a bit too general to capture the properties we want. So we should begin by defining some \u2018stronger\u2019 notions of mono/epi.\nDefinition. A regular monomorphism is an monomorphism which is an equaliser of some pair of parallel morphisms. A regular epimorphism is a coequaliser of some pair of parallel morphisms.\nExample. The kernel of a group/ring/module/etc. homomorphism is a regular monomorphism: indeed, $\\ker f$ is the equaliser of $f$ and the zero morphism. \nExample. Conversely, in an abelian category, every regular monomorphism is a kernel: indeed, the equaliser of $f, g : A \\to B$ is precisely the same thing as $\\ker (f-g)$, by the above observation.\nExample. In the category of topological spaces, the inclusion of a subspace $B \\hookrightarrow A$ is a regular monomorphism: it is the equaliser of its characteristic map $A \\to 2$ (where $2$ is given the indiscrete topology) and a constant map. Conversely, every regular monomorphism is (isomorphic to) the inclusion of a subspace.\nThis suggests we're on the right track. Let's look now at the epimorphisms.\nExample. The coimage of a group/ring/module/etc. homomorphism is a regular epimorphism: indeed, $\\operatorname{coim} f$ is the coequaliser of $\\ker f$ and the zero morphism. In particular, the projection maps onto quotient groups/rings/modules/etc. are regular epimorphisms. The converse is true in abelian categories by an argument similar to the one before.\nExample. Let $A / R$ be the quotient of a topological space $A$ by an equivalence relation $R$. Then the projection map $\\pi : A \\to A / R$ is a regular epimorphism. Recall that $R$ is a subset of the cartesian product $A \\times A$; so let us take $A \\times A$ with the induced product topology and topologise $R$ with subspace topology. Then we have reified $R$ as an object in $\\textbf{Top}$. Let $p_1, p_2 : R \\to A$ be the projections. It is not hard to verify that $\\pi : A \\to A / R$ is the coequaliser of $p_1$ and $p_2$, so it is indeed a regular epimorphism.\nUnfortunately, it turns out that in the category of groups, monomorphisms and regular monomorphisms are the same thing. (For a proof, see here.) So perhaps regular monomorphisms aren't a perfect fit for what we're looking for. Nonetheless, in sufficiently nice categories, we do have the following analogue of the first isomorphism theorem:\nTheorem. In any regular category, (the domain of) the image of a morphism is isomorphic (the codomain of) its regular coimage (the coequaliser of its kernel pair).\nExamples. Any abelian category is regular, as is any semiabelian category and any topos. \nSo I think this is probably the right level of generality, even though it doesn't cover the category of groups or the category of topological spaces. (In fact, the conclusion of the above theorem is blatantly false in the category of topological spaces: intuitively, the regular coimage is topologised by the domain, while the image is topologised by the codomain; so, for example, the map from a discrete space to an indiscrete space will have non-isomorphic image and regular coimage.)", "meta": {"post_id": 61062, "input_score": 46, "output_score": 41, "post_title": "Quotient objects, their universal property and the isomorphism theorems"}}
{"input": "I would like to prove that the equation $ 3^x+4^x=5^x $ has only one real solution ($x=2$)\nI tried to study the function $ f(x)=5^x-4^x-3^x $ (in order to use the intermediate value theorem) but I am not able to find the sign of $ f'(x)= \\ln(5)\\times5^x-\\ln(4)\\times4^x-\\ln(3)\\times3^x $ and I can't see any other method to solve this exercise...", "output": "One direct method is to divide directly by $5^x$ and get $1=(3/5)^x+(4/5)^x$. From here it is clear that the RHS is strictly decreasing, and there is a unique solution. Almost all exponential equations can be treated this way, by transforming them to\n\none increasing function equal to one decreasing function\none increasing/decreasing function equal to a constant.", "meta": {"post_id": 61812, "input_score": 25, "output_score": 43, "post_title": "Proving that $ 2 $ is the only real solution of $ 3^x+4^x=5^x $"}}
{"input": "I've read the proof for why $\\int_0^\\infty P(X >x)dx=E[X]$ for nonnegative random variables (located here) and understand its mechanics, but I'm having trouble understanding the intuition behind this formula or why it should be the case at all. Does anyone have any insight on this? I bet I'm missing something obvious.", "output": "For the discrete case, and if $X$ is nonnegative, $E[X] = \\sum_{x=0}^\\infty x P(X = x)$.  That means we're adding up $P(X = 0)$ zero times, $P(X = 1)$ once, $P(X = 2)$ twice, etc.  This can be represented in array form, where we're adding column-by-column:\n$$\\begin{matrix} P(X=1) & P(X = 2) & P(X = 3) & P(X = 4) & P(X = 5) & \\cdots \\\\ & P(X = 2) & P(X = 3) & P(X = 4) & P(X = 5) & \\cdots \\\\ & & P(X = 3) & P(X = 4) & P(X = 5) & \\cdots \\\\ & & & P(X = 4) & P(X = 5) & \\cdots \\\\ & &  &  & P(X = 5) & \\cdots\\end{matrix}.$$\nWe could also add up these numbers row-by-row, though, and get the same result.  The first row has everything but $P(X = 0)$ and so sums to $P(X > 0)$.  The second row has everything but $P(X =0)$ and $P(X = 1)$ and so sums to $P(X > 1)$.  In general, the sum of row $x+1$ is $P(X > x)$, and so adding the numbers row-by-row gives us $\\sum_{x = 0}^{\\infty} P(X > x)$, which thus must also be equal to $\\sum_{x=0}^\\infty x P(X = x) = E[X].$\nThe continuous case is analogous.\nIn general, switching the order of summation (as in the proof the OP links to) can always be interpreted as adding row-by-row vs. column-by-column.", "meta": {"post_id": 64186, "input_score": 88, "output_score": 144, "post_title": "Intuition behind using complementary CDF to compute expectation for nonnegative random variables"}}
{"input": "In the definition of smooth manifolds, complex manifolds, and similar constructions, one starts by defining a property on neighborhoods in the space, specifying how they relate on overlapping neighborhoods. An atlas is a set of such neighborhoods that covers the space. Some books (Lee, Warner) define the structure as the maximal atlas. Others define it as the equivalence class of compatible atlases.\nI was under the impression that the advantage of using the equivalence class definition instead of the maximal atlas definition was that the proof of the existence of such a maximal atlas requires Zorn's lemma, which some prefer not to use if not absolutely necessary.\nBut Lee and Warner's books both contain existence proofs for this maximal atlas; they start with any atlas, and then just take the set of all compatible charts. If that argument somehow relies on Zorn's lemma (or some other variant of choice), I can't see how. So what do you say? Is choice required, assumed for convenience but not required, or just not necessary at all?", "output": "Zorn's lemma is not required to prove the existence of a maximal atlas, though it is convenient. For one thing, we don't have to prove that compatibility of atlases is an equivalence relation. On the other hand, the obvious proof using Zorn's lemma requires some extra work to show that there is a unique maximal atlas containing any atlas. So let's do it without Zorn's lemma.\nDefinition. Two atlases on a manifold are compatible if their union is an atlas.\nLemma. Compatibility of atlases is an equivalence relation.\nProof. It is clear that compatibility is symmetric and reflexive, and it remains to be shown that compatibility is transitive. Let $\\mathcal{A}_1, \\mathcal{A}_2, \\mathcal{A}_3$ be three atlases on a $k$-manifold $M$, and suppose $\\mathcal{A}_1 \\cup \\mathcal{A}_2$ and $\\mathcal{A}_2 \\cup \\mathcal{A}_3$ are atlases. We wish to show $\\mathcal{A}_1 \\cup \\mathcal{A}_3$ is an atlas. So let $\\varphi_1 : U_1 \\to \\mathbb{R}^k$ be a chart in $\\mathcal{A}_1$, $\\varphi_3 : U_3 \\to \\mathbb{R}^k$ be a chart in $\\mathcal{A}_3$. $\\mathcal{A}_2$ is an atlas, so for each point $x$ in $U_1 \\cap U_3$ there is a chart $\\varphi_2 : U_2 \\to \\mathbb{R}^k$ in $\\mathcal{A}_2$ such that $x \\in U_2$; but $\\varphi_1$ and $\\varphi_2$ are compatible and $\\varphi_2$ and $\\varphi_3$ are compatible so we see that $\\varphi_1$ and $\\varphi_3$ are locally compatible at $x$. (Here, \u2018compatible\u2019 means that the transition map satisfies the relevant regularity condition. There may well be invocations of the axiom of choice hidden here, but I will assume that there are not.) Moreover, $x$ is arbitrary in $U_1 \\cap U_3$ so this shows $\\varphi_1$ and $\\varphi_3$ are compatible; and $\\varphi_1$ and $\\varphi_3$ are also arbitrary, so $\\mathcal{A}_1 \\cup \\mathcal{A}_3$ is an atlas.\nLemma. The class of atlases on a manifold is a set.\nProof. The class of atlases is a subclass of the set\n$$\\mathscr{P} \\left( \\bigcup_{U \\in \\mathscr{P}(M)} \\{ U \\to \\mathbb{R}^k \\} \\right)$$\nwhere $\\{ U \\to \\mathbb{R}^k \\}$ denotes the set of all functions $U \\to \\mathbb{R}^k$, so by the axiom of separation,  the class of atlases is a set.\nLemma. The union of arbitrarily many pairwise compatible atlases is an atlas.\nProof. Immediate.\nTheorem. Every atlas is contained in a unique maximal atlas.\nProof. From the above, it is clear that every atlas $\\mathcal{A}$ is contained in some equivalence class of atlases, and this equivalence class is a set of compatible atlases. Let $\\overline{\\mathcal{A}}$ be the union of all those atlases. Then $\\mathcal{A} \\subseteq \\overline{\\mathcal{A}}$, and $\\overline{\\mathcal{A}}$ is the unique maximal atlas containing $\\mathcal{A}$: for if $\\mathcal{A} \\subseteq \\mathcal{A}'$, then $\\mathcal{A}$ and $\\mathcal{A}'$ are compatible, so $\\mathcal{A}' \\subseteq \\overline{\\mathcal{A}}$ by construction.\n\nFor the sake of completeness I sketch a proof using Zorn's lemma.\nTheorem. Every atlas is contained in a maximal atlas.\nProof. The set of all atlases containing $\\mathcal{A}$ partially ordered by inclusion is a chain-complete poset: indeed, it is clear that if we have a chain $\\{ \\mathcal{A}_\\alpha \\}$, then $\\bigcup_\\alpha \\mathcal{A}_\\alpha$ is also an atlas. Thus, the hypotheses of Zorn's lemma are satisfied and there is some maximal atlas containing $\\mathcal{A}$.", "meta": {"post_id": 66554, "input_score": 49, "output_score": 47, "post_title": "Is Zorn's lemma required to prove the existence of a maximal atlas on a manifold?"}}
{"input": "When I read the definitions of material and logical implications, they seem to me pretty much equivalent.  Could someone give me an example illustrating the difference?\n(BTW, I have no problem with the equivalence between $\\lnot p \\vee q$ and $p \\to q$, aka \"if $p$ then $q$\".  My confusion is with the idea that there are two different forms of implication, material and logical.)\nThanks!", "output": "There is one level at which they can be distinguished. The following definitions are relatively common.\n\nMaterial implication is a binary connective that can be used to create new sentences; so $\\phi \\to \\psi$ is a compound sentence using the material implication symbol $\\to$. Alternatively, in some contexts, material implication is the truth function of this connective.\nLogical implication is a relation between two sentences $\\phi$ and $\\psi$, which says that any model that makes $\\phi$ true also makes $\\psi$ true.  This can be written as $\\phi \\models \\psi$, or sometimes, confusingly, as $\\phi \\Rightarrow \\psi$, although some people use $\\Rightarrow$ for material implication. \n\nIn this distinction, material implication is a symbol at the object level, while logical implication is a relation at the meta level. In other words, material implication is a function of the truth value of two sentences in one fixed model, but logical implication is not directly about the truth values of sentences in a particular model, it is about the relation between the truth values of the sentences when all models are considered. \nThere is a close relationship between the two notions in first-order logic. It is somewhat immediate from the definitions that if $\\phi \\to \\psi$ holds in every model then $\\phi \\models \\psi$, and conversely if $\\phi \\models \\psi$ then $\\phi \\to \\psi$ is true in every model.  This relationship becomes more fuzzy when we begin to look at other logics, and in particular it can be quite fuzzy when philosophers talk about material conditionals and logical implication independent of any formal system.", "meta": {"post_id": 68932, "input_score": 40, "output_score": 49, "post_title": "What's the difference between material implication and logical implication?"}}
{"input": "I read that there is a one-one correspondence between the ideals of $R/I$ and the ideals containing $I$. ($R$ is a ring and $I$ is any ideal in $R$)\nIs this bijection obvious? It's not to me. Can someone tell me what the bijection looks like explicitly? Many thanks for your help!", "output": "This is just one of the Isomorphism Theorems. It holds for groups, rings, modules, and in general any algebra (in the sense of universal algebra). The proofs are all the same; in fact, you can take the proof for groups and it will become a proof for rings mutatis mutandis.  Here it is, explicitly, for rings. \nLet $R$ be a ring, let $I$ be an ideal. The one-to-one correspondence between subrings of $R/I$ and subrings of $R$ that contain $I$ (which in fact also makes ideals correspond to ideals) is given as follows:\nLet $\\pi\\colon R\\to R/I$ be the canonical projection sending $r$ to the class $r+I$.\nGiven a subring $S$ of $R$ with $I\\subseteq S\\subseteq R$, we let \n$$\\pi(S) = \\{\\pi(s)\\mid s\\in S\\} = \\{s+I\\mid s\\in S\\}\\subseteq R/I.$$\nGiven a subring $T$ of $R/I$, we make it correspond to\n$$\\pi^{-1}(T) = \\{r\\in R\\mid \\pi(r)\\in T\\}.$$\n\n$\\pi(S)$ is a subring of $R/I$ whenever $S$ is a subring of $R$ that contains $I$. If $S$ is a (left, right, two-sided) ideal, then $\\pi(S)$ is a (left, right, two-sided) ideal of $R/I$.\nProof. $0\\in S$, so $\\pi(0) = 0+I \\in \\pi(S)$, hence $\\pi(S)$ is not empty. Also, if $(s+I),(t+I)\\in \\pi(S)$, with $s,t\\in S$, then $s-t\\in S$, so $(s+I)-(t+I) = (s-t)+I = \\pi(s-t)\\in \\pi(S)$. Thus, $\\pi(S)$ is a subgroup of $R/I$. And if $s+I,t+I\\in\\pi(S)$, with $s,t\\in S$, then $(s+I)(t+I) = st+I = \\pi(st)\\in \\pi(S)$ (since $S$ is a subring of $R$), so $\\pi(S)$ is a subring.\nIf in addition $S$ is a (left) ideal of $R$, then given $(s+I)\\in \\pi(S)$ and $(a+I)\\in R/I$, with $s\\in S$, we have $(a+I)(s+I) = as+I = \\pi(as)$; since $S$ is a (left) ideal, $s\\in S$ and $a\\in R$, then $as\\in S$, so $\\pi(as)\\in \\pi(S)$. Similar arguments establish the right and two-sided cases.\nIf $T$ is a subring of $R/I$, then $\\pi^{-1}(T)$ is a subring of $R$ that contains $I$. If $T$ is a (left, right, two-sided) ideal, then so is $\\pi^{-1}(T)$.\nProof. $0+I\\in T$, and since for all $a\\in I$, $\\pi(a)=a+I = 0+I\\in T$, then $a\\in \\pi^{-1}(T)$. Thus, $\\pi^{-1}(T)$ contains $I$. If $r,s\\in \\pi^{-1}(T)$, then so are $r-s$ and $rs$, since $\\pi(r-s) = (r-s)+I = (r+I)-(s+I)\\in T$ (since $r+I,s+I\\in T$ and $T$ is a subring) and $\\pi(rs) = rs+I = (r+I)(s+I)\\in T$ (since $T$ is closed under products and $r+I,s+I\\in T$). Thus, $\\pi^{-1}(T)$ is a subring of $R$.\nIf $T$ is a left ideal of $R/I$, and $s\\in\\pi^{-1}(T)$, $a\\in R$, then $\\pi(s)\\in T$, so $\\pi(as) = \\pi(a)\\pi(s)\\in T$ (since $T$ is a left ideal), so $as\\in\\pi^{-1}(T)$. Thus, $\\pi^{-1}(T)$ is a left ideal of $R$. Similar arguments establish the right and two-sided cases.\nThe correspondences are inverses of each other, hence they are bijections.\nProof.  If $(\\pi^{-1}\\circ\\pi)$ and $(\\pi\\circ\\pi^{-1})$ are both the identity, then $\\pi$ is an isomorphism.\nLet $S$ be an ideal of $R$ that contains $I$. Then $S\\subseteq \\pi^{-1}(\\pi(S))$ holds, because it holds for any subset and any function. Now let $a\\in \\pi^{-1}(\\pi(S))$. then $\\pi(a)\\in \\pi(S)$, so there exists $s\\in S$ such that $\\pi(a)=\\pi(s)$; hence $\\pi(a-s)\\in\\mathrm{ker}(\\pi) = I$. Thus, $a-s\\in I\\subseteq S$. Since $a-s,s\\in S$, and $S$ is a subring of $R$, then $a=(a-s)+s\\in S$. Thus, $\\pi^{-1}(\\pi(S))\\subseteq S$, proving $(\\pi^{-1}\\circ\\pi)=\\text{id}$.\nConversely, if $T$ is an ideal of $R/I$, then $\\pi(\\pi^{-1}(T))=T$, because $\\pi$ is onto and this equality holds for any surjective function. $\\Box$\nThe correspondences are inclusion-preserving.\nProof. For any function $f\\colon X\\to Y$ and subsets $A,B\\subseteq X$, if $A\\subseteq B$ then $f(A)\\subseteq f(B)$; and for any subsets $C,D$ of $Y$, if $C\\subseteq D$ then $f^{-1}(C)\\subseteq f^{-1}(D)$, so this follows from purely set-theoretic considerations.", "meta": {"post_id": 69578, "input_score": 22, "output_score": 41, "post_title": "Bijection between ideals of $R/I$ and ideals containing $I$"}}
{"input": "A number is an \"algebraic integer\" if it is the root to a monic polynomial with integer coefficients. Artin says (Algebra, p. 411):\n\nThe concept of algebraic integer was one of the most important discoveries of number theory. It is not easy to explain quickly why it is the right definition to use, but roughly speaking, we can think of the leading coefficient of the primitive irreducible polynomials $f(x)$ as a \"denominator.\" If $\\alpha$ is the root of an integer polynomial $f(x)=dx^n+a_{n-1}x^{n-1}...$ then $d\\alpha$ is an algebraic integer, because it is a root of the monic integer polynomial $x^n + a_{n-1}x^{n-1} + ... + d^{n-1}a_0$.\nThus we can \"clear the denominator\" in any algebraic number by multiplying it with a suitable integer to get an algebraic integer.\n\nWhen I first learned of algebraic integers, I looked online and saw some hints that maybe they were used to prove the Abel-Ruffini theorem. So I put off questioning their usage for a while; I now think I understand one proof of this theorem (the one at the end of Artin's Algebra) and it has nothing to do with algebraic integers (that I can tell).\nSo basically: why is it important if a number is an algebraic integer? I think I understand what he's saying about the relationship between roots of integer polynomials and algebraic integers, but I fail to see why this is \"one of the most important discoveries of number theory.\"", "output": "Suppose that we desire to consider as \"integers\" some subring $\\:\\mathbb I\\:$ of the field of all algebraic numbers. To be a purely algebraic notion, it cannot distinguish between conjugate roots, so if $\\rm\\:\\alpha,\\alpha'$ are roots of the same polynomial irreducible over $\\rm\\:\\mathbb Q\\:,\\:$ then $\\rm\\:\\alpha\\in\\mathbb I\\iff \\alpha'\\in\\mathbb I\\:.\\:$ Also we desire $\\rm\\:\\mathbb I\\cap \\mathbb Q = \\mathbb Z\\ $ so that our notion of algebraic integer is a faithful extension of the notion of a rational integer. Now suppose that $\\rm\\:f(x)\\:$ is the monic minimal polynomial over $\\rm\\:\\mathbb Q\\:$ of an algebraic \"integer\" $\\rm\\:\\alpha\\in \\mathbb I\\:.\\:$ Then $\\rm\\:f(x) = (x-\\alpha)\\:(x-\\alpha')\\:(x-\\alpha'')\\:\\cdots\\:$ has coefficients in $\\rm\\:\\mathbb I\\cap \\mathbb Q = \\mathbb Z\\:.\\:$ Therefore the monic minimal polynomial of elements $\\in\\mathbb I\\:$ must have coefficients $\\in\\mathbb Z\\:.\\:$ Conversely, one easily shows that the set of all such algebraic numbers contains $1$ and is closed under both difference and multiplication, so it forms a ring. Moreover, as Artin's quote shows, the quotient field of $\\rm\\:\\mathbb I\\:$ is the field of all algebraic numbers. Hence a few natural hypotheses on the notion of an algebraic integer imply the standard criterion in terms of minimal polynomials.\nBecause this notion of integer faithfully extends the notion of rational integers, we can employ algebraic integers to deduce results about rational integers. This often results in great simplifications because many diophantine equations become simpler - being \"linearized\" - when one factors them in algebraic extension fields. For example, see proofs about Pythagorean triples using Gaussian integers, or classical proofs of FLT for small exponents employing algebraic integers.", "meta": {"post_id": 70088, "input_score": 27, "output_score": 36, "post_title": "Why do we use this definition of \"algebraic integer\"?"}}
{"input": "I am new to differential geometry and I am trying to understand Gaussian curvature. The definitions found at Wikipedia and Wolfram sites are too mathematical. Is there any intuitive way to understand Gaussian curvature?", "output": "I know you're looking for an intuitive explanation, but I've always believed that intuition ought to come from concrete mathematical facts, if possible.  Otherwise, you have no way of knowing whether or not the intuition someone feeds you actually matches the formal mathematics.  (On that note, +1 for Joseph O'Rourke's answer.)\nThe Gaussian curvature, $K$, is given by $$K = \\kappa_1 \\kappa_2,$$ where $\\kappa_1$ and $\\kappa_2$ are the principal curvatures.  Just from this definition, we know a few things:\n\nFor $K$ to be a large positive number, then $\\kappa_1$ and $\\kappa_2$ should both be large and have the same sign (i.e. both positive or both negative).\nFor $K$ to be zero, either $\\kappa_1 = 0$ or $\\kappa_2 = 0$.\nFor $K$ to be a large negative number, then $\\kappa_1$ and $\\kappa_2$ should both be large but have opposite signs.\n\nNow recall that $\\kappa_1(p)$ and $\\kappa_2(p)$ are the maximum and minimum normal curvatures of all curves passing through $p.$  So:\n\n$K(p) > 0$ means that the curves through $p$ of extremal normal curvature \"curve the same way\" (such as the red curve and the green curve).  So, points in the purple region have $K > 0$.  In some sense, the surface is shaped like an elliptic paraboloid there (like a bowl).\n$K(p) = 0$ means that one of the curves through $p$ of extremal curvature has zero normal curvature (such as the yellow curve).  So, points along the yellow curve have $K = 0$.  In some sense, the surface is shaped like a parabolic cylinder there (like a bent piece of paper).\n$K(p) < 0$ means that the curves through $p$ of extremal curvature \"curve in opposite ways\" (such as the blue curve and green curve).  For example, points in the gray region have $K < 0$.  In some sense, the surface is shaped like a hyperbolic paraboloid there (a saddle).\n\n\nIn fact, using Dupin's Indicatrix (which is really just a 2nd-order Taylor expansion) we can make rigorous the notion of being \"locally like\" an elliptic paraboloid, a cylinder, or a hyperbolic paraboloid.", "meta": {"post_id": 70210, "input_score": 87, "output_score": 38, "post_title": "Is there any easy way to understand the definition of Gaussian Curvature?"}}
{"input": "I'm looking for subset $A$ of $\\mathbb R$ such that $A$ is a Borel set but $A$ is neither $F_\\sigma$ nor $G_\\delta$.", "output": "There are many examples. Here's one:\nObserve first that the rational numbers $\\mathbb{Q}$ are an $F_{\\sigma}$. This is because they are a countable union of points. The irrational numbers $\\mathbb{R} \\smallsetminus \\mathbb{Q} = \\bigcap_{q \\in \\mathbb{Q}} \\mathbb{R} \\smallsetminus \\{q\\}$ are thus a $G_{\\delta}$. Since both $\\mathbb{Q}$ and $\\mathbb{R} \\smallsetminus \\mathbb{Q}$ are dense and disjoint it follows from the Baire category theorem that $\\mathbb{Q}$ cannot be a $G_{\\delta}$. [Edit: See also this thread here containing several proofs that $\\mathbb{Q}$ can't be a $G_{\\delta}$ in $\\mathbb{R}$. These proofs explicitly avoid Baire].\nThe same reasoning shows that $F = \\mathbb{Q}_{\\geq 0}$ is an $F_{\\sigma}$ in $[0,\\infty)$, but isn't a $G_{\\delta}$, and that $G= \\mathbb{R}_{\\leq 0} \\smallsetminus \\mathbb{Q}_{\\leq 0}$ is a $G_{\\delta}$ in $(-\\infty,0]$, but isn't an $F_{\\sigma}$. Their union $F \\cup G$ is then an example of a Borel subset of $\\mathbb{R}$ which is neither an $F_{\\sigma}$ nor a $G_{\\delta}$ because if it were an $F_{\\sigma}$ then the same would hold for $G = (F \\cup G) \\cap (-\\infty,0)$, for example. I leave it as an exercise to show that $F \\cup G$ is both an $F_{\\sigma\\delta}$ and a $G_{\\delta\\sigma}$.\nThat's probably the easiest example. A few more (both more interesting but also more involved ones) can be found in this MO thread.\n\nFor a much more in-depth discussion of such ideas, I recommend looking into one of the following books:\n\nA.S. Kechris, Classical Descriptive Set Theory, Springer GTM\u00a0156.\nS.M. Srivastava, A course on Borel sets, Springer GTM\u00a0180.\nJ.C. Oxtoby, Measure and Category, Springer GTM\u00a02.\n\nSpecifically, look up the sections on the Borel hierarchy.", "meta": {"post_id": 73296, "input_score": 52, "output_score": 57, "post_title": "Example of a Borel set that is neither $F_\\sigma$ nor $G_\\delta$"}}
{"input": "I found this problem in a textbook of abstract algebra:\n\nLet $H$ be a subgroup of $G$. Prove that $$\\{x\\in G:xHx^{-1}\\subseteq H\\}$$ is a subgroup of $G$.\n\nIt's easy to prove that the set is closed under multiplication, but I'm stuck on proving that it is closed under inverses.\nIf $H$ is finite, say $H=\\{a_1,\\ldots,a_n\\}$, suppose $x$ is an element of the set. Then $xa_1x^{-1},\\ldots,xa_nx^{-1}$ are all distinct, hence they are exactly $a_1,\\ldots,a_n$, in some order. Therefore any element $b\\in H$ can be written as $xcx^{-1}$ for some $c\\in H$, and hence $x^{-1}bx=x^{-1}(xcx^{-1})x=c$ is also in $H$. So $x^{-1}$ is also an element of the set.\nHowever, the above method does not work if $H$ is infinite. The main idea is to prove that $x^{-1}ax\\in H$ for every $a\\in H$, given that $xax^{-1}\\in H$ for every $a\\in H$. I was trying to do some substitutions of $a$ to get the required result, but I can't seem to get the $x^{-1}$ to the left.\nAny help would be appreciated. It may be worth mentioning that I just started learning this group theory thing for a few days, so please adjust your explanation accordingly.\nThanks in advance.", "output": "The reason you are having trouble proving it is that it is not true as stated. \nFor a heavy-handed example, let $G$ be the free group on $x$ and $y$, and let $H$ be the subgroup generated by all elements of the form $x^nyx^{-n}$ with $n\\gt 0$. \nThen for any $a\\in H$ we have $xax^{-1}\\in H$. However, $x^{-1}yx\\notin H$, because any element of $H$ is a word that starts with a nonnegative power of $x$.\nTo fix the problem, you would need to require $xHx^{-1}=H$, rather than $xHx^{-1}\\subseteq H$. Then your argument would go through in the infinite case as well.\n\nOkay, here's an example you can get your hands on (courtesy my Math 257 notes with T.Y. Lam, Spring 97):\nLet $G$ be the group of all invertible $2\\times 2$ matrices with coefficients in $\\mathbb{Q}$, $G=\\mathrm{GL}_2(\\mathbb{Q})$.\nLet $H$ be the subgroup given by\n$$ H = \\left\\{\\left.\\left(\\begin{array}{cc}\r\n1 & m\\\\\r\n0 & 1\\end{array}\\right)\\in G\\ \\right|\\ m\\in\\mathbb{Z}\\right\\}.$$\nLet \n$$x = \\left(\\begin{array}{cc}\r\n2 & 0\\\\\r\n0 & 1\\end{array}\\right), \\qquad x^{-1} = \\left(\\begin{array}{cc}\r\n\\textstyle\\frac{1}{2} & 0 \\\\\r\n0 & 1\r\n\\end{array}\\right).$$\nThen for every $m\\in\\mathbb{Z}$ we have:\n$$\\begin{align*}\r\n\\left(\\begin{array}{cc}\r\n2 & 0\\\\ 0 & 1\\end{array}\\right)\\left(\\begin{array}{cc}1 & m\\\\0 & 1\\end{array}\\right) \\left(\\begin{array}{cc}\\textstyle\\frac{1}{2}&0\\\\0&1\\end{array}\\right)\r\n&= \\left(\\begin{array}{cc}\r\n2 & 2m\\\\0 & 1\\end{array}\\right)\\left(\\begin{array}{cc}\r\n\\textstyle\\frac{1}{2}&0\\\\0 & 1\\end{array}\\right)\\\\\r\n&= \\left(\\begin{array}{cc}\r\n1 & 2m\\\\\r\n0 & 1\\end{array}\\right)\\in H.\r\n\\end{align*}$$\nSo $xHx^{-1}\\subseteq H$. However, even though\n$$ \\left(\\begin{array}{cc}1&1\\\\0&1\\end{array}\\right)\\in H,$$\nwe have\n$$\\begin{align*}\r\nx^{-1}\\left(\\begin{array}{cc}1&1\\\\0&1\\end{array}\\right)x &= \\left(\\begin{array}{cc}\r\n\\textstyle\\frac{1}{2} & 0\\\\0 &1\\end{array}\\right)\\left(\\begin{array}{cc}1 & 1\\\\ 0 & 1\\end{array}\\right)\\left(\\begin{array}{cc}2 & 0\\\\0&1\\end{array}\\right)\\\\\r\n&= \\left(\\begin{array}{cc}\r\n\\textstyle\\frac{1}{2}&\\textstyle\\frac{1}{2}\\\\0&1\\end{array}\\right)\\left(\\begin{array}{cc}2&0\\\\0&1\r\n\\end{array}\\right)\\\\\r\n&= \\left(\\begin{array}{cc}\r\n1 & \\textstyle\\frac{1}{2}\\\\0&1\\end{array}\\right)\\notin H.\r\n\\end{align*}$$\nSo $x\\in \\{g\\in G\\mid ghg^{-1}\\in H\\text{ for all }h\\in H\\}$, but $x^{-1}\\notin\\{g\\in G\\mid ghg^{-1}\\in H\\text{ for all }h\\in H\\}$. So the set need not be closed under inverses.", "meta": {"post_id": 75613, "input_score": 30, "output_score": 38, "post_title": "The set of all $x$ such that $xHx^{-1}\\subseteq H$ is a subgroup, when $H\\leq G$"}}
{"input": "How to show that $x^4+1$ is irreducible in $\\mathbb Z[x]$ but it is reducible modulo every prime $p$?\n\nFor example I know that $x^4+1=(x+1)^4\\bmod 2$. Also $\\bmod 3$ we have that $0,1,2$ are not solutions of $x^4+1=0$ then if it is reducible the factors are of degree $2$. This gives that $x^4+1=(x^2+ax+b)(x^2+cx+d)$ and solving this system of equations $\\bmod 3$  gives that $x^4+1=(x^2+x+2) (x^2+2x+2) \\pmod 3$. But is there a simpler method to factor $x^4+1$ modulo a prime $p$?", "output": "If $-1$ is a square in $\\Bbb F_p$ (which includes the case $p=2$), say $a^2=-1$, then we have $$X^4+1=X^4-a^2=(X^2+a)(X^2-a).$$\nIf $p$ is odd and $2$ is a square in $\\Bbb F_p$, say $2=b^2$, then we have\n$$X^4+1=(X^2+1)^2-(bX)^2=(X^2+bX+1)(X^2-bX+1) $$\nIf $p$ is odd and neither $-1$ nor $2$ is a square, then their product $-2$ is a square, say $-2=c^2$. (Without using anything even remotely as deep as quadratic reciprocity, this follows immediately from the fact that $\\Bbb F_p^\\times$ is a cyclic group of even order). Then we have\n$$ X^4+1=(X^2-1)^2-(cX)^2=(X^2-cX-1)(X^2+cX-1)$$", "meta": {"post_id": 77155, "input_score": 50, "output_score": 68, "post_title": "Irreducible polynomial which is reducible modulo every prime"}}
{"input": "Let $f(n)$ denote the order of the smallest finite group which cannot be generated with less than $n$ elements. Trivially $f(n) \\leq 2^n$ since ${\\mathbb F}_2^n$ can be seen as a vector space with dimension $n$. Is the exact value of $f(n)$ known?", "output": "$f(n) = 2^n$\nLet $X$ be the set of n generators of the finite group $G$ where $G$ cannot be generated by $n-1$ elements. Consider a sequence of subgroups of $G$, the first subgroup being $G$ itself generated by all of $X$, each of the next subgroups generated by one fewer generator than the last, finally down the trivial group. Each subgroup in the sequence must have distinct order which divides the order of the previous subgroup in the sequence. Thus the order of $G$ must be the product of $n$ terms all greater than one. Thus the smallest possible order of $G$ is $2^n$.", "meta": {"post_id": 78179, "input_score": 33, "output_score": 35, "post_title": "Smallest order for finite group that needs many elements to generate it"}}
{"input": "Possible Duplicate:\nOrder of elements in abelian groups \n\n\nLet $G$ be an abelian group and suppose that $G$ has elements of orders $m$ and $n$, respectively. Prove that $G$ has an element whose order is the least common multiple of $m$ and $n$.\n\nI've attempted this problem for quite some time, but didn't seem to get anywhere.\nFirst, let $a$ and $b$ be the elements whose orders are $m$ and $n$, respectively. I guessed that we can find the element of order $lcm(m,n)$ explicitly, instead of simply proving its existence. Furthermore, I also guessed that the element can be expressed in the form $a^kb^l$, because the statement must also hold when $G$ is generated by $a$ and $b$.\nThen I let $k$ be the smallest positive integer such that $a^k$ is a power of $b$, say $a^k=b^l$. Then I proved that $l$ is also the smallest positive integer such that $b^l$ is a power of $a$, and that $ml=nk$. I'm not sure whether it's correct though.\nThen I tried to find the order of ab. I can prove that the order is divisible by $\\frac{lcm(m,n)}{\\gcd(m,n)}$, but I can't prove whether it is equal to $lcm(m,n)$. Apparently, taking any $a^ib^j$ won't be any better. And now, I'm at wits end.\nPlease tell me whether I'm on the right path. If not, please give me some adequate hints so I can work on it.", "output": "First we consider the case where $(m,n)=1$.\nBecause $(m,n)=1$ the least common multiple of $m$ and $n$ is $mn$. \nConsider the element $ab$.\nBecause $G$ is Abelian, $$(ab)^{mn} = \\overbrace{ab\\ ab\\ ab \\ldots ab}^{mn} = \\overbrace{aaa\\ldots a}^{mn} \\overbrace{bbb\\ldots b}^{mn} = a^{mn}b^{mn} = ee =e$$\nso the order of $ab$ is at most $mn$.\nSay the order of $ab$ is $k$, then we just showed that $k \\leq mn$.\nWe see that $$e= (ab)^k = (ab)^{km} = a^{km}b^{km} = eb^{km} = b^{km} $$\nso $n$ divides $km$. But since $(m,n)=1$ it follows that $n$ divides $k$.\nSimilarly  $$e=(ab)^k = (ab)^{kn} = a^{kn}b^{kn} = a^{kn}e = a^{kn} $$ so $m$ divides $kn$, and so $m$ divides $k$.\nThus $mn$ divides $k$.\nAnd since $k \\leq mn$ it follows that $k = mn$, and so $ab$ has order $mn$, the lowest common multiple of $m$ and $n$.\nWe need to take from this the fact that the order of a product of elements that have relatively prime orders is the product of the orders of those elements.\nNow we consider the case where $m$ and $n$ are not relatively prime.\nCall $L$ the least common multiple of $m$ and $n$.\nWe write $L=p^{r_1}_1\\cdots p^{r_s}_s$ for distinct primes $p_i$ and strictly positive powers $r_i$.\nIf we could find an element of $G$ with order $p^{r_i}_i$ for every $i$, then by the first half of this proof, the product of these elements would have order $L$ because prime powers are all relatively prime to prime powers of different primes.\nLet $i$ with $1 \\leq i \\leq s$ be given.\nWe note that $p^{r_i}_i$ divides either $m$ or $n$.\nThus $a^{m/p^{r_i}_i}$ or $b^{n/p^{r_i}_i}$ (whichever one divides evenly) has order $p^{r_i}_i$.\nTherefore we have shown that an element of order $L$, the least common multiple of $m$ and $n$, is in $G$.\nThe statement is not necessarily true if $G$ is not Abelian.\nConsider $G=S_3$.\nThis group contains a cycle of order 3, and a transposition (of order 2), but no element of order 6.", "meta": {"post_id": 78544, "input_score": 18, "output_score": 35, "post_title": "If ord$(a)=m$, ord$(b)=n$ then does there exist  $c$ such that ord $(c)=lcm(m,n)$?"}}
{"input": "Can the determinant (assuming it's non-zero) be used to determine that the vectors given are linearly independent, span the subspace and are a basis of that subspace? (In other words assuming I have a set which I can make into a square matrix, can I use the determinant to determine these three properties?)\nHere are two examples:\n\nSpan\nDoes the following set of vectors span $\\mathbb R^4$: $[1,1,0,0],[1,2,-1,1],[0,0,1,1],[2,1,2,-1]$? Now the determinant here is $1$, so the set of vectors span $\\mathbb R^4$.\nLinear Independence Given the following augmented matrix:\n\n$$\\left[\\begin{array}{ccc|c}\r\n1 & 2  & 1 & 0 \\\\\r\n0 & -1 & 0 & 0 \\\\\r\n0 &  0 & 2 & 0 \r\n\\end{array}\\right],\r\n$$\nwhere again the determinant is non-zero ($-2$) so this set S is linearly independent.\nOf course I am in trouble if you can't make a square matrix - I figure for spans you can just rref it, and I suppose so for linear independence and basis?", "output": "Most introductory books on Linear Algebra have a Theorem which says something like\nLet $A$ be a square $n \\times n$ matrix. Then the following are equivalent:\n\n$A$ is invertible.\n$\\det(A) \\neq 0$.\nThe columns of $A$ are linearly independent.\nThe columns of $A$ span $R^n$.\nThe columns of $A$ are a basis in  $R^n$.\nThe rows of $A$ are linearly independent.\nThe rows of $A$ span $R^n$.\nThe rows of $A$ are a basis in  $R^n$.\nThe reduced row echelon form of $A$ has a leading 1 in each row.\nand many other conditions.....\n\nWhat does this mean, it simply means that if you want to check if any of these conditions is true or false, you can simply pick whichever other condition from the list and check it instead..\nYour question is: Can instead of third or fourth condition, check the second? That's exactly what the Theorem says: YES.", "meta": {"post_id": 79356, "input_score": 34, "output_score": 67, "post_title": "Using the Determinant to verify Linear Independence, Span and Basis"}}
{"input": "Let $c$ denote the space of convergent sequences in $\\mathbb C$, $c_0\\subset c$ be the space of all sequences that converge to $0$. Given the uniform metric, both of them can be made into Banach spaces. It can be shown that the dual spaces of them are isometrically isomorphic, i.e. $c^*\\cong c_0^*$. Are $c$ and $c_0$ isometrically isomorphic? If not, how can one show the absence of such a isometric isomorphism? Thanks!", "output": "The closed unit ball of $c_0$ has no extreme points. The closed unit ball of $c$ has many extreme points, such as $(1,1,\\ldots)$. Since the property of being an extreme point is preserved by isometries, $c$ and $c_0$ are not isometrically isomorphic.", "meta": {"post_id": 80727, "input_score": 55, "output_score": 55, "post_title": "Are these two Banach spaces isometrically isomorphic?"}}
{"input": "I understand the whole concept of Rencontres numbers but I can't understand how to prove this equation\n$$D_{n,0}=\\left[\\frac{n!}{e}\\right]$$\nwhere $[\\cdot]$ denotes the rounding function (i.e., $[x]$ is the integer nearest to $x$). This equation that I wrote comes from solving the following recursion, but I don't understand how exactly the author calculated this recursion.\n$$\\begin {align*} \nD_{n+2,0} & =(n+1)(D_{n+1,0}+D_{n,0}) \\\\\nD_{0,0} & = 1 \\\\\nD_{1,0} & = 0 \n\\end {align*}\n$$", "output": "Derangements:\nA Derangement is a permutation, $P$, in which no element is mapped to itself; that is, $P(k)\\ne k$, for $1\\le k\\le n$. Let $\\mathcal{D}(n)$ be the number of derangements of $n$ items.\nHere are a few methods of computing $\\mathcal{D}(n)$.\nMethod 1 (build from smaller derangements):\nLet us count the number of derangements of $n$ items so that $P(P(n))=n$.  There are $n-1$ choices for $P(n)$, and for each of those choices, $\\mathcal{D}(n-2)$ ways to arrange the other $n-2$ items. Thus, there are $(n-1)\\mathcal{D}(n-2)$ derangements of $n$ items so that $P(P(n))=n$.\nLet us count the number of derangements of $n$ items so that $P(P(n))\\not=n$. There are $n-1$ choices for $P(n)$, and for each choice, there is a derangement of $n-1$ items identical to $P$ except that they map $P^{-1}(n)\\to P(n)$. Thus, there are $(n-1)\\mathcal{D}(n-1)$ derangements of $n$ items so that $P(P(n))\\not=n$.\nTherefore,\n$$\n\\mathcal{D}(n)=(n-1)(\\mathcal{D}(n-1)+\\mathcal{D}(n-2))\\tag{1}\n$$\nMethod 2 (count permutations):\nCount the number of permutations of $n$ items by counting how many fix exactly $k$ items.\nThere are $\\binom{n}{k}$ ways to choose the $k$ items to fix, then $\\mathcal{D}(n-k)$ ways to arrange the $n-k$ items that are not fixed. Since there are $n!$ permutations of $n$ items, we get\n$$\nn!=\\sum_{k=0}^n\\binom{n}{k}\\mathcal{D}(n-k)\\tag{2}\n$$\nand therefore, rearranging $(2)$ yields\n$$\n\\mathcal{D}(n)=n!-\\sum_{k=1}^n\\binom{n}{k}\\mathcal{D}(n-k)\\tag{3}\n$$\nMethod 3 (inclusion-exclusion):\nLet $S_i$ be the set of permutations of $n$ items which fix item $i$. Then the number of permutations in $k$ of the $S_i$ would be the number of permutations that fix $k$ items.  There are $\\binom{n}{k}$ ways to choose the $k$ items to fix, and $(n-k)!$ ways to arrange the other $n-k$ items. Thus, the number of permutations that fix at least $1$ item would be\n$$\n\\sum_{k=1}^n(-1)^{k-1}\\binom{n}{k}(n-k)!=\\sum_{k=1}^n(-1)^{k-1}\\frac{n!}{k!}\\tag{4}\n$$\nSince there are $n!$ permutations in total, the number of permutations that don't fix any items is\n$$\n\\begin{align}\n\\mathcal{D}(n)\n&=n!-\\sum_{k=1}^n(-1)^{k-1}\\frac{n!}{k!}\\\\\n&=\\sum_{k=0}^n(-1)^k\\frac{n!}{k!}\\tag{5}\\\\\n&\\approx \\frac{n!}{e}\n\\end{align}\n$$\nIn fact, the difference\n$$\n\\begin{align}\n\\left|\\frac{n!}{e}-\\mathcal{D}(n)\\right|\n&=\\left|\\sum_{k=n+1}^\\infty(-1)^k\\frac{n!}{k!}\\right|\\\\\n&=\\left|\\frac{1}{n+1}-\\frac{1}{(n+1)(n+2)}+\\frac{1}{(n+1)(n+2)(n+3)}-\\dots\\right|\\\\\n&<\\frac{1}{n+1}\\tag{6}\n\\end{align}\n$$\nThis method yields directly that $\\mathcal{D}(n)$ is the closest integer to $\\frac{n!}{e}$ for $n>0$.\nDerivation of the Closed Form from the Recursion:\nGiven $\\mathcal{D}(0)=1$ and $\\mathcal{D}(1)=0$, and the recursion $(1)$, let's derive $(5)$. Subtracting $n\\mathcal{D}(n-1)$ from both sides of $(1)$ yields\n$$\n\\mathcal{D}(n)-n\\mathcal{D}(n-1)=-(\\mathcal{D}(n-1)-(n-1)\\mathcal{D}(n-2))\\tag{7}\n$$\nUsing the initial conditions, $(7)$ implies\n$$\n\\mathcal{D}(n)-n\\mathcal{D}(n-1)=(-1)^n\\tag{8}\n$$\nDividing both sides of $(8)$ by $n!$ yields\n$$\n\\frac{\\mathcal{D}(n)}{n!}-\\frac{\\mathcal{D}(n-1)}{(n-1)!}=\\frac{(-1)^n}{n!}\\tag{9}\n$$\nEquation $(9)$ is very simple to solve for $\\frac{\\mathcal{D}(n)}{n!}$:\n$$\n\\frac{\\mathcal{D}(n)}{n!}=\\sum_{k=0}^n\\frac{(-1)^k}{k!}+C\\tag{10}\n$$\nPlugging $n=0$ into equation $(10)$ yields that $C=0$. Therefore,\n$$\n\\mathcal{D}(n)=n!\\sum_{k=0}^n\\frac{(-1)^k}{k!}\\tag{11}\n$$\n\nIncomplete Gamma Function:\n$$\n\\begin{align}\n\\mathcal{D}(n)\n&=n!\\sum_{k=0}^n\\frac{(-1)^k}{k!}\\tag{12a}\\\\\n&=\\sum_{k=0}^n(-1)^k\\frac{n!}{k!}\\tag{12b}\\\\\n&=\\sum_{k=0}^n(-1)^k\\binom{n}{k}(n-k)!\\tag{12c}\\\\\n&=\\sum_{k=0}^n(-1)^k\\binom{n}{k}\\int_0^\\infty x^{n-k}e^{-x}\\,\\mathrm{d}x\\tag{12d}\\\\\n&=\\int_0^\\infty\\sum_{k=0}^n\\binom{n}{k}(-1)^kx^{n-k}e^{-x}\\,\\mathrm{d}x\\tag{12e}\\\\\n&=\\int_0^\\infty(x-1)^ne^{-x}\\,\\mathrm{d}x\\tag{12f}\\\\\n&=\\frac1e\\int_{-1}^\\infty x^ne^{-x}\\,\\mathrm{d}x\\tag{12g}\\\\\n&=\\frac1e\\Gamma(n+1,-1)\\tag{12h}\n\\end{align}\n$$\nExplanation:\n$\\text{(12a):}$ $(11)$\n$\\text{(12b):}$ bring the factor of $n!$ inside the sum\n$\\text{(12c):}$ $\\frac{n!}{k!}=\\binom{n}{k}(n-k)!$\n$\\text{(12d):}$ $n!=\\int_0^\\infty x^ne^{-x}\\,\\mathrm{d}x$\n$\\text{(12e):}$ swap the finite sum and the integral\n$\\text{(12f):}$ apply the Binomial Theorem\n$\\text{(12g):}$ substitute $x\\mapsto x+1$\n$\\text{(12h):}$ $\\Gamma(n,s)=\\int_s^\\infty x^{n-1}e^{-x}\\,\\mathrm{d}x$ is the Incomplete Gamma Function\nNegative Integer Arguments:$\\newcommand{\\Ei}{\\operatorname{Ei}}\\newcommand{\\PV}{\\operatorname{PV}}$\n$$\n\\begin{align}\n\\mathcal{D}(-1)\n&=\\frac1e\\int_{-1}^\\infty\\frac{e^{-x}}x\\,\\mathrm{d}x\\tag{13a}\\\\\n&=-\\frac{\\Ei(1)+\\pi i}e\\tag{13b}\n\\end{align}\n$$\nExplanation:\n$\\text{(13a):}$ apply $\\text{(12g)}$\n$\\text{(13b):}$ $\\Ei(z)=-\\PV\\int_{-z}^\\infty\\frac{e^{-t}}{t}\\,\\mathrm{d}t$\n$\\phantom{\\text{(13b):}}$ the infinitesimal clockwise semicircle\n$\\phantom{\\text{(13b):}}$ around the singularity at $0$ gives $-\\pi i$\nFor $n\\ge2$,\n$$\n\\begin{align}\n\\mathcal{D}(-n)\n&=\\frac1e\\int_{-1}^\\infty x^{-n}e^{-x}\\,\\mathrm{d}x\\tag{14a}\\\\\n&=-\\frac1{n-1}\\frac1e\\int_{-1}^\\infty e^{-x}\\,\\mathrm{d}x^{1-n}\\tag{14b}\\\\\n&=-\\frac1{n-1}\\frac1e\\left((-1)^ne+\\int_{-1}^\\infty x^{1-n}e^{-x}\\mathrm{d}x\\right)\\tag{14c}\\\\\n&=\\frac{(-1)^{n-1}}{n-1}-\\frac1{n-1}\\frac1e\\int_{-1}^\\infty x^{1-n}e^{-x}\\mathrm{d}x\\tag{14d}\\\\\n&=\\frac{(-1)^{n-1}}{n-1}-\\frac1{n-1}\\mathcal{D}(1-n)\\tag{14e}\n\\end{align}\n$$\nExplanation:\n$\\text{(14a):}$ apply $\\text{(12g)}$\n$\\text{(14b):}$ prepare to integrate by parts\n$\\text{(14c):}$ integrate by parts\n$\\text{(14d):}$ distribute\n$\\text{(14e):}$ apply $\\text{(12g)}$\nMultiply $(14)$ by $(-1)^{n-1}(n-1)!$ and apply induction:\n$$\n\\begin{align}\n(-1)^{n-1}(n-1)!\\mathcal{D}(-n)\n&=(n-2)!+(-1)^{n-2}(n-2)!\\mathcal{D}(1-n)\\tag{15a}\\\\\n&=\\sum_{k=0}^{n-2}k!+\\mathcal{D}(-1)\\tag{15b}\n\\end{align}\n$$\nCombine $(13)$ and $(15)$ and divide by $(-1)^{n-1}(n-1)!$:\n$$\n\\mathcal{D}(-n)=\\frac{(-1)^{n-1}}{(n-1)!}\\left(\\sum_{k=0}^{n-2}k!-\\frac{\\Ei(1)+\\pi i}e\\right)\\tag{16}\n$$", "meta": {"post_id": 83380, "input_score": 33, "output_score": 63, "post_title": "I have a problem understanding the proof of Rencontres numbers (Derangements)"}}
{"input": "I recently solved a problem, which says that,\n\nA positive integer can be multiplied with another integer resulting in\n  a positive integer that is composed only of one and zero as digits.\n\nHow can I prove that this is true(currently I assume that it is). Also, is it possible to establish an upper bound on the length(number of digits) of the number generated?", "output": "Here is an alternate solution, which is based on the pigeonhole principle:\nList all the numbers 1, 11, 111, ... , 111...1 where the last number 111...1 contains $n+1$ ones.\nLook now at their remainders when divided by $n$. By the pigeonhole principle, two of them have the same remainder. But then their difference is of the form $1111..100000..0$ and is divisible by $n$..", "meta": {"post_id": 83932, "input_score": 51, "output_score": 145, "post_title": "A natural number multiplied by some integer results in a number with only ones and zeros"}}
{"input": "I'm trying to understand the derivation of Wiener deconvolution given on its Wikipedia page.  In the last couple steps under the derivation section, they take the derivative with respect to $G(f)$ of an equation that has both $G(f)$ and $G^\\ast(f)$ in it.  They simply state that $G^\\ast (f)$ acts as a constant in the differentiation.  However, it seems to me that if you don't treat $G(f)$ as a constant, then you shouldn't be able to treat $G^\\ast (f)$ as a constant because they are directly related.  \nI searched around some looking for an explanation.  I found this page, which seems to agree that the complex conjugate can be treated as a constant.  I also found some stuff about the Cauchy-Riemann equations, which seem to be related.  However, I haven't had any classes on complex analysis and don't understand the intuition behind why this can be done.\nWhy can the complex conjugate of a variable be treated as a constant when differentiating with respect to that variable?", "output": "The nomenclature of $\\dfrac{\\partial}{\\partial z}$ and $\\dfrac{\\partial}{\\partial\\bar{z}}$ is confusing because it gives the impression that these are really partial derivatives with respect to two independent variables, $z$ and $\\bar{z}$. However, it is clear that $z$ and $\\bar{z}$ are not independent.\nDifferentiable Functions and Conformal Maps\nA differentiable function on $\\mathbb{R}$ locally looks like a linear function, that is, there is a real constant, called $f'(x)$, so that for small $h$,\n$$\nf(x+h)=f(x)+f'(x)h+o(h)\\tag{1}\n$$\nAnalogously, a differentiable function on $\\mathbb{C}$ satisfies $(1)$ for some complex number $f'(x)$.\nMultiplication on $\\mathbb{C}$ acts as a rotation and radial scale when viewed as an action on $\\mathbb{R}^2$. Thus, if $f$ is differentiable on $\\mathbb{C}$,\n$$\nf(z+h)-f(z)=f'(z)h+o(h)\\tag{2}\n$$\nThat is, when $h$ is small, $h\\mapsto f(z+h)-f(z)$ looks like a scaled rotation. For this reason, a differentiable function on $\\mathbb{C}$ is called conformal: small features are replicated (scaled and rotated) and angles are preserved.\nComplex Conjugation and Orientation Reversal\nComplex conjugation, $z\\mapsto\\bar{z}$, is an orientation reversing isometry. Thus, when composed with a conformal map, either before or after, the composition is an orientation-reversing conformal map. Furthermore, double composition yields an orientation-preserving conformal map; for example, if $f(z)$ is conformal, then so is $\\overline{f(\\bar{z})}$.\nAs a function on $\\mathbb{R}^2$, complex conjugation can be represented by the matrix $\\begin{bmatrix}1&0\\\\0&-1\\end{bmatrix}$.\nConformal and Conjugate Conformal\nThe partial derivatives of a general differentiable function on $\\mathbb{R}^2$ given by $x+iy\\mapsto u+iv$ are usually given in a $2\\times2$ Jacobian matrix:\n$$\n\\frac{\\partial(u,v)}{\\partial(x,y)}=\\begin{bmatrix}\\frac{\\partial u}{\\partial x}&\\frac{\\partial v}{\\partial x}\\\\\\frac{\\partial u}{\\partial y}&\\frac{\\partial v}{\\partial y}\\end{bmatrix}\\tag{3}\n$$\nThe Cauchy-Riemann equations specify that $\\dfrac{\\partial u}{\\partial x}=\\dfrac{\\partial v}{\\partial y}$ and $\\dfrac{\\partial u}{\\partial y}=-\\dfrac{\\partial v}{\\partial x}$, which agrees with the following basis for the orientation-preserving conformal Jacobians on $\\mathbb{R}^2$:\n$$\n\\left\\{\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix},\\begin{bmatrix}0&1\\\\-1&0\\end{bmatrix}\\right\\}\\tag{4}\n$$\nNote that the determinant of any linear combination of these matrices has positive determinant (thus orientation is preserved).\nThe following basis for the orientation-reversing conformal Jacobians on $\\mathbb{R}^2$ follows by composing conjugation with $(4)$:\n$$\n\\left\\{\\begin{bmatrix}1&0\\\\0&-1\\end{bmatrix},\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\right\\}\\tag{5}\n$$\nNote that the determinant of any linear combination of these matrices has negative determinant (thus orientation is reversed).\nUsing $(4)$ and $(5)$, we can break any Jacobian into conformal and conjugate conformal parts. Using the component-wise orthogonality that exists among the bases, we can write the conformal part as\n$$\n\\frac{1}{2}\\left(\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}\\right)\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}+\\frac{1}{2}\\left(\\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y}\\right)\\begin{bmatrix}0&1\\\\-1&0\\end{bmatrix}\\tag{6}\n$$\nand the conjugate conformal part as\n$$\n\\frac{1}{2}\\left(\\frac{\\partial u}{\\partial x}-\\frac{\\partial v}{\\partial y}\\right)\\begin{bmatrix}1&0\\\\0&-1\\end{bmatrix}+\\frac{1}{2}\\left(\\frac{\\partial v}{\\partial x}+\\frac{\\partial u}{\\partial y}\\right)\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\tag{7}\n$$\n$\\dfrac{\\partial}{\\partial z}$, $\\dfrac{\\partial}{\\partial\\bar{z}}$, and Quaternions\nThe definitions of $\\dfrac{\\partial}{\\partial z}$ and $\\dfrac{\\partial}{\\partial\\bar{z}}$ say\n$$\n\\begin{align}\n\\frac{\\partial}{\\partial z}(u+iv)\n&=\\frac{1}{2}\\left(\\frac{\\partial}{\\partial x}-i\\frac{\\partial}{\\partial y}\\right)(u+iv)\\\\\n&=\\frac{1}{2}\\left(\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}\\right)+\\frac{i}{2}\\left(\\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y}\\right)\\tag{8}\n\\end{align}\n$$\nand\n$$\n\\begin{align}\n\\frac{\\partial}{\\partial\\bar{z}}(u+iv)\n&=\\frac{1}{2}\\left(\\frac{\\partial}{\\partial x}+i\\frac{\\partial}{\\partial y}\\right)(u+iv)\\\\\n&=\\frac{1}{2}\\left(\\frac{\\partial u}{\\partial x}-\\frac{\\partial v}{\\partial y}\\right)+\\frac{i}{2}\\left(\\frac{\\partial v}{\\partial x}+\\frac{\\partial u}{\\partial y}\\right)\\tag{9}\n\\end{align}\n$$\nThe space of $2\\times2$ Jacobians has $4$ dimensions, so trying to represent these $4$ dimensions with the $2$ dimensions of $\\mathbb{C}$, using $\\dfrac{\\partial}{\\partial z}$ and $\\dfrac{\\partial}{\\partial\\bar{z}}$, obscures something.\nThere is a common matrix representation of the complex numbers as $2\\times2$ real matrices where\n$$\n\\begin{align}\n\\mathbf{1}&\\leftrightarrow\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\tag{10}\\\\\n\\mathbf{i}&\\leftrightarrow\\begin{bmatrix}0&1\\\\-1&0\\end{bmatrix}\\tag{11}\n\\end{align}\n$$\nHowever, there is also a matrix representation of the quaternions as $2\\times2$ complex matrices where, in addition to $(10)$ and $(11)$,\n$$\n\\begin{align}\n\\mathbf{j}&\\leftrightarrow\\begin{bmatrix}i&0\\\\0&-i\\end{bmatrix}\\tag{12}\\\\\n\\mathbf{k}&\\leftrightarrow\\begin{bmatrix}0&-i\\\\-i&0\\end{bmatrix}\\tag{13}\n\\end{align}\n$$\nEmbed $(8)$ and $(9)$ in the quaternions to get\n$$\n\\left(\\frac{\\partial}{\\partial z}(u+iv)\\right)\\mathbf{1}\n=\\frac{\\mathbf{1}}{2}\\left(\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}\\right)+\\frac{\\mathbf{i}}{2}\\left(\\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y}\\right)\\tag{14}\n$$\nand\n$$\n\\left(\\frac{\\partial}{\\partial\\bar{z}}(u+iv)\\right)\\mathbf{j}\n=\\frac{\\mathbf{j}}{2}\\left(\\frac{\\partial u}{\\partial x}-\\frac{\\partial v}{\\partial y}\\right)+\\frac{\\mathbf{k}}{2}\\left(\\frac{\\partial v}{\\partial x}+\\frac{\\partial u}{\\partial y}\\right)\\tag{15}\n$$\nFinally, substituting $(10)$-$(13)$ into $(14)$ and $(15)$, it becomes apparent, upon comparison with $(6)$ and $(7)$, that $\\left(\\dfrac{\\partial}{\\partial z}(u+iv)\\right)\\mathbf{1}$ represents the conformal part of the Jacobian and $\\left(\\dfrac{\\partial}{\\partial\\bar{z}}(u+iv)\\right)\\mathbf{j}$ represents the conjugate conformal part.\nConclusion\nFor a general $f:\\mathbb{C}\\mapsto\\mathbb{C}$, $\\dfrac{\\partial}{\\partial z}f$ can be mapped to the conformal part of the $2\\times2$ Jacobian, $\\dfrac{\\partial f}{\\partial z}=\\dfrac{\\partial(u,v)}{\\partial(x,y)}$, and $\\dfrac{\\partial}{\\partial\\bar{z}}f$ can be mapped to the conjugate conformal part. It is merely convenience of notation that we write $\\dfrac{\\partial}{\\partial z}$ and $\\dfrac{\\partial}{\\partial\\bar{z}}$ because $\\dfrac{\\partial}{\\partial z}f\\;\\mathrm{d}z+\\dfrac{\\partial}{\\partial\\bar{z}}f\\;\\mathrm{d}\\bar{z}=\\mathrm{d}f$. However, they are not true partial derivatives, but $2$ pieces of a $2\\times2$ Jacobian composed of $4$ partial derivatives.\nSo, to answer the question asked, $z\\mapsto\\bar{z}$ is conjugate conformal, so $\\frac{\\partial}{\\partial z}\\bar{z}=0$; therefore, $\\bar{z}$ acts like a constant under $\\frac{\\partial}{\\partial z}$.", "meta": {"post_id": 85648, "input_score": 27, "output_score": 52, "post_title": "Why can the complex conjugate of a variable be treated as a constant when differentiating with respect to that variable?"}}
{"input": "I am using quaternions to represent orientation as a rotational offset from a global coordinate frame.\nIs it correct in thinking that quaternion distance gives a metric that defines the closeness of two orientations? i.e. similar orientations give low distances, and dissimilar orientations give high distances.\nDoes zero distance mean that the orientations are exactly the same?\nThis seems obvious, but I want to ensure that there are no subtleties that get in the way of logic.", "output": "First, I assume that you're using unit quaternions, i.e. quaternions $a+b\\,\\textbf{i}+c\\,\\textbf{j}+d\\,\\textbf{k}$ that satisfy $a^2 + b^2 + c^2 + d^2 = 1$.  If not, you'll want to scale your quaternions before computing distance.\nDistance between quaternions will correspond roughly to distance between orientations as long as the quaternions are fairly close to each other.  However, if you're comparing quaternions globally, you should remember that $q$ and $-q$ always represent the same orientation, even though the distance between them is $2$.\nThere are better ways to compute the closeness of two orientations that avoid this problem.  For example, the angle $\\theta$ of rotation required to get from one orientation to another is given by the formula\n$$\r\n\\theta \\;=\\; \\cos^{-1}\\bigl(2\\langle q_1,q_2\\rangle^2 -1\\bigr)\r\n$$\nwhere $\\langle q_1,q_2\\rangle$ denotes the inner product of the corresponding quaternions:\n$$\r\n\\langle a_1 +b_1 \\textbf{i} + c_1 \\textbf{j} + d_1 \\textbf{k},\\; a_2 + b_2 \\textbf{i} + c_2 \\textbf{j} + d_2 \\textbf{k}\\rangle \\;=\\; a_1a_2 + b_1b_2 + c_1 c_2 + d_1d_2.\r\n$$\n(This formula follows from the double-angle formula for cosine, together with the fact that the angle between orientations is precisely twice the angle between unit quaternions.)\nIf you want a notion of distance that can be computed without trig functions, the quantity\n$$\r\nd(q_1,q_2) \\;=\\; 1 - \\langle q_1,q_2\\rangle^2\r\n$$\nis equal to $(1-\\cos\\theta)/2$, and gives a rough estimate of the distance.  In particular, it gives $0$ whenever the quaternions represent the same orientation, and it gives $1$ whenever the two orientations are $180^\\circ$ apart.", "meta": {"post_id": 90081, "input_score": 41, "output_score": 69, "post_title": "Quaternion distance"}}
{"input": "There appears to be an interesting pattern in the decimal expansion of $\\dfrac1{243}$:\n\n$$\\frac1{243}=0.\\overline{004115226337448559670781893}$$\n\nI was wondering if anyone could clarify how this comes about?", "output": "$\\frac{1}{243}=\\frac{1}{333}+\\frac{10}{8991}$\n$\\frac{1}{333}=.\\overline{003}$\n$\\frac{1}{8991}=.\\overline{000111222333444555666777889}=\\frac{111}{998001}=\\frac{111}{10^6-2\\cdot10^3+1}$", "meta": {"post_id": 90690, "input_score": 13, "output_score": 36, "post_title": "Interesting pattern in the decimal expansion of $\\frac1{243}$"}}
{"input": "I am quite confused about the meaning of shadow price from explanations on the internet.\nIt can be understood as the value of a change in revenue if the constraint is relaxed, or how much you would be willing to pay for an additional resource.\nFor example:\n$$\\begin{array}{ll} \\text{maximize} & 5 x_1 + 4 x_2 + 6 x_3\\\\ \\text{subject to} & 6 x_1 + 5 x_2 + 8 x_ 3 \\leq 16 \\quad\\quad\\quad\\quad\\,\\,{(c_1)}\\\\ & 10 x_1 + 20 x_2 + 10 x_3 \\leq 35 \\quad\\quad\\quad{(c_2)}\\\\ & 0 \\leq x_1, x_2, x_3 \\leq 1\\\\\\end{array}$$\nSolving this problem, we get the shadow price of $c_1 = 0.727273$, $c_2 = 0.018182$.\nComparing $c_1$ and $c_2$, if one constraint can be relaxed, we should relax $c_1$ instead of $c_2$?", "output": "Here's perhaps a better way to think of the shadow price.  (I don't like the word \"relax\" here; I think it's confusing.)\nFor maximization problems like this one the constraints can often be thought of as restrictions on the amount of resources available, and the objective can be thought of as profit.  Then the shadow price associated with a particular constraint tells you how much the optimal value of the objective would increase per unit increase in the amount of resources available.  In other words, the shadow price associated with a resource tells you how much more profit you would get by increasing the amount of that resource by one unit.  (So \"How much you would be willing to pay for an additional resource\" is a good way of thinking about the shadow price.)  \nIn the example you give, there are 16 units available of the first resource and 35 units available of the second resource.  The fact that the shadow price of $c_1$ is 0.727273 means that if you could increase the first resource from 16 units to 17 units, you would get an additional profit of about \\$0.73.  Similarly, if you could increase the second resource from 35 units to 36 units then you would get an additional profit of about \\$0.02.  \nSo if you could increase just one resource by one unit, and the cost of increasing the first resource is the same as that of increasing the second resource (this assumption is not part of the model), then, yes, you should definitely increase the first resource by one unit.", "meta": {"post_id": 91504, "input_score": 40, "output_score": 67, "post_title": "Shadow prices in linear programming"}}
{"input": "Please correct any mistakes in this proof and, if you're feeling inclined, please provide a better one where \"better\" is defined by whatever criteria you prefer. \n\nAssume $2^{1/2}$ is irrational. \n$2^{1/3} * 2^{x} = 2^{1/2} \\Rightarrow x = 1/6$. \n$2^{1/3} * {2^{1/2}}^{1/3} = 2^{1/2}$.\nif $2^{1/2}$ is irrational, then ${2^{1/2}}^{1/3}$ is irrational. \n$2^{1/3} = 2^{1/2} / {2^{1/2}}^{1/3}$. \n$2^{1/3}$ equals an irrational number divided by an irrational number. \n$2^{1/3}$ is an irrational number.", "output": "I can't resist:  Suppose $2^{\\frac{1}{3}}=\\frac{n}{m}$.  Then $$2m^3=n^3,$$ or in other words $$m^3+m^3=n^3.$$  But this contradicts Fermats Last Theorem.", "meta": {"post_id": 91538, "input_score": 37, "output_score": 88, "post_title": "Prove $2^{1/3}$ is irrational."}}
{"input": "For the small values of n I have been able to check, it seems that for $n>3$, there exist whole numbers $x,y$ s.t. $n! = x^2 - y^2$. For example ..\n$4! = 5^2 - 1^2$\n$5! = 11^2 - 1^2$\n$6! = 27^2 - 3^2$\n$7! = 71^2 - 1^2$\n$8! = 201^2 - 9^2$\n$9! = 603^2 - 27^2$\n$10! = 1905^2 - 15^2$\n$11! = 6318^2 - 18^2$\n$12! = 21888^2 - 288^2$\nIn most of the cases above, the $x$ value is just the next integer larger than $\\sqrt{n!}$, though at $n=12$ and $n=17$ it's the one following that. With the tools at hand I've only been able to check this as far as $n=17$. \nI expect there's probably already a name for this, but not knowing that name, googling was coming up dry.", "output": "If $n >3$, then $n!$ is divisible by $4$.\nSo $n!=4k=(2)(2k)$ for some integer $k$.\nNote now that \n$$4k=(k+1)^2-(k-1)^2.$$\nIf $n$ is large, there are many representations of $n!$ as a difference of two squares.  For let $2a$ and $2b$ be any two even numbers whose product is $n!$. Then\n$$n!=4ab=(a+b)^2-(a-b)^2.$$\nComment: Let $a$ be an odd integer. Then $a+1$ and $a-1$ are even, and therefore $(a+1)/2$ and $(a-1)/2$ are integers. We have\n$$a=\\left(\\frac{a+1}{2}\\right)^2-\\left(\\frac{a-1}{2}\\right)^2,$$\nso $a$ is a difference of two squares.\nIf $a$ is divisible by $4$, the argument we gave above shows that $a$ is a difference of two squares.\nIf $a$ is even but not divisible by $4$, then $a$ is not a difference of two squares. For a difference of two even squares is divisible by $4$, and a difference of two odd squares is divisible by $8$.", "meta": {"post_id": 92297, "input_score": 22, "output_score": 37, "post_title": "are all $n!$ ($n>3$) the difference of two squares?"}}
{"input": "Given some Abelian group $(G, +)$, does there always exist a binary operation $*$ such that $(G, +, *)$ is a ring? That is, $*$ is associative and distributive:\n\\begin{align*}\n&a * (b * c) = (a*b) * c \\\\\n&a * (b + c) = a * b + a * c \\\\\n&(a + b) * c = a * c + b * c \\\\\n\\end{align*}\nWe also might have multiplicative identity $1 \\in G$, with $a * 1 = 1 * a = a$ for any $a \\in G$. Multiplication may or may not be commutative.\nDepending on the definition, the answer could be no in the case of the group with one element: then $1 = 0$. But the trivial ring is not a very interesting case. For cyclic groups the statement is certainly true, since $(\\mathbb{Z}_n, +, \\cdot)$ and $(\\mathbb{Z}, +, \\cdot)$ are both rings. What about in general? Is there some procedure to give arbitrary abelian groups ring structure?", "output": "If your group has the property that every element has finite order, but there is no upper bound on the orders of the elements, then it is not the additive abelian group of a ring with identity.  The reason is that if there were such a ring structure with an identity $1$, then $1$ would have finite additive order $k$, and then for all $a$ in your group, $k\\cdot a=(k\\cdot1)a=0a=0$, which forces $a$ to have order at most $k$.  \nFor each prime $p$, the Pr\u00fcfer $p$-group $\\mathbb Z(p^\\infty)$ is an example of such a group.  The quotient group $\\mathbb Q/\\mathbb Z$ is another.  Direct sums (but not direct products) of infinitely many finite cyclic groups of unbounded order would also be examples.", "meta": {"post_id": 93409, "input_score": 83, "output_score": 96, "post_title": "Does every Abelian group admit a ring structure?"}}
{"input": "It is obvious that $\\mathbb{Q}_r$ is topologically isomorphic to $\\mathbb Q_s$ while $r$ and $s$ denote different primes. But I really don't know whether it is true in the aspect of algebra. As I failed to prove it, I think that it is false, but I can't give a counterexample.\nLast I'm quite sorry that I'm new to MathJax and I don't know how to use it properly.Thanks for reading and I would appreciate it if you could solve my problem.", "output": "Never. Looking at the number of roots of unity in your field suffices to distinguish all ${\\mathbb{Q}}_p$ for odd values of $p$, because the number of roots of $1$ there is precisely $p-1$. It's different for the $2$-adic numbers, since they have two roots of unity, same as the $3$-adics. But the $2$-adics have a square root of $-7$ and the $3$-adics don't, whereas the $3$-adics have a square root of $10$ and the $2$-adics don't.", "meta": {"post_id": 93633, "input_score": 14, "output_score": 35, "post_title": "Is $\\mathbb Q_r$ algebraically isomorphic to $\\mathbb Q_s$ while r and s denote different primes?"}}
{"input": "Can someone explain why taking an average of an average usually results in a wrong answer?  Is there ever a case where the average of the average can be used correctly?  \nAs an example, let's say that an assessment is given to three schools and I want to find out the average score for all three schools combined and the average score per school.  When I attempt to add the three individual scores and divide by three I get a number that is very close (+/- 1 percent) to the actual overall average.", "output": "Thomas Andrews already answered the question, but I'd like to present a more analytical solution to the problem.\nThe average of averages is only equal to the average of all values in two cases:\n\nif the number of elements of all groups is the same; or\nthe trivial case when all the group averages are zero\n\nHere's why this is so.\nConsider two sets $X = \\{x_1, x_2, ..., x_n\\}$ and $Y = \\{y_1, y_2, ..., y_m\\}$ and their averages:\n$$ \\bar{x} = \\frac{\\sum_{i=1}^{n}{x_i}}{n} \\,,\\, \n   \\bar{y} = \\frac{\\sum_{i=1}^{m}{y_i}}{m}\n$$\nThe average of the averages is:\n$$ average(\\bar{x}, \\bar{y}) \n  = \\frac{\\frac{\\sum_{i=1}^{n}{x_i}}{n} + \\frac{\\sum_{i=1}^{m}{y_i}}{m}}{2}\n  = \\frac{\\sum_{i=1}^{n}{x_i}}{2n} + \\frac{\\sum_{i=1}^{m}{y_i}}{2m}\n$$\nNow consider the whole group \n$Z = \\{x_1, x_2, ..., x_n, y_1, y_2, ..., y_m\\}$ and its average:\n$$ \\bar{z} = \\frac{\\sum_{i=1}^{n}{x_i} + \\sum_{i=1}^{m}{y_i}}{n + m}$$\nFor the general case, we can see that these averages are different:\n$$  \\frac{\\sum_{i=1}^{n}{x_i}}{2n} + \\frac{\\sum_{i=1}^{m}{y_i}}{2m} \n\\ne \\frac{\\sum_{i=1}^{n}{x_i} + \\sum_{i=1}^{m}{y_i}}{n + m} \n$$\nThis answers the first OP question, as to why the average of averages usually gives the wrong answer.\nHowever, if we make $n = m$, we have:\n$$  \\frac{\\sum_{i=1}^{n}{x_i}}{2n} + \\frac{\\sum_{i=1}^{m}{y_i}}{2n} \n  = \\frac{\\sum_{i=1}^{n}{x_i} + \\sum_{i=1}^{n}{y_i}}{2n} \n$$\nThis is why the average of averages is equal to the average of the whole group when the groups have the same size.\nThe second case is trivial: $\\bar{x} = \\bar{y} = average(\\bar{x}, \\bar{y}) = 0$.\nNote that the above reasoning can be extended for any number of groups.", "meta": {"post_id": 95909, "input_score": 131, "output_score": 39, "post_title": "Why is an average of an average usually incorrect?"}}
{"input": "If $(V, \\langle \\cdot, \\cdot \\rangle)$ is a finite-dimensional inner product space and $f,g : \\mathbb{R} \\longrightarrow V$ are differentiable functions, a straightforward calculation with components shows that \n$$\r\n\\frac{d}{dt} \\langle f, g \\rangle = \\langle f(t), g^{\\prime}(t) \\rangle + \\langle f^{\\prime}(t), g(t) \\rangle\r\n$$\nThis approach is not very satisfying. However, attempting to apply the definition of the derivative directly doesn't seem to work for me. Is there a slick, perhaps intrinsic way, to prove this that doesn't involve working in coordinates?", "output": "This answer may be needlessly complicated if you don't want such generality, taking the approach of first finding the Fr\u00e9chet derivative of a bilinear operator.\nIf $V$, $W$, and $Z$ are normed spaces, and if $T:V\\times W\\to Z$ is a continuous (real) bilinear operator, meaning that there exists $C\\geq 0$ such that $\\|T(v,w)\\|\\leq C\\|v\\|\\|w\\|$ for all $v\\in V$ and $w\\in W$, then the derivative of $T$ at $(v_0,w_0)$ is $DT|_{(v_0,w_0)}(v,w)=T(v,w_0)+T(v_0,w)$.  (I am assuming that $V\\times W$ is given a norm equivalent with $\\|(v,w)\\|=\\sqrt{\\|v\\|^2+\\|w\\|^2}$.)  This follows from the straightforward computation \n$$\\frac{\\|T(v_0+v,w_0+w)-T(v_0,w_0)-(T(v,w_0)+T(v_0,w))\\|}{\\|(v,w)\\|}=\\frac{\\|T(v,w)\\|}{\\|(v,w)\\|}\\leq C\\frac{\\|v\\|\\|w\\|}{\\|(v,w)\\|}\\to 0$$\nas $(v,w)\\to 0$.\nWith $V=W$, $Z=\\mathbb R$ or $Z=\\mathbb C$, and $T:V\\times V\\to Z$ the inner product, this gives $DT_{(v_0,w_0)}(v,w)=\\langle v,w_0\\rangle+\\langle v_0,w\\rangle$.  Now if $f,g:\\mathbb R\\to V$ are differentiable, then $F:\\mathbb R\\to V\\times V$ defined by $F(t)=(f(t),g(t))$ is differentiable with $DF|_t(h)=h(f'(t),g'(t))$.  By the chain rule, \n$$D(T\\circ F)|_{t}(h)\r\n=DT|_{F(t)}\\circ DF|_t(h)=h(\\langle f'(t),g(t)\\rangle+\\langle f(t),g'(t)\\rangle),$$\nwhich means $\\frac{d}{dt} \\langle f, g \\rangle = \\langle f'(t),g(t)\\rangle+\\langle f(t),g'(t)\\rangle$.", "meta": {"post_id": 96265, "input_score": 65, "output_score": 39, "post_title": "Differentiating an Inner Product"}}
{"input": "I know that if $X$ were distributed as a standard normal, then $X^2$ would be distributed as chi-squared, and hence have expectation $1$, but I'm not sure about for a general normal.\nThanks", "output": "Use the identity\n$$\r\nE(X^2)=\\text{Var}(X)+[E(X)]^2\r\n$$\nand you're done.\nSince you know that $X\\sim N(\\mu,\\sigma)$, you know the mean and variance of $X$ already, so you know all terms on RHS.", "meta": {"post_id": 99025, "input_score": 24, "output_score": 48, "post_title": "What is the expectation of $ X^2$ where $ X$ is distributed normally?"}}
{"input": "Some days ago, I was thinking on a problem, which states that $$AB-BA=I$$ does not have a solution in $M_{n\\times n}(\\mathbb R)$ and $M_{n\\times n}(\\mathbb C)$. (Here $M_{n\\times n}(\\mathbb F)$ denotes the set of all $n\\times n$ matrices with entries from the field $\\mathbb F$ and $I$ is the identity matrix.)\nAlthough I couldn't solve the problem, I came up with this problem:\n\nDoes there exist a field $\\mathbb F$ for which that equation $AB-BA=I$ has a solution in $M_{n\\times n}(\\mathbb F)$?\n\nI'd really appreciate your help.", "output": "Let $k$ be a field. The first Weyl algebra $A_1(k)$ is the free associative $k$-algebra generated by two letters $x$ and $y$ subject to the relation $$xy-yx=1,$$ which is usually called the Heisenberg or Weyl commutation relation. This is an extremely important example of a non-commutative ring which appears in many places, from the algebraic theory of differential operators to quantum physics (the equation above is Heisenberg's indeterminacy principle, in a sense) to the pinnacles of Lie theory to combinatorics to pretty much anything else.\nFor us right now, this algebra shows up because \n\nan $A_1(k)$-modules are essentially the same thing as solutions to the equation $PQ-QP=I$ with $P$ and $Q$ endomorphisms of a vector space. \n\nIndeed:\n\nif $M$ is a left $A_1(k)$-module then $M$ is in particular a $k$-vector space and there is an homomorphism of $k$-algebras $\\phi_M:A_1(k)\\to\\hom_k(M,M)$ to the endomorphism algebra of $M$ viewed as a vector space. Since $x$ and $y$ generate the algebra $A_1(k)$, $\\phi_M$ is completely determined by the two endomorphisms $P=\\phi_M(x)$ and $Q=\\phi_M(y)$; moreover, since $\\phi_M$ is an algebra homomorphism, we have $PQ-QP=\\phi_1(xy-yx)=\\phi_1(1_{A_1(k)})=\\mathrm{id}_M$. We thus see that $P$ and $Q$ are endomorphisms of the vector space $M$ which satisfy our desired relation.\nConversely, if $M$ is a vector space and $P$, $Q:M\\to M$ are two linear endomorphisms, then one can show more or less automatically that there is a unique algebra morphism $\\phi_M:A_1(k)\\to\\hom_k(M,M)$ such that $\\phi_M(x)=P$ and $\\phi_M(y)=Q$. This homomorphism turns $M$ into a left $A_1(k)$-module.\nThese two constructions, one going from an $A_1(k)$-module to a pair $(P,Q)$ of endomorphisms of a vector space $M$ such that $PQ-QP=\\mathrm{id}_M$, and the other going the other way, are mutually inverse.\n\nA conclusion we get from this is that your question \n\nfor what fields $k$ do there exist $n\\geq1$ and matrices $A$, $B\\in M_n(k)$ \n  such that $AB-BA=I$?\n\nis essentially equivalent to\n\nfor what fields $k$ does $A_1(k)$ have finite dimensional modules?\n\nNow, it is very easy to see that $A_1(k)$ is an infinite dimensional algebra, and that in fact the set $\\{x^iy^j:i,j\\geq0\\}$ of monomials is a $k$-basis.\nTwo of the key properties of $A_1(k)$ are the following:\n\nTheorem. If $k$ is a field of characteristic zero, then $A_1(k)$ is a simple algebra\u2014that is, $A_1(k)$ does not have any non-zero proper bilateral ideals. Its center is trivial: it is simply the $1$-dimensional subspace spanned by the unit element.\n\nAn immediate corollary of this is the following\n\nProposition. If $k$ is a field of characteristic zero, the $A_1(k)$ does not have any non-zero finite dimensional modules. Equivalently, there do not exist $n\\geq1$ and a pair of matrices $P$, $Q\\in M_n(k)$ such that $PQ-QP=I$.\n\nProof. Suppose $M$ is a finite dimensional $A_1(k)$-module. Then we have an algebra homomorphism $\\phi:A_1(k)\\to\\hom_k(M,M)$ such that $\\phi(a)(m)=am$ for all $a\\in A_1(k)$ and all $m\\in M$. Since $A_1(k)$ is infinite dimensional and $\\hom_k(M,M)$ is finite dimensional (because $M$ is finite dimensional!) the kernel $I=\\ker\\phi$ cannot be zero \u2014in fact, it must hace finite codimension. Now $I$ is a bilateral ideal, so the theorem implies that it must be equal to $A_1(k)$. But then $M$ must be zero dimensional, for $1\\in A_1(k)$ acts on it at the same time as the identity and as zero. $\\Box$\nThis proposition can also be proved by taking traces, as everyone else has observed on this page, but the fact that $A_1(k)$ is simple is an immensely more powerful piece of knowledge (there are examples of algebras which do not have finite dimensional modules and which are not simple, by the way :) )\nNow let us suppose that $k$ is of characteristic $p>0$. What changes in term of the algebra? The most significant change is \n\nObservation. The algebra $A_1(k)$ is not simple. Its center $Z$ is generated by the elements $x^p$ and $y^p$, which are algebraically independent, so that $Z$ is in fact isomorphic to a polynomial ring in two variables. We can write $Z=k[x^p,y^p]$.\n\nIn fact, once we notice that $x^p$ and $y^p$ are central elements \u2014and this is proved by a straightforward computation\u2014 it is easy to write down non-trivial bilateral ideals. For example, $(x^p)$ works; the key point in showing this is the fact that since $x^p$ is central, the left ideal which it generates coincides with the bilateral ideal, and it is very easy to see that the left ideal is proper and non-zero.\nMoreover, a little playing with this will give us the following. Not only does $A_1(k)$ have bilateral ideals: it has bilateral ideals of finite codimension. For example, the ideal $(x^p,y^p)$ is easily seen to have codimension $p^2$; more generally, we can pick two scalars $a$, $b\\in k$ and consider the ideal $I_{a,b}=(x^p-a,y^p-b)$, which has the same codimension $p^2$. Now this got rid of the obstruction to finding finite-dimensional modules that we had in the characteristic zero case, so we can hope for finite dimensional modules now!\nMore: this actually gives us a method to produce pairs of matrices satisfying the Heisenberg relation. We just can pick a proper bilateral ideal $I\\subseteq A_1(k)$ of finite codimension, consider the finite dimensional $k$-algebra $B=A_1(k)/I$ and look for finitely generated $B$-modules: every such module is provides us with a finite dimensional $A_1(k)$-module and the observations above produce from it pairs of matrices which are related in the way we want.\nSo let us do this explicitly in the simplest case: let us suppose that $k$ is algebraically closed, let $a$, $b\\in k$ and let $I=I_{a,b}=(x^p-a,y^p-b)$. The algebra $B=A_1(k)/I$ has dimension $p^2$, with $\\{x^iy^j:0\\leq i,j<p\\}$ as a basis. The exact same proof that the Weyl algebra is simple when the ground field is of characteristic zero proves that $B$ is simple, and in the same way the same proof that proves that the center of the Weyl algebra is trivial in characteristic zero shows that the center of $B$ is $k$; going from $A_1(k)$ to $B$ we have modded out the obstruction to carrying out these proofs. In other words, the algebra $B$ is what's called a (finite dimensional) central simple algebra. Wedderburn's theorem now implies that in fact $B\\cong M_p(k)$, as this is the only semisimple algebra of dimension $p^2$ with trivial center. A consequence of this is that there is a unique (up to isomorphism) simple $B$-module $S$, of dimension $p$, and that all other finite dimensional $B$-modules are direct sums of copies of $S$. \nNow, since $k$ is algebraically closed (much less would suffice) there is an $\\alpha\\in k$ such that $\\alpha^p=a$. Let $V=k^p$ and consider the $p\\times p$-matrices $$Q=\\begin{pmatrix}0&&&&b\\\\1&0\\\\&1&0\\\\&&1&0\\\\&&&\\ddots&\\ddots\\end{pmatrix}$$ which is all zeroes expect for $1$s in the first subdiagonal and a $b$ on the top right corner, and $$P=\\begin{pmatrix}-\\alpha&1\\\\&-\\alpha&2\\\\&&-\\alpha&3\\\\&&&\\ddots&\\ddots\\\\&&&&-\\alpha&p-1\\\\&&&&&-\\alpha\\end{pmatrix}.$$ One can show that $P^p=aI$, $Q^p=bI$ and that $PQ-QP=I$, so they provide us us a morphism of algebras $B\\to\\hom_k(k^ p,k^ p)$, that is, they turn $k^p$ into a $B$-module. It must be isomorphic to $S$, because the two have the same dimension and there is only one module of that dimension; this determines all finite dimensional modules, which are direct sums of copies of $S$, as we said above..\nThis generalizes the example Henning gave, and in fact one can show that this procedure gives all $p$-dimensional $A_1(k)$-modules can be constructed from quotients by ideals of the form $I_{a,b}$. Doing direct sums for various choices of $a$ and $b$, this gives us lots of finite dimensional $A_1(k)$-modules and, then, of pairs of matrices satisfying the Heisenberg relation. I think we obtain in this way all the semisimple finite dimensional $A_1(k)$-modules but I would need to think a bit before claiming it for certain.\nOf course, this only deals with the simplest case. The algebra $A_1(k)$ has non-semisimple finite-dimensional quotients, which are rather complicated (and I think there are pretty of wild algebras among them...) so one can get many, many more examples of modules and of pairs of matrices.", "meta": {"post_id": 99175, "input_score": 54, "output_score": 53, "post_title": "Solutions to the matrix equation $\\mathbf{AB-BA=I}$ over general fields"}}
{"input": "Let $X$ be a compact K\u00e4hler manifold of complex dimension $\\dim_{\\mathbb C} = n$. Let $[\\omega]$ be the cohomology class of a K\u00e4hler metric on $X$. Then powers of the class $[\\omega]$ defines a linear morphism between cohomology groups\n$$ L^k :  H^{n-k}(X,\\mathbb C) \\longrightarrow H^{n+k}(X,\\mathbb C) $$\nwhich is simply given by cup product against the class $[\\omega]^k$. The hard Lefschetz theorem says that this is in fact an isomorphism of vector spaces.\nQuestion: Why do we call this the \"hard\" Lefschetz theorem?\nModern proofs of this theorem are not that involved; one picks a K\u00e4hler metric $\\omega$ and proves the K\u00e4hler identities on $X$, and the rest then follows from the existence of primitive decompositions. Thus it seems a bit of hype to call the theorem \"hard\".\nOne might think this is to distinguish this from another theorem of Lefschetz, often called the \"weak\" Lefschetz theorem, which gives a similar result in the case where $[\\omega]$ is the Chern class of an ample line bundle. But then we'd surely call this the \"strong\" Lefschetz theorem, right?", "output": "This question can only have a subjective answer (which is actually fun, from time to time!), so here  are a few personal remarks.  \n1) You are a dynamic PhD student working in 2012 under the supervision of Demailly, a world leader in complex algebraic geometry.\nYou have at your disposal a technology that didn't exist on Lefschetz' time: singular and De Rham cohomology, higher homotopy groups, K\u00e4hler manifolds, Hodge theory,...\nEven the abstract notion of a finite-dimensional vector space had not been axiomatized.\nSo  when you claim that the theorem is not that hard, you should not lose sight of  the historic context in which Lefschetz \"proved\" his theorem in 1924.  \n2) I wrote \"proved\" in quotes, since as Sabbah diplomaticallty puts it, Lefschetz' proof was \"insufficient\".\nSo the theorem was not easy, even for Lefschetz. \n3) The theorem has fascinated many Fields medalists and other giants who gave proofs of some version of the theorem: Andreotti, Frankel, Thom, Bott, Kodaira, Spencer, Artin, Grothendieck, Deligne.\nThis is certainly an indication of the depth of the theorem...  \n4) Like you I am enthusiastic about complex algebraic manifolds and am grateful for  the transcendental methods , like K\u00e4hler theory, which allow us to study them.\nHowever algebraic geometers also  want to consider algebraic varieties in characteristic $p$, and  there these transcendental tools  unfortunately completely break down.\nHard Lefschetz for smooth varieties over finite fields was proved by Deligne only in 1980, after much preliminary work by himself and Grothendieck (cf. SGA7).\n I would surmise that  the terminology \"Lefschetz vache\" introduced by Grothendieck is to be understood in that context.  \n5) Finally even in the complex case, I find the proof of hard Lefschetz starting from scratch not so easy.\nI'll let you and the  other users judge by linking to a free online course of Sabbah on Hodge theory and hard Lefschetz (in the Introduction  of which he writes the diplomatic remark mentioned above!)\nEdit\nSince this is a good-humoured, non-technical  answer,  I'll  take the liberty of quoting the following picturesque metaphor by Lefschetz:    \nIt was my lot to plant the harpoon of algebraic topology into the body of the whale of algebraic geometry", "meta": {"post_id": 99840, "input_score": 36, "output_score": 48, "post_title": "Why is the hard Lefschetz theorem \"hard\"?"}}
{"input": "Here's a little brain teaser, for your coffee break:\n$$\n62-63 = 1\n$$\nMove only one digit to make it right!\nHave fun!", "output": "$6263 \\neq 1$.\nI moved one character not a digit :)", "meta": {"post_id": 102946, "input_score": 20, "output_score": 43, "post_title": "Coffee Break Riddle"}}
{"input": "Prove that for any $x_0 \\in X$ and any $r>0$, the open ball $B_r(x_o)$ is open.\n\nMy attempt: Let $y\\in B_r(x_0)$. By definition, $d(y,x_0)<r$. I want to show there exists an $r_1\\in\\mathbb{R^+}$ s.t. $B_{r_{1}}(y)\\subseteq B_r(x_0)$. Let $a\\in B_{r_{1}}(y)$. Then, $d(a,y)<r_1$. For $a\\in B_{r}(x_0)$, $d(a,x_0)<r$. I want to show $d(a,y)<r_1$ implies $d(a,x_0)<r$. By triangle inequality, $d(a,y)\\leq d(a,x_0) + d(y,x_0) \\rightarrow$  $d(a,y)<r_1\\leq d(a,x_0)+d(y,x_0)<2r...$\nI'm a little stuck after this point.", "output": "You need to specify $r_1$.\nFor $y\\in  B_r(x_0)$, let $r_1=r-d(y,x_0)$. Then if $x\\in B_{r_1}(y)$\n$$\nd(x,x_0)\\le d(x,y)+d(y,x_0)< r_1+ d(y,x_0)=r.\n$$\nSo $ B_{r_1}(y)\\subseteq B_r(x_0)$. This shows that $B_r(x_0)$ is open.\n$\\quad\\quad\\quad\\quad\\quad\\quad\\quad$", "meta": {"post_id": 104083, "input_score": 32, "output_score": 88, "post_title": "An open ball is an open set"}}
{"input": "What is the difference between a direct product and a semi-direct product in group theory? \nBased on what I can find, difference seems only to be the nature of the groups involved, where a direct product can involve any two groups and the semi-direct product only allows a normal subgroup $N$ of some group $G$ and another subgroup of $G$ that intersects trivially with $N$. \nIs this all? What are the significance? Thank you.", "output": "Let's look at three related concepts, in increasing order of complexity:\n\nDirect products. We say that $G$ is (isomorphic to) a direct product of $M$ and $N$ if and only if there exist subgroups $H$ and $K$ of $G$ such that:\n\n$H\\cong M$ and $K\\cong N$;\n$H\\triangleleft G$ and $K\\triangleleft G$;\n$H\\cap K=\\{e\\}$;\n$G=HK$.\n\nSemidirect products. We say that $G$ is (isomorphic to) a semidirect product of $M$ by $N$ if and only if there exist subgroups $H$ and $K$ of $G$ such that:\n\n$H\\cong M$ and $K\\cong N$;\n$H\\triangleleft G$;\n$H\\cap K=\\{e\\}$;\n$G=HK$. \n\nExtensions. We say that $G$ is (isomorphic to) an extension of $M$ by $N$ if and only if there exists a subgroup $H$ of $G$ such that\n\n$H\\cong M$;\n$H\\triangleleft G$;\n$G/H\\cong N$.\n\n\n1 and 2 look very similar. In fact, 1 is a special case of 2 (when $K$ is normal); and 2 is a special case of 3: if $G=HK$, $H\\triangleleft G$, and $H\\cap K=\\{e\\}$, then $K$ maps isomorphically onto $G/H$ via the natural projection (the intersection with the kernel is trivial, so the projection restricted to $K$ is one-to-one; and every element of $G$ can be written as $x=hk$ with $h\\in H$ and $k\\in K$, so $Hx = Hk$, hence the map is onto when restricted to $K$). \nBut each one is a more general construction than the previous one, yielding more general types of groups.\nFor instance, in Direct Products, the conditions immediately imply that elements of $H$ commute with elements of $K$:\nLemma. Let $G$ be a group, and let $H$ and $K$ be normal subgroups of $G$. If $H\\cap K=\\{e\\}$, then $hk=kh$ for all $h\\in H$ and $k\\in K$.\nProof. Consider $hkh^{-1}k^{-1}$. Since $K$ is normal in $G$, then \n$$hkh^{-1}k^{-1} = (hkh^{-1})k^{-1} \\in (hKh^{-1})K = KK = K;$$\nand since $H$ is normal in $G$, then\n$$hkh^{-1}k^{-1} = h(kh^{-1}k^{-1}) \\in H(kHk^{-1}) = HH = H.$$\nTherefore, $hkh^{-1}k^{-1}\\in H\\cap K = \\{e\\}$, so $hkh^{-1}k^{-1}=e$. Multiplying on the right by $kh$ gives $hk=kh$, as desired. $\\Box$\nSo, for example, if all you know are direct abelian groups, then direct products will only give you abelian groups. If both $M$ and $N$ have exponent $k$, then direct products will give you a group of exponent $k$. (In fact, any identity satisfied by both $M$ and $N$ will be satisfied by $M\\times N$; but this is perhaps a little advanced for you right now, so don't worry too much about it).\nBy contrast, semidirect products are more complicated, because that second subgroup doesn't have to be normal. The argument above is invalid, and we don't always get that elements of $H$ and elements of $K$ commute (if they do, then you have a direct product). The smallest example is $S_3$, the nonabelian group of order $6$ viewed as the permutations of $\\{1,2,3\\}$, with $M=C_3$, the cyclic group of order $3$, $N=C_2$, the cyclic group of order $2$, and $H=\\{I, (1,2,3), (1,3,2)\\}$, $K = \\{I, (1,2)\\}$ (other choices of $K$ are possible). \nIn a semidirect product, the fact that $H$ is normal means that for every $k\\in K$ you have $kHk^{-1}=H$; that is, each $k$ induces an automorphism of $H$. So we can define a homomorphism $K\\to \\mathrm{Aut}(H)$, by letting $k$ map to the homomorphism $h\\mapsto khk^{-1}$. If this map is trivial, you get the direct product of $H$ and $K$. If the map is not trivial, then you get more interesting groups. Different homomorphisms may lead to nonisomorphic groups, so that now we have to be careful: while there is one and only one way to construct a \"direct product\" of two groups $M$ and $N$, there may, in general, be many (non-equivalent) ways of constructing semidirect products of $M$ by $N$.\nNote that it is now possible to have a semidirect product of abelian groups that is not abelian (as in the $S_3$ example). And it is no longer true that if both $M$ and $N$ are of exponent $k$, then a semidirect product will also have exponent $k$. For example, take $M=C_2\\times C_2 = \\{1,x\\}\\times\\{1,x\\}$, which is of exponent $2$, take $N=\\{1,n\\} = C_2$, also of exponent $2$, and let the nontrivial element of $N$ act on $M$ by the rule $n^{-1}(a,b)n = (b,a)$. Then $(x,1)n$ has order $4$:\n$$\\begin{align*}\r\n\\bigl((x,1)n\\bigr)^2 = (x,1)n(x,1)n &= (x,1)(n^{-1}(x,1)n) = (x,1)(1,x) = (x,x)\\\\\r\n\\bigl((x,1)n\\bigr)^3 = (x,x)(x,1)n &= (1,x)n\\\\\r\n\\bigl((x,1)n\\bigr)^4 = (1,x)n(x,1)n &= (1,x)(n^{-1}(x,1)n) = (1,x)(1,x) = (1,1).\r\n\\end{align*}$$\nExtensions are even more complex: in essence, every finite group can be viewed as a sequence of extensions of simple groups (hence, in part, the interest in classifying all finite simple groups). Not every extension is a semidirect product (or a direct product). For example, $\\mathbb{Z}_4$, the cyclic group of order $4$, is an extension of $\\mathbb{Z}_2$ by $\\mathbb{Z}_2$: the subgroup $H=\\{\\overline{0},\\overline{2}\\}$ is cyclic of order $2$ and normal, and the quotient $\\mathbb{Z}_4/H$ is of order $2$, hence cyclic of order $2$. If it were a semidirect product of $\\mathbb{Z}_2$ by $\\mathbb{Z}_2$, then being abelian it would necessarily be a direct product, and so would have exponent $2$; so it cannot be written as a semidirect product.\nAs I mentioned, every group can be expressed as a sequence of extensions using simple groups. By the Jordan-H\u00f6lder Theorem, although the sequence is not unique, the precise simple groups that occur is (counting multiplicity). \nThe definitions look quite similar: we just drop the condition of normality for one factor when going from direct product to semidirect product; we just exchange \"there is a subgroup isomorphic to $N$ that maps isomorphically onto the quotient\" with \"the quotient is isomorphic to $N$\" in going from semidirect product to extension. But the consequences of these \"little changes\" is large. Much like the difference between \"finite abelian group\" and \"finite group\" looks very small (just a single line dropped), but the implications in our ability to classify/understand the objects in question are enormous.", "meta": {"post_id": 106028, "input_score": 67, "output_score": 129, "post_title": "Semi-direct v.s. Direct products"}}
{"input": "Definition of a countable set, from Stanford, as I didn't want to quote Wikipedia:\n\nDefinition. A set S is countable if |S| = |N|.\nThus a set S is countable if there is a one-to-one mapping of Num onto S, that is, if S is the range of an infinite one-to-one sequence.\n\nSo it seems that if we can define a set of numbers that does not map one-to-one to the natural numbers, then it is not a countable set. The natural numbers quite obviously map one-to-one to the natural numbers, so can they possibly be uncountable?\nLet's say we have a list containing all of the natural numbers. Excerpt:\n\n...\n000099\n000100\n000101\n000102\n...\n\nWe can define a number that is different from each element in this list as follows: for the ith number in the list, the ith digit is one of the 8 (or 9) non-zero alternatives that make our new number different from the number on the list. For example:\n\n...\n00009 9\n0001 0 0\n000 1 01\n00 0 102\n...\n12 3456\n\nAs we keep going, we will end up with a sequence of non-zero digits, which forms a valid Natural number, that is not on our list of all natural numbers, so our mapping of the natural numbers to the natural numbers breaks.\nDoes this make sense, or is there something I'm missing?", "output": "Your error is in the assertion:\n\n... we will end up with a sequence of non-zero digits, which forms a valid Natural number...\n\nNatural numbers have only finitely many nonzero digits in their decimal expansion. This can be proven by induction: it is true of $1$; and the number of nonzero digits in the decimal expansion of $n+1$ is at most one more than that of $n$ (you need at most one more digit before you start padding with $0$s on the left).\nAn infinite list which contains infinitely many nonzero digits does not yield a natural number. In fact, one can prove that your procedure will produce a sequence of digits with infinitely many nonzero digits, hence not correspond to a natural number.\nLet's say that your procedure selects $0$ whenever possible (that is, whenever the $i$th digit, from right to left, of the $i$th natural number, is nonzero; you really want this, because if you insist, as in your post, that you always select a nonzero digit, then you guarantee what you get is not a natural number). Can the list be arranged in such a way that the $i$th digit of the $i$th number is nonzero for all $i\\gt N$ for some $N$? No: for any given $N$, there are $10^{N-1}$ natural numbers that require fewer than $N$ digits to write; since there are only $N$ positions prior to $N$, at least one of these $10^{N-1}$ numbers must be listed in a position below $N$; but if it is listed in position $j\\gt N$, then its $j$th digit will be $0$, so the constructed sequence will necessarily have a nonzero entry at $j\\gt N$. Since this holds for all $N$, the list obtained is never eventually $0$, and so does not correspond to a natural number.\n\nNow, if you want to extend your notion of numbers to include expressions with infinitely many nonzero digits, then your argument is correct: the set of all such \"numbers\" is not countable. However, those are not the natural numbers.", "meta": {"post_id": 106546, "input_score": 7, "output_score": 37, "post_title": "Can the natural numbers be uncountable?"}}
{"input": "Is a set of convex functions closed under composition? I don't necessarily need a proof, but a reference would be greatly appreciated.", "output": "There is no need for the first function in the composition to be nondecreasing. And here is a proof for the nondifferentiable case as well. The only assumptions are that the composition is well defined at the points involved in the proof for every $\\alpha \\in [0, 1]$ and that $f_n, f_{n - 1}, \\dots, f_1$ are convex nondecreasing functions of one variable and that $f_0 : \\mathbb R^n \\to \\mathbb R$ is a convex function.\nFirst let $g : \\mathbb R^m \\to \\mathbb R$ a convex function and $f : \\mathbb R \\to \\mathbb R$ a convex nondecreasing function, then, by convexity of $g$:\n$$\ng( \\alpha x + ( 1 - \\alpha ) y ) \\leq \\alpha g( x ) + ( 1 - \\alpha )g( y ).\n$$\nSo, using the fact that f is nondecreasing:\n$$\nf( g( \\alpha x + ( 1 - \\alpha ) y ) ) \\leq f( \\alpha g( x )+ ( 1 - \\alpha )g( y ) ).\n$$\nTherefore, again by convexity:\n$$\nf( g( \\alpha x + ( 1 - \\alpha ) y ) ) \\leq \\alpha f( g( x ) ) + ( 1 - \\alpha )f( g( y ) ).\n$$\nThis reasoning can be used inductively in order to prove the result that\n$$\nf_n \\circ f_{n - 1}\\circ\\cdots\\circ f_0\n$$\nis convex under the stated hypothesis. And the composition will be nondecreasing if $f_0$ is nondecreasing.", "meta": {"post_id": 108393, "input_score": 50, "output_score": 45, "post_title": "Is the composition of $n$ convex functions itself a convex function?"}}
{"input": "In the process of touching up some notes on infinite series, I came across the following \"result\":\n\nTheorem: For an ordered field $(F,<)$, the following are equivalent:\n(i) Every Cauchy sequence in $F$ is convergent.\n(ii) Absolutely convergent series converge: $\\sum_n |a_n|$ converges in $F$ $\\implies$ $\\sum_n a_n$ converges in $F$.\n\nBut at present only the proof of (i) $\\implies$ (ii) is included, and unfortunately I can no longer remember what I had in mind for the converse direction.  After thinking it over for a bit, I wonder if I was confusing it with this result:\n\nProposition: In a normed abelian group $(A,+,|\\cdot|)$, the following are equivalent:\n(i) Every Cauchy sequence is convergent.\n(ii) Absolutely convergent series converge: $\\sum_n |a_n|$ converges in $\\mathbb{R}$ $\\implies$ $\\sum_n a_n$ converges in $A$.\n\nFor instance, one can use a telescoping sum argument, as is done in the case of normed linear spaces over $\\mathbb{R}$ in (VIII) of this note.\nBut the desired result is not a special case of this, because by definition the norm on a normed abelian group takes values in $\\mathbb{R}^{\\geq 0}$, whereas the absolute value on an ordered field $F$ takes values in $F^{\\geq 0}$.\nI can show (ii) $\\implies$ (i) of the Theorem for ordered subfields of $\\mathbb{R}$.  Namely, every real number $\\alpha$ admits a signed binary expansion $\\alpha = \\sum_{n = N_0}^{\\infty} \\frac{\\epsilon_n}{2^n}$, with $N_0 \\in \\mathbb{Z}$ and $\\epsilon_n \\in \\{ \\pm 1\\}$, and the associated \"absolute series\" is $\\sum_{n=N_0}^{\\infty} \\frac{1}{2^n} = 2^{1-N_0}$.\nBecause an ordered field is isomorphic to an ordered subfield of $\\mathbb{R}$ iff it is Archimedean, this actually proves (ii) $\\implies$ (i) for Archimedean ordered fields.  But on the one hand I would prefer a proof of this that does not use the (nontrivial) result of the previous sentence, and on the other hand...what about non-Archimedean ordered fields?\nAdded: The article based on this question and answer has at last appeared:\n\nClark, Pete L.; Diepeveen, Niels J.;\nAbsolute Convergence in Ordered Fields.\nAmer. Math. Monthly 121 (2014), no. 10, 909\u2013916.\n\nIf you are a member of the MAA, you will be frustrated if you try to access it directly: the issue is currently advertised on their website but the articles are not actually available to members.  The article is available on JSTOR and through MathSciNet.  Anyway, here is an isomorphic copy.  Thanks again to Niels Diepeveen!", "output": "Proving this for arbitrary ordered fields is a little\ntrickier than for Archimedean fields, partly because\nthere are no concrete sequences -- other than eventually\nconstant ones -- that are guaranteed to converge or\neven to be Cauchy sequences and partly because we lack\nthe embedding in a completely ordered field.\nThese problems can be overcome by constructing all the\nnecessary sequences and series from the one Cauchy\nsequence we assume to exist. To begin with, note two\nimportant facts. First, for a Cauchy sequence to\nconverge, it is sufficient that some subsequence\nconverges. Second, any sequence has a strictly increasing\nsubsequence, a strictly decreasing subsequence or a\nconstant subsequence. For this problem, the latter\ncase is trivial and the first two can be reduced to each\nother by negation, so we need to prove only one of them.\nLet $K$ be an ordered field in which every absolutely\nconvergent series is convergent. If $\\{a_n\\}$ is a strictly\nincreasing Cauchy sequence in $K$, then $\\{a_n\\}$ converges.\nProof:\nLet $b_n = a_{n+1} - a_n$. Then ${b_n}$ is positive and\nconverges to $0$, so it has a strictly decreasing\nsubsequence $\\{b_{n_k}\\}$.\nLet $c_k = b_{n_k} - b_{n_{k+1}}$. We now have a convergent\nseries with positive terms $\\sum_{k=1}^\\infty c_k = b_{n_1}$.\nAs $\\{a_n\\}$ is a Cauchy sequence, it has a subsequence $\\{a_{m_k}\\}$\nsuch that $a_{m_{k+1}} - a_{m_k} < c_k$ for all $k$.\nNow consider the series $\\sum_{i=1}^\\infty d_i$ where\n$d_{2k-1} = a_{m_{k+1}} - a_{m_k}$ and\n$d_{2k} = a_{m_{k+1}} - a_{m_k} - c_k$.\nNote that $-c_k < d_{2k} < 0 < d_{2k-1} < c_k$, so we can pair off\nterms to get\n$$\r\n\\sum_{i=1}^\\infty |d_i| = \\sum_{k=1}^\\infty (d_{2k-1} - d_{2k})\r\n  = \\sum_{k=1}^\\infty c_k = b_{n_1}\r\n$$\nBy the hypothesis on $K$ we may conclude that $\\sum_{i=1}^\\infty d_i$\nconverges and\n$$\r\n\\sum_{i=1}^\\infty d_i + \\sum_{k=1}^\\infty c_k\r\n  = \\sum_{k=1}^\\infty (d_{2k-1} + d_{2k} + c_k)\r\n  = 2 \\sum_{k=1}^\\infty (a_{m_{k+1}} - a_{m_k}).\r\n$$\nBecause a Cauchy sequence with a convergent subsequence converges,\nwe have\n$$\r\n\\lim_{n \\to \\infty} a_n = a_{m_1} + \\sum_{k=1}^\\infty (a_{m_{k+1}} - a_{m_k})\r\n  = a_{m_1} + \\frac{1}{2}\\left(b_{n_1} + \\sum_{i=1}^\\infty d_i \\right)\r\n$$\n\nTo the question \"how did I come up with this?\": there are\nnot many things that could possibly work. The problem is\nset in an environment where none of the power tools of\nanalysis work. Basic arithmetic works, inequalities work,\nsome elementary properties of sequences and series work, but\nif you want to take a limit of something it'd better be\nconvergent by hypothesis or by construction.\nOne more or less obvious attack is by contraposition: assume\nthat there is a divergent Cauchy sequence and try to construct\na divergent, absolutely convergent series. Such a series\nmust be decomposable into a positive part $a$ and a negative\npart $b$, where $a+b$ diverges and $a-b$ converges. This\nis possible in several ways by taking $a$ and $b$ to be linear\ncombinations of known convergent and divergent series.\nA complication is that the terms of the convergent series\nmust dominate those of the divergent series, as they must\ncontrol the signs. I wasted a lot of time trying to get the\nconvergent series to do this, which is very hard, perhaps\nimpossible. Then I turned to the proof for vector spaces\nfor inspiration, and saw that it was in fact very easy to\nadjust the divergent series instead, as the partial sums\nare a Cauchy sequence. I also adopted the\noverall structure of that proof, which is why the final\nversion is not by contraposition.", "meta": {"post_id": 111164, "input_score": 57, "output_score": 45, "post_title": "In which ordered fields does absolute convergence imply convergence?"}}
{"input": "Cauchy's integral formula says\n$$\r\nf^{(n)}(z)=\\frac{n!}{2\\pi i}\\int_C\\frac{f(\\zeta)d\\zeta}{(\\zeta-z)^{n+1}}.\r\n$$\nIf we let $C$ be the circle of radius $r$, such that $|f(\\zeta)|\\leq M$ on $C$, then taking $z=a$, one obtains Cauchy's estimate that \n$$\r\n|f^{(n)}(a)|\\leq Mn!r^{-n}.\r\n$$\nHow is this derived? I see instead\n$$\r\n|f^{(n)}(a)|\\leq\\frac{n!}{2\\pi}\\int_C \\frac{|f(\\zeta)||d\\zeta|}{|\\zeta-a|^{n+1}}\\leq Mn!\\int_C\\frac{|d\\zeta|}{|\\zeta-a|^{n+1}}\r\n$$\nbut I don't see how this eventually gets to Cauchy's estimate.", "output": "By Cauchy's integral formula you have given, we have \n$$f^{(n)}(a)=\\frac{n!}{2\\pi i}\\int_C\\frac{f(\\zeta)d\\zeta}{(\\zeta-a)^{n+1}}$$\nwhere $C$ is a circle of radius $r$ centered at $a$. Therefore, $C$ can be parametrized as $\\zeta=a+re^{i\\theta}$, $0\\leq \\theta\\leq 2\\pi$, which implies \n$$|f^{(n)}(a)|=\\left|\\frac{n!}{2\\pi i}\\int_0^{2\\pi}\\frac{f(a+re^{i\\theta})rie^{i\\theta}d\\theta}{(re^{i\\theta})^{n+1}}\\right|\\leq\\frac{n!}{2\\pi }\\int_0^{2\\pi}\\left|\\frac{f(a+re^{i\\theta})rie^{i\\theta}}{(re^{i\\theta})^{n+1}}\\right|d\\theta$$\n$$=\\frac{n!}{2\\pi }\\int_0^{2\\pi}\\frac{|f(a+re^{i\\theta})|}{r^n}d\\theta\\leq \\frac{n!}{2\\pi }\\int_0^{2\\pi}\\frac{M}{r^n}d\\theta=\\frac{Mn!}{r^n}$$\nwhere the last equality follows from $|e^{i\\theta}|=1$ and $|i|=1$, and the last inequality follows from the assumption that $|f|\\leq M$ on $C$.", "meta": {"post_id": 114349, "input_score": 22, "output_score": 37, "post_title": "How is Cauchy's estimate derived?"}}
{"input": "I'm trying to show the inclusion :\n$\\ell^p\\subseteq\\ell^q$ for real-value sequences, and show that the norms satisfy: $\\|\\cdot\\|_q<\\|\\cdot\\|_p$.\nI think I can show the first part without much trouble:\nTake $a_n$ in $\\ell^p$, then the partial sums are a Cauchy sequence, i.e., for any $\\epsilon>0$ , there is a natural $N$ with $|S_{n,p}-S_{k,p}|<\\epsilon$ for $n,k>N$, and $S_{n,p}$ the partial sums of $|a_n|^p$ and the individual terms go to $0$. So, we choose an index $J$ with $a_j<1$ for $j>J$. We then use that $f(x)=a^x$ decreases in $[0,1]$. This means that $|a_j|^p<|a_j|^q$.\nSo the tail of $S_{n,q}$, the partial sums of $|a_n|^q$ decrease fast-enough to converge, by comparison with the tail of $S_{n,p}$.\nBut I'm having trouble showing $\\|\\cdot\\|_q<\\|\\cdot\\|_p$ . Also, is there a specific canonical embedding between the two spaces?", "output": "Let $x\\in \\ell^p$ and $0<p<q<+\\infty$. If $x=0$, then everything is obvious. Otherwise consider $e=\\frac{x}{\\Vert x\\Vert_p}$. For all $k\\in\\mathbb{N}$ we have $|e_k|\\leq 1$ and $\\Vert e\\Vert_p=1$. Now since $p<q$ we get\n$$\n\\Vert e\\Vert_q=\n\\left(\\sum\\limits_{k=1}^\\infty |e_k|^q\\right)^{1/q}\\leq \n\\left(\\sum\\limits_{k=1}^\\infty |e_k|^p\\right)^{1/q}=\n\\Vert e\\Vert_p^{p/q}=1\n$$\nThen we can write\n$$\n\\Vert x\\Vert_q=\\Vert \\Vert x\\Vert_p e\\Vert_q=\\Vert x\\Vert_p\\Vert e\\Vert_q\\leq\\Vert x\\Vert_p\n$$\nIn fact this inequality means that $\\ell^p\\subseteq \\ell^q$. Also we can exclude the equality sign in this inclusion, because the sequence $x(k)=k^{-\\frac{1}{p}}$ belongs to $\\ell^q$ but not to $\\ell^p$. If we assume that $p\\geq 1$, we can speak of normed spaces $\\ell^p$ and $\\ell^q$. Then the last inequality means that the natural inclusion $i:\\ell^p\\to \\ell^q:x\\mapsto x$ is a continuous linear operator. \nIt is worth to note that the inequality $\\Vert\\cdot\\Vert_p\\leq C\\Vert\\cdot\\Vert_q$ is impossible for any constant $C\\geq 0$. Indeed consider sequences\n$$\nx_n(k)=\n\\begin{cases}\n1,\\qquad 1\\leq k\\leq n\\\\\n0,\\qquad k>n\n\\end{cases}\n$$\nThen\n$$\nC\\geq\\lim\\limits_{n\\to\\infty}\\frac{\\Vert x_n\\Vert_p}{\\Vert x_n\\Vert_q}=\\lim\\limits_{n\\to\\infty}n^{\\frac{1}{p}-\\frac{1}{q}}=+\\infty.\n$$\nTherefore such a constant $C>0$ doesn't exist.", "meta": {"post_id": 114650, "input_score": 19, "output_score": 41, "post_title": "$\\ell^p\\subseteq\\ell^q$ for $0<p<q<\\infty$ and $\\|\\cdot\\|_q<\\|\\cdot\\|_p$"}}
{"input": "What is the \"standard basis\" for fields of complex numbers?\nFor example, what is the standard basis for $\\Bbb C^2$ (two-tuples of the form: $(a + bi, c + di)$)?  I know the standard for $\\Bbb R^2$ is $((1, 0), (0, 1))$.  Is the standard basis exactly the same for complex numbers?\nP.S. - I realize this question is very simplistic, but I couldn't find an authoritative answer online.", "output": "Just to be clear, by definition, a vector space always comes along with a field of scalars $F$.  It's common just to talk about a \"vector space\" and a \"basis\"; but if there is possible doubt about the field of scalars, it's better to talk about a \"vector space over $F$\" and a \"basis over $F$\" (or an \"$F$-vector space\" and an \"$F$-basis\").\nYour example, $\\mathbb{C}^2$, is a 2-dimensional vector space over $\\mathbb{C}$, and the simplest choice of a $\\mathbb{C}$-basis is $\\{ (1,0), (0,1) \\}$.\nHowever, $\\mathbb{C}^2$ is also a vector space over $\\mathbb{R}$.  When we view $\\mathbb{C}^2$ as an $\\mathbb{R}$-vector space, it has dimension 4, and the simplest choice of an $\\mathbb{R}$-basis is $\\{(1,0), (i,0), (0,1), (0,i)\\}$.\nHere's another intersting example, though I'm pretty sure it's not what you were asking about:\nWe can view $\\mathbb{C}^2$ as a vector space over $\\mathbb{Q}$.  (You can work through the definition of a vector space to prove this is true.)  As a $\\mathbb{Q}$-vector space, $\\mathbb{C}^2$ is infinite-dimensional, and you can't write down any nice basis.  (The existence of the $\\mathbb{Q}$-basis depends on the axiom of choice.)", "meta": {"post_id": 123448, "input_score": 79, "output_score": 104, "post_title": "What is the \"standard basis\" for fields of complex numbers?"}}
{"input": "Can someone give an example of a ring $R$, a left $R$-module $M$ and a submodule $N$ of $M$ such that $M$ is finitely generated, but $N$ is not finitely generated?\n\nI tried a couple of examples of modules I know and got nothing...", "output": "Consider the simplest possible nontrivial (left) $R$-module: $R$ itself. It's certainly finitely generated, by $\\{ 1 \\}$. The submodules are exactly the (left) ideals of $R$. So you want a ring which has (left) ideals which are not finitely generated. For example, you could use a non-Noetherian commutative ring, such as $\\mathbb{Z}[X_1, X_2, X_3, \\ldots ]$.", "meta": {"post_id": 125015, "input_score": 43, "output_score": 64, "post_title": "Finitely generated module with a submodule that is not finitely generated"}}
{"input": "Let $A$ and $B$ be two sets of real numbers. Define the distance from $A$ to $B$ by $$\\rho (A,B) = \\inf \\{ |a-b| : a \\in A, b \\in B\\} \\;.$$ Give an example to show that the distance between two closed sets can be $0$ even if the two sets are disjoint.", "output": "Let $A = \\mathbb N$ and let $B = \\left\\{n+\\frac{1}{2n} :n\\in \\mathbb N\\right\\}$.  Then A and B are closed and disjoint, but $$\\inf \\{|a\u2212b|:a \\in A,b \\in B\\} = \\inf \\left | \\frac{1}{2n}\\right| = 0$$", "meta": {"post_id": 125709, "input_score": 26, "output_score": 40, "post_title": "Example to show the distance between two closed sets can be 0 even if the two sets are disjoint"}}
{"input": "In my introductory Analysis course, we learned two definitions of continuity.\n$(1)$ A function $f:E \\to \\mathbb{C}$ is continuous at $a$ if every sequence $(z_n) \\in E$ such that $z_n \\to a$ satisfies $f(z_n) \\to f(a)$.\n$(2)$ A function $f:E \\to \\mathbb{C}$ is continuous at $a$ if $\\forall \\varepsilon>0, \\exists \\delta >0:\\forall z \\in E, |z-a|<\\delta \\implies |f(z)-f(a)|<\\varepsilon$.\nThe implication $(2)\\implies(1)$ is trivial (though I will happily post a proof if there is sufficient interest).  The proof of the implication $(1)\\implies(2)$ is worth remarking on, though.\nProof that $(1)\\implies(2)$:\nSuppose on the contrary that $\\exists \\varepsilon>0:\\forall \\delta>0, \\exists z \\in E:\\left (|z-a|<\\delta \\; \\mathrm{and} \\; |f(z)-f(a)|\\ge \\varepsilon\\right )$.  Let $A_n$ be the set $\\{z\\in E:|z-a|<\\frac{1}{n} \\; \\mathrm{ and }\\; |f(z)-f(a)|\\ge\\varepsilon\\}$.  Now use the Axiom of Choice to construct a sequence $(z_n)$ with $z_n \\in A_n \\; \\forall n \\in \\mathbb{N}$.  But now $a-\\frac{1}{n}<z_n<a+\\frac{1}{n}\\; \\forall n \\in \\mathbb{N}$ so $z_n \\to a$.  So $f(z_n) \\to f(a)$.  But $|f(z_n)-f(a)|\\ge\\varepsilon\\; \\forall n \\in \\mathbb{N}$, which is a contradiction.\nYou will have noticed that the above proof uses the Axiom of Choice (the lecturer didn't explicitly spell out the dependence, but it's definitely there).  My question is: is it possible to prove that $(1) \\implies (2)$ without using the Axiom of Choice.  I strongly suspect that it isn't.  In that case, can anyone prove that we have to use the Axiom of Choice?  I can think of three ways to do this:\n(A) Show that $\\left( (1) \\implies (2)\\right)\\implies \\mathrm{AC}$.  I suspect that this statement is untrue. This is definitely untrue, as Arthur points out, because I only used the axiom of countable choice, which is strictly weaker than AC.\n(B) Show that $(1)\\implies (2)$ is equivalent to some other statement known to require the Axiom of Choice (the obvious example being the well-ordering of the real numbers).\n(C) Construct or show the existence of a model of ZF in which there exist sequences which satisfy $(1)$ but not $(2)$.\nOf course, if anyone can think of another way, I would be very interested to hear about it.\nOne final note - I am aware that very many theorems in Analysis use the Axiom of Choice in one way or another, and that this is just one example of such a theorem.  If there exists a model of ZF like the one described in (C), is the study of Analysis in that model interesting?", "output": "The implication $(1)\\to (2)$ can be proved in ZF alone, though this requires major revision of the argument.\nAssume that $f:\\Bbb R\\to\\Bbb R$ is sequentially continuous (i.e., satisfies (1)). Let $x\\in\\Bbb R$ be arbitrary.\n\nClaim: $f\\upharpoonright(\\Bbb Q\\cup\\{x\\})$ is continuous at $x$.\nProof: Enumerate $\\Bbb Q=\\{q_n:n\\in\\omega\\}$. If $f\\upharpoonright(\\Bbb Q\\cup\\{x\\})$ is not continuous at $x$, there is an $\\epsilon>0$ such that for each $k\\in\\omega$, $$A_k\\triangleq\\{q\\in\\Bbb Q:|q-x|<2^{-k}\\text{ and }|f(q)-f(x)|\\ge\\epsilon\\}\\ne\\varnothing\\;.$$ Let $$n(0)=\\min\\{k\\in\\omega:q_k\\in A_0\\}\\;.$$ Given $n(m)$, let $$n(m+1)=\\min\\{k\\in\\omega:k>n(m)\\text{ and }q_k\\in A_{m+1}\\}\\;.$$ Then $\\langle q_{n(k)}:k\\in\\omega\\rangle\\to x$, but $|f(q_{n(k)})-f(x)|\\ge\\epsilon$ for all $k\\in\\omega$, which is a contradiction. Note that no choice was used in this construction. $\\dashv$\n\nNow let $\\epsilon>0$. It follows from the Claim that there is a $\\delta>0$ such that $|f(x)-f(q)|\\le\\epsilon$ whenever $q\\in\\Bbb Q$ and $|x-q|\\le\\delta$. Now suppose that $y\\in\\Bbb R$ with $|x-y|\\le\\delta$. Let $I$ be the closed interval whose endpoints are $x$ and $y$. For each $k\\in\\omega$ let $$n(k)=\\min\\{m\\in\\omega:q_m\\in I\\text{ and }|q_m-y|<2^{-k}\\}\\;.$$ Then $\\langle q_{n(k)}:k\\in\\omega\\rangle\\to y$, so $\\langle f(q_{n(k)}):k\\in\\omega\\rangle\\to f(y)$. However, each $q_{n(k)}\\in I$, so $|q_{n(k)}-x|\\le\\delta$ for each $k\\in\\omega$, and therefore $|f(q_{n(k)})-f(x)|\\le\\epsilon$ for each $k\\in\\omega$. Clearly this implies that $|f(y)-f(x)|\\le\\epsilon$ as well.\nThus, we\u2019ve shown that for each $\\epsilon>0$ there is a $\\delta>0$ such that $|f(y)-f(x)|\\le\\epsilon$ whenever $|y-x|<\\delta$, which is sufficient.\nThis argument is expanded from the ZF proof of Theorem 3.15 in Horst Herrlich, Axiom of Choice, Lecture Notes in Mathematics 1876. Note that it is not true in ZF that a function $f:\\Bbb R\\to\\Bbb R$ is continuous at a point $x$ iff it is sequentially continuous at $x$: in Theorem 4.54 he proves that this assertion is equivalent to $\\mathbf{CC}(\\Bbb R)$, the assertion that every countable family of non-empty subsets of $\\Bbb R$ has a choice function.\nAdded: I failed to notice that the functions in the original question are complex-valued, not real-valued. However, the argument can easily be adapted to $\\Bbb R^2$, replacing $\\Bbb Q$ by $\\Bbb Q^2$ and using the $\\max$ norm.\nAdded2: The only slightly tricky bit is figuring out what to use for $I$. Suppose that $x=\\langle x_1,x_2\\rangle,y=\\langle y_1,y_2\\rangle\\in\\Bbb R^2$ with $0<\\|x-y\\|\\le\\delta$. If $x_1\\ne y_1$ and $x_2\\ne y_2$, let $$I=\\big[\\min\\{x_1,y_1\\},\\max\\{x_1,y_1\\}\\big]\\times\\big[\\min\\{x_2,y_2\\},\\max\\{x_2,y_2\\}\\big]\\;.$$ If $x_1\\ne y_1$ and $x_2=y_2$, let $$I=\\big[\\min\\{x_1,y_1\\},\\max\\{x_1,y_1\\}\\big]\\times\\big[x_2,x_2+\\delta\\big]\\;,$$ and if $x_1=y_1$ and $x_2\\ne y_2$ let $$I=\\big[x_1,x_1+\\delta\\big]\\times\\big[\\min\\{x_2,y_2\\},\\max\\{x_2,y_2\\}\\big]\\;.$$\nLet $\\Bbb Q^2=\\{q_n:n\\in\\omega\\}$ be an enumeration of $\\Bbb Q^2$, and for $k\\in\\omega$ let $$n(k)=\\min\\{m\\in\\omega:q_m\\in I\\text{ and }\\|q_m-y\\|<2^{-k}\\}\\;.$$ Everything else is the same as before, except that $|\\cdot|$ must be replaced throughout by $\\|\\cdot\\|$, where $$\\|\\langle x,y\\rangle\\|=\\max\\{|x|,|y|\\}\\;.$$\nWith very minor modification this works for $\\Bbb R^n$ for any $n\\in\\Bbb Z^+$.", "meta": {"post_id": 126010, "input_score": 71, "output_score": 35, "post_title": "Continuity and the Axiom of Choice"}}
{"input": "I'm self-studying some complex analysis, and apparently holomorphic bijections between two annuli exist precisely when the ratios of the radii are the same. More exactly, if $A_{\\sigma,\\rho}=\\{z\\in\\mathbb{C}:\\sigma<|z|<\\rho\\}$, then there is a holomorphic bijection between $A_{\\sigma,\\rho}$ and $A_{\\sigma',\\rho'}$ iff $\\rho/\\sigma=\\rho'/\\sigma'$.\nIs there a reference where this fact is proven? Or can a proof be included here if it's not overly involved? Thanks.", "output": "This result (sometimes called Schottky's theorem) can be proved without any heavy machinery like Riemann mapping or even Schwarz reflection. I give two versions of the proof, with comments at the end.\nClaim. If there exists a bijective holomorphic map $f: A_{r,R}\\to A_{s,S}$, then $S/s\\ge R/r$.\nNote that we actually get equality by considering the inverse.\nProof. Normalize to $r=s=1$. By composing $f$ with inversion, we can make sure that $|f(z)|\\to 1$ as $|z|\\to1$. For $1<t<R$ let $A(t)$ denote the area within the Jordan curve $f(\\{z:|z|=t\\})$. There is a standard way to relate the area to the coefficient of the Laurent series $f(z)=\\sum_{n\\in\\mathbb Z} c_n z^n$: namely, use Green's formula for area in complex notation. \n$$A(t)=\\frac{1}{2i} \\int_{0}^{2\\pi}  f(te^{i\\theta}) \\bar f(te^{i\\theta})\\, d\\theta = \\pi \\sum_{n\\in\\mathbb Z}  n|c_n|^2t^{2n}$$ \nSince $A(1+)=\\pi$, we have \n$$(1)\\qquad\\qquad \\sum_{n\\in\\mathbb Z}  n|c_n|^2=1.$$\nFrom here the proof can proceed in at least two ways.\nVersion I: stick to area. Using (1), write\n$$A(t)-\\pi t^2=\\pi t^2 \\sum_{n\\in\\mathbb Z}  n|c_n|^2(t^{2n-2}-1)\\ge0$$\nwhere the inequality holds term-wise. Hence $A(R-)\\ge \\pi R^2$, which implies $S\\ge R$. QED\nVersion II: use $L^2$ norm. For $1<t<R$ define\n$$U(t)=\\frac{1}{2\\pi t} \\int_{|z|=t} |f(z)|^2|dz|$$ \nAgain, this can be put in terms of coefficients either by direct computation with $|f|^2=f\\bar f$, or by Parseval's identity: \n $U(t)= \\sum_{n\\in\\mathbb Z} |c_n|^2 t^{2n}$.\nSince $U(1+)=1$ and \n$$U'(t)-2t= 2t\\sum_{n\\in\\mathbb Z}  n|c_n|^2 (t^{2n-2}-1)\\ge 0 $$\nit follows that $U(R-)\\ge R^2$. But $|f|\\le S$, which implies $S\\ge R$. QED\nComments: the idea of relating coefficients to area as in (1) goes back to Gronwall. In the context of this problem it was used by P\u00f3lya and Szeg\u0151 in Problems and Theorems in Analysis II, part IV, problem 83 on page 15. Their solution is essentially Version I. I learned it from a short note by Pietro Poggi-Corradini. \nThe idea of using $L^2$ norm instead of area may look counterproductive, since Version II is longer. But it is a more robust argument, which works for harmonic maps as well. Robert Burckel and Pietro pointed out Version II to us as a simplification of $L^2$ argument specialized to holomorphic maps.", "meta": {"post_id": 133578, "input_score": 33, "output_score": 34, "post_title": "When can we find holomorphic bijections between annuli?"}}
{"input": "I am trying to get a better grasp of representation theory. I was asking myself \"what is the essential difference between representations of some group $G$ and a $KG$ module? How are they related, and what is the distinction?\"\nWhat's confusing me is: I can understand matrix representations of a group in a simple way, since they are isomorphic to some permutation group, but what about a module? How do I get things cleared out? I need some insight.", "output": "There's essentially no real difference between modules and representations. Think of them as two sides of the same coin.\nGiven a $\\mathbb{K}G$-module $V$, you have a linear action of $G$ on a $\\mathbb{K}$-vector space $V$. This in turn gives you a homomorphism from $G$ to $\\mathrm{GL}(V)$ (invertible $\\mathbb{K}$-linear endomorphisms). Such a homomorphism is a representation. And then this can be turned around. Given a representation, you get an associated module.\nSpecifically, let $V$ be a $\\mathbb{K}G$-module and let $g,h \\in G$, $v,w\\in V$, and $c\\in\\mathbb{K}$. Give a name to the map: $v \\mapsto g\\cdot v$ say: $\\varphi(g):V \\to V$ (so $\\varphi(g)(v)=g \\cdot v$). Then $\\varphi(g)(v+cw)$ $=g\\cdot(v+cw)$ $=g\\cdot v+cg\\cdot w$ $=\\varphi(g)(v)+c\\varphi(g)(w)$. Thus $\\varphi(g)$ is $\\mathbb{K}$-linear. Then because $\\varphi(1)$ is the identity map ($1 \\cdot v=v$) and $\\varphi(g^{-1})(\\varphi(g)(v))=g^{-1}\\cdot g\\cdot v=(g^{-1}g\\cdot v=1\\cdot v=v$ etc. we get $\\varphi(g)$ is an invertible linear map. Therefore: $\\varphi:G \\to \\mathrm{GL}(V)$. Moreover, $\\varphi(gh)=\\varphi(g)\\varphi(h)$ (easy to check) so $\\varphi$ is a homomorphism (which we call a representation). Without going into the details, this all reverses.\nSo $\\mathbb{K}G$-modules = representations of $G$ on $\\mathbb{K}$-vector spaces.\nIf you've studied group actions, you've already seen this type of correspondence. Let $G$ act on $X$. Then the map $x \\mapsto g \\cdot x$ turns out to be a bijection on $X$. Thus if we define $\\varphi(g)(x)=g\\cdot x$ for all $x\\in X$, then $\\varphi(g) \\in S(X)$ (permutations on $X$). Moreover, $\\varphi(gh)=\\varphi(g)\\varphi(h)$ so $\\varphi : G\\to S(X)$ is a group homomorphism. We call such things permutation representations. And again this can be reversed. Given a permutation representation: $\\varphi:G \\to S(X)$, one can define  a group action $g \\cdot x \\equiv \\varphi(g)(x)$. \nSo $G$-action on $X$ = permutation representation of $G$ on $X$.\nIf you look into other branches of algebra, you'll see this kind of thing over and over again: Lie algebra modules = Lie algebra representations etc.\nIt's just different points of view. You can either think of \"Algebra Thing\" acting on \"Thing\" or a homomorphism from \"Algebra Thing\" to Maps from \"Things to Things\".", "meta": {"post_id": 134062, "input_score": 27, "output_score": 43, "post_title": "Understanding the difference between group representations and modules"}}
{"input": "I think $\\frac{d}{dx} \\int f(x) dx = f(x)$ right? So $\\frac{d}{dx} \\int^b_a f(x) dx = [f(x)]^b_a = f(a)-f(b)$? But why when: \n$$f(x) = \\int^{x^3}_{x^2} \\sqrt{7+2e^{3t-3}}$$\nthen \n$$f'(x) = \\color{red}{(x^3)'}\\sqrt{7+2e^{3x-3}} - \\color{red}{(x^2)'}\\sqrt{7+2e^{3x-3}}$$\nWhere did the $(x^3)'$ and $(x^2)'$ come from?", "output": "$\\int_a^bf(x)\\,dx$ is a number, so ${d\\over dx}\\int_a^bf(x)\\,dx=0$. \nNow suppose $\\int g(x)\\,dx=F(x)$. Then $\\int_{x^2}^{x^3}g(t)\\,dt=F(x^3)-F(x^2)$, so ${d\\over dx}\\int_{x^2}^{x^3}g(t)\\,dt=(x^3)'F'(x^3)-(x^2)'F'(x^2)=3x^2g(x^3)-2xg(x^2)$.", "meta": {"post_id": 139183, "input_score": 19, "output_score": 36, "post_title": "Differentiating Definite Integral"}}
{"input": "Prove that \n\nif $f$ is of bounded variation in $[a,b]$, it is the difference of two positive, monotonic increasing functions; and \nthe difference of two bounded monotonic increasing functions is a function of bounded variation.", "output": "Let $f$ a function of bounded variation. Let $F(x):=\\sup \\sum_{j=1}^{n-1}|f(x_{j+1})-f(x_j)|=:\\operatorname{Var}[a,x]$, where the supremum is taken over the $x_1,\\ldots,x_n$ which satisfy $a=x_1<x_2<\\ldots<x_n=x$. Since $f$ is of bounded variation, $F$ is bounded, and by definition increasing. Let $G:=F-f$. We have to show that $G$ is bounded and increasing. Boundedness follows from this property for $f$ and $F$, now fix $a\\leq x_1<x_2\\leq b$. We have \n$$G(x_2)-G(x_1)=F(x_2)-f(x_2)-F(x_1)+f(x_1)\\geq 0$$\nbecause $\\operatorname{Var}[a,x_1]+f(x_2)-f(x_1)\\leq \\operatorname{Var}[a,x_1]+|f(x_2)-f(x_1)|\\leq \\operatorname{Var}[a,x_2]$.\nIf $f$ and $g$ are of bounded variation so is $f-g$. If $f$ is increasing then we have, if $a=x_0<x_1<\\ldots<x_n=b$ that $\\sum_{j=1}^{n-1}|f(x_{j+1})-f(x_j)|=|f(b)-f(a)|$, so $f$ is of bounded variation. So the difference of two bounded monotonic increasing functions is of bounded variation.", "meta": {"post_id": 141338, "input_score": 38, "output_score": 58, "post_title": "Bounded variation, difference of two increasing functions"}}
{"input": "I want to find the expected value of $\\text{max}\\{X,Y\\}$ where $X$ ist $\\text{exp}(\\lambda)$-distributed and $Y$ ist $\\text{exp}(\\eta)$-distributed. X and Y are independent.\nI figured out how to do this for the minimum of $n$ variables, but i struggle with doing it for 2 with the maximum.\n(The context in which this was given is waiting for the later of two trains, with their arrival times being exp-distributed).\nThanks!", "output": "The minimum of two independent exponential random variables with parameters $\\lambda$ and $\\eta$ is also exponential with parameter $\\lambda+\\eta$. \nAlso $\\mathbb E\\big[\\min(X_1,X_2)+\\max(X_1,X_2)\\big]=\\mathbb E\\big[X_1+X_2\\big]=\\frac{1}{\\lambda}+\\frac{1}{\\eta}$. Because $\\mathbb E\\big[\\min(X_1,X_2)\\big]=\\frac{1}{\\lambda+\\eta}$, we get $\\mathbb E\\big[\\max(X_1,X_2)\\big]=\\frac{1}{\\lambda}+\\frac{1}{\\eta}-\\frac{1}{\\lambda+\\eta}.$", "meta": {"post_id": 146973, "input_score": 16, "output_score": 46, "post_title": "Expected Value of the maximum of two exponentially distributed random variables"}}
{"input": "Is there a connexion between :\n1) The monodromy group of a topological space.\n2) The $\\ell$-adic monodromy theorem of Grothendieck.\n3) The $p$-adic monodromy conjecture of Fontaine (which is now proved). \nI am mainly interested in the link between 2) and 3).", "output": "A topological space does not have a monodromy group (unless someone is abusing terminology).  It has a fundamental group (more precisely, once we fix a base-point, it has a fundamental group relative to that base-point).\nIf $f:X \\to S$ is a fibre bundle, then the cohomology spaces of the fibres of $X$ (say with $\\mathbb Q$ coefficients, just to fix ideas, although any other coefficients would be okay too; and in some fixed degree $i$) glue together to form a local system over $S$ (i.e., a locally constant sheaf of $\\mathbb Q$-vector spaces), which (once we fix a base-point $s$), we can identify with a representation of $\\pi_1(S,s)$; indeed, the representation is on the vector space\n$H^i(X_s, \\mathbb Q),$ where $X_s := f^{-1}(s)$ is the fibre over $s$.\nIntuitively, if $c$ is a cohomology class on $X_s$, and $\\gamma$ is a loop based at $s$, then you can move $c$ through the fibres $X_{s'}$ as $s'$ moves along $\\gamma$, until you get back to $X_s$.\nTo understand this, you will need to think about examples.  A good one to start with is the fibre bundle $S^2 \\to \\mathbb R P^2$, taking $i = 0$, so that $H^0(X_s)$ is just the $\\mathbb Q$-vector space of dimension $2$ spanned by\nthe two points of $S^2$ lying over a point $s \\in \\mathbb R P^2$.\nA harder example, but more directly relevant to algebraic geometry, is the Legendre family of elliptic curves $y^2 := x(x-1)(x-\\lambda)$ (I mean the projective curves, although following tradition I am just writing down the affine equations) parameterized by\n$\\lambda \\in S = \\mathbb C P^1 \\setminus \\{0,1,\\infty\\}.$\nHere the interesting case is $i = 1$, i.e. the family of $H^1$'s of the fibres.\n\nEhresmann's theorem says that any smooth proper map of varieties $f: X \\to S$ over $\\mathbb C$ is topologically a fibre bundle, so this gives lots of examples of monodromy arising from algebraic geometry.\nIf the base $S$ is an algebraic curve, and $D^{\\times}$ is any copy of the punctured disk sitting inside $S$ (you should think of $S$ as being a punctured Riemann surface, like the above example of $\\mathbb C P^1 \\setminus \\{0,1,\\infty\\}$, and $D^{\\times}$ as being a neighbourhood of one of the punctures), then you can pull back $X$ to $D^{\\times}$, and consider the action of $\\pi_1(D^{\\times}) \\cong \\mathbb Z$ on the local system of $H^i$. (This is the local monodromy around the puncture.)\nGrothendieck's monodromy theorem says that this local monodromy action is always quasi-unipotent, i.e. some power of the generator of $\\pi_1(D^{\\times})$ acts unipotently.\n\nThere is a variant of all of the above working with $\\ell$-adic  cohomology  in the etale topology rather than usual cohomology in the setting of complex varieties, which makes sense over any ground field.\nThis leads one to think about $\\ell$-adic representations of $p$-adic Galois groups (such as $G_{\\mathbb Q_p}$) in geometric terms.  In this context, the analogue of Grothendieck's monodromy theorem is that the tame inertia acts quasi-unipotently; this follows from the famous relation $\\varphi N = p N \\varphi$ (where $\\varphi$ is Frobenius and $N$ is the log of a generator of tame inertia).  (Note that Grothendieck was able to deduce the monodromy theorem in its original geometric context from this rather easy and general theorem about $\\ell$-adic reps. of $p$-adic Galois groups.)\n\nIn Fontaine's $p$-adic Hodge theory, the analogue, for a $p$-adic representation of a $p$-adic Galois group, of tame inertia acting quasi-unipotently, is that the $p$-adic representation should be potentially semi-stable.  This is not true of all $p$-adic representations, but Fontaine conjectured that it was true for those that are de Rham.  This is his monodromy conjecture, now proved by Andre, Kedlaya, Mebkhout, and Berger.", "meta": {"post_id": 148250, "input_score": 26, "output_score": 35, "post_title": "What is the idea of a monodromy?"}}
{"input": "My question is related with the definition of Cauchy sequence\nAs we know that a sequence $(x_n)$ of real numbers is called Cauchy, if for every positive real number \u03b5, there is a positive integer $N \\in \\mathbb{N}$ such that for all natural numbers $m, n > N$ \n$\\mid x_m -x_n\\mid < \\epsilon$\nMy questions are\n1  : What is the significance of choosing $m, n > N$  ? \n2:  How to choose $N$?\n3: Can we see it geometrically?\nI would be pleased if someone can make me understand through examples.\nThanks", "output": "In the comment you say you know what a converging sequence is. Over the reals a Cauchy sequence is the same thing. So why do we care about them, you might ask. Here is why:\nRecall: A sequence $(a_n)$ of real numbers converges to the limit $a\\in \\mathbb R$ if $\\forall \\epsilon>0\\ \\exists N\\in\\mathbb N:\\forall n\\geq N\\ |a-a_n|<\\epsilon$. \nYou probably have seen examples of converging sequences. $\\frac 1n$ is one of them, and the limit is $0$. I assume you have also seen the reason for that (it boils down to the Archimedean axiom). In general the game works as follows: You guess the right limit and then do some algebraic manipulation till you find a suitable $N$ for every possible $\\epsilon >0$.\nIt might bother you that you have to know the limit before you can actually show something. Is there no other way to formulate convergence which doesn't rely on the right guess of the limit? Indeed there is: Cauchy sequences. Note that the limit doesn't show up in the definition and we can start proving things without assuming that some number is the limit. \nSo what makes a limit converging? The difference of subsequent elements of you sequence should definitely be arbitrarily small. So a naive guess for a condition would be\n$$|a_n-a_{n+1}|<\\epsilon$$\nfor all $n\\geq N$.\nBut this is not enough as you can see from the sequence\n$$s_n=\\sum_{i=1}^n\\frac 1i.$$\nIt turns out that the stronger assumption \n$$|a_n-a_{m}|<\\epsilon$$\nfor all $n,m\\geq N$ is enough. So you may not only compare two subsequent elements of your sequence but any two which appear after a certain time. \nSo we can relatively easy show that every converging sequence is Cauchy. What about the other direction, namely every Cauchy sequence converges? Well, depending on your set up you can define the reals precisely as the completion of $\\mathbb Q$. And here lies the true strengh of Cauchy sequences:\nImage you don't know yet what the reals are. You just know the rationals $\\mathbb Q$ (you define the natural numbers via the Peano axioms and derive first the integers and then the rationals from there). \nThen you can write down the sequence \n$$a_1=1,\\ a_{n+1}=\\frac {a_n}2+\\frac1a_n$$\nThis sequence converges to $\\sqrt 2$, but wait: we don't even know what $\\sqrt 2$ is. All we know are the rationals. But the seqeunce still converges, or doesn't it? The definition of convergence requires a limit but there is no suitable limit in $\\mathbb Q$. But we can take the closest thing to convergence available: Cauchy sequences. We can show that the sequence is Cauchy. We then say in $\\mathbb R$ any Cauchy sequence converges and have just defined the reals.\nAnd this idea goes on. You can take any space you like (you probably don't know many examples yet, but  you can take the space of all polynomials, or all continuous functions etc.) We can define distances (just as the absolut value on the reals) and can write down sequences. The notion of convergence does not always make sense. The notion of Cauchy sequence does.\nEdit: Obviously there are other definitions of the real numbers. If you want try proving the following:\nEverey bounded sequence of real numbers has a converging subsequence $\\Rightarrow$ every Cauchy sequence of real numbers converges.", "meta": {"post_id": 148713, "input_score": 32, "output_score": 36, "post_title": "Understanding the definition of Cauchy sequence"}}
{"input": "I really don't understand how to calculate ramification points for a general map between Riemann Surfaces. If anyone has a good explanation of this, would they be prepared to share it? Disclaimer: I'd prefer an explanation that avoids talking about projective space!\nI'll illustrate my problem with an example. The notion of degree for a holomorphic map is not well defined for non-compact spaces, such as algebraic curves in $\\mathbb{C}^2$. I've had advice from colleagues not to worry about this and to use the notion of degree anyway, because it works in a different setting (I don't know which). In particular consider the algebraic curve defined by \n$$p(z,w)=w^3-z(z^2-1)$$\nand the first projection map \n$$f(z,w)=z$$\nIn order to find the ramification points of this we know that generically $v_f(x)=1$ and clearly when $z\\neq0,\\pm 1$ we have $|f^{-1}(z)|=3$ so the 'degree' should be $3$. Thus $z=0,\\pm1$ are ramification points with branching order 3. I've had feedback that this is correct. Why did this work?\nNow let's look at an extremely similar example. Consider the algebraic curve defined by \n$$p(z,w)=w^2-z^3+z^2+z$$\nand the second projection map\n$$g(z,w)=w$$\nNow again we see the 'degree' of $g$ should be $3$. Now $g^{-1}(i)=\\{(1,i),(-1,i)\\}$. So by the degree argument exactly one of these is a ramification point, of branching order 2. Is this correct? If so, how do I tell which one it is?\nFinally in more generality, does this method work for the projection maps of all algebraic curves in $\\mathbb{C}^2$? Sorry for the long exposition!\nEdit: Here's an idea I just had. If our map $f$ is proper then we don't need $X$ to be compact for $\\deg(f)$ to be well defined. Now the projection map is clearly proper (I think) so that's why this works. Am I right? This of course raises the natural question - 'what standard maps are proper'? I guess I should ask this in a separate question though!", "output": "Let's look at your second example. Let $p(z, w) = w^2 - z^3 + z^2 + z$, and let $Y = \\{ p(z, w) = 0 \\}$. Then,\n$$p(z, i) = -z^3 + z^2 + z - 1 = -(z - 1)^2 (z+1)$$\nso I claim $(1, i)$ has ramification index $2$ while $(-1, i)$ has ramification index $1$. Indeed, observe that \n\\begin{align}\n\\frac{\\partial p}{\\partial z} & = -3 z^2 + 2 z + 1 \\\\\n\\frac{\\partial p}{\\partial w} & = 2 w\n\\end{align}\nso by an inverse function theorem argument, we find that $(z, w) \\mapsto z$ is locally a chart near both $(-1, i)$ and $(1, i)$. In this chart, your function $g : Y \\to \\mathbb{C}$ is given by $z \\mapsto \\sqrt{z^3 - z^2 - z}$. Let us take Taylor expansions around $\\pm 1$:\n\\begin{align}\ng(z) - i & = -i (z-1)^2 + O((z-1)^3) \\\\\ng(z) - i & = -2 i (z+1) + O((z+1)^2)\n\\end{align}\nHence, the ramification index at $(1, i)$ is indeed $2$ and at $(-1, i)$ it is $1$.\n\nMorally, what is going on is that your curves are dense open subsets of projective curves. Indeed, your first curve is given in homogeneous coordinates by\n$$w^3 - z (z^2 - u^2) = 0$$\nand your second curve is given by\n$$w^2 u - z^3 + z^2 u + z u^2 = 0$$\nand one can check by hand that these curves are smooth \"at infinity\", so we have the desired embedding of the original affine algebraic curves into projective (hence compact) algebraic curves. Degree is well-defined on the latter, so is well-defined on the former by restriction; the only trouble is that there may be \"missing\" preimages and so the equation relating degrees and ramification indices becomes an inequality:\n$$\\text{deg}(g) \\ge \\sum_{x \\in g^{-1} \\{w\\}} \\nu_x (g)$$\nFor example, take the affine hyperbola $z w - 1 = 0$ and the projection $(z, w) \\mapsto w$; this function has degree $1$ (once we embed it in the projective closure), but obviously there are no preimages of $0$ in the affine hyperbola.\n\nLet's develop a generic method of dealing with affine plane curves. Let $p : \\mathbb{C}^2 \\to \\mathbb{C}$ be a polynomial function in two variables, and suppose $Y = \\{ p(z, w) = 0 \\}$ is a smooth algebraic curve. Let $f : Y \\to \\mathbb{C}$ be the projection $(z, w) \\mapsto w$. For each fixed complex number $b$, we get a polynomial function $p(-, b)$, say of degree $d$. Now, because $\\mathbb{C}$ is algebraically closed, we can write\n$$p(z, b) = c (z - a_1)^{e_1} \\cdots (z - a_n)^{e_n}$$\nfor some distinct complex numbers $a_1, \\ldots, a_n$, $c \\ne 0$, and positive integers $e_1, \\ldots, e_n$, such that $e_1 + \\cdots + e_n = d$. Suppose also that\n$$\\frac{\\partial p}{\\partial w}(a_i, b) \\ne 0$$\nfor all $a_i$; then an inverse function theorem argument shows that $(z, w) \\mapsto z$ is a chart near each $(a_i, b)$. I claim that the ramification index of $f$ at $(a_i, b)$ is $e_i$ under these hypotheses. Indeed, when $z$ is a local parameter, we have\n$$0 = \\frac{\\partial p}{\\partial z} + \\frac{\\mathrm{d} w}{\\mathrm{d} z} \\frac{\\partial p }{\\partial w}$$\nso if $e_i > 1$, we have $\\frac{\\partial p}{\\partial z} (a_i, b) = 0$, so we must have $\\frac{\\mathrm{d} w}{\\mathrm{d} z} (a_i) = 0$ because $\\frac{\\partial p}{\\partial w} (a_i, b) \\ne 0 $ by hypothesis \u2013 implying $f(z) - b = O((z - a_i)^2)$. Playing around with total derivatives more, we eventually find that the first non-zero coefficient of $f(z) - b$ around $a_i$ is the coefficient of $(z - a_i)^{e_i}$, as required.\nOn the other hand, when we have $\\frac{\\partial p}{\\partial w} (a_i, b) = 0$, then by non-degeneracy we must have $\\frac{\\partial p}{\\partial z} (a_i, b) \\ne 0$, and we must have $e_i = 1$ and $(z, w) \\mapsto w$ is a chart near $(a_i, b)$. But then obviously the ramification index of $f$ at $(a_i, b)$ must be $1$. So in either case the ramification index of $f$ at $(a_i, b)$ is equal to $e_i$. Convenient, no?", "meta": {"post_id": 151087, "input_score": 41, "output_score": 53, "post_title": "Understanding Ramification Points"}}
{"input": "I know that for every $n\\in\\mathbb{N}$, $n\\ge 1$, there exists $p(x)\\in\\mathbb{F}_p[x]$ s.t. $\\deg p(x)=n$ and $p(x)$ is irreducible over $\\mathbb{F}_p$.\n\nI am interested in counting how many such $p(x)$ there exist (that is, given $n\\in\\mathbb{N}$, $n\\ge 1$, how many irreducible polynomials of degree $n$ exist over $\\mathbb{F}_p$).\n\nI don't have a counting strategy and I don't expect a closed formula, but maybe we can find something like \"there exist $X$ irreducible polynomials of degree $n$ where $X$ is the number of...\".\nWhat are your thoughts ?", "output": "Theorem: Let $\\mu(n)$ denote the M\u00f6bius function. The number of monic irreducible polynomials of degree $n$ over $\\mathbb{F}_q$ is the necklace polynomial\n$$M_n(q) = \\frac{1}{n} \\sum_{d | n} \\mu(d) q^{n/d}.$$\n(To get the number of irreducible polynomials just multiply by $q - 1$.)\nProof. Let $M_n(q)$ denote the number in question. Recall that $x^{q^n} - x$ is the product of all the monic irreducible polynomials of degree dividing $n$. By counting degrees, it follows that\n$$q^n = \\sum_{d | n} d M_d(q)$$\n(since each polynomial of degree $d$ contributes $d$ to the total degree). By M\u00f6bius inversion, the result follows. \nAs it turns out, $M_n(q)$ has a combinatorial interpretation for all values of $q$: it counts the number of aperiodic necklaces of length $n$ on $q$ letters, where a necklace is a word considered up to cyclic permutation and an aperiodic necklace of length $n$ is a word which is not invariant under a cyclic permutation by $d$ for any $d < n$. More precisely, the cyclic group $\\mathbb{Z}/n\\mathbb{Z}$ acts by cyclic permutation on the set of functions $[n] \\to [q]$, and $M_n(q)$ counts the number of orbits of size $n$ of this group action. This result also follows from M\u00f6bius inversion.\nOne might therefore ask for an explicit bijection between aperiodic necklaces of length $n$ on $q$ letters and monic irreducible polynomials of degree $n$ over $\\mathbb{F}_q$ when $q$ is a prime power, or at least I did a few years ago and it turns out to be quite elegant. \nLet me also mention that the above closed form immediately leads to the \"function field prime number theorem.\" Let the absolute value of a polynomial of degree $d$ over $\\mathbb{F}_q$ be $q^d$. (You can think of this as the size of the quotient $\\mathbb{F}_q[x]/f(x)$, so in that sense it is analogous to the norm of an element of the ring of integers of a number field.) Then the above formula shows that the number of monic irreducible polynomials $\\pi(n)$ of absolute value less than or equal to $n$ satisfies\n$$\\pi(n) \\sim \\frac{n}{\\log_q n}.$$", "meta": {"post_id": 152880, "input_score": 17, "output_score": 41, "post_title": "How many irreducible polynomials of degree $n$ exist over $\\mathbb{F}_p$?"}}
{"input": "Is it right to consider assigning a fundamental group to a topological space the same as having a functor from $\\mathbf{Top}$ to $\\mathbf{Grp}$ ?\nAre there any other examples of such functors ?", "output": "Assigning the fundamental group to a topological space is definitely a functor. But you have to keep in mind that a fundamental group is always taken with respect to a base point, and hence the functor assigns a pair $(X,x_0)$ consisting of a topological space $X$ and a point $x_0\\in X$ to its fundamental group $\\pi_1(X,x_0)$. As such, the functor goes from $\\mathbf{Top}_\\ast$ to $\\mathbf{Grp}$.\nIn more detail, the fundamental group $\\pi_1(X,x_0)$ is the group of homotopy classes of loops starting and ending in the base point $x_0$. It is not so hard to show that the map $[\\gamma]\\mapsto[f\\circ\\gamma]$ is well-defined for each loop $\\gamma$ from $x_0$ to $x_0$ and each morphism $f:(X,x_0)\\to(Y,y_0)$ of the category $\\mathbf{Top}_\\ast$; we take this map to be $\\pi_1(f)$. Roughly, this is a definition by post-composition, so it is immediate that this functor respects identity morphisms and compositions.\n\nThis is a side remark, because you have been asking about the fundamental group explicitly. But I feel it is in place here because it is natural to consider a functor with domain $\\mathbf{Top}$ that is `like taking the fundamental group'.\nInstead of $\\mathbf{Top}_\\ast\\to\\mathbf{Grp}$, one could also work with a functor $\\mathbf{Top}\\to\\mathbf{Grpd}$, where the category $\\mathbf{Grpd}$ is the category of groupoids (categories in which all morphisms are isomorphisms). The functor sends a topological space $X$ to the groupoid which has the points of $X$ as objects and between two points $x$ and $y$ of $X$ the morphisms are the homotopy classes of paths from $x$ to $y$. This gives you the fundamental groupoid rather than the fundamental group.\nThe n-lab has more information on the fundamental groupoid.\n\nThere are many more examples of functors from $\\mathbf{Top}$ or related categories. An important one is the singular functor to the category $\\mathbf{Sset}$ of simplicial sets. The category if simplicial sets is defined as follows: first you consider the category $\\Delta$ consisting of an object $[n]$ for each natural number $n$, where $[n]$ is the partial ordered set $\\{0,\\ldots,n\\}$ with the usual order; the morphisms are the order preserving maps. Then $\\mathbf{Sset}$ is the category of contravariant functors from $\\Delta$ to $\\mathbf{Set}$. \nFor each natural number $n$, there is the topological space\n$$\n|\\Delta^n|:=\\big\\{(t_0,\\ldots,t_n)\\in[0,1]^{n+1}:\\textstyle\\sum_{i=0}^n t_i=1\\big\\},\n$$\nwhich is called the standard $n$-simplex. To test your understanding of these definitions, you can show that the map $[n]\\mapsto|\\Delta^n|$ is a functor from $\\Delta$ to $\\mathbf{Top}$. Now we can define the functor $S:\\mathbf{Top}\\to\\mathbf{Sset}$, which is called the singular functor, by assigning to each topological space $X$ the functor\n$$\nn\\mapsto\\mathbf{Top}(|\\Delta^n|,X)\n$$\nIt turns out that the simplicial sets $S(X)$ have very nice properties. One of them is that they really are $\\infty$-groupoids. Also, the set $\\mathbf{Top}(|\\Delta^n|,X)$ is used to define the $n$-th homology group of $X$, which is gives yet another functor from the category of topological spaces. All of these functors have been (and are) important for the investigation of topological spaces.", "meta": {"post_id": 153180, "input_score": 22, "output_score": 37, "post_title": "Fundamental group as a functor"}}
{"input": "If we have a sequence of random variables $X_1,X_2,\\ldots,X_n$ converges in distribution to $X$, i.e. $X_n \\rightarrow_d X$, then is\n$$\n\\lim_{n \\to \\infty} E(X_n) = E(X)\n$$\ncorrect?\nI know that converge in distribution implies $E(g(X_n)) \\to E(g(X))$ when $g$ is a bounded continuous function.  Can we apply this property here?", "output": "With your assumptions the best you can get is via Fatou's Lemma:\n$$\\mathbb{E}[|X|]\\leq \\liminf_{n\\to\\infty}\\mathbb{E}[|X_n|]$$\n(where you used the continuous mapping theorem to get that $|X_n|\\Rightarrow |X|$).\nFor a \"positive\" answer to your question: you need the sequence $(X_n)$ to be uniformly integrable:\n$$\\lim_{\\alpha\\to\\infty} \\sup_n  \\int_{|X_n|>\\alpha}|X_n|d\\mathbb{P}= \\lim_{\\alpha\\to\\infty} \\sup_n \\mathbb{E} [|X_n|1_{|X_n|>\\alpha}]=0.$$\nThen, one gets that $X$ is integrable and $\\lim_{n\\to\\infty}\\mathbb{E}[X_n]=\\mathbb{E}[X]$.\nAs a remark, to get uniform integrability of $(X_n)_n$ it suffices to have for example:\n$$\\sup_n \\mathbb{E}[|X_n|^{1+\\varepsilon}]<\\infty,\\quad \\text{for some }\\varepsilon>0.$$", "meta": {"post_id": 153293, "input_score": 41, "output_score": 45, "post_title": "Does convergence in distribution implies convergence of expectation?"}}
{"input": "I am trying to prove it by induction, but I'm stuck\n$$\\mathrm{fib}(0) = 0 < 0! = 1;$$\n$$\\mathrm{fib}(1) = 1 = 1! = 1;$$\nBase case n = 2,\n$$\\mathrm{fib}(2) = 1 < 2! = 2;$$\nInductive case assume that it is true for (k+1) $k$\nTry to prove that $\\mathrm{fib}(k+1) \\leq(k+1)!$\n$$\\mathrm{fib}(k+1) = \\mathrm{fib}(k) + \\mathrm{fib}(k-1) \\qquad(LHS)$$\n$$(k+1)! = (k+1) \\times k \\times (k-1) \\times \\cdots \\times 1 = (k+1) \\times k! \\qquad(RHS)$$\n......\nHow to prove it?", "output": "$$\nF_{k+1} = F_k + F_{k-1} \\le k! + (k - 1)! \\le k! + k! \\le 2 k! \\le (k + 1) k!\n$$", "meta": {"post_id": 154756, "input_score": 8, "output_score": 43, "post_title": "How to prove that $\\mathrm{Fibonacci}(n) \\leq n!$, for $n\\geq 0$"}}
{"input": "In which cases is the inverse of a matrix equal to its transpose, that is, when do we have $A^{-1} = A^{T}$? Is it when $A$ is orthogonal?", "output": "If $A^{-1}=A^T$, then $A^TA=I$. This means that each column has unit length and is perpendicular to every other column. That means it is an orthonormal matrix.", "meta": {"post_id": 156735, "input_score": 55, "output_score": 74, "post_title": "In which cases is the inverse of a matrix equal to its transpose?"}}
{"input": "Referring to this lecture , I want to know what is the difference between supremum and maximum. It looks same as far as the lecture is concerned when it explains pointwise supremum and pointwise maximum", "output": "A maximum of a set must be an element of the set. A supremum need not be.\nExplicitly, if $X$ is a (partially) ordered set, and $S$ is a subset, then an element $s_0$ is the supremum of $S$ if and only if:\n\n$s\\leq s_0$ for all $s\\in S$; and\nIf $t\\in X$ is such that $s\\leq t$ for all $s\\in S$, then $s_0\\leq t$.\n\nBy contrast, an element $m$ is the maximum of $S$ if and only if:\n\n$s\\leq m$ for all $s\\in S$; and\n$m\\in S$.\n\nNote that if $S$ has a maximum, then the maximum must be the supremum: indeed, if $t\\in X$ is such that $s\\leq t$ for all $s\\in S$, then in particular $m\\in S$, so $m\\leq t$, proving that $m$ satisfies the conditions to be the supremum.\nBut it is possible for a set to have a supremum but not a maximum. For instance, in the real numbers, the set of all negative numbers does not have a maximum: there is no negative number $m$ with the property that $n\\leq m$ for all negative numbers $n$. However, the set of all negative numbers does have a supremum: $0$ is the supremum of the set of negative numbers. Indeed, $a\\leq 0$ for all negative numbers $a$; and if $a\\leq b$ for all negative numbers $a$, then $0\\leq b$.\nThe full relationship between supremum and maximum is: \n\n\nIf $S$ has a maximum $m$, then $S$ also has a supremum and in fact $m$ is also a supremum of $S$. \nConversely, if $S$ has a supremum $s$, then $S$ has a  maximum if and only if $s\\in S$, in which case the maximum is also $s$. \n\n\nIn particular, if a set has both a supremum and a maximum, then they are the same element. The set may also have neither a supremum nor a maximum (e.g., the rationals as a subset of the reals). But if it has only one them, then it has a supremum which is not a maximum and is not in the set.", "meta": {"post_id": 160451, "input_score": 65, "output_score": 84, "post_title": "Difference between supremum and maximum"}}
{"input": "I just need a bit of help clarifying the definition of a compact set.\nLet's start with the textbook definition: \n\nA set $S$ is called compact if, whenever it is covered by a collection of open sets $\\{G\\}$, $S$ is also covered by a finite sub-collection $\\{H\\}$ of $\\{G\\}$.   \n\nQuestion: Does $\\{H\\}$ need to be a proper subset of $\\{G\\}$? If, for instance, $\\{G\\}$ is already a finite collection, does that mean $S$ is automatically covered by a finite sub-collection of $\\{G\\}$? Also, is there any need for the open sets in $\\{H\\}$ to be bounded sets?", "output": "As with many statements involving nested quantifiers, it may help to think of this in terms of a game.  Suppose you are trying to prove that a certain space $G$ is compact.  $G$ is compact if, for every open covering $C$ of $G$, there is a finite subcovering.  So the game goes like this:\n\nYou say  \u201c$G$ is compact.\u201d\nYour adversary says \u201cIt is not.  Here is an open covering $C$.\u201d  (The adversary gives you a family of open sets whose union contains $G$.) \nYou reply \u201cHere is a finite subcovering of $C$.\u201d  (You reply with a finite subset of $C$ whose union still contains $G$.)\n\nIf you succeed in step 3, you win.  If you fail, you lose.  (If you're trying to prove that $G$ is not compact, you and the adversary exchange roles.)\nIf the adversary presents a finite open covering $C$ in step 2, you have an easy countermove in step 3: just hand back $C$ itself, and you win!\nBut to prove that $G$ is compact you also have to be able to find a countermove for any infinite covering $C$ that the adversary gives you.\nMust your finite subcovering be a proper subset of $C$?  No.  If this were required, the adversary would always be able to win in step 2 by handing you a covering $C$ with only a single element,  $C=\\{ G \\}$. Then the only proper subset you could hand back would be $\\lbrace\\mathstrut\\rbrace$, which is not a covering of $G$, and therefore the would be no nonempty compact sets.  That would be silly, so you have to be allowed to hand back $C$ unchanged in step 3.", "meta": {"post_id": 160578, "input_score": 19, "output_score": 35, "post_title": "Understanding the definition of a compact set"}}
{"input": "If $G$ and $H$ are groups with presentations $G=\\langle X|R \\rangle$ and $H=\\langle Y| S \\rangle$, then of course $G \\times H$ has presentation $\\langle X,Y | xy=yx \\ \\forall x \\in X \\ \\text{and} \\  y \\in Y, R,S \\rangle$.  Given two group presentations $G=\\langle X|R \\rangle$ and $H=\\langle Y| S \\rangle$ and a homomorphism $\\phi: H \\rightarrow \\operatorname{Aut}(G)$, what is a presentation for $G \\rtimes H$?  Is there a nice presentation, as in the direct product case?  Thanks!", "output": "Let $G = \\langle X \\mid R\\rangle$ and $H = \\langle Y \\mid S\\rangle$, and let $\\phi\\colon H\\to\\mathrm{Aut}(G)$.  Then the semidirect product $G\\rtimes_\\phi H$ has the following presentation:\n$$\nG\\rtimes_\\phi H \\;=\\; \\langle X, Y \\mid R,\\,S,\\,yxy^{-1}=\\phi(y)(x)\\text{ for all }x\\in X\\text{ and }y\\in Y\\rangle\n$$\nNote that this specializes to the presentation of the direct product in the case where $\\phi$ is trivial.\n\u00a0\nFor example, let $G = \\langle x \\mid x^n = 1\\rangle$ be a cyclic group of order $n$, let $H = \\langle y \\mid y^2=1\\rangle$ be a cyclic group of order two, and let $\\phi\\colon H \\to \\mathrm{Aut}(G)$ be the homomorphism defined by $\\phi(y)(x) = x^{-1}$.  Then the semidirect product $G\\rtimes_\\phi H$ is the dihedral group of order $2n$, with presentation\n$$\nG\\rtimes_\\phi H \\;=\\; \\langle x,y\\mid x^n=1,y^2=1,yxy^{-1}=x^{-1}\\rangle.\n$$", "meta": {"post_id": 160870, "input_score": 27, "output_score": 45, "post_title": "Group presentation for semidirect products"}}
{"input": "This problem was given to me by a friend: \n\nProve that $\\Pi_{i=1}^m \\mathbb{S}^{n_i}$ can be smoothly embedded in a Euclidean space of dimension $1+\\sum_{i=1}^m n_i$. \n\nThe solution is apparently fairly simple, but I am having trouble getting a start on this problem. Any help?", "output": "Note first that $\\mathbb{R}\\times\\mathbb{S}^n$ smoothly embeds in $\\mathbb{R}^{n+1}$ for each $n$, via $(t,\\textbf{p})\\mapsto e^t\\textbf{p}$.\nTaking the Cartesian product with $\\mathbb{R}^{m-1}$, we find that $\\mathbb{R}^m\\times\\mathbb{S}^n$ smoothly embeds in $\\mathbb{R}^{m}\\times\\mathbb{R}^n$ for each $m$ and $n$.\nBy induction, it follows that $\\mathbb{R}\\times\\prod_{i=1}^m \\mathbb{S}^{n_i}$ smoothly embeds in a Euclidean space of dimension $1+\\sum_{i=1}^m n_i$.\n\nThe desired statement follows.", "meta": {"post_id": 161293, "input_score": 33, "output_score": 57, "post_title": "Product of spheres embeds in Euclidean space of 1 dimension higher"}}
{"input": "I have heard that any submodule of a free module over a p.i.d. is free.\nI can prove this for finitely generated modules over a p.i.d.  But the proof involves induction on the number of generators, so it does not apply to modules that are not finitely generated.\nDoes the result still hold?  What's the argument?", "output": "Let $F$ be a free $R$-module, where $R$ is a PID, and $U$ be a submodule. Then $U$ is also free (and the rank is at most the rank of $F$). Here is a hint for the proof.\nLet $\\{e_i\\}_{i \\in I}$ be a basis of $F$. Choose a well-ordering $\\leq$ on $I$ (this requires the Axiom of Choice). Let $p_i : F \\to R$ be the projection on the $i$th coordinate. Let $F_i$ be the submodule of $F$ generated by the $e_j$ with $j \\leq i$. Let $U_i = U \\cap F_i$. Then $p_i(U_i)$ is a submodule of $R$, i.e. has the form $R a_i$. Choose some $u_i \\in U_i$ with $p_i(u_i)=a_i$. If $a_i=0$, we may also choose $u_i=0$.\nNow show that the $u_i \\neq 0$ constitute a basis of $U$. Hint: Transfinite induction.\nThe same proof shows the more general result: If $R$ is a hereditary ring (every ideal of $R$ is projective over $R$), then any submodule of a free $R$-module is a direct sum of ideals of $R$.", "meta": {"post_id": 162945, "input_score": 49, "output_score": 58, "post_title": "Submodule of free module over a p.i.d. is free even when the module is not finitely generated?"}}
{"input": "Problem: Let $f$ be defined for all real $x$, and suppose that \n$$|f(x)-f(y)|\\le (x-y)^2$$\nfor all real $x$ and $y$. Prove $f$ is constant.\nSource: W. Rudin, Principles of Mathematical Analysis, Chapter 5, exercise 1.", "output": "For any $x\\in\\mathbb{R}$,\n$$\n\\begin{align}\n|f'(x)|\n&=\\lim_{h\\to0}\\frac{|f(x+h)-f(x)|}{|h|}\\\\\n&\\le\\lim_{h\\to0}\\frac{h^2}{|h|}\\\\\n&=0\n\\end{align}\n$$\nTherefore, $f$ is constant.", "meta": {"post_id": 164804, "input_score": 24, "output_score": 37, "post_title": "Show $f$ is constant if $|f(x)-f(y)|\\leq (x-y)^2$."}}
{"input": "I have to evaluate:\n$$\\int_{0}^{\\pi/2}\\frac{\\sqrt{\\sin x}}{\\sqrt{\\sin x}+\\sqrt{\\cos x}}\\, \\mathrm{d}x. $$\nI can't get the right answer! So please help me out!", "output": "Let $I$ denote the integral and consider the substitution $u= \\frac{\\pi }{2} - x.$ Then $I = \\displaystyle\\int_0^{\\frac{\\pi }{2}} \\frac{\\sqrt{\\cos u}}{\\sqrt{\\cos u } + \\sqrt{\\sin u }} du$ and $2I = \\displaystyle\\int_0^{\\frac{\\pi }{2}} \\frac{\\sqrt{\\cos u} + \\sqrt{\\sin u }}{\\sqrt{\\cos u } + \\sqrt{\\sin u }} du = \\frac{\\pi }{2}.$ Hence $I = \\frac{\\pi }{4}.$\nIn general, $ \\displaystyle\\int_0^a f(x) dx = \\displaystyle\\int_0^a f(a-x) $ $dx$ whenever $f$ is integrable, and $\\displaystyle\\int_0^{\\frac{\\pi }{2}} \\frac{\\cos^a x}{\\cos^a x + \\sin^a x } dx = \\displaystyle\\int_0^{\\frac{\\pi }{2}} \\frac{\\sin^a x}{\\cos^a x + \\sin^a x } dx = \\frac{\\pi }{4}$ for $a>0$ (same trick.)", "meta": {"post_id": 167409, "input_score": 21, "output_score": 38, "post_title": "Evaluating $\\int_{0}^{\\frac{\\pi}{2}}\\frac{\\sqrt{\\sin x}}{\\sqrt{\\sin x}+\\sqrt{\\cos x}}\\, \\mathrm{d}x$"}}
{"input": "A maximal ideal  is always a prime ideal, and the quotient ring  is always a field. In general, not all prime ideals are maximal. 1\n\nIn $2\\mathbb{Z}$, $4 \\mathbb{Z} $ is a maximal ideal. Nevertheless it is not prime because $2 \\cdot 2 \\in 4\\mathbb{Z}$ but $2 \\notin 4\\mathbb{Z}$. What is that is misunderstand?", "output": "Let $R$ be a ring, not necessarily with identity, not necessarily commutative.\nAn ideal $\\mathfrak{P}$ of $R$ is said to be a prime ideal if and only if $\\mathfrak{P}\\neq R$, and whenever $\\mathfrak{A}$ and $\\mathfrak{B}$ are ideals of $R$, then $\\mathfrak{AB}\\subseteq \\mathfrak{P}$ implies $\\mathfrak{A}\\subseteq \\mathfrak{P}$ or $\\mathfrak{B}\\subseteq \\mathfrak{P}$. \n(The condition given by elements, $ab\\in P$ implies $a\\in P$ or $b\\in P$, is stronger in the case of noncommutative rings, as evidence by the zero ideal in the ring $M_2(F)$, with $F$ a field, but is equivalent to the ideal-wise definition in the case of commutative rings; this condition is called \"strongly prime\" or \"totally prime\". Generally, with noncommutative rings, \"ideal-wise\" versions of multiplicative ideal properties are weaker than \"element-wise\" versions, and the two versions are equivalent in commutative rings).\nWhen the ring does not have an identity, you may not even have maximal ideals. But here is what you can rescue; recall that if $R$ is a ring, then $R^2$ is the ideal of $R$ given by all finite sums of elements of the form $ab$ with $a,b\\in R$ (that is, it is the usual ideal-theoretic product of $R$ with itself, viewed as ideals). When $R$ has an identity, $R^2=R$; but even when $R$ does not have an identity, it is possible for $R^2$ to equal $R$. \nTheorem. Let $R$ be a ring, not necessarily with identity, not necessarily commutative. If $R^2=R$, then every maximal ideal of $R$ is also a prime ideal. If $R^2\\neq R$, then any  ideal that contains $R^2$ is not a prime ideal. In particular,  if $R^2\\neq R$ and there is a maximal ideal containing $R^2$, this ideal is maximal but not prime.\nProof. Suppose that $R^2=R$. Let $\\mathfrak{M}$ be a maximal ideal of $R$; by assumption, we know that $\\mathfrak{M}\\neq R$. Now assume that $\\mathfrak{A},\\mathfrak{B}$ are two ideals such that $\\mathfrak{A}\\not\\subseteq \\mathfrak{M}$ and $\\mathfrak{B}\\not\\subseteq\\mathfrak{M}$. We will prove that $\\mathfrak{AB}$ is not contained in $\\mathfrak{M}$ (we are proving $\\mathfrak{M}$ is prime by contrapositive). Then by the maximality of $\\mathfrak{M}$, it follows that $\\mathfrak{M}+\\mathfrak{A}=\\mathfrak{M}+\\mathfrak{B}=R$.\nThen we have:\n$$\\begin{align*}\nR &= R^2\\\\\n &= (\\mathfrak{M}+\\mathfrak{A})(\\mathfrak{M}+\\mathfrak{B})\\\\ \n&= \\mathfrak{M}^2 + \\mathfrak{AM}+\\mathfrak{MB}+\\mathfrak{AB}\\\\\n&\\subseteq \\mathfrak{M}+\\mathfrak{M}+\\mathfrak{M}+\\mathfrak{AB}\\\\\n&=\\mathfrak{M}+\\mathfrak{AB}\\\\\n&\\subseteq R,\n\\end{align*}$$\nhence $\\mathfrak{M}\\subsetneq\\mathfrak{M}+\\mathfrak{AB}=R$. Therefore, $\\mathfrak{AB}\\not\\subseteq\\mathfrak{M}$. Thus, $\\mathfrak{M}$ is a prime ideal, as claimed.\nNow suppose that $R^2\\neq R$ and $\\mathfrak{I}$ is an ideal of $R$ that contains $R^2$. If $\\mathfrak{I}=R$, then $\\mathfrak{I}$ is not prime. If $\\mathfrak{I}\\neq R$, then $RR\\subseteq \\mathfrak{I}$, but $R\\not\\subseteq \\mathfrak{I}$, so $\\mathfrak{I}$ is not prime. In particular, if $\\mathfrak{M}$ is a maximal ideal containing $R^2$, then $\\mathfrak{M}$ is not prime. $\\Box$\nIn your example, we have $R=2\\mathbb{Z}$, $R^2=4\\mathbb{Z}\\neq R$, so any ideal that contains $R^2$ (in particular, the ideal $R^2$ itself) is not prime. And since $4\\mathbb{Z}$ is a maximal ideal containing $R^2$, exhibiting a maximal ideal that is not prime. (In fact, $2\\mathbb{Z}$ has maximal ideals containing any given ideals; this can be proven directly, or invoking the fact that it is noetherian)", "meta": {"post_id": 169188, "input_score": 28, "output_score": 55, "post_title": "A maximal ideal is always a prime ideal?"}}
{"input": "I'm a mathematics student in abstract algebra and algebraic geometry. Most of my books cover a great deal of category theory and it is an essential tool in understanding these two subjects.\nRecently, I started taking some functional analysis courses and I discovered that there is almost no category theory done in these courses. But since most of the spaces studied in functional analysis are objects in categories (e.g. the normed spaces form a category), I find it rather strange that the books leave the category theory out.\nIs there a reason for this?", "output": "There are two questions here, in reality, I think. \nFirst, in brief, I am told by many people that I \"do functional analysis in the theory of automorphic forms\", and I certainly do find a categorical viewpoint very useful. Second, in brief, it is my impression that the personality-types of many people who'd style themselves \"(functional) analysts\" might be hostile to or disinterested in the worldview of any part of (even \"naive\") category theory.\nIn more detail: as a hugely important example, I think the topology on test functions on $\\mathbb R^n$ is incomprehensible without realizing that it is a (directed) colimit (direct limit). The archetype of incomprehensible/unmotivated \"definition\" (rather than categorical characterization) is in Rudin's (quite admirable in many ways, don't misunderstand me!) \"Functional Analysis\"' definition of that topology. \nThat is, respectfully disagreeing with some other answers, I do not think the specific-ness of concrete function spaces reduces the utility of a (naive-) categorical viewpoint.\nFrom a sociological or psychological viewpoint, which I suspect is often dominant, it is not hard to understand that many people have a distaste for the structuralism of (even \"naive\", in the sense of \"naive set theory\") category theory. And, indeed, enthusiasm does often lead to excess. :)\nI might claim that we are in a historical epoch in which the scandals of late 19th and early 20th century set theory prey on our minds (not to mention the mid-19th century scandals in analysis), while some still react to the arguable excesses of (the otherwise good impulses of) Bourbaki, react to certain exuberances of category theory advocates... and haven't yet reached the reasonable equilibrium that prosaically, calmly, recognizes the utilities of all these things. \nEdit: since this question has resurfaced... in practical terms, as in L. Schwartz' Kernel Theorem in the simplest case of functions on products of circles, the strong topology on duals of Levi-Sobolev spaces is the colimit of Hilbert space topologies on duals (negatively-indexed Levi-Sobolev spaces) of (positively-indexed) Levi-Sobolev spaces. As I have remarked in quite a few other places, it was and is greatly reassuring to me that a \"naive-categorical\" viewpoint immediately shows that there is a unique (up to unique isomorphism) reasonable (!) topology there... \nSimilarly, for pseudo-differential operators, and other \"modern\" ideas, it is very useful to recast their description in \"naive-categorical\" terms, thereby seeing that the perhaps-seeming-whimsy in various \"definitions\" is not at all whimsical, but is inevitable.\nA different example is characterization of \"weak/Gelfand-Pettis\" integrals: only \"in my later years\" have I appreciated the unicity of characterization, as opposed to \"construction\" (as in a Riemann/Bochner integral).", "meta": {"post_id": 169205, "input_score": 110, "output_score": 70, "post_title": "Why don't analysts do category theory?"}}
{"input": "Why is associativity required for groups?\nI'm doing a linear algebra paper and we're focusing on groups at the moment, specifically proving whether something is or is not a group. There are four axioms:\n\nThe set is closed under the operation.\nThe operation is associative.\nThe exists and identity in the group.\nEach element in the group has an inverse which is also in the group.\n\nWhy does the operation need to be associative?\nThanks", "output": "It is not that associativity is required for groups... That is quite backwards: the truth is actually that groups are associative.\nYour question seems to come from the idea that people decided how to define groups and then began to study them and find them interesting. In reality, it happened the other way around: people had studied groups way before actually someone gave a definition. When a definition was agreed upon, people looked at the groups they had at hand and saw that they happened to be associative (and that that was a useful piece of information about them when working with them) so that got included in the definition.\nIf I may say so, it is this which is important to understand. The way we teach abstract algebra nowdays somewhat obscures this fact, but this is how essentially everything comes to be.", "meta": {"post_id": 172694, "input_score": 20, "output_score": 57, "post_title": "Why is associativity required for groups?"}}
{"input": "I just wonder if my following solution is true.\nLet $X,Y$ be sets, let $f:X\\to Y$ be a function, let $\\{Y_i\\}_{i\\in I}$ be a family of subsets of $Y$. (Note: I use equalities instead of mutual containment)\n$$\\begin{align}f^{-1}\\left[\\bigcup_{i\\in I} Y_i\\right]\n &= \\{x \\in X: \\mbox{there exists an}\\quad i \\in I\\mbox{ such that } y \\in Y_i,f(x)=y\\}\n \\\\&=\\bigcup_{i \\in  I} f^{-1}\\left[Y_i\\right]\n\\end{align}$$\nI initially do not know how to get from the left to right, but when I put both sets in set notation, they turn out to be the same, hence the one line proof. Something go ultimately wrong?", "output": "The statement is true, and your argument is essentially right, but I would say that you are skipping steps to achieve that identification.  (Also, you should not have those $\\infty$'s on top of the union symbol). I would instead add:\n$$\\begin{align*}\nf^{-1}\\left[\\bigcup_{i\\in I}Y_i\\right] &= \\left\\{ x\\in X\\;\\left|\\; f(x)\\in\\bigcup_{i\\in I}Y_i\\right\\}\\right.\\\\\n&=\\Biggl\\{x\\in X\\;\\Biggm|\\; \\exists i\\in I\\text{ such that }f(x)\\in Y_i\\Biggr\\}\\\\\n&= \\bigcup_{i\\in I}\\{x\\in X\\mid f(x)\\in Y_i\\}\\\\\n&= \\bigcup_{i\\in I}f^{-1}[Y_i].\n\\end{align*}$$\nThe first equality is by definition of inverse image; the second by definition of the union; the third is by definition of union; and the fourth by definition of inverse image.", "meta": {"post_id": 174401, "input_score": 21, "output_score": 36, "post_title": "Inverse image of a union equals the union of the inverse images"}}
{"input": "Let $\\mathbb{Z}[X]$ be the ring of polynomials in one variable over $\\Bbb Z$.\n\nMy question: Is every prime ideal of $\\mathbb{Z}[X]$ one of following types?\nIf yes, how would you prove this?\n\n$(0)$.\n\n$(f(X))$, where $f(X)$ is an irreducible polynomial.\n\n$(p)$, where $p$ is a prime number.\n\n$(p, f(X))$, where $p$ is a prime number and $f(X)$ is an irreducible polynomial modulo $p$.", "output": "Let $\\mathfrak{P}$ be a prime ideal of $\\mathbb{Z}[x]$. Then $\\mathfrak{P}\\cap\\mathbb{Z}$ is a prime ideal of $\\mathbb{Z}$: this holds whenever $R\\subseteq S$ are commutative rings. Indeed, if $a,b\\in R$, $ab\\in R\\cap P$, then $a\\in P$ or $b\\in P$ (since $P$ is prime). (More generally, the contraction of a prime ideal is always a prime ideal, and $\\mathfrak{P}\\cap\\mathbb{Z}$ is the contraction of $\\mathfrak{P}$ along the embedding $\\mathbb{Z}\\hookrightarrow\\mathbb{Z}[x]$).\nThus, we have two possibilities: $\\mathfrak{P}\\cap\\mathbb{Z}=(0)$, or $\\mathfrak{P}\\cap\\mathbb{Z}=(p)$ for some prime integer $p$.\nCase 1. $\\mathfrak{P}\\cap\\mathbb{Z}=(0)$. If $\\mathfrak{P}=(0)$, we are done; otherwise, let $S=\\mathbb{Z}-\\{0\\}$. Then $S\\cap \\mathfrak{P}=\\varnothing$, $S$ is a multiplicative set, so we can localize $\\mathbb{Z}[x]$ at $S$ to obtain $\\mathbb{Q}[x]$; the ideal $S^{-1}\\mathfrak{P}$ is prime in $\\mathbb{Q}[x]$, and so is of the form $(q(x))$ for some irreducible polynomial $q(x)$. Clearing denominators and factoring out content we may assume that $q(x)$ has integer coefficients and the gcd of the coefficients is $1$. \nI claim that $\\mathfrak{P}=(q(x))$. Indeed, from the theory of localizations, we know that $\\mathfrak{P}$ consists precisely of the elements of $\\mathbb{Z}[x]$ which, when considered to be elements of $\\mathbb{Q}[x]$, lie in $S^{-1}\\mathfrak{P}$. That is, $\\mathfrak{P}$ consists precisely of the rational multiples of $q(x)$ that have integer coefficients. In particular, every integer multiple of $q(x)$ lies in $\\mathfrak{P}$, so $(q(x))\\subseteq \\mathfrak{P}$. But, moreover, if $f(x)=\\frac{r}{s}q(x)\\in\\mathbb{Z}[x]$, then $s$ divides all coefficients of $q(x)$; since $q(x)$ is primitive, it follows that $s\\in\\{1,-1\\}$, so $f(x)$ is actually an integer multiple of $q(x)$. Thus, $\\mathfrak{P}\\subseteq (q(x))$, proving equality.\nThus, if $\\mathfrak{P}\\cap\\mathbb{Z}=(0)$, then either $\\mathfrak{P}=(0)$, or $\\mathfrak{P}=(q(x))$ where $q(x)\\in\\mathbb{Z}[x]$ is irreducible.\nCase 2. $\\mathfrak{P}\\cap\\mathbb{Z}=(p)$.\nWe can then consider the image of $\\mathfrak{P}$ in $\\mathbb{Z}[x]/(p)\\cong\\mathbb{F}_p[x]$. The image is prime, since the map is onto; the prime ideals of $\\mathbb{F}_p[x]$ are $(0)$ and ideals of the form $(q(x))$ with $q(x)$ monic irreducible over $\\mathbb{F}_p[x]$. If the image is $(0)$, then $\\mathfrak{P}=(p)$, and we are done. \nOtherwise, let $p(x)$ be a polynomial in $\\mathbb{Z}[x]$ that reduces to $q(x)$ modulo $p$ and that is monic. Note that $p(x)$ must be irreducible in $\\mathbb{Z}[x]$, since any nontrivial factorization in $\\mathbb{Z}[x]$ would induce a nontrivial factorization in $\\mathbb{F}_p[x]$ (since $p(x)$ and $q(x)$ are both monic). \nI claim that $\\mathfrak{P}=(p,p(x))$. Indeed, the isomorphism theorems guarantee that $(p,p(x))\\subseteq \\mathfrak{P}$. Conversely, let $r(x)\\in\\mathfrak{P}(x)$. Then there exists a polynomial $s(x)\\in\\mathbb{F}_p[x]$ such that $s(x)q(x) = \\overline{r}(x)$. If $t(x)$ is any polynomial that reduces to $s(x)$ modulo $p$, then $t(x)p(x)-r(x)\\in (p)$, hence there exists a polynomial $u(x)\\in\\mathbb{Z}[x]$ such that $r(x) = t(x)p(x)+pu(x)$. Therefore, $r(x)\\in (p,p(x))$, hence $\\mathfrak{P}\\subseteq (p,p(x))$, giving equality.\nThus, if $\\mathfrak{P}\\cap\\mathbb{Z}[x]=(p)$ with $p$ a prime, then either $\\mathfrak{P}=(p)$ or $\\mathfrak{P}=(p,p(x))$ with $p(x)\\in\\mathbb{Z}[x]$ irreducible.\nThis proves the desired classification.", "meta": {"post_id": 174595, "input_score": 152, "output_score": 133, "post_title": "Classification of prime ideals of $\\mathbb{Z}[X]$"}}
{"input": "After just having learned about $p$-adic numbers I've now got another question which I can't figure out from the Wikipedia page.\nAs far as I understand, the $p$-adic numbers are basically completing the rational numbers in the same way the real numbers do, except with a different notion of distance where differences in more-significant digits correspond to small distances, instead of differences in less-significant digits. So if I understand correctly, the $p$-adic numbers contain the rational numbers, but not the irrational numbers, while the non-rational $p$-adic numbers are not in $\\mathbb{R}$ (someone please correct me if I'm wrong).\nNow the real numbers do not depend on the base you write the numbers in. However the construction of the $p$-adic numbers seems to depend on the $p$ chosen. On the other hand I am sure that the construction of the real numbers can be written in a way that it apparently depends on the base, so the appearance might be misleading.\nTherefore my question: Are the $p$-adic numbers the same for each $p$ (that is, are e.g. $2$-adic and $3$-adic numbers the same numbers, only written in different bases), or are they different (except for the rational numbers, of course). For example, take the $2$-adic number $x := ...1000001000010001001011$ (i.e. $\\sum_{n=0}^\\infty 2^{n(n+1)/2}$), which IIUC isn't rational (because it's not periodic). Can $x$ also be written as $3$-adic number, or is there no $3$-adic number corresponding to this series?\nIn case they are different, is there some larger field which contains all $p$-adic numbers for arbitrary $p$?", "output": "No, the different $p$-adic number systems are not in any way compatible with one another.\nA $p$-adic number is a not a number that is $p$-adic; it is a $p$-adic number.  Similarly, a real number is not a number that is real, it is a real number.  There is not some unified notion of \"number\" that these are all subsets of; they are entirely separate things, though there may be ways of identifying bits of them in some cases (e.g., all of them contain a copy of the rational numbers).\nNow, someone here is bound to point out that if we take the algebraic closure of some $\\mathbb{Q}_p$, the result will be algebraically isomorphic to $\\mathbb{C}$.  But when we talk about $p$-adic numbers we are not just talking about their algebra, but also their absolute value, or at least their topology; and once you account for this they are truly different.  (And even if you just want algebraic isomorphism, this requires the axiom of choice; you can't actually identify a specific isomorphism, and there's certainly not any natural way to do so.)\nHow can we see that they are truly different?  Well, first let's look at the algebra.  The $5$-adics, for instance, contain a square root of $-1$, while the $3$-adics do not.  So if you write down a $5$-adic number which squares to $-1$, there cannot be any corresponding $3$-adic number.\nBut above I claimed something stronger -- that once you account for the topology, there is no way to piece the various $p$-adic number systems together, which the above does not rule out.  How can we see this?  Well, let's look at the topology when we look at the rational numbers, the various $p$-adic topologies on $\\mathbb{Q}$.  These topologies are not only distinct -- any finite set of them is independent, meaning that if we let $\\mathbb{Q}_i$ be $\\mathbb{Q}$ with the $i$'th topology we're considering, then the diagonal is dense in $\\mathbb{Q}_1 \\times \\ldots \\times \\mathbb{Q}_n$.\nPut another way -- since these topologies all come from metrics -- this means that for any $c_1,\\ldots,c_n\\in\\mathbb{Q}$, there exists a sequence of rational numbers $a_1,a_2,\\ldots$ such that in topology number 1, this converges to $c_1$, but in topology number two, it converges to $c_2$, and so forth.  (In fact, more generally, given any finite set of inequivalent absolute values on a field, the resulting topologies will be independent.)\nSo even on $\\mathbb{Q}$, the different topologies utterly fail to match up, so there is no way they can be pieced together by passing to some larger setting.", "meta": {"post_id": 174818, "input_score": 25, "output_score": 44, "post_title": "Are all $p$-adic number systems the same?"}}
{"input": "$$\\begin{bmatrix} 1235 &2344 &1234 &1990\\\\\n2124 & 4123& 1990& 3026 \\\\\n1230 &1234 &9095 &1230\\\\\n1262 &2312& 2324 &3907 \n\\end{bmatrix}$$\nClearly, its determinant is not zero and, hence, the matrix is invertible. \nIs there a more elegant way to do this?\nIs there a pattern among these entries?", "output": "Find the determinant.  To make calculations easier, work modulo $2$! The diagonal is $1$'s, the rest are $0$'s.  The determinant is odd, and therefore non-zero.", "meta": {"post_id": 175561, "input_score": 154, "output_score": 513, "post_title": "Is the following matrix invertible?"}}
{"input": "I'm having trouble verifying why the following is correct.\n$$p(x, y \\mid z)= p(x \\mid y, z) p(y \\mid z)$$\nI tried grouping the $(x, y)$ together and split by the conditional, which gives me\n$$p(x, y \\mid z) = p(z\\mid x, y) p(x, y)/p(z)$$\nHowever, this did not bring me any closer. I'm uncertain about what kind of manipulations are allowed given more than 2 variables.\nSay an expression like:\n$$p(a, b, c)$$\nThen I know from the chain rule that I can break it down to:\n$$p(a, b, c)=p(a \\mid b, c) p(b, c) = p(a \\mid b, c) p(b \\mid c) p(c)$$\nIs it allowed to split by the second comma:\n$$p(a, b, c) = p(a, b \\mid c) p(c) ?$$\nAnd even more complicated and expression like:\n$$p(a|b,c)$$\nAm I allowed to rewrite this expression by grouping (a|b) together to give me something like\n$$p(a|b,c)=p((a|b)|c)p(c)$$\nAnd does this expression even make sense?", "output": "$\\Pr(a,b,c)=\\Pr(a,b\\mid c)\\Pr(c)$ is allowed.  \nYou are simply saying $\\Pr(d,c)=\\Pr(d\\mid c)\\Pr(c)$ where $d = a \\cap b$.\nCombine this with $\\Pr(a,b,c)=\\Pr(a\\mid b,c)\\Pr(b,c)=\\Pr(a\\mid b,c)\\Pr(b\\mid c)\\Pr(c)$ and divide through by nonzero $\\Pr(c)$ to get $\\Pr(a,b\\mid c)=\\Pr(a\\mid b,c)\\Pr(b\\mid c)$.", "meta": {"post_id": 176301, "input_score": 47, "output_score": 46, "post_title": "Conditional and joint probability manipulations when there are 3 variables"}}
{"input": "When learning mathematics I tend to try to reduce all the concepts I come across to some matter of interaction between sets and functions (or if necessary the more general Relation) on them. Possibly with some extra axioms thrown in here and there if needed, but the fundamental idea is that of adding additional structure on sets and relations between them.\nI've recently tried applying this view to calculus and have been running into some confusions. Most importantly I'm not sure how to interpret Limits. I've considered viewing them as a function that takes 3 arguments, a function, the function's domain and some value (the \"approaches value\") then outputs a single value.\nHowever this \"limit function\" view requires defining the limit function over something other then the Reals or Complexes due to the notion of certain inputs and outputs being \"infinity\". This makes me uncomfortable and question whether my current approach to mathematics is really as elegant as I'd thought. Is this a reasonable approach to answering the question of what limits actually \"are\" in a general mathematical sense? How do mathematicians tend to categorize limits with the rest of mathematics?", "output": "Do you by any chance have a computer science background? Your ideal of reducing everything (even operations like limits) to function and sets has a flavor of wanting mathematics to work more or less like a programming language -- this is a flavor that I (being a computer scientist) quite approve of, but you should be aware that the ideal is not quite aligned with how real mathematicians write mathematics.\nFirst, even though everything can be reduced to sets and functions -- indeed, everything can be reduced to sets alone, with functions just being sets of a particular shape -- doing so is not necessarily a good way to think about everything all of the time. Reducing everything to set theory is the \"assembly language\" of mathematics, and while it will certainly make you a better mathematician to know how this reduction works, it is not the level of abstraction you'll want to do most of your daily work at.\nIn contrast to the \"untyped\" assembly-level set theory, the day-to-day symbol language of mathematics is a highly typed language. The \"types\" are mostly left implicit in writing (which can be frustrating for students whose temperament lean more towards the explicit typing of most typed computer languages), but they are supremely important in practice -- almost every notation in mathematics has dozens or hundreds of different meanings, between which the reader must choose based on what the types of its various sub-expressions are. (Think \"rampant use of overloading\" from a programming-language perspective). Mostly, we're all trained to do this disambiguation unconsciously.\nIn most cases, of course, the various meanings of a symbol are generalizations of each other to various degrees. This makes it a particular bad idea to train oneself to think of the symbol of denoting this or that particular function with such-and-such particular arguments and result. A fuzzier understanding of the intention behind the symbol will often make it easier to guess which definition it's being used with in a new setting, which makes learning new material easier (even though actual proofwork of course needs to be based on exact, explicit definitions).\nIn particular, even restricting our attention to real analysis, the various kinds of limits (for $x\\to a$, $x\\to \\infty$, one-sided limits and so forth) are all notated with the same $\\lim$ symbols, but they are technically different things. Viewing $\\lim_{x\\to 5}f(x)$ and $\\lim_{x\\to\\infty} f(x)$ as instances of the same joint \"limit\" function is technically possible, but also clumsy and (more importantly) not even particularly enlightening. It is better to think of the various limits as a loose grouping of intuitively similar but technically separate concepts.\nThis is not to say that there's not interesting mathematics to be made from studying ways in which the intuitive similarity between the different kind of limits can be formalized, producing some general notion of limit that has the ordinary limits as special cases. (One solution here is to say that the \"$x\\to \\cdots$\" subscript names a variable to bind while also denoting a net to take the limit over). All I'm saying is that such a general super-limit concept is not something one ought to think of when doing ordinary real analysis.\nFinally (not related to your question about limits), note that the usual mathematical language makes extensive use of abstract types. The reals themselves are a good example: it is possible to give an explicit construction of the real numbers in terms of sets and functions (and every student of mathematics deserves to know how), but in actual mathematical reasoning numbers such as $\\pi$ or $2.6$ are not sets or functions, but a separate sort of things that can only be used in the ways explicitly allowed for real numbers. \"Under the hood\" one might consider $\\pi$ to \"really be\" a certain set of functions between various other sets, but that is an implementation detail that is relevant only at the untyped set-theory level.\n(Of course, the various similarities between math and programming languages I go on about here are not coincidences. They arose from programming-language design as deliberate attempts to create formal machine-readable notations that would \"look and feel\" as much like ordinary mathematical symbolism as they could be made to. Mathematics had all of these things first; computer science was just first to need to name them).", "meta": {"post_id": 180392, "input_score": 28, "output_score": 43, "post_title": "What kind of \"mathematical object\" are limits?"}}
{"input": "For solving differential equations, especially the ones of the form\n$$g(x)dx = h(y)dy$$\nwe solve the equation by integrating on both sides to reveal the solution.\nUnderstanding this for differentiating the equation on both sides is relatively easy. We know that we can formulate an alternative equation in terms of differentials for the original equation involved and come out with a new differential equation that holds because of the properties of the differentials.\nBut how does it work for integration on both sides? Am I missing any point here? I have referred to multiple books but none give a satisfactory explanation. Integrating an equation on both sides seems really wrong, if I may dare to use the word.\nPlease help. I'm stuck with this thing and I can only begin to understand differential equations once this is cleared from my head.\nThank you very much!", "output": "The original equation was presumably \n$$h(y)\\frac{dy}{dx}=g(x),$$\nor something equivalent to this.\nYou are given the mysterious rule about \"splitting\" $\\frac{dy}{dx}$. You probably were told at one time that $\\frac{dy}{dx}$ is not a fraction, and now all of a sudden we are treating it as a fraction!\nSo let us not split it. Suppose that $H(y)$ is an antiderivative of $h(y)$, that is, a function whose derivative wiith respect to $y$ is $h(y)$. Let $G(x)$ be a function whose derivative with respect to $x$ is $g(x)$. \nWe recognize $h(y)\\frac{dy}{dx}$ as the derivative with respect to $x$ of $H(y)$ (Chain Rule).  So our equation can be written as\n$$\\frac{d}{dx} H(y)=\\frac{d}{dx}G(x).$$\nThus $H(y)$ and $G(x)$ have the same derivative with respect to $x$. So they differ by a constant, and we find\n$$H(y)=G(x)+C.$$\nNow the important part: this is exactly what we get when we \"split\" $\\frac{dy}{dx}$ and integrate on both sides. So whether or not the splitting and integrating makes sense, it gives the right answer. \nIf you wish, splitting and integrating can be treated as a senseless mnemonic that works, a \"shortcut\" to the real calculation using the Chain Rule. In fact, the individual terms $dy$ and $dx$ can be given meaning, but it is a little complicated. And Applied (and less Applied) people have an essentially correct intuition based on adding up \"infinitely small\" quantities. Unfortunately, it takes considerable effort to make that intuition rigorous.", "meta": {"post_id": 182344, "input_score": 34, "output_score": 40, "post_title": "Can anyone explain the intuitive meaning of 'integrating on both sides of the equation' when solving differential equations?"}}
{"input": "Could any one give an example of a bijective map from $\\mathbb{R}^3\\rightarrow \\mathbb{R}$?\nThank you.", "output": "First, note that it is enough to find a bijection $f:\\Bbb R^2\\to \\Bbb R$, since then $g(x,y,z) = f(f(x,y),z)$ is automatically a bijection from $\\Bbb R^3$ to $\\Bbb R$.\nNext, note that since there is a bijection from $[0,1]\\to\\Bbb R$ (see appendix), it is enough to find a bijection from the unit square $[0,1]^2$ to the unit interval $[0,1]$. By constructions in the appendix, it does not really matter whether we consider $[0,1]$, $(0,1]$, or $(0,1)$, since there are easy bijections between all of these.\nMapping the unit square to the unit interval\nThere are a number of ways to proceed in finding a bijection from the unit square to the unit interval.  Usully one starts with the \"interleaving\" technique I mentioned in the comments, mapping $$\\langle 0.a_1a_2a_3\\ldots,\\\\\\qquad 0.b_1b_2b_3\\ldots\\rangle$$ to $$0.a_1b_1a_2b_2a_3b_3\\ldots.$$  To see why this doesn't quite work, consider the numbers $\\frac 12=0.50000\\ldots$ and $\\frac9{22} = 0.40909\\ldots$.  Both of these are the image of the pair $\\langle\\frac12, 0\\rangle$, so we have not described a bijection.\nThis problem can be fixed.\n(In answering this question, I tried many web searches to try to remember the fix, and I was amazed at how many sources I found that ignored the problem, either entirely, or by handwaving. I never did find it; I had to remember it. Sadly, I cannot remember where I saw it first.)\nFirst, we will deal with $(0,1]$ rather than with $[0,1]$; bijections between these two sets are well-known, or see the appendix. For real numbers with two decimal expansions, such as $\\frac12$, we will agree to use only the expansion that ends with nines rather than the one that ends with zeroes. So for example we represent $\\frac12$ as $0.4999\\ldots$.\nNow instead of interleaving single digits, we will break each input number into chunks, where each chunk consists of some number of zeroes (possibly none) followed by a single non-zero digit.  For example, $\\frac1{200} = 0.00499\\ldots$ is broken up as $$004\\ 9\\ 9\\ 9\\ldots,$$ and $0.01003430901111\\ldots$ is broken up as $$01\\ 003\\ 4\\ 3\\ 09\\ 01\\ 1\\ 1\\ldots.$$\nThis chunking process is well-defined since we are ignoring representations that contain infinite sequences of zeroes.\nNow instead of interleaving digits, we interleave chunks.  To interleave $0.004999\\ldots$ and $0.01003430901111\\ldots$, we get $$0.004\\ 01\\ 9\\ 003\\ 9\\ 4\\ 9\\ldots.$$  This is obviously reversible. It can never produce a result that ends with an infinite sequence of zeroes, and similarly the reverse mapping can never produce a number with an infinite sequence of trailing zeroes, so we win.  A problem example similar to the one from a few paragraphs ago is resolved as follows: $\\frac12 = 0.4999\\ldots$ is the unique image of $\\langle 0.4999\\ldots, 0.999\\ldots\\rangle$ and $\\frac9{22} = 0.40909\\ldots$ is the unique image of $\\langle 0.40909\\ldots, 0.0909\\ldots\\rangle$.\nThis is enough to answer the question posted, but I will give some alternative approaches.\nContinued fractions\nAccording to the paper \"Was Cantor Surprised?\" by Fernando Q. Gouve\u00e2, Cantor originally tried interleaving the digits himself, but Dedekind pointed out the problem of nonunique decimal representations. Cantor then switched to an argument like the one Robert Israel gave in his answer, based on continued fraction representations of irrational numbers.  He first constructed a bijection from $(0,1)$ to its irrational subset (see this question for the mapping Cantor used and other mappings that work), and then from pairs of irrational numbers to a single irrational number by interleaving the terms of the infinite continued fractions.  Since Cantor dealt with numbers in $(0,1)$, he could guarantee that every irrational number had an infinite continued fraction representation of the form $$x = x_0 + \\dfrac{1}{x_1 + \\dfrac{1}{x_2 + \\ldots}}$$\nwhere $x_0$ was zero, avoiding the special-case handling for $x_0$ in Robert Israel's solution.\nCantor-Schr\u00f6der-Bernstein mappings\nThe Cantor-Schr\u00f6der-Bernstein theorem takes an injection $f:A\\to B$ and an injection $g:B\\to A$, and constructs a bijection between $A$ and $B$.\nSo if we can find an injection $f:[0,1)^2\\to[0,1)$ and an injection $g:[0,1)\\to[0,1)^2$, we can invoke the CSB theorem and we will be done.\n$g$ is quite trivial;  $x\\mapsto \\langle x, 0\\rangle$ is one of many obvious injections.\nFor $f$ we can use the interleaving-digits trick again, and we don't have to be so careful because we need only an injection, not a  bijection. We can choose the representation of the input numbers arbitrarily; say we will take the $0.5000\\ldots$ representation rather than the $0.4999\\ldots$ representation. Then we interleave the digits of the two input numbers.  There is no way for the result to end with an infinite sequence of nines, so we are guaranteed an injection.\nThen we apply CSB to $f$ and $g$ and we are done.\nAppendix\n\nThere is a bijection from $(-\\infty, \\infty)$ to $(0, \\infty)$.  The map $x\\mapsto e^x$ is an example.\n\nThere is a bijection from $(0, \\infty)$ to $(0, 1)$.  The map $x\\mapsto \\frac2\\pi\\tan^{-1} x$ is an example, as is $x\\mapsto{x\\over x+1}$.\n\nThere is a bijection from $[0,1]$ to $(0,1]$.  Have $0\\mapsto \\frac12, \\frac12\\mapsto\\frac23,\\frac23\\mapsto\\frac34,$ and so on. That takes care of $\\left\\{0, \\frac12, \\frac23, \\frac34,\\ldots\\right\\}$.  For any other $x$, just map $x\\mapsto x$.\n\nSimilarly, there is a bijection from $(0,1]$ to $(0,1)$.", "meta": {"post_id": 183361, "input_score": 157, "output_score": 250, "post_title": "Examples of bijective map from $\\mathbb{R}^3\\rightarrow \\mathbb{R}$"}}
{"input": "I am reading a collection of problems by the Russian mathematician Vladimir Arnol'd, titled A Mathematical Trivium. I am taking a stab at this one:\n\nCalculate the $100$th derivative of the function $$\\frac{x^2 + 1}{x^3 - x}.$$\n\nThe derivative is non-trivial (in the sense that I computed it for a few rounds, and it only became more assertive). My first thought was to let\n$$f(x) = x^2 + 1, \\text{ } g(x) = \\frac{1}{x^3 - x}$$\nand apply the Leibnitz rule for products,\n$$fg^{(n)}(x) = \\sum_{k=0}^n {n\\choose k} f^{(n-k)}(x)g^{(k)}(x) .$$\nSince $f$ is vanishing after the third differentiation, we get\n$$fg^{(100)}(x) = {100 \\choose 2}f^{(2)}g^{(98)} + {100 \\choose 1}f^{(1)}g^{(99)} {100 \\choose 0}f^{(0)}g^{(100)} \\\\= 9900g^{(98)} + 200xg^{(99)} + (x^2 + 1)g^{(100)}$$\nThis would be great if we could compute the last few derivatives of $g$. Indeed, we can boil this down: notice that\n$$g(x) = h(x)i(x)j(x), \\hspace{4mm} h(x) = \\frac{1}{x-1}, \\text{  } i(x) = \\frac{1}{x}, \\text{  } j(x) = \\frac{1}{x+1};$$\nfurther, $h, i,$ and $j$ have friendly behavior under repeated differentation, e.g. $h^{(n)}(x) = \\frac{(-1)^n n!}{(x-1)^{n + 1}}$.\nSo overall, it is possible to use Leibnitz again to beat a lengthy derivative out of this function, (namely,\n$$g^{(n)}(x) = \\sum_{k=0}^n {n \\choose k} h^{(n-k)}(x) \\Bigl(\\sum_{l=0}^k {k \\choose l} i^{(k-l)}(x) j^{(l)}(x)\\Bigr)$$\nwith the details filled in). \nHowever, this is really pretty far from computing the derivative.\n\nSo, my question: does anyone know how to either improve the above argument, or generate a new one, which can resolve the problem?", "output": "We have a partial fraction decomposition\n$$\n\\frac{x^2+1}{x^3-x}=\\frac{-1}{x}+\\frac{1}{x+1}+\\frac{1}{x-1}\n$$\nIt follows that\n$$\n\\left(\\frac{d}{dx}\\right)^{100}\\frac{x^2+1}{x^3-x}=\\frac{-100!}{x^{101}}+\\frac{100!}{(x+1)^{101}}+\\frac{100!}{(x-1)^{101}}\n$$", "meta": {"post_id": 183488, "input_score": 57, "output_score": 95, "post_title": "The $100$th derivative of $(x^2 + 1)/(x^3 - x)$"}}
{"input": "I have a question about the Sobolev Space $H^1_0(U)$, where $U$ is a open subset of $\\mathbb{R}^n$. Let us denote with $H^{-1}(U)$ the dual space of $H^1_0$. \n\n\nHow is the acting of $H^{-1}$ and $H^1_0$ defined, i.e.\n  $$\n\\langle \\phi,u\\rangle\n$$ where $\\phi\\in H^{-1}$ and $u\\in H^1_0$?  \nFurthermore, if I have elements $v,u\\in H^1_0$, why is it true that\n  $$(u,v)_{L^2} = \\langle u,v\\rangle$$\n  where the latter should denote again the dual pairing of $H^1_0$ and $H^{-1}$. \n\n\nThanks for your help\nhulik", "output": "As you defined, $H^{-1}$ is an abstract space, consisting of continuous linear functionals on $H^1_0$. So let us take an element $\\phi\\in H^{-1}$. What is $\\phi$? This is a linear functional on $H^1_0$, so it can act on any function $u\\in H^1_0$ and spit out a number. We denote this number by $\\phi(u)$ or $\\langle \\phi,u\\rangle$. The latter notation has the advantage of appearing more symmetric, and cleaner when you have complicated expressions instead of $\\phi$ or $u$, as in $\\langle \\alpha\\phi+\\beta \\exp(F),uv+\\xi\\rangle$.\nNow $H^{-1}$ contains many familiar operations on functions. Remember that the elements of $H^{-1}$ are indeed operations, that do something on functions to get numbers. Evaluating a function at a given point $x$, and integrating a function against another given function are examples of such operations, which are therefore potential elements of $H^{-1}$. Let us take a function $v\\in L^2$. Then we can define a linear operation $\\phi$ by\n$$\n\\phi(u) = \\int vu = (v,u)_{L^2}.\\qquad\\qquad(*)\n$$\nIs $\\phi\\in H^{-1}$? We have to check two things: linearity, and continuity. Obviously $\\phi$ is linear: \n$$\n\\phi(\\alpha u+\\beta w)=\\int v(\\alpha u+\\beta w)=\\alpha\\int vu + \\beta\\int vw\n=\\alpha\\phi(u)+\\beta\\phi(w).\n$$\nContinuity can be checked by using the Cauchy-Bunyakowsky-Schwarz inequality:\n$$\n|\\phi(u)|=|(v,u)_{L^2}|\\leq \\|v\\|_{L^2}\\|u\\|_{L^2}\\leq \\|v\\|_{L^2}\\|u\\|_{H^1}.\n$$\nSo $\\phi\\in H^{-1}$. In other words, if we define a mapping $T:v\\mapsto \\phi$ by ($*$), then $T(L^2)\\subset H^{-1}$. Is $T:L^2\\to H^{-1}$ injective? In other words, is it possible that two different functions $v_1$ and $v_2$ to give rise to the same functional $\\phi$? This would mean that\n$$\n(v_1,u)_{L^2} = (v_2,u)_{L^2},\n\\qquad\\textrm{or}\\qquad\n(v_1-v_2,u)_{L^2} = 0,\n$$\nfor all $u\\in H^1_0$.\nIn particular, the preceding is true for all compactly supported smooth functions $u$, which implies that $v_1$ and $v_2$ must agree with each other almost everywhere. This means $v_1=v_2$ in $L^2$.  So $T:L^2\\to H^{-1}$ is injective. What all this means is that we can think the image $T(L^2)$ as being $L^2$ itself, so we can think of $L^2$ being a subset of $H^{-1}$. \nNow we write simply $v$ instead of $\\phi=T(v)$.\nHence\n$$\n\\langle v,u\\rangle = (v,u)_{L^2}.\n$$", "meta": {"post_id": 183644, "input_score": 17, "output_score": 35, "post_title": "How is the acting of $H^{-1}$ on $H^1_0$ defined?"}}
{"input": "$\\pi$ can be represented as $C/D$, and $C/D$ is a fraction, and the definition of an irrational number is that it cannot be represented as a fraction. \nThen why is $\\pi$ an irrational number?", "output": "A rational number is a number that can be expressed as $p/q$, where $p$ and $q$ are integers. The number $\\pi$ cannot be expressed in this form; hence it is irrational.\nIn other words, the definition of \"fraction\" does not include ratios like \"circumference/diameter\" in which the numerator and denominator are arbitrary numbers, not necessarily integers. In the case of \"circumference/diameter\" (which you denoted $\\pi = C/D$), it will always be the case that if the diameter is an integer, the circumference ($C = \\pi D$) is not an integer, and if the circumference is an integer, the diameter ($D = C/\\pi$) is not an integer: precisely because $\\pi$ is irrational.\nNote that a definition of \"fraction\" that allowed arbitrary real numbers as the numerator and denominator would be not very useful, as it would allow \"fractions\" like $\\pi = \\pi / 1$, or indeed, for any number $x$, the representation of $x$ as a \"fraction\" $x = x/1$.", "meta": {"post_id": 184675, "input_score": 51, "output_score": 49, "post_title": "Why is $\\pi$ irrational if it is represented as $C/D$?"}}
{"input": "Suppose $M$ and $N$ are two connected oriented smooth manifolds of dimension $n$. Conventionally, people use $M\\#N$ to denote the connecte sum of the two. (The connected sum is constructed from deleting an $n$-ball from each manifold and glueing the boundary.) I think there should be a general approach of finding the homology and cohomology groups of $M\\#N$ from the homology and cohomology groups of $M$ amd $N$. However, using Mayer-Vietoris sequence becomes quite complicated whent the spaces are complicated. For example: 1) $M=S^1 \\times S^3$, $N= \\mathbb{CP}^2$. The difficulty comes from understanding the map $H^i(U\\cap V)\\rightarrow H^{i+1}(M\\#N)$.\nIf we define the $M\\#_{rev}N$ to be the connected sum, however, glueing the $n$-ball in the reversed order, then we can ask the same problem. the case $n=2$ is easier to think of the picture when the orientation is reversed. It is also easier since the glueing is \"fixed\" in the sense there is only one parameter discribing the boundary of $n$-ball, aka, $S^1$. What happens when $n$ goes higher?\nI would appreciate it if someone could explain this general question with examples ($M\\#N$ and $M\\#_{rev}N$, where $M=S^1 \\times S^3$, $N= \\mathbb{CP}^2$) in the context.", "output": "The procedure for finding homology and cohomology of the spaces in question is a neat little trick. From here on out, I'll just treat the homology case, but the cohomology follows from the same arguments. Collapse the $S^{n-1}$ you're gluing along to a point- this turns $M\\# N$ into $M\\vee N$. Since $(M\\# N, S^{n-1})$ is a good pair, the homology can be identified with the relative homology of the pair $(M\\# N,S^{n-1})$. From this, we get the following long exact sequence:\n$$\\cdots\\to \\widetilde{H_i}(S^{n-1})\\to \\widetilde{H_i}(M\\# N) \\to \\widetilde{H_i}(M\\vee N)\\to\\cdots$$\nBy a simple Mayer-Vietoris argument, we have that $\\widetilde{H}_i(M\\vee N)\\cong \\widetilde{H}_i(M)\\oplus \\widetilde{H}_i(N)$. Since $\\widetilde{H_i}(S^{n-1})$ is zero except for $i=n-1$, we have automatically that $H_i(M\\# N)\\cong H_i(M\\vee N)\\cong H_i(M)\\oplus H_i(N)$ for $i\\neq n-1,n$. The only interesting case is as follows:\n$$0\\to \\widetilde{H_n}(M\\# N)\\to \\widetilde{H_n}(M\\vee N) \\to \\widetilde{H}_{n-1}(S^{n-1})\\to \\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N) \\to 0$$\nNow, we start getting into some casework depending on whether none, one, or both of $M,N$ are orientable. In the case that both are orientable, the above sequence turns into \n$$0\\to \\mathbb{Z} \\to \\mathbb{Z}\\oplus\\mathbb{Z} \\to \\mathbb{Z} \\to \\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N) \\to 0$$\nas their connected sum is also orientable. From this, we see that $\\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N)$ must be an isomorphism.\nIf just one of $M,N$ is orientable, then their connected sum is non-orientable, and the following happens:\n$$0\\to 0 \\to \\mathbb{Z}\\oplus0 \\to \\mathbb{Z} \\to \\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N) \\to 0$$\nin which case we still have that that $\\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N)$ must be an isomorphism.\nIf neither of $M,N$ are orientable, then their connected sum is non-orientable, in which case the long exact sequence does the following:\n$$0\\to 0 \\to 0 \\to \\mathbb{Z} \\to \\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N) \\to 0$$\nand thus $\\widetilde{H}_{n-1}(M\\# N)$ is an extension of $\\widetilde{H}_{n-1}(M\\vee N)$ by $\\mathbb{Z}$. To figure out what extension it is, one needs to inspect the map $S^{n-1}\\to M\\# N$ and the corresponding map on homology. Nothing too surprising can happen- $H^{n-1}(M\\# N)$ is the direct sum of a free abelian group and a finite abelian group.\nNote that during this argument, it was never necessary to talk about the orientation of the gluing- so $M\\# N$ and $M\\#_{rev}N$ have the same homology/cohomology. No description of $S^{n-1}$ was ever used except for it having reduced homology only in degree $n-1$, so the process does not care very much about what dimension your manifolds are.\nNow, for the example where $M=S^1\\times S^3$ and $N=\\mathbb{C}P^2$. $M$ has homology $H_0\\cong H_1\\cong H_3\\cong H_4\\cong \\mathbb{Z}$ and all other groups zero, while $N$ has homology $H_0\\cong H_2\\cong H_4\\cong \\mathbb{Z}$ and all other groups zero. Using the procedure above, we have that the homology of $M\\# N$ is as follows: $H_0\\cong H_1\\cong H_2\\cong H_3\\cong H_4\\cong \\mathbb{Z}$. The result is the same for $M\\#_{rev}N$.\n\nAddendum: It has been pointed out in the comments that the arguments about what happens with the maps between the various copies of $\\Bbb Z$ are not quite complete. The key gap is showing that the map from $\\widetilde{H}_n(M\\vee N)\\to \\widetilde{H}_{n-1}(S^{n-1})$ is surjective when at least one of $M,N$ are orientable. The fix is reasonable, and is presented below to make this answer as complete as possible.\nThe sequence for a good pair gives that the map $H_n(M\\# N,S^{n-1})\\to H_{n-1}(S^{n-1})$ is given by taking the boundary of relative chains. Without loss of generality, $M$ is orientable, so consider the relative class in $H_n(M\\# N,S^{n-1})$ which is the fundamental class of $M$ minus the disc we delete to perform the connected sum operation. This has boundary exactly the fundamental class of $S^{n-1}$ (up to possibly a sign change for orientation issues) by the definition of how we chose $S^{n-1}$ to glue along. As we identified $H_n(M\\# N,S^{n-1}) \\cong \\widetilde{H}_n(M\\vee N) \\cong \\widetilde{H}_n(M)\\oplus \\widetilde{H}_n(N)$, we see that both the map $\\Bbb Z\\oplus\\Bbb Z\\to \\Bbb Z$ and $\\Bbb Z\\oplus 0\\to \\Bbb Z$ are surjective. Thus the conclusion about $\\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N)$ being an isomorphism when at least one of $M,N$ is orientable holds.", "meta": {"post_id": 187413, "input_score": 39, "output_score": 58, "post_title": "Computing the homology and cohomology of connected sum"}}
{"input": "Let $A$ be a $m \\times n$ matrix with entries from some field $F$. Define the determinant rank of $A$ to be the largest possible size of a nonzero minor, i.e. the size of the largest invertible square submatrix of $A$. It is true that the determinant rank is equal to the rank of a matrix, which we define to be the dimension of the row/column space.\nIt's not difficult to see that $\\text{rank} \\geq \\text{determinant rank}$. If some submatrix of $A$ is invertible, then its columns/rows are linearly indepedent, which implies that the corresponding rows/columns of $A$ are also linearly indepedent.\nIs there a nice proof for the converse?", "output": "If the matrix $A$ has rank $k$, then it has $k$ linearly independent lines. Those form an $k\\times n$ submatrix, which of course also has rank $k$. But if it has rank $k$, then it has $k$ linearly independent columns. Those form a $k\\times k$ submatrix of $A$, which of course also has rank $k$. But a $k\\times k$ submatrix with rank $k$ is a full-rank square matrix, therefore invertible, thus is has a non-zero determinant. And therefore the determinant rank has to be at least $k$.", "meta": {"post_id": 187497, "input_score": 34, "output_score": 45, "post_title": "Proof that determinant rank equals row/column rank"}}
{"input": "Let's choose an open covering for $\\left [ 0,1 \\right ]$. For example $$\\left \\{ \\left ( \\frac 1 n,1-\\frac 1 n \\right )  \\mid  n\\in \\{ 3,4,\\dots\\} \\right \\}.$$\nHow can one choose a finite open subcover to prove compactness?", "output": "Here is a quick and elegant proof of the actual result that $[0,1]$ is compact based on real induction:\nLet $\\mathcal{O}$ be an (arbitrary!) open cover. Let $P$ be the set of points $x$ in $[0,1]$ such that $[0,x]$ can be covered by finitely many elements of $\\mathcal{O}$. We have $0\\in P$ and $P$ is bounded above by $1$. Therefore, $P$ has a supremum $s$. \nWe first show that $[0,s]$ can be covered by finitely many sets in $\\mathcal{O}$. This is trivial when $s=0$, so assume $s>0$. Let $O_s\\in\\mathcal{O}$ be a set containing $s$. Then there is an $\\epsilon \\in (0, s)$ such that $(s-\\epsilon,s]\\subseteq O_s$. By assumption, there is a finite subcover of $[0,s-\\epsilon/2]$. By adding $O_s$ to that finite subcovering, we get a finite subcovering of $[0,s]$.\nWe now show that $s=1$. Suppose $s<1$ and let $O_s\\in\\mathcal{O}$ be a set containing $s$. Then there is an $\\epsilon>0$ such that $[s,s+\\epsilon)\\subseteq O_s$. So taking a finite subcover of $[0,s]$ and adding the set $O_s$ gives us a finite subcover of $[0,s+\\epsilon/2]$, contradicting the construction of $s$.", "meta": {"post_id": 188996, "input_score": 12, "output_score": 55, "post_title": "Showing that $[0,1]$ is compact"}}
{"input": "Can anyone explain to me what a blow-up is? If would be great if someone could provide a definition and some examples. Any free introductory texts are welcome too.  Thanks!", "output": "The basic idea of blowups in algebraic geometry is to remove a point from an algebraic variety and replace it by the projectivized tangent cone at that point. The result is a new space, with a projective map down to the old, such that the fibre over the \"centre\" (the point we blow up) is an effective Cartier divisor. This generalizes in a couple directions: we can blow-up on closed subvarieties, and the construction extends also to schemes (and probably to more general spaces also).\nBlowups satisfy a universal property with respect to replacing the centre by an effective Cartier divisor (i.e., a subvariety that is locally defined by a single nonzerodivisor): Let $Y\\subseteq X$ be a closed subvariety. A morphism $\\pi:\\widetilde X\\to X$ is called a blowup of X with centre Y if the two following properties hold:\n(a) $E:=\\pi^{-1}(Y)$ is an effective Cartier divisor on $\\widetilde X,$\n(b) $\\pi$ satisfies a universal property with respect to (a), namely for every morphism $\\tau:Z\\to X$ such that $\\tau^{-1}(Y)$ is an effective Cartier divisor, there is a unique morphism $\\varphi:Z\\to \\widetilde X$ such that $\\pi\\circ\\varphi=\\tau.$\nBlowups can be easily constructed in a few ways. One way is by computing charts that can be glued. For example, let $X$ be an affine variety over the field $k$ with coordinate ring $k[X],$ and let $I=(g_1,\\dots, g_l)\\subseteq k[X]$ be an ideal cutting out the subvariety $Y.$ Suppose for simplicity that $X$ is irreducible, so that $k[X]$ is a domain. Then $\\widetilde X$ is defined by the collection of charts $U_i=\\operatorname{Spec}\\left(k[X][g_1/g_i,\\ldots, g_l/g_i]\\right),$ where $k[X][g_1/g_i,\\ldots, g_l/g_i]\\subseteq k(X)$ can be viewed as a subring of the fraction field of $k[X].$\nHere is an easy example. We compute $\\widetilde X$ where $X=\\Bbb A^2$ has centre $Y=\\{(0,0)\\}.$ So $I_Y=(x,y),$ and $\\widetilde X$ has two charts: $U_x=\\operatorname{Spec}(k[x,y,y/x])=\\operatorname{Spec}(k[x,y/x]),$ and $U_y=\\operatorname{Spec}(k[x,y,x/y])=\\operatorname{Spec}(k[y,x/y]).$ That was easy.\nHere is another example, which generalizes. Knowing $\\widetilde X=\\widetilde{\\Bbb A^2}$, we can compute the blowup of say $V=V(y^2-x^3)\\subseteq X$ as the strict transform under the map $\\pi:\\widetilde X\\to X.$ What I mean is that in the $x$-chart of $\\widetilde X,$ the map $\\pi$ has a dual description given by $k[x,y]\\to k[x,y,y/x],x\\mapsto x,y\\mapsto x\\cdot y/x,$ and we can use this to get $\\widetilde V.$ In this chart, the equation for $V$ maps $y^2-x^3\\mapsto (x\\cdot y/x)^2-x^3= x^2((y/x)^2-x).$ Saying that $\\widetilde V$ is the strict transform means that it has a chart defined by $(y/x)^2-x$ inside $\\Bbb A^2=\\operatorname{Spec}(k[x,y/x]).$ There is a second chart which you can compute in exactly the same way.\nThere is much, much more that can be said about blowups, they are a central technique in algebraic geometry in my view. I think the best you can do to learn them is to compute as many examples as possible. There is a great section in Eisenbud and Harris' book on blow-ups. You should try to compute every example there.", "meta": {"post_id": 193681, "input_score": 24, "output_score": 43, "post_title": "What is a blow-up?"}}
{"input": "While studying complex variables, I could learn that $f(z)=|z|^{2}$ has only one point which is $z=0$ that $f$ being differentiable and $f$ being not differentiable at any other points.\nThen, I was wondering if there is a function $f: \\mathbb R \\to \\mathbb R$ that has only one point differentiable and not on any other points.\nIn intuition, it seems there are no such point!\nHowever, I have no idea how I can prove this...\nAdditional question is that would there be any function $f: \\mathbb R \\to \\mathbb R$ that has only one point continuous and not on any other points.\nI think this is pretty interesting things to think about! :-)", "output": "Let $$p(x)= \\begin{cases} 0,& x\\in\\mathbb Q\\\\\\\\1,& x\\in \\mathbb R-\\mathbb Q \\end{cases}$$ Now take $f(x)=x^2p(x)$.", "meta": {"post_id": 194194, "input_score": 28, "output_score": 44, "post_title": "Is there a function $f: \\mathbb R \\to \\mathbb R$ that has only one point differentiable?"}}
{"input": "I have a doubt about the real meaning of the derivative of a vector field. This question seems silly at first but the doubt came when I was studying the definition of tangent space.\nIf I understood well a vector is a directional derivative operator, i.e.: a vector is an operator that can produce derivatives of scalar fields. If that's the case then a vector acts on a scalar field and tells me how the field changes on that point. \nHowever, if a vector is a derivative operator, a vector field defines a different derivative operator at each point. So differentiate a vector would be differentiate a derivate operator, and that seems strange to me at first. I thought for example that the total derivative of a vector field would produce rates of change of the field, but my studies led me to a different approach, where the total derivative produces rates of change only for scalar fields and for vector fields it produces the pushforward.\nSo, what's the real meaning of differentiating a vector field knowing all of this?", "output": "As I understand it, these are your questions:\n\nHow does one define the derivative of a vector field?  Do we just take the \"derivatives\" of each vector in the field?  If so, what does it mean to take the derivative of a differential operator, anyway?\nWhy does the total derivative of a scalar field give information about rates of change, while the \"total derivative\" of a vector field gives the pushforward (which doesn't seem to relate to rates of change)?\n\nI think the best way to answer these questions is to provide a broader context:\n\nIn calculus, we ask how to find derivatives of functions $F\\colon \\mathbb{R}^m \\to \\mathbb{R}^n$.  The typical answer is the total derivative $DF\\colon \\mathbb{R}^m \\to L(\\mathbb{R}^m, \\mathbb{R}^n)$, which assigns to each point $p \\in \\mathbb{R}^m$ a linear map $D_pF \\in L(\\mathbb{R}^m, \\mathbb{R}^n)$.  With respect to to the standard bases, this linear map can be represented as a matrix:\n$$D_pF = \\begin{pmatrix}\n \\left.\\frac{\\partial F^1}{\\partial x^1}\\right|_p & \\cdots & \\left.\\frac{\\partial F^1}{\\partial x^m}\\right|_p \\\\\n \\vdots & & \\vdots \\\\\n \\left.\\frac{\\partial F^n}{\\partial x^1}\\right|_p & \\cdots & \\left.\\frac{\\partial F^n}{\\partial x^m}\\right|_p\n\\end{pmatrix}$$\nPersonally, I think this encodes the idea of \"rate of change\" very well.  (Just look at all those partial derivatives!)\nLet's now specialize to the case $m = n$.  Psychologically, how does one intuit these functions $F\\colon \\mathbb{R}^n \\to \\mathbb{R}^n$?  There are two usual answers:\n\n(1) We intuit $F\\colon \\mathbb{R}^n \\to \\mathbb{R}^n$ as a map between two different spaces.  Points from the domain space get sent to points in the codomain space.\n(2) We intuit $F\\colon \\mathbb{R}^n \\to \\mathbb{R}^n$ as a vector field.  Every point in $\\mathbb{R}^n$ is assigned an arrow in $\\mathbb{R}^n$.\n\nThis distinction is important.  When we generalize from $\\mathbb{R}^n$ to abstract manifolds, these two ideas will take on different forms.  Consequently, this means that we will end up with different concepts of \"derivative.\"\n\nIn case (1), the maps $F\\colon \\mathbb{R}^m \\to \\mathbb{R}^n$ generalize to smooth maps between manifolds $F \\colon M \\to N$.  In this setting, the concept of \"total derivative\" generalizes nicely to \"pushforward.\"  That is, it makes sense to talk about the pushforward of a smooth map $F \\colon M \\to N$.\nBut you asked about vector fields, which brings us to case (2).  In this case, we first have to be careful about what we mean by \"vector\" and \"vector field.\"\n\nA vector $v_p \\in T_pM$ at a point $p$ is (as you say) a directional derivative operator at the point $p$.  This means that $v_p$ inputs a scalar field $f\\colon M \\to \\mathbb{R}$ and outputs a real number $v_p(f) \\in \\mathbb{R}$.\nA vector field $v$ on $M$ is a map which associates to each point $p \\in M$ a vector $v_p \\in T_pM$.  This means that a vector field defines a derivative operator at each point.\nTherefore: a vector field $v$ can be regarded as an operator which inputs scalar fields $f\\colon M \\to \\mathbb{R}$ and outputs scalar fields $v(f)\\colon M \\to \\mathbb{R}$.\n\nIn this setting, it no longer makes sense to talk about the \"total derivative\" of a vector field.  You've said it yourself: what would it even mean to talk about \"derivatives\" of vectors, anyway?  This doesn't make sense, so we'll need to go a different route.\n\nIn differential geometry, there are two ways of talking about the derivative of a vector field with respect to another vector field:\n\nConnections (usually denoted $\\nabla_wv$ or $D_wv$)\nLie derivatives (usually denoted $\\mathcal{L}_wv$ or $[w,v]$)\n\nIntuitively, these notions capture the idea of \"infinitesimal rate of change of a vector field $v$ in the direction of a vector field $w$.\"\nQuestion: What do these constructions look like in $\\mathbb{R}^n$?\nTaking advantage of the fact that we're in $\\mathbb{R}^n$, we can look at our vector fields in the calculus way: as functions $v\\colon \\mathbb{R}^n \\to \\mathbb{R}^n$.  As such, we can write the components as $v = (v^1,\\ldots, v^n)$.\n\nThe (Levi-Civita) connection of $v$ with respect to $w$ is defined as\n  $$\\nabla_wv = (w(v^1), \\ldots, w(v^n)),$$\n  where $$w(v^i) := w^1\\frac{\\partial v^i}{\\partial x^1} + \\ldots + w^n\\frac{\\partial v^i}{\\partial x^n}.$$\nThe Lie derivative of $v$ with respect to $w$ has a technical definition in terms of flows that I don't want to go into, but the bottom line is that it's similar to Rod Carvalho's answer.\n\nAlso, in $\\mathbb{R}^n$ we have the pleasant formula\n$$\\mathcal{L}_wv = \\nabla_wv - \\nabla_vw,$$\nwhich aids in computation.", "meta": {"post_id": 195000, "input_score": 37, "output_score": 42, "post_title": "Meaning of derivatives of vector fields"}}
{"input": "There was an interesting problem asked about triples $(x,y,z)$ which are solutions of \n$$x! = y! + z!.$$ \nHere $(2,1,1)$ is a solution because $2! = 1! + 1!$, as are $(2,1,0)$ and $(2,0,1)$. \nNow I wanted to analyze this a bit further and thought of using the gamma function definition of a factorial, to see where it led and this is what I got:\n$x! = y! + z!$\n$ \\Gamma(x) = \\Gamma(y) + \\Gamma(z) $\n$\\int_{-\\infty}^{\\infty}t^xe^{-t}dt =  \\int_{-\\infty}^{\\infty}t^ye^{-t}dt + \\int_{-\\infty}^{\\infty}t^ze^{-t}dt $\n$\\int_{-\\infty}^{\\infty}t^xe^{-t}dt - \\int_{-\\infty}^{\\infty}t^ye^{-t}dt - \\int_{-\\infty}^{\\infty}t^ze^{-t}dt = 0$\n$\\int_{-\\infty}^{\\infty}(t^x - t^y - t^z)e^{-t}dt = 0$\nNow my line of thinking was that, in a manner similar to the fundamental lemma of the calculus of variations, this should imply that the above integral can only be true for arbitrary values of $x, y$ and $z$ if $t^x - t^y - t^z = 0$. \nI'm not really sure if that is justifiable so I'd appreciate some comment on this. \nThe reason I continued on despite the uncertainty is that when you're left with the polynomial $t^x - t^y - t^z = 0$ you encounter a strange fact. First off, plugging in a triple like $(2,1,1)$ results in $t^2 - t^1 - t^1 = 0$ which implies $t^2 = 2t$ thus $t = 2$. Now plugging in $t = 2$ gives $2^2 = 1^2 + 1^2$. In other words you get what you'd expect. However plugging in a triple you know is inconsistent, like $(0,0,0)$, gives\n$t^0 = t^0 + t^0$\n$1 = 1 + 1$\n$1 = 2$.\nYou get absurdities when you plug in inconsistent triples.\nSo while $(2,1,0)$, which works, gives \n$t^2 - t^1 - t^0 = 0$ \n$t^2 - t - 1 = 0$\nwhose roots are\n$\\frac{1}{2} + \\frac{\\sqrt{5}}{2}$\n$\\frac{1}{2} - \\frac{\\sqrt{5}}{2}$\nand on plugging these into $t^x - t^y - t^z = 0$ gives a consistent equality for both roots, an inconsistent triple like $(3,2,1)$ leaves us with $t^3 - t^2 - t = 0$, one of whose roots are $t = 0$ (which gives a consistent equality $0^3 - 0^2 - 0 = 0$ whereas another one of it's roots, $t = \\frac{1}{2} + \\frac{\\sqrt{5}}{2}$ does not give a consistent equality \n$(\\frac{1}{2} + \\frac{\\sqrt{5}}{2})^3 - (\\frac{1}{2} + \\frac{\\sqrt{5}}{2})^2 - (\\frac{1}{2} + \\frac{\\sqrt{5}}{2}) = 0$. \nIf all of the above is magically okay you can use this idea to show that no triples $(n, n - 1, n - 2)$ will work for $n$ greater than $2$. I'd like to go further with it but I'm afraid it's all just conceptually flawed, is it?\nA more interesting question stems from something a great lecturer told me, she basically analyzed the whole question geometrically and said something about certain triples having to do with the area of those gamma integrals canceling out. I thought I understood what she was saying but I actually don't so I'd appreciate any comment on what this could mean.\nI hope I was clear enough, thanks for your time.", "output": "$x! = y! + z!$ does not have any solutions in integers with $x \\geq 3.$ As soon as $x \\geq 3,$ we have $(x-1)! \\leq x! / 3.$ With the necessary $y,z < x,$ we get $y! \\leq x! / 3, \\; \\; z! \\leq x! / 3,$ so $y! + z! \\leq 2 x! / 3$ and $y! + z! \\neq x!$", "meta": {"post_id": 206679, "input_score": 23, "output_score": 44, "post_title": "Integer solutions of $x! = y! + z!$"}}
{"input": "My professor sort of skimmed through this concept, giving only the definition and one example (i.e. $(0,1) \\subset \\mathbb{R}$ vs. $(0,1) \\subset \\mathbb{R}^2$, where $(0,1)$ is open relative to $\\mathbb{R}$ but not $\\mathbb{R}^2$), but no real explanation. I understand, more or less, this particular example, but I'm having trouble understanding this more generally.\nCan somebody please intuitively explain this concept in the context of a metric space? (If it matters, we are using Rudin's Principles Of Mathematical Analysis).", "output": "In order to know what \"open relative to\" means, you have to first know what \"open\" means:\n\nA subset S of a metric space X is called open if, for every point p of S, there is a positive real number $\\epsilon$ such that every point in X of distance less than $\\epsilon$ from p lies in S.\n\nNow, if we're working in some metric space and consider a subset A of that space, you can, if you wish, disregard the rest of the space and think of A as a metric space in itself. The notion of \"open relative to A\" is what you get when you realize, looking at the above definition, that some subsets of A that are not open subsets of the larger space may still be open subsets of A.\nFor example, suppose you look at the interval I=[0,1] as a metric space in itself, and disregard the rest of the real line. Then the subset S=(0.5,1] may seem at first not to be an open set, because you think, \"Hey, there are points arbitrarily close to 1 that are not in S, so 1 is actually a boundary point!\" Well, yes... except that none of those arbitrarily-close points are actually in I, so from the point of view of I as a space in itself, they don't exist. Every point in I sufficiently close to 1 is also in S, so if we want to work in I alone, we have to concede that S is open.\nThe moral of this story is that sets aren't inherently open: the definition depends on what metric space they are considered as subsets of. If you are only talking about one space it's harmless to just write \"open,\" but otherwise you may have to be more specific. If you are considering both $\\mathbb{R}$ and I, for example, writing \"(0.5,1] is open\" is ambiguous. So instead you write \"(0.5,1]\" is open relative to I.\"", "meta": {"post_id": 210815, "input_score": 20, "output_score": 50, "post_title": "Could someone explain the concept of a set being \"open relative\" to another set?"}}
{"input": "I don't understand this one part in the proof for convergent sequences are bounded.\n\nProof:\nLet $s_n$ be a convergent sequence, and let $\\lim s_n = s$. Then taking $\\epsilon = 1$ we have:\n$n > N \\implies |s_n - s| < 1$\nFrom the triangle inequality we see that: $ n > N \\implies|s_n| - |s| < 1 \\iff |s_n| < |s| + 1$.\nDefine $M= \\max\\{|s|+1, |s_1|, |s_2|, ..., |s_N|\\}$. Then we have $|s_n| \\leq M$ for all $n \\in N$.\n\nI do not understand the defining $M$ part. Why not just take $|s| + 1$ as the bound, since for $n > N \\implies |s_n| < |s| + 1$?", "output": "$|s|+1$ is a bound for $a_n$ when $n > N$. We want a bound that applies to all $n \\in \\mathbb{N}$. To get this bound, we take the supremum of $|s|+1$ and all terms of $|a_n|$ when $n \\le N$. Since the set we're taking the supremum of is finite, we're guaranteed to have a finite bound $M$.", "meta": {"post_id": 213936, "input_score": 41, "output_score": 43, "post_title": "Prove: Convergent sequences are bounded"}}
{"input": "Well, I need a deformation retract from $GL_n^{+}(\\mathbb{R})$ to $SO(n)$\nHere is what I tried, let $A\\in GL_n^{+}(\\mathbb{R})$ $A=(A_1,\\dots,A_n)$ where $A_i$'s are column vectors, Recall that the Gram-Schmidt algorithm turns A into an orthogonal matrix by the following sequence of steps. First normalise $A_1$ (i.e. make it unit length) $A_1\\mapsto \\frac{A_1}{|A_1|}$ next I make $A_2$ orthogonal to $A_1$ like $A_2\\mapsto A_2-\\langle A_1,A_2\\rangle A_1$ and normalize $A_2\\mapsto \\frac{A_2}{|A_2|}$  like this up to $A_n$\nBut I am not getting an explicit homotopy which  gives me a deformation retract $GL_n^{+}(\\mathbb{R})$ to $SO(n)$", "output": "Here is a geometric way to see this. To any ordered basis $(v_1,v_2,\\ldots,v_n)$ of your vector space $V$ associate the \"flag\" of subspaces $V_0=\\{0\\}$, $V_1=\\langle v_1\\rangle$, $V_2=\\langle v_1,v_2\\rangle$, ... $V_n=\\langle v_1,v_2,\\ldots,v_n\\rangle=V$. The Gram-Schmidt algorithm turns any such basis into an orthonormal basis $(b_1,\\ldots,b_n)$ that gives rise to the same flag of subspaces. It is moreover the unique such basis (orthonomal and with the same flag) for which in addition each $b_i$, inside $V_i$, is on the same side of the hyperplane $V_{i-1}$ as the original basis vector $v_i$.\nNow taking $V=\\Bbb R^n$ we can identify $GL_n^+(\\Bbb R)$ with the set of ordered bases $(v_1,v_2,\\ldots,v_n)$ with $\\det(v_1,v_2,\\ldots,v_n)>0$, and $SO(n)$ with the set of ordered orthonormal bases $(b_1,b_2,\\ldots,b_n)$ with $\\det(b_1,b_2,\\ldots,b_n)>0$. Now for such a basis $(v_1,v_2,\\ldots,v_n)$ let $(b_1,\\ldots,b_n)$ be the orthonormal basis associated to it under Gram-Schmidt, and simultaneously (or successively if you prefer) deform every $v_i$ linearly to $b_i$, as $t\\mapsto (1-t)v_i+tb_i$. The intermediate vectors stay inside $V_i$, and since $b_i$ is on the same side as $v_i$, they never enter $V_{i-1}$. This means the deformed vectors stay linearly independent at all times, so the deformation takes place inside $GL_n(\\Bbb R)$. As the determinant cannot vanish anywhere we have $\\det(v_1,v_2,\\ldots,v_n)>0\\implies \\det(b_1,b_2,\\ldots,b_n)>0$ and we have a deformation retract of $GL_n^+(\\Bbb R)$ to $SO(n)$. It is in fact a strong deformation retract: elements of $SO(n)$ remain fixed.", "meta": {"post_id": 214784, "input_score": 16, "output_score": 44, "post_title": "deformation retract of $GL_n^{+}(\\mathbb{R})$"}}
{"input": "I'm looking for some intuition regarding universal covers of topological spaces. \n$\\textbf{Setup:}$ For a topological space $X$ with sufficient adjectives we can construct a/the simply connected covering space of it by looking at equivalence classes of paths at a given base point. We then can put a topology in the standard way done by Hatcher - an open set around an equivalence class of paths, say $[\\gamma]$  is the set of $[\\gamma\\cdot\\eta]$ where $\\eta$ is a path starting at $\\gamma(1)$ contained in $U$ open in $X$.\nHere are my questions:\nQ: I find this topological space, as constructed above, non-intuitive. Certainly I dont know how I would manipulate it and make topological arguments in it. What is the 'right' way of thinking about the topology here? Or is this construction useful solely for proving the existence of simply connected covers?\nQ: Often times it is tractable to construct a simply connected covering by ad-hoc methods (fancy guessing). The projective plane, torus, etc all spring to mind. By universality I know that the covering space obtained by any ad-hoc method is $\\textit{the}$ universal covering space obtained by the above method, so there is an isomorphism of these two. Is there a standard way to see this isomorphism? Being really concrete, say in the cases of $\\mathbb RP^2$, or $S^1\\times S^1$.\nIn simple terms: how can I 'see' what the universal cover looks like from the general construction?", "output": "Suppose $p:\\overline{C}\\to C$ is a universal covering. By definition, around every point in $C$ is an open set that lifts up to $\\overline{C}$. So, locally, $\\overline{C}$ looks just like $C$. Suppose one wanted to cut up $C$ into small (contractible) patches and then stitch them together again to form $\\overline{C}$ - the problem is that $\\overline{C}$ is to be simply connected, so if (say) we started stitching patches around a nontrivial loop in $C$ when we wrap back around to the basepoint we can't stitch that last patch back to the original patch, instead we have to create a copy of the original patch and continue on from there.\nConsider the space $C=\\Bbb C\\setminus\\{0\\}$. If one takes a counterclockwise loop from $-1$ around $0$ back to itself, then the last patch cannot be stitched to the first, so we should make a copy of the original patch to stitch it to. In the picture below, we've literally lifted the copy above the original:\n$\\hskip 2in$ \nIf we continue this process indefinitely, then there will lots of copies of pieces of $C$ that are being stitched together. Given a point in $C$ in a patch, there will be many copies of that patch in our quilt, and so many lifts of that point - what allows us to distinguish between lifts of the same point is how we got to it from the original basepoint. Thus, we can interpret points in $\\overline{C}$ as points in the original space $C$ but with a \"memory\" of how we got there from a basepoint.\nThis inspires us to formalize our construction by letting elements of $\\overline{C}$ be paths in $C$, modulo endpoint-fixing homotopy. Points in $\\overline{C}$ are specified by points in $C$ with a memory of how we got there from the basepoint, so if we got to $x\\in C$ via a path $\\gamma$ in $C$ and $U$ is any basic nbhd of $x\\in C$, then the lift $\\overline{U}$ of that nbhd is comprised of points $\\overline{u}$, and to specify these $\\overline{u}$ we must say which points of $C$ they are (done: they lie above $U$) and how we got to them. We got to these points in $\\overline{U}$ by first travelling along $\\gamma$ from the basepoint to $x$ and then wiggled around within $U$ itself.\nAs for your other question, try lifting the paths. Say that $D\\to C$ is a covering, where $D$ is a familiar space you know well, and in particular you know $D$ is simply connected. Our construction of $\\overline{C}$ is comprised of paths emanating from (say) $x\\in C$. To see what the corresponding point of $D$ is, just lift the path from $C$ to $D$ and look at its endpoint! This is the isomorphism.", "meta": {"post_id": 216376, "input_score": 27, "output_score": 36, "post_title": "Universal cover via paths vs. ad hoc constructions"}}
{"input": "The $p$-norm on $\\mathbb R^n$ is given by $\\|x\\|_{p}=\\big(\\sum_{k=1}^n\n|x_{k}|^p\\big)^{1/p}$. For $0 < p < q$ it can be shown that $\\|x\\|_p\\geq\\|x\\|_q$ (1, 2). It appears that in $\\mathbb{R}^n$ a number of opposite inequalities can also be obtained. In fact, since all norms in a finite-dimensional vector space are equivalent, this must be the case. So far, I only found the following: $\\|x\\|_{1} \\leq\\sqrt n\\,\\|x\\|_{2}$(3),  $\\|x\\|_{2} \\leq \\sqrt n\\,\\|x\\|_\\infty$ (4). Geometrically, it is easy to see that opposite inequalities must hold in $\\mathbb R^n$. For instance, for $n=2$ and $n=3$ one can see that for $0 < p < q$, the spheres with radius $\\sqrt n$ with $\\|\\cdot\\|_p$ inscribe spheres with radius $1$ with $\\|\\cdot\\|_q$.\nIt is not hard to prove the inequality (4). According to Wikipedia, inequality (3) follows directly from Cauchy-Schwarz, but I don't see how. For $n=2$ it is easily proven (see below), but not for $n>2$. So my questions are:\n\nHow can relation (3) be proven for arbitrary $n\\,$?\nCan this be generalized into something of the form $\\|x\\|_{p} \\leq C \\|x\\|_{q}$ for arbitrary $0<p < q\\,$?\nDo any of the relations also hold for infinite-dimensional spaces, i.e. in $l^p$ spaces?\n\n\nNotes:\n$\\|x\\|_{1}^{2} = |x_{1}|^2 + |x_{2}|^2 + 2|x_{1}||x_{2}| \\leq |x_{1}|^2 + |x_{2}|^2 + \\big(|x_{1}|^2 + |x_{2}|^2\\big) = 2|x_{1}|^2 + 2|x_{2}|^2$, hence  $=2\\|x\\|_{2}^{2}$\n$\\|x\\|_{1} \\leq \\sqrt 2 \\|x\\|_{2}$.\nThis works because $|x_{1}|^2 + |x_{2}|^2 \\geq 2|x_{1}\\|x_{2}|$, but only because $(|x_{1}| - |x_{2}|)^2 \\geq 0$, while for more than two terms $\\big(|x_{1}| \\pm |x_{2}| \\pm \\dotsb \\pm |x_{n}|\\big)^2 \\geq 0$ gives an inequality that never gives the right signs for the cross terms.", "output": "Using Cauchy\u2013Schwarz inequality we get for all $x\\in\\mathbb{R}^n$\n$$\n\\Vert x\\Vert_1=\n\\sum\\limits_{i=1}^n|x_i|=\n\\sum\\limits_{i=1}^n|x_i|\\cdot 1\\leq\n\\left(\\sum\\limits_{i=1}^n|x_i|^2\\right)^{1/2}\\left(\\sum\\limits_{i=1}^n 1^2\\right)^{1/2}=\n\\sqrt{n}\\Vert x\\Vert_2\n$$\nSuch a bound does exist. Recall H\u00f6lder's inequality \n$$\n\\sum\\limits_{i=1}^n |a_i||b_i|\\leq\n\\left(\\sum\\limits_{i=1}^n|a_i|^r\\right)^{\\frac{1}{r}}\\left(\\sum\\limits_{i=1}^n|b_i|^{\\frac{r}{r-1}}\\right)^{1-\\frac{1}{r}}\n$$\nApply it to the case $|a_i|=|x_i|^p$, $|b_i|=1$ and $r=q/p>1$\n$$\n\\sum\\limits_{i=1}^n |x_i|^p=\n\\sum\\limits_{i=1}^n |x_i|^p\\cdot 1\\leq\n\\left(\\sum\\limits_{i=1}^n (|x_i|^p)^{\\frac{q}{p}}\\right)^{\\frac{p}{q}}\n\\left(\\sum\\limits_{i=1}^n 1^{\\frac{q}{q-p}}\\right)^{1-\\frac{p}{q}}=\n\\left(\\sum\\limits_{i=1}^n |x_i|^q\\right)^{\\frac{p}{q}} n^{1-\\frac{p}{q}}\n$$\nThen\n$$\n\\Vert x\\Vert_p=\n\\left(\\sum\\limits_{i=1}^n |x_i|^p\\right)^{1/p}\\leq\n\\left(\\left(\\sum\\limits_{i=1}^n |x_i|^q\\right)^{\\frac{p}{q}} n^{1-\\frac{p}{q}}\\right)^{1/p}=\n\\left(\\sum\\limits_{i=1}^n |x_i|^q\\right)^{\\frac{1}{q}} n^{\\frac{1}{p}-\\frac{1}{q}}=\\\\=\nn^{1/p-1/q}\\Vert x\\Vert_q\n$$\nIn fact $C=n^{1/p-1/q}$ is the best possible constant.\nFor infinite dimensional case such inequality doesn't hold. For explanation see this answer.", "meta": {"post_id": 218046, "input_score": 100, "output_score": 137, "post_title": "Relations between p norms"}}
{"input": "In a lecture note from MIT on number theory says: \nTheorem 5. The greatest common divisor of a  and b  is equal to the smallest positive linear combination of a  and b.\nFor example, the greatest common divisor of 52 and 44 is 4. And, sure enough, 4 is a\nlinear combination of 52 and 44: \n6 \u00b7 52 + (\u22127) 44 = 4\nWhat about 12 and 6 their gcd is 6 but 0 which is less than 6 can be", "output": "You wrote it yourself: the gcd is the smallest positive linear combination. Smallest positive linear combination is shorthand for smallest positive number which is a linear combination. It is true that $0$ is a linear combination of $12$ and $6$ with integer coefficients, but $0$ is not positive.  \nThe proof is not difficult, but it is somewhat lengthy. We give full detail below. \nLet $e$ be the smallest positive linear combination $as+bt$ of $a$ and $b$, where $s$ and $t$ are integers. Suppose in particular that $e=ax+by$.\nLet $d=\\gcd(a,b)$. Then $d$ divides $a$ and $b$, so it divides $ax+by$. Thus $d$ divides $e$, and therefore in particular $d\\le e$.\nWe show that in fact $e$ is a common divisor of $a$ and $b$, which will imply that $e\\le d$. That, together with our earlier $d\\le e$, will imply that $d=e$.\nSo it remains to show that $e$ divides $a$ and $e$ divides $b$. We show that $e$ divides $a$. The proof that $e$ divides $b$ is essentially the same.\nSuppose to the contrary that $e$ does not divide $a$. Then when we try to divide $a$ by $e$, we get a positive remainder. More precisely,\n$$a=qe+r,$$\nwhere $0\\lt r\\lt e$. Then\n$$r=a-qe=a-q(ax+by)=a(1-qx)+b(-qy).$$\nThis means that $r$ is a linear combination of $a$ and $b$, and is positive and less than $e$. This contradicts the fact that $e$ is the smallest positive linear combination of $a$ and $b$.", "meta": {"post_id": 219941, "input_score": 22, "output_score": 51, "post_title": "Is greatest common divisor of two numbers really their smallest linear combination?"}}
{"input": "According to Wikipedia,\n\nThe collection of all algebraic objects of a given type will usually\nbe a proper class. Examples include the class of all groups, the class\nof all vector spaces, and many others. In category theory, a category\nwhose collection of objects forms a proper class (or whose collection\nof morphisms forms a proper class) is called a large category.\n\nI am aware of Russell's Paradox, which explains why not everything is a set, but how can we show the collection of all groups is a proper class?", "output": "The collection of singletons is not a set. Therefore the collection of all trivial groups is not a set.\nIf you wish to consider \"up to isomorphism\", note that for every infinite cardinal $\\kappa$ you can consider the free group, or free abelian group with $\\kappa$ generators. These are distinct (up to isomorphism, that is), and since the collection of cardinals is not a set the collection of groups cannot be a set either.", "meta": {"post_id": 226413, "input_score": 28, "output_score": 39, "post_title": "Why is the collection of all groups a proper class rather than a set?"}}
{"input": "Would someone be so kind to explain this to me:\n$$\\pi_nk=\\left\\{\\begin{array}{cl}1&\\textrm{if }k=\\arg\\min_j\\left\\Vert\\mathbf x_n-\\mu_j\\right\\Vert^2\\\\0&\\textrm{otherwise}\\end{array}\\right..$$\nEspecially the $\\arg\\min$ part.\n(It's from the $k$-means algorithm.)", "output": "$\\arg \\min$ (or $\\arg \\max$) return the input(s) for which the output is minimum (or maximum).\nFor example:\nThe graph illustrates the function $f(x)=2 \\sin(x-0.5)+\\cos(x)^2$.\nThe global minimum of $f(x)$ is $\\min(f(x)) \\approx -2$, while $\\arg \\min f(x)  \\approx 4.9$.", "meta": {"post_id": 227626, "input_score": 84, "output_score": 53, "post_title": "Meaning of \u201carg min\u201d"}}
{"input": "I have a question about the following property, which I didn't know so far: \n\nWhy does the It\u014d integral have zero expectation? Is this true for every integrator and integrand? Or is this restricted to special processes, i.e. is\n  $$\\mathbb{E}\\left[\\int f \\, \\mathrm{d}M\\right]=0$$\n  for all local Martingales $M$ and predictable $f$, such that the integral is well defined? \n\nThank you for clarification.", "output": "This statement is wrong in general.\nIt may fail even when the integrator $M_t$ is a Brownian motion. In fact,\n\nGiven a probability distribution $P$ on $\\mathbb{R}$, it is possible\n  to find an adapted $t$-measurable process $f(\\omega,t)$, with\n  $\\mathbb{P}\\left(\\int_0^1 f^2(\\omega,t)\\,dt<\\infty\\right)=1$ such\n  that the random variable $$\\int_0^1 f(\\omega,t) \\, dB_t$$ has\n  distribution $P$.\n\nThis statement is known as Dudley's representation theorem (see the original paper). Hence, the expectation of the stochastic integral may take any real value, be infinite or not exist at all. \n\nAnother counterexample arises from the stochastic differential equation $$dX_t = X^2_t\\, dB_t, \\quad X_0=x, \\quad \\textrm{where } x>0.$$\nIt may be shown that the solution exists, is unique, is a strictly positive local martingale, but $\\mathbb{E} X_t \\to 0$ as $t\\to \\infty$.\nSee the details in George Lowther's blog, where this example is taken from.\n\nA sufficient condition for the integral $\\int_0^t f(\\omega, s)\\, dB_s$ to be a martingale on $[0,T]$ is that\n\n$f(\\omega,s)$ is adapted, measurable in s, and\n$\\mathbb{E}\\left(\\int_0^T f^2(\\omega,s)\\,ds\\right) < \\infty$.\n\nIn this case, indeed, $\\mathsf{E} \\left(\\int_0^T f(\\omega,s)\\, dB_s\\right)=0$.\n\nIf the integrator $M_t$ is an arbitrary martingale, and the integrand $f$ is bounded, then the integral is a martingale, and the expectation of the integral is again zero (proof).\n\nFinally, if the integrator $M_t$ is a local martingale, very little can be said about the expectation of the integral. If $f(\\omega,t)$ is sufficiently nice, the integral $\\int_0^t f(\\omega,s) \\, dM_s$ is a local martingale, but that does not guarantee that the expectation is zero, as the second counterexample above shows.", "meta": {"post_id": 232932, "input_score": 34, "output_score": 36, "post_title": "It\u014d Integral has expectation zero"}}
{"input": "What is the difference, if any, between kernel and null space?\nI previously understood the kernel to be of a linear map and the null space to be of a matrix: i.e., for any linear map $f : V \\to W$,\n$$\n\\ker(f) \\cong \\operatorname{null}(A),\n$$\nwhere\n\n$\\cong$ represents isomorphism with respect to $+$ and $\\cdot$, and\n$A$ is the matrix of $f$ with respect to some source and target bases.\n\nHowever, I took a class with a professor last year who used $\\ker$ on matrices. Was that just an abuse of notation or have I had things mixed up all along?", "output": "The terminology \"kernel\" and \"nullspace\" refer to the same concept, in the context of vector spaces and linear transformations. It is more common in the literature to use the word nullspace when referring to a matrix and the word kernel when referring to an abstract linear transformation. However, using either word is valid. Note that a matrix is a linear transformation from one coordinate vector space to another. Additionally, the terminology \"kernel\" is used extensively to denote the analogous concept as for linear transformations for morphisms of various other algebraic structures, e.g. groups, rings, modules and in fact we have a definition of kernel in the very abstract context of abelian categories.", "meta": {"post_id": 235350, "input_score": 59, "output_score": 59, "post_title": "What is the difference between kernel and null space?"}}
{"input": "I verified experimentally that in Java the equality\nMath.sqrt(x*x) = x\n\nholds for all long x such that x*x doesn't overflow. Here, Java long is a $64$ bit signed type and double is a IEEE binary floating point type with at least $53$ bits mantissa and sufficiently long exponent.\nMathematically, there are two imprecise functions involved:\n\nConversion from long to double which loses precision due to the mantissa being only $53$ bits where $63$ bits would be needed. This operation is guaranteed to return the closest representable result.\nComputing square root, which is also guaranteed to return the closest representable result.\n\nMathematically, this can be expressed like\n$$\n\\mathop\\forall_{x \\in {\\mathbb N} \\atop x \\le 3037000499}\n\\mathrm{round}\\left(\\sqrt{\\mathrm{round}(x^2)}\\right)\n= x\n$$\nwhere $\\mathrm{round}$ is the rounding function from $\\mathbb R$ into the set of all numbers representable as double.\nI'm looking for a proof since no experiment can assure it works across all machines.", "output": "The idea is simple: Find upper and lower bounds for\n$$X := \\sqrt{\\mathrm{round}(x^2)}$$\nand show that $\\mathrm{round}(X) = x$.\n\nLet $\\mathrm{ulp}(x)$ denote the unit of least precision at $x$\nand let $E(x)$ and $M(x)$ denote the exponent and mantissa of $x$, i.e.,\n$$x = M(x) \\cdot 2^{E(x)}$$\nwith $1 \\le M(x) < 2$ and $E(x) \\in \\mathbb Z$. Define\n$$\\Delta(x) = \\frac{\\mathrm{ulp}(x)}x = \\frac{\\mu \\cdot 2^{E(x)}}x = \\frac\\mu{M(x)}$$\nwhere $\\mu=2^{-52}$ is the machine epsilon.\nExpressing the rounding function by its relative error leads to\n$$X = \\sqrt{(1+\\epsilon) \\cdot x^2} = \\sqrt{(1+\\epsilon)} \\cdot x\n< \\big( 1+\\frac\\epsilon2 \\big) \\cdot x$$\nWe know that $|\\epsilon| \\le \\frac12\\Delta(x^2)$ and get (ignoring the trivial case $x=0$)\n$$\\frac Xx < 1 + \\frac{\\Delta(x^2)}4 = 1 + \\frac\\mu{4 M(x^2)}$$\n\nBy observing $M(x)$ and $M(x^2)$ e.g. over the interval $[1, 4]$,\nit can be easily be shown that $\\frac{M(x)}{M(x^2)} \\le \\sqrt2$ which gives us\n$$\\frac Xx < 1 + \\frac{\\mu\\sqrt2}{4 M(x)}$$\nand therefore\n$$X < x + \\frac{\\sqrt2}4 \\frac{\\mu}{M(x)} \\cdot x < x + \\frac12 \\mathrm{ulp}(x)$$\n\nAnalogously we get the corresponding lower bound. Just instead of\n$$\\sqrt{(1+\\epsilon)} < \\big( 1+\\frac\\epsilon2 \\big)$$\nwe use something like\n$$\\sqrt{(1-\\epsilon)} > \\big( 1 - (1+\\epsilon) \\cdot \\frac\\epsilon2 \\big)$$\nwhich suffices, since we used a very generous estimate ($\\sqrt2/4<\\frac12$) in the last step.\n\nBecause of $|X-x|$ being smaller than $\\frac12 \\mathrm{ulp}(x)$, $x$ is the double closest to $X$, therefore $\\mathrm{round}(X)$ must equal to $x$, q.e.d.", "meta": {"post_id": 237865, "input_score": 57, "output_score": 45, "post_title": "Show that floating point $\\sqrt{x \\cdot x} \\geq x$ for all long $x$."}}
{"input": "It\u2019s from the book \u201cLinear Algebra and its Applications\u201d by Gilbert Strang, page 260.\n$$(I-A)^{-1}=I+A+A^2+A^3+\\ldots$$\nNonnegative matrix $A$ has the largest eigenvalue $\\lambda_1<1$.\nThen, the book says $(I-A)^{-1}$ has the same eigenvector, with eigenvalue $1/(1-\\lambda_1)$.\nWhy? Is there any other formulas between inverse matrix and eigenvalue that I don\u2019t know?", "output": "A matrix $A$ has an eigenvalue $\\lambda$ if and only if $A^{-1}$ has eigenvalue $\\lambda^{-1}$. To see this, note that\n$$A\\mathbf{v} = \\lambda\\mathbf{v} \\implies A^{-1}A\\mathbf{v} = \\lambda A^{-1}\\mathbf{v}\\implies A^{-1}\\mathbf{v} = \\frac{1}{\\lambda}\\mathbf{v}$$\nIf your matrix $A$ has eigenvalue $\\lambda$, then $I-A$ has eigenvalue $1 - \\lambda$ and therefore $(I-A)^{-1}$ has eigenvalue $\\frac{1}{1-\\lambda}$.", "meta": {"post_id": 237871, "input_score": 54, "output_score": 207, "post_title": "Inverse matrix\u2019s eigenvalue?"}}
{"input": "As normed division algebras, when we go from the complex numbers to the quaternions, we lose commutativity. Moving on to the octonions, we lose associativity. Is there some analogous property that we lose moving from the reals to the complex numbers?", "output": "The most obvious property that we lose is the linear (or total) ordering of the real line.", "meta": {"post_id": 240959, "input_score": 32, "output_score": 44, "post_title": "What do we lose passing from the reals to the complex numbers?"}}
{"input": "How does one create an explicit bijection from the reals to the set of all sequences of reals? I know how to make a  bijection from $\\mathbb R$ to $\\mathbb {R \\times R}$.\nI have an idea but I am not sure if it will work. I will post it as my own answer because I don't want to anchor your answers and I want to see what other possible ways of doing this are.", "output": "The nicest trick in the book is to find a bijection between $\\mathbb R$ and $\\mathbb{N^N}$, in this case we are practically done. Why?\n$$\\mathbb{(N^N)^N\\sim N^{N\\times N}\\sim N^N}$$\nAnd the bijections above are easy to calculate (I will leave those to you, the first bijection is a simple Currying, and for the second you can use Cantor's pairing function).\nSo if we can find a nice bijection between the real numbers the infinite sequences of natural numbers we are about done. Now, we know that $\\mathbb{N^N}$ can be identified with the real numbers, in fact continued fractions form a bijection between the irrationals and $\\mathbb{N^N}$. \nWe first need to handle the rational numbers, but that much is not very difficult. Take an enumeration of the rationals (e.g. Calkin-Wilf tree) in $(0,1)$, suppose $q_i$ is the $i$-th rational in the enumeration; now we take a sequence of irrationals, e.g. $r_n = \\frac1{\\sqrt{n^2+1}}$, and we define the following function:\n$$h(x)=\\begin{cases} r_{2n} & \\exists n: x=r_n\\\\ r_{2n+1} & \\exists n: x=q_n \\\\ x &\\text{otherwise}\\end{cases}$$\nNow we can finally describe a list of bijections which, when composed, give us a bijection between $\\mathbb R$ and $\\mathbb{R^N}$.\n\n$\\mathbb{R^N\\to (0,1)^N}$ by any bijection of this sort.\n$\\mathbb{(0,1)^N\\to \\left((0,1)\\setminus Q\\right)^N}$ by the encoding given by $h$.\n$\\mathbb{\\left((0,1)\\setminus Q\\right)^N\\to \\left(N^N\\right)^N}$ by continued fractions.\n$\\mathbb{\\left(N^N\\right)^N\\to N^{N\\times N}}$ by Currying.\n$\\mathbb{N^{N\\times N}\\to N^N}$ by a pairing function.\n$\\mathbb{N^N\\to (0,1)\\setminus Q}$ by decoding the continued fractions.\n$\\mathbb{(0,1)\\setminus Q\\to (0,1)}$ by the decoding of $h$, i.e. $h^{-1}$.\n$\\mathbb{(0,1)\\to R}$ by any bijection of this sort, e.g. the inverse of the bijection used for the first step.", "meta": {"post_id": 243590, "input_score": 48, "output_score": 50, "post_title": "Bijection from $\\mathbb R$ to $\\mathbb {R^N}$"}}
{"input": "What is the geometric meaning of the determinant of a matrix? I know that \"The determinant of a matrix represents the area of \u200b\u200ba rectangle.\" Perhaps this phrase is imprecise, but I would like to know something more, please.\nThank you very much.", "output": "If you think about the matrix as representing a linear transformation, then the determinant (technically the absolute value of the determinant) represents the \"volume distortion\" experienced by a region after being transformed.  So for instance, the matrix $2I$ stretches a square of area 1 into a square with area 4, since the determinant is 4.  This idea works in all dimensions too, not just 2 or 3!  \nThis also translates well when you get in to more general mappings $f:\\Bbb{R}^n\\rightarrow\\Bbb{R}^n$; if the function is nice enough, you can represent it \"locally\" by a linear transformation.  The (absolute value of the) determinant of this linear transformation gives the \"local\" volume distortion of the function - i.e. how much the function is stretching or compressing regions of space near a point.", "meta": {"post_id": 250534, "input_score": 39, "output_score": 41, "post_title": "Geometric meaning of the determinant of a matrix"}}
{"input": "Could you guide me how to prove that any monotone function from $R\\rightarrow R$ is Borel measurable?\nSince monotone functions are continuous away from countably many points, would that be helpful in proving the measurability?", "output": "Hint: If $f$ is monotone, then, for every real number $x$, the set $$f^{-1}((-\\infty,x])=\\{t\\mid f(t)\\leqslant x\\}$$ is either $\\varnothing$ or $(-\\infty,+\\infty)$ or $(-\\infty,z)$ or $(-\\infty,z]$ or $(z,+\\infty)$ or $[z,+\\infty)$ for some real number $z$.\nTo show this, assume for example that $f$ is nondecreasing and that $u$ is in $f^{-1}((-\\infty,x])$, then show that, for every $v\\leqslant u$, $v$ is also in $f^{-1}((-\\infty,x])$.", "meta": {"post_id": 252421, "input_score": 30, "output_score": 50, "post_title": "Are Monotone functions Borel Measurable?"}}
{"input": "In \"Surely You're Joking, Mr. Feynman!,\" Nobel-prize winning Physicist Richard Feynman said that he challenged his colleagues to give him an integral that they could evaluate with only complex methods that he could not do with real methods:\n\nOne\u00a0time\u00a0I boasted, \"I can do by other\u00a0methods any integral anybody else\u00a0needs contour integration to do.\"\nSo\u00a0Paul [Olum] puts up\u00a0this tremendous damn integral he\u00a0had\u00a0obtained\u00a0by starting\u00a0out\u00a0with a\u00a0complex function that he\u00a0knew\u00a0the\u00a0answer\u00a0to, taking\u00a0out the\u00a0real part of it and\u00a0leaving\u00a0only the\u00a0complex part. He\u00a0had\u00a0unwrapped\u00a0it so it was only possible\u00a0by contour integration! He was always deflating\u00a0me\u00a0like\u00a0that. He\u00a0was a\u00a0very smart fellow.\n\nDoes anyone happen to know what this integral was?", "output": "I doubt that we will ever know the exact integral that vexed Feynman.\nHere is something similar to what he describes.\nSuppose $f(z)$ is an analytic function on the unit disk.\nThen, by Cauchy's integral formula,\n$$\\oint_\\gamma \\frac{f(z)}{z}dz = 2\\pi i f(0),$$\nwhere $\\gamma$ traces out the unit circle in a counterclockwise manner.\nLet $z=e^{i\\phi}$.\nThen\n$\\int_0^{2\\pi}f(e^{i\\phi}) d\\phi = 2\\pi f(0).$\nTaking the real part of each side we find \n$$\\begin{equation*}\n\\int_0^{2\\pi} \\mathrm{Re}(f(e^{i\\phi}))d\\phi = 2\\pi \\mathrm{Re}(f(0)).\\tag{1}\n\\end{equation*}$$\n(We could just as well take the imaginary part.)\nClearly we can build some terrible integrals by choosing $f$ appropriately.\nExample 1.\nLet $\\displaystyle f(z) = \\exp\\frac{2+z}{3+z}$.\nThis is a mild choice compared to what could be done ...\nIn any case, $f$ is analytic on the disk.\nApplying (1), and after some manipulations of the integrand, we find\n$$\\int_0^{2\\pi}\n\\exp\\left(\\frac{7+5 \\cos\\phi}{10+6\\cos\\phi}\\right)\n\\cos \\left(\n\\frac{\\sin\\phi}{10+6 \\cos\\phi}\n\\right)\nd\\phi = 2\\pi e^{2/3}.$$\nExample 2.\nLet $\\displaystyle f(z) = \\exp \\exp \\frac{2+z}{3+z}$. \nThen \n\\begin{align*}\\int_0^{2\\pi} & \n\\exp\\left(\n \\exp\\left(\n  \\frac{7+5 \\cos \\phi}{10+6 \\cos \\phi}\n \\right) \n \\cos\\left(\n  \\frac{\\sin \\phi}{10+6 \\cos \\phi}\n \\right)\n\\right) \\\\\n& \\times\\cos\\left(\n \\exp\\left(\n     \\frac{7+5 \\cos \\phi}{10+6 \\cos \\phi}\n    \\right) \n \\sin\\left(\n     \\frac{\\sin \\phi}{10+6 \\cos \\phi}\n    \\right)\n\\right) = 2\\pi e^{e^{2/3}}.\n\\end{align*}", "meta": {"post_id": 253910, "input_score": 251, "output_score": 77, "post_title": "The Integral that Stumped Feynman?"}}
{"input": "Let $p$ and $q$ be polynomials (maybe in several variables, over a field), and suppose they have $m$ and $n$ non-zero terms respectively. We can assume $m\\leq n$. Can it ever happen that the product $p\\cdot q$ has fewer than $m$ non-zero terms?\nI ask this because I vaguely recall seeing a positive answer in book somewhere (probably about computation or algorithms since the polynomials were unwieldy). If anyone knows what book this is from it would be much appreciated.", "output": "Here's an elementary example. Start with the well-known identity $x^n - 1 = (x-1) (x^{n-1} + x^{n-2} + \\ldots + x + 1)$. If $n$ is odd, we can factor $x^n+1$ in a similar way by flipping the signs: $x^n + 1 = (x+1) (x^{n-1} - x^{n-2} + \\ldots - x + 1)$. Now mix and match the two:\n$$\\begin{align*}\n  x^{2n} - 1 &= (x^n - 1) (x^n + 1) \\\\\n  &= (x-1) (x^{n-1} + x^{n-2} + \\ldots + x + 1) (x+1) (x^{n-1} - x^{n-2} + \\ldots - x + 1) \\\\\n  &= (x+1) (x^{n-1} + x^{n-2} + \\ldots + x + 1) (x-1) (x^{n-1} - x^{n-2} + \\ldots - x + 1) \\\\\n  &= (x^n + 2x^{n-1} + 2x^{n-2} + \\ldots + 2x + 1) (x^n - 2x^{n-1} + 2x^{n-2} - \\ldots + 2x - 1)\n\\end{align*}$$\nI don't see an obvious generalization to even values of $n$.", "meta": {"post_id": 256028, "input_score": 49, "output_score": 46, "post_title": "Does multiplying polynomials ever decrease the number of terms?"}}
{"input": "What would be an example of a function that is continuous, but not uniformly continuous?\nWill $f(x)=\\frac{1}{x}$ on the domain $(0,2)$ be an example? And why is it an example?\nPlease explain strictly using relevant definitions.", "output": "Clearly $\\,\\displaystyle{\\frac{1}{x}}\\,$ is continuous in $\\,(0,2)\\,$ as it is the quotient of two polynomials and the denominator  doesn't vanish there.\nNow, if the function was uniformly continuous there then\n$$\\forall\\,\\epsilon>0\\,\\,\\exists\\,\\delta>0\\,\\,s.t.\\,\\,|x-y|<\\delta\\Longrightarrow \\left|\\frac{1}{x}-\\frac{1}{y}\\right|<\\epsilon$$\nBut taking $\\,\\epsilon=1\\,$ , then for any $\\,\\delta>0\\,$ we take\n$$x:=\\min(\\delta,1)\\,\\,,\\,y=\\frac{x}{2}\\Longrightarrow |x-y|=\\frac{x}{2}<\\delta, \\,\\text{but nevertheless}$$\n$$\\left|\\frac{1}{x}-\\frac{1}{y}\\right|=\\left|\\frac{1}{x}-\\frac{2}{x}\\right|=\\left|\\frac{1}{x}\\right|\\geq 1=\\epsilon$$", "meta": {"post_id": 262325, "input_score": 22, "output_score": 42, "post_title": "Coming up with an example, a function that is continuous but not uniformly continuous"}}
{"input": "I am trying to understand the similarities and differences between the minimal polynomial and characteristic polynomial of Matrices.\n\nWhen are the minimal polynomial and characteristic polynomial the same\nWhen are they different\nWhat conditions (eigenvalues/eigenvectors/...) would imply 1 or 2\nPlease tell me anything else about these two polynomials that is essential in comparing them.", "output": "The minimal polynomial is quite literally the smallest (in the sense of divisibility) nonzero polynomial that the matrix satisfies. That is to say, if $A$ has minimal polynomial $m(t)$ then $m(A)=0$, and if $p(t)$ is a nonzero polynomial with $p(A)=0$ then $m(t)$ divides $p(t)$.\nThe characteristic polynomial, on the other hand, is defined algebraically. If $A$ is an $n \\times n$ matrix then its characteristic polynomial $\\chi(t)$ must have degree $n$. This is not true of the minimal polynomial.\nIt can be proved that if $\\lambda$ is an eigenvalue of $A$ then $m(\\lambda)=0$. This is reasonably clear: if $\\vec v \\ne 0$ is a $\\lambda$-eigenvector of $A$ then\n$$m(\\lambda) \\vec v = m(A) \\vec v = 0 \\vec v = 0$$\nand so $m(\\lambda)=0$. The first equality here uses linearity and the fact that $A^n\\vec v = \\lambda^n \\vec v$, which is an easy induction.\nIt can also be proved that $\\chi(A)=0$. In particular that $m(t)\\, |\\, \\chi(t)$.\nSo one example of when (1) occurs is when $A$ has $n$ distinct eigenvalues. If this is so then $m(t)$ has $n$ roots, so has degree $\\ge n$; but it has degree $\\le n$ because it divides $\\chi(t)$. Thus they must be equal (since they're both monic, have the same roots and the same degree, and one divides the other).\nA more complete characterisation of when (1) occurs (and when (2) occurs) can be gained by considering Jordan Normal Form; but I suspect that you've only just learnt about characteristic and minimal polynomials so I don't want to go into JNF.\nLet me know if there's anything else you'd like to know; I no doubt missed some things out.", "meta": {"post_id": 262334, "input_score": 39, "output_score": 50, "post_title": "Minimal polynomials and characteristic polynomials"}}
{"input": "Describe why norms are continuous function by mathematical symbols.", "output": "A function $f$ from a metric space to a metric space is continuous if for all $x$ in the domain, for all $\\varepsilon>0$, the exists $\\delta>0$ such that for all points $y$ in the domain, if the distance from $x$ to $y$ is less than $\\delta$, then the distance from $f(x)$ to $f(y)$ is less than $\\varepsilon$.\nIf $f$ is a norm, then it maps a vector space into $\\mathbb R$, and the distance from $x$ to $y$ is $f(x-y)$.\nIn this case it suffices to take $\\delta=\\varepsilon$, for the following reason.  Suppose the distance from $x$ to $y$ is less than $\\delta=\\varepsilon$.  Then $f(x-y)=f(y-x)<\\varepsilon$ (where the equality follows from the definition of \"norm\").  Now recall that norms satisfy a triangle inequality:\n$$\nf(x) \\le f(y) + f(x-y)\n$$\n$$\nf(y) \\le f(x) + f(y-x)\n$$\nSo\n$$\nf(y)-f(x) \\le f(y-x)<\\varepsilon\\text{ and }f(x)-f(y) \\le f(x-y)<\\varepsilon,\n$$\nso\n$$\n|f(x)-f(y)|<\\varepsilon,\n$$\ni.e.\n$$\n\\Big(\\text{distance from $f(x)$ to $f(y)$}\\Big) <\\varepsilon.\n$$", "meta": {"post_id": 265284, "input_score": 33, "output_score": 38, "post_title": "Why are norms continuous?"}}
{"input": "$ f(x) = 6x^7\\sin^2(x^{1000}) e^{x^2} $\nFind $ f^{(2013)}(0) $\nA math forum friend suggest me to use big O symbol, however have no idea what that is, so how does that helping?", "output": "Note that,\n$$ 6\\,x^{7} \\sin\\left(x^{1000}\\right)\\sin\\left(x^{1000}\\right)e^{x^2} $$\n$$ =  6\\,x^{7} \\left( x^{1000}-\\frac{x^{3000}}{3!}+\\dots \\right)\\left( x^{1000}-\\frac{x^{3000}}{3!}+\\dots \\right)\\left(1+\\frac{x^2}{1!}+\\frac{x^4}{4!}+\\dots\\right) $$\n$$ = 6x^7x^{2000}\\left( 1-\\frac{x^{2000}}{3!} +\\dots\\right)^2\\left(1+\\frac{x^2}{1!}+\\frac{x^4}{2!}+\\dots\\right) $$\n$$ = 6x^{2007}\\left(1+\\frac{x^2}{1!}+\\frac{x^4}{2!}+\\frac{x^6}{3!}+\\dots\\right)\\left( 1-\\frac{x^{2000}}{3!} +\\dots\\right)^2  $$\nNow, it is clear that the coefficient of $x^{2013}$ is $1$, which implies that \n$$ \\frac{f^{(2013)}(0)}{(2013)!} = 1 \\implies f^{(2013)}(0)=(2013)!. $$", "meta": {"post_id": 267846, "input_score": 28, "output_score": 53, "post_title": "please solve a 2013 th derivative question?"}}
{"input": "The Gelfand\u2013Naimark Theorem states that an arbitrary C*-algebra $ A $ is isometrically *-isomorphic to a C*-algebra of bounded operators on a Hilbert space. There is another version, which states that if $ X $ and $ Y $ are compact Hausdorff spaces, then they are homeomorphic iff $ C(X) $ and $ C(Y) $ are isomorphic as rings. Are these two related anyway?", "output": "The first result that you stated is commonly known as the Gelfand-Naimark-Segal Theorem. It is true for arbitrary C*-algebras, and its proof employs a technique known as the GNS-construction. This technique basically allows one to construct a Hilbert space $ \\mathcal{H} $ from a given C*-algebra $ \\mathcal{A} $ such that $ \\mathcal{A} $ can be isometrically embedded into $ B(\\mathcal{H}) $ as a C*-subalgebra.\nThe Gelfand-Naimark Theorem, on the other hand, states that every commutative C*-algebra $ \\mathcal{A} $, whether unital or not, is isometrically *-isomorphic to $ {C_{0}}(X) $ for some locally compact Hausdorff space $ X $. When $ X $ is compact, $ {C_{0}}(X) $ and $ C(X) $ become identical.\nNote: The assumption of commutativity is essential for stating the Gelfand-Naimark Theorem. This is because we cannot realize a non-commutative C*-algebra as the commutative C*-algebra $ {C_{0}}(X) $, for some locally compact Hausdorff space $ X $.\nWhat follows is a statement of the Gelfand-Naimark Theorem, with the utmost level of precision.\n\nGelfand-Naimark Theorem Let $ \\mathcal{A} $ be a commutative C*-algebra. If $ \\mathcal{A} $ is unital, then $ \\mathcal{A} $ is isometrically *-isomorphic to $ C(X) $ for some compact Hausdorff space $ X $. If $ \\mathcal{A} $ is non-unital, then $ \\mathcal{A} $ is isometrically *-isomorphic to $ {C_{0}}(X) $ for some non-compact, locally compact Hausdorff space $ X $.\n\nThis result is often first established for the case when $ \\mathcal{A} $ is unital. One basically tries to show that the compact Hausdorff space $ X $ can be taken to be the set $ \\Sigma $ of all non-zero characters on $ \\mathcal{A} $, where $ \\Sigma $ is equipped with a special topology. Here, a character on $ \\mathcal{A} $ means a linear functional $ \\phi: \\mathcal{A} \\to \\mathbb{C} $ satisfying $ \\phi(xy) = \\phi(x) \\phi(y) $ for all $ x,y \\in \\mathcal{A} $. A rough outline of the proof is given below.\n\nShow that every character has sup-norm $ \\leq 1 $. Hence, $ \\Sigma \\subseteq {\\overline{\\mathbb{B}}}(\\mathcal{A}^{*}) $, where $ {\\overline{\\mathbb{B}}}(\\mathcal{A}^{*}) $ denotes the closed unit ball of $ \\mathcal{A}^{*} $.\nEquip $ {\\overline{\\mathbb{B}}}(\\mathcal{A}^{*}) $ with the subspace topology inherited from $ (\\mathcal{A}^{*},\\text{wk}^{*}) $, where $ \\text{wk}^{*} $ denotes the weak*-topology. By the Banach-Alaoglu Theorem, $ {\\overline{\\mathbb{B}}}(\\mathcal{A}^{*}) $ then becomes a compact Hausdorff space.\nProve that $ \\Sigma $ is a weak*-closed subset of $ \\left( {\\overline{\\mathbb{B}}}(\\mathcal{A}^{*}),\\text{wk}^{*} \\right) $. Hence, $ \\Sigma $ becomes a compact Hausdorff space with the subspace topology inherited from $ \\left( {\\overline{\\mathbb{B}}}(\\mathcal{A}^{*}),\\text{wk}^{*} \\right) $.\nFor each $ a \\in \\mathcal{A} $, define $ \\hat{a}: \\Sigma \\to \\mathbb{C} $ by $ \\hat{a}(\\phi) \\stackrel{\\text{def}}{=} \\phi(a) $ for all $ \\phi \\in \\Sigma $. We call $ \\hat{a} $ the Gelfand-transform of $ a $.\nShow that $ \\hat{a} $ is a continuous function from $ (\\Sigma,\\text{wk}^{*}) $ to $ \\mathbb{C} $ for each $ a \\in \\mathcal{A} $. In other words, $ \\hat{a} \\in C((\\Sigma,\\text{wk}^{*})) $ for each $ a \\in \\mathcal{A} $.\nFinally, prove that $ a \\longmapsto \\hat{a} $ is an isometric *-isomorphism from $ \\mathcal{A} $ to $ C((\\Sigma,\\text{wk}^{*})) $.\n\n\nLet us now take a look at the following theorem, which the OP has asked about.\n\nIf $ X $ and $ Y $ are compact Hausdorff spaces, then $ X $ and $ Y $ are homeomorphic if and only if $ C(X) $ and $ C(Y) $ are isomorphic as C*-algebras (not only as rings).\n\nOne actually does not require the Gelfand-Naimark Theorem to prove this result. Let us see a demonstration.\nProof\n\nThe forward direction is trivial. Take a homeomorphism $ h: X \\to Y $, and define $ h^{*}: C(Y) \\to C(X) $ by $ {h^{*}}(f) \\stackrel{\\text{def}}{=} f \\circ h $ for all $ f \\in C(Y) $. Then $ h^{*} $ is an isometric *-isomorphism.\nThe other direction is non-trivial. Let $ \\Sigma_{X} $ and $ \\Sigma_{Y} $ denote the set of non-zero characters of $ C(X) $ and $ C(Y) $ respectively. As $ C(X) $ and $ C(Y) $ are isomorphic C*-algebras, it follows that $ \\Sigma_{X} \\cong_{\\text{homeo}} \\Sigma_{Y} $. We must now prove that $ X \\cong_{\\text{homeo}} \\Sigma_{X} $. For each $ x \\in X $, let $ \\delta_{x} $ denote the Dirac functional that sends $ f \\in C(X) $ to $ f(x) $. Next, define a mapping $ \\Delta: X \\to \\Sigma_{X} $ by $ \\Delta(x) \\stackrel{\\text{def}}{=} \\delta_{x} $ for all $ x \\in X $. Then $ \\Delta $ is a homeomorphism from $ X $ to $ (\\Delta[X],\\text{wk}^{*}) $ (this follows from the fact that $ X $ is a completely regular space). We will be done if we can show that $ \\Delta[X] = \\Sigma_{X} $. Let $ \\phi \\in \\Sigma_{X} $. As $ \\phi: C(X) \\to \\mathbb{C} $ is surjective (as it maps the constant function $ 1_{X} $ to $ 1 $), we see that $ C(X)/\\ker(\\phi) \\cong \\mathbb{C} $. According to a basic result in commutative ring theory, $ \\ker(\\phi) $ must then be a maximal ideal of $ C(X) $. As such,\n$$\n\\ker(\\phi) = \\{ f \\in C(X) ~|~ f(x_{0}) = 0 \\}\n$$\nfor some $ x_{0} \\in X $ (in fact, all maximal ideals of $ C(X) $ have this form; the compactness of $ X $ is essential). By the Riesz Representation Theorem, we can find a regular complex Borel measure $ \\mu $ on $ X $ such that $ \\phi(f) = \\displaystyle \\int_{X} f ~ d{\\mu} $ for all $ f \\in C(X) $. As $ \\phi $ annihilates all functions that are vanishing at $ x_{0} $, Urysohn's Lemma implies that $ \\text{supp}(\\mu) = \\{ x_{0} \\} $. Hence, $ \\phi = \\delta_{x_{0}} $, which yields $ \\Sigma_{X} \\subseteq \\Delta[X] $. We thus obtain $ \\Sigma_{X} = \\Delta[X] $, so $ X \\cong_{\\text{homeo}} \\Sigma_{X} $. Similarly, $ Y \\cong_{\\text{homeo}} \\Sigma_{Y} $. Therefore, $ X \\cong_{\\text{homeo}} Y $ because\n$$\nX \\cong_{\\text{homeo}} \\Sigma_{X} \\cong_{\\text{homeo}} \\Sigma_{Y} \\cong_{\\text{homeo}} Y.\n$$\n\n\nWe actually have the following general categorical result.\n\nLet $ \\textbf{CompHaus} $ denote the category of compact Hausdorff spaces, where the morphisms are proper continuous mappings. Let $ \\textbf{C*-Alg} $ denote the category of commutative unital C*-algebras, where the morphisms are unit-preserving *-homomorphisms. Then there is a contravariant functor $ \\mathcal{F} $ from $ \\textbf{CompHaus} $ to $ \\textbf{C*-Alg} $ such that\n(1) $ \\mathcal{F}(X) = C(X) $ for all $ X \\in \\textbf{CompHaus} $, and\n(2) $ \\mathcal{F}(h) = h^{*} $ for all proper continuous mappings $ h $. If $ h: X \\to Y $, then $ h^{*}: C(Y) \\to C(X) $, which highlights the contravariant nature of $ \\mathcal{F} $.\nFurthermore, $ \\mathcal{F} $ is a duality (i.e., contravariant equivalence) of categories.\n\nThe role of the Gelfand-Naimark Theorem in this result is to prove that $ \\mathcal{F} $ is an essentially surjective functor, i.e., every commutative C*-algebra can be realized as $ \\mathcal{F}(X) = C(X) $ for some $ X \\in \\textbf{CompHaus} $.", "meta": {"post_id": 268002, "input_score": 25, "output_score": 43, "post_title": "Gelfand-Naimark Theorem"}}
{"input": "I found in the Wolfram MathWorld page of the Axiom of the Empty Set that this is one of the Zermelo-Fraenkel Axioms, however on the page about these ZFC Axioms I read that it is an axiom that can be deduced from the Axiom of Subsets and the Axiom of Foundation (or Axiom of Regularity), so, the existence of the Empty Set is an axiom of ZFC or not?", "output": "In short, we do not need to adopt this as an axiom. But...\nIf there are sets at all, the axiom of subsets tells us that there is an empty set: If $x$ is a set, then $\\{y\\in x\\mid y\\ne y\\}$ is a set, and is empty, since there are no elements $y$ of $x$ for which $y\\ne y$. The axiom of extensionality then tells us that there is only one such empty set.\nSo, the issue is whether we can prove that there are any sets. The axiom of infinity tells us that there is a set (which is infinite, or inductive, or whatever formalization you use). But this seems like a terrible overkill to check that there are sets, to postulate that there are infinitely many.\nSome people prefer to have an axiom that states that there are sets. Of course, some people then just prefer to have an axiom that states that there is an empty set, so we at once have that there are sets, and avoid having to apply comprehension to check that the empty set exists. \nOther people adopt a formalization of first order logic in which we can prove that there are sets. More carefully, most formalizations of logic (certainly the one I prefer) prove as a theorem that the universe of discourse is nonempty. In the context of set theory, this means \"there are sets\". This is pure logic, before we get to the axioms of set theory. Under this approach, we do not need the axiom that states that there are sets, and the existence of the empty set can be established as explained above.\n(The logic proof that there are sets is not particularly illuminating or philosophically significant. Usually, one of the axioms of first order logic is that $\\forall x\\,(x=x)$. If $\\exists x\\,(x=x)$ --the formal statement corresponding to \"there are sets\"-- is false, then $\\forall x\\,(x\\ne x)$. Instantiating, we obtain $x\\ne x$, and instantiating the axiom  $\\forall x\\,(x=x)$ we obtain $x=x$, and one of these conclusions is the negation of the other, which is a contradiction. This is not particularly illuminating, because of course we choose our logical axioms and rules of instantiation so that this silly argument can go through, it is not a deep result, and probably we do not gain much insight from it.)\nIt turns out that yet some others prefer to allow the possibility that there are empty universes of discourse, so their formalization of first order logic is slightly different, and in this case, we have to adopt some axiom to conclude that there is at least one set.\nAt the end of the day, this is considered a minor matter, more an issue of personal taste than a mathematical question.", "meta": {"post_id": 278863, "input_score": 32, "output_score": 47, "post_title": "The existence of the empty set is an axiom of ZFC or not?"}}
{"input": "What is the difference between homotopy and homeomorphism? Let X and Y be two spaces, Supposed X and Y are homotopy equivalent and have the same dimension, can it be proved that they are homeomorphic? Otherwise, is there any counterexample? Moreover, what conditions should be added to homotopy to get homeomorphism?\nWe assume additionally both X and Y are orientable.", "output": "Let $X$ be the letter\n$$\\ \\ \\ \\ \\ \\mathsf{X}\\ \\ \\ \\ \\ $$\nand $Y$ be the letter\n$$\\ \\ \\ \\ \\ \\mathsf{Y}\\ \\ \\ \\ \\ $$\nThen $X$ and $Y$ are homotopy-equivalent, but they are not homeomorphic.\n\nSketch proof: let $f:X\\to Y$ map three of the prongs of the $\\mathsf{X}$ on to the $\\mathsf{Y}$ in the obvious way, and let it map the fourth prong to the point at the centre.  Let $g:Y\\to X$ map the $\\mathsf{Y}$ into those three prongs of the $\\mathsf{X}$.  Then $f$ and $g$ are both continuous, and $f$ is a surjection but is not injective, while $g$ is an injection but is not surjective.  Now the compositions $f\\circ g$ and $g\\circ f$ are both easily seen to be homotopic to the identities on $X$ and $Y$, so $X$ and $Y$ are homotopy-equivalent.  \nIn other words, observe that $\\mathsf Y$ is a deformation retract of $\\mathsf X$.  Alternatively, observe that $\\mathsf X$ and $\\mathsf Y$ both retract on to the point at the centre.\nOn the other hand, $X$ and $Y$ are not homeomorphic.  For example, removing the point at the centre of the $\\mathsf{X}$ yields a space with four connected components, while removing any point from the $\\mathsf{Y}$ yields at most three connected components.", "meta": {"post_id": 281339, "input_score": 61, "output_score": 85, "post_title": "What is the difference between homotopy and homeomorphism?"}}
{"input": "Let $G$ be a connected Lie group and  $U$ any neighbourhood of the identity element. How to prove that $U$ generates $G$.", "output": "By replacing $U$ with $U \\cap U^{-1}$ if necessary, assume that $U = U^{-1}$.\nConsider the set generated by $U$:\n$$S = \\{g_1 \\cdots g_n : g_1,\\cdots, g_n \\in U \\text{ for some $n$} \\}$$\nWe want to show that $S = G$ by showing that $S$ is nonempty, open, and closed. Connectedness of $G$ would then imply $S = G$. \nNon-emptiness is evident.\nFor openness, note that for any $g \\in S$, $gU \\subset S$.\nFor closedness, note that if $g \\notin S$, then $gU$ is disjoint with $S$. Otherwise if $gu \\in S$, we have $g = guu^{-1} \\in S$ as well.", "meta": {"post_id": 282442, "input_score": 23, "output_score": 35, "post_title": "a neighbourhood of identity $U$ generates $G$ where $G$ is a connected lie group"}}
{"input": "Let $f$ be a convex function on a convex domain $\\Omega$ and $g$ a convex non-decreasing function on $\\mathbb{R}$. prove that the composition of $g(f)$ is convex on $\\Omega$. Under what conditions is $g(f)$ strictly convex.\nMy attempt, since $f$ is convex, $$f([1-t]x_0 +ty_0)\\le [1-t]f(x_0) + tf(y_0)\\:,\\quad\n t \\in [0,1] \n\\,\\text{and} \\: x_0,y_0\\in \\Omega$$\n Since $g$ is convex $$g([1-s]x_1 +sy_1) \\le [1-s]g(x_1) + sg(y_1)\\:,\\quad s \\in [0,1]\\:and \\: x_1,y_1 \\in \\mathbb{R}$$\n So $$g([1-s]f([1-t]x_2 +ty_2) +sf([1-t]x_2 +ty_2)) \\\\\\le [1-s]g([1-t]f(x_2) + tf(y_2)) + sg([1-t]f(x_3) + tf(y_3))\\: for\\:x_2,y_2,x_3,y_3 \\in \\Omega.$$ Im not sure if this is always true.\nAny help would be appreciated. \nThanks", "output": "We want to prove that for $x, y \\in \\Omega$, $(g \\circ f)\\left(\\lambda x + (1 - \\lambda) y\\right) \\le \\lambda (g \\circ f)(x) + (1 - \\lambda)(g \\circ f)(y)$.\nWe have:\n\\begin{align}\n(g \\circ f)\\left(\\lambda x + (1 - \\lambda) y\\right) &= g\\left(f\\left(\\lambda x + (1 - \\lambda) y\\right)\\right) \\\\\n&\\le g\\left(\\lambda f(x) + (1 - \\lambda) f(y)\\right) & \\text{(} f \\text{ convex and } g \\text{ nondecreasing)} \\\\\n&\\le \\lambda g(f(x)) + (1 - \\lambda)g(f(y)) & \\text{(} g \\text{ convex)} \\\\\n&= \\lambda (g \\circ f)(x) + (1 - \\lambda)(g \\circ f)(y)\n\\end{align}", "meta": {"post_id": 287716, "input_score": 40, "output_score": 54, "post_title": "The composition of two convex functions is convex"}}
{"input": "One typically studies analysis in $\\mathbb{R}^n$ after studying analysis in $\\mathbb{R}$.  Why can't the same be said of $\\mathbb{C}$?", "output": "I would say that the main reason that several complex variables is rarely seen in the undergraduate curriculum (and even not that often in the graduate curriculum unless the department has some specialists in SCV) is that you can't get very far without lots of prerequisites.\nYou can for example start by proving Cauchy's integral formula for a polydisc, and from that Liouville's theorem and a few other well known results from one complex variable quickly follow.\nFrom Cauchy's integral forumla, it also follows that holomorphic functions of several variables admit power series expansions (but the domain of convergence is not usually a ball in $\\mathbb{C}^n$: compare $\\sum_{j,k} z^j w^k$, $\\sum_{k} (z+w)^k$ and $\\sum_{k} z^k w^k$ for a few examples of what might happen). From here you can go on and study logarithmically convex Reinhardt domains. \nNote, however, that the mere definition of a holomorphic function in several variables is a little problematic. You want to say that a function is holomorphic if is is holomorphic in each variable separately, but to show that this is equivalent to other plausible definitions (without assuming that the function is for example locally bounded or jointly continuous) is surprisingly difficult.\nYou may even get as far as showing a version of Hartogs' extension theorem: If $\\Omega$ is a domain in $\\mathbb{C^n}$ and $K$ is a compact subset such that $\\Omega \\setminus K$ is connected, every holomorphic function on $\\Omega\\setminus K$ extends to $\\Omega$. (Here $n > 1$, of course.)\nI think this is about how far you can get without bringing in tools from PDE, potential theory, algebra (sheaf theory), functional analysis, differential geometry, distribution theory and probably a few more fields.\nThe big highlight in a first course in several complex variables is usually to solve the Levi problem, i.e. to characterize the domains of existence for holomorphic functions. (Hartogs' extension theorem shows that some domains are unnatural to study, since all holomorphic functions extend to a bigger domain.) This is usually done with H\u00f6rmanders $L^2$-solution of the $\\bar\\partial$-equation. (Or via sheaf theory a la Oka.) While it's not strictly necessary to have a modern PDE course as a prerequisite, it's certainly valuable. At the very least you need to know some functional analysis (and preferably some potential theory as well), including some exposure to unbounded linear operators on Hilbert spaces to be able to understand the H\u00f6rmander solution. (For the sheaf theory solution, you need a healthy background in algebra instead.)\nSimilarly, you need some differential geometry (at least familiarity with differential forms and tangent bundles) to understand the more complicated integral formulas such as Bochner-Martnielli's formula and the geometric aspects of pseduoconvexity, which is central for a deeper understanding of SCV. In fact, the interplay between the complex geometry of the domain and the corresponding function theory is a reoccuring theme in SCV. Function theory in strictly pseudoconvex domains, for example, look rather different from function theory in weakly pseudoconvex domains. (Many finer points concerning weakly pseudoconvex domains are still open problems.)\nSumming up, to do a real meaningful course in SCV, you really need more background than what is reasonable to expect from an undergraduate. After all SCV is really a 20th century field of mathematics! The Levi problem for example wasn't solved until the early 50's (H\u00f6rmander's solution is as late as 1965).", "meta": {"post_id": 289466, "input_score": 68, "output_score": 160, "post_title": "Why isn't several complex variables as fundamental as multivariable calculus?"}}
{"input": "What is the Topology of point-wise convergence? It has been stated in lectures but I am unfamiliar with it.", "output": "Let $F$ be a family of functions from a set $X$ to a space $Y$. $F$ might, for instance, be the set of all functions from $X$ to $Y$, or it might be the set of all continuous functions from $X$ to $Y$, if $X$ is a topological space. Each $f:X\\to Y$ can be thought of as a point in the Cartesian product $Y^{|X|}$. To see this, for each $x\\in X$ let $Y_x$ be a copy of the space $Y$. Then a function $f:X\\to Y$ corresponds to the point in $\\prod_{x\\in X}Y_x$ whose $x$-th coordinate is $f(x)$, and of course $\\prod_{x\\in X}Y_x$ is just the product of $|X|$ copies of $Y$, i.e., $Y^{|X|}$.\nThe product $Y^{|X|}$ is a topological space with the product topology; $F\\subseteq Y^{|X|}$, so $F$ inherits a topology from the product topology on $Y^{|X|}$. This inherited topology is the topology of pointwise convergence on $F$.\nIt can easily be shown that a sequence $\\langle f_n:n\\in\\Bbb N\\rangle$ in $F$ converges to some $f\\in F$ in this topology if and only if for each $x\\in X$, $\\langle f_n(x):n\\in\\Bbb N\\rangle$ converges to $f(x)$ in $Y$. (More generally, a net $\\langle f_d:d\\in D\\rangle$ in $F$ converges to some $f\\in F$ if and only if for each $x\\in X$ the net $\\langle f_d(x):x\\in D\\rangle$ converges to $f(x)$ in $Y$.) This is the reason for the pointwise in the name.\nVery often $Y$ is $\\Bbb R^n$ or $\\Bbb C^n$ for some $n$, and $X$ is some topological space. The topological structure of $X$ has no bearing on the topology of pointwise convergence, though it may help to determine the set $F$ of functions under consideration (e.g., the continuous ones).", "meta": {"post_id": 293004, "input_score": 48, "output_score": 85, "post_title": "What is the Topology of point-wise convergence?"}}
{"input": "What is the difference between homotopy and isotopy at the intuitive level.Some diagrammatic explanation will be helpful for me.", "output": "Isotopies are much stricter! \nA homotopy is a continuous one-parameter family of continuous functions.\nAn isotopy is a continuous one-parameter family of homeomorphisms. \nYou can think of a homotopy between two spaces as a deformation that involves bending, shrinking and stretching, but doesn't have to be one-to-one or onto. For example, a punctured torus is homotopy equivalent to a wedge of two circles (a \"figure 8\"), which can be pictured by sticking your fingers into the puncture and stretching the torus back onto the meridian and longitude lines. \nBut this map is certainly not a homeomorphism -- even the dimension is wrong, not to mention that a wedge of two circles is not a manifold. \nAn isotopy is a deformation that involves only bending. It must be one-to-one and onto at every step. In this way, any two handlebodies of equal genus are isotopic.", "meta": {"post_id": 296170, "input_score": 23, "output_score": 41, "post_title": "Isotopy and Homotopy"}}
{"input": "What is immersion and submersion at the intuitive level. What can be visually done in each case?", "output": "First of all, note that if $f : M \\to N$ is a submersion, then $\\dim M \\geq \\dim N$, and if $f$ is an immersion, $\\dim M \\leq \\dim N$.\nThe Rank Theorem may provide some insight into these concepts. The following statement of the theorem is taken from Lee's Introduction to Smooth Manifolds (second edition); see Theorem $4.12$.\n\nSuppose $M$ and $N$ are smooth manifolds of dimensions $m$ and $n$, respectively, and $F : M \\to N$ is a smooth map with constant rank $r$. For each $p \\in M$ there exist smooth charts $(U, \\varphi)$ for $M$ centered at $p$ and $(V, \\psi)$ for $N$ centered at $F(p)$ such that $F(U) \\subseteq V$, in which $F$ has a coordinate representation of the form $$\\hat{F}(x^1, \\dots, x^r, x^{r+1}, \\dots, x^m) = (x^1, \\dots, x^r, 0, \\dots, 0).$$ In particular, if $F$ is a smooth submersion, this becomes $$\\hat{F}(x^1, \\dots, x^n, x^{n+1}, \\dots, x^m) = (x^1, \\dots, x^n),$$ and if $F$ is a smooth immersion, it is $$\\hat{F}(x^1, \\dots, x^m) = (x^1, \\dots, x^m, 0, \\dots, 0).$$\n\nSo a submersion locally looks like a projection $\\mathbb{R}^n\\times\\mathbb{R}^{m-n} \\to \\mathbb{R}^n$, while an immersion locally looks like an inclusion $\\mathbb{R}^m \\to \\mathbb{R}^m\\times\\mathbb{R}^{n-m}$.", "meta": {"post_id": 297988, "input_score": 51, "output_score": 40, "post_title": "Intuitive meaning of immersion and submersion"}}
{"input": "My friend and I were discussing this and we couldn't figure out how to prove it one way or another.\nThe only rational values I can figure out for $\\sin(x)$ (or $\\cos(x)$, etc...) come about when $x$ is some product of a fraction of $\\pi$.\nIs $\\sin(x) $ (or other trigonometric function) necessarily irrational if $ x $ is rational?\nEdit:\nExcluding the trivial solution of 0.", "output": "If $\\sin x$ is rational (or even just algebraic), then $\\cos x=\\pm \\sqrt{1-\\sin^2 x}$ is algebraic. Therefore $e^{ix}=\\cos x+i\\sin x$ is algebraic, so by the Lindemann-Weierstrass theorem, $x$ cannot have been nonzero algebraic -- in particular not nonzero rational.", "meta": {"post_id": 299124, "input_score": 40, "output_score": 41, "post_title": "Is sin(x) necessarily irrational where x is rational?"}}
{"input": "Are $(\\mathbb{R},+)$ and $(\\mathbb{C},+)$ isomorphic as additive groups?\nI know that there is a bijection between $\\mathbb{R}$  and $\\mathbb{C}$, and this question asks whether they are isomorphic as abelian groups, are they referring to the additive abelian group? If so is there any simple isomorphism I can find? I know nothing about Hamel basis. Thanks.", "output": "Assuming the axiom of choice, yes.\nObserve that both these abelian groups are actually $\\mathbb Q$-vector spaces, and they have the same dimension, so they must be isomorphic as vector spaces, and such isomorphism is also a group isomorphism. This is in fact a stronger requirement than just group isomorphism, but nevermind that.\nIt is consistent with the failure of the axiom of choice that these two are not isomorphic, though. So one cannot give an explicit isomorphism between them.", "meta": {"post_id": 302514, "input_score": 41, "output_score": 38, "post_title": "Are $(\\mathbb{R},+)$ and $(\\mathbb{C},+)$ isomorphic as additive groups?"}}
{"input": "Math people:\nIt is my understanding that set theorists/logicians compare cardinalities of sets using injections rather than surjections.  Wikipedia defines countable sets in terms of injections.  Cantor's diagonal proof that the real numbers are uncountable involves showing that there is no surjection from $\\mathbb{N}$ to $(0,1)$.  So do I need the Axiom of Choice to accept Cantor's Diagonal Proof?\nI browsed the Similar Questions and I could not find an answer.  I apologize if this is a duplicate.\nStEFAN (Stack Exchange FAN)", "output": "No. You don't need choice for this.\nFor two reasons:\n\nIf there is an injection from a non-empty set $A$ into $B$ then there is a surjection from $B$ onto $A$. This does not require the axiom of choice, although the inverse implication (that a surjection has an injective inverse) is in fact equivalent to the axiom of choice.\nTo add on this, $\\mathbb N$ is well-ordered without the axiom of choice, so if there is a surjection from $\\mathbb N$ onto a set $A$, then there is an injection from $A$ into $\\mathbb N$ as well.\n\nThe axiom of choice is used when the existence of something is to be shown. In the diagonal proof you assume that you are given a certain list, and you define from that list a new function which is not on the list. This process does not require the axiom of choice.", "meta": {"post_id": 304378, "input_score": 27, "output_score": 38, "post_title": "Do you need the Axiom of Choice to accept Cantor's Diagonal Proof?"}}
{"input": "What is the rule for computing $ \\text{E}[X^{2}] $, where $ \\text{E} $ is the expectation operator and $ X $ is a random variable?\nLet $ S $ be a sample space, and let $ p(x) $ denote the probability mass function of $ X $.\nIs\n$$\n\\text{E}[X^{2}] = \\sum_{x \\in S} x^{2} \\cdot p(x),\n$$\nor do I also need to square the $ x $ appearing in $ p(x) $?", "output": "In general, if $ (\\Omega,\\Sigma,P) $ is a probability space and $ X: (\\Omega,\\Sigma) \\to (\\mathbb{R},\\mathcal{B}(\\mathbb{R})) $ is a real-valued random variable, then\n$$\n\\text{E}[X^{2}] = \\int_{\\Omega} X^{2} ~ d{P}.\n$$\nAlthough this formula works for all cases, it is rarely used, especially when $ X $ is known to have certain nice properties.\nExamples:\n\nIf $ X $ is a discrete random variable (i.e., its cumulative distribution function (cdf) is a step-function) and $ p $ is its probability mass function (pmf), then we can use the formula\n$$\n\\text{E}[X^{2}] = \\sum_{x \\in \\text{Range}(X)} x^{2} \\cdot p(x).\n$$\nIf $ X $ is an absolutely continuous random variable (i.e., its cdf is an absolutely continuous function), then it possesses a probability density function (pdf) $ f $. We thus have the formula\n$$\n\\text{E}[X^{2}] = \\int_{\\mathbb{R}} x^{2} f(x) ~ d{\\mu(x)},\n$$\nwhere $ \\mu $ is the standard Borel measure on $ \\mathbb{R} $. Of course, if $ f $ is continuous, then we can simply compute the improper Riemann integral\n$$\n\\text{E}[X^{2}] = \\int_{- \\infty}^{\\infty} x^{2} f(x) ~ d{x}.\n$$", "meta": {"post_id": 306659, "input_score": 47, "output_score": 35, "post_title": "Computing the Expectation of the Square of a Random Variable: $ \\text{E}[X^{2}] $."}}
{"input": "Assume we have a sequence of rational numbers $a=(a_n)$. Assume we have a summation function $S: \\mathscr {L}^1 \\mapsto \\mathbb R, \\ \\ S(a)=\\sum a_n$ ($\\mathscr {L}^1$ is the sequence space whose sums of absolute values converges). Assume also that $S(a) \\in \\mathbb R \\setminus \\mathbb Q$. \nI would like to know if every such sequence $a$ has a subsequence $b$ (infinitely long) such that $S(b) \\in \\mathbb Q$.\nTake as an example $a_n = 1/n^2$. Then $S(a)=\\pi^2/6$. But $a$ has a subsequence $b=(b_n)=(1/(2^n)^2)$ (ie. all squares of powers of $2$). Then $S(b)=4/3$. Is this case with every such sequence?", "output": "No.  For example, take the sequence $a_n=2^{-2^n}$, $n=1$, $2$, $\\dots$.  An infinite subsequence $(a_{n_k})$ of $(a_n)$ will have sum \n$$S:=\\sum_k a_{n_k}=\\sum_k 2^{-2^{n_k}}.$$\nSo, the binary expansion of $S$ will have $1$s in positions $2^{n_1}$, $2^{n_2}$, $2^{n_3}$, $\\dots$, and $0$s everywhere else.  This is not a periodic sequence, so $S$ must be irrational.", "meta": {"post_id": 311695, "input_score": 23, "output_score": 36, "post_title": "Does every sequence of rationals, whose sum is irrational, have a subsequence whose sum is rational"}}
{"input": "How do you prove that the Sobolev space $H^s(\\mathbb{R}^n)$ is an algebra if $s>\\frac{n}{2}$, i.e. if $u,v$ are in $H^s(\\mathbb{R}^n)$, then so is $uv$? Actually I think we should also have $\\lVert uv\\rVert_s \\leq C \\lVert u\\rVert_s \\lVert v\\rVert_s$. Recall that $\\lVert f\\rVert_s=\\lVert(1+|\\eta|^2)^{s/2}\\,\\hat{f}(\\eta)\\rVert$, the norm on $H^s(\\mathbb{R}^n)$. This is an exercise from Taylor's book, Partial differential equations I.", "output": "Note that\n$$\n\\begin{split}\n(1+|\\xi|^2)^p\n&\\leq (1+2|\\xi-\\eta|^2+2|\\eta|^2)^p\\\\\n&\\leq 2^p(1+|\\xi-\\eta|^2+1+|\\eta|^2)^p\\\\\n&\\leq c(1+|\\xi-\\eta|^2)^p + c(1+|\\eta|^2)^p,\n\\end{split}\n$$\nfor $p>0$, where $c=\\max\\{2^{p},2^{2p-1}\\}$. \nPut $\\langle\\xi\\rangle=\\sqrt{1+|\\xi|^2}$.\nThen we have\n$$\n\\begin{split}\n\\langle\\xi\\rangle^s |\\widehat{uv}(\\xi)|\n&\\leq \\int \\langle\\xi\\rangle^s |\\hat{u}(\\xi-\\eta)\\hat{v}(\\eta)|\\,\\mathrm{d}\\eta\\\\\n&\\leq c\\int \\langle\\xi-\\eta\\rangle^s |\\hat{u}(\\xi-\\eta)\\hat{v}(\\eta)|\\,\\mathrm{d}\\eta\n+ c\\int \\langle\\eta\\rangle^s |\\hat{u}(\\xi-\\eta)\\hat{v}(\\eta)|\\,\\mathrm{d}\\eta\\\\\n&\\leq c|\\langle\\cdot\\rangle^s\\hat u|*|\\hat v| + c|\\hat u|*|\\langle\\cdot\\rangle^s\\hat v|,\n\\end{split}\n$$\nwhich, in light of Young's inequality, implies\n$$\n\\|uv\\|_{H^s} \\leq c\\|u\\|_{H^s} \\|\\hat v\\|_{L^1} + c\\|\\hat u\\|_{L^1}\\|v\\|_{H^s}.\n$$\nFinally, we note that $\\|\\hat u\\|_{L^1}\\leq C\\,\\|u\\|_{H^s}$ when $s>\\frac{n}2$.", "meta": {"post_id": 314820, "input_score": 38, "output_score": 39, "post_title": "Sobolev space $H^s(\\mathbb{R}^n)$ is an algebra with $2s>n$"}}
{"input": "How to justify the convergence and calculate the sum of the series:\n$$\\sum_{n=1}^{+\\infty}\\frac{1}{1^2+2^2+\\cdots+n^2}.$$", "output": "$$\\begin{array}{lcl}\n\\sum_{n=1}^\\infty \\frac{1}{1^2+2^2+\\cdots+n^2}&=& \\sum_{n=1}^\\infty\\frac{6}{n(n+1)(2n+1)} \\\\ &=& 6\\sum_{n=1}^\\infty \\frac{1}{2n+1} \\left( \\frac{1}{n}-\\frac{1}{n+1}\\right) \\\\ &=& 12\\sum_{n=1}^\\infty \\frac{1}{2n(2n+1)} -12\\sum_{n=1}^\\infty \\frac{1}{(2n+1)(2n+2)} \\\\ &=& 12\\sum_{n=1}^\\infty \\left[ \\frac{1}{2n}-\\frac{1}{2n+1} \\right] - 12\\sum_{n=1}^\\infty \\left[ \\frac{1}{2n+1}-\\frac{1}{2n+2} \\right]\\\\\n&=& 12(1-\\ln 2)- 12\\left(\\ln 2-\\frac{1}{2}\\right)\\\\ &=& 18-24\\ln 2\n\\end{array}\n$$", "meta": {"post_id": 317219, "input_score": 22, "output_score": 35, "post_title": "The series $\\sum_{n=1}^{+\\infty}\\frac{1}{1^2+2^2+\\cdots+n^2}.$"}}
{"input": "Is there any upper bound for an expression like:\n$$\\left( a_1 + a_2 + \\cdots + a_n\\right)^{1/2} ?$$\nI need it for $n=3$. I know Hardy's inequality but it is for exponent greater than 1. Is there anything for the square root?", "output": "Elementary proof from scratch: $$(\\sqrt{a_1}+\\sqrt{a_2})^2 = a_1+a_2+2\\sqrt{a_1a_2}\\ge a_1+a_2 $$ hence $$\\sqrt{a_1+a_2}\\le \\sqrt{a_1}+\\sqrt{a_2}$$\nFor general $n$, by induction: \n$$\\sqrt{(a_1+\\dots+a_{n-1})+a_n}\\le \\sqrt{a_1+\\dots+a_{n-1}}+\\sqrt{a_n} \\le \\sqrt{a_1}+\\dots+\\sqrt{a_n}$$\n\nMore generally, the function $f(x)=  x^p$ is subadditive for $0<p<1$, meaning $f(a+b)\\le f(a)+f(b)$. A fun way to prove this is \n$$\nf(a+b)-f(b)=\\int_b^{a+b} f'(x)\\,dx = \\int_0^{a} f'(x+b)\\,dx\\le \\int_0^{a} f'(x)\\,dx = f(a)\n$$ \nwhere the inequality holds because $f'$ is decreasing.", "meta": {"post_id": 318649, "input_score": 28, "output_score": 51, "post_title": "Upper bound for $( a_1 + a_2 + \\cdots + a_n)^{1/2}$"}}
{"input": "I've been taught that $1^\\infty$ is undetermined case. Why is it so? Isn't $1*1*1...=1$ whatever times you would multiply it? So if you take a limit, say $\\lim_{n\\to\\infty} 1^n$, doesn't it converge to 1? So why would the limit not exist?", "output": "It isn\u2019t: $\\lim_{n\\to\\infty}1^n=1$, exactly as you suggest. However, if $f$ and $g$ are functions such that $\\lim_{n\\to\\infty}f(n)=1$ and $\\lim_{n\\to\\infty}g(n)=\\infty$, it is not necessarily true that\n$$\\lim_{n\\to\\infty}f(n)^{g(n)}=1\\;.\\tag{1}$$\nFor example, $$\\lim_{n\\to\\infty}\\left(1+\\frac1n\\right)^n=e\\approx2.718281828459045\\;.$$\nMore generally,\n$$\\lim_{n\\to\\infty}\\left(1+\\frac1n\\right)^{an}=e^a\\;,$$\nand as $a$ ranges over all real numbers, $e^a$ ranges over all positive real numbers. Finally,\n$$\\lim_{n\\to\\infty}\\left(1+\\frac1n\\right)^{n^2}=\\infty\\;,$$\nand\n$$\\lim_{n\\to\\infty}\\left(1+\\frac1n\\right)^{\\sqrt n}=0\\;,$$\nso a limit of the form $(1)$ always has to be evaluated on its own merits; the limits of $f$ and $g$ don\u2019t by themselves determine its value.", "meta": {"post_id": 319764, "input_score": 32, "output_score": 42, "post_title": "1 to the power of infinity, why is it indeterminate?"}}
{"input": "One of fundamental inequalities on logarithm is:\n$$ 1 - \\frac1x \\leq \\log x \\leq x-1 \\quad\\text{for all $x > 0$},$$\nwhich you may prefer write in the form of\n$$ \\frac{x}{1+x} \\leq \\log{(1+x)} \\leq x \\quad\\text{for all $x > -1$}.$$\nThe upper bound is very intuitive -- it's easy to derive from Taylor series as follows:\n$$ \\log(1+x) = \\sum_{n=1}^\\infty (-1)^{n+1}\\frac{x^n}{n} \\leq (-1)^{1+1}\\frac{x^1}{1} = x.$$\nMy question is: \"what is the intuition behind the lower bound?\" I know how to prove the lower bound of $\\log (1+x)$ (maybe by checking the derivative of the function $f(x) = \\frac{x}{1+x}-\\log(1+x)$ and showing it's decreasing) but I'm curious how one can obtain this kind of lower bound. My ultimate goal is to come up with a new lower bound on some logarithm-related function, and I'd like to apply the intuition behind the standard logarithm lower-bound to my setting.", "output": "If you don't already know that $\\log(x)=\\int_1^x\\frac1t\\,dt$, one way to define the logarithm function is: $$\\text{$\\log(x)$ is the area under the curve $y=\\frac1t$ from $t=1$ to $t=x$.}$$ The picture below shows that this area is sandwiched between two rectangles, each of width $x-1$. The smaller rectangle has height $1/x$, while the larger one has height $1$. In other words, we have the following inequalities:\n$$(x-1)\\cdot \\frac1x\\le\\log(x)\\le(x-1)\\cdot 1$$", "meta": {"post_id": 324345, "input_score": 68, "output_score": 44, "post_title": "Intuition behind logarithm inequality: $1 - \\frac1x \\leq \\log x \\leq x-1$"}}
{"input": "If we are in a sequence space, then the $ l^{p} $-norm of the sequence $ \\mathbf{x} = (x_{i})_{i \\in \\mathbb{N}} $ is $ \\displaystyle \\left( \\sum_{i=1}^{\\infty} |x_{i}|^{p} \\right)^{1/p} $.\nThe $ l^{\\infty} $-norm of $ \\mathbf{x} $ is $ \\displaystyle \\sup_{i \\in \\mathbb{N}} |x_{i}| $.\nProve that the limit of the $ l^{p} $-norms is the $ l^{\\infty} $-norm.\nI saw an answer for $ L^{p} $-spaces, but I need one for $ l^{p} $-spaces. Besides, I didn\u2019t really understand the $ L^{p} $-answer either.\nThanks for your help!", "output": "Let me state the result properly:\n\nLet $x=(x_n)_{n \\in \\mathbb{N}} \\in \\ell^q$ for some $q \\geq 1$. Then $$\\|x\\|_{\\infty} = \\lim_{p \\to \\infty} \\|x\\|_p. \\tag{1}$$\n\nNote that $(1)$ fails, in general, not hold if $x=(x_n)_{n \\in \\mathbb{N}} \\notin \\ell^q$ for all $q \\geq 1$ (consider for instance $x_n := 1$ for all $n \\in \\mathbb{N}$.)\nProof of the result: Since $$|x_k| \\leq \\left(\\sum_{j=1}^{\\infty} |x_j|^p \\right)^{\\frac{1}{p}}=\\|x\\|_p$$ for all $k \\in \\mathbb{N}$, $p \\geq 1$, we have $\\|x\\|_{\\infty} \\leq \\|x\\|_p$. Thus, in particular $$\\|x\\|_{\\infty} \\leq \\liminf_{p \\to \\infty} \\|x\\|_p. \\tag{1}$$\nOn the other hand, we know that $$\\|x\\|_p = \\left( \\sum_{j=1}^{\\infty} |x_j|^{p-q} \\cdot |x_j|^q \\right)^{\\frac{1}{p}} \\leq \\|x\\|_{\\infty}^{\\frac{p-q}{p}} \\cdot \\left( \\sum_{j=1}^{\\infty} |x_j|^q \\right)^{\\frac{1}{p}} = \\|x\\|_{\\infty}^{1-\\frac{q}{p}} \\cdot \\|x\\|_q^{\\frac{q}{p}}$$ for all $q<p$ where we used $|x_j| \\leq \\|x\\|_{\\infty}$ for all $j \\in \\mathbb{N}$. Therefore, we arrive at\n$$ \\limsup_{p \\to \\infty} \\|x\\|_p \\leq \\limsup_{p \\to \\infty} \\left( \\|x\\|_{\\infty}^{1-\\frac{q}{p}} \\cdot \\|x\\|_q^{\\frac{q}{p}}\\right) = \\|x\\|_{\\infty} \\cdot 1. \\tag{2}$$\nHence, $$\\limsup_{p \\to \\infty} \\|x\\|_p \\leq \\|x\\|_{\\infty} \\leq \\liminf_{p \\to \\infty} \\|x\\|_p.$$ This shows that $\\lim_{p \\to \\infty} \\|x\\|_p$ exists and equals $\\|x\\|_{\\infty}$.", "meta": {"post_id": 326172, "input_score": 38, "output_score": 62, "post_title": "The $ l^{\\infty} $-norm is equal to the limit of the $ l^{p} $-norms."}}
{"input": "Please scroll down to the bold subheaded section called Exact questions if you are too bored to read through the whole thing. \nI am a physics undergrad, trying to carry out some research work on topological solitons. I have been trying to read a paper that uses K\u00e4hler Manifolds. My guide just expects me to learn the mathematical definitions, without understanding it(or he expects me to study complex manifolds by scratch by myself), within 3-4 days, with exams going on, but I find this highly discomforting. So, it would be great if someone could tell me what is a K\u00e4hler manifold, highlighting the essential features of the definition and what they do, and intuitive explanations behind each. Why are they mathematically important? Also, some reasons as to why they are used in Physics?\nThe definition on Wikipedia is very obscure, linking you to 7-8 pages, and you forget what you are actually looking for. I have read the following definition from Nakahara: \n\nA K\u00e4hler Manifold is an hermitian manifold, whose K\u00e4lher form is closed i.e. $d\\Omega=0$. \n\nAfter searching the internet, I know the following: \n\nA Hermitian manifold is a complex manifold equipped with a metric $g$,\n  such that $g_p(X,Y)=g_p(J_pX,J_p Y)$, where $p \\in M$ and $X,Y \\in\n T_pM$\n\nAgain the web tells me that, $J$ is a linear map between the tangent spaces at a point such that $J^2=-1$. Lastly, the K\u00e4hler form $\\Omega$ is a tensor field whose action is given by $\\Omega_p(X,Y)=(J_pX,Y)$. \nExact questions: \nThis is what I would really really want to understand. What is the meaning and motivation for $J^2=-1$? What is the intuitive meaning and motivation for the definition of the Hermitian manifold, and the K\u00e4hler form? Most importantly, what does the K\u00e4hler form is closed really mean?\nI am sorry for the long question, and would be delighted, even if I got a partial answer. Looking forward for the replies. \nI am not looking for exact arguments, but an intuitive overall picture. \nBackground: I understand definitions of real manifolds, tangent spaces, and a differential forms. I have no intuition about exterior derivatives. I have a fair understanding of what is a complex manifold, and a few examples of Riemann surfaces.", "output": "First, as I think you know, K\u00e4hler manifolds are just special cases of Hermitian manifolds (complex manifolds with a Hermitian metric).  There are a number of very different, but equivalent, ways to define K\u00e4hler manifolds.  Here are a couple that might help:\n\nOn a Riemannian manifold $X$, we can always choose 'Riemann normal coordinates' at any point $p \\in X$.  These are coordinates in which the metric takes its canonical form $g_{ab} = \\delta_{ab}$ at $p$, and all its first derivatives vanish at $p$.  On a general Hermitian manifold, it may not be possible to find holomorphic coordinates in which this is true. \nK\u00e4hler manifolds are exactly those manifolds on which we can always find a holomorphic change of coordinates which, at some given point, sets the metric to its canonical form, and its first derivatives to zero.\nAnother characterisation of K\u00e4hler manifolds is as Hermitian manifolds for which the Christoffel symbols of the Levi-Civita connection are pure.  In other words, $\\Gamma^i_{jk}$ and $\\Gamma^{\\bar i}_{\\bar j\\bar k}$ may be non-zero, but all 'mixed' symbols like $\\Gamma^{\\bar i}_{jk}$ vanish.  This means that (anti-)holomorphic vectors get parallel transported to (anti-)holomorphic vectors.\nEquivalent to the above is to say that $n$-dimensional K\u00e4hler manifolds are precisely $2n$-dimensional Riemannian manifolds with holonomy group contained in $U(n)$.\n\nAll of this can be found in Moroianu's \"Lectures on K\u00e4hler Geometry\", which is quite a nice concise book.\nThere is a lot more to be said about K\u00e4hler manifolds, but hopefully this gives you some intuition about them.", "meta": {"post_id": 329342, "input_score": 50, "output_score": 45, "post_title": "What exactly is a K\u00e4hler Manifold?"}}
{"input": "Given a random vector $\\mathbf x \\sim N(\\mathbf{\\bar x}, \\mathbf{C_x})$ with normal distribution. $\\mathbf{\\bar x}$ is the mean value vector and $\\mathbf{C_x}$ is the covariance matrix of $\\mathbf{x}$.\nAn affine transformation is applied to the $\\mathbf{x}$ vector to create a new random $\\mathbf{y}$ vector:\n$$\n\\mathbf{y} = \\mathbf{Ax} + \\mathbf{b}\n$$\nCan we find mean value $\\mathbf{\\bar y}$ and covariance matrix $\\mathbf{C_y}$ of this new vector $\\mathbf{y}$ in terms of already given parameters ($\\mathbf{\\bar x}$, $\\mathbf{C_x}$, $\\mathbf{A}$ and $\\mathbf{b}$)?\nCan you please show the steps. Once I learn the method, I will use it on several other distributions myself.", "output": "We find the mean of $\\mathbf{y}$ by using the fact that $\\mathbb{E}\\{\\}$ is a linear operator.\n$$\n\\mathbf{\\bar{y}} = \\mathbb{E}\\{\\mathbf{y}\\} = \\mathbb{E}\\{\\mathbf{A}\\mathbf{x}+\\mathbf{b}\\} = \\mathbf{A}\\mathbb{E}\\{\\mathbf{x}\\}+\\mathbf{b} = \\mathbf{A}\\mathbf{\\bar{x}}+\\mathbf{b}\n$$\nThen we find covariance of\n$$\n\\begin{array}{rcl}\n\\mathbf{C_y} & \\triangleq & \\mathbb{E}\\{(\\mathbf{y}-\\mathbf{\\bar{y}})(\\mathbf{y}-\\mathbf{\\bar{y}})^\\top\\} \\\\\n& = & \\mathbb{E} \\Big\\{ \\Big[ (\\mathbf{A}\\mathbf{x}+\\mathbf{b})-(\\mathbf{A}\\mathbf{\\bar{x}}+\\mathbf{b}) \\Big] \\Big[ (\\mathbf{A}\\mathbf{x}+\\mathbf{b})-(\\mathbf{A}\\mathbf{\\bar{x}}+\\mathbf{b}) \\Big] ^\\top \\Big\\} \\\\\n& = & \\mathbb{E} \\Big\\{ \\Big[ \\mathbf{A}(\\mathbf{x}-\\mathbf{\\bar{x}}) \\Big] \\Big[ \\mathbf{A}(\\mathbf{x}-\\mathbf{\\bar{x}}) \\Big] ^\\top \\Big\\} \\\\\n& = & \\mathbb{E} \\Big\\{ \\mathbf{A}(\\mathbf{x}-\\mathbf{\\bar{x}}) (\\mathbf{x}-\\mathbf{\\bar{x}})^\\top \\mathbf{A}^\\top \\Big\\} \\\\\n& = & \\mathbf{A} \\mathbb{E} \\Big\\{ (\\mathbf{x}-\\mathbf{\\bar{x}}) (\\mathbf{x}-\\mathbf{\\bar{x}})^\\top  \\Big\\} \\mathbf{A}^\\top \\\\\n& = & \\mathbf{A}\\mathbf{C_x}\\mathbf{A}^\\top\n\\end{array}\n$$\nThen, $\\mathbf{y}$ is defined as,\n$$\n\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{A}\\mathbf{\\bar{x}+\\mathbf{b}, \\mathbf{A}\\mathbf{C_x}\\mathbf{A}^\\top})\n$$\nThat is,\n$$\nf_\\mathbf{Y}(\\mathbf{y)}\n= {1 \\over \\sqrt{\\lvert2\\pi\\mathbf{A}\\mathbf{C_x}\\mathbf{A}^\\top\\rvert}}\n\\exp\\left(- {1 \\over 2} \\big[\\mathbf{y}-(\\mathbf{A}\\mathbf{\\bar{x}}+\\mathbf{b}) \\big]^\\top (\\mathbf{A}\\mathbf{C_x}\\mathbf{A}^\\top)^{-1} \\big[\\mathbf{y}-(\\mathbf{A}\\mathbf{\\bar{x}}+\\mathbf{b}) \\big] \\right)\n$$", "meta": {"post_id": 332441, "input_score": 49, "output_score": 60, "post_title": "Affine transformation applied to a multivariate Gaussian random variable - what is the mean vector and covariance matrix of the new variable?"}}
{"input": "I don't understand questions that involve a binomial expression where you have a fraction choose $k$ or a negative number choose $k$. I understand and am able to do it when there are no fractions and they are all positive. We learned the generalized formula but I get the wrong answer when the question involves fractions or negative numbers. \neg: $$2/3 \\choose 2$$\nor $$-4 \\choose 3$$\nI am definitely not understanding something here, please help.", "output": "You know that $$\\binom{x}k=\\frac{x^{\\underline k}}{k!}\\;,$$ where $x^{\\underline k}$ is the falling factorial: $x^{\\underline k}=x(x-1)(x-2)\\dots(x-k+1)$. Thus,\n$$\\binom{2/3}2=\\frac{(2/3)^{\\underline 2}}{2!}=\\frac{\\left(\\frac23\\right)\\left(\\frac23-1\\right)}2=\\frac{\\left(\\frac23\\right)\\left(-\\frac13\\right)}2=-\\frac19\\;,$$\nand\n$$\\binom{-4}3=\\frac{(-4)^{\\underline 3}}{3!}=\\frac{(-4)(-4-1)(-4-2)}6=-\\frac{4\\cdot5\\cdot6}6=-20\\;.$$\nWith specific small numbers you can always just do the arithmetic, as I\u2019ve done here. Some more general calculations are also possible without too much difficulty. For instance:\n$$\\begin{align*}\n\\binom{1/2}n&=\\frac{(1/2)^{\\underline n}}{n!}\\\\\n&=\\frac{\\left(\\frac12\\right)\\left(-\\frac12\\right)\\left(-\\frac32\\right)\\dots\\left(-\\frac{2n-3}2\\right)}{n!}\\\\\n&=(-1)^{n-1}\\frac{(2n-3)!!}{2^nn!}\\\\\n&=(-1)^{n-1}\\frac{2^{n-1}(n-1)!(2n-3)!!}{2^{2n-1}n!(n-1)!}\\\\\n&=(-1)^{n-1}\\frac{(2n-2)!!(2n-3)!!}{2^{2n-1}n!(n-1)!}\\\\\n&=\\frac{(-1)^{n-1}}{2^{2n-1}n}\\frac{(2n-2)!}{(n-1)!^2}\\\\\n&=\\frac{(-1)^{n-1}}{2^{2n-1}n}\\binom{2n-2}{n-1}\n\\end{align*}$$", "meta": {"post_id": 340124, "input_score": 17, "output_score": 34, "post_title": "Binomial coefficients $1/2\\choose k$"}}
{"input": "prove that$$\\sum_{n=0}^{\\infty}\\left(\\frac{(-1)^n}{2n+1}\\sum_{k=0}^{2n}\\frac{1}{2n+4k+3}\\right)=\\frac{3\\pi}{8}\\log(\\frac{1+\\sqrt5}{2})-\\frac{\\pi}{16}\\log5 $$\nThis problem, I think use \n$$\\sum_{k=0}^{2n}\\dfrac{1}{2n+4k+3}=H_{10n+3}-H_{2n+3}$$\nThank you everyone help", "output": "First of all let us introduce the function\n$$ F(x)=\\sum_{n=0}^{\\infty}\\frac{(-1)^n}{2n+1}\\sum_{k=0}^{2n}\\frac{x^{2n+4k+3}}{2n+4k+3} $$\nWe need to compute $F(1)$. Let us start by calculating\n$$ x^{-2}F'(x)=\\sum_{n=0}^{\\infty}\\frac{(-1)^n}{2n+1}\\sum_{k=0}^{2n}x^{2n+4k}=\n\\sum_{n=0}^{\\infty}\\frac{(-1)^n}{2n+1}\\frac{x^{2n}-x^{4+10n}}{1-x^4}=\\frac{\\arctan x-\\arctan x^5}{x(1-x^4)},$$\nwhere the first step is obtained by straightforward differentiation, the second equality follows by summation of finite geometric series, and the third one is obtained from the series representation $\\arctan x=\\sum_{n=0}^{\\infty}\\frac{(-1)^nx^{2n+1}}{2n+1}$. Since it is clear from the definition that $F(0)=0$, we find\n$$ F(1)=\\int_0^1\\frac{x\\left(\\arctan x-\\arctan x^5\\right) dx}{1-x^4}$$\nIntegrate by parts using that $\\int\\frac{xdx}{1-x^4}=\\frac14\\ln\\frac{1+x^2}{1-x^2}$ (and saying appropriate words about the limits). This gives\n\\begin{align*}\nF(1)=\\frac14\\int_0^1\\left(\\frac{1}{1+x^2}-\\frac{5x^4}{1+x^{10}}\\right)\\ln\\left(\\frac{1-x^2}{1+x^2}\\right)dx=\\qquad\\qquad\\qquad\\\\\n=\\frac14\\int_0^1\\frac{(1+x^2)(1-3x^2+x^4)}{x^8-x^6+x^4-x^2+1}\\ln\\left(\\frac{1-x^2}{1+x^2}\\right)dx.\\qquad\\qquad (\\mathrm{A})\n\\end{align*}\nNotice that the change of integration variable $x\\rightarrow 1/x$ and parity allow to rewrite (A) as \n$$F(1)=\\frac{1}{16}\\int_{-\\infty}^{\\infty}\\frac{(1+x^2)(1-3x^2+x^4)}{x^8-x^6+x^4-x^2+1}\\ln\\left|\\frac{1-x^2}{1+x^2}\\right|dx $$\nNext we write this in terms of complex contour integral:\n$$F(1)=\\frac{1}{16}\\mathrm{Re}\\left\\{\\int_C f(z)dz\\right\\}\\qquad\\qquad (\\mathrm{B})$$\nwith \n$$ f(z)=\\frac{(1+z^2)(1-3z^2+z^4)}{z^8-z^6+z^4-z^2+1}\\ln\\left(\\frac{1-z^2}{1+z^2}\\right) $$\nAt this point, some comments are necessary:\n\nthe function $f(z)$ has a number of singularities in the complex $z$-plane. First of all one has 8 simple poles $z=\\exp\\pm\\frac{ik\\pi}{10}$, $k=1,3,7,9$. One also has logarithmic branch points $z=\\pm1,\\pm i$. Accordingly, we introduce 4 branch cuts: $B_{1}=[1,\\infty)$, $B_{-1}=(-\\infty,-1]$, $B_i=[i,i\\infty)$, $B_{-i}=(-i\\infty,-i]$.\nthe contour of integration $C$ runs from $-\\infty$ to $+\\infty$ slightly above the real axis (we could put it slightly below and modify what follows). The logarithms in $f(z)$ will be defined on their main sheets for $z\\in(-1,1)$, then $f(z)$ is unambigously defined in the cut plane by analytic continuation.\nGoing slightly above the branch cuts $B_{\\pm1}$ produces (irrelevant) imaginary parts in the logarithms. This explains the need to take real part in (B).\n\nNow the idea is to pull the contour $C$ to $i\\infty$. The integral will then be given by the sum of residues at $z=\\exp\\frac{ik\\pi}{10}$, $k=1,3,7,9$, plus integral of the jump of $f(z)$ on the cut $B_i$. It is not difficult to understand that the latter is real (rational prefactor $\\frac{(1+z^2)(1-3z^2+z^4)}{z^8-z^6+z^4-z^2+1}$ is real on the cut, the logarithm jumps by $2\\pi i$ and the integral is along $B_i$\nso that the integration variable is pure imaginary). Therefore we are left with\n$$F(1)=\\frac{1}{16}\\left[2\\pi\\int_1^{\\infty}\\frac{(1-x^2)(1+3x^2+x^4)dx}{x^8+x^6+x^4+x^2+1}+\\mathrm{Re}\\left\\{2\\pi i \\sum_{k=1,3,7,9}\\mathrm{res}_{z=\\exp\\frac{ik\\pi}{10}}f(z)\\right\\}\\right]\\qquad(\\mathrm{C})$$\nIn the first integral we have a rational function, so it can be calculated by elementary means:\n$$\\int_1^{\\infty}\\frac{(1-x^2)(1+3x^2+x^4)dx}{x^8+x^6+x^4+x^2+1}=\\left[\\frac12\\ln\\frac{1+x+x^2+x^3+x^4}{1-x+x^2-x^3+x^4}\\right]_{x=1}^{x=\\infty}=-\\frac{\\ln5}{2}$$\nThe residues are also relatively easily computed:\n\\begin{align}\n\\mathrm{res}_{z=\\exp\\frac{i\\pi}{10}}f(z)=\\frac{i}{2}\\left(\\ln\\tan\\frac{\\pi}{10}-\\frac{i\\pi}{2}\\right),\\\\\n\\mathrm{res}_{z=\\exp\\frac{3i\\pi}{10}}f(z)=-\\frac{i}{2}\\left(\\ln\\tan\\frac{3\\pi}{10}-\\frac{i\\pi}{2}\\right),\\\\\n\\mathrm{res}_{z=\\exp\\frac{7i\\pi}{10}}f(z)=-\\frac{i}{2}\\left(\\ln\\tan\\frac{3\\pi}{10}+\\frac{i\\pi}{2}\\right),\\\\\n\\mathrm{res}_{z=\\exp\\frac{9i\\pi}{10}}f(z)=\\frac{i}{2}\\left(\\ln\\tan\\frac{\\pi}{10}+\\frac{i\\pi}{2}\\right).\n\\end{align}\nSubstituting this into (C), we finally obtain\n$$ F(1)=\\frac{\\pi}{8}\\left(\\ln\\frac{\\tan\\frac{3\\pi}{10}}{\\tan\\frac{\\pi}{10}}-\\frac{\\ln 5}{2}\\right) $$\nThe statement now follows from the easily verified identity $\\frac{\\tan\\frac{3\\pi}{10}}{\\tan\\frac{\\pi}{10}}=\\left(\\frac{1+\\sqrt{5}}{2}\\right)^3$. $\\blacksquare$", "meta": {"post_id": 341302, "input_score": 23, "output_score": 47, "post_title": "How can I find $\\sum\\limits_{n=0}^{\\infty}\\left(\\frac{(-1)^n}{2n+1}\\sum\\limits_{k=0}^{2n}\\frac{1}{2n+4k+3}\\right)$?"}}
{"input": "Cauchy's Hypothesis or Noll's theorem states that $\\vec{t}(\\vec{X},t;\\partial \\Omega) = \\vec{t}(\\vec{X},t;\\vec{N})$ where $\\vec{N}$ is the outward unity normal to the positively oriented surface $\\partial \\Omega$. This translates to words as the dependence of the surface interaction vector on the surface on which it acts is only through the normal $\\vec{N}$. My question is what is the significance of the semicolon (;)? How does it differ from the comma (,) used to separated the function's first two arguments?", "output": "A semicolon is used to separate variables from parameters. Quite often, the terms variables and parameters are used interchangeably, but with a semicolon the meaning is that we are defining a function of the parameters that returns a function of the variables.\nFor example, if I write $f(x1,x2,\\ldots;p1,p2,\\ldots)$ then I mean that by supplying the parameters $(p1, p2,\\ldots)$, I create a new function whose arguments are $(x1, x2,\\ldots)$.\nSo the general syntax is $\\operatorname{functionName}(\\mathrm{variables};\\mathrm{parameters})$.\nIn Noll's theorem it says that the function created by supplying $\\partial \\Omega$ is the same as that created by supplying $\\vec{N}$. That's rather a nice way of saying that the function created only depends on $\\vec{N}$.", "meta": {"post_id": 342268, "input_score": 35, "output_score": 75, "post_title": "What does the semicolon ; mean in a function definition"}}
{"input": "I want to compute the inverse Laplace transform of a function\n$$\n   F(z) = e^{-\\sqrt{z}}.\n$$\nThis problem seems very nontrivial to me. Here one can find the answer: the inverse Laplace transform of one variable function $e^{-\\sqrt{z}}$ is equal to\n$$\n   \\mathcal{L}^{-1}[e^{-\\sqrt{z}}](x) = \\frac{1}{2 \\sqrt{\\pi}} x^{-\\frac{3}{2}} \\exp \\left( -\\frac{1}{4x} \\right).\n$$\nBut what is the simpliest way to do it? Post's formula requires knowledge of all degree derivatives of $e^{-\\sqrt{z}}$ and I think that it isn't a good way. The classical inversion formula is of the form\n$$\n    \\mathcal{L}^{-1}[F(z)](x) = \\frac{1}{2 \\pi i}\\int\\limits_{\\sigma - i \\infty}^{\\sigma + i \\infty} F(z) e^{zx}\\,dz = \\frac{1}{2 \\pi i} \\int\\limits_{\\sigma - i \\infty}^{\\sigma + i \\infty} e^{-\\sqrt{z}+zx} \\, dz.\n$$\nTo compute it I make a substitution $p = \\sqrt{z}$. Then I'm looking for the image of the line $\\sigma + i \\mathbb{R}$. If I'm not mistaken it is the angle with vertice at $\\sqrt{\\sigma}$ and with rays $\\sqrt{\\sigma} + e^{i \\frac{\\pi}{4}} [0,\\infty)$ and $\\sqrt{\\sigma}+e^{-i\\frac{\\pi}{4}} [0,\\infty)$ (not exactly, these rays are curvilinear, but I think that this doesn't matter because of Cauchy formula). I will denote it $\\Lambda$. So \n$$\n   \\mathcal{L}^{-1}[e^{-\\sqrt{z}}] = \\frac{1}{\\pi i} \\int\\limits_{\\Lambda} e^{-p + p^2 x}p \\, dp.\n$$\nThen I should look for residues, but integrand doesn't have them in finite part of $\\mathop{\\mathrm{conv}} \\Lambda$. Please help me wiith it.", "output": "You can use a contour integration without that substitution as follows by deforming the Bromwich contour about the negative real axis and exploiting a branch cut of $\\sqrt{z}$ about that axis.  So, consider the integral\n$$\\oint_C dz \\: e^{-\\sqrt{z}} e^{z t}$$\nwhere $C$ is a keyhole contour about the negative real axis, as pictured below.\n\nWe will define $\\text{Arg}{z} \\in (-\\pi,\\pi]$, so the branch is the negative real axis.  There are $6$ pieces to this contour, $C_k$, $k \\in \\{1,2,3,4,5,6\\}$, as follows.\n$C_1$ is the contour along the line $z \\in [c-i R,c+i R]$ for some large value of $R$.\n$C_2$ is the contour along a circular arc of radius $R$ from the top of $C_1$ to just above the negative real axis.\n$C_3$ is the contour along a line just above the negative real axis between $[-R, -\\epsilon]$ for some small $\\epsilon$.\n$C_4$ is the contour along a circular arc of radius $\\epsilon$ about the origin.\n$C_5$ is the contour along a line just below the negative real axis between $[-\\epsilon,-R]$.\n$C_6$ is the contour along the circular arc of radius $R$ from just below the negative real axis to the bottom of $C_1$.\nWe will show that the integral along $C_2$,$C_4$, and $C_6$ vanish in the limits of $R \\rightarrow \\infty$ and $\\epsilon \\rightarrow 0$.\nOn $C_2$, the real part of the argument of the exponential is\n$$R t \\cos{\\theta} - \\sqrt{R} \\cos{\\frac{\\theta}{2}}$$\nwhere $\\theta \\in [\\pi/2,\\pi)$.  Clearly, $\\cos{\\theta} < 0$ and $\\cos{\\frac{\\theta}{2}} > 0$, so that the integrand exponentially decays as $R \\rightarrow \\infty$ and therefore the integral vanishes along $C_2$.\nOn $C_6$, we have the same thing, but now $\\theta \\in (-\\pi,-\\pi/2]$.  This means that, due to the evenness of cosine, the integrand exponentially decays again as $R \\rightarrow \\infty$ and therefore the integral also vanishes along $C_6$.\nOn $C_4$, the integral vanishes as $\\epsilon$ in the limit $\\epsilon \\rightarrow 0$.  Thus, we are left with the following by Cauchy's integral theorem (i.e., no poles inside $C$):\n$$\\left [ \\int_{C_1} + \\int_{C_3} + \\int_{C_5}\\right] dz \\: e^{-\\sqrt{z}} e^{z t} = 0$$\nOn $C_3$, we parametrize by $z=e^{i \\pi} x$ and the integral along $C_3$ becomes\n$$\\int_{C_3} dz \\: e^{-\\sqrt{z}} e^{z t} = e^{i \\pi} \\int_{\\infty}^0 dx \\: e^{-i \\sqrt{x}} e^{-x t}$$\nOn $C_5$, however, we parametrize by $z=e^{-i \\pi} x$ and the integral along $C_5$ becomes\n$$\\int_{C_5} dz \\: e^{-\\sqrt{z}} e^{z t} = e^{-i \\pi} \\int_0^{\\infty} dx \\: e^{i \\sqrt{x}} e^{-x t}$$\nWe may now write\n$$-\\frac{1}{i 2 \\pi} \\int_0^{\\infty} dx \\: e^{- x t} \\left ( e^{i \\sqrt{x}} - e^{-i \\sqrt{x}} \\right ) + \\frac{1}{i 2 \\pi} \\int_{c-i \\infty}^{c+i \\infty} ds \\: e^{-\\sqrt{s}} e^{s t} = 0$$\nTherefore, the ILT of $\\hat{f}(s) = e^{-\\sqrt{s}}$ is given by\n$$\\begin{align}\\frac{1}{i 2 \\pi} \\int_{c-i \\infty}^{c+i \\infty} ds \\: e^{-\\sqrt{s}} e^{s t} &= \\frac{1}{i 2 \\pi} \\int_0^{\\infty} dx \\: e^{- x t} \\left ( e^{i \\sqrt{x}} - e^{-i \\sqrt{x}} \\right )\\\\ &= \\frac{1}{\\pi} \\int_{-\\infty}^{\\infty} du\\: u \\,e^{-t u^2} \\sin{u}\\end{align}$$\nThe last step involved substituting $x=u^2$ and exploiting the evenness of the integrand.  This integral may be evaluated as follows:\n$$\\begin{align}\\frac{1}{\\pi} \\int_{-\\infty}^{\\infty} du\\: u \\,e^{-t u^2} \\sin{u} &= \\frac{1}{\\pi} \\Im{\\left [\\int_{-\\infty}^{\\infty} du\\:u\\, e^{-t u^2} e^{i u} \\right]}\\\\ &= \\frac{1}{\\pi} \\Im{\\left [\\int_{-\\infty}^{\\infty} du\\:u\\, e^{-t (u-i/(2 t))^2} e^{-1/(4 t)}\\right ]}\\\\ &= \\frac{1}{\\pi} e^{-1/(4 t)} \\Im{\\left [\\int_{-\\infty}^{\\infty} dv \\: \\left ( v + \\frac{i}{2 t} \\right ) e^{-t v^2} \\right]}\\\\ &= \\frac{1}{\\pi} e^{-1/(4 t)}  \\frac{1}{2 t} \\sqrt{\\frac{\\pi}{t}} \\end{align}$$\nTherefore the result is that\n$$\\mathcal{L}^{-1}[e^{-\\sqrt{z}}](t) = \\frac{1}{i 2 \\pi} \\int_{c-i \\infty}^{c+i \\infty} dz \\: e^{-\\sqrt{z}} e^{z t} = \\frac{1}{2 \\sqrt{\\pi}} t^{-3/2} e^{-\\frac{1}{4 t}}$$\nas was to be shown.", "meta": {"post_id": 347933, "input_score": 23, "output_score": 39, "post_title": "Compute the inverse Laplace transform of $e^{-\\sqrt{z}}$"}}
{"input": "If $A$ and $B$ are $n\\times n$ matrices such that $AB = BA$ (that is, $A$ and $B$ commute), show that \n\n$$ e^{A+B}=e^A e^B$$ \n\nNote that $A$ and $B$ do NOT have to be diagonalizable.", "output": "$$\\begin{align*}e^{A}e^{B} &= \\left(\\sum \\frac{A^{n}}{n!}\\right)\\left(\\sum\\frac{B^{n}}{n!}\\right)\\\\\n          &=\\sum^{\\infty}_{m=0}\\sum^{\\infty}_{n=0}\\frac{A^{m}B^{n}}{m!n!}\\\\\n          &=\\sum^{\\infty}_{l=0}\\sum^{l}_{m=0}\\frac{A^{m}B^{l-m}}{m!(l-m)!}\\\\\n          &=\\sum^{\\infty}_{l=0}\\frac{1}{l!}\\sum^{l}_{m=0}\\frac{l!}{m!(l-m)!}A^{m}B^{l-m}\\\\\n          &=\\sum^{\\infty}_{l=0}\\frac{(A+B)^{l}}{l!}\\\\\n          &= e^{A+B}\\end{align*}$$\nNote:A and B have to commute.\nAlso, I set l=m+n.  I did this because we want to use the binomial theorem to simplify this.", "meta": {"post_id": 356752, "input_score": 23, "output_score": 54, "post_title": "Show that $ e^{A+B}=e^A e^B$"}}
{"input": "What are the basic differences between $\\mathbb C$ and $\\mathbb R^2$?\nThe points in these two sets are written as ordered pairs, I mean the structure looks similar to me. So what is the reason to denote these two sets differently?", "output": "The answer to this question depends on what you mean by $\\mathbf{R}^2$. You can write $\\mathbf{R}\\times\\mathbf{R}$, but the \"$\\times$\" can have several different meanings depending on which category you are working in.\nAs sets:\nIf you view $\\mathbf{R}\\times\\mathbf{R}$ and $\\mathbf{C}$ as sets, this means you are ignoring any possible structure on these things besides their elements (so forget about multiplication, addition, anything.) From this perspective, $\\mathbf{R}\\times\\mathbf{R}$ and $\\mathbf{C}$ are the same object (in technical terms they are isomorphic in the category of sets) because there is a bijection\n$$(a,b)\\ \\leftrightarrow\\ a+bi.$$\nA key thing to note about this is that \"$\\times$\" refers to a direct product of sets, i.e. the Cartesian product. This will change as we add more structure.\nAs real vector spaces:\nYou can give each of $\\mathbf{R}\\times\\mathbf{R}$ and $\\mathbf{C}$ the structure of a real vector space, meaning you can add vectors and multiply by real numbers. Then from the theory of linear algebra, we know that $\\mathbf{R}\\times\\mathbf{R}$ has a basis of $\\{(1,0), (0,1)\\}$ and $\\mathbf{C}$ has a (real) basis of $\\{1, i\\}$. Since these real vector spaces both have dimension 2, they are isomorphic (in the linear algebra sense, i.e. in the category of $\\mathbf{R}$-modules). So from this perspective they are again the same object.\nNote here that $\\mathbf{R}\\times\\mathbf{R}$ can be interpreted as $\\mathbf{R}\\oplus \\mathbf{R}$ which may be more familiar to linear algebra students. The point is that now we are requiring more from the operation (it has to preserve addition of vectors now).\nAs rings:\nHere is where the difference comes in. We can think of $\\mathbf{R}$ and $\\mathbf{C}$ as rings, meaning we can add and multiply elements together according to some axioms. Then if you write $\\mathbf{R}\\times\\mathbf{R}$, you mean a direct product in the category of rings, so now multiplication in $\\mathbf{R}\\times\\mathbf{R}$ has to satisfy\n$$(a,b)\\cdot (c,d)=(ac,bd).$$\nBut in particular this means things like\n$$(1,0)\\cdot (0,1)=(0,0),$$\nwhich means it is possible for two nonzero things to have a product zero. In contrast, if $z_1z_2=0$ in $\\mathbf{C}$, then either $z_1=0$ or $z_2=0$. In this way, $\\mathbf{R}\\times\\mathbf{R}$ and $\\mathbf{C}$ have fundamentally different behavior as rings. Because of this, there is no isomorphism of rings between the two objects.\nAs fields:\nA field is a commutative ring with more structure (we can invert multiplication for nonzero things). It turns out that $\\mathbf{C}$ can be given the structure of a field because $z^{-1}$ exists for any nonzero $z\\in\\mathbf{C}$, but $\\mathbf{R}\\times\\mathbf{R}$ cannot be a field because equations like $(1,0)\\cdot (0,1)=(0,0)$ mess everything up (try to cancel something from the left side).\n\n\ntl;dr -- You have to specify what you mean by \"$\\times$\". $\\mathbf{C}$ and $\\mathbf{R}\\times\\mathbf{R}$ are exactly the same until you start saying you want to do things like multiply elements together.", "meta": {"post_id": 364044, "input_score": 41, "output_score": 73, "post_title": "Difference between $\\mathbb C$ and $\\mathbb R^2$"}}
{"input": "I need to calculate the following integral:\n$$\\int_0^\\infty \\left(\\left(2\\ S(x)-1\\right)^2+\\left(2\\ C(x)-1\\right)^2\\right)^2 x\\ \\mathrm dx,$$\nwhere \n$$S(x)=\\int_0^x\\sin\\frac{\\pi z^2}{2}\\mathrm dz,$$\n$$C(x)=\\int_0^x\\cos\\frac{\\pi z^2}{2}\\mathrm dz$$\nare the Fresnel integrals. \nNumerical integration gives an approximate result $0.31311841522422385...$ that is close to $\\frac{16\\log2-8}{\\pi^2}$, so it might be the answer.", "output": "Step 1. Reduction of the integral\n\nLet $I$ denote the integral in question. With the change of variable $v = \\frac{\\pi x^2}{2}$, we have\n$$ I = \\frac{1}{\\pi} \\int_{0}^{\\infty} \\left\\{ (1 - 2 C(x) )^{2} + (1 - 2S(x))^{2} \\right\\}^{2} \\, dv $$\nwhere $x = \\sqrt{2v / \\pi}$ is understood as a function of $v$. By noting that\n$$ 1-2 S(x) = \\sqrt{\\frac{2}{\\pi}} \\int_{v}^{\\infty} \\frac{\\sin u}{\\sqrt{u}} \\, du \\quad \\text{and} \\quad 1-2 C(x) = \\sqrt{\\frac{2}{\\pi}} \\int_{v}^{\\infty} \\frac{\\cos u}{\\sqrt{u}} \\, du, $$\nwe can write $I$ as\n$$ I = \\frac{4}{\\pi^{3}} \\int_{0}^{\\infty} \\left| A(v) \\right|^{4} \\, dv \\tag{1} $$\nwhere $A(v)$ denotes the function defined by \n$$ A(v) = \\int_{v}^{\\infty} \\frac{e^{iu}}{\\sqrt{u}} \\, du. $$\n\n\nStep 2. Simplification of $\\left| A(v) \\right|^2$.\n\nNow we want to simplify $\\left| A(v) \\right|^2$. To this end, we note that for $\\Re u > 0$,\n$$ \\frac{1}{\\sqrt{u}}\n= \\frac{1}{\\Gamma\\left(\\frac{1}{2}\\right)} \\frac{\\Gamma\\left(\\frac{1}{2}\\right)}{u^{1/2}}\n= \\frac{1}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\frac{e^{-ux}}{\\sqrt{x}} \\, dx\n= \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{\\infty} e^{-ux^{2}} \\, dx \\tag{2} $$\nUsing this identity,\n\\begin{align*}\nA(v)\n&= \\frac{2}{\\sqrt{\\pi}} \\int_{v}^{\\infty} e^{iu} \\int_{0}^{\\infty} e^{-u x^2} \\, dx du\n = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\int_{v}^{\\infty} e^{-(x^2-i)u} \\, du dx \\\\\n&= \\frac{2 e^{iv}}{\\sqrt{\\pi}} \\int_{0}^{\\infty} e^{-v x^2} \\int_{0}^{\\infty} e^{-(x^2-i)u} \\, du dx\n = \\frac{2 e^{iv}}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\frac{e^{-v x^2}}{x^2-i} \\, dx.\n\\end{align*}\nThus by the polar coordinate change $(x, y) \\mapsto (r, \\theta)$ followed by the substitutions $r^2 = s$ and $\\tan \\theta = t$, we obtain\n\\begin{align*}\n\\left| A(v) \\right|^2\n&= A(v) \\overline{A(v)}\n = \\frac{4}{\\pi} \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\frac{e^{-v (x^2+y^2)}}{(x^2-i)(y^2 + i)} \\, dxdy \\\\\n&= \\frac{4}{\\pi} \\int_{0}^{\\infty} \\int_{0}^{\\frac{\\pi}{2}}  \\frac{r e^{-v r^2}}{(r^2 \\cos^{2}\\theta-i)(r^2 \\sin^{2}\\theta + i)} \\, d\\theta dr \\\\\n&= \\frac{2}{\\pi} \\int_{0}^{\\infty} \\int_{0}^{\\frac{\\pi}{2}}  \\frac{e^{-v s}}{(s \\cos^{2}\\theta-i)(s \\sin^{2}\\theta + i)} \\, d\\theta ds \\\\\n&= \\frac{2}{\\pi} \\int_{0}^{\\infty} \\frac{e^{-v s}}{s} \\int_{0}^{\\frac{\\pi}{2}} \\left( \\frac{1}{s \\cos^{2}\\theta-i} + \\frac{1}{s \\sin^{2}\\theta + i} \\right) \\, d\\theta ds \\\\\n&= \\frac{2}{\\pi} \\int_{0}^{\\infty} \\frac{e^{-v s}}{s} \\int_{0}^{\\infty} \\left( \\frac{1}{s -i(t^2 + 1)} + \\frac{1}{s t^2 + i (t^2 + 1)} \\right) \\, dt ds.\n\\end{align*}\nEvaluation of the inner integral is easy, and we obtain\n\\begin{align*}\n\\left| A(v) \\right|^2\n&= 2 \\int_{0}^{\\infty} \\frac{e^{-v s}}{s} \\Re \\left( \\frac{i}{\\sqrt{1 + is}} \\right) \\, ds.\n\\end{align*}\nApplying $(2)$ again, we find that\n\\begin{align*}\n\\left| A(v) \\right|^2\n&= 2 \\int_{0}^{\\infty} \\frac{e^{-v s}}{s} \\Re \\left( \\frac{i}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\frac{e^{-(1+is)u}}{\\sqrt{u}} \\, du \\right) \\, ds \\\\\n&= \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\frac{e^{-v s}}{s} \\int_{0}^{\\infty} \\frac{e^{-u} \\sin (su)}{\\sqrt{u}} \\, du\\, ds \\\\\n&= \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\frac{e^{-u}}{\\sqrt{u}} \\int_{0}^{\\infty}  \\frac{\\sin (su)}{s} \\, e^{-v s} \\, ds\\, du \\\\\n&= \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\frac{e^{-u}}{\\sqrt{u}} \\arctan \\left( \\frac{u}{v} \\right) \\, du \\\\\n&= \\frac{4\\sqrt{v}}{\\sqrt{\\pi}} \\int_{0}^{\\infty} e^{-vx^{2}} \\arctan (x^2) \\, dx \\qquad (u = vx^2) \\tag{3}\n\\end{align*}\nHere, we exploited the identity\n$$ \\int_{0}^{\\infty} \\frac{\\sin x}{x} e^{-sx} \\, dx = \\arctan \\left(\\frac{1}{s}\\right), $$\nwhich can be proved by differentiating both sides with respect to $s$.\n\n\nStep 3. Evaluation of $I$.\n\nPlugging $(3)$ to $(1)$ and applying the polar coordinate change, $I$ reduces to\n\\begin{align*}\nI\n&= \\frac{64}{\\pi^{4}} \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\int_{0}^{\\infty} v e^{-v(x^{2}+y^{2})} \\arctan (x^2) \\arctan (x^2) \\, dx dy dv \\\\\n&= \\frac{64}{\\pi^{4}} \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\frac{\\arctan (x^2) \\arctan (x^2)}{(x^2 + y^2)^2} \\, dx dy \\\\\n&= \\frac{64}{\\pi^{4}} \\int_{0}^{\\frac{\\pi}{2}} \\int_{0}^{\\infty} \\frac{\\arctan (r^2 \\cos^2 \\theta) \\arctan (r^2 \\sin^2 \\theta)}{r^3} \\, dr d\\theta \\\\\n&= \\frac{32}{\\pi^{4}} \\int_{0}^{\\frac{\\pi}{2}} \\int_{0}^{\\infty} \\frac{\\arctan (s \\cos^2 \\theta) \\arctan (s \\sin^2 \\theta)}{s^2} \\, ds d\\theta. \\qquad (s = r^2) \\tag{4}\n\\end{align*}\nNow let us denote\n$$ J(u, v) = \\int_{0}^{\\infty} \\frac{\\arctan (us) \\arctan (vs)}{s^2} \\, ds. $$\nThen a simple calculation shows that\n$$ \\frac{\\partial^{2} J}{\\partial u \\partial v} J(u, v) = \\int_{0}^{\\infty} \\frac{ds}{(u^2 s^2 + 1)(v^2 s^2 + 1)} = \\frac{\\pi}{2(u+v)}. $$\nIndeed, both the contour integration method or the partial fraction decomposition method work here. Integrating, we have\n$$ J(u, v) = \\frac{\\pi}{2} \\left\\{ (u+v) \\log(u+v) - u \\log u - v \\log v \\right\\}. $$\nPlugging this to $(4)$, it follows that\n\\begin{align*}\nI\n&= -\\frac{64}{\\pi^{3}} \\int_{0}^{\\frac{\\pi}{2}} \\sin^2 \\theta \\log \\sin \\theta \\, d\\theta\n = -\\frac{16}{\\pi^{3}} \\frac{\\partial \\beta}{\\partial z}\\left( \\frac{3}{2}, \\frac{1}{2} \\right)\n\\end{align*}\nwhere $\\beta(z, w)$ is the beta function, satisfying the following beta function identity\n$$ \\beta (z, w) = 2 \\int_{0}^{\\infty} \\sin^{2z-1}\\theta \\cos^{2w-1} \\theta \\, d\\theta = \\frac{\\Gamma(z)\\Gamma(w)}{\\Gamma(z+w)}. $$\nTherefore we have\n\\begin{align*}\nI\n&= \\frac{16}{\\pi^{3}} \\frac{\\Gamma\\left(\\frac{3}{2}\\right)\\Gamma\\left(\\frac{1}{2}\\right)}{\\Gamma(2)} \\left\\{ \\psi_{0} (2) - \\psi_{0} \\left(\\tfrac{3}{2} \\right) \\right\\}\n = \\frac{8}{\\pi^2} \\int_{0}^{1} \\frac{\\sqrt{x} - x}{1 - x} \\, dx\n = \\frac{8 (2 \\log 2 - 1)}{\\pi^2},\n\\end{align*}\nwhere $\\psi_0 (z) = \\dfrac{\\Gamma'(z)}{\\Gamma(z)}$ is the digamma function, satisfying the following identity\n$$ \\psi_{0}(z+1) = -\\gamma + \\int_{0}^{1} \\frac{1 - x^{z}}{1 - x} \\, dx. $$", "meta": {"post_id": 390847, "input_score": 47, "output_score": 59, "post_title": "An integral involving Fresnel integrals $\\int_0^\\infty \\left(\\left(2\\ S(x)-1\\right)^2+\\left(2\\ C(x)-1\\right)^2\\right)^2 x\\ \\mathrm dx,$"}}
{"input": "I want to know if there is a way to simplify, or a closed form solution of $tr(\\Sigma^{-1})$ where $\\Sigma$ is a symmetric positive definite matrix.", "output": "Let $A$ be symmetric positive definite matrix hence $\\exists$ a diagonal matrix $D$ whose diagonal entries are nonzero and $A=P D P^{-1}$ so $A^{-1} = P D^{-1} P^{-1}$ and $Tr(A^{-1})= Tr(D^{-1})$.  Now $D$ being diagonal matrix with non zero diagonal entries $D^{-1}$ has diagonal entries reciprocal of the diagonal entries of $D$ so $Tr(D^{-1})$ is sum of the inverses of the diagonal entries of $D$.", "meta": {"post_id": 391128, "input_score": 29, "output_score": 42, "post_title": "Trace of an Inverse Matrix"}}
{"input": "Consider the following integral:\n$$\\mathcal{I}(\\mu,\\nu)=\\int_0^\\infty\\ln\\frac{J_\\mu(x)^2+Y_\\mu(x)^2}{J_\\nu(x)^2+Y_\\nu(x)^2}\\mathrm dx,$$\nwhere $J_\\mu(x)$ is the Bessel function of the first kind:\n$$J_\\mu(x)=\\sum _{n=0}^\\infty\\frac{(-1)^n}{\\Gamma(n+1)\\Gamma(n+\\mu+1)}\\left(\\frac x2\\right)^{2n+\\mu}$$\nand $Y_\\mu(x)$ is the Bessel function of the second kind:\n$$Y_\\mu(x)=\\frac{J_\\mu(x)\\cos(\\mu\\pi)-J_{-\\mu}(x)}{\\sin(\\mu\\pi)}.$$\nI was not able to rigorously establish a closed form for $\\mathcal{I}(\\mu,\\nu)$, but based on numerical integration I made a conjecture:\n$$\\forall\\mu,\\nu\\in\\mathbb{R},\\hspace{1cm}\\mathcal{I}(\\mu,\\nu)\\stackrel{?}{=}\\frac{\\pi}{2}(\\mu^2-\\nu^2).$$\nCould you please help me to find out if this conjecture is true?\n\nIf the conjecture is true, then taking the derivative with respect to $\\mu$ at $\\mu=1$ we get the following corollary:\n$$\\int_0^\\infty\\frac{J_0(x)J_1(x)+Y_0(x)Y_1(x)}{J_1(x)^2+Y_1(x)^2}x^{-1}\\mathrm dx\\stackrel{?}{=}\\frac{\\pi}{2}.$$\n\nAs pointed by O.L., the conjecture is equivalent to\n$$\\int_0^{\\infty}\\ln\\frac{\\pi\\,x\\,H^{(1)}_\\nu(x)H^{(2)}_\\nu(x)}{2}\\mathrm dx\\stackrel{?}{=}\\frac{\\pi\\,(4\\nu^2-1)}{8},$$\nwhere $H^{(1)}_\\nu(x)=J_\\nu(x)+i\\,Y_\\nu(x)$ and $H^{(2)}_\\nu(x)=J_\\nu(x)-i\\,Y_\\nu(x)$ are the Hankel functions of the first and second kind.", "output": "I will assume that $\\nu$ is real, as in the formulation\nof the question. Something similar may be true for\ncomplex $\\nu$ by a similar argument, but there would be an  extra complication (perhaps\nonly notational) of working with $\\nu$ and\nits conjugate $\\overline{\\nu}$ instead of just $\\nu$.\nThe Hankel functions $H^{(i)}_{\\nu}(z)$ are entire\nexcept for a branch cut along the negative real axis.\nLet\n$$A^{(1)}_{\\nu}(z) =  \\frac{H^{(1)}_{\\nu-1}(z)}{ H^{(1)}_{\\nu}(z)}, \\qquad\nA^{(2)}_{\\nu}(z) =  \\frac{H^{(2)}_{\\nu-1}(z)}{ H^{(2)}_{\\nu}(z)},$$\n$$ B^{(1)}_{\\nu}(z) =  \\frac{H^{(1)}_{\\nu+1}(z)}{ H^{(1)}_{\\nu}(z)}, \\qquad\nB^{(2)}_{\\nu}(z) =  \\frac{H^{(2)}_{\\nu+1}(z)}{ H^{(2)}_{\\nu}(z)}.$$\nWe introduce the following notation:\nfor a function $F(z)$ with a branch cut along the negative axis, we let $F(x^{+})$\nand $F(x^{-})$ denote the limit of $F(z)$ as $z \\rightarrow x$ from the region\n$\\mathrm{Im}(z) > 0$ and $\\mathrm{Im}(z) < 0$ respectively.\nThere are identities as follows:\n$$\\text{Eq. 1:} \\quad  A^{(1)}_{\\nu}(x^{+}) = \\overline{A^{(2)}_{\\nu}(x^{-})} = -A^{(2)}_{\\nu}(-x)\n= - \\overline{A^{(1)}_{\\nu}(-x)},$$\n$$\nA^{(2)}_{\\nu}(x^{+}) = \\overline{A^{(1)}_{\\nu}(x^{-})}.$$\nand the same equations hold for $B$. Note that\nit's not true that\n$A^{(2)}_{\\nu}(x^{+}) = -A^{(2)}_{\\nu}(-x)$, the lack of symmetry\nhere is related to the branch cut. This is an important point.  The behavior of  $A^{(1)}_{\\nu}(z)$ is bad in the region near $x^{-}$, and correspondingly\n $A^{(2)}_{\\nu}(z)$ is bad near $x^{+}$.\nThe Hankel function has nice  asymptotic expansions for large $z$.\nThe ratio of such functions at arguments\n$\\nu$ differing by integers is particularly nice, because the complex phase cancels.\nIn particular, one has the following:\n$$A^{(1)}_{\\nu}(z) \\sim \ni \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right)\n+ \\frac{(2 \\nu - 1)}{2} \\cdot \\frac{1}{z}  + O(z^{-3}),$$\n$$A^{(2)}_{\\nu}(z) \\sim \n-i \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right)\n+ \\frac{(2 \\nu - 1)}{2} \\cdot \\frac{1}{z}  + O(z^{-3}),$$\n$$  B^{(2)}_{\\nu}(z)  \\sim \ni \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right)\n+ \\frac{(2 \\nu + 1)}{2} \\cdot \\frac{1}{z}  + O(z^{-3}),$$\n$$  B^{(1)}_{\\nu}(z)  \\sim \n-i \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right)\n+ \\frac{(2 \\nu + 1)}{2} \\cdot \\frac{1}{z}  + O(z^{-3}).$$\nThis holds outside the bad regions mentioned above. In particular,\nit holds for $A^{(1)}_{\\nu}(z)$ and $B^{(1)}_{\\nu}(z)$ for $z$ with argument\nin $[-\\pi + \\epsilon,\\pi]$, and for $A^{(2)}_{\\nu}(z)$ and $B^{(2)}_{\\nu}(z)$ with argument\nin $[-\\pi,\\pi - \\epsilon]$.\nLet $C_R$ be the semi-circle\nwith centre $0$ and radius $R$ in the upper half plane, oriented anti-clockwise, and thought of as lying\nabove the branch cut in $(-\\infty,0])$. Note that this is contained within the range where the asymptotic holds for $A^{(1)}_{\\nu}(z)$, \nand hence\n$$\\lim_{R \\rightarrow \\infty} \\int_{C_R} z  \\left(\\frac{H^{(1)}_{\\nu-1}(z)}{ H^{(1)}_{\\nu}(z)} - i \\right) \n-  \\frac{(2 \\nu - 1)}{2} dz\n=   \\pi i \\cdot \\left( -i  \\cdot \\frac{(4 \\nu^2 - 1)}{8} \\right) = \n\\frac{\\pi (4 \\nu^2 - 1)}{8}  $$\nThe main term comes from the residue theorem\n(applied to a half circle, hence the $\\pi i$ rather than $2 \\pi i$ factor), and\nthe error term comes from the fact that the integral of $O(z^{-2})$ over a \nhalf-circle of radius\n$R$ and circumference $\\pi R$ is $O(R^{-1})$.\nWe now use the following fact: $H^{(1)}_{\\nu}(z)$ has no zeros in the upper  half plane.\nI say that it is a fact, but I couldn't find a reference (Edit: proof of this fact included at the end of this answer). I proved it rigorously by an explicit contour integral \ncomputation for various ranges of values of $\\nu$, however. (Certainly, by the asymptotic expansion which is valid in the entire upper half plane, it follows that any such zeros, if they exist, must be within some small radius, which one can eliminate by computing $(2 \\pi i)^{-1} \\oint d \\log(f)$.) \nBy the residue theorem (taking $C$ above to be the circle in the upper half plane), we get,\nfor any holomorphic integrand,\n$$0 = \\oint  = \\int_{C} + \\int^{R}_{-R},$$\nand hence\n$$ \\lim_{R \\rightarrow \\infty}  \\int^{R}_{-R} z  \\left(\\frac{H^{(1)}_{\\nu-1}(z)}{ H^{(1)}_{\\nu}(z)} - i \\right) \n-  \\frac{(2 \\nu - 1)}{2} dz\n = - \\frac{\\pi (4 \\nu^2 - 1)}{8} .$$\nNote that the integrand has order $O(1/z)$, so one really has to take the integrand\nfrom $-R$ to $R$ and then take the limit for this to make sense.\nOne may apply the same analysis to $H^{(2)}_{\\nu}$, except now the zero free region\nof $H^{(2)}_{\\nu}$\nis in the lower half plane --- this follows by symmetry from the\nidentity $H^{(1)}_{\\nu}(\\overline{z}) = \\overline{H^{(2)}_{\\nu}(z)}$, noting\nthat we are once again in the correct region as far as asymptotics goes.\n  Hence we deduce that\n$$\\lim_{R \\rightarrow \\infty} \\int^{-R}_{R} z  \\left(\\frac{H^{(2)}_{\\nu+1}(z)}{ H^{(2)}_{\\nu}(z)} - i \\right) \n-  \\frac{(2 \\nu + 1)}{2} dz = - \\frac{\\pi (4 \\nu^2 - 1)}{8} .$$\nNote that the direction of the integral has changed, for orientation reasons.\nWarning! There's also another difference between this and the previous integral. The first integral was above the branch cut and this integral is below\n the branch cut. However, in the first case,\nwe were integrating values of the form $A^{(1)}_{\\nu}(x^+)$, which was the good value (in the\nsense that it was related to three other values by symmetry in equation 1), and\nhere we are integrating $B^{(2)}_{\\nu}(x^{-})$, which also is related to three\nother values by the same equations.\nCorrecting\nthe order of the second integrand and then subtracting the results,  we get\n$$\\lim_{R \\rightarrow \\infty} \\int^{R}_{-R} 1 + z \n\\left(\\frac{H^{(1)}_{\\nu-1}(z)}{ H^{(1)}_{\\nu}(z)} - \\frac{H^{(2)}_{\\nu+1}(z)}{ H^{(2)}_{\\nu}(z)} \\right)\n dz = - 2 \\cdot  \\frac{\\pi (4 \\nu^2 - 1)}{8}.$$\nWe now make two observations: the integrand is now $O(z^{-2})$ for large $z$, and hence\nit exists as a definite integral. Moreover, the integrand is even. In light of the warning,\nwe should really specify that the\nintegrand for values $x \\in (-\\infty,0]$ is precisely:\n$$1 + x \\left(A^{(1)}_{\\nu}(x^{+}) - B^{(2)}_{\\nu}(x^{-})\\right).$$\n(One should check this is the correct function to make the integrand\neven.) We deduce that\n$$\n- \\int^{\\infty}_{0} 1 +  z \\left(\\frac{H^{(1)}_{\\nu-1}(z)}{ H^{(1)}_{\\nu}(z)} - \n\\frac{H^{(2)}_{\\nu+1}(z)}{ H^{(2)}_{\\nu}(z)} \\right) dz = \\frac{\\pi (4 \\nu^2 - 1)}{8}.$$\nLet \n$$I(\\nu) = \\int^{\\infty}_{0} \\log \\frac{\\pi x H^{(1)}_{\\nu}(x)  H^{(2)}_{\\nu}(x)}{2}  \\cdot dx.$$\nIntegrating by parts, and being a little bit careful about what happens at $0$, and\nexpressing the derivatives on Hankel functions in terms of Hankel  functions of other arguments, we find that\n$$I(\\nu) = -  \\int^{\\infty}_{0} 1 +  x \\left(\\frac{H^{(1)}_{\\nu-1}(x)}{ H^{(1)}_{\\nu}(x)} - \n\\frac{H^{(2)}_{\\nu+1}(x)}{ H^{(2)}_{\\nu}(x)} \\right) dx = \\frac{\\pi (4 \\nu^2 - 1)}{8}.$$\nAs noted in the comments, this was the identity to be proved.\nAlternatively, integration by parts also shows that\n$$\\frac{\\pi (\\mu^2 - \\nu^2)}{2} = I(\\mu) - I(\\nu)\n=  \\int^{\\infty}_{0} \\log \\frac{H^{(1)}_{\\mu}(x)  H^{(2)}_{\\mu}(x)}{H^{(1)}_{\\nu}(x)  H^{(2)}_{\\nu}(x)} \\cdot dx,$$\nand hence\n$$ \\int^{\\infty}_{0} \\log \\frac{J_{\\mu}(x)^2 + Y_{\\mu}(x)^2}{J_{\\nu}(x)^2 + Y_{\\nu}(x)^2} \\cdot\ndx = \\frac{\\pi (\\mu^2 - \\nu^2)}{2}.$$\nEdit: Proof that $H^{(1)}_{\\nu}(z)$ has no zeros in the upper half plane for real $\\nu > 0$.\nI realized that the proof can be completed in a similar manner. Let\n$$G^{(1)}_{\\nu}(z) =  d \\log H^{(1)}_{\\nu}(z)\n=\\frac{1}{2} \\left(A^{(1)}_{\\nu}(z) -  B^{(1)}_{\\nu}(z)\\right).$$\nThen we have an asymptotic formula, as before:\n$$A^{(1)}_{\\nu}(z) \\sim \ni \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right)\n+ \\frac{(2 \\nu - 1)}{2} \\cdot \\frac{1}{z}  + O(z^{-3}), $$\n$$  B^{(1)}_{\\nu}(z)  \\sim \n- i \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right)\n+ \\frac{(2 \\nu + 1)}{2} \\cdot \\frac{1}{z}  + O(z^{-3}), $$\nand thus\n$$G^{(1)}_{\\nu}(z) \\sim  i \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right) - \\frac{1}{2z} + O(z^{-3}),$$\nThis is valid for $z$ with argument in $[-\\pi + \\epsilon,\\pi]$, so in particular is valid in the upper half plane.\nIf $C_R$ denotes the circle in the upper half plane, we find that:\n$$\\lim_{R \\rightarrow \\infty} \\int_{C_R} (G^{(1)}_{\\nu}(z) - i) dz = - \\frac{\\pi i}{2},$$\nbecause it is $O(R^{-1})$ plus the contribution from the $1/(2z)$ term.\nLet $\\Omega_R$ denote the boundary of the  region bounded by by the interval $[-R,R]$ and $C_R$.\nThe function $H^{(1)}_{\\nu}(z)$ and thus $H^{(1)}_{\\nu}(z) e^{-iz}$ is holomorphic in $\\Omega_R$ \naway from $z = 0$. At zero (and $\\nu > 0$)  we have an asymptotic\n$$H^{(1)}_{\\nu}(z) \\sim  - i \\cdot \\frac{\\Gamma(\\nu)}{\\pi} \\left(\\frac{2}{z}\\right)^{\\nu}.$$\nIt follows that the number of zeros\nin the upper half plane is given, accounting for the singularity at $0$ (modified by a factor\nof two since this integral only accounts for half of the singularity) by\n$$\\frac{\\nu}{2} + \\lim_{R \\rightarrow \\infty} \\frac{1}{2 \\pi i} \\oint_{\\Omega_R} d  \\log (H^{(1)}_{\\nu}(z) e^{-iz}) dz$$\n$$= \\frac{\\nu}{2} + \\frac{1}{2 \\pi i} \\left( \\lim_{R \\rightarrow \\infty} \\int_{C_R}  (G^{(1)}_{\\nu}(z) - i) dz + \n\\lim_{R \\rightarrow \\infty} \\int_{-R}^{R} (G^{(1)}_{\\nu}(z) - i) dz\\right).$$\nWe computed the first integral above, hence it suffices to compute:\n$$\\frac{\\nu}{2}    -\\frac{1}{4} + \\frac{1}{2 \\pi i} \\lim_{R \\rightarrow \\infty} \\int_{-R}^{R} (G^{(1)}_{\\nu}(z) - i) dz.$$\nWe may write this as the integral\n$$\\frac{\\nu}{2}  -\\frac{1}{4} + \\frac{1}{2 \\pi i} \\int_{0}^{\\infty} \\left( G^{(1)}_{\\nu}(z) + G^{(1)}_{\\nu}(-z) - 2 i \\right) dz.$$\nThis expression evaluates to an integer, which is the number of zeros of $H^{(1)}_{\\nu}(z)$\nin the upper half plane.\nIn particular, evaluating this integral numerically for a random value (say $\\nu = \\pi$) shows\nthat, for this value, it is equal to zero. Now suppose that we vary $\\nu$. Since this integral\nevaluates to an integer, to complete the proof, it suffices to show that it varies continuously.\nFor the integral over $[R,\\infty)$ this is clear  for large enough $R$ from the asymptotic formula. For small values, it suffices to show that $H^{(1)}_{\\nu}(z)$ doesn't have any zeros on\nthe real line $[0,R]$, since otherwise the integrand is continuous in $\\nu$, and the continuity\nof the integral is clear. Now on the real axis, we have, by definition,\n$$H^{(1)}_{\\nu}(z) = J_{\\nu}(z) + i \\cdot Y_{\\nu}(z).$$\nSince $\\nu$ is real, it suffices to show that $J_{\\nu}(z)$ and $Y_{\\nu}(z)$\ndo not have any simultaneous zeros.  However, the zeros of these Bessel functions\nare well known to interlace (see Watson, A treatise on the theory of Bessel functions),\nand the result is established. Edit: Actually, there's a much easier proof that  $J_{\\nu}(z)$ and $Y_{\\nu}(z)$\ndo not have any common zeros --- they are linearly independent solutions to a second order ODE!", "meta": {"post_id": 395818, "input_score": 82, "output_score": 63, "post_title": "Closed form for $\\int_0^\\infty\\ln\\frac{J_\\mu(x)^2+Y_\\mu(x)^2}{J_\\nu(x)^2+Y_\\nu(x)^2}\\mathrm dx$"}}
{"input": "I have heard $\\varphi$ called the most irrational number. Numbers are either irrational or not though, one cannot be more \"irrational\" in the sense of a number that can not be represented as a ratio of integers. What is meant by most irrational? Define what we mean by saying one number is more irrational than another, and then prove that there is no $x$ such that $x$ is more irrational than $\\varphi$.\nNote: I have heard about defining irrationality by how well the number can be approximated by rational ones, but that would need to formalized.", "output": "How well can a number $\\alpha$ be approximated by rationals?\nTrivially, we can find infinitely many $\\frac pq$ with $|\\alpha -\\frac pq|<\\frac 1q$, so something better is needed to talk about a good approximation.\nFor example, if $d>1$, $c>0$ and there are infinitely many $\\frac pq$ with $|\\alpha-\\frac pq|<\\frac c{q^d}$, then we can say that $\\alpha$ can be approximated better than another number if it allows a higher $d$ than that other number. Or for equal values of $d$, if it allows a smaller $c$.\nIntriguingly, numbers that can be approximated exceptionally well by rationals are transcendental (and at the other end of the spectrum, rationals can be approximated exceptionally poorly  - if one ignores the exact approximation by the number itself). On the other hand, for every irrational $\\alpha$, there exists $c>0$ so that for infinitely many rationals $\\frac pq$ we have $|\\alpha-\\frac pq|<\\frac c{q^2}$. The infimum of allowed $c$ may differ among irrationals and it turns out that it depends on the continued fraction expansion of $\\alpha$.\nEspecially, terms $\\ge 2$ in the continued fraction correspond to better approximations than those for terms $=1$. Therefore, any number with infinitely many terms $\\ge 2$ allows a smaller $c$ than a number with only finitely many terms $\\ge2$ in the continued fraction. But if all but finitely many of the terms are $1$, then $\\alpha$ is simply a rational transform of $\\phi$, i.e. $\\alpha=a+b\\phi$ with $a\\in\\mathbb Q, b\\in\\mathbb Q^\\times$.", "meta": {"post_id": 395938, "input_score": 64, "output_score": 53, "post_title": "Why is $\\varphi$ called \"the most irrational number\"?"}}
{"input": "I'm not sure wether or not the following sum uniformly converge on $\\mathbb{R}$ :\n$$\\sum_{n=1}^{\\infty} \\frac{\\sin(n x) \\sin(n^2 x)}{n+x^2}$$\nCan someone help me with it? (I can't use Dirichlet' because of the areas where $x$ is close to $0$)", "output": "The series does converge uniformly. For the proof, put $S_n(x) = \\sum_{k = 0}^n \\sin{(kx)}\\sin{(k^2 x)}$ for $n\\geq 0$. The general idea is to use summation by parts to reduce ourselves to showing that $S_n(x)$ is bounded uniformly, and then to prove that by giving a closed form for $S_n(x)$.\nFirst, the summation by parts (I write $S_n$ in place of $S_n(x)$ for brevity):\n$$\n\\begin{align}\n\\sum_{n = 1}^N{\\sin{(nx)}\\sin{(n^2 x)}\\over n + x^2} & = \\sum_{n = 1}^N {1\\over n+x^2}(S_n - S_{n-1}) \\\\\n& = \\sum_{n = 1}^N {S_n\\over n+x^2} - \\sum_{n = 1}^N {S_{n-1}\\over n+x^2} \\\\\n& = \\sum_{n = 1}^N {S_n\\over n+x^2} - \\sum_{n = 0}^{N-1} {S_{n}\\over n+1+x^2} \\\\\n& = {S_N\\over N+x^2} - {S_0\\over 1 + x^2} + \\sum_{n = 1}^{N-1} S_n\\left({1\\over n+x^2} - {1\\over n+1+x^2}\\right) \\\\\n& = {S_N\\over N+x^2} + \\sum_{n = 1}^{N-1} {S_n\\over (n+x^2)(n+1+x^2)}.\n\\end{align}\n$$\nFrom here it is clear that it is enough to prove that $S_n = S_n(x)$ is uniformly bounded in $x$.\nTo do this, note that \n$$\n\\begin{align}\n2\\sin{(kx)}\\sin{(k^2 x)} &= \\cos{\\{(k^2 - k)x\\}} - \\cos{\\{(k^2 + k)x\\}} \\\\ \n& = \\cos{\\{k(k - 1)x\\}} - \\cos{\\{(k+1)k)x\\}},\n\\end{align}\n$$ \nand therefore that\n$$\n\\begin{align}\n2S_n(x) & = \\sum_{k = 0}^n 2\\sin{(kx)}\\sin{(k^2 x)} \\\\\n& = \\sum_{k = 0}^n\\left(\\cos{\\{k(k - 1)x\\}} - \\cos{\\{(k+1)k)x\\}}\\right)\n\\end{align}\n$$\nThe last sum telescopes, and we are left with\n$$\n2S_n(x) = 1 - \\cos{\\{n(n+1)x\\}},\n$$\nwhich is plainly bounded uniformly in $x$. So we're done.", "meta": {"post_id": 397097, "input_score": 43, "output_score": 67, "post_title": "Uniform convergence of $\\sum_{n=1}^{\\infty} \\frac{\\sin(n x) \\sin(n^2 x)}{n+x^2}$"}}
{"input": "We're all familair with this beautiful proof whether or not an irrational number to an irrational power can be rational. It goes something like this:\nTake $(\\sqrt{2})^{\\sqrt{2}}$\nIf it's rational, then you proved it, if it's irrational, take $((\\sqrt{2})^{\\sqrt{2}} ){^\\sqrt{2}} = 2$ and you've proved it.\nI'm wondering if you can raise $\\pi$ or $e$ to a certain non-trivial real power to make it rational? And if not, where is the proof that it can't be done? \np.s. - I almost left out the real part, but then I realized that $e^{i\\pi} = -1$.", "output": "Of course. Pick any positive rational $p$ and let $x=\\log_\\pi p$, then $\\pi^x=p$.", "meta": {"post_id": 399478, "input_score": 22, "output_score": 44, "post_title": "Can you raise $\\pi$ to a real power to make it rational?"}}
{"input": "Browsing the web I came across this:\n\nThe conjugacy class of an element $g\\in A_{n}$:\n\nsplits if the cycle decomposition of $g\\in A_{n}$ comprises cycles of distinct odd length. Note that the fixed points are here treated as cycles of length $1$, so it cannot have more than one fixed point; and\ndoes not split if the cycle decomposition of $g$ contains an even cycle or contains two cycles of the same length.\n\n\nAnybody with a proof?", "output": "Note the following: (1) The conjugacy class in $S_n$ of an element $\\sigma \\in A_n$ splits, iff there is no element $\\tau \\in S_n\\setminus A_n$ commuting with $\\sigma$. For if there is one, for each $\\tau' \\in S_n \\setminus A_n$ we have\n$$ \\tau'\\sigma{\\tau'}^{-1} = \\tau'\\sigma\\tau\\tau^{-1}\\tau'{}^{-1}\n    = (\\tau'\\tau)\\sigma(\\tau'\\tau)^{-1}\n$$\nand $\\tau\\tau' \\in A_n$. On the other hand, if $\\tau\\sigma\\tau^{-1}$ and $\\sigma$ with $\\tau \\in S_n\\setminus A_n$ are conjugate in $A_n$, then for some $\\tau' \\in A_n$, we have $\\tau\\sigma\\tau^{-1} = \\tau'\\sigma\\tau'^{-1}$, giving\n$$ \\tau'{}^{-1}\\tau \\sigma = \\sigma\\tau'{}^{-1}\\tau $$\nand hence $\\tau'{}^{-1}\\tau \\in S_n\\setminus A_n$ commutes with $\\sigma$.\nNow suppose, $\\sigma$ has a cycle $c_i$ of even length. A cycle of even length is an element of $S_n \\setminus A_n$, and as $\\sigma$ commutes with its cycles, we are done by the above. If $\\sigma$ has two cycles $(a_1\\ldots a_\\ell)$ and $(b_1 \\ldots b_\\ell)$ of the same odd length $\\ell$, then $(a_1b_1) \\ldots (a_\\ell b_\\ell)$ is a product of $\\ell$ transpositions (hence odd, so an element of $S_n \\setminus A_n$) commuting with $\\sigma$.\nNow suppose $\\sigma = c_1 \\cdots c_s$ is a product of odd cycles $c_i$ of distinct length $d_i$. Let $\\tau \\in S_n$ be a permutation commuting with $\\sigma$. Then $\\tau$ must fix each of the $c_i$, that is, $\\tau$ must be of the form $\\tau = c_1^{a_1} \\cdots c_s^{a_s}$ for some $a_i \\in \\mathbb Z$. But as the $c_i$ are even permutations (as cycles of odd length), we have $\\tau \\in A_n$. So no $\\tau \\in S_n \\setminus A_n$ commutes with $\\sigma$ and we are done.", "meta": {"post_id": 404656, "input_score": 22, "output_score": 40, "post_title": "Splitting of conjugacy class in alternating group"}}
{"input": "Lemma $1.92$ in Rotman's textbook (Advanced Modern Algebra, second edition) states,\nLet $G = \\langle a \\rangle$ be a cyclic group.\n(i) Every subgroup $S$ of $G$ is cyclic.\n(ii) If $|G|=n$, then $G$ has a unique subgroup of order $d$ for each divisor $d$ of $n$.\n\nI understand how every subgroup must be cyclic and that there must be a subgroup for each divisor of $d$. But how is that subgroup unique? I'm having trouble understanding this intuitively. For example, if we look at the cyclic subgroup $\\Bbb{7}$, we know that there are $6$ elements of order $7$. So we have six different cyclic subgroups of order $7$, right?\nThanks in advance.", "output": "Let $d$ be a divisor of $n=|G|$. Consider $H=\\{ x \\in G : x^d =1 \\}$. Then $H$ is a subgroup of $G$ and $H$ contains all elements of $G$ that have order $d$ (among others).\nIf $K$ is a subgroup of $G$ of order $d$, then $K$ is cyclic, generated by an element of order $d$. Hence, $K\\subseteq H$.\nOn the other hand, $x\\in H$ iff $x=g^k$ with $0\\le k < n$ and $g^{kd}=1$, where $g$ is a generator of $G$. Hence, $kd=nt$ and so $k=(n/d) t$. The restriction $0\\le k<n$ implies $0\\le t<d$, and so $H$ has exactly $d$ elements. Therefore, $K=H$.", "meta": {"post_id": 410389, "input_score": 40, "output_score": 50, "post_title": "Subgroups of a cyclic group and their order."}}
{"input": "Let $f:[0,1]\\to[0,1]$ be a continuous function such that its derivative $f'$ exists on $(0,1)$. Inspired by a similar question of myself here, I want to ask: \n\nIf $E\\subset[0,1]$ is a Borel set, is $f(E)$ still a Borel set? \n\n\nRemark: \n\nIt is known that $f$ maps sets of Lebesgue measure zero to sets of Lebesgue measure zero(for example, one may refer to this post), so it follows easily that $f$ maps Lebesgue measurable sets to Lebesgue measurable sets.\nEven if $f$ is assumed to be $C^1$, the answer is unclear to me.\nAs pointed by George Lowther in a comment in the same post, it is well known that for the natural projection $p:\\mathbb R^2\\to \\mathbb R$, $p(x,y)=x$, which is clearly real analytic, the image of  a Borel set of $\\mathbb R^2$ under $p$ may not be a Borel set in $\\mathbb R$. More details can be found here. It follows that for a high dimensional analog of the question, i.e. for $f:[0,1]^n\\to\\mathbb R^m$, $n\\ge 2$, even when $f$ is real analytic, the answer is negative. \n\n\nAny hint or suggestion is appreciated. Thanks in advance.", "output": "Even for $C^\\infty$ functions $f\\colon[0,1]\\to[0,1]$, it is possible that $f(E)$ is not Borel for some Borel sets $E$.\nStarting with the simpler case where $f$ is only required to be continuous, one way to construct a counterexample is to start with a space filling curve $g\\colon[0,1]\\to[0,1]^2$ whose image is the entire unit square. Then write $f=\\pi\\circ g$ where $\\pi\\colon[0,1]^2\\to[0,1]$ is the standard projection $\\pi(x,y)=x$. It is then a standard fact that there exist Borel sets $F\\subset[0,1]^2$ whose projection $\\pi(F)$ is not Borel. In fact, the projections of Borel sets are precisely the analytic sets, and there do exist analytic sets which are not Borel. Then, taking $E=g^{-1}(F)$, this is Borel set whose image under $f$ is the non-Borel set $\\pi(F)$.\nNow, moving on to the differentiable - and, even, smooth - case, the first thing to note is that the image of a differentiable function $g\\colon[0,1]\\to[0,1]^2$ must have zero Lebesgue measure, so cannot fill the unit square. However, this is not necessary. All we need is that the image of $g$ contains $C^2$ for some closed uncountable set $C\\subset[0,1]$ (e.g., a Cantor set). Such sets are always Borel isomorphic to the interval $[0,1]$ (as are all uncountable Polish spaces). So, we can procede as above and let $E=g^{-1}(F)$ where $F\\subset C^2$ is Borel such that $\\pi(F)$ is not Borel.\nSo, it remains to find a closed uncountable $C\\subset[0,1]$ and smooth $g\\colon[0,1]\\to[0,1]^2$ containing $C^2$ in its image. I will choose $C$ to be a 'thin Cantor set'. By this, I mean that $C$ is constructed in the same way as the fat Cantor set, except we remove larger and larger proportions of the set at each stage of the construction instead of smaller and smaller proportions. Choose a sequence $\\alpha_0,\\alpha_1,\\ldots\\in(0,1/2)$ with $\\alpha_n\\to0$ as $n\\to\\infty$. Then define a sequence of closed sets as follows. Let $C_0=[0,1]$. For each $n\\ge0$, once $C_n$ has been chosen (and is a finite union of disjoint closed intervals), we remove the centre $1-2\\alpha_n$ proportion of each interval in $C_n$ to obtain $C_{n+1}$. Then, $C=\\bigcap_nC_n$. Equivalently, $C$ is the set of points which can be written as\n$$\n(1-\\alpha_0)x_0+\\alpha_0(1-\\alpha_1)x_1+\\alpha_0\\alpha_1(1-\\alpha_2)x_2+\\alpha_0\\alpha_1\\alpha_2(1-\\alpha_3)x_3+\\cdots\n$$\nfor $x_0,x_1,\\ldots\\in\\lbrace0,1\\rbrace$. Note that if we had $\\alpha_0=\\alpha_1=\\cdots=1/3$ then this is the usual Cantor middle thirds set.\nNow, I'll describe the function $g$. This will be done in an inductive fashion using self-similarity. We can set $g=g_0$ where, for each $n\\ge0$, $g_n\\colon[0,1]\\to[0,1]^2$ contains scaled and maybe reflected copies of another such map $g_{n+1}$ in each of the four corner sections $[0,\\alpha_n]^2$, $[0,\\alpha_n]\\times[1-\\alpha_n,1]$, $[1-\\alpha_n,1]^2$, $[1-\\alpha_n,1]\\times[0,\\alpha_n]$ of the unit square, and follows straight lines in between these regions. This is as in the image below.\n\nLet $\\theta\\colon[0,1]\\to[0,1]$ be a smooth function with $\\theta(0)=0$, $\\theta(1)=1$ and such that its derivatives all vanish at the endpoints. For example, $\\theta(x)$ could be taken to be the integral of $\\exp(-1/(x(1-x)))$ appropriately scaled. Then, to be precise, we define $g=g_0$ where $g_n=(g_{n,1},g_{n,2})\\colon[0,1]\\to[0,1]^2$ are defined inductively by\n$$\ng_n(x)=\\begin{cases}\n\\alpha_n(g_{n+1,2}(7x),g_{n+1,1}(7x))&{\\rm if\\ } 0\\le x\\le 1/7,\\\\[.3em]\n(0,\\alpha_n+(1-2\\alpha_n)\\theta(7x-1))&{\\rm if\\ } 1/7 < x < 2/7,\\\\[.3em]\n\\alpha_n(g_{n+1,1}(7x-2),g_{n+1,2}(7x-2)-1)+(0,1)&{\\rm if\\ } 2/7\\le x\\le 3/7,\\\\[.3em]\n(\\alpha_n+(1-2\\alpha_n)\\theta(7x-3),1-\\alpha_n)&{\\rm if\\ } 3/7 < x < 4/7,\\\\[.3em]\n\\alpha_n(g_{n+1,1}(7x-4)-1,g_{n+1,2}(7x-4)-1)+(1,1)&{\\rm if\\ } 4/7 \\le x \\le 5/7,\\\\[.3em]\n(1,1-\\alpha_n-(1-2\\alpha_n)\\theta(7x-5))&{\\rm if\\ } 5/7 < x < 6/7,\\\\[.3em]\n\\alpha_n(-g_{n+1,2}(7x-6),1-g_{n+1,1}(7x-6))+(1,0)&{\\rm if\\ } 6/7 \\le x \\le 1.\n\\end{cases}\n$$\nThis does uniquely define $g$. Note that if $\\tilde g_n$ was any other sequence of functions to the unit square satisfying the same recurrence equations, then we have $\\lVert\\tilde g_n - g_n\\rVert=\\alpha_n\\lVert\\tilde g_{n+1} - g_{n+1}\\rVert$, where $\\lVert\\cdot\\rVert$ is the supremum norm. Also, as they both map into the unit square, $\\lVert \\tilde g_n-g_n\\rVert\\le\\sqrt{2}$. Putting these together,\n$$\n\\lVert\\tilde g_n-g_n\\rVert\\le\\alpha_n\\alpha_{n+1}\\cdots\\alpha_{n+r-1}\\sqrt{2}\n$$\nfor all $r\\ge0$. Letting $r$ go to infinity shows that $\\tilde g_n=g_n$.\nWe can also construct a sequence of smooth approximations to $g$. For each $N\\ge0$ define a map $g_N^{(N)}(x)=(\\theta(x),0)$. Then, inductively define $g_{N-1}^{(N)},g_{N-2}^{(N)},\\ldots,g_0^{(N)}$ using the recurrence equation above. Note that, at each step, $g^{(N)}_n$ is a smooth function with $g^{(N)}_n(0)=(0,0)$ and $g^{(N)}_n(1)=(1,0)$, and whose derivatives vanish to all orders at the end points. Next, using $D^rg$ to represent the r'th order derivatives of a function $g$, we have\n$$\n\\lVert D^rg_n^{(N)}\\rVert=7^r\\max\\left(\\alpha_n\\lVert D^rg_{n+1}^{(N)}\\rVert,(1-2\\alpha_n)\\lVert D^r\\theta\\rVert\\right)\n$$\nfor all $r\\ge1$. From this, we can see that $\\lVert D^rg_n^{(N)}\\rVert$ is bounded by $(7^r\\alpha_n)\\cdots(7^r\\alpha_{n+k-1})7^r\\lVert D^r\\theta\\rVert$ for some $k\\ge0$. As we chose $\\alpha_n$ to be tending to zero, this is bounded by some constant $L_r$ independently of $n,k$. Next, for any natural numbers $n < M\\le N$, the recurrence equation gives\n$$\n\\begin{align}\n\\lVert D^rg_n^{(M)}-D^rg_n^{(N)}\\rVert &= 7^r\\alpha_n\\lVert D^rg_{n+1}^{(M)}-D^rg_{n+1}^{(N)}\\rVert\\cr\n&=(7^r\\alpha_n)(7^r\\alpha_{n+1})\\cdots(7^r\\alpha_{M-1})\\lVert D^rg_M^{(M)}-D^rg_M^{(N)}\\rVert\\cr\n&\\le(7^r\\alpha_n)(7^r\\alpha_{n+1})\\cdots(7^r\\alpha_{M-1})2L_r\n\\end{align}\n$$\nAgain since we chose $\\alpha_n$ tending to zero, this shows that $\\lVert D^rg_n^{(M)}-D^rg_n^{(N)}\\rVert$ tends to zero as $M,N$ go to infinity. Therefore, the sequence of functions $g_n^{(N)}$ converges uniformly as $N\\to\\infty$, along with its derivatives to all orders. The limit, $g_n$, satisfies the defining recurrence equations above. So, $g$ is infinitely differentiable.\nNote, finally, that by choosing $x=x_0/7+x_1/7^2+x_2/7^3+\\cdots$ for appropriate $x_n\\in\\lbrace0,2,4,6\\rbrace$ we can find $x$ so that $g(x)$ is equal to any desired element of $C^2$.", "meta": {"post_id": 415759, "input_score": 29, "output_score": 36, "post_title": "Is the image of a Borel subset of $[0,1]$ under a differentiable map still a Borel set?"}}
{"input": "I'm currently learning about exact sequences in grad sch Algebra I course, but I really can't get the intuitive picture of the concept and why it is important at all.\nCan anyone explain them for me? Thanks in advance.", "output": "There are many good answers here.  I'd just like to add one example that made exact sequences 'click' for me, related to \"Euler's Formula\" relating the number of vertices ($V$), edges ($E$), and faces ($F$) of a simple non-self-intersecting polyhedron:\n$$ |F| - |E| + |V| = 2$$\nNow what does this have to do with exact sequences, you may well ask!  Well if you consider the free abelian groups generated by the set of faces, edges, and vertices separately, and create certain linear maps between them (see 'boundary maps' for simplicial homology), then you almost get an exact sequence:\n$$\n \\mathbb{Z}[F] \\to \\mathbb{Z}[E] \\to \\mathbb{Z}[V]\n$$\nIn fact, this sequence is exact at the middle term.  If we append two rank $1$ groups on the left and right (one with a generator the the whole solid $S$, and one generated by the symbol $e =$ '$\\emptyset$'), then you do get an exact sequence:\n$$\n  0 \\to \\mathbb{Z}[S] \\to \\mathbb{Z}[F] \\to \\mathbb{Z}[E] \\to \\mathbb{Z}[V]\n  \\to \\mathbb{Z}[e] \\to 0\n$$\nThen Euler's Formula is the statement just that the alternating sum of ranks is $0$ (because there is no torsion to keep track of).\n$$\n -1 + |F| - |E| + |V| - 1 = 0,\n$$\nor\n$$\n  |F| - |E| + |V| = 2.\n$$\nHope this helps!", "meta": {"post_id": 419329, "input_score": 97, "output_score": 44, "post_title": "Intuitive meaning of Exact Sequence"}}
{"input": "Let $R$ be a commutative ring and $I$ be an ideal of $R$.\nIs it true that $I$ is a principal ideal if and only if $I$ is a free $R$-module?", "output": "It is true that, if $R$ is a commutative ring and $I$ is a nonzero ideal of $R$, then $I$ is free iff $I$ is principal and generated by a non-zerodivisor.\nProof: Say $I$ is free.  By way of contradiction, suppose $I$ has an $R$-basis containing more than one element.  Let $e_1$ and $e_2$ be distinct elements of this basis.  Then we have $e_2e_1-e_1e_2=0$, which is impossible, since the $e_i$ are linearly independent (this is where we use commutativity).  Thus, $I$ has finite rank, and its rank is $1$.  Say $I$ is generated over $R$ by $e \\in R$; notice that $e$ must be a non-zerodivisor, or else $\\{e\\}$ would not be linearly independent.\nConversely, suppose $I$ is a principal ideal generated by a non-zerodivisor $e$.  Then the map $R \\to I$ given by $r \\mapsto re$ is an isomorphism of $R$-modules.", "meta": {"post_id": 423641, "input_score": 22, "output_score": 41, "post_title": "Principal ideal and free module"}}
{"input": "If $x$ is a positive rational number, but not an integer, then can $x^{x^{x^x}}$ be a rational number ?\n\nWe can prove that if $x$ is a positive rational number but not an integer, then $x^x$ can not be rational: \nDenote $x=\\dfrac{b}{a},(a,b)=1,x^x=\\dfrac{d}{c},(c,d)=1,$\n$$\\left(\\dfrac{b}{a}\\right)^\\dfrac{b}{a}=\\dfrac{d}{c} \\hspace{12pt}\\Rightarrow  \\hspace{12pt}\\left(\\dfrac{b}{a}\\right)^b=\\left(\\dfrac{d}{c}\\right)^a \\hspace{12pt}\\Rightarrow  \\hspace{12pt}b^b c^a=d^a a^b$$\nSince $(a,b)=1,(c,d)=1,$ we have $c^a\\mid a^b$ and $a^b\\mid c^a$, hence $a^b=c^a.$\nSince $(a,b)=1$, $a^b$ must be an $ab$-th power of an integer, assume that $a^b=t^{ab},$ then $a=t^a,$ where $t$ is a positive integer, this is impossible if $t>1,$ so we get $t=1,a=1$, hence $x$ is an integer.\nThen from Gelfond\u2013Schneider theorem , we can prove that if $x$ is a positive rational number but not an integer, then $x^{x^x}$ can not be rational. In fact, it can not be an algebraic number, because both $x$ and $x^x$ are algebraic numbers and $x^x$ is not a rational number.\n\nCan we prove that $x^{x^{x^x}}$ is irrational?\nCan $x^{x^{\\dots (n-th)^{\\dots x}}}~(n>1)$ be rational?", "output": "Let $x_1 = x$ and by induction $x_{n+1} = x^{x_n}$: so $x_1 = x$ is rational by hypothesis, $x_2 = x^x$ is algebraic irrational, $x_3 = x^{x^x}$ is transcendental by the Gelfond-Schneider theorem, and the question is to prove that $x_4, x_5,\\ldots$ are transcendental (or at least, irrational).\nI will assume Schanuel's conjecture and use it to prove by induction on $n\\geq 2$ that $x_3,x_4,\\ldots,x_n$ are algebraically independent (and, in particular, transcendental).  For $n=2$ there is nothing to prove: so let me assume the statement true for $n$ and prove it for $n+1$.\nSince $x_2$ is irrational, $x_1$ and $x_2$ are linearly independent over $\\mathbb{Q}$.  The induction hypothesis implies that $1,x_3,\\ldots,x_n$ are linearly independent over $\\mathbb{Q}^{\\mathrm{alg}}$ (the algebraics), so in particular $x_1,\\ldots,x_n$ are linearly independent over $\\mathbb{Q}$, and, of course, this implies that $x_1\\cdot\\log(x),\\ldots,x_n\\cdot\\log(x)$ are also such.\nNow Schanuel's conjecture then implies that among the $2n$ quantities $x_1 \\log(x),\\ldots,x_n \\log(x), x_2,\\ldots,x_{n+1}$ at least $n$ are algebraically independent.  Of course, we can remove $x_2$ from that list since it is algebraic, we can similarly replace both $x_1 \\log(x)$ and $x_2 \\log(x)$ by simply $\\log(x)$: so among $\\log(x),x_3,\\ldots,x_{n+1},x_3 \\log(x),\\ldots,x_n \\log(x)$ at least $n$ are algebraically independent.  But (for any $i$) this independent set cannot contain all three of $x_i$, $\\log(x)$ and $x_i\\log(x)$, and if it contains two of them then we can choose any two (namely, $x_i$ and $\\log(x)$): so that, in fact, the $n$ quantities $\\log(x),x_3,\\ldots,x_n,x_{n+1}$ are algebraically independent, which concludes the induction step (and moreover shows that $\\log(x)$ is also independent with the rest).", "meta": {"post_id": 430797, "input_score": 128, "output_score": 48, "post_title": "Can $x^{x^{x^x}}$ be a rational number?"}}
{"input": "Consider a function $f(t)$ with Fourier Transform $F(s)$. So $$F(s) = \\int_{-\\infty}^{\\infty} e^{-2 \\pi i s t} f(t) \\ dt$$\nWhat is the Fourier Transform of $f'(t)$? Call it $G(s)$.So $$G(s) = \\int_{-\\infty}^{\\infty} e^{-2 \\pi i s t} f'(t) \\ dt$$\nWould we consider $\\frac{d}{ds} F(s)$ and try and write $G(s)$ in terms of $F(s)$?", "output": "A simpler way, using the anti-transform:\n$$f(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} F(\\omega) \\, e^{i \\omega t} d\\omega$$\n$$f'(t) = \\frac{d}{dt}\\!\\left( \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} F(\\omega) \\, e^{i \\omega t} d\\omega \\right)= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty}   i \\omega \\, F(\\omega) \\, e^{i \\omega t} d\\omega$$\nHence the Fourier transform of $f'(t)$ is $ i \\omega \\, F(\\omega)$", "meta": {"post_id": 430858, "input_score": 75, "output_score": 132, "post_title": "Fourier Transform of Derivative"}}
{"input": "How to get the radius of an ellipse at a specific angle by knowing its semi-major and semi-minor axes?\nPlease take a look at this picture :", "output": "The polar form of the equation for an ellipse with \"horizontal\" semi-axis $a$ and \"vertical\" semi-axis $b$ is\n$$r = \\frac{ab}{\\sqrt{a^2\\sin^2\\theta+b^2\\cos^2\\theta}}$$\nHere, $\\theta$ represents the angle measured from the horizontal axis ($30.5^\\circ$ in your case), and $r$ is the distance from the center to the point in question (the radius you seek).", "meta": {"post_id": 432902, "input_score": 28, "output_score": 35, "post_title": "How to get the radius of an ellipse at a specific angle by knowing its semi-major and semi-minor axes?"}}
{"input": "Suppose I have: $w=2+3i$ and $x=1+2i$. What does it really mean to divide $w$ by $x$? \nEDIT: I am sorry that I did not tell my question precisely. (What you all told me turned out to be already known facts!) I was trying to ask the geometric intuition behind the division of complex numbers.", "output": "When you divide $a$ by $b$, you're asking \"What do I multiply $b$ by in order to get $a$?\".\nMultiplying two complex numbers multiplies their magnitudes and adds their phases:\n\nSo when you divide a complex $a$ by a complex $b$ you are asking: \"How much do I need to scale $b$ and rotate $b$ in order to get $a$? Please give me a complex number with a magnitude equivalent to how much I must scale and a phase equivalent to how much I must rotate.\".\nExample\nConsider $\\frac{1 + i}{1 - i}$. How much do we need to scale and rotate $1-i$ in order to make it the same as $1+i$?\nWell, when graphed on the complex plane you can see that $1-i$ has a 45 degree clockwise rotation and a magnitude of $\\sqrt{2}$. $1+i$, on the other hand, has a 45 degree counter-clockwise rotation and the same magnitude of $\\sqrt{2}$.\nSince the magnitudes are the same, we don't need any scaling. Our result's magnitude will be 1.\nTo rotate from 45 degrees clockwise to 45 degrees counter-clockwise, we must rotate 90 degrees counter-clockwise. Thus our result will have a phase of 90 degrees counter-clockwise (which is upwards along the imaginary Y axis).\nMove a distance of 1 up the imaginary Y axis and you get the answer... $\\frac{1 + i}{1 - i} = i$. We can confirm this by doing the multiplication: $(1-i) \\cdot i = i+1$.", "meta": {"post_id": 434275, "input_score": 40, "output_score": 58, "post_title": "What does it mean to divide a complex number by another complex number?"}}
{"input": "Related: Can a sum of square roots be an integer?\n\nExcept for the obvious cases $n=0,1$, are there any values of $n$ such that $\\sum_{k=1}^n\\sqrt k$ is an integer? How does one even approach such a problem? (This is not homework - just a problem I thought up.)", "output": "No, it is not an integer.\nLet $p_1=2<p_2<p_3<\\cdots <p_k$ be all the primes $\\le n$. It is known that $$K=\\mathbb{Q}(\\sqrt{p_1},\\sqrt{p_2},\\ldots,\\sqrt{p_k})$$ is a Galois extension of the rationals of degree $2^k$.\nThe Galois group $G$ is an elementary abelian 2-group. An automorphism $\\sigma\\in G$ is fully determined by a sequence of $k$ signs $s_i\\in\\{+1,-1\\}$, $\\sigma(\\sqrt{p_i})=s_i\\sqrt{p_i}$, $i=1,2,\\ldots,k$. \nSee this answer/question for a proof of the dimension of this field extension. There are then several ways of getting the Galois theoretic claims. For example we can view $K$ as a compositum of linearly disjoint quadratic Galois extensions, or we can use the basis given there to verify that all the above maps $\\sigma$ are distinct automorphisms.\nFor the sum $S_n=\\sum_{\\ell=1}^n\\sqrt{\\ell}\\in K$ to be a rational number, it has to be fixed by all the automorphisms in $G$. This is one of the basic ideas of Galois correspondence. But clearly $\\sigma(S_n)<S_n$ for all the non-identity automorphisms $\\sigma\\in G$, so this is not the case.", "meta": {"post_id": 442259, "input_score": 73, "output_score": 87, "post_title": "Is $\\sqrt1+\\sqrt2+\\dots+\\sqrt n$ ever an integer?"}}
{"input": "Ok, so the Chi-Squared distribution with $n$ degrees of freedom is the sum of the squares of $n$ independent Gaussian random variables.\nThe trouble is, my Gaussian random variables are not independent. They do however all have zero mean and the same variance. Supposing I have a covariance matrix---which again is not a diagonal matrix because they aren't independent, but all the elements along the diagonal are equal to each other because they have the same variance, and in fact the covariance matrix is a symmetric Toeplitz matrix (and I'm not saying that this is important to the solution if there is one, but if it's a necessary property to get anywhere, by all means use that fact)---is there some way to decompose this sum of squares of these Gaussian random variables into perhaps a sum of chi-squared random variables and possibly Gaussian random variables? In other words, I can't directly just square them all and add them together and call it a chi squared distribution because a chi squared distribution is a sum of independent Gaussian squares, and they aren't independent.\nI know how to find a linear transformation of the Gaussian random variables which are $n$ independent Gaussians, but that's no help because they aren't the things being squared, you see.", "output": "Lets assume you have $X=(X_1, \\dots, X_n)$ a random vector with multinormal distribution with expectation vector $\\mu$ and covariance matrix $\\Sigma$. We are interested in the quadratic form $Q(X)= X^T A X  = \\sum \\sum a_{ij} X_i X_j$. Define $Y = \\Sigma^{-1/2} X$ where we  are assuming $\\Sigma$ is invertible.  Write also $Z=(Y-\\Sigma^{-1/2} \\mu)$, which will have expectation zero and variance matrix the identity. \nNow\n$$\n  Q(X) = X^T A X= (Z+\\Sigma^{-1/2} \\mu)^T \\Sigma^{1/2}A\\Sigma^{1/2} (Z+\\Sigma^{-1/2} \\mu).\n$$\nUse the spectral theorem now and write $\\Sigma^{1/2}A \\Sigma^{1/2} = P^T \\Lambda P$\nwhere $P$ is an orthogonal matrix (so that$P P^T=P^T P=I$) and $\\Lambda$ is diagonal with positive diagonal elements $\\lambda_1, \\dotsc, \\lambda_n$.  Write $U = P Z$ so that $U$ is multivariate normal with identity covariance matrix and expectation zero. \nWe can compute\n$$\n Q(X) = (Z+\\Sigma^{-1/2} \\mu)^T \\Sigma^{1/2}A\\Sigma^{1/2} (Z+\\Sigma^{-1/2} \\mu)  \\\\\n      = (Z+\\Sigma^{-1/2} \\mu)^T  P^T \\Lambda P  (Z+\\Sigma^{-1/2} \\mu)  \\\\\n      = (PZ+P\\Sigma^{-1/2} \\mu)^T \\Lambda   (PZ+P\\Sigma^{-1/2} \\mu)   \\\\\n  = (U+b)^T  \\Lambda  (U+b)\n$$\nwhere now\n$b = P \\Sigma^{-1/2} \\mu $. (There was a small typo in above defs of $U$ and $b$, now corrected.)  So:\n$$\n  Q(X) = X^T A X = \\sum_{j=1}^n \\lambda_j (U_j+b_j)^2\n$$\nIn your case, $\\mu=0$ so $b=0$ so your quadratic form is a linear combination of independent chi-square variables, each with one degree of freedom.  In the general case, we will get a linear combination of independent non-central chisquare variables.\nIf you want to work numerically with that distribution, there is an CRAN package (that is, package for R) implementing it, called  CompQuadForm.\nIf you want (much) more detail, there is a book dedicated to the topic, Mathai & Provost: \"Quadratic forms in random variables\".", "meta": {"post_id": 442472, "input_score": 40, "output_score": 54, "post_title": "Sum of squares of dependent Gaussian random variables"}}
{"input": "I am studying an undergraduate text about math logic.\nThe proofs of the two G\u00f6del's incompleteness theorems are not completely formal: they are admittedly simpler that the real proofs.\nFor what I understood, I deduce the two theorems are valid for both classical and intuitionistic logic.\nIs my deduction correct?", "output": "The usual proof of G\u00f6del's First Incompleteness Theorem is entirely constructive. We don't have to rely on excluded middle, or have to rely on proving an existential quantification for which we can't produce a witness. For recall: the proof consists in (a) giving a recipe which takes a suitable specification of a sufficiently strong theory $T$ and constructs a certain sentence $G_T$ and then (b) showing $G_T$ is undecidable in that theory. The construction of $G_T$ is clever though simple when you see how, and involves no infinitary ideas. The proof of undecidability involves a pair of reductios, but both of the non-contentious type [like  \"Suppose $T \\vdash G_T$: then contradiction; so $T \\nvdash G_T$\"]. So overall the proof is intuitionistically acceptable. \nThe usual proof of the Second Incompleteness Theorem then consists, at heart, in showing that the proof of the First Theorem can be coded up in arithmetic. Again it's all constructive, and so is intuitionistically acceptable.", "meta": {"post_id": 448527, "input_score": 24, "output_score": 34, "post_title": "Are the G\u00f6del's incompleteness theorems valid for both classical and intuitionistic logic?"}}
{"input": "I want to know if there is some systematic way (using some combinatorial argument)\nto find the number of elements of conjugacy classes of $S_n$ for some given $n$.\nFor example, let's consider $S_5$. If the representative for the conjugacy class\nis an $m$-cycle then Dummit and Foote gives a formula on how to compute the number of elements in the conjugacy class. This is not a problem. But what about when the representative is not an $m$-cycle. As an example we can consider the conjugacy class that gives rise by the partition $2+3$ of $5$. A representative for the conjugacy class would be $(1 2)(3 4 5)$. How can I find the number of such elements?.\nQuestion?: \n\nDoes $ {5\\choose 2}\\cdot { 3 \\choose 3}\\cdot 2$ give me what I want?\n\nReasoning: For the first parenthesis I need to choose $2$ elements out of $5$ and for the second set of parenthesis I need to choose $3$ out of the remaining $3$ (noting that they can't be repeats). Finally we can permute these two parenthesis in two ways, thus giving me the above number. \nIs this reasoning correct?. If not how does one find the number of elements of such conjugacy classes.\nAs always, any help is greatly appreciated.", "output": "More systematically, you have $n!$ choices to arrange $1,\\ldots, n$. Place them into the parentheses pattern in this order to obtain an element of the conjugacy class.\nFor each $r$-cycle, you divide by $r$ as only the cyclic order within a cycle plays a role, not which element we start with. Then, if there are $n_r$ cycles of length $r$, divide by $n_r!$ as the order in which the cycles are listed is not important. Note that this must also be done for the cycles of length $1$!\nThis gives us\n$$\n\\frac{n!}{\\prod_{r}r^{n_r}n_r!} $$\nThus in $S_5$, there are $\\frac{5!}{2\\cdot 3}$ conjugates of $(1\\,2)(3\\,4\\,5)$. Similarly, there are $\\frac{7!}{2\\cdot2\\cdot 2!\\cdot 3!}$ conjugates of $(1\\,2)(3\\,4)$ in $S_7$.", "meta": {"post_id": 449041, "input_score": 27, "output_score": 50, "post_title": "counting the number of elements in a conjugacy class of $S_n$"}}
{"input": "I was reading Topology from Munkres and got confused by the definition of a subbasis. What is/are the difference between basis and subbasis in a topology?", "output": "Consider $S=\\{\\{0,1\\},\\{0,2\\}\\}$. What is the topological space $T(S)$ generated by $S?$ By definition, $S$ will then be a subbasis of $T(S)$.\nWell, we want to all requirements to hold true and find that $T(S) = \\{\\emptyset, \\{0\\}, \\{0,1\\}, \\{0,2\\}, \\{0,1,2\\}\\}$ (check this!). \nIs $S$ a basis? No, because you cannot write $\\{0\\}$ as a union of any elements in $S$.\nSo you see that subbasis and basis are two different notions, even for a very basic example. \nA subbasis can be thought of, and is actually defined to be, the \"smallest set that becomes my topological space if I complete it under the property of being a topological space, i.e. fulfiling the axioms of topological space\".\nThe two terms are related nevertheless. Every basis is a subbasis, and in one of the equivalent definitions of subbasis you will find that you already get a basis from your subbasis.", "meta": {"post_id": 449554, "input_score": 69, "output_score": 39, "post_title": "Difference between basis and subbasis in a topology?"}}
{"input": "Will product of path connected topological spaces be necessarily path connected? Why or why not?\nGive me some hints. Thank you.", "output": "This is very straightforward from applying the definitions involved:\nAssume $X=\\prod_{i\\in I}X_i$ with $X_i$ path-connected.\nLet $x=(x_i)_{i\\in I}$, $y=(y_i)_{i\\in I}$ be two points in $X$.\nBy assumption, thee exist continuous paths $\\gamma_i\\colon[0,1]\\to X_i$ with $\\gamma_i(0)=x_i$ and $\\gamma_i(1)=y_i$.\nBy definition of product, there exists a unique continuous $\\gamma\\colon[0,1]\\to X$ such that $\\pi_i\\circ\\gamma=\\gamma_i$ for all $i\\in I$ where $\\pi_i$ is the projection $X\\to X_i$. That makes $\\gamma$ a path from $x$ to $y$.", "meta": {"post_id": 454627, "input_score": 15, "output_score": 34, "post_title": "Product of path connected spaces is path connected"}}
{"input": "I have some trouble solving this problem:\nDoes there exist a holomorphic function $f$ on $\\mathbb C\\setminus \\{0\\}$ such that\n$$|f(z)|\\geq \\frac{1}{\\sqrt{|z|}}$$\nfor all $z\\in\\mathbb C \\setminus \\{0\\}$?\nI don't know where to start. My intuition is that you would get a problem with the singularity near $0$, but I am not sure how to prove it. Any help would be appreciated! Thanks!", "output": "Such an inequality guarantees that $f$ has no zeros, hence we can invert the inequality and find\n$$\\left\\lvert\\frac{1}{f(z)}\\right\\rvert \\leqslant \\sqrt{\\lvert z\\rvert},$$\nand that says that $1/f$ has a removable singularity (with value $0$) in the origin, thus, without loss of generality, $g = 1/f$ is an entire funtion that grows at most as fast as $\\sqrt{\\lvert z\\rvert}$. But such an estimate forces $g$ to be constant, hence $g \\equiv 0$. That on the other hand means $f \\equiv \\infty$, so $f$ was not an analytic function.\n\nAddendum: The Cauchy integral for the derivative of $g$ yields, for $\\lvert z\\rvert \\leqslant R$:\n$$\\begin{align}\n\\lvert g'(z)\\rvert &= \\left\\lvert \\frac{1}{2\\pi i} \\int_{\\lvert\\zeta\\rvert = 2R} \\frac{g(\\zeta)}{(\\zeta-z)^2}\\,d\\zeta\\right\\rvert\\\\\n&\\leqslant \\frac{1}{2\\pi} \\int_0^{2\\pi} \\frac{\\lvert g(2Re^{it})\\rvert}{\\lvert2Re^{it}-z\\rvert^2}2R\\,dt\\\\\n&\\leqslant \\frac{1}{2\\pi}\\int_0^{2\\pi}\\frac{2R\\sqrt{2R}}{R^2}\\,dt = \\frac{2\\sqrt{2}}{\\sqrt{R}},\n\\end{align}$$\nand letting $R \\to \\infty$ shows $g' \\equiv 0$.\nSimilarly, when $h$ is an entire function, and you have an estimate $\\lvert h(z)\\rvert \\leqslant c\\cdot \\lvert z\\rvert^\\alpha$ for all $\\lvert z\\rvert \\geqslant K$, the Cauchy integral for the $n$-th derivative of $h$ yields an estimate $\\lvert h^{(n)}(z)\\rvert \\leqslant C\\cdot R^{\\alpha - n}$ for all $\\lvert z\\rvert \\leqslant R/2$, where the constant $C$ depends on $n$ but not on $R$, and thus $h^{(n)} \\equiv 0$ if $n > \\alpha$, i.e. if you have such an estimate, then $h$ is a polynomial of degree $\\leqslant \\lfloor \\alpha\\rfloor$.", "meta": {"post_id": 463729, "input_score": 17, "output_score": 36, "post_title": "Does there exist an holomorphic function such that $|f(z)|\\geq \\frac{1}{\\sqrt{|z|}}$?"}}
{"input": "Let $\\mathcal{L}$ denote the $\\sigma$-algebra of Lebesgue measurable sets on $\\mathbb{R}$.  Then, if memory serves, there is an example (and of course, if there is one, there are many) of a continuous function $f:\\mathbb{R}\\rightarrow \\mathbb{R}$ that is not measurable in the sense that $f:(\\mathbb{R},\\mathcal{L})\\rightarrow (\\mathbb{R},\\mathcal{L})$ is measurable, but unfortunately, I was not able to recall the example.  Could somebody please enlighten me?\nNote that this is not in contradiction with the usual \"Every continuous function is measurable.\", because in this statement it is implicit that the co-domain is equipped with the Borel sets, not the Lebesgue measurable sets.", "output": "The standard example is given by  the function $g(x)=f(x)+x$, where $f$ is the devil's staircase function of Cantor. It turns out that the function  $g$ is a homeomorphism from $[0,1]$ onto $[0,2]$  and has the property that $\\mu(g(C))=1$ (where $C$ is the Cantor set). Pick a non measurable $A\\subset g(C)$. First note that  $B=g^{-1}(A)$ is measurable since $B\\subset C$. It follows that $g^{-1}$ is continuous, $B$ is Lebesgue measurable but $(g^{-1})^{-1}(B)$ is non measurable.", "meta": {"post_id": 479441, "input_score": 48, "output_score": 39, "post_title": "Example of a continuous function that is not Lebesgue measurable"}}
{"input": "I read somewhere that $xy=1$ is a closed set in $\\Bbb{R}^2$. \nA closed set is defined as the complement of an open set, or one which contains all its limit points. In metric spaces, it is defined as the complement of the union of balls $B(x,\\epsilon)$, where $\\epsilon>0$.  For example, $(-\\infty,0)$ is open in $\\Bbb{R}$ as it is $\\bigcup_{i=-1}^{-\\infty}B(i,1)$, where $i\\in \\Bbb{Z}^-$. \nIs $xy=1$, or any graph for that matter, closed because we can take any point $( x,y)$ outside of it and draw an open set centred on it such that $B((x,y),\\epsilon)$ lies completely inside the complement of the graph, thus saying the complement is the union of open sets (hence open)?\nThanks in advance!", "output": "More simply, note that $(x,y)\\mapsto xy$ is a continuous function $\\Bbb R^2\\to\\Bbb R,$ and that $\\bigl\\{(x,y)\\in\\Bbb R^2:xy=1\\bigr\\}$ is the preimage of the closed set $\\{1\\},$ so is closed by continuity.", "meta": {"post_id": 480355, "input_score": 13, "output_score": 38, "post_title": "How is $xy=1$ closed in $\\Bbb{R}^2$?"}}
{"input": "In relation to my previous question, I am curious about what exactly are the normal subgroups of a dihedral group $D_n$ of order $2n$.\nIt is easy to see that cyclic subgroups of $D_n$ is normal. But I suspect that case analysis is needed to decide whether dihedral subgroups of $D_n$ is normal.\nA little bit of Internet search suggests the use of semidirect product $(\\mathbb Z/n\\mathbb Z) \\rtimes (\\mathbb Z/2\\mathbb Z) \\cong D_n$, but I do not know the condition for subgroups of a semidirect product to be normal.\nI would be grateful if you could suggest a way to enumerate the normal subgroups of $D_n$ that does not resort to too much of case analysis.", "output": "Here is a nice answer: the dihedral group is generated by a rotation $R$ and a reflection $F$ subject to the relations $R^n=F^2=1$ and $(RF)^2=1$.\nFor $n$ odd the normal subgroups are given by $D_n$ and $\\langle R^d \\rangle$ for all divisors $d\\mid n$. If $n$ is even, there are two more normal subgroups, i.e., $\\langle R^2,F \\rangle$ and $\\langle R^2,RF \\rangle$.", "meta": {"post_id": 484828, "input_score": 32, "output_score": 42, "post_title": "Normal subgroups of dihedral groups"}}
{"input": "I'm confused how it can be true that the product of an infinite number of Hausdorff spaces $X_\\alpha$ can be Hausdorff. \nIf $\\prod_{\\alpha \\in J} X_\\alpha$ is a product space with product topology, the basis elements consists of of products $\\prod_{\\alpha \\in J} U_{\\alpha}$ where $U_{\\alpha}$ would equal $X_{\\alpha}$ for all but finitely many $\\alpha$'s. If this is the case and we had two distinct points, $x$ and $y$, in $\\prod_{\\alpha \\in J} X_{\\alpha}$ and a basis element, $B_x$ containing $x$ then In looking for a basis element,$B_y$ that contains $y$ but is disjoint from $B_x$ then for every $\\alpha$ such that the open set $U_\\alpha$ of $B_x$ is equal to $X_\\alpha$ the open set $U_\\alpha$ for $B_y$ would have to be empty. But then it couldn't possibly contain $y$. (or any other point of $\\prod_{\\alpha \\in J} X_\\alpha$ for that matter). How then is it possible that $\\prod_{\\alpha \\in J} X_\\alpha$ is Hausdorff in the product topology?\nWhat am I missing?", "output": "If $x$ and $y$ are distinct points of $\\prod_{\\alpha\\in J}X_\\alpha$, then there is at least one $\\alpha_0\\in J$ on which they differ, meaning that $x_{\\alpha_0}\\ne y_{\\alpha_0}$. $X_{\\alpha_0}$ is Hausdorff, so there are open sets $U_{\\alpha_0}$ and $V_{\\alpha_0}$ in $X_{\\alpha_0}$ such that $x_{\\alpha_0}\\in U_{\\alpha_0}$, $y_{\\alpha_0}\\in V_{\\alpha_0}$, and $U_{\\alpha_0}\\cap V_{\\alpha_0}=\\varnothing$. Now let $U_\\alpha=V_\\alpha=X_\\alpha$ for each $\\alpha\\in J\\setminus\\{\\alpha_0\\}$, let $U=\\prod_{\\alpha\\in J}U_\\alpha$, and let $V=\\prod_{\\alpha\\in J}V_\\alpha$; then $U$ and $V$ are basic open sets in $\\prod_{\\alpha\\in J}X_\\alpha$, $x\\in U$, $y\\in V$, and $U\\cap V=\\varnothing$. The only statement there that might not be immediately evident is that $U\\cap V=\\varnothing$; to see this, note that if $z\\in U\\cap V$, then $z_{\\alpha_0}\\in U_{\\alpha_0}\\cap V_{\\alpha_0}=\\varnothing$, so no such $z$ can exist.\nThus, $\\prod_{\\alpha\\in J}X_\\alpha$ is Hausdorff.", "meta": {"post_id": 487626, "input_score": 31, "output_score": 50, "post_title": "The product of Hausdorff spaces is Hausdorff"}}
{"input": "What is the Fourier transform of the indicator of the unit ball in $\\mathbb R^n$?\nI think it is known as one of special functions, so I would be happy to know which one.", "output": "Let $\\alpha_d = \\dfrac{\\pi^{d/2}}{\\Gamma\\left(\\frac{d}{2}+1\\right)}$ the volume of the $d$-dimensional unit ball. Since the characteristic function of the unit ball is rotationally symmetric, so is its Fourier transform, hence let's compute it at the point $\\xi = (0,\\,\\dotsc,\\,0,\\,\\rho)$ with $\\rho > 0$:\n$$\\begin{align}\n\\hat{\\chi}_B(\\xi) &= \\frac{1}{(2\\pi)^{n/2}} \\int_{\\lVert x\\rVert < 1} e^{-i x_n\\rho}\\, dx_1\\, \\dotsc\\, dx_n\\\\\n&= \\frac{1}{(2\\pi)^{n/2}} \\int_{-1}^1 \\left(1-x_n^2\\right)^{(n-1)/2}\\alpha_{n-1} e^{-i x_n\\rho}\\,dx_n\\\\\n&= \\frac{1}{2^{n/2}\\sqrt{\\pi}\\Gamma\\left(\\frac{n+1}{2}\\right)} \\int_0^\\pi \\sin^n \\varphi e^{-i\\rho\\cos\\varphi}\\,d\\varphi.\n\\end{align}$$\nRecalling that we have the Bessel functions\n$$J_p(x) = \\frac{(x/2)^p}{\\sqrt{\\pi}\\Gamma\\left(p + \\frac12\\right)}\\int_0^\\pi \\sin^{2p} \\varphi e^{-ix\\cos\\varphi}\\,d\\varphi,$$\nwe see that\n$$\\hat{\\chi}_B(\\xi) = \\lVert \\xi\\rVert^{-n/2}J_{n/2}(\\lVert\\xi\\rVert).$$", "meta": {"post_id": 489391, "input_score": 27, "output_score": 36, "post_title": "Fourier transform of the indicator of the unit ball"}}
{"input": "Is there any simple explanation for Haar Measure and its geometry?\nhow do we understand analogy Between lebesgue measure and Haar Measure?\nHow to show integration with respect to Haar Measure?\nwhat do we mean by integrating with respect to Measure?", "output": "The question leaves your background a bit unclear, so I chat descriptively about the three Haar measures you are likely to be aware of without even knowing that they are Haar measures. Nothing in what follows is rigorous, but rather seeks to give you a taste of what Haar measure is about.\nWe can measure the size $m(S)$ of a subset $S$ of $\\mathbb{R}$ simply by the integral\n$$\nm(S)=\\int_S 1\\,dx.\n$$\nIf $S=[a,b]$ is an interval, this gives the length $m(S)=b-a$. Here we can think of that $dx$ as a measure (technically it's not, but I ignore that here).\nWhat makes this into a Haar measure is the fact that it is translation invariant.\nIf $c\\in\\mathbb{R}$ is a constant, and $c+S=\\{c+s\\mid s\\in S\\}$, then\n$$\nm(c+S)=\\int_{c+S}1\\,dx=\\int_S1\\,dx=m(S),\n$$\nbecause the substitution $t=x+c$ transforms one integral to the other. Effectively we use\n$$\nd(x+c)=dx\n$$\nand say that $dx$ is a translation invariant measure, i.e. a Haar measure of the additive group of reals.\nWhat about the multiplicative group of positive reals? A Haar measure should be invariant under the group operation. So if $S\\subseteq\\mathbb{R}$ (is a measurable subset), we want\n$m(S)=m(cS)$ to hold for all $c>0$. Obviously the above measure $\\int_S\\,dx$ won't do. For example, the length of an interval is not invariant under scaling.\nThis time we should use the definition\n$$\nm(S)=\\int_S\\frac{dx}x\n$$\ninstead. Here the 1-form $x^{-1}dx$ works. Basically because\n$$\n\\frac{d(cx)}{cx}=\\frac{c\\,dx}{cx}=\\frac{dx}x.\n$$\nYou can check that whenever $0<a<b$, we have\n$$\nm([a,b])=\\int_a^b\\frac{dx}x=\\ln b-\\ln a=\\int_{ca}^{cb}\\frac{dx}x=m([ca,cb]).\n$$\nThus $x^{-1}\\,dx$ is a Haar measure of the multiplicative group of reals.\nA third instance of Haar measure that you have surely seen is that of the unit circle of the complex plane $C=\\{e^{i\\phi}\\mid 0\\le\\phi <2\\pi\\}$. Here the group operation is multiplication. Multiplication by the number $e^{i\\phi_0}$ amounts to rotating the circle counterclockwise by the angle $\\phi_0$. Here\nthe \"measure\" $d\\phi$ will be invariat under such rotations as\n$$\nd(\\phi+\\phi_0)=d\\phi,\n$$\nor, if $S\\subseteq C$, then\n$$\nm(S)=\\int_Sd\\phi=\\int_{e^{i\\phi_0}S}d(\\phi+\\phi_0)=\\int_{e^{i\\phi_0}S}d\\phi=m(e^{i\\phi_0}S).\n$$\nIn the last example (as the group $C$ is compact as a topological space), it is customary (but not necessary for all purposes) to divide the measure by $2\\pi$ so that $m(C)=1$.\nThe fun facts are that for Lie groups and locally compact topological groups a Haar measure always exists. It is unique up to a constant multiplier in many important cases such as compact. For non-commutative groups (e.g. matrix groups) there is a distinction between invariance under group operation from the left or from the right. The two notions coincide in the compact case.\n\nSketching an example, where there is a difference between left and right invariant.\nConsider the group $G$ of real upper triagular matrices of the form\n$$\nG=\\left\\{\\left(\\begin{array}{cc}\\sqrt{y}&\\frac{x}{\\sqrt y}\\\\0&\\frac1{\\sqrt{y}}\n\\end{array}\\right)\\mid x,y\\in\\mathbb{R},y>0\\right\\}.\n$$\nLet us denote the above element of $G$ by $g(x,y)$.\nConsider the differential\n$$\ndg(x,y)=\\frac{\\partial}{\\partial y}g(x,y)\\,dy+\\frac{\\partial}{\\partial x}g(x,y)\\,dx.\n$$\nFor any fixed element $g(r,s)\\in G$ we see that\n$$\n(g(r,s)g(x,y))^{-1}d(g(r,s)g(x,y))=g(x,y)^{-1}g(r,s)^{-1}g(r,s)dg(x,y)\n=g(x,y)^{-1}dg(x,y),\n$$\nso the entries of the matrix of 1-forms \n$$\ng(x,y)^{-1}dg(x,y)=\\left(\n\\begin{array}{cc}\n\\frac{dy}{2y}&\\frac{dx}y\\\\0&-\\frac{dy}{2y}\\end{array}\\right)\n$$\nare left invariant. Therefore the exterior product of the top row entries\n$$\n\\frac{dy}{2y}\\wedge\\frac{dx}y=\\frac12\\frac{dy\\wedge dx}{y^2}\n$$\nis a left-invariant 2-form. Number theorists will recognize this (up to a scalar factor) as the\nhyperbolic metric of the upper half plane. We can define an action of $G$\non the upper half plane by the recipe of fractional linear\ntransformations. Let $\\tau$ be an arbitrary element of the upper half-plane \nand define $g\\cdot\\tau=z_1/z_2$, where \n$$\n\\left(\\begin{array}{c}z_1\\\\\nz_2\\end{array}\\right)\n=g\\left(\n\\begin{array}{c}\\tau\\\\1\\end{array}\\right).\n$$\nHere $g(y,x)\\cdot i=x+iy$, so the entire upper half plane is the orbit of $i$ under $G$.\nWe get a right invariant 2-form similarly by using\n$$\ndg(x,y)g(x,y)^{-1}=\\left(\n\\begin{array}{cc}\n\\frac{dy}{2y}&dx-\\frac{x\\,dy}y\\\\0&-\\frac{dy}{2y}\\end{array}\\right).\n$$\nAgain the wedge product of the entries of the upper row gives a right invariant 2-form\n$$\n\\frac{dy}{2y}\\wedge \\left(dx-\\frac{x\\,dy}y\\right)=\\frac{dy\\wedge dx}{2y}.\n$$\nWe see that this time the left and right invariant measures are not scalar multiples of each other.", "meta": {"post_id": 494225, "input_score": 60, "output_score": 130, "post_title": "What is Haar Measure?"}}
{"input": "Let $f: X \\to Y$ be continuous and proper (a map is proper iff the preimage of a compact set is compact). Furthermore, assume that $Y$ is locally compact and Hausdorff (there are various ways of defining local compactness in Hausdorff spaces, but let's say this means each point $y \\in Y$ has a local basis of compact neighborhoods).\nProve that $f$ is a closed map.\nI know that this proof cannot require much more than a basic topological argument. But there's just something that I'm missing. \nWe can start with $C \\subseteq X$ closed, and then try to show that $Y \\setminus F(C)$ is open (for each $q \\in Y \\setminus F(C)$, we would want to find an open set $V_q$ with $q \\in V_q \\subseteq Y \\setminus F(C)$). \nHints or solutions are greatly appreciated.", "output": "Let $C \\subset X$ be closed. Let $y \\in Y - f(C)$. Since $Y$ is locally compact, $y$ has a neighborhood $V$ with compact closure. Since $f$ is proper, $f^{-1}(\\overline{V})$ is compact in $X$. Let $E = C \\cap f^{-1}(\\overline{V})$. $E$ is compact; thus, $f(E)$ is compact. Since $Y$ is Hausdorff, $f(E)$ is closed. Let $\\hat V = V - f(E)$. $\\hat V$ is a neighborhood of $y$ disjoint from $f(C)$ as desired.", "meta": {"post_id": 501510, "input_score": 38, "output_score": 46, "post_title": "Show that a proper continuous map from $X$ to locally compact $Y$ is closed"}}
{"input": "This is taken from the Car Talk puzzler of the week, seen here: http://www.cartalk.com/content/mathematic-mistake-0?question\nI'll summarize it thusly:\nA hotshot mathematician calls a press conference because he's found a counterexample to Fermat's Last Theorem (which claims that $A^x + B^x = C^x$ has no integer solutions for $A, B$ and $C$ when $x > 2$). However, just to be dramatic (and annoying), he doesn't reveal the whole counterexample, but just the values of $A, B$ and $C$, which are 91, 56 and 121, respectively. The 10-year-old child of one of the reporters attending the press conference raises his hand, and says \"Sorry, sir, but you're wrong.\"\nThe question is: How did the child know?", "output": "7 divides both 91 and 56, but not 121.", "meta": {"post_id": 504954, "input_score": 17, "output_score": 40, "post_title": "Mistaken counterexample to FLT; where's the mistake?"}}
{"input": "I always see this word $\\mathcal{F}$-measurable, but really don't understand the meaning. I am not able to visualize the meaning of it.\nNeed some guidance on this.\nDon't really understand $\\sigma(Y)$-measurable as well. What is the difference?", "output": "Let $(\\Omega,\\mathcal{F},P)$ be a probability space, i.e. $\\Omega$ is a non-empty set, $\\mathcal{F}$ is a sigma-algebra of subsets of $\\Omega$ and $P:\\mathcal{F}\\to [0,1]$ is a probability measure on $\\mathcal{F}$. Now, suppose we have a function $X:\\Omega\\to\\mathbb{R}$ and we want to \"measure\" the probability of $X$ belonging to some subset of $\\mathbb{R}$. That is, we want to assign the probability to sets of the form $$\\{X\\in A\\}:=X^{-1}(A)=\\{\\omega\\in\\Omega\\mid X(\\omega)\\in A\\}$$ for Borel sets $A\\in\\mathcal{B}(\\mathbb{R})$. For this to make sense, we need to make sure that $\\{X\\in A\\}\\in\\mathcal{F}$ for all $A\\in\\mathcal{B}(\\mathbb{R})$, otherwise we can't assign a probability to it (recall that $P$ is only defined on $\\mathcal{F}$). \nWhenever $X:\\Omega\\to\\mathbb{R}$ satisfies that $X^{-1}(A)\\in\\mathcal{F}$ for all $A\\in\\mathcal{B}(\\mathbb{R})$ we say that $X$ is $(\\mathcal{F},\\mathcal{B}(\\mathbb{R}))$-measurable or just $\\mathcal{F}$-measurable when there is no chance of confusion. Thus, for a random variable $X$, it makes sense to assign the probability to any set of the form $\\{X\\in A\\}$, and this defines the distribution of $X$:\n$$\nP_X(A):=P(\\{X\\in A\\}),\\quad A\\in\\mathcal{B}(\\mathbb{R}).\n$$ \nNote that a random variable is a synonym for an $\\mathcal{F}$-measurable function.\nIf $Y:\\Omega\\to\\mathbb{R}$ is a random variable, then $\\sigma(Y)$ is, by definition, given as\n$$\n\\sigma(Y)=\\sigma(\\{Y^{-1}(A)\\mid\\ A\\in\\mathcal{B}(\\mathbb{R})\\}),\n$$\ni.e. the smallest sigma-algebra containing all sets of the form $Y^{-1}(A)$. Another way of characterizing $\\sigma(Y)$ is by saying that it is the smallest sigma-algebra we can put on $\\Omega$ that makes $Y$ measurable.", "meta": {"post_id": 508790, "input_score": 13, "output_score": 35, "post_title": "What does it mean by $\\mathcal{F}$-measurable?"}}
{"input": "Proof.  Let $\\sum_{k = 0}^N c_k \\rightarrow s$, let $\\sigma_N = (S_0 + \\dots + S_{N-1})/N$ be the $Nth$ Cesaro sum where $S_K$ is the $Kth$ partial sum of the series.  Then $s - \\sigma_N \\\\= s - c_0 - c_1(N-1)/N + c_2(N-2)N +\\dots+c_{N-1}/N \\\\ =c_1/N + c_2 2/N + \\dots + c_{N-1}(N-1)/N + c_N + \\dots$\nWhere do I go from here?", "output": "You don't need the fact that it's a series, which is maybe why this is confusing for you. \nSuppose $S_n \\to S$ is a converging sequence. Then $\\frac 1n \\sum_{k=1}^n S_k \\to S$ also. Roughly speaking, if you take $n$ large enough, then all the big terms (big index, not big in value) are close to $S$ ; all the small terms (small index) will get killed when $n$ goes to infinity. \nNon-roughly speaking, \n$$\n\\left| \\left( \\frac 1n \\sum_{k=1}^n S_k \\right) - S \\right| = \\frac 1n \\left| \\sum_{k=1}^n (S_k - S) \\right| \\le \\frac 1n \\sum_{k=1}^n |S_k - S| = \\frac {\\sum_{k=1}^{\\ell} |S_k - S|}{n} + \\frac {\\sum_{k=\\ell+1}^n |S_k - S|}{n}\n$$\nLet $\\varepsilon > 0$, and choose $\\ell$ such that for all $k > \\ell$, $|S_k - S| < \\varepsilon/2$ by convergence of $S_k$ to $S$. Now that $\\ell$ is fixed, choose $N$ large enough so that for all $n > N$, \n$$\n\\frac{\\sum_{k=1}^{\\ell} |S_k - S|}{n} < \\varepsilon / 2.\n$$\n(Note that the numerator does not depend on $n$ so we still have freedom.) It follows that for all $n > N$,\n$$\n\\frac {\\sum_{k=1}^{\\ell} |S_k - S|}{n} + \\frac {\\sum_{k=\\ell+1}^n |S_k - S|}{n} \\le \\frac {\\sum_{k=1}^{\\ell} |S_k - S|}{n} + \\frac{(n-\\ell) (\\varepsilon/2)}n  \\le \\varepsilon.\n$$\nFor your particular problem, put $S_n = \\sum_{k=0}^n c_k$. \nHope that helps,", "meta": {"post_id": 514802, "input_score": 18, "output_score": 48, "post_title": "Convergence of series implies convergence of Cesaro Mean."}}
{"input": "By definition, we have\n$$\n\\|V\\|_p\n:= \\sqrt[p]{\\displaystyle \\sum_{i=1}^{n}|v_i|^p}\n\\qquad \\text{and} \\qquad\n\\|A\\|_p\n:= \\sup_{x\\not=0}\\frac{||Ax||_p}{||x||_p}\n$$\nand if $A$ is finite, we change sup to max.\n\nHowever I don't really get how we get to the definition of $||A||_1$ as the maximum absolute column sum of the matrix as stated in Wikipedia\n\nFor example, assume $A=\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{bmatrix}$.\nThen\n$$\n||A||_1\n= \\max_{x\\not=0} \\frac{\\left\\|\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{bmatrix}\\cdot \\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\right\\| }{\\left\\|\\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\right\\|}\n= \\max_{x\\not=0} \\frac{|a_{11}x_1+a_{12}x_2|+|a_{21}x_1+a_{22}x_2|}{|x_1|+|x_2|}\n$$\nThat's what I have gotten so far, but I don't really see how this is related to the max of the column sum. Can anyone help me explain this?", "output": "Let's denote the columns of $A$ by $A_1,\\, \\dotsc,\\, A_n$. Then for every $x \\in \\mathbb{R}^n$, we have\n$$\\begin{align}\n\\lVert Ax \\rVert_1 &= \\left\\lVert\\sum_{\\nu=1}^n x_\\nu\\cdot A_\\nu \\right\\rVert_1\\\\\n&\\leqslant \\sum_{\\nu=1}^n \\lVert x_\\nu\\cdot A_\\nu\\rVert_1\\\\\n&= \\sum_{\\nu=1}^n \\lvert x_\\nu\\rvert\\cdot\\lVert A_\\nu\\rVert_1\\\\\n&\\leqslant \\max \\left\\{\\lVert A_\\nu\\rVert_1 : 1 \\leqslant \\nu \\leqslant n\\right\\} \\left(\\sum_{\\nu=1}^n \\lvert x_\\nu\\rvert\\right)\\\\\n&= \\max \\left\\{\\lVert A_\\nu\\rVert_1 : 1 \\leqslant \\nu \\leqslant n\\right\\}\\cdot \\lVert x\\rVert_1.\n\\end{align}$$\nThat shows that\n$$\\lVert A\\rVert_1 \\leqslant \\max \\left\\{\\lVert A_\\nu\\rVert_1 : 1 \\leqslant \\nu \\leqslant n\\right\\},$$\nand choosing $x = e_m$, where $m$ is the index where the absolute column sum has its maximum shows the converse inequality, hence equality.", "meta": {"post_id": 519279, "input_score": 31, "output_score": 48, "post_title": "Why is the matrix norm $||A||_1$ maximum absolute column sum of the matrix?"}}
{"input": "Definition: The c.d.f. $F$ of a random variable $X$ is a function defined for each real number $x$ as follows:$$F(x)=\\Pr(X\\leq x) \\text{ for } -\\infty<x<\\infty$$\n\nLet $$F(x^-)=\\lim_{y\\rightarrow x,\\,y<x}F(y)$$ and $$F(x^+)=\\lim_{y\\rightarrow x,\\,y>x}F(y)$$\n\n\nProperty of cumulative distribution function: A c.d.f. is always continuous from the right; that is , $F(x)=F(x^+)$ at every point $x$.\n\n\n\nProof: Let $y_1>y_2>\\dots$ be a sequence of numbers that are decreasing such that $$\\lim_{n\\rightarrow \\infty}y_n=x.$$Then the event $\\{X\\leq x\\}$ is the intersection of all the events $\\{X\\leq y_n\\}$ for $n=1,2,\\dots$ .Hence, $$F(x)=\\Pr(X\\leq x)=\\lim_{n\\rightarrow \\infty} \\Pr(X\\leq y_n)=F(x^+).$$\n\n\nNow I think the left inequality can also be proved in the similar way as:\nLet $y_1<y_2<\\dots$ be a sequence of numbers that are increasing such that $$\\lim_{n\\rightarrow \\infty}y_n=x.$$Then the event $\\{X\\leq x\\}$ is the union of all the events $\\{X\\leq y_n\\}$ for $n=1,2,\\dots$ .Hence, $$F(x)=\\Pr(X\\leq x)=\\lim_{n\\rightarrow \\infty}\\Pr(X\\leq y_n)=F(x^-).$$\nWhere am I wrong?", "output": "You write:\n\nThen the event $\\{X\\leq x\\}$ is the union of all the events $\\{X\\leq y_n\\}$ for $n=1,2,\\dots$.\n\nThis is the faulty step. To wit:\n\nIf $y_n\\lt x$ for every $n$ and $y_n\\to x$, then $\\bigcup\\limits_n\\{X\\leqslant y_n\\}$ is equal to $\\{X\\lt x\\}$, not to $\\{X\\leqslant x\\}$. \n\nYou might want to check that $x$ is in $(-\\infty,y_n]$ for no $n$ whatsoever hence $x$ is not in $\\bigcup\\limits_n(-\\infty,y_n]$, and in fact $\\bigcup\\limits_n(-\\infty,y_n]$ is equal to $(-\\infty,x)$, not to $(-\\infty,x]$.", "meta": {"post_id": 522270, "input_score": 20, "output_score": 34, "post_title": "Why left continuity does not hold in general for cumulative distribution functions?"}}
{"input": "Let $\\large{I} = \\Large \\int\\limits_{-\\infty}^{\\infty} \\normalsize e^{\\small -\\frac{y^2}{2}}\\  dy$. \nThen, my textbook says,\n$$\\tag{1}\nI^2 = \\left( \\int\\limits_{-\\infty}^{\\infty} \\normalsize e^{\\small -\\frac{y^2}{2}}\\  dy\\ \\right) \\left( \\int\\limits_{-\\infty}^{\\infty} \\normalsize e^{\\small -\\frac{x^2}{2}}\\  dx\\ \\right) = \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} e^{-(y^2 +x^2)/2}\\ dy\\ dx $$\nI am not seeing how we reach the rightmost, iterated integral, and I do not remember the calculus how or why this works. Why can we restate the product of two integrals, each having funcions of  different variable, as an iterated integral including both variables? \nLet $\\large{I} = \\Large \\int\\limits_{-\\infty}^{\\infty} \\normalsize f_y(y)\\  dy$ for any continuous, differentiable $ f_y: \\mathbb{R} \\to \\mathbb{R}$\nNow, does (2) generally hold, $\\forall x, y \\in \\mathbb{R}$ ?\n$$\\tag{2}\nI^2 = \\left( \\int\\limits_{-\\infty}^{\\infty} \\normalsize f_y(y)\\  dy\\ \\right) \\left( \\int\\limits_{-\\infty}^{\\infty} \\normalsize f_x(x)\\  dx\\ \\right) = \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} f_y(y)f_x(x) dy\\ dx $$\nor is result(1) specific to the particular function $e^{-(y^2)/2}$?\nPlease show the work or reasoning behind how and why this transformation is valid.", "output": "Note first, that for any integrable $f \\colon \\def\\R{\\mathbb R}\\R \\to \\R$ and any $\\alpha \\in \\R$ we have\n$$ \\int_\\R \\alpha f(x)\\, dx = \\alpha \\int_\\R f(x) \\, dx $$\nNow note that with $\\alpha := \\int_\\R f(y)\\, dy$ (this depends on $f$, but given $f$ it is a constant) this gives\n$$ \\int_\\R  \\left(\\int_\\R f(y)\\, dy\\right)\\, f(x)\\, dx = \\int_\\R f(y)\\, dy  \\cdot \\int_\\R f(x)\\, dx $$\nNow, for every fixed $x \\in \\R$, we apply the above again, now for $\\alpha = f(x)$ (which is not depending on $y$), giving\n$$  \\left(\\int_\\R f(y)\\, dy\\right)\\, f(x) =  \\int_\\R f(x)f(y)\\, dy $$\naltogether\n$$ \\int_\\R  \\int_\\R f(y)f(x)\\, dy\\, dx = \\int_\\R f(y)\\, dy  \\cdot \\int_\\R f(x)\\, dx $$", "meta": {"post_id": 549923, "input_score": 60, "output_score": 61, "post_title": "How the product of two integrals is iterated integral? $\\int\\cdot \\int = \\iint$"}}
{"input": "I need your assistance with evaluating the integral\n$$\\int_0^\\infty\\frac{1}{x\\,\\sqrt{2}+\\sqrt{2\\,x^2+1}}\\cdot\\frac{\\log x}{\\sqrt{x^2+1}}dx$$\nI tried manual integration by parts, but it seemed to only complicate the integrand more. I also tried to evaluate it with a CAS, but it was not able to handle it.", "output": "It is easy to see that the integral is equivalent to\n$$\n\\begin{align*}\n\\int_0^\\infty \\frac{1}{x\\sqrt{2}+\\sqrt{2x^2+1}}\\frac{\\log x}{\\sqrt{1+x^2}}dx &= \\sqrt{2}\\int_0^\\infty \\frac{\\sqrt{x^2+\\frac{1}{2}}-x}{\\sqrt{1+x^2}}\\log x\\; dx\\tag{1}\n\\end{align*}\n$$\nThis integral is a special case of the following generalised equation:\n$$\\begin{align*}\\mathcal{I}(k) :&= \\int_0^\\infty \\frac{\\sqrt{x^2+k^2}-x}{\\sqrt{1+x^2}}\\log x\\; dx \\\\ &= E'(k)-\\left(\\frac{1+k^2}{2} \\right)K'(k)+\\left(k^2 K'(k)-E'(k) \\right)\\frac{\\log k}{2}+\\log 2-1 \\tag{2}\\end{align*}$$\nwhere $K'(k)$ and $E'(k)$ are complementary elliptic integrals of the first and second kind respectively.\nPutting $k=\\frac{1}{\\sqrt{2}}$ in equation $(2)$,\n$$\n\\begin{align*}\n\\mathcal{I}\\left(\\frac{1}{\\sqrt{2}}\\right)&=E'\\left(\\frac{1}{\\sqrt{2} }\\right)-\\frac{3}{4}K'\\left(\\frac{1}{\\sqrt{2}} \\right)-\\left\\{\\frac{1}{2} K'\\left(\\frac{1}{\\sqrt{2}} \\right)-E'\\left(\\frac{1}{\\sqrt{2}}  \\right)\\right\\}\\frac{\\log 2}{4}+\\log 2-1 \n\\end{align*}\n$$\nUsing the special values,\n$$\n\\begin{align*}\nE'\\left(\\frac{1}{\\sqrt2} \\right) &= \\frac{\\Gamma\\left(\\frac{3}{4} \\right)^2}{2\\sqrt\\pi}+\\frac{\\sqrt{\\pi^3}}{4\\Gamma\\left(\\frac{3}{4} \\right)^2}\\\\\nK'\\left(\\frac{1}{\\sqrt2} \\right) &= \\frac{\\sqrt{\\pi^3}}{2\\Gamma\\left(\\frac{3}{4} \\right)^2}\n\\end{align*}\n$$\nwe get\n$$\n\\mathcal{I}\\left(\\frac{1}{\\sqrt{2}}\\right)=\\frac{1+\\log\\sqrt[4]2}{2\\sqrt{\\,\\pi}}\\Gamma\\left(\\frac34\\right)^2-\\frac{\\sqrt{\\,\\pi^3}}8\\Gamma\\left(\\frac34\\right)^{-2}+(\\log 2-1)\\, \\tag{3}\n$$\nPutting this in equation $(1)$, we get the answer that Cleo posted.\n\nHow to prove Equation $(2)$?\nWe begin with Proposition 7.1 of \"The integrals in Gradshteyn and Ryzhik:\nPart 16\" by Boettner and Moll.\n$$\\int_0^\\infty \\frac{\\log x}{\\sqrt{(1+x^2)(m^2+x^2)}}dx = \\frac{1}{2}K'(m)\\log m$$\nMultiplying both sides by $m$ and integrating from $0$ to $k$:\n$$\n\\begin{align*}\n\\int_0^\\infty \\frac{\\sqrt{x^2+k^2}-x}{\\sqrt{1+x^2}}\\log x\\; dx &= \\frac{1}{2}\\int_0^k m K'(m)\\log(m)\\; dm\n\\end{align*}\n$$\nThe result follows since\n$$\\begin{align*} \\int m K'(m)\\log(m)\\; dm &= 2E'(m)-\\left(1+m^2 \\right)K'(m)+\\left(m^2 K'(m)-E'(m) \\right)\\log m\\\\ &\\quad +\\text{constant} \\tag{4}\n\\end{align*}$$\nOne can verify equation $(4)$ easily by differentiating both sides with respect to $m$ and using the identities\n$$\n\\begin{align*}\n\\frac{dE'(k)}{dk}&= \\frac{k}{k^{'2}}(K'(k)-E'(k))\\\\\n\\frac{dK'(k)}{dk}&= \\frac{k^2 K'(k)-E^{'}(k)}{kk^{'2}}\n\\end{align*}\n$$", "meta": {"post_id": 554624, "input_score": 35, "output_score": 39, "post_title": "Integral $\\int_0^\\infty\\frac{1}{x\\,\\sqrt{2}+\\sqrt{2\\,x^2+1}}\\cdot\\frac{\\log x}{\\sqrt{x^2+1}}\\mathrm dx$"}}
{"input": "This is somewhat similar to my previous question: Closed form for $\\int_0^1\\frac{x^{5/6}}{(1-x)^{1/6}\\,(1+2\\,x)^{4/3}}\\log\\left(\\frac{1+2x}{x\\,(1-x)}\\right)\\,dx$\nIs it possible to find a closed form for this integral?\n$$Q=\\int_0^1\\sqrt{\\frac{2-x}{(1-x)\\,x}}\\,\\log\\left(\\frac{(2-x)\\,x}{1-x}\\right)dx$$", "output": "First, we transform the integral into a more computable form by using some substitutions.\n$$\\begin{align*}\\displaystyle Q &= \\int_0^1\\sqrt{\\frac{2-x}{(1-x)\\,x}}\\,\\log\\left(\\frac{(2-x)\\,x}{1-x}\\right)dx\\\\ &=\\int_0^1 \\sqrt{\\frac{1+u}{u(1-u)}}\\log \\left( \\frac{(1+u)(1-u)}{u}\\right)du \\quad \\color{blue}{\\text{where }u=1-x}\\\\\n&= \\int_0^1 \\frac{1+u}{\\sqrt{u(1-u^2)}}\\log \\left( \\frac{1-u^2}{u}\\right)du \\\\\n&= \\frac{1}{2}\\int_0^1 \\frac{1+\\sqrt{t}}{t^{\\frac{3}{4}}\\sqrt{1-t}}\\log \\left(\\frac{1-t}{\\sqrt{t}} \\right)dt \\quad \\color{blue}{\\text{where }t=u^2} \\\\\n&= \\frac{1}{2}\\int_0^1 \\frac{\\log(1-t)}{t^{\\frac{3}{4}}\\sqrt{1-t}}dt-\\frac{1}{4}\\int_0^1 \\frac{\\log(t)}{t^{\\frac{3}{4}}\\sqrt{1-t}}dt \\\\\n&\\quad +\\frac{1}{2}\\int_0^1 \\frac{\\log(1-t)}{t^{\\frac{1}{4}}\\sqrt{1-t}}dt-\\frac{1}{4}\\int_0^1 \\frac{\\log(t)}{t^{\\frac{1}{4}}\\sqrt{1-t}}dt \\tag{1}\n\\end{align*}$$\nThese four integrals can be evaluated by calculating derivatives of beta function in terms of digamma function. For e.g.\n$$\n\\begin{align*}\n\\int_0^1 \\frac{\\log(1-t)}{t^{\\frac{3}{4}}\\sqrt{1-t}}dt  &= \\frac{d}{dz}\\left\\{ \\int_0^1 t^{-\\frac{3}{4}}(1-t)^{z-1} \\; dt\\right\\}_{z=\\frac{1}{2}}\\\\&= \\frac{d}{dz}\\left\\{ \\frac{\\Gamma \\left( \\frac{1}{4}\\right)\\Gamma(z)}{\\Gamma \\left( \\frac{1}{4}+z\\right)} \\right\\}_{z=\\frac{1}{2}}\\\\ &= \\frac{\\Gamma \\left( \\frac{1}{4}\\right)\\sqrt{\\pi}}{\\Gamma \\left( \\frac{3}{4}\\right)}\\left\\{\\psi_0 \\left(\\frac{1}{2} \\right) -\\psi_0 \\left(\\frac{3}{4} \\right)\\right\\}\\\\ &= \\pi^{3/2}\\frac{\\sqrt{2}}{\\Gamma \\left( \\frac{3}{4}\\right)^2}\\left\\{\\log 2-\\frac{\\pi}{2} \\right\\} \\tag{2} \\\\\n\\end{align*}\n$$\nTo get the last expression, I used the special values\n$$\n\\begin{align*}\n\\psi_0 \\left(\\frac{3}{4}\\right) &= -\\gamma +\\frac{\\pi}{2}-3\\log 2 \\\\\n\\psi_0 \\left(\\frac{1}{2}\\right) &= -\\gamma -2\\log 2\n\\end{align*}\n$$\nUsing the same technique, the other three integrals can be evaluated:\n$$\n\\begin{align*}\n\\int_0^1 \\frac{\\log(t)}{t^{\\frac{3}{4}}\\sqrt{1-t}}dt&= -\\pi^{5/2}\\frac{\\sqrt2}{\\Gamma \\left( \\frac{3}{4}\\right)^2}\\tag{3} \\\\\n\\int_0^1 \\frac{\\log(t)}{t^{\\frac{1}{4}}\\sqrt{1-t}}dt&=\\frac{(4\\pi-16)\\Gamma \\left( \\frac{3}{4}\\right)^2}{\\sqrt{2\\pi}} \\tag{4} \\\\\n\\int_0^1 \\frac{\\log(1-t)}{t^{\\frac{1}{4}}\\sqrt{1-t}}dt&=\\frac{2(-8+\\pi+2\\log 2)\\Gamma \\left( \\frac{3}{4}\\right)^2}{\\sqrt{2\\pi}} \\tag{5}\n\\end{align*}\n$$\nSubstituting the results of equations $(2),(3),(4)$ and $(5)$ in $(1)$ gives\n$$Q=\\frac{\\Gamma\\left(\\frac34\\right)^{-2}\\pi^2\\log2-\\Gamma\\left(\\frac34\\right)^2(4-2\\log2)}{\\sqrt{2\\,\\pi}}$$", "meta": {"post_id": 566513, "input_score": 31, "output_score": 46, "post_title": "Closed form for $\\int_0^1\\sqrt{\\frac{2-x}{(1-x)\\,x}}\\,\\log\\left(\\frac{(2-x)\\,x}{1-x}\\right)dx$"}}
{"input": "This question is inspired by a previous question. It was shown that, for all function $f \\in \\mathcal{C} ([0, 1])$, \n$$ \\lim_{n \\to + \\infty} \\sum_{k=0}^{n} f \\left( \\frac{k}{n+1} \\right) - \\sum_{k=0}^{n-1} f \\left( \\frac{k}{n} \\right) = \\int_0^1 f (x) \\ dx.$$\nA stronger statement would be that there exists some constant $a(f)$ such that:\n$$\\sum_{k=0}^{n-1} f \\left( \\frac{k}{n} \\right) = n \\int_0^1 f (x) \\ dx + a(f) + o(1),$$\nor, in other words, that there is an asymptotic development at order $1$ of the Riemann sums:\n$$\\frac{1}{n} \\sum_{k=0}^{n-1} f \\left( \\frac{k}{n} \\right) = \\int_0^1 f (x) \\ dx + \\frac{a(f)}{n} + o(n^{-1}).$$\nGiven $f$, can we always find such a constant $a(f)$? If this is false, can we find a counter-example? If this is true, can $a(f)$ be written explicitely?\nI have had a quick look at the litterature, but most asymptotics for the Riemann sums involve different meshes, which depend on the function $f$.", "output": "On the subspace $\\mathcal{C}^1([0,1])$ of continuously differentiable functions, we have\n$$\\lim_{n\\to\\infty} \\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right) - n\\int_0^1 f(x)\\,dx = \\frac{f(0) - f(1)}{2}.$$\nWe can see that by computing\n$$\\begin{align}\n&\\Biggl\\lvert\\frac12\\left(f\\left(\\frac{k}{n}\\right) + f\\left(\\frac{k+1}{n}\\right)\\right) - n\\int_{k/n}^{(k+1)/n} f(x)\\,dx\\Biggr\\rvert\\\\\n&\\qquad = \\frac{n}{2}\\left\\lvert \\int_{k/n}^{(k+1)/n} \\left(f\\left(\\frac{k+1}{n}\\right)-f(x)\\right) - \\left(f(x) - f\\left(\\frac{k}{n}\\right)\\right)\\,dx\\right\\rvert\\\\\n&\\qquad = \\frac{n}{2}\\left\\lvert\\int_{k/n}^{(k+1)/n}\\int_x^{(k+1)/n} f'(t)\\,dt - \\int_{k/n}^x f'(t)\\,dt\\,dx\\right\\rvert\\\\\n&\\qquad = \\frac{n}{2}\\left\\lvert\\int_{k/n}^{(k+1)/n}\\left(t-\\frac{k}{n}\\right)f'(t) - \\left(\\frac{k+1}{n}-t\\right)f'(t)\\,dt\\right\\rvert\\\\\n&\\qquad = n\\left\\lvert\\int_{k/n}^{(k+1)/n} \\left(t-\\frac{k+\\frac12}{n}\\right)f'(t)\\,dt\\right\\rvert\\\\\n&\\qquad = n\\left\\lvert\\int_{k/n}^{(k+1)/n} \\left(t-\\frac{k+\\frac12}{n}\\right)\\left(f'(t)- f'\\left(\\frac{k+\\frac12}{n}\\right)\\right)\\,dt\\right\\rvert\\\\\n&\\qquad \\leqslant n \\cdot\\omega_{f'}\\left(\\frac{1}{2n}\\right) \\int_{k/n}^{(k+1)/n} \\left\\lvert t-\\frac{k+\\frac12}{n}\\right\\rvert\\,dt\\\\\n&\\qquad = \\frac{1}{4n}\\cdot\\omega_{f'}\\left(\\frac{1}{2n}\\right),\n\\end{align}$$\nwhere\n$$\\omega_{f'}(\\delta) = \\sup \\left\\lbrace \\lvert f'(s) - f'(t)\\rvert : s,t\\in [0,1], \\lvert s-t\\rvert \\leqslant \\delta\\right\\rbrace$$\nis a modulus of continuity of $f'$. Summing up we obtain\n$$\\left\\lvert \\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right) - \\frac{f(0)-f(1)}{2} - n\\int_0^1 f(x)\\,dx\\right\\rvert \\leqslant \\frac14 \\cdot\\omega_{f'}\\left(\\frac{1}{2n}\\right),$$\nand the continuity of $f'$ means $\\lim\\limits_{\\delta\\searrow 0} \\omega_{f'}(\\delta) = 0$.\nBut there is no map $\\alpha \\colon \\mathcal{C}([0,1]) \\to \\mathbb{C}$ such that for every $f \\in \\mathcal{C}([0,1])$ we have\n$$\\lim_{n\\to\\infty} \\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right) - n\\int_0^1 f(x)\\,dx = \\alpha(f),$$\nor equivalently\n$$\\frac{1}{n}\\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right) = \\int_0^1 f(x)\\,dx + \\frac{\\alpha(f)}{n} + o\\left(\\frac1n\\right).$$\nFor if there were, since $\\mathcal{C}([0,1])$ is a Banach space under the supremum norm, the Banach-Steinhaus theorem (uniform boundedness principle) would assert that the family\n$$T_n \\colon f \\mapsto \\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right) - n\\int_0^1 f(x)\\,dx$$\nis equicontinuous, or norm-bounded.\nHowever, it is easy to see that\n$$\\lVert T_n\\rVert = 2n.$$\nThus the set of $f \\in \\mathcal{C}([0,1])$ such that\n$$\\lim_{n\\to\\infty} \\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right) - n\\int_0^1 f(x)\\,dx$$\nexists is meagre (it is, however, strictly larger than $\\mathcal{C}^1([0,1])$).\nNote that the previous question only considered continuously differentiable functions, i.e. $\\mathcal{C}^1([0,1])$. The Banach-Steinhaus theorem shows that\n$$\\lim_{n\\to\\infty} \\sum_{k=0}^n f\\left(\\frac{k}{n+1}\\right) - \\underbrace{\\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right)}_{S_n(f)}$$\ndoes not exist for all $f\\in\\mathcal{C}([0,1])$, since $\\lVert S_{n+1} - S_n\\rVert = 2n-1$ is not bounded.", "meta": {"post_id": 569750, "input_score": 27, "output_score": 37, "post_title": "Speed of convergence of Riemann sums"}}
{"input": "So I want to show that $\\mathbb{Q}(\\sqrt{2+\\sqrt{2}})$ is Galois over $\\mathbb{Q}$ and determine its Galois group. \nMy thoughts are as follows:\nDefine $\\alpha := \\sqrt{2+\\sqrt{2}}$. Then it is easily shown that $\\alpha$ satisfies $\\alpha^4-4\\alpha^2+2=0$.\nDefine $f(x) := x^4-4x^2+2$. Then $f$ is irreducible over $\\mathbb{Q}$ by Eisenstein with $p=2$.\nSo we have that $f$ is the irreducible polynomial for $\\alpha$ over $\\mathbb{Q}$. \nFurther $|\\mathbb{Q}(\\alpha):\\mathbb{Q}|=4$.\nFor $\\mathbb{Q}({\\alpha})$ to be Galois, it must contain all roots of $f$. \nDefine $K=\\mathbb{Q}(\\alpha)$ for convenience.\nDefine $\\alpha := \\alpha_1$.\nSince $f$ has only even powers, we know that $-\\alpha := \\alpha_2$ is a root, and therefore contained in $K$ since $K$ is a field.\nWe note that the other two roots are $\\alpha_3=\\sqrt{2-\\sqrt{2}}$ and $\\alpha_4=-\\sqrt{2-\\sqrt{2}}$.\nSo in order to show $K$ is Galois, it must be shown that $\\alpha_3$ and $\\alpha_4$ lie in $K$.\nNow $\\alpha_1^2=2+\\sqrt2$ and so $\\sqrt2 \\in K$. Thus $-\\sqrt2 \\in K$ since $K$ is a field.\nCan somebody explain why $\\alpha_3$ and $\\alpha_4$ lie in $K$?\nNext we are to determine the Galois group of $K$.\nAssuming $K$ is Galois, since it has degree $4$ over $\\mathbb{Q}$ (shown earlier), we know that its Galois group has size $4$. There are only two groups of size $4$, namely $V_4$ and $C_4$, the Klein four group and the cyclic group of order $4$.\nHow do we determine which of these choice is in fact the Galois Group?", "output": "$\\alpha^2-2=\\sqrt 2\\in K$ by closure of multiplication and addition.\n$$\\frac{\\sqrt 2}{\\sqrt{2+\\sqrt{2}}}=\\frac{\\sqrt 2 \\cdot\\sqrt{2-\\sqrt 2}}{\\sqrt{2+\\sqrt{2}}\\sqrt{2-\\sqrt 2}}=\\frac{\\sqrt 2\\cdot\\sqrt{2-\\sqrt 2}}{\\sqrt{4-2}}=\\sqrt{2-\\sqrt 2}$$\nSince $K$ is a field, it has multiplicative inverses and is closed under multiplication, so $\\sqrt{2-\\sqrt 2}\\in K$.\nWe can determine the nature of $\\mathrm{Gal}(K/\\mathbb{Q})$ by the order of each element. If $f$ is a field automorphism of $K$ and $f(\\sqrt{2+\\sqrt 2})=\\sqrt{2-\\sqrt 2}$, then $f(\\sqrt 2)=f(\\alpha^2-2)=f(\\alpha)^2-2=-\\sqrt 2$. Therefore $$f(f(\\alpha))=f\\left(\\sqrt{2-\\sqrt 2}\\right)=f\\left(\\frac{\\sqrt{2}}{\\sqrt{2+\\sqrt 2}}\\right)=\\frac{f(\\sqrt 2)}{f(\\sqrt{2+\\sqrt 2})}=\\frac{-\\sqrt{2}}{\\sqrt{2-\\sqrt{2}}}=-\\sqrt{2+\\sqrt 2}$$\nTherefore $\\mathrm{ord}(f)> 2$ and must divide $4=|\\mathrm{Gal}(F/\\mathbb{Q})|$, so $\\mathrm{ord}(f)=4$.  It follows that the Galois group is cyclic and abelian.", "meta": {"post_id": 575171, "input_score": 33, "output_score": 43, "post_title": "Galois Group of $\\sqrt{2+\\sqrt{2}}$ over $\\mathbb{Q}$"}}
{"input": "I'm studying summation. Everything I know so far is that:\n$\\sum_{i=1}^n\\ i = \\frac{n(n+1)}{2}\\ $\n$\\sum_{i=1}^{n}\\ i^2 = \\frac{n(n+1)(2n+1)}{6}\\ $\n$\\sum_{i=1}^{n}\\ i^3 = \\frac{n^2(n+1)^2}{4}\\ $\nUnfortunately, I can't find neither on my book nor on the internet what the result of:\n$\\sum_{i=1}^n\\log i$.\n$\\sum_{i=1}^n\\ln i$.\nis.\nCan you help me out?", "output": "By using the fact that $$\\log a + \\log b = \\log ab $$ then\n$$ \\sum^n \\log i = \\log (n!) $$\n$$ \\sum^n \\ln i = \\ln (n!) $$", "meta": {"post_id": 589027, "input_score": 19, "output_score": 41, "post_title": "What's the formula to solve summation of logarithms?"}}
{"input": "The mean value theorem for holomorphic functions states that if $f$ is analytic in $D$ and $a \\in D$, then $f(a)$ equals the integral around any circle centered at $a$ divided by $2\\pi$.  But if $f$ is analytic, then the line integral around any closed curve is 0, so $f(a) = 0$.  Why does the MVT not result in all holomorphic functions being identically zero?  There must be something I'm missing here.", "output": "The difference is that for the mean value property, we consider the integral with the measure/form $d\\varphi$, and for the integral theorem, the measure/form is $dz$.\nThe mean value property is actually the Cauchy integral formula for the centre of a disk:\n$$\\begin{align}\nf(a) &= \\frac{1}{2\\pi i} \\int_{\\lvert z-a\\rvert = r} \\frac{f(z)}{z-a}\\,dz\\\\\n&= \\frac{1}{2\\pi i} \\int_0^{2\\pi} \\frac{f(a+re^{i\\varphi})}{(a+re^{i\\varphi})-a}\\, d(a+re^{i\\varphi})\\\\\n&= \\frac{1}{2\\pi i} \\int_0^{2\\pi} \\frac{f(a+re^{i\\varphi})}{re^{i\\varphi}}rie^{i\\varphi}\\,d\\varphi\\\\\n&= \\frac{1}{2\\pi} \\int_0^{2\\pi} f(a+re^{i\\varphi})\\,d\\varphi.\n\\end{align}$$\nFor a parametrised circle with centre $a$, we have $d\\varphi = \\dfrac{dz}{i(z-a)}$, so the integral of $d\\varphi$ over a circle does not vanish, while the integral of $dz$ does.", "meta": {"post_id": 595395, "input_score": 33, "output_score": 50, "post_title": "Mean value theorem for holomorphic functions"}}
{"input": "I have a question about something I'm wondering about. I've read somewhere that\nL'Hopitals rule can also be applied to complex functions, when they are analytic.\nSo if have for instance:\n$$\n\\lim_{z \\rightarrow 0} \\frac{\\log(1+z)}{z} \\stackrel{?}{=} \\lim_{z \\rightarrow 0} \\frac{1}{(1+z)} = 1\n$$\nNow i'm wondering if this is correct? Also if we take $|z|<1$, is it then correct?\nThanks,", "output": "L'Hopital's rule is a local statement: it concerns  the behavior of functions near a particular point. The global issues (multivaluedness, branch cuts) are irrelevant. For example, if you consider $\\lim_{z\\to 0}$, then it's automatic that only small values of $z$ are in play.  Saying \"take $|z|<1$\" is redundant.\nGenerally, you have a point $a\\in\\mathbb C$ and some neighborhood of $a$ in which   $f,g$ are holomorphic. If $f(a)=g(a)=0$, then \n$$\\lim_{z\\to a}\\frac{f(z)}{z-a}=f'(a),\\qquad \\lim_{z\\to a}\\frac{g(z)}{z-a}=g'(a) \\tag{1}$$\nhence \n$$\\lim_{z\\to a}\\frac{f(z)}{g(z)}= \\lim_{z\\to a}\\frac{f(z)/(z-a)}{g(z)/(z-a)} =\\frac{f'(a)}{ g'(a)}$$\nNote that the above is a  simple special case of  the L'Hopital's rule, because we have (1). It's basically just the definition of derivative.", "meta": {"post_id": 602650, "input_score": 52, "output_score": 74, "post_title": "Is L'Hopitals rule applicable to complex functions?"}}
{"input": "I learned that every compact set is closed and bounded; and also that an open set is usually not compact. \nHow to show that a concrete open set, for example the interval $(0,1)$, is not compact? I tried to show that $(0,1)$ has no finite sub cover.", "output": "I think the following may be a source of confusion: the statement \"$(0,1)$ has no finite sub cover\" doesn't make any sense.  You first have to choose a cover of $(0,1)$ by open sets.  Then this may or may not have a finite sub-cover. \nIf $(0,1)$ were compact, any such cover would (by the definition of compact in terms of open covers) have to have a finite subcover.  In fact, $(0,1)$ is not compact, and so what this means is that we can find some cover of $(0,1)$ by open sets which does not have a finite subcover.\nOmnomnomnom gives an example of such a cover in their answer, and there are lots of others; here's one: the cover $\\{U_2, \\ldots, U_n , \\ldots\\}$ where $U_n = (1/n, 1-1/n).$\nHere is a cover which is finite, and hence does have a finite subcover (namely, itself): $\\{(0,1)\\}$.\nHere is another: $\\{(0,1/2), (1/3,1)\\}$.\nHere is an infinite cover which admits a finite subcover $\\{(0,1), (0,1/2), \n\\ldots, (0,1/n) , \\ldots \\}$.\nHopefully these examples help to clarify what the definition of compactness in terms of finite subcovers is about.", "meta": {"post_id": 609217, "input_score": 16, "output_score": 39, "post_title": "Why is an open interval not a compact set?"}}
{"input": "I have a classmate learning algebra.He ask me how to compute the de Rham cohomology of the punctured plane  $\\mathbb{R}^2\\setminus\\{0\\}$ by an elementary way,without homotopy type,without Mayer-Vietoris,just by Calculus. I have tried and failed.Is it possible to compute the de Rham cohomology just by Calculus?", "output": "Let $M = \\mathbb{R}^2 \\setminus \\{0\\}$. We have a global coordinates $(x, y)$ on $M$. We want to compute the cohomology of the complex\n$$0 \\to \\Omega^0(M)\\ \\stackrel{d}{\\to}\\ \\Omega^1(M)\\ \\stackrel{d}{\\to}\\ \\Omega^2(M)\\ \\to 0$$\nwhere $\\Omega^k(M)$ is the space of smooth $k$-forms on $M$. So we want to compute $H^k(M) = Z^k(M)/B^k(M)$ where $Z^k(M) = \\{\\alpha \\in \\Omega^k(M),\\, d\\alpha = 0\\}$ (closed $k$-forms) and $B^k(M) = \\{d\\beta,\\, \\beta \\in \\Omega^{k-1}(M)\\}$ (exact $k$-forms). In this situation,\n\nElements of $\\Omega^0(M)$ are just smooth functions $f(x,y)$ on $M$\nElements of  $\\Omega^1(M)$ can be written $u(x,y) dx + v(x,y) dy$ (where $u$ and $v$ are smooth functions on $M$)\nElements of $\\Omega^2(M)$ can be written $g(x,y) dx \\wedge dy$ (where $g$ is a smooth function on $M$)\n\n\nCompute $H^0(M)$:\n$B^0(M) = \\{0\\}$ and $Z^0(M)$ consists of functions $f(x,y)$ such that $df = 0$. Since $M$ is connected, this implies that $f$ is constant. It follows that $H^0(M)$ is isomorphic to $\\mathbb{R}$.\n\nCompute $H^1(M)$:\nThis is the where the all the fun happens :)\nLet $\\alpha \\in Z^1(M)$, this means that $\\alpha = u(x,y) dx + v(x,y) dy$ with $\\frac{\\partial v}{\\partial x} - \\frac{\\partial u }{\\partial y} = 0$.\n\nLemma 1: Let $R$ be a closed rectangle in $\\mathbb{R}^2$ which does not contain the origin. Then $\\int_{\\partial R} \\alpha = 0$.\n\nProof: This is just Green's theorem (or Stokes' theorem) (in its most simple setting, where it's not hard to show directly).\n\nLemma 2: Let $R$ and $R'$ be closed rectangles in $\\mathbb{R}^2$ whose interiors contain the origin. Then $\\int_{\\partial R} \\alpha = \\int_{\\partial R'} \\alpha$.\n\nProof: This is a consequence of Lemma 1. It might be a bit tedious to write down (several cases need to be addressed, according to the configuration of the two rectangles), but it's fairly easy. You need to cut and rearrange integrals along a bunch of rectangles so that the two initial integrals agree up to integrals along rectangles who do not contain the origin.\nLet's denote by $\\lambda(\\alpha)$ the common value of all integrals $\\int_{\\partial R} \\alpha$ when $R$ is a closed rectangle whose interior contains the origin.\n\nLemma 3: If $\\alpha$ is exact iff $\\lambda(\\alpha) = 0$.\n\nProof: \"$\\Rightarrow$\" is trivial1, let's prove the converse. Fix permanently some point $m_0 \\in M$, whichever you like best. For any $m\\in M$, consider a rectangle $R$ in $\\mathbb{R}^2$ whose boundary contains $m_0$ and $m$ but avoids the origin. Let $\\gamma$ be one of the two paths joining $m_0$ and $m$ along $\\partial R$. Let $f(m) = \\int_\\gamma \\alpha$. Since $\\lambda(\\alpha) = 0$, this definition does not depend on the choice of the rectangle or the path. In other words $f: M \\rightarrow \\mathbb{R}$ is well defined. Let's see that $df = \\alpha$. Check that $\\frac{f(x+h, y) - f(x,y)}{h} = \\frac{1}{h}\\int_x^{x+h} u(t,y) dt$, so that taking the limit when $h\\rightarrow 0$ yields $\\frac{\\partial f}{\\partial x} = u$. Same for $v$.\n\nFinally let's consider the $1$-form $d\\theta = \\frac{-ydx + xdy}{x^2 + y^2}$. NB: Be well aware that $d\\theta$ is a misleading (but standard) notation: it is not an exact form.\n\nLemma 4: $d\\theta$ is a closed $1$-form and $\\lambda(d\\theta) = 2\\pi$.\n\nProof: This is a direct computation.\nNow're done:\n\nProposition: $H^1(M)$ is the one-dimensional vector space spanned by $[d\\theta]$.\n\nwhere $[d\\theta]$ denotes the class of $d\\theta$ in $H^1(M)$. Note that $[d\\theta] \\neq 0$: see Lemma 4 and 3.\nProof: Let $\\alpha$ be a closed $1$-form. Consider $\\beta = \\alpha - \\frac{\\lambda(\\alpha)}{2 \\pi} d\\theta$. We have $\\lambda(\\beta) = 0$ so $\\beta$ is exact by Lemma 3. This proves that $[\\alpha] = \\frac{\\lambda(\\alpha)}{2 \\pi} [d\\theta]$.\n\nCompute $H^2(M)$:\nLet's show that $H^2(M) = 0$, in other words every closed $2$-form on $M$ is exact. This solution is taken from Ted Shifrin in the comments below.\nHere's the idea: in polar coordinates, a $2$-form $\\omega$ can be written $\\omega = f(r,\\theta) dr \\wedge d\\theta$. Then $\\eta = (\\int_1^r f(\\rho, \\theta) d\\rho)\\, d\\theta$ is a primitive of $\\omega$.\nAlthough it's not very insightful, this can be checked by a direct computation without refering to a change of variables. \nLet $\\omega(x,y) = g(x,y) dx \\wedge dy$. Define $$h(x,y) = \\int_1^{\\sqrt{x^2+y^2}} t\\, g\\left(\\frac{tx}{\\sqrt{x^2+y^2}}, \\frac{ty}{\\sqrt{x^2+y^2}}\\right) \\,dt$$\nCheck that $h(x,y) d\\theta$ is a primitive of $\\omega$.\nNB: Any course / book / notes on de Rham cohomology will show that if $M$ is a connected compact orientable manifold, $H^n(M) \\approx \\mathbb{R}$. However, I don't think I've read anywhere that when $M$ is not compact, $H^n(M) = 0$. I wonder if there is an \"elementary\" proof.\n\n1 I'll explain this by request of OP. It is a general fact that if $\\gamma : [a, b] \\rightarrow M$ is a ${\\cal C}^1$ path and $\\alpha$ is a smooth exact one-form i.e. $\\alpha = df$ where $f$ is a smooth function, then $\\int_\\gamma \\alpha = f(\\gamma(b)) - f(\\gamma(a))$. This is because $\\int_\\gamma \\alpha = \\int_a^b \\alpha(\\gamma(t))(\\gamma'(t))\\,dt$ (by definition) and here $\\alpha(\\gamma(t))(\\gamma'(t)) = df(\\gamma(t))(\\gamma'(t)) = f'(\\gamma(t)) \\gamma'(t) = \\frac{d}{dt}\\left(f(\\gamma(t))\\right)$. \nThis fact extends to piecewise ${\\cal C}^1$ maps by cutting the integral. In particular, is $\\gamma$ is a closed piecewise ${\\cal C}^1$ path and $\\alpha$ is exact, then $\\int_\\gamma \\alpha = 0$.\nIn fact, it is useful (e.g. for Cauchy theory in complex analysis) to know that\n\nA one-form is closed iff its integral along any homotopically trivial loop is zero (any boundary of a rectangle contained in the open set is enough).\nA one-form is exact iff its integral along any loop is zero.\n\nNote that the \"smooth\" condition can be weakened.", "meta": {"post_id": 612837, "input_score": 34, "output_score": 56, "post_title": "how to compute the de Rham cohomology of the punctured plane just by Calculus?"}}
{"input": "What is the remainder when $$1! + 2! + 3! +\\cdots+ 1000!$$ is divided by $12$.\nI have tried to find the answer using the Binomial Theorem but that doesn't help.\nHow will we do this?\nPlease help.", "output": "If $n\\ge 4$, then $4!=24$ divides $n!$ $-$ in particular $12$ divides $n!$ when $\\ge 4$. \nThus\n$$\n1!+2!+\\cdots+1000!=1!+2!+3! \\!\\!\\!\\!\\pmod{12}=9\\!\\!\\!\\!\\pmod{12}.\n$$", "meta": {"post_id": 618992, "input_score": 14, "output_score": 47, "post_title": "What is the remainder when $1! + 2! + 3! +\\cdots+ 1000!$ is divided by $12$?"}}
{"input": "What is the mean and variance of Squared Gaussian: $Y=X^2$ where: $X\\sim\\mathcal{N}(0,\\sigma^2)$?\nIt is interesting to note that Gaussian R.V here is zero-mean and non-central Chi-square Distribution doesn't work.\nThanks.", "output": "We can avoid using the fact that $X^2\\sim\\sigma^2\\chi_1^2$, where $\\chi_1^2$ is the chi-squared distribution with $1$ degree of freedom, and calculate the expected value and the variance just using the definition. We have that\n$$\n\\operatorname E X^2=\\operatorname{Var}X=\\sigma^2\n$$\nsince $\\operatorname EX=0$ (see here).\nAlso,\n$$\n\\operatorname{Var}X^2=\\operatorname EX^4-(\\operatorname EX^2)^2.\n$$\nThe fourth moment $\\operatorname EX^4$ is equal to $3\\sigma^4$ (see here). Hence,\n$$\n\\operatorname{Var}X^2=3\\sigma^4-\\sigma^4=2\\sigma^4.\n$$", "meta": {"post_id": 620045, "input_score": 39, "output_score": 41, "post_title": "Mean and variance of Squared Gaussian: $Y=X^2$ where: $X\\sim\\mathcal{N}(0,\\sigma^2)$?"}}
{"input": "The residue at infinity is given by:\n$$\\underset{z_0=\\infty}{\\operatorname{Res}}f(z)=\\frac{1}{2\\pi i}\\int_{C_0} f(z)dz$$\nWhere $f$ is an analytic function except at finite number of singular points and $C_0$ is a closed countour so all singular points lie inside it.\nIt can be proven that the residue at infinity can be computed calculating the residue at zero.\n$$\\underset{z_0=\\infty}{\\operatorname{Res}}f(z)=\\underset{z_0=0}{\\operatorname{Res}}\\frac{-1}{z^2}f\\left(\\frac{1}{z}\\right)$$\nThe proof is just to expand $-\\frac{1}{z^2}f\\left(\\frac{1}{z}\\right)$ as a Laurent series and to see that the $1/z$ is the integral mentioned.\nI can see that we change $f(z)$ to $f(1/z)$ so the variable tends to infinity.\nBut, is there any intutive reason of why we introduce the $-1/z^2$ factor?", "output": "The thing is that functions do not have residues, but rather differentials have residues. This is something which can be quite confusing in a first complex analysis class. The \"residue of a function\" is not invariant under a change of local parameter, but the residue of a differential is. For this reason, what is usually called the \"residue at $0$ of $f(z)$\" is actually the residue at $0$ of $f(z)dz$.\nWhen you change the coordinate from $z$ to $w=1/z$, the differential $dz$ is transformed into $-dw/w^2$, which explains the change of sign and the extra factor. Thus, \n$$f(z)dz = \\frac{-1}{w^2} f(1/w) dw.$$\nThe \"residue of $f$ at $\\infty$\" is the residue at $0$ of $\\frac{-1}{w^2} f(1/w) dw$.", "meta": {"post_id": 629495, "input_score": 21, "output_score": 37, "post_title": "Intuition behind the residue at infinity"}}
{"input": "Can $18$ consecutive positive integers be separated into two groups, such that their product is equal?  We cannot leave out any number and neither we can take any number more than once. \nMy work:\nWhen the smallest number is not  $17$ or its multiple, there cannot exist any such arrangement as $17$ is a prime.  \nWhen the smallest number is a multiple of $17$ but not of $13$ or $11$, then no such arrangement exists.  \nBut what happens, when the smallest number is a multiple of $ 17 $ and $13$ or $11$ or both?\nPlease help!", "output": "If $18$ consecutive positive integers could be separated into two groups with equal products, then the product of all $18$ integers would be a perfect square. However, the product of two or more consecutive positive integers can never be a perfect square, according to a famous theorem of P. Erd\u0151s, Note on products of consecutive integers, J. London Math. Soc. 14 (1939), 194-198.", "meta": {"post_id": 638646, "input_score": 70, "output_score": 51, "post_title": "Can $18$ consecutive integers be separated into two groups,such that their product is equal?"}}
{"input": "I have a sequence $T_1,T_2,\\ldots$ of independent exponential random variables with paramter $\\lambda$. I take the sum $S=\\sum_{i=1}^n T_i$ and now I would like to calculate the probability density function.\nWell, I know that $P(T_i>t)=e^{-\\lambda t}$ and therefore $f_{T_i}(t)=\\lambda e^{-\\lambda t}$ so I need to find $P(T_1+\\cdots+T_n>t)$ and take the derivative. But I cannot expand the probability term, you have any ideas?", "output": "The usual way to do this is to consider the moment generating function, noting that if $S = \\sum_{i=1}^n X_i$ is the sum of IID random variables $X_i$, each with MGF $M_X(t)$, then the MGF of $S$ is $M_S(t) = (M_X(t))^n$.  Applied to the exponential distribution, we can get the gamma distribution as a result.\nIf you don't go the MGF route, then you can prove it by induction, using the simple case of the sum of the sum of a gamma random variable and an exponential random variable with the same rate parameter.  Let's actually do this.  Suppose $Y \\sim {\\rm Gamma}(a,b)$ and $X \\sim {\\rm Exponential}(b)$ are independent, so that $$f_Y(y) = \\frac{b^a y^{a-1} e^{-by}}{\\Gamma(a)} \\mathbb 1(y > 0), \\quad f_X(x) = be^{-bx} \\mathbb 1(x > 0), \\quad a, b > 0.$$  Then, we notice that if $a = 1$, $Y$ would also be exponential (i.e., the exponential distribution is a special case of the Gamma with $a = 1$).  Now consider $Z = X+Y$.  The PDF is $$\\begin{align*} f_Z(z) &= \\int_{y=0}^z f_Y(y) f_X(z-y) \\, dy \\\\ &= \\int_{y=0}^z \\frac{b^{a+1} y^{a-1} e^{-by} e^{-b(z-y)}}{\\Gamma(a)} \\, dy \\\\ &= \\frac{b^{a+1} e^{-bz}}{\\Gamma(a)} \\int_{y=0}^z y^{a-1} \\, dy \\\\ &= \\frac{b^{a+1} e^{-bz}}{\\Gamma(a)} \\cdot \\frac{z^a}{a} = \\frac{b^{a+1} z^a e^{-bz}}{\\Gamma(a+1)}. \\end{align*}$$  But this is just a gamma PDF with new shape parameter $a^* = a+1$.  So, it is easy to see by induction that the sum of $n$ IID exponential variables with common rate parameter $\\lambda$ is gamma with shape parameter $a = n$, and rate parameter $b = \\lambda$.", "meta": {"post_id": 655302, "input_score": 23, "output_score": 38, "post_title": "Gamma Distribution out of sum of exponential random variables"}}
{"input": "Prove that a closed ball in a metric space is a closed set\n\nMy attempt: Suppose $D(x_0, r)$ is a closed ball. We show that $X \\setminus D $ is open. In other words, we need to find an open ball contained in $X \\setminus D$. \nPick $$t \\in X-D \\implies d(t,x_0) > r \\implies d(t,x_0) - r > 0 $$ Let $B(y, r_1)$ be an open ball, and pick $z \\in B(y,r_1)$. Then, we must have $d(y,z) < r_1 $. We need to choose $r_1$ so that $d(z,x_0) > r$. Notice by the triangle inequality\n$$ d(x_0,t) \\leq d(x_0,z) + d(z,t) \\implies d(z,x_0) \\geq d(x_0,t) - d(z,t) > d(x_0,t) - r_1.$$\nNotice, if we pick $r_1 = d(t,x_0)-r$ then we are done.\nIs this correct?", "output": "Your proof consists of some correct steps done in the wrong order, which makes it something other than a valid proof. It looks more like scratchwork done in preparation for a proof.  I  rewrite it below, with some of the more important additions in bold. I will also change $t$ to $y$ throughout; when you wrote \"$y$\" you probably meant the same thing as \"$t$\".\n\nSuppose $D(x_0, r)$ is a closed ball. We show that $X\\setminus D(x_0,r) $ is open. In other words, for every point $y\\in X\\setminus D(x_0,r)$ we need to find an open ball contained in $X \\setminus D$ with center $y$.\nSince $y \\in X\\setminus D(x_0,r)$, it follows that $d(y,x_0) > r$, so $d(y,x_0) - r > 0 $. Let $r_1 = d(y,x_0)-r$.\nI claim that the open ball $B(y, r_1)$ is contained in $X\\setminus D(x_0,r)$. To prove this, consider any $z \\in B(y,r_1)$.  Notice by the triangle inequality\n$$ d(x_0,y) \\leq d(x_0,z) + d(z,y) \\implies d(z,x_0) \\geq d(x_0,y) - d(z,y) > d(x_0,y) - r_1 =r.$$\nThis shows $z\\in X\\setminus D(x_0,r)$, which completes the proof.", "meta": {"post_id": 661759, "input_score": 22, "output_score": 39, "post_title": "A closed ball in a metric space is a closed set"}}
{"input": "If $X$ and $Y$ are independent random variables, are the statements below true\n$$E(e^{X+Y} ) = E(e^X)\\times E(e^Y)$$\nand \n$$E(X^2\\times Y^2) = E(X^2)\\times E(Y^2),$$\nwhere $E(\\cdot)$ = expectation?", "output": "Yes, because $E(PQ)=E(P)E(Q)$ when the random variables $P$ and $Q$ are independent. In each case you can simply define new random variables that are functions of the first.\n\n$P=e^X, Q=e^Y \\implies PQ=e^X e^Y= e^{X+Y}$\n$P=X^2, Q=Y^2$\n\n\nIf $X$ and $Y$ are independent , how do you knew that $X^2$ and $Y^2$ are independent?\n\nThis can be looked at in two ways.\nFirst of all, and this is what I was relying on above, you can appeal to our everyday understanding of how the world works. E.g.  let $X$ and $Y$ be the results of rolling two dice. $X$ and $Y$ are independent. Now, let's say we square each result. We clearly haven't introduced any dependency by doing this, so $P=X^2$ and $Q=Y^2$ are independent.\nSecondly, you can look more deeply at the underlying mathematics. The general result is JohnK's answer and a specific instance of that is justified in korrok's answer.\nExpectation of two random variables $X$, $Y$ is defined as the sum of the products of the values of those random variables times their joint probabilities. For continuous random variables this is\n$$\\mathrm{E}(XY)=\\int\\int xy \\; f_{XY}(x,y) \\;\\mathrm{d}x\\mathrm{d}y $$\nwhere the integrals are over the range that $X$ and $Y$ can take, and $f_{XY}$ is the joint probability density of $X$ and $Y$.\nIn the more general case of $\\alpha(x)$ as some function of $x$ and $\\beta(y)$ as some function of $y$ (you had $\\alpha = \\beta = x\\mapsto x^2$), the expectation of their product is defined similarly.\n$$\\mathrm{E}(\\alpha(X)\\beta(Y))=\\int\\int \\alpha(x)\\beta(y) \\; f_{XY}(x,y) \\;\\mathrm{d}x\\mathrm{d}y $$\nBecause $X$ and $Y$ are independent, you can factorize the probability density  $f_{XY}$ into the product of the probability density $f_X(x)$ for $X$ and the probability density $f_Y(y)$ for $Y$, i.e.: $f_{XY} = f_X(x)f_Y(y)$. So\n$$\\mathrm{E}(\\alpha(X)\\beta(Y))=\\int\\int \\alpha(x)\\beta(y) \\; f_X(x)f_Y(y)\\;\\mathrm{d}x\\mathrm{d}y $$\nRearranging the integrand we see that the integrand is the product of terms that only depend on $x$ and terms that only depend on $y$ so the integral itself can be split into two. Each of those two integrals is the definition of an expectation.\n$$\\begin{align}\n\\mathrm{E}(\\alpha(X)\\beta(Y))\n&= \\int\\int \\alpha(x)f_X(x) \\; \\beta(y)f_Y(y)\\;\\mathrm{d}x\\mathrm{d}y \\\\\n&= \\int \\alpha(x)f_X(x) \\;\\mathrm{d}x \\int\\beta(y)f_Y(y)\\;\\mathrm{d}y \\\\\n&= \\mathrm{E}( \\alpha(X))\\mathrm{E}(\\beta(Y))\n\\end{align}$$", "meta": {"post_id": 667911, "input_score": 23, "output_score": 34, "post_title": "Expected value of the product of functions of two independent random variables"}}
{"input": "If there are 5 points on the surface of a sphere, then there is a closed half sphere, containing at least 4 of them.\nIt's in a pigeonhole list of problems. But, I think I have to use rotations in more than 1 dimension.\nRegards", "output": "Pick two distinct points out of your 5 (if all 5 are identical then they clearly all lie in a single hemisphere).  These two points define at least one great circle (if they're antipodal, they define infinitely many); pick a great circle they define.  This circle then cuts the sphere into two hemispheres.  Now pigeonhole the other three points between these two hemispheres.", "meta": {"post_id": 675646, "input_score": 40, "output_score": 74, "post_title": "if there are 5 points on a sphere then 4 of them belong to a half-sphere."}}
{"input": "I am looking for books or papers which tell me something about representation theory of finite groups over $\\mathbb{Q}$ (or finite extensions thereof which are not splitting fields of the group algebra).\nTo be more precise, I'd like to learn of theorems which, for example, provide me with the following information, given a finite group $G$: how many irreducible representations with coefficients of $\\mathbb{Q}$ are there, how can I compute their characters and can this information somehow be obtained from the \"usual\" character table of the group.\nI have some background in ordinary representation theory over $\\mathbb{C}$.", "output": "This is a well-established theory, which is very nicely presented in the second volume of the two-volume work of Curtis and Reiner. Here is the gist of it:\nSince a rational representation is also a complex representation, you still have character theory to help you. In particular, a rational representation is still uniquely determined by its character, which, of course, only take values in $\\mathbb{Q}$.\nSo suppose that you wanted to do the converse: start with the knowledge of all the complex representations (including the full character table), and construct all the irreducible rational ones. The absolute Galois group of $\\mathbb{Q}$ acts on the set of complex representations by acting on each entry in each matrix, and so also acts on the set of characters. If $\\chi$ is the character of an irreducible rational representation, then it must be invariant under the Galois action. In particular, if $\\phi$ is an irreducible complex character sitting inside $\\chi$, then every Galois conjugate $\\phi^\\sigma$ also has to sit in $\\chi$ with the same multiplicity. So the first step is to take an irreducible complex character $\\phi$ and to \"rationalise\" it by $\\chi = \\sum_{\\sigma\\in \\text{Gal}}\\phi^\\sigma$, with the sum running over the distinct Galois conjugates of $\\phi$.\nSo now you have a $\\mathbb{Q}$-valued character, but it does not mean that the corresponding representation can be realised over $\\mathbb{Q}$ (as an example, think of the standard representation of the quaternion group $Q_8$). However, there is a unique minimal integer $m(\\chi)$ such that $m(\\chi)\\chi$ can be realised over $\\mathbb{Q}$, and this representation is in fact irreducible over $\\mathbb{Q}$. This $m(\\chi)$ is called the Schur index of $\\chi$, and is also nicely treated in Curtis and Reiner, but also in Isaacs for example. It is now easy to see that all irreducible rational representations arise in this way. If you are interested in general number fields, then you only have to average over the Galois conjugates over that field, but you may still have a Schur index flying around.\nThe answer to your question about the number of irreducible rational representations is really neat: it is equal to the number of conjugacy classes of cyclic subgroups of $G$ (as opposed to conj classes of elements, like in the complex case). I seem to remember that this is proven, among other places, in Serre's book on representation theory. This is one of the ways of stating Artin's induction theorem.", "meta": {"post_id": 676303, "input_score": 18, "output_score": 40, "post_title": "Representation theory over $\\mathbb{Q}$"}}
{"input": "I want to show that for $n>0$, $2^n$ and $2^n + 1$ have the same number of digits.\nWhat I did was I found that the formula for the number of digits of a number $x$ is $\\left \\lfloor{\\log_{10}(x)}\\right \\rfloor + 1$, so basically if I subtract that formula with $x = 2^n$ with the formula with $x = 2^n + 1$, I should get zero.\n$\\left \\lfloor{\\log_{10}(2^n)}\\right \\rfloor + 1 - (\\left \\lfloor{\\log_{10}(2^n + 1)}\\right \\rfloor + 1) = \\left \\lfloor{\\log_{10}(2^n)}\\right \\rfloor  -\\left \\lfloor{\\log_{10}(2^n + 1)}\\right \\rfloor $.\nAt this point, I don't know of a way to simplify this any further to make it equal $0$. I thought about mentioning that $\\log_{10}(x)$ increases slower than $x$ as $x$ increases, which would mean the difference of the floor of the logs of two consecutive numbers may be close to zero, but that doesn't cut it to prove that $2^n, 2^n + 1$ have exactly the same number of digits.\nAre there any special floor or log properties I could use to make this easier? Any help is appreciated.", "output": "Note the only way $2^n+1$ can have one more digit than $2^n$ is if $2^n$ ended in a $9$ (actually ends is $\\cdots 999999$ but that is not important). $2^n$ can never end in a $9$.", "meta": {"post_id": 685618, "input_score": 16, "output_score": 53, "post_title": "Show that these two numbers have the same number of digits"}}
{"input": "Is there some sort of intuition or a good ilustrative example for random variables being $\\sigma$-algebra measurable? I understand the definition, but when looking at martingales, the meaning of random variables being measurable eludes me. So my question is mainly aimed on the case of martingales where sequence of random variables is adapted to some filtration.\nIn Interpretation of sigma algebra, the asker asks (among many others) a similar question, but I don't think it contains an actual answer to this question.", "output": "Maybe this can help you to understand the concept of conditional expectation, behind your question.\nSuppose you have a probability space $(\\Omega, \\mathcal P (\\Omega), \\mathbb{P})$, where $\\mathcal P (\\Omega)$ denotes the set of all possible subsets of $\\Omega$ (evidently, a $\\sigma$-algebra),  and $\\mathbb{P}$ is a probability measure (in this case, a function from $\\mathcal P (\\Omega)$ to [0,1]).\nSuppose you have a random variable (measurable function) $X:(\\Omega, \\mathcal P (\\Omega)) \\to (\\mathbb{R}, \\mathcal B (\\mathbb R ))$, where $\\mathcal B (\\mathbb R )$ is the usual Borel $\\sigma$-algebra.\nTake as a sub-$\\sigma$-algebra the trivial one, $\\mathcal F = \\{\\emptyset, \\Omega\\}$. Suppose we only know the conditional expectation $\\mathbb E(X | \\mathcal F)$, but not $X$ itself. How much do we know about X? Well,  $Y = \\mathbb E(X | \\mathcal F)$ is a random variable, $\\mathcal F$/ $\\mathcal B (\\mathbb R )$- measurable. From Y, we can only determine ONE thing (think about this!):\n$$\\mathbb E(Y) = \\mathbb E(\\mathbb E(X | \\mathcal F)) = \\mathbb E X.$$\nSo, what is $\\mathbb{E}(X | \\mathcal F)$? It is the most simplified knowledge that we can have; we arrive at this if we determine the expectation of the random variable but know nothing about its values in particular events (in $\\mathcal P (\\Omega)$).\n(In fact, $Y$ is constant... otherwise, it would not be measurable.) \nSuppose now that we enlarge this $\\sigma$-algebra, say to $\\mathcal F' = \\{\\emptyset, A, A^c, \\Omega\\}$, for some non-trivial set $A$. Again, suppose that we only know $\\mathbb{E}(X | \\mathcal F')$, not X. Then, we can determine three things about the variable:\n$$\\mathbb E(X 1_A), \\, \\mathbb E(X 1_{A^c}) \\text{ and } \\mathbb E (X).$$\nConclusion: a bigger $\\sigma$ algebra implies more knowledge about the random variable X (we are interested in that one)!\nCheck that in the extreme case, when $\\mathcal F'' =\\mathcal P (\\Omega)$, the knowledge of $\\mathbb E (X|\\mathcal F'')$ allows us to determine all the expected values $\\mathbb E(X 1_{\\{X=x\\}})= x\\mathbb P (X=x)$, because the events $\\{X=x\\}$ are contained in $\\mathcal F''$ (like every other subset). If $X$ only take a finite number of different values (for instance, when $\\Omega$ is finite), these expectations are enough to determine  the probability of all the events $\\{X=x\\}$. (When $X$ is continuous, the above reasoning is not very useful, for the subsets $\\{X=x\\}$ have probability zero and the expectations above are zero too. Anyway, by the general properties of the conditional expectation, $\\mathbb E(X|\\mathcal F'') = X$, because $X$ is $F''$-measurable. In this sense, we can say that the variable is recovered from its conditional expectation.)", "meta": {"post_id": 690531, "input_score": 45, "output_score": 38, "post_title": "Intuition for random variable being $\\sigma$-algebra measurable?"}}
{"input": "Say we have an $n\\times m$ matrix $X$. What are the specific properties that $X$ must have so that $A=X^TX$ invertible?\nI know that when the rows and columns are independent, then matrix $A$ (which is square) would be invertible and would have a non-zero determinant. However, what confuses me is, what sort of conditions must we have on each row of $X$ such that $A$ would be invertible. \nIt would be very nice to have a solution of the form:\n\nwhen $n > m$ then $X$ must have...\nwhen $n < m$ then $X$ must have...\nwhen $n = m$ then $X$ must have...\n\nI think in the 3rd case we just need $X$ to be invertible but I was unsure of the other two cases.", "output": "Precisely when the rank of $X$ is $m$ (which forces $n\\geq m$). \nThe key observation is that for $v\\in\\mathbb R^m$, $Xv=0$ if and only if $X^TXv=0$. For the non-trivial implication, if $X^TXv=0$, then $v^TX^TXv=0$, that is $(Xv)^TXv=0$, which implies that $Xv=0$.\nIf the rank of $X$ is $m$, this means that $X$ is one-to-one when acting on $\\mathbb R^m$. So by the observation, $X^TX$ is one-to-one, which makes it invertible (as it is square). \nConversely, if the rank of $X$ is less than $m$, there exists $v\\in\\mathbb R^m$ with $Xv=0$. Then $X^TXv=0$, and $X^TX$ cannot be invertible.", "meta": {"post_id": 691812, "input_score": 38, "output_score": 35, "post_title": "Proof of when is $A=X^TX$ invertible?"}}
{"input": "I am working on a physics task, and reduced it to the following equation for $y$:\n$$\\frac{1}{4y^3}-\\frac{2}{(y^2+b^2)^{\\frac{3}{2}}}=0$$\nI handed it to Mathematica, and it gave me two real solutions, $$y_{1,2} = \\pm\\frac{b}{\\sqrt{3}},$$\nalong with some complex ones.\nMy question is, how can I see this? I mean, how can I solve such an equation by hand? I tried moving stuff around a bit, but all I found was a sextic equation, which I really didn't feel like approaching. Is there any easier way? Does it lower the hassle, if only real solutions are to be found?", "output": "First, get rid of the denominators,\n$$\\begin{align}\n\\frac{1}{4y^3} - \\frac{2}{(y^2+b^2)^{3/2}} &= 0\\\\\n\\iff \\frac{1}{4y^3} &= \\frac{2}{(y^2+b^2)^{3/2}}\\\\\n\\iff (y^2+b^2)^{3/2} &= 8y^3.\n\\end{align}$$\nThen, raise it to the $2/3$-th power to simplify it, introducing a third root of unity,\n$$\\begin{align}\ny^2 + b^2 &= 4\\rho y^2\\\\\n\\iff b^2 &= (4\\rho-1)y^2\\\\\n\\iff y &= \\pm \\frac{b}{\\sqrt{4\\rho-1}}.\n\\end{align}$$\nChoosing $\\rho = 1$ as the third root of unity yields the two real solutions. $\\rho = e^{\\pm 2\\pi i/3}$ yields non-real solutions.", "meta": {"post_id": 694335, "input_score": 11, "output_score": 55, "post_title": "Is it possible to solve this equation by hand?"}}
{"input": "As you know, we use the \"Secretary Problem\" to choose the single best candidate. Now I would like to know can we use this rule to find the worst candidate, too? If yes, how to accomplish this?", "output": "Yes. \nThe algorithm simply finds the participant who is optimal according to some property, but does not care what that property means (as long as it defines an ordering relation on the candidates). Changing that property from being good to being bad (or whatever else) is just a definition without algorithmic relevance.", "meta": {"post_id": 697501, "input_score": 14, "output_score": 45, "post_title": "Can I use the \"Secretary Problem\" to find the worst candidate, too?"}}
{"input": "How can I prove that\n$$\n\\lim_{n\\rightarrow\\infty}\\frac{\\left\\lfloor x^{n+1} \\right\\rfloor}{\\left\\lfloor x^n \\right\\rfloor}=x,\n$$\nwhenever $x>1$. Here $\\left\\lfloor \\cdot\\right\\rfloor$ denotes the floor function,\nor the integer part function. \nThe integer part $\\lfloor z\\rfloor$ of $z$ is the largest integer, which does not exceed $z$.\nThanks for your answer.", "output": "Since $y-1< \\lfloor y\\rfloor\\le y$, for every $y\\in\\mathbb R$, then\n$$\n\\frac{x^{n+1}-1}{x^{n}}<\\frac{\\lfloor x^{n+1}\\rfloor}{\\lfloor x^n\\rfloor}< \\frac{x^{n+1}}{x^n-1},\n$$\nand hence\n$$\nx-\\frac{1}{x^n}<\\frac{\\lfloor x^{n+1}\\rfloor}{\\lfloor x^n\\rfloor}<x+\\frac{x}{x^n-1},\n$$\nor\n$$\n-\\frac{1}{x^n}<\\frac{\\lfloor x^{n+1}\\rfloor}{\\lfloor x^n\\rfloor}-x<\\frac{x}{x^n-1}.\n$$\nSince both\n$\n-\\frac{1}{x^n},\\,\\frac{x}{x^n-1}\\to 0, \\quad\\text{as}\\quad n\\to\\infty,\n$\nthen $\\frac{\\lfloor x^{n+1}\\rfloor}{\\lfloor x^n\\rfloor}\\to x$.", "meta": {"post_id": 707294, "input_score": 8, "output_score": 34, "post_title": "If $\\,x>1$, then $\\lim\\limits_{n\\rightarrow\\infty}\\frac{\\left\\lfloor x^{n+1} \\right\\rfloor}{\\left\\lfloor x^n \\right\\rfloor}=x$."}}
{"input": "I have seen descriptions of the \"line with two origins\" using quotient spaces. My professor has defined it in an alternate way. However, I can not wrap my head around how the following descriptions forms a line with two origins. \nConsider $X=\\mathbb{R} \\setminus \\{0\\} \\cup \\{p,q\\}$, that is $X$ is the union of the reals minus $0$, and two points. Consider sets of the type\n$U_a = (-a,0) \\cup \\{p\\} \\cup (0,a)$\n$V_a=(-a,0) \\cup \\{q\\} \\cup (0,a)$\nwhere $a >0$. And let\n$\\mathcal{B}=\\{U_a\\}_{a>0} \\cup \\{V_a\\}_{a >0} \\cup \\{ \\text{all open intervals of} \\hspace{2mm} \\mathbb{R} \\hspace{2mm} \\text{not containing the origin} \\}$\nThen $\\tau=\\{\\bigcup_{\\alpha} B_{\\alpha} \\big | B_{\\alpha} \\in \\mathcal{B} \\}$.\nHow is this a line with two origins?", "output": "Let's make the construction a little simpler.\nWe start with a line, $\\Bbb R$.\n\nThen we removed the origin $0$, so $\\Bbb R\\setminus\\{0\\}$.  Let's call that $\\def\\rstar{\\mathbb R^\\star}\\rstar$.\n\nThen we add a new point $p$\u2014a new point, not a real number\u2014and we have $\\Bbb \\rstar\\cup\\{p\\}$.\nAnd the way we add back this point is special.  Where before we had some open set $U$ that contained $0$, we now have an open set $U\\setminus\\{0\\}\\cup\\{p\\}$.  This set is exactly $U$, but with $0$ replaced by $p$. Where before we had an open set $V$ not containing $0$, we keep $V$ unchanged; $V$ is still open.\n\nSo the open sets are just like the ones we had before, except that $0$ has been replaced by $p$.  Topologically, the point $p$ behaves just like $0$ did before.  Where before we had $0$ in some open set $U$, we now have $p$ in some analogous open set $U\\setminus\\{0\\}\\cup\\{p\\}$. $p$ is a perfect replacement for the origin $0$ that we deleted.  Really it's just $0$, but with another name.\nThis new space, $\\Bbb R\\setminus\\{0\\}\\cup\\{p\\}$, is exactly $\\Bbb R$, except that $0$ has been removed and replaced  by $p$.  It's easy to show that this space is topologically identical to $\\Bbb R$.  The homeomorphism is particularly simple: it is the identity function, except that it takes $0$ to $p$, because $\\Bbb R\\setminus\\{0\\}\\cup\\{p\\}$ has  has $p$ instead of $0$.\nGot that?\n\nNow we add another new point $q$, in exactly the same way we added $p$: if $U$ was open before, and $0\\in U$, then $U\\setminus\\{0\\}\\cup\\{q\\}$ is open now.  This set is exactly $U$, but with $0$ replaced by $q$.\n\nSo now we have something like $\\Bbb R$, except it has this extra point $q$.  But $q$ has all the same properties that $p$ has!  And in particular, just as $p$ was a perfect replacement for the origin $0$ that we deleted, $q$ is also a perfect replacement for the origin $0$ that we deleted.\nAnd just as $\\rstar\\cup\\{p\\}$ was homeomorphic to $\\Bbb R$, so is  $\\rstar\\cup\\{q\\}$ homeomorphic to $\\Bbb R$.\nBut this new space is $\\rstar\\cup\\{p,q\\}$ and has both $p$ and $q$. We can delete either one of them and get a space identical to $\\Bbb R$.\nWe deleted the origin $0$ and replaced it with $p$ and with $q$, so we now have something like $\\Bbb R$, except that instead of one origin it has two, $p$ and $q$.\n\nSo it is called the line with two origins.", "meta": {"post_id": 709777, "input_score": 10, "output_score": 36, "post_title": "The Line with two origins"}}
{"input": "I have been reading Terry Tao's notes on Real Analysis and there's a part he just says, but does not really explain, so I am wondering if someone here would. The notes are http://terrytao.wordpress.com/2010/10/02/245a-notes-4-modes-of-convergence/ and my particular question is from Section 4, Corollary 3. It goes as follows,\nLet $f_n \\rightarrow f$ in $L^1$ then there exists a sub sequence $(f_{n_j}) \\subset (f_n)$ such that $f_{n_j} \\rightarrow f$ pointwise a.e. Moreover $(f_{n_j})$ converges almost uniformly to $f$.\nThe proof he gives is simply that since $\\|f_n-f\\|_1 \\rightarrow 0$ as $n \\rightarrow \\infty$ we can pick a sub sequence such that $\\|f_{n_j}-f\\|_1<2^{-j}$ which is enough to show pointwise a.e and almost uniform convergence. But what allows you to pick such a sub sequence is it maybe some Cauchy property or is it some weird construction? Then how do you go from that to pointwise a.e and even almost uniform convergence. I am assuming that for almost uniform, you do something similar to Egorov's theorem without the assumption the domain of $f$ has finite measure. Also I am aware that if you get almost uniform, you immediately have pointwise a.e, but I'd like to see how to get to both. Thank you.", "output": "Choose $N_{k}$ such that $N_{1} \\le N_{2} \\le N_{3} \\le \\cdots$ and such that $\\|f_{m}-f_{n}\\| < 1/2^{k}$ whenever $m, n \\ge N_{k}$. This is possible because $\\{ f_{n}\\}_{n=1}^{\\infty}$ is a Cauchy sequence. Then $\\{ f_{N_{k}}\\}_{k=1}^{\\infty}$ is a subsequence such that $\\|f_{N_{l}}-f_{N_{m}}\\| < 1/2^{k}$ whenever $l,m \\ge k$. Then\n$$\n           f_{N_{m}}=f_{N_{1}}+\\sum_{l=1}^{m}f_{N_{l+1}}-f_{N_{l}}\n$$\nconverges pointwise a.e. absolutely because\n$$\n         g_{m}=|f_{N_{1}}| + \\sum_{l=1}^{m}|f_{N_{l+1}}-f_{N_{l}}|\n$$\nconverges pointwise a.e. to an extended real function $0 \\le g \\le \\infty$ such that, by the monotone convergence theorem,\n$$\n         \\int g\\,d\\mu = \\int |f_{N_{1}}|d\\mu+\\sum_{l=1}^{\\infty}\\int |f_{N_{l+1}}-f_{N_{l}}|d\\mu = \\|f_{N_{1}}\\|+\\sum_{l=1}^{\\infty}\\|f_{N_{l+1}}-f_{N_{l}}\\| < \\infty.\n$$\nSo $g < \\infty$ a.e., which means that $\\lim_{l}f_{N_{l}}$ converges pointwise a.e.. to an $L^{1}$ function.", "meta": {"post_id": 714744, "input_score": 26, "output_score": 38, "post_title": "$L^1$ convergence gives a pointwise convergent subsequence"}}
{"input": "Let $A$ be a ring (which might or might not be commutative), and let $M,N$ and $K$ be three bi-modules over $A$.\nThere are two hom-tensor adjunctions. One says that\n\n$Hom_A(M\\otimes_A N, K) \\cong Hom_A(M,Hom_A(N,K))$.\n\nThe other says that\n\n$Hom_A(M\\otimes_A N, K) \\cong Hom_A(N,Hom_A(M,K))$.\n\nAre these isomorphisms of bimodules?\nIf so, does this mean that the two bimodules $Hom_A(N,Hom_A(M,K))$ and $Hom_A(M,Hom_A(N,K))$ are isomorphic?", "output": "Be careful. It's cleanest to describe the tensor-hom adjunction with three different rings instead of one, to make it as hard as possible to accidentally write down the wrong thing, so let $A, B, C$ be three different rings, let $_A M_B$ be an $(A, B)$-bimodule, let $_B N_C$ be a $(B, C)$-bimodule, and let $_A K_C$ be an $(A, C)$-bimodule. Then\n$$\\text{Hom}_C(M \\otimes_B N, K) \\cong \\text{Hom}_B(M, \\text{Hom}_C(N, K))$$\nas $(A, A)$-bimodules, and\n$$\\text{Hom}_A(M \\otimes_B N, K) \\cong \\text{Hom}_B(N, \\text{Hom}_A(M, K))$$\nas $(C, C)$-bimodules.\nSpecializing to the case that $A = B = C$ shows that your notation is sloppy (to be fair, so is mine): when you write $\\text{Hom}_A$ you haven't been careful about whether this means left $A$-module or right $A$-module homomorphisms, and it has different meanings in the different parts of your adjunctions unless $A$ is commutative and $M, N, K$ are plain $A$-modules, in which case there's no need to make left/right distinctions.\n(Specifically, $\\text{Hom}_A$ means left the second, fifth, and sixth times you used it, but right the first, third, and fourth times.)", "meta": {"post_id": 723368, "input_score": 15, "output_score": 38, "post_title": "Hom-tensor adjunctions"}}
{"input": "Would I be right to think that $$\\int dx \\,\\,\\,\\frac{\\partial}{\\partial x} f(x,y)=f(x,y)$$\nOr are there pathological cases?", "output": "If $x$ and $y$ are independent variables (and thus the $y$ is held constant during integration), then it is true that\n$$\n\\int \\frac{\\partial f}{\\partial x} dx = f(x,y) + C(y)\n$$\nwhere $C(y)$ is equivalent to the integration constant for the univariate case. As such, up to the \"constant\", you are right.\nIf $y=y(x)$, then it is not that simple. For instance, if $f(x,y)=x^2-xy+y^2$ and you integrate along the line $y=2x$, then you are actually integrating\n$$\n\\int \\frac{\\partial f}{\\partial x} dx = \\int (2x-y) dx = \\int 0 dx = 0\n$$", "meta": {"post_id": 754742, "input_score": 30, "output_score": 44, "post_title": "Integrating a Partial Derivative"}}
{"input": "Ordinary trigonometric functions are defined independently of exponential function, and then shown to be related to it by Euler's formula. \nCan one define hyperbolic cosine so that the formula\n$$\\cosh{x}=\\dfrac{e^x+e^{-x}}{2}$$\nbecomes something to be proven?", "output": "The more-geometrically-minded of us take $\\cosh u$ and $\\sinh u$ to be defined via the \"unit hyperbola\", $x^2 - y^2 = 1$, in a manner directly analogous to $\\cos\\theta$ and $\\sin\\theta$. Specifically, given $P$ a point on the hyperbola with vertex $V$, and defining $u$ as twice(?!) the area of the hyperbolic sector $OVP$, then $\\cosh u$ and $\\sinh u$ are, respectively the $x$- and $y$-coordinates of $P$.\n\nJust as in circular trig, we can assign measures $u$ (in \"hyperbolic radians\") to angles ---from the flat angle (when $u=0$) to half a right angle (when $u=\\infty$)--- and associate those measures with the lengths of the corresponding $\\cosh$ and $\\sinh$ segments. And, just as in circular trig (prior to the advent of imaginary numbers), we might be forgiven for suspecting that the correspondences $u \\leftrightarrow \\cosh u$ and $u \\leftrightarrow \\sinh u$ are \"non-arithmetical\", which is to say: that no arithmetical formula converts angle measures to their associated trig values.\nHowever, it turns out that the correspondences are not non-arithmetical; to find the appropriate arithmetical conversion formula, all we need is a bit of calculus ...\n\nEdit. (Two years later!) Check the edit history for an inelegant argument that I now streamline with the help of this trigonograph, in which lengths from the unit hyperbola have been scaled by $\\sqrt{2}$ (and, thus, areas by $2$):\n\nBecause the hyperbola is rectangular, we have that $|OX|\\cdot|XY|$ is a constant (here, $1$), which guarantees that the regions labeled $v$ have the same area (namely, $1/2$), and therefore that the regions labeled $u$ have the same area (namely, $u$). Now, the bit of calculus I promised, to evaluate $u$ as the area under the reciprocal curve:\n$$u = \\int_1^{|OX|}\\frac{1}{t}dt = \\ln|OX| \\quad\\to\\quad |OX| = e^{u} \\quad\\to\\quad |XY| = \\frac{1}{e^u}$$\nWith that, we clearly have\n$$2\\,\\sinh u \\;=\\; e^{u}- e^{-u} \\qquad\\qquad 2\\,\\cosh u \\;=\\; e^{u} + e^{-u}$$\nas desired. Easy-peasy!\nEnd of edit.\n\nThat hyperbolic radians are defined via doubling the area of a hyperbolic sector may seem at odds with the common definition of circular radians in terms of arc-length, but it's hard to argue with success, given the elegance of the formulas above. Even so, the hyperbolic twice-the-sector-area definition can be seen as directly analogous to the circular case, since circular radians are also definable as \"twice-the-sector-area\": In the unit circle, the sector with angle measure $\\pi/2$ radians has area $\\pi/4$ (it's a quarter-circle), the sector with angle measure $\\pi$ radians has area $\\pi/2$ (it's a half-circle), and the \"sector\" with angle measure $2\\pi$ radians has area $\\pi$ (it's the full circle); in these, and all other, cases, the angle measure is twice the sector area.", "meta": {"post_id": 757091, "input_score": 24, "output_score": 46, "post_title": "Alternative definition of hyperbolic cosine without relying on exponential function"}}
{"input": "I'm trying to find an example of a non-separable subspace of a separable space.\nWhat kind of examples are there?", "output": "If you don't care about separation axioms (e.g. Hausdorff, etc.) then you can take the following example:\n$\\Bbb R$ with the topology defined as $U$ is open if and only if $0\\in U$ or $U=\\varnothing$. Then $\\{0\\}$ is dense in this topology so the space is separable. \nBut $\\Bbb R\\setminus\\{0\\}$ is discrete (since given $x\\in\\Bbb R\\setminus\\{0\\}$ the set $\\{x,0\\}$ is open, so $\\{x\\}$ is relatively open). And uncountable discrete spaces cannot be separable.", "meta": {"post_id": 758424, "input_score": 23, "output_score": 35, "post_title": "Give an example of a non-separable subspace of a separable space"}}
{"input": "The question is: Prove that if $\\lambda$ is an eigenvalue of a matrix A with corresponding eigenvector x, then $\\lambda^2$ is an eigenvalue of $A^2$ with corresponding eigenvector x.\nI assume I need to start with the equation $Ax=\\lambda x$ and end up with $A^2 x=\\lambda^2 x$ but between those I am kind of lost. I have manipulated the equations several different ways and just can't seem to end up where I need to be. Help would be greatly appreciated as I believe this will be on a test tomorrow.", "output": "We know $Ax = \\lambda x$.  Then $A \\lambda x = \\lambda(Ax) = \\lambda^2x = A^2x$.  Putting this into a more readable mathematical sentence, we get:\n$$A^2x = A(Ax) = A\\lambda x = \\lambda(Ax) = \\lambda^2x$$\nYou were done and didn't realize it.  :)", "meta": {"post_id": 767835, "input_score": 19, "output_score": 35, "post_title": "Proving Eigenvalue squared is Eigenvalue of $A^2$"}}
{"input": "Does anyone know if this number is algebraic or transcendental, and why?\n$$\\sum\\limits_{n = 1}^\\infty  {10}^{ - n(n + 1)/2}  = 0.1010010001000010000010000001 \\ldots $$", "output": "The number $0.1010010001000010000010000001\\ldots$ is transcendental.\nConsider following three Jacobi theta series defined by\n$$\\begin{align}\n\\theta_2(q) \n&= 2q^{1/4}\\sum_{n\\ge 0} q^{n(n+1)} = 2q^{1/4}\\prod_{n=1}^\\infty (1-q^{4n})(1 + q^{2n})\\\\\n\\theta_3(q)\n&= \\sum_{n\\in\\mathbb{Z}} q^{n^2} = \\prod_{n=1}^\\infty (1-q^{2n})(1+ q^{2n-1})^2\\\\\n\\theta_4(q)\n&= \\theta_3(-q) =\n\\sum_{n\\in\\mathbb{Z}} (-1)^n q^{n^2} = \\prod_{n=1}^\\infty (1-q^{2n})(1- q^{2n-1})^2\\\\\n\\end{align}\n$$\nand for any $m \\in \\mathbb{Z}_{+}$, $k \\in \\{ 2, 3, 4 \\}$, use \n$\\displaystyle D^m\\theta_k(q)$ as a shorthand for \n$\\displaystyle \\left( q\\frac{d}{dq} \\right)^m \\theta_k(q)$.\nBased on Corollary 52 of a survey article Elliptic functions and Transcendence by M. Waldschmidt in 2006,\n\nLet $i, j$ and $k \\in \\{ 2,3,4 \\}$ with $i \\ne j$. Let $q \\in \\mathbb{C}$\n  satisfy $0 < |q| < 1$. Then each of the two fields\n  $$\n\\mathbb{Q}( q, \\theta_i(q), \\theta_j(q), D\\theta_k(q))\n\\quad\\text{ and }\\quad\n\\mathbb{Q}( q, \\theta_k(q), D\\theta_k(q), D^2\\theta_k(q))\n$$\n  has transcendence degree $\\ge 3$ over $\\mathbb{Q}$\n\nWe know for any non-zero algebraic $q$ with $|q| < 1$, the three $\\theta_k(q)$, in particular $\\theta_2(q)$ is transcendental. Since \n$$\\sum_{n=1}^\\infty 10^{-n(n+1)/2} = \\frac{\\sqrt[8]{10}}{2} \\theta_2\\left(\\frac{1}{\\sqrt{10}}\\right) - 1$$\nand using the fact $\\frac{1}{\\sqrt{10}}$ and $\\frac{\\sqrt[8]{10}}{2}$ are both algebraic, we find the number at hand is transcendental.", "meta": {"post_id": 778218, "input_score": 50, "output_score": 74, "post_title": "Is $0.1010010001000010000010000001 \\ldots$ transcendental?"}}
{"input": "If a function's integral can't be written, then how can we find exact values for it over areas? Can we only ever estimate it? Why can't we make new functions to define these strange unwritable anti-derivatives?", "output": "We can, and do all the time! For example, \nThe Gamma function\n$$\n\\Gamma (z) = \\int_0^{\\infty} t^{z-1}\\mathrm{e}^{-t} \\ \\mathrm{d}t.\n$$\nThe Beta function \n$$\n\\mathrm{B}(z,y) = \\int_0^1 t^{z-1}(1-t)^{y-1}\\,\\mathrm{d}t.\n$$\nThe Exponential integral function\n$$\n\\mathrm{E}_1(z) = \\int_z^\\infty \\frac{e^{-t}}{t}\\,  \\mathrm{d}t.\n$$\nThe Error function\n$$\n\\operatorname{erf}(x) = \\frac{2}{\\sqrt{\\pi}}\\int_{0}^x e^{-t^2}\\,\\mathrm dt.$$\nThe Elliptic integral of the second kind\n$$\n E(\\phi,k)=\\int_0^{\\phi} \\sqrt{1-k^2\\sin^2\\theta} \\ \\mathrm{d}\\theta. \n$$\nThe Logarithmic integral function\n$$\n {\\rm Li} (x) = \\int_2^x \\frac{\\mathrm{d}t}{\\ln t},\n$$\nand many, many more very important \"special functions' are defined by definite integrals. If you go back even to elementary functions, you can define the logarithm via the following integral - \nThe Logarithm\n$$\n\\ln (t) = \\int_1^t \\frac{1}{x} \\, dx.\n$$\nAs to your first question, how do we find areas under these curves if we don't have an elementary antiderivative? Well, how do you find the area under the curve $1/t$ from $1$ to $5$? The above integral tells you the value is $\\ln 5$, but what is that value, exactly? We can only approximate it, given the best methods we have! \nThe same is true of all the above functions. At some special values they have exact values, given perhaps by integers, rational or irrational numbers, or a combination of common mathematical constants such as $\\pi,\\mathrm{e},\\gamma,$ Catalan's constant, etc. (another interesting question is - why are these constants special enough to have names? Because they come up all the time! The same is true for the above functions) .\nBut for almost all values we must approximate the value of the function by computing the definite integral numerically (Trapezoid rule, Simpson's rule, more advanced techniques), or using some other representation of the integral such as an infinite sum etc.", "meta": {"post_id": 780170, "input_score": 11, "output_score": 34, "post_title": "How can a function have an antiderivative that can't be written?"}}
{"input": "Are there infinitely many Fibonacci numbers that are also powers of 2? If not, which is the largest?", "output": "Fibonacci numbers have just about the greatest divisibility rule you could expect. Fibonacci numbers share common divisors exactly when their corresponding indices share common divisors, $\\gcd(F[m],F[n])$ = $F_{\\gcd(m,n)}$. \nThis result means that the Fibonacci index of any power of $2$ greater than $8$ must be divisible by $6$ as $F_6 = 8$ and this means that the index of power of $2$ Fibonacci number greater that $8$ must be a power of 6 and therefore must be divisible by $F_{36}$. \nHowever $F_{36}$ is also divisible by $F_{9}$ since $9$ divides $36$ and given that $F_9 = 34, F_{36}$ is therefore divisible by $34$ and cannot be a power of $2$. \nSince any candidate powers of $2$ greater than $8$ must be divisible by $34$ there can be no Fibonacci numbers greater than $8$ which are powers of $2$.", "meta": {"post_id": 795763, "input_score": 28, "output_score": 38, "post_title": "Fibonacci numbers that are powers of 2"}}
{"input": "I originally started off by listing all the primes: $p<200$ then trying to calculate the prime factorisation of each (which I realise is a silly thing to do)\nI believe there must be a simpler way to find the smallest and largest prime factors of $\\dfrac{200!}{180!}$.\nIf I list the prime factorisation of $180$ and $200$ does that help me in any way?\nI have calculated a similar thing before but with similar numbers and I'm not really sure how to deal with these larger numbers?\nThank you for any help", "output": "Note $\\dfrac{200!}{180!}=200(199)\\cdots(181)$. The smallest prime factor is easy to calculate - it's just $2$, since $2$ is the smallest prime and the product contains even factors. The largest prime factor is a little more interesting. You have to find the largest prime factor out of any of the elements in the product. However, noting that $199$ is prime, $200$ has no prime factors greater than $199$, and $199$ is greater than all other elements in the product - never mind their prime factors - gives us the result that the largest prime factor is $199$.", "meta": {"post_id": 804499, "input_score": 15, "output_score": 41, "post_title": "Finding smallest and largest prime factor of $\\frac{200!}{180!}$"}}
{"input": "I was just a little bit curious about the general statement of this theorem. Honestly, I am not at all interested in fully understanding this, so it is not that I am too lazy to read plenty of books about it, but I would like to know a little bit more, what this means. \nTherefore I would like to go with an example: \nLet $(Tf)(x):= \\frac{df}{dx}(x)+\\sin(x)f(x)$ be a differential operator on $[0,L]$ for some $L \\>>0$. \nApparently, the first question would be: Is this operator Fredholm? I do understand what \nit means for an operator to be Fredholm and I understand the definition of the Fredholm index, but I don't see whether this one actually is such an example of a Fredholm operator. In case that this is true. Where does topology come into play? I know the definition of an index for a path, but this topological index seems to be different. Maybe this example is not that good, as we are not studying something on any abstract manifolds, but still, could anybody elaborate on this?", "output": "Fredholm theory, originally developed for studying (systems of) differential equations, had been around for several decades when Gel'fand et al., working in the '50s, noticed that the Fredholm index was homotopy invariant, i.e., if $F_t$, $a \\leq t \\leq b$ is a continuous path of Fredholm operators, then $\\operatorname{Index}(F_t)$ is constant in $t$, and so began wondering if the Fredholm index of at least some Fredholm operators could be computed in terms of honest-to-goodness topological invariants. What Atiyah and Singer then showed was that if $D : E \\to F$ is an elliptic (pseudo)differential operator between smooth vector bundles $E$ and $F$ over a compact orientable manifold $X$, then\n$$\n \\operatorname{Index}(D) = \\operatorname{Index}_{\\text{top}}(D),\n$$\nwhere\n\n$\\operatorname{Index}(D)$ is the Fredholm index of $D$, a purely analytic datum,\n$\\operatorname{Index}_{\\text{top}}(D)$ is the topological index of $D$, the pairing of a certain cohomology class on $X$, obtained from $D$, with the fundamental homology class $[X]$ of $X$.\n\nLet me now give the two most basic examples of Atiyah--Singer in action:\n\n(Example of odd-dimensional Atiyah--Singer) Let $\\gamma : S^1 \\to \\mathbb{C} \\setminus \\{0\\}$ be a continuous closed path in the plane that doesn't pass through the origin. Recall that the Hardy space on the circle $S^1$ is the closed subspace\n$$\n H^2(S^1) = \\left\\{ f = \\sum_{n=-\\infty}^\\infty a_n e^{i n x} \\in L^2(S^1) \\text{ such that } \\forall n < 0, \\; a_n = 0 \\right\\}\n$$\nof $L^2(S^1)$, and let $P$ be the orthogonal projection from $L^2(S^1)$ onto $H^2(S^1)$. Let $M_\\gamma : L^2(S^1) \\to L^2(S^1)$ be the multiplication operator $(M_\\gamma f)(x) = \\gamma(x)f(x)$, and define the Toeplitz operator $T_\\gamma : H^2(S^1) \\to H^2(S^1)$ by $T_\\gamma = P \\circ M_\\gamma \\circ P$. Then Atiyah-Singer, in this special case, reduces to the Toeplitz index theorem, i.e.,\n$$\n \\operatorname{Index}(T_\\gamma) = -\\text{winding number of $\\gamma$},\n$$\nwhich is certainly a topological invariant of interest.\n(Example of even-dimensional Atiyah--Singer) Let $X$ be a Riemann surface, and view $d+d^\\ast$ as an operator $(X \\times \\mathbb{C}) \\oplus \\wedge^2 T^\\ast_{\\mathbb{C}} X \\to T^\\ast_{\\mathbb{C}} X$, i.e., as a map from even-degree forms to odd-degree forms. Then, on the one hand,\n$$\n \\operatorname{Index}(d+d^\\ast) = \\chi(X),\n$$\nwhere $\\chi(X)$ is the Euler characteristic of $X$, whilst on the other hand,\n$$\n \\operatorname{Index}_{\\text{top}}(d+d^\\ast) = \\frac{1}{2\\pi}\\int_X K dA,\n$$\nwhere $K$ is the Gaussian curvature of $X$. Thus, Atiyah--Singer for the elliptic differential operator $d+d^\\ast$ boils down to the Gauss--Bonnet theorem, i.e.,\n$$\n \\int_X K dA = 2\\pi\\chi(X).\n$$\n\nNow, I'm afraid your example of $T := \\tfrac{d}{dx} + \\sin(x)$ isn't really going to tell you all that much. On the one hand, the closed interval $[0,L]$ is a manifold with boundary, and on the other, you need to impose boundary conditions anyway to actually have a well-defined operator, so that you'd might as well take $L = 2k\\pi$ for some $k \\in \\mathbb{N}$ and impose periodic boundary conditions. Then $T$ will define an elliptic differential operator on $L^2(S^1)$ with the same principal symbol $\\sigma(T)$ as $D := \\tfrac{d}{dx}$, since\n$$\n \\sigma(D)(df) := i[D,f] = if^\\prime = i[T,f] =: \\sigma(T)(df)\n$$\nfor all $f \\in C^\\infty(S^1)$, and hence will have the same topological index.\nHowever, you can directly show that $\\ker(D) = \\mathbb{C}$ and that $\\ker(D^\\ast) = \\ker(-D) = \\mathbb{C}$, so that\n$$\n \\operatorname{Index}(T) = \\operatorname{Index}_{\\mathrm{top}}(T) = \\operatorname{Index}_{\\mathrm{top}}(D) = \\operatorname{Index}(D) = 0.\n$$\nAlternatively, since $\\sin(x)$ is smooth, the multiplication by $\\sin(x)$ defines a bounded operator on the Sobolev spaces $W^{s,2}(S^1)$ for each $s \\geq 0$, and hence $T_t := \\tfrac{d}{dx} + t\\sin(x)$ defines a continuous one-parameter family of elliptic first-order differential operators on $L^2(S^1)$, so that by heat-theoretic methods (cf. Roe, Elliptic operators, topology and asymptotic methods 2nd ed., pp. 144-145), the analytic index $\\operatorname{Index}(T_t)$ is constant in $t$, and hence\n$$\n \\operatorname{Index}(T) = \\operatorname{Index}(T_1) = \\operatorname{Index}(T_0) = \\operatorname{Index}(D).\n$$\nAs it turns out, it's a general fact that the index of an elliptic differential operator on a closed odd-dimensional manifold necessarily vanishes---observe that Toeplitz operators are pseudodifferential, not differential, operators.\nNow, what's the point of Atiyah--Singer? The yoga is similar to that of, say, Stokes's theorem, where one side of the equation is sometimes easier to deal with, and the other side sometimes easier instead.\n\nOn the one hand, Fredholm indices, which were originally devised for studying systems of differential equations, can be very difficult to compute, but the right-hand-side of Atiyah--Singer is sometimes computable instead.\nOn the other hand, you might want to know if a certain topological quantity, which is, a priori, only rational, is actually an integer; if you can realise it as the right-hand-side of Atiyah--Singer for some suitable (pseudo)differential operator, then you know that it is an integer, since the left-hand-side is necessarily an integer.", "meta": {"post_id": 815451, "input_score": 23, "output_score": 39, "post_title": "What is the Atiyah-Singer index theorem about?"}}
{"input": "I was reading a textbook and saw an alternative formulation of nowhere dense. I am not sure how to prove this alternate formulation below:\nThe Normal Nowhere Dense Statement:\nLet $X$ be a metric space. A subset $A \u2286 X$ is called nowhere dense in $X$ if the interior of\nthe closure of $A$ is empty, i.e. $(\\overline{A})^{\\circ} = \u2205$. Otherwise put, $A$ is nowhere dense i\ufb00 it is contained in a closed set with empty interior. \nAlternate Formulation:\n\"Passing to complements, we can say equivalently that $A$ is nowhere dense i\ufb00 its complement contains a dense open set.\"\nDoes anyone know how I can prove this? It seems rather painfully straightforward but I am not sure how to show it exactly. Thank you!", "output": "First, you should know that, for any $B\\subseteq X$, $X\\setminus\\overline{B}=(X\\setminus B)^\\circ$ and that $X\\setminus B^\\circ=\\overline{X\\setminus B}$. Now\n\\begin{align*}\nA\\text{ nowhere dense }&\\iff\\left(\\overline{A}\\right)^\\circ=\\varnothing\\\\\n&\\iff X\\setminus(\\overline{A})^\\circ=X\\\\\n&\\iff\\overline{X\\setminus \\overline{A}}=X\\\\\n&\\iff\\overline{(X\\setminus A)^\\circ}=X\\\\\n&\\iff (X\\setminus A)^\\circ\\text{ is dense in }X\\\\\n&\\iff(X\\setminus A)\\text{ contains a dense open subset}.\n\\end{align*}\nThe last equivalence may not be so obvious if you're not very used to metric spaces. See below, if necessary:\n\nIf $(X\\setminus A)^\\circ$ is dense in $X$, then $(X\\setminus A)^\\circ$ is a dense open subset of $X\\setminus A$.Conversely, if $(X\\setminus A)$ contains a dense open subset $D$, then $D\\subseteq (X\\setminus A)^\\circ$, so $(X\\setminus A)^\\circ$ is dense as well.", "meta": {"post_id": 829752, "input_score": 35, "output_score": 45, "post_title": "How to show the that a set $A$ nowhere dense is equivalent to the complement of $A$ containing a dense open set?"}}
{"input": "The least common multiple of  $1,2,\\dotsc,n$ is $[1,2,\\dotsc,n]$, then\n$$\\lim_{n\\to\\infty}\\sqrt[n]{[1,2,\\dotsc,n]}=e$$\n\nwe can show this by prime number theorem, but I don't know how to  start\nI had learnt  that it seems we can find the proposition in G.H. Hardy's number theory book, but I could not find it.\nI am really grateful for any help", "output": "Let's look how the least common multiple evolves.\nIf $n > 1$ is a prime power, $n = p^k$ ($k \\geqslant 1$), then no number $< n$ is divisible by $p^k$, but $p^{k-1} < n$, so $[1,2,\\dotsc,n-1] = p^{k-1}\\cdot m$, where $p\\nmid m$. Then $[1,2,\\dotsc,n] = p^k\\cdot m$, since on the one hand, we see that $p^k\\cdot m$ is a common multiple of $1,2,\\dotsc,n$, and on the other hand, every common multiple of $1,2,\\dotsc,n$ must be a multiple of $p^k$ as well as of $m$.\nIf $n > 1$ is not a prime power, it is divisible by at least two distinct primes, say $p$ is one of them. Let $k$ be the exponent of $p$ in the factorisation of $n$, and $m = n/p^k$. Then $ 1 < p^k < n$ and $1 < m < n$, so $p^k\\mid [1,2,\\dotsc,n-1]$ and $m\\mid [1,2,\\dotsc,n-1]$, and since the two are coprime, also $n = p^k\\cdot m \\mid [1,2,\\dotsc,n-1]$, which means that then $[1,2,\\dotsc,n] = [1,2,\\dotsc,n-1]$.\nTaking logarithms, we see that for $n > 1$\n$$\\begin{align}\n\\Lambda (n) &= \\log [1,2,\\dotsc,n] - \\log [1,2,\\dotsc,n-1]\\\\\n&= \\begin{cases} \\log p &, n = p^k\\\\ \\;\\: 0 &, \\text{otherwise}.\\end{cases}\n\\end{align}$$\n$\\Lambda$ is the von Mangoldt function, and we see that\n$$\\log [1,2,\\dotsc,n] = \\sum_{k\\leqslant n} \\Lambda(k) = \\psi(n),$$\nwhere $\\psi$ is known as the second Chebyshev function.\nWith these observations, it is clear that\n$$\\lim_{n\\to\\infty} \\sqrt[n]{[1,2,\\dotsc,n]} = e\\tag{1}$$\nis equivalent to\n$$\\lim_{n\\to\\infty} \\frac{\\psi(n)}{n} = 1.\\tag{2}$$\nIt is well-known and easy to see that $(2)$ is equivalent to the Prime Number Theorem (without error bounds)\n$$\\lim_{x\\to\\infty} \\frac{\\pi(x)\\log x}{x} = 1.\\tag{3}$$\nTo see the equivalence, we also introduce the first Chebyshev function,\n$$\\vartheta(x) = \\sum_{p\\leqslant x} \\log p,$$\nwhere the sum extends over the primes not exceeding $x$. We have\n$$\\vartheta(x) \\leqslant \\psi(x) = \\sum_{n\\leqslant x}\\Lambda(n) = \\sum_{p\\leqslant x}\\left\\lfloor \\frac{\\log x}{\\log p}\\right\\rfloor\\log p \\leqslant \\sum_{p\\leqslant x} \\log x = \\pi(x)\\log x,$$\nwhich shows - the existence of the limits assumed -\n$$\\lim_{x\\to\\infty} \\frac{\\vartheta(x)}{x} \\leqslant \\lim_{x\\to\\infty} \\frac{\\psi(x)}{x} \\leqslant \\lim_{x\\to\\infty} \\frac{\\pi(x)\\log x}{x}.$$\nFor $n \\geqslant 3$, we can split the sum at $y = \\frac{x}{(\\log x)^2}$ and obtain\n$$\\pi(x) \\leqslant \\pi(y) + \\sum_{y < p \\leqslant x} 1 \\leqslant \\pi(y) + \\frac{1}{\\log y}\\sum_{y < p < x}\\log p \\leqslant y + \\frac{\\vartheta(x)}{\\log y},$$\nwhence\n$$\\frac{\\pi(x)\\log x}{x} \\leqslant \\frac{y\\log x}{x} + \\frac{\\log x}{\\log y}\\frac{\\vartheta(x)}{x} = \\frac{1}{\\log x} + \\frac{1}{1 - 2\\frac{\\log \\log x}{\\log x}}\\frac{\\vartheta(x)}{x}.$$\nSince $\\frac{1}{\\log x}\\to 0$ and $\\frac{\\log\\log x}{\\log x} \\to 0$ for $x\\to \\infty$, it follows that (once again assuming the existence of the limits)\n$$\\lim_{x\\to\\infty} \\frac{\\pi(x)\\log x}{x} \\leqslant \\lim_{x\\to\\infty} \\frac{\\vartheta(x)}{x},$$\nand the proof of the equivalence of $(1)$ and $(3)$ is complete.", "meta": {"post_id": 834220, "input_score": 35, "output_score": 42, "post_title": "least common multiple $\\lim\\sqrt[n]{[1,2,\\dotsc,n]}=e$"}}
{"input": "It always baffled me why $L^p$-spaces are the spaces of choice in almost any area (sometimes with some added regularity (Sobolev/Besov/...)). I understand that the exponent allows for convenient algebraic manipulations, but is there more behind it than mathematical convenience? \nWhat bugs me about $L^p$-spaces is that they don't build a scale (of inclusions) but still only allow for one parameter, so by making a choice of exponent you make a choice about two (to my current knowledge) unrelated properties of your function, a) its behavior at singularities (which get milder with high exponent) and b) its tail behavior (which gets less nice with high exponent). How can it still be a good idea to ask \"does this operator map $L^p$ to $L^p$\" rather than \"what does this operator do with singularities and what does it do with tails\"? Of course answers to the latter will be harder to formulate and prove, but is that all?", "output": "This is indeed a very good and natural question, as one usually learns that there is a whole spectrum of $L^p$-spaces but then in practice only $L^2$ (and, to a less extent, $L^1$ and $L^\\infty$) seems to pop up. Why should we care about $L^{\\frac{3}{2}}$? Of course this question has many possible answers, and I find that a convincing one comes from the context of nonlinear analysis.  \n\nThe basic observation is the fact that $$\\lVert u^k\\rVert_{L^p}=\\lVert u\\rVert_{L^{kp}}^k.$$ So, when dealing with nonlinear problems, we can expect that we will have to play some trick with the index $p$. We won't be able to stay in the comfortable $L^2$-space all the time. \n\nFor an example of this let us consider the following PDE: \n$$\\tag{1}\n-\\Delta u (x)= u^2(x), \\qquad x\\in \\mathbb{R}^3.\n$$\nThe associated linear inhomogeneous problem \n$$\\tag{2}\n-\\Delta u= h\n$$\ncan be solved very satisfactorily in the functional setting of $L^2$-space via the Fourier transform: assuming that everything lies in $L^2(\\mathbb{R}^3)$, we can Fourier transform termwise in (2) and write $\\hat{u}(\\xi)=\\lvert\\xi\\rvert^{-2}\\hat{h}$, which can then be anti-transformed back to \n$$u(x)= \\left(\\lvert 4\\pi y\\rvert^{-1} \\ast h\\right) (x)\\stackrel{\\text{def}}{=}(-\\Delta)^{-1} h.$$\n(Note that $\\lvert 4\\pi y\\rvert^{-1}$ is exactly the fundamental solution of the Laplace operator). Setting $h=u^2$, we can now reformulate the nonlinear equation (1) as follows: \n$$\\tag{3}\nu=(-\\Delta)^{-1}\\left( u^2\\right),$$\nwhich is now an equation of fixed-point type. We want to approach it via the contraction mapping principle, by showing that the mapping \n$$\\Phi(u)=(-\\Delta)^{-1}\\left( u^2\\right)$$ \nis contractive on some complete metric space to be specified later. To do so we need some estimates on $\\Phi$ and those can be provided by the Hardy-Littlewood-Sobolev inequality, which in our case ($\\alpha=2,\\ n=3$) reads \n$$\\lVert (-\\Delta)^{-1} f\\rVert_{L^q(\\mathbb{R}^3)} \\le C \\lVert f\\rVert_{L^p(\\mathbb{R}^3)}, \\qquad 2+\\frac{3}{q}=\\frac{3}{p}.\n$$\n(The condition on $p$ and $q$ can be recovered via the scaling argument, by observing that both sides of this inequality are homogeneous with respect to the scaling $f(x)\\mapsto f(\\lambda x)$, and therefore the degrees of homogeneity must match). With $f=u^2$ this inequality reads \n$$\\tag{4}\n\\lVert \\Phi(u)\\rVert_{L^q(\\mathbb{R}^3)}\\le C \\lVert u\\rVert_{L^{2p}(\\mathbb{R}^3)}^2.$$\nIt is now clear that our hands are tied: the only way to get something meaningful is to have $q=2p$, which means that $q=\\frac{3}{2}$. Thus the right functional setting for this problem is $L^\\frac{3}{2}(\\mathbb{R}^3)$-space. \nIndeed, if we let $B_R\\subset L^{\\frac{3}{2}}(\\mathbb{R}^3)$ denote the closed ball of radius $R$, we see from (4) that $\\Phi(B_R)\\subset B_R$ if $R< 1/C$. Then, again by (4), we see that \n$$\n\\begin{split}\n\\lVert \\Phi(u)-\\Phi(v)\\rVert_{L^{\\frac{3}{2}}(\\mathbb{R}^3)}&\\le C \\lVert u^2-v^2\\rVert_{L^{\\frac{3}{4}}(\\mathbb{R}^3)} \\\\ \n&=C\\lVert (u+v)(u-v)\\rVert_{L^{\\frac{3}{4}}(\\mathbb{R}^3)} \\\\\n&\\le C \\lVert u+v\\rVert_{L^{\\frac{3}{2}}(\\mathbb{R}^3)}\\lVert u-v\\rVert_{L^{\\frac{3}{2}}(\\mathbb{R}^3)} \\\\\n&\\le 2RC\\lVert u-v\\rVert_{L^{\\frac{3}{2}}(\\mathbb{R}^3)}.\n\\end{split}\n$$\nThis means that the map $\\Phi\\colon B_R\\to B_R$ is contractive if $R<\\frac{1}{2 C}$. \nAs a final remark, let us observe that we have actually proven two facts:\n\nthe existence of a unique solution to (1) in small neighborhoods of the origin in $L^\\frac{3}{2}(\\mathbb{R}^3)$-space;\nthe fact that the sequence $$\\begin{cases} u_{n+1}=(-\\Delta)^{-1}\\left( u_n^2 \\right) \\\\ \\lVert u_0\\rVert_{L^{\\frac{3}{2}}(\\mathbb{R}^3)} \\le R \\end{cases}$$ converges in the $L^{\\frac{3}{2}}(\\mathbb{R}^3)$ topology to the solution to (1), no matter which initial condition $u_0$ we choose (provided that $R$ is sufficiently small).\n\nFact 2. justifies also the necessity to deal with convergence issues in $L^p$-spaces with $p\\ne 2$.", "meta": {"post_id": 843108, "input_score": 35, "output_score": 37, "post_title": "Why are $L^p$-spaces so ubiquitous?"}}
{"input": "$\\phi(\\pi(\\phi^\\pi)) = 1$\nI saw it on an expired flier for a lecture at the university. I don't know what $\\phi$ is, so I tried asking Wolfram Alpha to solve $x \\pi x^\\pi = 1$ and it gave me a bunch of results with $i$, and I don't know what that is either.", "output": "It's a joke based on the use of the $\\phi$ function (Euler's totient function), the $\\pi$ function (the prime counting function), the constant $\\phi$ (the golden ratio), and the constant $\\pi$. Note $\\phi^\\pi\\approx 4.5$, so there are two primes less than $\\phi^\\pi$ (they are $2$ and $3$), so $\\pi(\\phi^\\pi)=2$. There is only one positive integer less than or equal to $2$ which is also relatively prime to $2$ (this number is $1$), so $\\phi(2)=1$. Hence we have\n$$\\phi(\\pi(\\phi^\\pi))=\\phi(2)=1$$", "meta": {"post_id": 861618, "input_score": 13, "output_score": 35, "post_title": "Is the equation $\\phi(\\pi(\\phi^\\pi)) = 1$ true? And if so, how?"}}
{"input": "Let $A$ and $B$ be symmetric matrices.\nProve:\n\n$AB=BA$\n$AB$ is a symmetric matrix\n\nAs for 1. due to the axiom $(AB)^T=B^T A^T$ so $AB=BA$\nAs for 2. I did not find any axiom that can support the claim, but from test I found that it is true for symmetric matrices when the entries on the diagonal are equal.", "output": "Both claims are false and almost any $A$ and $B$ are counterexamples. For a specific example, you can see $$\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 & 2 \\\\ 2 & 3 \\end{pmatrix} = \\begin{pmatrix} 3 & 5 \\\\ 3 & 5 \\end{pmatrix}$$ while $$\\begin{pmatrix} 1 & 2 \\\\ 2 & 3 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 3 & 3 \\\\ 5 & 5 \\end{pmatrix}.$$", "meta": {"post_id": 874469, "input_score": 17, "output_score": 51, "post_title": "Symmetric matrix multiplication"}}
{"input": "I discovered the following conjecture by evaluating the integral numerically and then using some inverse symbolic calculation methods to find a possible closed form:\n$$\\int_0^\\infty\\frac{\\ln x}{\\sqrt{x\\vphantom{1}}\\ \\sqrt{x+1}\\ \\sqrt{2x+1}}dx\\stackrel{\\color{#808080}?}=\\frac{\\pi^{3/2}\\,\\ln2}{2^{3/2}\\,\\Gamma^2\\left(\\tfrac34\\right)}.\\tag1$$\nThe equality holds numerically with a precision of at least $1000$ decimal digits. But so far I was not able to find a proof of it.\nBecause the integral can be represented as a derivative of a hypergeometic function with respect to its parameter, the conjecture can be rewritten as\n$$\\frac{d}{da}{_2F_1}\\left(a,\\ \\tfrac12;\\ 1;\\ \\tfrac12\\right)\\Bigg|_{a=\\frac12}\\stackrel{\\color{#808080}?}=\\frac{\\sqrt\\pi\\,\\ln2}{2\\,\\Gamma^2\\left(\\tfrac34\\right)}\\tag2$$\nor, using a series expansion of the hypergeometric function, as\n$${\\large\\sum}_{n=0}^\\infty\\frac{H_{n-\\frac12}\\ \\Gamma^2\\left(n+\\tfrac12\\right)}{2^n\\ \\Gamma^2\\left(n+1\\right)}\\stackrel{\\color{#808080}?}=-\\frac{3\\,\\pi^{3/2}\\,\\ln2}{2\\,\\Gamma^2\\left(\\tfrac34\\right)}\\tag3,$$\nwhere $H_q$ is the generalized harmonic number, $H_q=\\gamma+\\psi_0\\left(q+1\\right).$\nCould you suggest any ideas how to prove this?", "output": "$$I:=\\int_{0}^{\\infty}\\frac{\\ln{(x)}}{\\sqrt{x}\\,\\sqrt{x+1}\\,\\sqrt{2x+1}}\\mathrm{d}x.$$\n\nAfter first multiplying and dividing the integrand by 2, substitute $x=\\frac{t}{2}$:\n$$I=\\int_{0}^{\\infty}\\frac{2\\ln{(x)}}{\\sqrt{2x}\\,\\sqrt{2x+2}\\,\\sqrt{2x+1}}\\mathrm{d}x=\\int_{0}^{\\infty}\\frac{\\ln{\\left(\\frac{t}{2}\\right)}}{\\sqrt{t}\\,\\sqrt{t+2}\\,\\sqrt{t+1}}\\mathrm{d}t.$$\nNext, substituting $t=\\frac{1}{u}$ yields:\n$$\\begin{align}\nI\n&=-\\int_{0}^{\\infty}\\frac{\\ln{(2u)}}{\\sqrt{u}\\sqrt{u+1}\\sqrt{2u+1}}\\mathrm{d}u\\\\\n&=-\\int_{0}^{\\infty}\\frac{\\ln{(2)}}{\\sqrt{u}\\sqrt{u+1}\\sqrt{2u+1}}\\mathrm{d}u-\\int_{0}^{\\infty}\\frac{\\ln{(u)}}{\\sqrt{u}\\sqrt{u+1}\\sqrt{2u+1}}\\mathrm{d}u\\\\\n&=-\\int_{0}^{\\infty}\\frac{\\ln{(2)}}{\\sqrt{u}\\sqrt{u+1}\\sqrt{2u+1}}\\mathrm{d}u-I\\\\\n\\implies I&=-\\frac{\\ln{(2)}}{2}\\int_{0}^{\\infty}\\frac{\\mathrm{d}x}{\\sqrt{x}\\sqrt{x+1}\\sqrt{2x+1}}.\n\\end{align}$$\nMaking the sequence of substitutions $x=\\frac{u-1}{2}$, then $u=\\frac{1}{t}$, and finally $t=\\sqrt{w}$, puts this integral into the form of a beta function:\n$$\\begin{align}\n\\int_{0}^{\\infty}\\frac{\\mathrm{d}x}{\\sqrt{x}\\sqrt{x+1}\\sqrt{2x+1}}\n&=\\int_{1}^{\\infty}\\frac{\\mathrm{d}u}{\\sqrt{u-1}\\sqrt{u+1}\\sqrt{u}}\\\\\n&=\\int_{1}^{\\infty}\\frac{\\mathrm{d}u}{\\sqrt{u^2-1}\\sqrt{u}}\\\\\n&=\\int_{1}^{0}\\frac{t^{3/2}}{\\sqrt{1-t^2}}\\frac{(-1)}{t^2}\\mathrm{d}t\\\\\n&=\\int_{0}^{1}\\frac{\\mathrm{d}t}{\\sqrt{t}\\,\\sqrt{1-t^2}}\\\\\n&=\\frac12\\int_{0}^{1}\\frac{\\mathrm{d}w}{w^{3/4}\\,\\sqrt{1-w}}\\\\\n&=\\frac12\\operatorname{B}{\\left(\\frac14,\\frac12\\right)}\\\\\n&=\\frac12\\frac{\\Gamma{\\left(\\frac12\\right)}\\Gamma{\\left(\\frac14\\right)}}{\\Gamma{\\left(\\frac34\\right)}}\\\\\n&=\\frac{\\pi^{3/2}}{2^{1/2}\\Gamma^2{\\left(\\frac34\\right)}}\n\\end{align}$$\nHence,\n$$I=-\\frac{\\ln{(2)}}{2}\\frac{\\pi^{3/2}}{2^{1/2}\\Gamma^2{\\left(\\frac34\\right)}}=-\\frac{\\pi^{3/2}\\,\\ln{(2)}}{2^{3/2}\\,\\Gamma^2{\\left(\\frac34\\right)}}.~~~\\blacksquare$$\n\nPossible Alternative: You could also derive the answer from the complete elliptic integral of the first kind instead of from the beta function by making the substitution $t=z^2$ instead of $t=\\sqrt{w}$.\n$$\\begin{align}\n\\int_{0}^{\\infty}\\frac{\\mathrm{d}x}{\\sqrt{x}\\sqrt{x+1}\\sqrt{2x+1}}\n&=\\int_{1}^{\\infty}\\frac{\\mathrm{d}u}{\\sqrt{u-1}\\sqrt{u+1}\\sqrt{u}}\\\\\n&=\\int_{1}^{\\infty}\\frac{\\mathrm{d}u}{\\sqrt{u^2-1}\\sqrt{u}}\\\\\n&=\\int_{1}^{0}\\frac{t^{3/2}}{\\sqrt{1-t^2}}\\frac{(-1)}{t^2}\\mathrm{d}t\\\\\n&=\\int_{0}^{1}\\frac{\\mathrm{d}t}{\\sqrt{t}\\,\\sqrt{1-t^2}}\\\\\n&=2\\int_{0}^{1}\\frac{\\mathrm{d}z}{\\sqrt{1-z^4}}\\\\\n&=2\\,K{(-1)}\\\\\n&=\\frac{\\Gamma^2{\\left(\\frac14\\right)}}{2\\sqrt{2\\pi}}\\\\\n&=\\frac{\\pi^{3/2}}{2^{1/2}\\Gamma^2{\\left(\\frac34\\right)}}.\n\\end{align}$$", "meta": {"post_id": 877460, "input_score": 40, "output_score": 50, "post_title": "Prove ${\\large\\int}_0^\\infty\\frac{\\ln x}{\\sqrt{x}\\ \\sqrt{x+1}\\ \\sqrt{2x+1}}dx\\stackrel?=\\frac{\\pi^{3/2}\\,\\ln2}{2^{3/2}\\Gamma^2\\left(\\tfrac34\\right)}$"}}
{"input": "Does anyone know the exact proof of this limit result?\n$$\\lim_{n\\to\\infty} \\left(1+\\frac{x}{n}\\right)^n = e^x$$", "output": "$$e^{\\ln{(1 + \\frac{x}{n})^n} }=e^{n \\ln(1+\\frac{x}{n})}$$\n$$\\lim_{n \\to +\\infty} (1 + \\frac{x}{n})^n\n=\\lim_{n \\to +\\infty} e^{n \\ln(1+\\frac{x}{n})} \\\\ \n=e^{\\lim_{n \\to +\\infty} n \\ln(1+\\frac{x}{n})}\n=e^{\\lim_{n \\to +\\infty}\\frac{ \\ln(1+\\frac{x}{n})}{\\frac{1}{n}}}$$ \nApply L'Hopital's Rule:\n$$=e^{\\lim_{n \\to +\\infty}\\frac{(\\frac{-x}{n^2})\\frac{1}{1+\\frac{x}{n}}}{-\\frac{1}{n^2}}}\n=e^{\\lim_{n \\to +\\infty}\\frac{x}{1+\\frac{x}{n}}}\n=e^x$$\nTherefore, $$(1+\\frac{x}{n})^n \\to e^x$$", "meta": {"post_id": 882741, "input_score": 49, "output_score": 52, "post_title": "Limit of $(1+ x/n)^n$ when $n$ tends to infinity"}}
{"input": "I did some numerical approximation of $$\\sum_{n=-\\infty}^\\infty \\exp(-(x+n)^2)$$ and found that this function is \"almost\" constant ($\\approx 1.772$). Why does the sum fluctuate little? Is there a closed form for this sum?\nAdded: since $f(x) = \\sum_{n=-\\infty}^\\infty \\exp(-(x+n)^2)$ has period $1$ and is even, can we give an upper bound of $\\sup\\{ f(x)/f(y) : (x,y)\\in [0,0.5]^2\\}$?", "output": "Recall the general case of the Poisson sum formula:\n$$\\sum_{-\\infty}^\\infty f(x+n)\n=\\sum_{k=-\\infty}^\\infty e^{2\\pi i k x} \n\\int_{-\\infty}^{\\infty} e^{-2\\pi i k y}f(y)\\,dy$$ \nThen \n$\\displaystyle\\int_{-\\infty}^{\\infty} e^{-2\\pi i k y}e^{-y^2}\\,dy$ is a Gaussian integral, and (skipping the tedious step of completing the square) evaluates to \n$\\sqrt{\\pi} e^{ -k^2 \\pi^2}$. So\n$$\\sum_{-\\infty}^\\infty e^{-(x+n)^2}\n= \\sqrt{\\pi}\\sum_{k=-\\infty}^\\infty e^{2\\pi i k x} e^{-k^2 \\pi^2}=\\sqrt{\\pi}+2\\sqrt{\\pi}\\sum_{k=1}^\\infty e^{-k^2 \\pi^2}\\cos 2\\pi kx $$\nObserve that this means that the function has an average value of $\\sqrt\\pi$, and that we have a tower of corrections which are each exponentially smaller than the last. To a very good approximation, then, we can take the function to be $$\\sqrt\\pi+2\\sqrt\\pi e^{-\\pi^2} \\cos 2\\pi x$$ with a variation around the value $\\sqrt\\pi\\approx1.77245$ of merely $\\pm2\\sqrt\\pi e^{-\\pi^2}\\approx\\pm0.0002$. So the function is very flat.\nThis is also consistent with Dmoreno's observation that the sum is a Jacobi theta function, since the argument above amounts to a proof that $\\vartheta_3(x;\\tau)$ has its known Fourier expansion\n$$\\vartheta_3(z,q)=\\sum_{-\\infty}^\\infty q^{n^2}e^{2 i n z}=1+2\\sum_{n=1}^\\infty q^{n^2} \\cos 2n z$$\nfor the case $q=e^{-\\pi^2},$ $z=\\pi x$.\nAdded:\nWe can generalize the above calculation to obtain the sum \n$$\\sum\\limits_{-\\infty}^\\infty \\exp\\left[-\\left(\\dfrac{x+n}{a}\\right)^2\\right]\n=\\sqrt{\\pi}|a|\\vartheta_3(\\pi x,e^{-\\pi^2 a^2})=\\sqrt{\\pi}|a|\\left[1+2\\sum_{k=1}^\\infty e^{-\\pi^2 a^2 k^2} \\cos(2\\pi k x)\\right]$$\nObserve that if we pick $a$ to be small, then $e^{-a^2 \\pi^2}$ (formally the elliptic nome $q$) need not be small. In that case the approximation given earlier breaks down. The cases $a=\\pi^{-1}$ and $a=(2\\pi)^{-1}$ give striking examples, as evident in these WolframAlpha plots [1] [2]: the sums definitely aren't flat!\nTo explain this in less formal terms, note that the sum consists of an infinite set of shifted Gaussians $\\{e^{-(x+n)/a^2}\\}$. If $a$ is small, then each Gaussian is narrowly peaked and does not overlap much with its neighbors; consequently, the sum of all of them together leads to a 'comb' of narrow peaks. But if $a$ is not small--and $a=1$ is not--then each Gaussian overlaps strongly with its neighbors, and so the resulting 'comb' due to the sum has its gaps mostly filled in (i.e. nearly flat.)", "meta": {"post_id": 891974, "input_score": 15, "output_score": 34, "post_title": "Why is $\\sum_{n=-\\infty}^{\\infty}\\exp(-(x+n)^2)$ \"almost\" constant?"}}
{"input": "Let $A$ be a square and singular matrix of order $n$.\nIs $\\operatorname{adj}(A)$ necessarily singular? What would be the rank of $\\operatorname{adj}(A)$?", "output": "No, the adjugate of a singular matrix can be non-singular. But it happens only for the $1\\times 1$ zero matrix.\nHere is a complete classification, referring to this answer and this one. One always has $$\\def\\adj{\\operatorname{adj}}A \\cdot \\adj(A) = \\det(A) I_n.$$\n\nIf $A$ has rank$~n$, then it is invertible, and so is $\\det(A)$, and $\\adj(A)=\\det(A)A^{-1}$ is invertible too, and has rank$~n$.\nIf $A$ has rank$~n-1$ then at least one $(n-1)\\times(n-1)$ minor is nonzero, and so $\\adj(A)\\neq0$. On the other hand by the given relation the image of $\\adj(A)$ is contained in the kernel of $A$ which has dimension$~1$ by rank-nullity; it follows that $\\adj(A)$ has rank$~1$ in this case.\nIf $A$ has rank${}<n-1$ then all $(n-1)\\times(n-1)$ minors are equal to zero, and so $\\adj(A)$ has rank$~0$.\n\nThe cases where $\\adj(A)$ has rank$~n$ are the first case for any$~n$, and the second case for $n=1$. (And, I would be inclined to say, the last case for $n=0$; but that of course cannot happen at all.) So the only case where $A$ is singular but $\\adj(A)$ is not, is the case $A=(0)$ (with $n=1$).", "meta": {"post_id": 892039, "input_score": 14, "output_score": 36, "post_title": "Is adjoint of singular matrix singular? What would be its rank?"}}
{"input": "Let $A$ be a positive-definite real matrix in the sense that $x^T A x > 0$ for every nonzero real vector $x$. I don't require $A$ to be symmetric.\nDoes it follow that $\\mathrm{det}(A) > 0$?", "output": "Here is en eigenvalue-less proof that if $x^T A x > 0$ for each nonzero real vector $x$, then $\\det A > 0$.\nConsider the function $f(t) = \\det \\left(t \\cdot I + (1-t) \\cdot A\\right)$ defined on the segment $[0, 1]$. Clearly, $f(0) = \\det A$ and $f(1) = 1$. Note that $f$ is continuous. If we manage to prove that $f(t) \\neq 0$ for every $t \\in [0, 1]$, then it will imply that $f(0)$ and $f(1)$ have the same sign (by the intermediate value theorem), and the proof will be complete.\nSo, it remains to show that $f(t) \\neq 0$ whenever $t \\in [0, 1]$. But this is easy. If $t \\in [0, 1]$ and $x$ is a nonzero real vector, then\n$$\nx^T (tI + (1-t)A) x = t \\cdot x^T x + (1-t) \\cdot x^T A x > 0,\n$$\nwhich implies that $tI + (1-t)A$ is not singular, which means that its determinant is nonzero, hence $f(t) \\neq 0$. Done.\nPS: The proof is essentially topological. We have shown that there is a path from $A$ to $I$ in the space of all invertible matrices, which implies that $\\det A$ and $\\det I$ can be connected by a path in $\\mathbb{R} \\setminus 0$, which means that $\\det A > 0$. One could use the same techniqe to prove other similar facts. For instance, this comes to mind: if $S^2 = \\{(x, y, z) \\mid x^2 + y^2 + z^2 = 1\\}$ is the unit sphere, and $f: S^2 \\to S^2$ is a continuous map such that $(v, f(v)) > 0$ for every $v \\in S^2$, then $f$ has degree $1$.", "meta": {"post_id": 892729, "input_score": 35, "output_score": 50, "post_title": "Does a positive definite matrix have positive determinant?"}}
{"input": "The dual of a norm $\\|\\cdot \\|$ is defined as:\n$$\\|z\\|_* = \\sup \\{ z^Tx \\text{ } | \\text{ } \\|x\\| \\le 1\\}$$\nCould anybody give me an intuition of this concept? I know the definition, I am using it to solve problems, but in reality I still lack intuitive understanding of it.", "output": "Here's the way I like to think about it.  I'll start with the finite dimensional space $\\Bbb{R}^n$ because it looks like that's where you are, but I'll give an analogy for infinite dimensional spaces as well.\nThe quantity $z^Tx$ represents a linear functional on $\\Bbb{R}^n$, that is a linear function which eats a vector and spits out a real number:\n$$\nf_z(x):\\Bbb{R}^n\\rightarrow\\Bbb{R}\\quad \\text{such that }\\quad f_z(\\alpha x+\\beta y)=\\alpha f_z(x)+\\beta f_z(y)\\quad \\forall \\alpha,\\beta\\in\\Bbb{R},x,y\\in\\Bbb{R}^n\n$$\nBecause of the Riesz Representation Theorem, we know that any linear function $f:\\Bbb{R}^n\\rightarrow\\Bbb{R}$ will take the form $f=f_z$ for some $z\\in\\Bbb{R}^n$, i.e. $f(x) = z^Tx$.\nThe question is now this: given a linear function(al) $f_z(\\cdot)$, how \"big\" is it?  Well, to measure the size of vectors, we look at norms, so the idea is simple: how big is the number $f_z(x)=z^Tx$ relative to the size (norm) of $x$?  This is exactly the number \n$$\n\\frac{z^Tx}{\\|x\\|}\n$$  We then say that the norm of $z$ is the largest this quantity can possibly be: \n$$\n\\|z\\|_* = \\sup_{x\\neq 0} \\frac{z^Tx}{\\|x\\|}\n$$  In a way, this is a kind of \"stretch factor\", but the stretching is measured with respect to $\\|x\\|$, which is the way we're measuring the size of $x$.  With a simple one-line proof, you can show that my way of defining $\\|z\\|_*$ is the same as yours.\nThis idea extends to infinite dimensional normed spaces such as $L^p$ as well - every normed space has a \"dual\" space of (continuous/bounded) linear functionals, i.e. mappings which eat vectors (which might actually be functions) and spit out numbers.  Each of these functionals has an associated \"size\", and that size is given by the dual norm: \n$$\n\\|f\\|_* = \\sup_{x\\neq 0}\\frac{f(x)}{\\|x\\|}\n$$ \nTo really complete the picture - and to expand on a couple of comments - it helps to also think about the dual norm as a special case of an operator norm.  The idea behind a general operator norm is pretty much the same as what I described above, but for a more general linear operator $A:X\\rightarrow Y$ where $X$ and $Y$ are any normed linear spaces.  In the case of linear functionals, $X$ is a vector space like $\\Bbb{R}^n$ or $L^p$ etc, and $Y$ is simply the 'base field', $\\Bbb{R}$ (or more generally $\\Bbb{C}$).  The idea is that $A$ eats vectors and spits out other vectors, and to measure the \"size\" of $A$ we might look again at the ratio of the size of $Ax$ (measured with the $Y$ norm) to the size of $x$ (measured with the $X$ norm):\n$$\n\\frac{\\|Ax\\|_Y}{\\|x\\|_X}\n$$  The largest of these values over nonzero $x\\in X$ is a good value for the size of $A$, because it tells us a sort of worst-case stretch factor: \n$$\n\\|A\\|=\\sup_{x\\neq 0}\\frac{\\|Ax\\|_Y}{\\|x\\|_X}\n$$\nThis is very similar to the idea of a singular value - in fact, if we use the Euclidean norm $\\|\\cdot\\|_2$, the operator norm of a matrix is its largest singular value!", "meta": {"post_id": 903484, "input_score": 62, "output_score": 91, "post_title": "Dual norm intuition"}}
{"input": "Prove without using induction that the following formula:$$\\sum_{k=0}^n (-1)^k\\binom{n}{k}=0$$ is valid for every $n\\ge1$.\n\nProgress\nFor each odd $n$ we can use the identity:$$\\binom{n}{k}=\\binom{n}{n-k}$$ In fact all terms equidistant from the end points are opposite. My question is: if $n$ is even how can we prove it?", "output": "Set $a=1,b=-1$ in the Binomial Expansion formula for non-negative intger $n$ $$(a+b)^n=\\sum_{k=0}^n\\binom nk a^{n-k}b^k$$", "meta": {"post_id": 907742, "input_score": 8, "output_score": 37, "post_title": "Alternating sum of binomial coefficients is equal to zero"}}
{"input": "This thought jumped out of me during my calculus teaching seminar. \nIt is well known that the classical L'Hospital rule claims that for the $\\frac{0}{0}$ indeterminate case, we have:\n$$\n\\lim_{x\\rightarrow A}\\frac{f(x)}{g(x)}=\\lim_{x\\rightarrow A}\\frac{f'(x)}{g'(x)}\n$$\nwhere the later could take any value including $\\infty$. Here we assume that right hand side limit exist. \nHowever, to apply it one often has to take the derivative of $f'(x)$ again at $A$, and in principle one assumes by repeatedly applying this rule we can resolve the problem by plug in the value into the function's derivative at $A$. My question is, what if the student ask if it is possible for $\\lim_{x\\rightarrow A} f(x),\\lim_{x\\rightarrow A} f'(x)\\cdots \\lim^{n}_{x\\rightarrow A}f^{n}(x)$ be all zero for any $n$, so the rule 'fails'. How should we answer the question properly?\nFor example, consider the well-known non-analytic smooth function:\n$$f(x)=\n\\begin{cases}\ne^{-1/x}& x> 0\\\\\n0& x\\le 0\n\\end{cases}\n$$\nIt is a trivial exercise to verify that $f^{n}(0)=0$ for any $n\\in \\mathbb{N}$. Now using L'Hospital rule we compute (as if we are a low level student)\n$$\n1=\\lim_{x\\rightarrow 0^{+}}\\frac{f(x)}{f(x)}=\\lim_{x\\rightarrow 0^{+}}\\frac{f'(x)}{f'(x)}=\\lim_{x\\rightarrow 0^{+}}\\frac{f''(x)}{f''(x)}\\cdots =\\frac{0}{0}=?\n$$\nas the chain does not stop if the student applies the rule faithfully and blindly. This is a silly example, but in general for non-analytical functions I think this kind of thing could happen. And there should be more non-analytical functions than analytical functions. Is there a way for us to resolve this at introductory calculus level, so that the student know what to do, without introducing `confusing concepts' like $\\epsilon-\\delta$ language, Cauchy mean value theorem, Taylor series, and infinitesimals?", "output": "Even for analytical functions, this kind of thing can happen. \nConsider $\\displaystyle\\lim_{x \\to \\infty}\\dfrac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$. \nBlindly applying L'Hopital's Rule repeatedly gives: \n$\\displaystyle\\lim_{x \\to \\infty}\\dfrac{e^{x}-e^{-x}}{e^{x}+e^{-x}} = \\lim_{x \\to \\infty}\\dfrac{e^{x}+e^{-x}}{e^{x}-e^{-x}} = \\lim_{x \\to \\infty}\\dfrac{e^{x}-e^{-x}}{e^{x}+e^{-x}} = \\cdots$. \nBut if we divide the numerator and denominator by $e^x$ we get: \n$\\displaystyle\\lim_{x \\to \\infty}\\dfrac{e^{x}-e^{-x}}{e^{x}+e^{-x}} = \\lim_{x \\to \\infty}\\dfrac{1-e^{-2x}}{1+e^{-2x}} = \\dfrac{1+0}{1+0} = 1$.\n\nAlso, consider $\\displaystyle\\lim_{x \\to 0^+}x \\ln x = \\lim_{x \\to 0^+}\\dfrac{\\ln x}{1/x}$. \nBlindly applying L'Hopital's Rule repeatedly gives: \n$\\displaystyle\\lim_{x \\to 0^+}x \\ln x = \\lim_{x \\to 0^+}\\dfrac{\\ln x}{1/x} = \\lim_{x \\to 0^+}\\dfrac{1/x}{-1/x^2} = \\lim_{x \\to 0^+}\\dfrac{-1/x^2}{2/x^3} = \\lim_{x \\to 0^+}\\dfrac{2/x^3}{-6/x^4} = \\cdots$.\nBut it we stop after applying L'Hopital's Rule once and simplify stuff, we get: \n$\\displaystyle\\lim_{x \\to 0^+}x \\ln x = \\lim_{x \\to 0^+}\\dfrac{\\ln x}{1/x} = \\lim_{x \\to 0^+}\\dfrac{1/x}{-1/x^2} = \\lim_{x \\to 0^+} -x = 0$. \n\nIn both of these problems, the solution was to use basic algebra instead of just L'Hopital's Rule. The techniques taught in introductory calculus will not solve every limit problem in the world, but they will solve the problems encountered in introductory calculus. The important thing for students is to know many techniques and learn to figure out which ones will work for a given problem. Many students learn L'Hopital's Rule and then forget how to use every other tool. This is why after teaching L'Hopital's Rule, you should throw in a few examples where L'Hopital's Rule fails. This way, they think of L'Hopital's Rule as just another tool instead of magic.", "meta": {"post_id": 912650, "input_score": 70, "output_score": 64, "post_title": "When does L' Hopital's rule fail?"}}
{"input": "While trying to find the list of axioms of ZF on the Web and in literature I noticed that the lists I had found varied quite a bit. Some included the axiom of empty set, while others didn't. \nThat is perfectly understandable - the statement of the axiom is provable from the axiom schema of specification. Some lists also contained the axiom of pairing, while others didn't - I've heard here on MSE that the statement of this axiom is also provable. \nI was wondering: are there other axioms of ZF statements of which are also provable that I don't know of? What is the true commonly accepted list of ZF axioms which doesn't contain any redundant axioms included just for emphasis?", "output": "Here is my preferred list of axioms, they are written in the language of $\\in$, and $=$ is a logical symbol.\n\nExtensionality. $\\forall x\\forall y(x=y\\leftrightarrow\\forall z(z\\in x\\leftrightarrow z\\in y))$. Two sets are equal if and only if they have the same elements.\nUnion. $\\forall x\\exists y\\forall u(u\\in y\\leftrightarrow\\exists v(v\\in x\\land u\\in v))$. If $x$ is a set, then $\\bigcup x$ is a set.\nRegularity. $\\forall x(\\exists y(y\\in x)\\rightarrow\\exists y(y\\in x\\land\\forall z(z\\in x\\rightarrow z\\notin y)))$. The $\\in$ relation is well-founded.\nPower set. $\\forall x\\exists y\\forall z(z\\in y\\leftrightarrow\\forall u(u\\in z\\rightarrow u\\in x))$. If $x$ is a set, then $\\mathcal P(x)$ is a set.\nReplacement schema. If $\\varphi(x,y,p_1,\\ldots,p_n)$ is a formula in the language of set theory, then: $$\\forall p_1\\ldots\\forall p_n\\\\ \\forall u(\\forall x(x\\in u\\rightarrow(\\exists y\\varphi(x,y,p_1,\\ldots,p_n)\\rightarrow\\exists y(\\varphi(x,y,p_1,\\ldots,p_n)\\land\\forall z(\\varphi(x,z,p_1,\\ldots,p_n)\\rightarrow z=y)))\\rightarrow\\exists v\\forall y(y\\in v\\leftrightarrow\\exists x(x\\in u\\land\\varphi(x,y,p_1,\\ldots,p_n))).$$ For every fixed parameters, $p_1,\\ldots,p_n$, and for every set $u$, if for every $x\\in u$ there is at most one $y$ such that $\\varphi(x,y,p_1,\\ldots,p_n)$, namely the formula, with the fixed parameters, define a partial function on $u$, then there is some $v$ which is exactly the range of this function.\nInfinity. $$\\exists x(\\exists y(y\\in x\\land\\forall z(z\\notin y))\\land\\forall u(u\\in x\\rightarrow\\exists v(v\\in x\\land\\forall w(w\\in v\\leftrightarrow w\\in u\\lor w=u))))\\text{.}$$ There exist a set $x$ which has the empty set as an element, and whenever $y\\in x$, then $y\\cup\\{y\\}\\in x$ as well.\n\nI wrote those purely in the language of $\\in$, as you can see, to avoid any claims that I need to use $\\subseteq$ or $\\mathcal P$ or $\\bigcup$. I will now allow myself these addition to the language.\nFrom these axioms we can easily:\n\nProve there is an empty set: it is the element of the set guaranteed to exist in the infinity axiom.\nProve the pairing axiom: By the power set axiom, $\\mathcal P(\\varnothing)$ exists, and its power set $\\{\\varnothing,\\{\\varnothing\\}\\}$ exists too. Now consider the formula $\\varphi(x,y,a,b,c,d)$ whose content is $$(x=a\\land y=c)\\lor(x=b\\land y=d).$$\nGiven two sets, $u,v$ consider the replacement axiom for $\\varphi$ with the parameters: $\\varphi(x,y,\\varnothing,\\mathcal P(\\varnothing),u,v)$, and the domain $\\mathcal{P(P(\\varnothing))}$. Then there is a set who is the range of the function $\\varphi$ defines here, which is exactly $\\{u,v\\}$.\nSpecification schema: Suppose that $\\varphi(x,p_1,\\ldots,p_n)$ is a formula in the language of set theory, and $A$ is a set which exists. Define $\\psi(x,y,p_1,\\ldots,p_n)$ to be $\\varphi(x,p_1,\\ldots,p_n)\\land x=y$. Easily we can prove that given any element of $A$ there is at most one element satisfying $\\psi(x,y,p_1,\\ldots,p_n)$ (with the fixed parameters). And therefore the range of the function defined is $\\{x\\in A\\mid\\varphi(x,p_1,\\ldots,p_n)\\}$ as wanted.\n\nAnd so on and so forth. The choice of axiomatization usually doesn't matter. But it does matter when one has to verify the axioms by hand for one reason or another, then it might be fortuitous to add explicit axioms or it might be better to keep it minimal. Depending on the situation.\nIt is also an important question what axioms you keep, or add, when you consider weakening of $\\sf ZF$. You can remove replacement, but add specification, or perhaps specification for a particular class of formulas; or you can remove extensionality and then the choice whether to use Replacement or Collection schemas really prove a big different; and so on.", "meta": {"post_id": 916072, "input_score": 31, "output_score": 34, "post_title": "What axioms does ZF have, exactly?"}}
{"input": "Pythagoras says that $\\cos^2 \\theta + \\mathrm{sin}^2\\theta = 1$ for all real $\\theta$.\n(Vague) Question. Is this the only relationship between the functions $\\cos$ and $\\sin$?\nMore precisely:\nLet $\\langle \\cos,\\sin\\rangle$ denote the intersection of all subalgebras of the $\\mathbb{R}$-algebra of all functions $\\mathbb{R} \\rightarrow \\mathbb{R}$ containing $\\{\\cos,\\sin\\}$. (By default, all my algebras are unital, associative and commutative.) Let $A$ denote the $\\mathbb{R}$-algebra presented by the generators $\\{c,s\\}$ and the relation $c^2+s^2=1$. There is a unique $\\mathbb{R}$-algebra homomorphism $\\varphi : A \\rightarrow \\langle \\cos,\\sin\\rangle$ given as follows. $$\\varphi(c) = \\cos, \\,\\,\\varphi(s) = \\sin$$\nWe know that $\\varphi$ is surjective.\n\nQuestion. Is $\\varphi$ injective?\n\nSo consider $f \\in A$. Then $f = \\sum_{i,j : \\mathbb{N}}a_{ij}s^ic^i$ for certain choices of $a_{ij} : \\mathbb{R}$. Now suppose $\\varphi(f)=0$. We want to show that $f=0$. Ideas, anyone?", "output": "If I understand well, you are asking if the $\\mathbb R$-algebra generated by $\\sin$ and $\\cos$, that is, $\\mathbb R[\\sin,\\, \\cos]$ is isomorphic to $\\mathbb R[X,Y]/(X^2+Y^2-1)$. \nConsider the surjective morphism $\\varphi:\\mathbb R[X,Y]\\to\\mathbb R[\\sin,\\,\\cos]$ defined by $\\varphi(X)=\\sin$, $\\varphi(Y)=\\cos$. Then $(X^2+Y^2-1)\\subseteq\\ker\\varphi$. Conversely, let $f\\in\\ker\\varphi$. We can write $f=(X^2+Y^2-1)g+r$ where $\\deg_Xr\\le 1$, so $r=a(Y)+b(Y)X$. Moreover, $a(\\cos)+b(\\cos)\\sin=0$. This means that $a(\\cos x)+b(\\cos x)\\sin x=0$ for all $x\\in\\mathbb R$. Changing $x$ by $-x$ we get $a(\\cos x)-b(\\cos x)\\sin x=0$ for all $x\\in\\mathbb R$, so $a(\\cos x)=0$ for all $x\\in\\mathbb R$, and $b(\\cos x)=0$ for all $x\\in\\mathbb R$, $x\\ne k\\pi$. Since $a,b$ are polynomials this is enough to conclude $a=b=0$, and therefore $r=0$. We thus proved that $\\ker\\varphi=(X^2+Y^2-1)$.", "meta": {"post_id": 937517, "input_score": 54, "output_score": 60, "post_title": "Is Pythagoras the only relation to hold between $\\cos$ and $\\sin$?"}}
{"input": "This is a Putnam Problem that I have been trying to solve (on and off) for two years, but I have failed. I am in Calculus BC. This problem comes from the book \"Calculus Eighth Edition by Larson, Hostetler, and Edwards\". This problem is at the end of the first section of the chapter 8 exercises. Here's the problem:\nEvaluate $$\\int_2^4 \\frac{\\sqrt{\\ln(9-x)}\\,dx}{\\sqrt{\\ln(9-x)} + \\sqrt{\\ln(x+3)}}.$$\nPlease. Any help is very much appreciated. So are solutions. Thank you!\nEdit: I like the solution given, but I was interested to see if there is any other way of doing the problem? I'm excited to see the results.", "output": "Let\n$$ \\mathcal{I}=\\int_{2}^{4}\\dfrac{\\sqrt{\\ln(9-x)}}{\\sqrt{\\ln(9-x)}+\\sqrt{\\ln(3+x)}}\\,\\mathrm{d}x $$\nNow, use that\n$$ \\int_{a}^{b}f(x)\\,\\mathrm{d}x\\overset{(1)}{=}\\int_{a}^{b}f(a+b-x)\\,\\mathrm{d}x $$\nThen,\n$$ \\mathcal{I}=\\int_{2}^{4}\\dfrac{\\sqrt{\\ln(3+x)}}{\\sqrt{\\ln(3+x)}+\\sqrt{\\ln(9-x)}}\\,\\mathrm{d}x $$\nAdd up these two integrals to get\n$$ 2\\mathcal{I}=\\int_{2}^{4}\\dfrac{\\sqrt{\\ln(9-x)}+\\sqrt{\\ln(3+x)}}{\\sqrt{\\ln(9-x)}+\\sqrt{\\ln(3+x)}}\\,\\mathrm{d}x $$\nThus,\n$$ \\mathcal{I}=1 $$\nIn order to prove $(1)$, write the integral using another variable, say, $t$:\n$$ \\int_{a}^{b}f(a+b-x)\\,\\mathrm{d}x=\\int_{a}^{b}f(a+b-t)\\,\\mathrm{d}t $$\nIn the latter one, set $x=a+b-t$ and $\\mathrm{d}t=-\\mathrm{d}x$ and change the limits of integration to obtain\n$$ \\begin{aligned}\n\\int_{a}^{b}f(a+b-t)\\,\\mathrm{d}t&=-\\int_{b}^{a}f(x)\\,\\mathrm{d}x\\\\\n&=\\int_{a}^{b}f(x)\\,\\mathrm{d}x.\n\\end{aligned} $$", "meta": {"post_id": 957510, "input_score": 18, "output_score": 36, "post_title": "A Putnam Integral $\\int_2^4 \\frac{\\sqrt{\\ln(9-x)}\\,dx}{\\sqrt{\\ln(9-x)} + \\sqrt{\\ln(x+3)}}.$"}}
{"input": "Let $X$ be a metric space. Prove if $X$ is compact, then $X$ is\n  separable.\n\n\nX separable $\\iff X$ contains a countable dense subset.  \n$E \\subset X $ dense in $X \\iff \\overline{E} = X$.\n$X$ compact $\\iff$ every open cover of $X$ admits a finite subcover.\n\none might also use that $X$ compact in a metric space implies closed and bounded. \nProof\nWe want to show $\\exists E \\subset X. X = \\overline{E} = E \\cup E'$  where $E'$ denotes the set of limit points. If $X$ is compact then a subset $E$ of $X$ would be compact, so that subset is closed and bounded. Since $E$ is closed, $E \\supseteq E'$. Now we need to show that $E$ is a countable dense subset... But I don't know where to go from here. I have the following:\nHint: cover $X$ with neighborhoods of radius $\\frac{1}{n}$ - so for every positive integer $n$, there are finitely many neighborhoods of radius $\\frac{1}{n}$ whose union covers $X$. So maybe it would be better to work with the open cover definition of compact.", "output": "One approach is to prove that if $X$ is a compact metric space then $X$ is totally bounded. This means that for every $\\varepsilon > 0$ there is a finite number, say $n(\\varepsilon)$, of points, call them $x_1,\\dots,x_{n(\\varepsilon)}$, such that the balls $B_\\varepsilon(x_1),\\dots,B_\\varepsilon(x_{n(\\varepsilon)})$ cover $X$. This is actually quite simple to prove, because if $X$ is a compact metric space, then given $\\varepsilon > 0$, the cover $\\{ B_{\\varepsilon}(x) : x \\in X \\}$ has a finite subcover of the desired form.\nFrom there, cover $X$ with finitely many balls of radius $1$; extract the center of each. Now every point is within $1$ of a point in your (finite) set. Cover $X$ with finitely many balls of radius $1/2$; extract the center from each. Now every point is within $1/2$ of a point in your (still finite) set. Repeat for each $1/m$ for $m \\in \\mathbb{N}$ and take the countable union. A countable union of finite sets is countable, so you have your countable dense subset.", "meta": {"post_id": 974233, "input_score": 42, "output_score": 59, "post_title": "Prove if $X$ is a compact metric space, then $X$ is separable."}}
{"input": "My brother's friend gave me the following wicked integral with a beautiful result\n\n\\begin{equation}\n{\\Large\\int_0^\\infty} \\frac{dx}{\\sqrt{x} \\bigg[x^2+\\left(1+2\\sqrt{2}\\right)x+1\\bigg] \\bigg[1-x+x^2-x^3+\\cdots+x^{50}\\bigg]}={\\large\\left(\\sqrt{2}-1\\right)\\pi}\n\\end{equation}\n\nHe claimed the above integral can be generalised to the following form\n\\begin{equation}\n{\\Large\\int_0^\\infty} \\frac{dx}{\\sqrt{x} \\bigg[x^2+ax+1\\bigg] \\bigg[1-x+x^2-x^3+\\cdots+(-x)^{n}\\bigg]}=\\ldots\n\\end{equation}\nThis is a challenging problem. How to prove it and what is the closed-form of the general integral?", "output": "Indeed let\n$$\nI(n,a)=\\int_0^\\infty\\frac{dx}{\\sqrt{x}(1+ax+x^2)(\\sum_{k=0}^n(-x)^k)}\n$$\nThe change of variables $x\\leftarrow 1/x$ yields\n$$\nI(n,a)=\\int_0^\\infty\\frac{(-1)^nx^{n+1}dx}{ \\sqrt{x}(1+ax+x^2)(\\sum_{k=0}^n(-x)^k)}\n$$\nThus\n$$\n2I(n,a)=\\int_0^\\infty\\frac{1+x}{\\sqrt{x}(1+ax+x^2)}dx=\n2\\int_0^\\infty\\frac{1+t^2}{ 1+at^2+t^4}dt\n$$\nOr equivalently, setting $u=t-1/t$,\n$$\nI(n,a)=\n\\int_{-\\infty}^\\infty\\frac{du}{ 2+a+u^2} =\\frac{\\pi}{\\sqrt{2+a}}.\n$$", "meta": {"post_id": 978560, "input_score": 57, "output_score": 56, "post_title": "Evaluating $\\int_0^\\infty \\frac{dx}{\\sqrt{x}[x^2+(1+2\\sqrt{2})x+1][1-x+x^2-x^3+...+x^{50}]}$"}}
{"input": "In a course on differential manifolds and Lie groups, the following theorem was stated, though never proven:\n\nLet $M$ and $N$ be smooth manifolds, and suppose $G$ is a Lie group acting on $M$. If the group action is free and proper, then $M/G$ has a manifold structure so that the quotient map $\\pi:M\\to M/G$ is smooth. Additionally, if $f:M/G\\to N$ and $f\\circ\\pi$ is smooth, then $f$ is smooth.\n\nThe definition of a proper group action was given as follows: given any compact set $K \\subset M$, the action of $G$ on $M$ is proper if and only if the set $\\{g\\in G:gK\\cap K\\neq\\emptyset\\}$ has compact closure in $G$.\nI am confused about two things:\n\n1) The definition of proper group action differs from the one I got from Wikipedia and the books I have, where the acting group is assumed to be discrete. Moreover, I found it highly contrived due to a lack of understanding on my part. What exactly is this definition helping us to see? The need for the closure to be compact, for instance, is lost on me.\n2) Removing the context of manifolds and Lie groups, if one were to look at arbitrary group actions, then how would the proper action be defined? What kind of topology on G makes sense?\n\nI apologise for any naivet\u00e9; I am new to group actions, even in the algebraic case. Thank you for your patience!", "output": "If $G$ is a topological group acting on a topological space $M$, the usual definition is that the action is proper if the map $G\\times M\\to M\\times M$ defined by $(g,x)\\mapsto (g\\cdot x,x)$ is a proper map, which means that the preimage of every compact set is compact. This definition works both in the topological category and in the smooth category. \nFor sufficiently nice spaces, there are other characterizations that are often useful. For example:\n\nIf $M$ is Hausdorff, then properness is equivalent to the condition you described.\nIf $M$ and $G$ are Hausdorff and first-countable, then properness is equivalent to the following condition: If $(x_i)$ is a sequence in $M$ and $(g_i)$ is a sequence in $G$ such that both $(x_i)$ and $(g_i\\cdot x_i)$ converge, then a subsequence of $(g_i)$ converges.\n\nActions of compact groups (on Hausdorff spaces) are always proper, so properness really has meaning only for actions of noncompact groups. In this case, properness has the intuitive meaning that \"most\" of $G$ (i.e., all but a compact subset) moves compact sets of $M$ far away from themselves.  \nYou can find more information on group actions in the topological category in my Introduction to Topological Manifolds (2nd ed.), and in the smooth category in Introduction to Smooth Manifolds (2nd ed.), among other places.", "meta": {"post_id": 987038, "input_score": 20, "output_score": 44, "post_title": "Clarification of notion of proper group action."}}
{"input": "I've been studying differential geometry using Do Carmo's book. There's the notion of exponential map, but I don't understand why it is called \"exponential\" map. How does it has something to do with our common notion of exponentiation? \nI read from the book The road to reality (by R. Penrose) that it is related to taking exponentiation when making (finite) Lie group elements from Lie algebra elements. It seems like using Taylor's theorem on a manifold so we have, for example, there was the following equation explaining why it is the case. \n$f(t) = f(0) + f'(0)t + \\frac{1}{2!}f''(0) t^2+\\cdots = (1+t\\frac{d}{dx}+\\frac{1}{2!}t^2\\frac{d^2}{dx^2}+\\cdots)f(x)|_{x=0} = e^{t\\frac{d}{dx}}f(x)|_{x=0}$. \nThe differential operator can be thought of as a vector field on a manifold, and it is how Lie algebra elements (which are vectors, on a group manifold (Lie group), in a tangent space at the identity element). If I understood correctly, the truth is that this is exactly the exponential map that sends a vector on a tangent space into the manifold in such a way that it becomes the end point of a geodesic (determined by the vector) having the same length. \nWhy is the above Taylor expansion valid on a manifold? Why is the exponential map the same as taking exponentiation?", "output": "The idea of an exponential is the continuous compounding of small actions. Suppose you start with an object $p$, perform an action on it $v$, and then add the result back to the original object. What happens if you instead take half as much action but do it twice? What about if you take one tenth the action but do it ten times? The exponential function tries to capture this idea:\n$$\\exp (\\text{action}) = \\lim_{n \\rightarrow \\infty} \\left(\\text{identity} + \\frac{\\text{action}}{n}\\right)^n.$$\nOn a differentiable manifold there is no addition, but we can consider this action as pushing a point a short distance in the direction of the tangent vector,\n$$``\\left(\\text{identity} + \\frac{\\text{v}}{n}\\right)\"p := \\text{push }p\\text{ by} \\frac{1}{n} \\text{ units of distance in the }v \\text{ direction}.$$\nDoing this over and over, we have \n$``\\left(\\text{identity} + \\frac{\\text{v}}{n}\\right)^n\"p$ means push $p$ by $\\frac{1}{n}$ units of distance in the $v$ direction, then push it again in the same direction you already pushed it, and keep doing so until you have pushed it $n$ times. \nSo long as $\\frac{1}{n}$ is small enough that pushing points and vectors in a tangent direction makes sense, what we end out doing is pushing the point $p$ a total of $1$ unit of distance along the geodesic generated by $v$.", "meta": {"post_id": 999515, "input_score": 37, "output_score": 67, "post_title": "Meaning of Exponential map"}}
{"input": "Is an irrational number times a rational number always irrational?\nIf the rational number is zero, then the result will be rational. So can we conclude that in general, we can't decide, and it depends on the rational number?", "output": "Any nonzero rational number times an irrational number is irrational. Let $r$ be nonzero and rational and $x$ be irrational. If $rx=q$ and $q$ is rational, then $x=q/r$, which is rational. This is a contradiction.", "meta": {"post_id": 1009570, "input_score": 10, "output_score": 54, "post_title": "Is irrational times rational always irrational?"}}
{"input": "A group of ten people sits down, uniformly at random, around a table. Ken\n  and John are part of this group. Determine the probability that Ken and John sit\n  next to each other.\n\nThere are $10!$ ways to arrange the seating for everyone, there are 10 possible ways for John and Ken to sit together.\n$$\\operatorname{Pr}(J\\ \\&\\ K ) = \\frac{10}{10!} = \\frac{1}{9!}$$\nAm I correct?", "output": "Two of nine people sit next to John.  The probability that Ken is one of these two is $\\frac29$.", "meta": {"post_id": 1016151, "input_score": 19, "output_score": 57, "post_title": "Probability that Ken and John set next to each other"}}
{"input": "If both the Poisson and Binomial distribution are discrete, then why do we need two different distributions?", "output": "The binomial distribution counts discrete occurrences among discrete trials.\nThe poisson distribution counts discrete occurrences among a continuous domain.\nIdeally speaking, the poisson should only be used when success could occur at any point in a domain.  Such as, for example, cars on a road over a period of time, or random knots in a string over a length, etc.  We are talking about infinitely many infinitesimally small trials, each having at most one success.\nIn practice, though, the poisson can be used to approximate the binomial under certain conditions, but it is only a rough approximation. Such as using the Normal curve in place of a Binomial under the right conditions.", "meta": {"post_id": 1050184, "input_score": 50, "output_score": 35, "post_title": "Difference between Poisson and Binomial distributions."}}
{"input": "Suppose $f,g$ are two Riemann  Integrable functions .Is it true that $f\\circ g$ is also Riemann  Integrable?\nTrying this for a long time but not getting the answer", "output": "A function on a bounded interval is Riemann-integrable iff it is bounded and almost everywhere continuous. So the functions\n$$\nf(x) = \\begin{cases} 1 & \\text{ for }x \\ne 0 \\\\ 0 & \\text{ for } x = 0  \\end{cases}\n\\quad \\text{ and } \\quad\ng(x) = \\begin{cases} 1/q & \\text{ for }x=p/q \\\\ 0 & \\text{ for } x \\notin \\mathbb{Q}  \\end{cases}\n$$\nare Riemann-integrable over any bounded interval, since $f$ is continuous everywhere except at $0$, and $g$ is continuous at every irrational $x$. (In the definition $x=p/q$ is the unique representation of rational $x$ with $p$ and $q$ relatively prime integers and $q>0$.)\nThe composition of these functions is\n$$\nf(g(x)) = \\begin{cases} 1 & \\text{ for }x \\in \\mathbb{Q} \\\\ 0 & \\text{ for } x \\notin \\mathbb{Q}  \\end{cases}\n$$\nwhich is nowhere continuous, so not Riemann-integrable over any interval.", "meta": {"post_id": 1060834, "input_score": 32, "output_score": 82, "post_title": "Composition of two Riemann integrable functions"}}
{"input": "I'm a student just starting calculus in college, and my math skills are pretty stale.\nSo... how come finding limits using change of variable works?\nFor example:\n$$\\lim_{x \\to 1}\\frac{x\\cos(x-1) -1}{x-1}$$\nA way to solve this is to invent \"out of thin air\" $t = x-1$, and then the limit above is equal to:\n$$\\lim_{t \\to 0}\\frac{(t + 1)\\cos(t) - 1}{t}$$\nHow come this works?\nA limit is not an algebraic equation. what about domains of definition? \nWe are actually finding a different limit of a different function in a different place, how come they are equal (in general)?\n\njust to clarify I'm not asking about this specific example. I'm asking in general, when can you do this to find limits? when not? and why?", "output": "There is a very general result which guarantees such substitutions. Let $\\lim\\limits_{x \\to a}f(x) = L$ exist and let $\\lim\\limits_{t \\to b}g(t) = a$ exist and also assume that $g(t) \\neq a$ when $t$ is in a certain neighborhood of $b$ then $\\lim_{t \\to b}f(g(t)) = L$.\nPlease understand that the theorem is valid only under the conditions given in the above result and one of the first conditions is that $\\lim_{x \\to a}f(x)$ exists. If we don't know in advance whether the limit of $f(x)$ exists then how do we make a substitution $x = g(t)$ (in this question we put $x = t + 1$)?\nTo answer this we need to understand that the substitution $x = g(t)$ ($x = t + 1$) used here is invertible so that we have an inverse substitution $t = h(x)$ ($t = x - 1$) with $x = g(h(x)), t = h(g(t))$ which will allow us to infer the existence of limit $\\lim_{x \\to a}f(x)$ on the basis of existence of limit $\\lim_{t \\to b}f(g(t))$ via the theorem given in the beginning of this post.\nAnother condition which is very very important is to ensure that $g(t) \\neq a$ when $t$ is near $b$. Clearly this holds in the substitution used in the current question when $x = t + 1$ and $a = 1, b = 0$.\nIf we think deeply we will find that if $g(t)$ is invertible in the neighborhood of $t = b$ then it will automatically ensure that $g(t) \\neq a$ in a certain neighborhood of $b$. So in practice we use the following :\n\nTheorem: If $x = g(t)$ is an invertible function with inverse $t = h(x)$ in the deleted neighborhood of $t = b$ and $\\lim\\limits_{t \\to b}g(t) = a, \\lim\\limits_{x \\to a}h(x) = b$ then either both the limits $\\lim\\limits_{x \\to a}f(x)$ and $\\lim\\limits_{t \\to b}f(g(t))$ exist and are equal or both of them don't exist.\n\nNote that there is no condition on $f$ for the above theorem.", "meta": {"post_id": 1069642, "input_score": 39, "output_score": 37, "post_title": "Finding a limit using change of variable- how come it works?"}}
{"input": "I'm planning on self-studying linear algebra, and trying to decide on a book.  I'm thinking of using Hoffman and Kunze.\n What sort of experience is required to handle Hoffman and Kunze?\nSo far, I've read most of Axler's Linear Algebra Done Right. (It was for a class in high school, so we just worked through it and got as far as we got.) I feel like I understand it pretty well, and I really liked it, but I've read that it has a rather unusual approach and I would like to try something different.\nI've read that Hoffman and Kunze is good, but that it is heavy on the algebra.  I'm not sure how do calibrate that, though.  Does it mean \"Don't use it for linear algebra for engineers\" or \"You should have a year of algebra, but if you have that, it's not a big deal\".  (I guess it's somewhere in between.)\nI specifically like that it includes a strong emphasis on matrices, which are pointedly ignored in Axler, without devolving into being just a manual for computation.\nThis is my impression of the book from having read around (mostly here), but if something of it is wrong, please correct me.  I have very little experience to provide comparison and normalize the different recommendations I've read.", "output": "Hoffman & Kunze is to linear algebra what baby Rudin is to analysis.  If you plan to major in mathematics or physics, you should read it, study it, and do as many exercises from it as possible.  The amount of abstract algebra in H&K is minimal, and all the definitions/background is provided so that the text is self contained.  So if you were fine with Axler, you'll have no issue (though the prose in H&K is significantly dryer and more demanding than the conversational Axler, and may take some getting used to).\nThe reason why Axler's approach is \"unusual\" is that he doesn't use determinants in his presentation of eigenvalues, etc.  This is nice in theory and provides alternative proofs which can be illuminating, but ultimately the approach limits one's ability to perform concrete calculations.  Also, determinants cannot be banished forever in linear algebra, so at some point you need to learn about them, and the chapter in H&K covering them is excellent (though probably the most technically difficult chapter in the text).  Other than that, it's all basically standard fare and the level of difficulty is comparable to H&K, though H&K covers quite a bit more than Axler.", "meta": {"post_id": 1079266, "input_score": 20, "output_score": 41, "post_title": "Is Hoffman-Kunze a good book to read next?"}}
{"input": "Could anyone help me to find the mistake in the following problem? Based on the formula of the sum of a geometric series:\n\\begin{equation}\n1 + x + x^{2} + \\cdots + x^{n} + \\cdots = \\frac{1}{1 - x}\n\\end{equation}\n\\begin{equation}\n1 + \\frac{1}{x} + \\frac{1}{x^{2}} + \\cdots + \\frac{1}{x^{n}} + \\cdots = \\frac{1}{1 - 1/x} = \\frac{x}{x-1}\n\\end{equation}\nAdding both equations\n\\begin{equation}\n2 + x + \\frac{1}{x} + x^{2} + \\frac{1}{x^{2}} + \\cdots + x^{n} + \\frac{1}{x^{n}} + \\cdots = \\frac{1}{1 - x} + \\frac{x}{x-1} = \\frac{1-x}{1-x} = 1\n\\end{equation}\nSo,\n\\begin{equation}\n2 + x + \\frac{1}{x} + x^{2} + \\frac{1}{x^{2}} + \\cdots + x^{n} + \\frac{1}{x^{n}} + \\cdots = 1\n\\end{equation}\nAnd the left side is always bigger than $2$ for $x>0$. \nWhat is wrong?? Thanks in advance", "output": "The first series only applies when $|x| < 1$ whereas the second series only applies when $\\left|\\frac{1}{x}\\right| < 1$ (i.e. $|x| > 1$). By adding them, you are assuming that they both apply simultaneously, but they don't (for any $x$).", "meta": {"post_id": 1081892, "input_score": 22, "output_score": 38, "post_title": "What is wrong with the sum of these two series?"}}
{"input": "Prove that a function $f:\\mathbb{R}\\to\\mathbb{R}$ which satisfies\n$$f\\left({\\frac{x+y}3}\\right)=\\frac{f(x)+f(y)}2$$\nis a constant function.\nThis is my solution: constant function have derivative $0$ for any number, so I need to prove that $f'$ is always $0$. I first calculated $\\frac{d}{dx}$ and then $\\frac{d}{dy}$:\n$$f'\\left({\\frac{x+y}3}\\right)\\frac13=\\frac{f'(x)}2$$\n$$f'\\left({\\frac{x+y}3}\\right)\\frac13=\\frac{f'(y)}2$$\nFrom this I can see that $\\frac{f'(x)}2=\\frac{f'(y)}2$. Multiplying by $2$ and integrating I got:\n$$f(x)=f(y)+C$$\nfor some constant $C\\in\\mathbb{R}$. By definition of $f$ it is true for any $x,y\\in\\mathbb{R}$, so I can write\n$$f(y)=f(x)+C$$\nAdding this two equation and simplifying I got\n$$C=0$$\nso $f(x)=f(y)$ for all $x,y\\in\\mathbb{R}$. Is my solution mathematically correct. Is this complete proof, or I missed something?", "output": "It is not even necessary to assume that $f$ is continuous.\n\nBy letting $y = 2x$, we see that $f(x) = f(2x)$\nLetting $y = -4x$, we get $f(-x) = \\frac{f(x) + f(-4x)}{2}$.  However, from (1), $f(-4x) = f(-2x) = f(-x)$, so this simplifies to $f(-x) = f(x)$\nFinally, let $y = -x$ and simplifying gives $2f(0) = f(x) + f(-x)$.  Substituting in from (2), this becomes $f(0) = f(x)$.  Since this holds for all $x \\in \\mathbb{R}$, we conclude that $f$ must be constant.", "meta": {"post_id": 1093227, "input_score": 21, "output_score": 53, "post_title": "A function that satisfies $f\\left({\\frac{x+y}3}\\right)=\\frac{f(x)+f(y)}2$ must be a constant"}}
{"input": "If $G$ is a group of even order, prove it has an element $a \\neq e$ satisfying $a^2 = e$.\nMy proof:\nLet $|G| = 2n$. Since $G$ is finite, there exists, $a \\in G$ such that $a^p = e$ and by Lagrange's Theorem, p divides 2n. By Euclid's lemma, since p does not divide 2, p divides n. Let $n = pk$. Hence, $(a^n)^2 = (a^{pk})^2 = ((a^p)^k)^2 = (e^k)^2 = e$. Therefore, $a^n$ is an element that satisfy the condition.\nIs my solution OK?\nFor this problem, I am just wondering how I can solve this problem without using Lagrange's Theorem, as this problem is an exercise before the Lagrange's Theorem was taught.", "output": "The following is perhaps one of most simple proofs:\nPair up if possible each element of $\\;G\\;$ with its inverse, and observe that\n$$g^2\\neq e\\iff g\\neq g^{-1}\\iff \\;\\text{there exists the pair}\\;\\;(g, g^{-1})$$\nNow, there is one element that has no pairing: the unit $\\;e\\;$ (since indeed $\\;e=e^{-1}\\iff e^2=e$), so since the number of elements of $\\;G\\;$ is even there must be at least one element more, say $\\;e\\neq a\\in G\\;$ ,  without a pairing, and thus $\\;a=a^{-1}\\iff a^2=e\\;$", "meta": {"post_id": 1111868, "input_score": 27, "output_score": 84, "post_title": "If $G$ is a group of even order, prove it has an element $a\\neq e$ satisfying $a^2=e$."}}
{"input": "I read through similar questions, but I couldn't find an answer to this:\nHow do you determine the symmetric matrix A if you know:\n$\\lambda_1 = 1, \\  eigenvector_1 = \\pmatrix{1& 0&-1}^T;$\n$\\lambda_2 = -2, \\ eigenvector_2 = \\pmatrix{1& 1& 1}^T;$\n$\\lambda_3 = 2,  \\ eigenvector_3 = \\pmatrix{-1& 2& -1}^T;$\nI tried to solve it as an equation system for each line, but it didn't work somehow.\nI tried to find the inverse of the eigenvectors, but it brought a wrong matrix.\nDo you know how to solve it?\nThanks!", "output": "Writing the matrix down in the basis defined by the eigenvalues is trivial.  It's just\n$$\nM=\\left(\n\\begin{array}{ccc}\n  1 & 0 & 0 \\\\\n  0 & -2 & 0 \\\\\n  0 & 0 & 2\n\\end{array}\n\\right).\n$$\nNow, all we need is the change of basis matrix to change to the standard coordinate basis, namely:\n$$\nS = \\left(\n\\begin{array}{ccc}\n 1 & 1 & -1 \\\\\n 0 & 1 & 2 \\\\\n -1 & 1 & -1 \\\\\n\\end{array}\n\\right).\n$$\nThis is just the matrix whose columns are the eigenvectors.  We can change to the standard coordinate bases by computing $SMS^{-1}$.  We get\n$$\nSMS^{-1} = \\frac{1}{6}\\left(\n\\begin{array}{ccc}\n 1 & -8 & -5 \\\\\n -8 & 4 & -8 \\\\\n -5 & -8 & 1 \\\\\n\\end{array}\n\\right).\n$$\nYou can check that this matrix has the desired eigensystem.  For example,\n$$\n\\frac{1}{6}\\left(\n\\begin{array}{ccc}\n 1 & -8 & -5 \\\\\n -8 & 4 & -8 \\\\\n -5 & -8 & 1 \\\\\n\\end{array}\n\\right)\n\\left(\n  \\begin{array}{c}\n    -1 \\\\ 2 \\\\ -1\n  \\end{array}\n\\right)\n=\n\\left(\n  \\begin{array}{c}\n    -2 \\\\ 4 \\\\ -2\n  \\end{array}\n\\right).\n$$", "meta": {"post_id": 1119668, "input_score": 28, "output_score": 43, "post_title": "Determine a matrix knowing its eigenvalues and eigenvectors"}}
{"input": "$A  \\underset{\\mathbb{C}}{\\sim} B \\overset{\\text{def}}{\\iff} A=C^{-1}BC, \\space C\\in M_{n}(\\mathbb{C})$ and similarly for $\\underset{\\mathbb{R}}{\\sim}$.\n\nI want to prove that $ A \\underset{\\mathbb{C}}{\\sim} B$ for $A,B \\in M_{n}(\\mathbb{R})$ therefore $A \\underset{\\mathbb{R}}{\\sim} B$.\n\nMy idea is that elementary divisors of $A,B$ over $\\mathbb{C}$ are the same, and if $(x-z)^k$ is elementary divisor than $(x-\\overline{z})^k$ is also elementary divisor $\\implies$ $A,B$ have same elementary divisors over $\\mathbb{R}$. But i think it's not clear.", "output": "If $ A \\underset{\\mathbb{C}}{\\sim} B$, there is a matrix $C \\in GL_n(\\mathbb{C})$ such that $A=C^{-1}BC$.\nSo $CA=BC$.\n$C=P+iQ$ with $P,Q \\in M_n(\\mathbb{R})$.\nIf $A \\in M_n(\\mathbb{R})$ and $B \\in M_n(\\mathbb{R})$, we have $CA=BC \\implies (P+iQ)A=B(P+iQ)\\implies PA=BP$ and $QA=BQ$.\nThe polynomial $\\det (P+XQ)$ is not null, because $\\det(P+iQ)=\\det C \\neq 0$.\nSo, there is a value $\\lambda \\in \\mathbb{R}$ such hat $\\det(P+\\lambda Q) \\neq 0$.\nLet $D=P+\\lambda Q$, $DA=BD$ because $PA=BP$ and $QA=BQ$.\nSo, $A=D^{-1}BD$ and $A \\underset{\\mathbb{R}}{\\sim} B$", "meta": {"post_id": 1129628, "input_score": 25, "output_score": 38, "post_title": "Similarity of real matrices over $\\mathbb{C}$"}}
{"input": "I am trying to prove that $\\mathbb{R}$ with the lower limit topology is not second-countable.\nTo do this, I'm trying to form an uncountable union $A$ of disjoint, half-open intervals of the form $[a, b)$, $a < b$. Is this possible? I think this would imply the $A$ is open but no countable union of basis elements could coincide with $A$ therefore making the real numbers with the lower limit topology not second-countable.\nI think there must exist something like $A$ described above but I am having trouble visualizing it and coming up with a formula to represent it.\nMaybe there is some other way to show it is not second-countable.", "output": "Suppose $\\mathcal B$ is a base for the \"lower limit\" topology on $\\mathbb R$, better known as the Sorgenfrey line. By the definition of a base for a topology, for any open set $U$ and any point $x\\in U$ there is a basic open set $B\\in\\mathcal B$ such that $x\\in B\\subseteq U$. Hence, for any point $x\\in\\mathbb R$, since $[x,\\infty)$ is an open set containing $x$, we can choose a set $B_x\\in\\mathcal B$ with $\\min B_x=x$. Since the sets $B_x(x\\in\\mathbb R)$ are distinct, this shows that $|\\mathcal B|\\ge|\\mathbb R|\\gt\\aleph_0$.", "meta": {"post_id": 1135993, "input_score": 32, "output_score": 53, "post_title": "$\\mathbb{R}$ with the lower limit topology is not second-countable"}}
{"input": "I came across this simple proof of Fermat's last theorem. Some think it's legit. Some argued that the author's assumptions are flawed. It's rather lengthy but the first part goes like this:\nLet $x,y$ be $2$ positive non-zero coprime integers and $n$ an integer greater than $2$. According to the binomial theorem:$$(x+y)^n=\\sum_{k=0}^{n}\\binom{n}{k}x^{n-k}{y^k}$$\nthen,$$(x+y)^n-x^n=nx^{n-1}y+\\sum_{k=2}^{n-1}\\binom{n}{k}x^{n-k}{y^k}+y^{n}$$\n$$(x+y)^n-x^n=y(nx^{n-1}+\\sum_{k=2}^{n-1}\\binom{n}{k}x^{n-k}y^{k-1}+y^{n-1})$$\n$$y(nx^{n-1}+\\sum_{k=2}^{n-1}\\binom{n}{k}x^{n-k}y^{k-1}+y^{n-1})=z^n$$\nIn the first case, he assumed that the 2 factors are coprime when $\\gcd(y,n)=1$ . Then he wrote: \n$$y=q^n$$\n$$ nx^{n-1}+\\sum_{k=2}^{n-1}\\binom{n}{k}x^{n-k}y^{k-1}+y^{n-1}=p^n$$\nBy replacing $y$ by $q^n$,\n\\begin{equation} nx^{n-1}+\\sum_{k=2}^{n-1}\\binom{n}{k}x^{n-k}q^{n(k-1)}+q^{n(n-1)}=p^n (*)\n\\end{equation}\nfrom this bivariate polynomial,he fixed alternatively $x$ and $y=q^n$ and by applying the rational root theorem, he obtained  $$q^{n(n-1)}-p^n=nxt   $$ and\n$$  nx^{n-1}-p^n=q^ns $$ \n($s,t$ non-zero integers)\nby equating $p^x$: $$ q^{n(n-1)}-sq^n=nx(t-x^{n-2})$$\nThen, he uses one of the trivial solutions of Fermat's equations. He wrote, when $x+y=1$,if $x=0$ then $y=1$ and vice versa. \nTherefore, he wrote: \n$x=0$ iff $q^{n(n-1)}=sq^n$, he obtains: $$q=1$$ or $$s=q^{n-2}$$ \nBy substituting $s$ by $q^{n-2}$ in $nx^{n-1}-p^n=q^ns$, he obtains: $$nx^{n-1}-p^n=q^{n(n-1)}$$\nThen, he replace that expression in equation (*) and pointed out that:$$\\sum_{k=2}^{n-1}\\binom{n}{k}x^{n-k}q^{n(k-1)}=0$$. Since $x,y=q^n$ are positive integers for all $n>2$, a sum of positive numbers can not be equal to zero. Which leads to a contradiction.\nWhat do you think?", "output": "There is a \"trick\", due to Marc Krasner, which prevents you from wasting time in examining \"elementary\" arithmetic proofs of Fermat's Last Theorem. \"Elementary\" means precisely that the proof uses only addition and multiplication (operations in a ring), and perhaps also the existence and unicity of decomposition into prime factors (so the ring in question is factorial). I suppose this is the case here, although not all details are given. Then, without checking anything, you can be assured that the reasoning is certainly wrong. This is because all such \"elementary\" arguments can be repeated word for word in the ring $Z_p$ of p-adic integers, which is factorial (and a lot more !), but in which FLT is false, because in the field $Q_p$ of p-adic numbers, the equation $x^p + y^p = 1$ always has non trivial solutions (if you take $y$ to be a high power of $p$, then p-adic analysis tells you that $1 - y^p$ has a p-th root in $Q_p$).", "meta": {"post_id": 1139251, "input_score": 10, "output_score": 92, "post_title": "Fermat's Last Theorem simple proof"}}
{"input": "How  evaluate this integral?\n $$I=\\int_0^{\\pi/2}\\frac{x^2\\log^2{(\\sin{x})}}{\\sin^2x}\\,dx$$\n Note: $$\\int_0^{\\pi/2}\\frac{x^2\\log{(\\sin x)}}{\\sin^2x}dx=\\pi\\ln{2}-\\frac{\\pi}{2}\\ln^22-\\frac{\\pi^3}{12}.$$", "output": "Let's start out with the auxiliary result\n\\begin{equation*}\n\\int_0^{\\pi/2}\\frac{x^2\\log{(\\sin(x))}}{\\sin^2(x)}dx=\\pi\\ln{(2)}-\\frac{\\pi}{2}\\ln^2(2)-\\frac{\\pi^3}{12}.\n\\end{equation*}\nBy the integration by parts all reduces to \n$$\\int_0^{\\pi/2} \\cot(x) (x^2 \\cot(x)+2 x\\log(\\sin(x))) \\ dx=\\int_0^{\\pi/2} x^2 \\cot^2(x) \\ dx+2\\int_0^{\\pi/2} x\\cot(x) \\log(\\sin(x))) \\ dx.$$\nFor the first integral in the right-hand side we apply the integration by parts that yields\n\\begin{equation*}\n\\begin{aligned}\n\\int_0^{\\pi/2} x^2 \\cot^2(x) \\ dx &=-\\frac{\\pi^3}{8}+\\int_0^{\\pi/2} 2 x (x+\\cot (x)) \\ dx \\\\\n                                                        &=-\\frac{\\pi^3}{24}+2\\int_0^{\\pi/2}x \\cot (x) \\ dx \\\\\n                                                        &=-\\frac{\\pi^3}{24}-2\\int_0^{\\pi/2} \\log(\\sin(x)) \\ dx \\\\\n                                                        &=-\\frac{\\pi^3}{24}-\\int_0^{\\pi} \\log(\\sin(x)) \\ dx \\\\                                                        \n\\end{aligned}\n\\end{equation*}\nwhere in the penultimate equality we used again the integration by parts, and then the symmetry.\nThen, \n\\begin{equation*}\n\\begin{aligned}\n\\int_0^{\\pi} \\log(\\sin(x)) \\ dx &=\\int_0^{\\pi} \\log(2\\sin(x/2)\\cos(x/2)) \\ dx \\\\\n                                                &=\\pi \\log(2)+ \\int_0^{\\pi} \\log(\\sin(x/2)) \\ dx+ \\int_0^{\\pi} \\log(\\cos(x/2)) \\ dx.                                    \n\\end{aligned}\n\\end{equation*}\nLetting $x/2=y$ in both integrals in the right-hand side, we obtain that \n\\begin{equation*}\n\\begin{aligned}\n\\int_0^{\\pi} \\log(\\sin(x)) \\ dx &=\\pi \\log(2) + 2\\int_0^{\\pi/2} \\log(\\sin(x)) \\ dx+2\\int_0^{\\pi/2} \\log(\\cos(x)) \\ dx \\\\      \n                                                &=\\pi \\log(2) + 4\\int_0^{\\pi/2} \\log(\\sin(x)) \\ dx \\\\ \n                                                &=\\pi \\log(2) + 2\\int_0^{\\pi} \\log(\\sin(x)) \\ dx \\\\                               \n\\end{aligned}\n\\end{equation*}\nwhence we get that\n$$\\int_0^{\\pi} \\log(\\sin(x)) \\ dx =-\\pi\\log(2).$$\nThen, \n$$\\int_0^{\\pi/2} x^2 \\cot^2(x) \\ dx=\\pi\\log(2)-\\frac{\\pi^3}{24}.$$\nOn the other hand, the integration by parts yields that\n\\begin{equation*}\n\\begin{aligned} \n2\\int_0^{\\pi/2}x\\cot(x) \\log(\\sin(x)) \\ dx &=-2\\int_0^{\\pi/2}  (\\log ^2(\\sin (x))+ x \\cot (x) \\log (\\sin (x))) \\ dx \\\\\n                                                                    &=-2\\int_0^{\\pi/2}  \\log ^2(\\sin (x)) \\ dx -2\\int_0^{\\pi/2} x \\cot (x) \\log (\\sin (x)) \\ dx\n\\end{aligned}\n\\end{equation*}\nwhence we have that \n$$\\int_0^{\\pi/2}x\\cot(x) \\log(\\sin(x)) \\ dx=-\\frac{1}{2}\\int_0^{\\pi/2}  \\log ^2(\\sin (x)) \\ dx.$$\nAccording to the trigonometric form of the beta function, we have that \n$$\\int_0^{\\pi/2} \\sin^a(x)\\cos^b(x) \\ dx=\\frac{1}{2}B \\left(\\frac{1}{2}(a+1),\\frac{1}{2}(b+1)\\right).$$\nDifferentiating $2$ times with respect to $a$ and then letting $a\\to 0$ and $b\\to 0$, we obtain that\n\\begin{equation*}\n\\begin{aligned} \n\\int_0^{\\pi/2}  \\log ^2(\\sin (x)) \\ dx &=\\frac{1}{2} \\lim_{b \\to 0} \\lim_{a \\to 0} \\frac{\\partial^2}{\\partial a^2}\\left(B \\left(\\frac{1}{2}(a+1),\\frac{1}{2}(b+1)\\right)\\right) \\\\\n                                                           &=\\frac{1}{24} \\left(\\pi ^3+12 \\pi  \\log ^2(2)\\right).\n\\end{aligned}\n\\end{equation*}\nThus, \n$$\\int_0^{\\pi/2}x\\cot(x) \\log(\\sin(x)) \\ dx=-\\frac{1}{48} \\left(\\pi ^3+12 \\pi  \\log ^2(2)\\right).$$\nand finally our auxiliary result is proved.\n$$\\int_0^{\\pi/2}\\frac{x^2\\log{(\\sin(x))}}{\\sin^2(x)}dx=\\pi\\ln{(2)}-\\frac{\\pi}{2}\\ln^2(2)-\\frac{\\pi^3}{12}.$$\nNow, we prove the main result, \n\\begin{equation*}\n\\int_0^{\\pi/2}\\frac{x^2\\log^2{(\\sin(x))}}{\\sin^2(x)}dx=\\left(\\frac{\\pi ^3 }{6} +2 \\pi \\right)  \\log (2)+\\frac{1}{3} \\pi  \\log ^3(2)+\\frac{1}{8}\\pi  \\zeta (3)-\\frac{\\pi ^3}{6}-\\pi  \\log ^2(2).\n\\end{equation*}\nApplying the integration by parts, we get \n$$2\\int_0^{\\pi/2} x^2 \\cot ^2(x) \\log (\\sin (x)) \\ dx+2 \\int_0^{\\pi/2} x \\cot (x) \\log ^2(\\sin (x)) \\ dx$$\nFor the integral in the left side we make use of the integration by parts that yields \n$$2\\int_0^{\\pi/2} x^2 \\cot ^2(x) \\log (\\sin (x)) \\ dx$$\n$$=2 \\int_0^{\\pi/2} x^3 \\cot (x) \\ dx + 2 \\int_0^{\\pi/2} x^2 \\cot ^2(x) \\ dx + 4\\int_0^{\\pi/2} x^2 \\log (\\sin (x)) \\ dx+4  \\int_0^{\\pi/2} x \\cot (x) \\log (\\sin (x)) \\ dx$$\n$$=\\frac{2}{3} \\int_0^{\\pi/2} x^3 \\cot (x) \\ dx + 2 \\int_0^{\\pi/2} x^2 \\cot ^2(x) \\ dx +4  \\int_0^{\\pi/2} x \\cot (x) \\log (\\sin (x)) \\ dx$$\nand since the last $2$ integrals are already compute (see the auxiliary result), we obtain \n$$\\frac{2}{3} \\int_0^{\\pi/2} x^3 \\cot (x) \\ dx + 2 \\int_0^{\\pi/2} x^2 \\cot ^2(x) \\ dx +4  \\int_0^{\\pi/2} x \\cot (x) \\log (\\sin (x)) \\ dx$$\n$$=\\frac{2}{3} \\int_0^{\\pi/2} x^3 \\cot (x) \\ dx+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}$$\nand integrating by parts again, we get \n$$-2\\int_0^{\\pi/2} x^2 \\log(\\sin(x)) \\ dx+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}.$$\nUsing that $\\displaystyle \\log(\\sin(x))=-\\log(2)-\\sum_{n=1}^{\\infty} \\frac{\\cos(2 n x)}{n}$, we obtain\n$$-2\\int_0^{\\pi/2} x^2\\left(-\\log(2)-\\sum_{n=1}^{\\infty} \\frac{\\cos(2 n x)}{n}\\right)\\ dx+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}$$\n$$=2\\log(2)\\int_0^{\\pi/2} x^2\\ dx+2\\int_0^{\\pi/2} x^2 \\sum_{n=1}^{\\infty} \\frac{\\cos(2 n x)}{n} \\ dx+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}$$\n$$=2\\int_0^{\\pi/2} x^2 \\sum_{n=1}^{\\infty} \\frac{\\cos(2 n x)}{n} \\ dx+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}+\\frac{1}{12} \\pi ^3 \\log (2)$$\n$$=2  \\sum_{n=1}^{\\infty} \\int_0^{\\pi/2} x^2 \\frac{\\cos(2 n x)}{n} \\ dx+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}+\\frac{1}{12} \\pi ^3 \\log (2)$$\n$$=-\\frac{3}{8} \\pi \\zeta(3)+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}+\\frac{1}{12} \\pi ^3 \\log (2).$$\nTherefore, we have that \n$$2\\int_0^{\\pi/2} x^2 \\cot ^2(x) \\log (\\sin (x)) \\ dx=-\\frac{3}{8} \\pi \\zeta(3)+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}+\\frac{1}{12} \\pi ^3 \\log (2).$$\nFor the remaining integral, we use the integration by parts again that yields \n$$2 \\int_0^{\\pi/2} x \\cot (x) \\log ^2(\\sin (x)) \\ dx=-2\\int_0^{\\pi/2} \\log ^3(\\sin (x)) \\ dx-4 \\int_0^{\\pi/2} x \\cot (x) \\log ^2(\\sin (x)) \\ dx$$\nand thus\n$$2 \\int_0^{\\pi/2} x \\cot (x) \\log ^2(\\sin (x)) \\ dx=-\\frac{2}{3}\\int_0^{\\pi/2} \\log ^3(\\sin (x)) \\ dx.$$\nAccording to the trigonometric form of the beta function, we know that\n$$\\int_0^{\\pi/2} \\sin^a(x)\\cos^b(x) \\ dx=\\frac{1}{2}B \\left(\\frac{1}{2}(a+1),\\frac{1}{2}(b+1)\\right).$$\nDifferentiating $3$ times with respect to $a$ and then letting $a\\to 0$ and $b\\to 0$, we obtain that\n\\begin{equation*}\n\\begin{aligned} \n\\int_0^{\\pi/2}  \\log ^3(\\sin (x)) \\ dx &=\\frac{1}{2} \\lim_{b \\to 0} \\lim_{a \\to 0} \\frac{\\partial^3}{\\partial a^3}\\left(B \\left(\\frac{1}{2}(a+1),\\frac{1}{2}(b+1)\\right)\\right) \\\\\n                                                           &=-\\frac{3 \\pi }{4}\\zeta (3)-\\frac{1}{2} \\pi  \\log ^3(2)-\\frac{1}{8} \\pi ^3 \\log (2).\n\\end{aligned}\n\\end{equation*}\nThus, \n$$2 \\int_0^{\\pi/2} x \\cot (x) \\log ^2(\\sin (x)) \\ dx=\\frac{1}{2}\\pi  \\zeta (3)+\\frac{1}{3} \\pi  \\log ^3(2)+\\frac{1}{12} \\pi ^3 \\log (2).$$\nHence, \n$$\\int_0^{\\pi/2}\\frac{x^2\\log^2{(\\sin(x))}}{\\sin^2(x)}dx=\\left(\\frac{\\pi ^3 }{6} +2 \\pi \\right)  \\log (2)+\\frac{1}{3} \\pi  \\log ^3(2)+\\frac{1}{8}\\pi  \\zeta (3)-\\frac{\\pi ^3}{6}-\\pi  \\log ^2(2).$$\nQ.E.D.", "meta": {"post_id": 1142705, "input_score": 28, "output_score": 38, "post_title": "Evaluate $\\int_0^{\\pi/2}\\frac{x^2\\log^2{(\\sin{x})}}{\\sin^2x}dx$"}}
{"input": "I'm trying to find a closed form for this integral:\n$$I=\\int_0^\\infty\\arctan\\left(\\frac{2\\pi}{x-\\ln\\,x+\\ln\\left(\\frac\\pi2\\right)}\\right)\\frac{dx}{x+1}$$\nIts approximate numeric value is\n$$I\\approx3.3805825284453469793953592216276992165696856825906055108192183...$$\nAny help is appreciated. Thanks!", "output": "Computing a Related Contour Integral:\nDefine\n$$f(z)=\\frac{i}{2}\\frac{z-1}{1+az}\\left(\\frac{1}{z-\\ln{z}+\\ln\\left(\\frac{\\pi}{2}\\right)}+\\frac{1}{z-\\ln{z}+2\\pi i+\\ln\\left(\\frac{\\pi}{2}\\right)}\\right)$$\nand let $\\gamma$ denote a keyhole contour deformed around $[0,\\infty]$. Restricting the argument between $0$ and $2\\pi$, it is not hard to see that $f(z)$ has poles at $z=-\\dfrac{1}{a}$, $z=-W_{-1}\\left(-\\dfrac{\\pi}{2}\\right)=\\dfrac{\\pi i}{2}$, and $z=-W_0\\left(-\\dfrac{\\pi}{2}\\right)=-\\dfrac{\\pi i}{2}$. The residues at these poles are\n\\begin{align}\n\\operatorname*{Res}_{z=\\frac{\\pi i}{2}}f(z)\n&=\\frac{i}{2}\\frac{\\frac{\\pi i}{2}-1}{\\frac{\\pi i}{2}a+1}\\frac{1}{1-\\frac{2}{\\pi i}}\\\\\n\\operatorname*{Res}_{z=-\\frac{\\pi i}{2}}f(z)\n&=\\frac{i}{2}\\frac{\\frac{\\pi i}{2}+1}{\\frac{\\pi i}{2}a-1}\\frac{1}{1+\\frac{2}{\\pi i}}\\\\\n\\operatorname*{Res}_{z=-\\frac{1}{a}}f(z)\n&=-\\frac{i}{2}u'(a)\\left(\\frac{1}{u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)-\\pi i}+\\frac{1}{u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)+\\pi i}\\right)\\\\\n\\end{align}\nwhere $u(a)=\\ln{a}-\\dfrac{1}{a}$. By the residue theorem,\n\\begin{align}\n\\oint_{\\gamma}f(z)\\ dz\n&=2\\pi i\\sum_{z_k\\in\\left\\{-a^{-1}, \\pm\\pi i/2\\right\\}}\\operatorname*{Res}_{z=z_k}f(z)\\\\\n&=\\pi\\left(\\frac{2\\left(u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)}{\\left(u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+\\pi^2}u'(a)-\\frac{2\\pi^2a}{\\pi^2a^2+4}\\right)\n\\end{align}\n\nParameterisation of the Contour Integral:\nWe take the argument of $z$ to be $0$ above the branch cut, and $2\\pi$ below the branch cut. Also, the contribution from the big arc is clearly $2\\pi i\\times\\dfrac{i}{2}\\times\\dfrac{1}{a}\\times (1+1)=-\\dfrac{2\\pi}{a}$. Taking all of these points into consideration, we eventually arrive at\n\\begin{align}\n\\oint_\\gamma f(z)\\ dz+\\frac{2\\pi}{a}\n&=\\small\\frac{i}{2}\\int^\\infty_0\\frac{x-1}{1+ax}\\left(-\\frac{1}{x-\\ln|x|-2\\pi i+\\ln\\left(\\frac{\\pi}{2}\\right)}+\\frac{1}{x-\\ln|x|+2\\pi i+\\ln\\left(\\frac{\\pi}{2}\\right)+\\pi^2}\\right)\\ dx\\\\\n&=2\\pi\\int^\\infty_0\\frac{x-1}{\\left(x-\\ln{x}+\\ln\\left(\\frac{\\pi}\n{2}\\right)\\right)^2+4\\pi^2}\\frac{dx}{1+ax}\\\\\n\\end{align}\n\nObtaining the Closed Form:\nIntegrating with respect to $a$, we obtain\n\\begin{align}\n\\small\\int^\\infty_0\\frac{2\\pi\\left(1-\\frac{1}{x}\\right)\\ln(1+ax)}{\\left(x-\\ln{x}+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+4\\pi^2}\\ dx\n&\\small=\\ \\pi\\int\\left(\\frac{2\\left(u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)}{\\left(u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+\\pi^2}u'(a)-\\frac{2\\pi^2a}{\\pi^2a^2+4}+\\frac{2}{a}\\right)\\ da\\\\\n&=\\small\\pi\\left(\\ln\\left(\\left(u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+\\pi^2\\right)-\\ln\\left(\\pi^2a^2+4\\right)+\\ln{a^2}\\right)+\\text{const.}\\\\\n&=\\small\\pi\\ln\\left(\\frac{\\left(\\ln{a}-\\frac{1}{a}+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+\\pi^2}{\\pi^2+\\frac{4}{a^2}}\\right)+\\text{const.}\n\\end{align}\nLetting $a\\to 0$, we find that the constant term is $\\pi\\ln{4}$. Plugging in $a=1$ and integrating by parts, we finally arrive at the closed form.\n\\begin{align}\n\\int^\\infty_0\\frac{2\\pi\\left(1-\\frac{1}{x}\\right)\\ln(1+x)}{\\left(x-\\ln{x}+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+4\\pi^2}\\ dx\n&=\\int^\\infty_0\\arctan\\left(\\frac{2\\pi}{x-\\ln{x}+\\ln\\left(\\frac{\\pi}{2}\\right)}\\right)\\frac{dx}{1+x}\\\\\n&=\\left.\\pi\\ln\\left(\\frac{\\left(\\ln{a}-\\frac{1}{a}+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+\\pi^2}{\\frac{\\pi^2}{4}+\\frac{1}{a^2}}\\right)\\right|_{a=1}\\\\\n&=\\color{red}{\\pi\\ln\\left(\\frac{\\ln^2\\left(\\frac{\\pi}{2}\\right)-2\\ln\\left(\\frac{\\pi}{2}\\right)+1+\\pi^2}{\\frac{\\pi^2}{4}+1}\\right)}\n\\end{align}", "meta": {"post_id": 1150822, "input_score": 39, "output_score": 54, "post_title": "Closed form for $\\int_0^\\infty\\arctan\\Bigl(\\frac{2\\pi}{x-\\ln\\,x+\\ln(\\frac\\pi2)}\\Bigr)\\frac{dx}{x+1}$"}}
{"input": "Evaluate the integral:\n$$\\displaystyle \\int_{0}^{\\frac{\\pi}{4}}\\tan^{-1}\\left(\\frac{\\sqrt{2}\\cos3 \\phi}{\\left(2\\cos 2 \\phi+ 3\\right)\\sqrt{\\cos 2 \\phi}}\\right)d\\phi$$ \nI have no clue on how to attack it.\nThe only thing I noticed is that there exists a symmetry around $\\pi/8$, meaning that from $\\pi/8$ to $\\pi/4$ is the negative of zero to $\\pi/4$. But, there exists a root of the integrand at $\\pi/6$ and the limit of the integrand at $\\pi/4$ is $-\\infty$.\nConjecture: The integral is $0$ for the reason of symmetry I mentioned above.\nHowever I cannot prove that. I would appreciate your help.", "output": "By replacing $\\phi$ with $\\arctan(t)$, then using integration by parts, we have:\n$$ I = \\int_{0}^{1}\\frac{1}{1+t^2}\\,\\arctan\\left(\\frac{\\sqrt{2}(1-3t^2)}{(5+t^2)\\sqrt{1-t^2}}\\right)\\,dt =\\frac{\\pi^2}{8}-\\int_{0}^{1}\\frac{3\\sqrt{2}\\, t \\arctan(t)}{(3-t^2)\\sqrt{1-t^2}}\\,dt.$$\nNow comes the magic. Since:\n$$\\int \\frac{3\\sqrt{2}\\,t}{(3-t^2)\\sqrt{1-t^2}}\\,dt  = -3\\arctan\\sqrt{\\frac{1-t^2}{2}}\\tag{1}$$\nintegrating by parts once again we get:\n$$ I = \\frac{\\pi^2}{8}-3\\int_{0}^{1}\\frac{1}{1+t^2}\\arctan\\sqrt{\\frac{1-t^2}{2}}\\,dt \\tag{2}$$\nhence we just need to prove that:\n$$ \\int_{0}^{1}\\frac{dt}{1+t^2}\\,\\arctan\\sqrt{\\frac{1-t^2}{2}}=\\int_{0}^{\\frac{1}{\\sqrt{2}}}\\frac{\\arctan\\sqrt{1-2t^2}}{1+t^2}\\,dt=\\color{red}{\\frac{\\pi^2}{24}}\\tag{3}$$\nand this is not difficult since both\n$$\\int_{0}^{1}\\frac{dt}{1+t^2}(1-t^2)^{\\frac{2m+1}{2}},\\qquad \\int_{0}^{\\frac{1}{\\sqrt{2}}}\\frac{(1-2t^2)^{\\frac{2m+1}{2}}}{1+t^2}\\,dt $$\ncan be computed through the residue theorem or other techniques. For instance:\n$$\\int_{0}^{1}\\frac{(1-t)^{\\frac{2m+1}{2}}}{t^{\\frac{1}{2}}(1+t)}\\,dt = \\sum_{n\\geq 0}(-1)^n \\int_{0}^{1}(1-t)^{\\frac{2m+1}{2}} t^{n-\\frac{1}{2}}\\,dt=\\sum_{n\\geq 0}(-1)^n\\frac{\\Gamma\\left(m+\\frac{3}{2}\\right)\\Gamma\\left(n+\\frac{1}{2}\\right)}{\\Gamma(m+n+2)}$$\nor just:\n$$\\int_{0}^{1}\\frac{\\sqrt{\\frac{1-t^2}{2}}}{(1+t^2)\\left(1+\\frac{1-t^2}{2}u^2\\right)}\\,dt = \\frac{\\pi}{2(1+u^2)}\\left(1-\\frac{1}{\\sqrt{2+u^2}}\\right)\\tag{4}$$\nfrom which:\n$$\\int_{0}^{1}\\frac{dt}{1+t^2}\\,\\arctan\\sqrt{\\frac{1-t^2}{2}}=\\frac{\\pi}{2}\\int_{0}^{1}\\frac{du}{1+u^2}\\left(1-\\frac{1}{\\sqrt{2+u^2}}\\right) =\\color{red}{\\frac{\\pi^2}{24}} $$\nas wanted, since:\n$$ \\int \\frac{du}{(1+u^2)\\sqrt{2+u^2}}=\\arctan\\frac{u}{\\sqrt{2+u^2}}.$$", "meta": {"post_id": 1151817, "input_score": 28, "output_score": 35, "post_title": "Integrate $ \\int_{0}^{\\frac{\\pi}{4}}\\tan^{-1}\\left(\\frac{\\sqrt{2}\\cos3 \\phi}{\\left(2\\cos 2 \\phi+ 3\\right)\\sqrt{\\cos 2 \\phi}}\\right)d\\phi$"}}
{"input": "I know that the following implications are true:\n$$\\text{Almost sure convergence} \\Rightarrow \\text{ Convergence in probability } \\Leftarrow \\text{ Convergence in }L^p $$\n$$\\Downarrow$$\n$$\\text{Convergence in distribution}$$\nI am looking for some (preferably easy) counterexamples for the converses of these implications.", "output": "Convergence in probability does not imply convergence almost surely: Consider the sequence of random variables $(X_n)_{n \\in \\mathbb{N}}$ on the probability space $((0,1],\\mathcal{B}((0,1]))$ (endowed with Lebesgue measure $\\lambda$) defined by $$\\begin{align*} X_1(\\omega) &:= 1_{\\big(\\frac{1}{2},1 \\big]}(\\omega) \\\\ X_2(\\omega) &:= 1_{\\big(0, \\frac{1}{2}\\big]}(\\omega) \\\\ X_3(\\omega) &:= 1_{\\big(\\frac{3}{4},1 \\big]}(\\omega) \\\\ X_4(\\omega) &:= 1_{\\big(\\frac{1}{2},\\frac{3}{4} \\big]}(\\omega)\\\\ &\\vdots \\end{align*}$$ Then $X_n$ does not convergence almost surely (since for any $\\omega \\in (0,1]$ and $N \\in \\mathbb{N}$ there exist $m,n \\geq N$ such that $X_n(\\omega)=1$ and $X_m(\\omega)=0$). On the other hand, since $$\\mathbb{P}(|X_n|>0) \\to 0  \\qquad \\text{as} \\, \\,  n \\to \\infty,$$ it follows easily that $X_n$ converges in probability to $0$.\nConvergence in distribution does not imply convergence in probability: Take any two random variables $X$ and $Y$ such that $X \\neq Y$ almost surely but $X=Y$ in distribution. Then the sequence $$X_n := X, \\qquad n \\in \\mathbb{N}$$ converges in distribution to $Y$. On the other hand, we have $$\\mathbb{P}(|X_n-Y|>\\epsilon) = \\mathbb{P}(|X-Y|>\\epsilon) >0$$ for $\\epsilon>0$ sufficiently small, i.e. $X_n$ does not converge in probability to $Y$.\nConvergence in probability does not imply convergence in $L^p$ I: Consider the probability space $((0,1],\\mathcal{B}((0,1]),\\lambda|_{(0,1]})$ and define $$X_n(\\omega) := \\frac{1}{\\omega} 1_{\\big(0, \\frac{1}{n}\\big]}(\\omega).$$ It is not difficult to see that $X_n \\to 0$ almost surely; hence in particular $X_n \\to 0$ in probability. As $X_n \\notin L^1$, convergence in $L^1$ does not hold. Note that $L^1$-convergence fails because the random variables are not integrable.\nConvergence in probability does not imply convergence in $L^p$ II: Consider the probability space $((0,1],\\mathcal{B}((0,1]),\\lambda|_{(0,1]})$ and define $$X_n(\\omega) := n 1_{\\big(0, \\frac{1}{n}\\big]}(\\omega).$$ Then $$\\mathbb{P}(|X_n|>\\epsilon) = \\frac{1}{n} \\to 0 \\qquad \\text{as} \\, \\, n \\to \\infty$$ for any $\\epsilon \\in (0,1)$. This shows that $X_n \\to 0$ in probability. Since $$\\mathbb{E}X_n = n \\cdot \\frac{1}{n} = 1$$ the sequence does not converge to $0$ in $L^1$. Note that $L^1$-convergence fails although the random variables are integrable. (Just as a side remark: This example shows that convergence in probability does also not imply convergence in $L^p_{\\text{loc}}$.)", "meta": {"post_id": 1170559, "input_score": 25, "output_score": 41, "post_title": "Convergence types in probability theory : Counterexamples"}}
{"input": "I'm doing a little project on the $\\zeta$ function, and I am at a complete loss of what it is actually doing.  I understand it is way over my head, but when I am plugging say $\\zeta(1 + i)$ into WolframAlpha, what is it even calculating? Wikipedia and .edu sites don't seem to have an answer, which is making me think there is no single answer.\nThanks, smart people of MathStack!\nEDIT:\nWhy is everyone voting to close this? I understand it's similar to a different question, but this question might provide someone with some different intuition.  Either way - this helped me a ton! Thanks @Mixed_Math.", "output": "The Riemann zeta function $\\zeta(s)$ is a sum of reciprocals of powers of natural numbers,\n$$\\zeta(s) = \\sum_{n \\geq 1} \\frac{1}{n^s}.$$\nAs written, this makes sense for complex numbers $s$ so long as $\\text{Re } s > 1$. For these numbers, there is little more to be said.\nBut you've asked about an interesting number: $\\zeta(1 + i)$, and $\\text{Re }(1 + i) \\not > 1$. What's happening there is a bit subtle, and a bit abusive in terms of notation.\nIt turns out there is another function (let's call it $Z(s)$) which makes sense for all complex numbers $s$ except for $s = 1$, and which exactly agrees with $\\zeta(s)$ when $\\text{Re } s > 1$. If you're familiar with some calculus or complex analysis, then you should also know that the function $Z(s)$ is also complex differentiable everywhere except for $s = 1$. This is a very special property that distinguishes $Z(s)$. The theory of complex analysis (in particular, the theory of \"analytic continuation\") gives that there can be at most one function that extends $\\zeta(s)$ to a larger region, like $Z(s)$ does.\nIn this sense, we could realize that $Z(s)$ is uniquely determined by $\\zeta(s)$. As it agrees with $\\zeta(s)$ everywhere $\\zeta(s)$ (initially) makes sense, it might even be reasonable to just use the name $\\zeta(s)$ instead of $Z(s)$. That is, when I write $\\zeta(s)$, what I'm really saying is\n$$\\zeta(s) = \\begin{cases}\n\\zeta(s) & \\text{if Re }s > 1 \\\\\nZ(s) & \\text{otherwise }\n\\end{cases}$$\nIt is this function that W|A computes when you ask it for $\\zeta(1 + i)$.\nAlthough what I've written is true (and important), it doesn't answer one aspect of your question\n\nWhat is it even calculating?\n\nI mentioned there exists this function $Z(s)$, or rather that it is possible to give meaningful values to $\\zeta(s)$ for all $s \\neq 1$. But how? Stated differently, yo're asking what is the analytic continuation of the Riemann zeta function?\nThe continuation is unique, but the steps to get there are not. I'll give a very short, incomplete proof that describes one way to calculate $\\zeta(1+i)$.\nWe start by considering $\\displaystyle h(s) =  \\sum_{n \\geq 1} \\frac{2}{(2n)^s}$. Performing some rearrangements,\n$$\\begin{align}\nh(s) &=  \\sum_{n \\geq 1} \\frac{2}{(2n)^s} \\\\\n&= \\frac{1}{2^{s - 1}} \\sum_{n \\geq 1} \\frac{1}{n^s} \\\\\n&= \\frac{1}{2^{s - 1}} \\zeta(s)\n\\end{align}$$\nLet's subtract this from the regular zeta function. On the one hand,\n$$ \\zeta(s) - h(s) = \\zeta(s)(1 - \\frac{1}{2^{s-1}}).$$\nOn the other hand,\n$$ \\begin{align}\\zeta(s) - h(s) &= \\sum_{n \\geq 1} \\left( \\frac{1}{n^s} - \\frac{2}{(2n)^s} \\right) \\\\\n&= \\sum_{n \\geq 1} \\frac{(-1)^{n+1}}{n^s},\n\\end{align}$$\nand this last series makes sense for $\\text{Re } s > 0$. (If you haven't looked at alternating series before, this might not be obvious. But the idea is that the sign changes cancel out a lot of the growth, so much that it converges for a larger region).\nIn total, this means that\n$$\\zeta(s) = (1 - 2^{s - 1})^{-1} \\sum_{n \\geq 1} \\frac{(-1)^{n+1}}{n^s},$$\nand you can just \"plug in\" $1+i$ here. [Notice that the problem when $s = 1$ is apparent here, as you cannot divide by $0$.] In practice, it's an infinite sum, so you'll take the first very many terms to get the value of $\\zeta(1+i)$ to any precision you want.\nFor completeness, it also turns out that\n$$\\pi^{-s/2} \\zeta(s) \\Gamma(\\tfrac{s}{2}) = \\pi^{(s-1)/2} \\zeta(1-s) \\Gamma(\\tfrac{1-s}{2}),$$\nwhich lets us transform values of  $\\zeta(s)$ for $\\text{Re } s > 0$ into values when $\\text{Re } s < 1$. The $\\Gamma(z)$ function here is called the \"Gamma function\" (it's an integral, a sort of generalization of a factorial) and this equation is called the symmetric functional equation of the zeta function.", "meta": {"post_id": 1180388, "input_score": 14, "output_score": 38, "post_title": "What exactly *is* the Riemann zeta function?"}}
{"input": "Let $f\\colon \\mathbb R^+\\to\\mathbb R$ be a function that satisfies the following conditions:\n$$\\tag1 \\lim_{x\\to 1}f(x)=0 $$\n$$\\tag2f(x_1)+f(x_2)=f(x_1x_2)$$\nShow that $f$ is continuous in its domain.\nI managed to show that $f$ is continuous at $x=1$, but I have no idea how to continue from there. Here's what I've done so far:\nBecause $\\lim_{x\\to 1}f(x)=0$, for every \u03f5>0 there exists a \u03b4>0 so that\n$$0<|x - 1|<\u03b4\u21d2|f(x)-0|<\u03f5$$ \nTo prove continuity at $x=1$ it's enough to show that $f(1)=0$ using the condition 2):\n$$f(1)+f(1)=f(1 \u00b71)$$\n$$f(1)=f(1)-f(1)$$\n$$f(1)=0$$\nSo now we have the definition of continuity at $x=1$:\n$$|x - 1|<\u03b4\u21d2|f(x)-f(1)|<\u03f5$$", "output": "You have shown continuity at $x=1$, i.e., $$\\lim_{x\\to1}f(x)=f(1).$$\nConsequently, for any $x_0\\ne0$\n$$\\lim_{x\\to x_0}f(x)=\\lim_{x\\to x_0}f\\left(\\frac x{x_0}\\cdot x_0\\right)=\\lim_{x\\to x_0}\\left(f\\left(\\frac x{x_0}\\right)+f(x_0)\\right)=f(1)+f(x_0)=f(x_0).$$", "meta": {"post_id": 1198295, "input_score": 15, "output_score": 34, "post_title": "Prove that function is continuous without knowing the function explicitly"}}
{"input": "First of all, I am doing some mathematical background information for a software I am creating. \nWhat I want to achieve is the point on an object rotating towards where the mouse is. Like in tank games, where the turret rotated depending on mouseX and mouseY \nIn terms of programming, this can be achieved by using the atan2 function, that returns an angle between two points (I believe).\nWhat I want to do is find the angle between an object, and mouse click.\n\nIs there a special maths formula for this? Because my research on google, brings 'atan2' since I'm a programmer. Since most languages have their own maths library, it is abstraction. I want to know how the formula works, in terms of maths. \nThe question is, what is the formula called for this; to find the angle between two points. So my object can rotate towards the mouseX and mouseY position", "output": "The atan2 function is an extended version of the trigonometric inverse tangent function. In the figure below, $\\tan{\\theta}=y/x$, so $\\tan^{-1}{y/x}=\\theta$. The atan2 function just calculates $\\tan^{-1}{y/x}$ with y and x as separate parameters. The reason it does so is to give a more accurate answer: since the tangent function is periodic, there are multiple values of x and y that would appear to have the same angle (but do not). Also, atan2 provides the correct values when y/x is undefined, such as at $\\pi/2$.\n\nAnother way to find the angle you're looking for is to use vectors. Using the x-axis as one of the vectors and $\\vec{OP}$ as another one, you could use the formula \n$$\\cos{\\theta}=\\frac{u\\cdot v}{||u||\\times||v||}$$\nNote that whichever way you use, you need two lines to measure an angle. You would have to choose a reference line to measure the angle $\\theta$ with; most commonly one would use the x-axis.", "meta": {"post_id": 1201337, "input_score": 20, "output_score": 44, "post_title": "Finding the angle between two points"}}
{"input": "Can we give an example of Lebesgue non-measurable function, for which set $\\{x: f(x)=C\\}~\\forall C\\in\\mathbb{R}$ is measurable? Thanks.", "output": "Let $S$ a non-measurable subset of $]0,+\\infty[$. Define $$g(x)=\\begin{cases} x\\text{ if } x\\in S\\\\-x\\text{ if } x\\notin S\\end{cases}$$\n$g^{-1}(y)$ is finite $\\forall y\\in \\mathbb{R}$, but $\\{ g\\geq 0\\}\\setminus\\ ]-\\infty,0]=S$ is not measurable.", "meta": {"post_id": 1237593, "input_score": 19, "output_score": 35, "post_title": "Lebesgue non-measurable function"}}
{"input": "Let $f: \\Bbb C \\to \\Bbb C$ be an entire (analytic on the whole plane) function such that exists $\\omega_1,\\omega_2 \\in \\mathbb{S}^1$, linearly independent over $\\Bbb R$ such that: $$f(z+\\omega_1)=f(z)=f(z+\\omega_2), \\quad \\forall\\,z\\in \\Bbb C.$$Prove that $f$ is constant.\n\nThe intuition seems clear to me, we have the three vertices of a triangle given by $0$, $\\omega_1$ and $\\omega_2$. All points in the plane are one of the vertices of that triangle under a suitable parallel translation. The constant value will be $f(0)$, fine. Throwing values for $z$ there, I have found that $$f(n\\omega_1) = f(\\omega_1) = f(0)=f(\\omega_2) = f(n\\omega_2), \\quad \\forall\\, n \\in \\Bbb Z.$$\nI don't know how to improve the above for, say, rationals (at least). Some another ideas would be:\n\nChecking that $f' \\equiv 0$. I don't have a clue of how to do that.\nWrite $w = a\\omega_1+b\\omega_2$, with $a,b \\in \\Bbb R$, do stuff and conclude that $f(w) = f(0)$. This approach doesn't seem good, because I only have a weak result with integers above.\nFinding that $f$ coincides with $f(0)$ on a set with an accumulation point. This seems also bad: the set on with $f$ coincides with $f(0)$ by which I found above is discrete. \n\nNothing works and this is getting annoying... And I don't see how analyticity comes in there. I'll be very thankful if someone can give me an idea.\n(On a side note.. I know that this title is not informative at all. Feel free to edit if you come up with something better.)", "output": "The values the function take on the plane are the values it takes in the compact parallelogram with vertices on $0,\\omega_1,\\omega_2,\\omega_1+\\omega_2$. Therefore the entire function is bounded, and hence constant by Liouville's theorem.\n\nEvery point $z$ of the plane can be written as $z=x\\omega_1+y\\omega_2$ with $x,y$ reals, since $\\omega_1,\\omega_2$ are independent over the reals. Then $$f(z)=f(\\{x\\}\\omega_1+\\{y\\}\\omega_2),$$ where $0\\leq\\{x\\}<1$ is such that $x-\\{x\\}$ is integer. The point $\\{x\\}\\omega_1+\\{y\\}\\omega_2$ is inside the compact parallelogram with vertices $0,\\omega_1,\\omega_2,\\omega_1+\\omega_2$.", "meta": {"post_id": 1244416, "input_score": 23, "output_score": 38, "post_title": "Proving that a doubly-periodic entire function $f$ is constant."}}
{"input": "Given that $f$ and $g$ are two real functions and both are differentiable, is it true to say that $h=\\max{(f,g)} $ is differentiable too?\nThanks", "output": "No. Consider $f(x)=x$ and $g(x)=-x$. You get $\\max(f(x),g(x))=|x|$.", "meta": {"post_id": 1246224, "input_score": 18, "output_score": 61, "post_title": "Is the max of two differentiable functions differentiable?"}}
{"input": "I read the following statement in a book on Calculus, as part of my mathematics course:\n\nTechnically this separation of $\\frac{dy}{dx}$ is not mathematically valid. However, the resulting integration leads to correct answer.\n\nThe book also contains the following:\n\nTo solve a differential equation by separation of variables:\n\nget all the $x$ values on one side and all the $y$ values on the other side by multiplication and division.\nseparate $\\frac{dy}{dx}$ as if it were a fraction.\nintegrate both sides.\n\n\nNote: This box doesn't refer to a particular problem. It refers to a class of problems of differential equations which can be solved using the Method of Separation of Variables.\nMy high school mathematics teacher told me that this is the most fundamental way to solve differential equations but the textbook says it is not mathematically valid. I am not able to understand why are certain methods being followed without having a mathematical proof. Or am I wrong?", "output": "The problem with this form of separation of variables (I say \"this form\" because \"separation of variables\" can refer to multiple things) is that treating the derivative $dy/dx$ as a ratio is a purely formal algebraic manipulation. There is a way to arrive at the same results in a rigorous fashion, but textbooks often don't address this.\nTo elaborate, separation of variables in ODEs most commonly refers to a method of solving the ODE\n$$\n\\frac{dy}{dx} = g(x)h(y)\n$$\nfor the unknown function $y(x)$. Introductory textbooks often tell you to split the \"fraction\" $dy/dx\"$ and unite common variables, like so:\n$$\n\\frac{dy}{h(y)} = g(x)dx\n$$\nand then integrate both sides, as long as $h(y)\\neq 0$, to obtain\n$$\nH(y(x)) = \\int g(x)~dx + C,\n$$\nwhere $H(y)$ is an antiderivative of $\\frac{1}{h(y)}$.\nUnfortunately \"$dy$\" and \"$dx$\" have no actual mathematical meaning in this context, so all we've done is pull a little algebraic trick without understanding why it works. To resolve this, we rearrange:\n$$\n\\frac{1}{h(y)}\\frac{dy}{dx} = g(x).\n$$\nIntegrating in $x$,\n$$\n\\int \\frac{1}{h(y(x))}\\frac{dy}{dx}(x)~dx = \\int g(x)~dx + C.\n$$\nNow if $H(y)$ is an antiderivative of $1/h(y)$, then by the chain rule\n$$\n\\frac{d}{dx}H(y(x)) = \\frac{1}{h(y(x))}\\frac{dy}{dx}(x)\n$$\nso the left-hand integral is\n$$\n\\int \\frac{1}{h(y(x))}\\frac{dy}{dx}(x)~dx = \\int \\frac{d}{dx}H(y(x))~dx = H(y(x))\n$$\nleading us to our desired conclusion,\n$$\nH(y(x)) = \\int g(x)~dx + C.\n$$\nDoing it this way gives a rigorous justification of the result, but frankly the abuse of notation with the symbolic approach is much easier for most to memorize, so it is often the way it is taught to students. However, I think not explaining why the abuse of notation works confuses many students, both about the method and about their already flimsy understanding of the derivative.", "meta": {"post_id": 1252405, "input_score": 73, "output_score": 135, "post_title": "Is it mathematically valid to separate variables in a differential equation?"}}
{"input": "Is every subgroup of a normal subgroup normal ?\nThat is if $H$ is a normal subgroup of a group $G$ and $K$ is a subgroup of $H$, then $K$ is a normal subgroup of $G$. Is it true ? If not what is the example?  \nProgress\n$a\\in G$ and $k\\in K$. Then $k\\in H$, since $K\\subseteq H$.\nNow, $aka^{-1}=k_1aa^{-1}=k_1\\in K$  [since $H$ is normal in $G$, $ak=k_1a$]  \nThis implies  that $K$ is normal in $H$.\nIs my approach correct ?", "output": "The silly counterexample is this: if $H$ is not normal in $G$, then we have\n$$H \\not\\lhd G\\quad G\\lhd G$$\nIndeed, this need not even be true if $K$ itself is normal in $H$. For example, in $S_4$, we have\n$$C_2 \\lhd V_4\\lhd S_4$$\nbut $C_2\\not\\lhd S_4$.\n(Here, $V_4 = \\{(1), (12)(34),(13)(24),(14)(23)\\}$ and $C_2 =  \\{(1), (12)(34)\\}$) \nThe flaw in your argument is taking $ak = k_1 a$ where $k_1\\in K$. The fact that $a\\in G$ and $H \\lhd G$ only allows you to assume that $k_1 \\in H$.", "meta": {"post_id": 1275420, "input_score": 22, "output_score": 37, "post_title": "Is every subgroup of a normal subgroup normal?"}}
{"input": "Let be $1\\leq p\\in\\mathbb{R}$, denote:\n  $$\\ell^p(\\mathbb {R})=\\left\\{(x_n)\\subset \\mathbb{R}: (x_n) \\mbox{ is a sequence with } \\displaystyle\\sum_{n=1}^{\\infty}|x_n|^p<\\infty \\right\\}$$ \n  Prove that: \n\n\nThe function: $d_p:\\ell^p(\\mathbb{R})\\times\\ell^p(\\mathbb{R})\\to \\mathbb{R}$ is a metric for $\\ell^p(\\mathbb{R})$  where $d_p(x_n,y_n)= \\left| \\displaystyle\\sum_{n=1}^\\infty |x_n-y_n|^p \\right|^\\frac{1}{p}$ (Only triangular inequality, I work in $\\mathbb{R}$, should I assume Minkowski inequality and its done?)\n$\\ell^p(\\mathbb{R})$ is a complete metric space.\n\nit is right? I mean $p\\in\\mathbb{R}$? I've never work with $\\ell^p$ spaces, this is a question from introduction to topology.", "output": "Let $\\left( x^{(n)}\\right)_{n=1}^{\\infty} \\subset \\ell^p$ be a Cauchy sequence. Since I see you have troubles with your notations of sequence of sequences, this is the notation that I will use for each element $x^{(n)}$ in the sequence:\n$$\nx^{(n)} = \\left( x_j^{(n)}\\right)_{j=1}^{\\infty} = \\left( x_1^{(n)},x_2^{(n)}, \\cdots \\right)\\in \\ell^p\n$$ \nFor $x= \\left( x_j\\right)_{j=1}^{\\infty} , y= \\left( y_j\\right)_{j=1}^{\\infty} \\in \\ell^p$, lets define the $p$-norm $\\|\u00a0\\cdot \\|_p$ as the one who induces $d_p$, that is $\\|x-y\\|_p=d_p(x,y)$. Precisely \n$$\n\\|x-y\\|_p= \\left(\\sum_{j=1}^{\\infty} \\left|x_j-y_j\\right|^p\\right)^{1/p}\n$$\nNow lets continue, take $\\varepsilon>0$, then there exist a $N=N(\\varepsilon) \\in \\mathbb{N}$, such that if $m,n >N$ then\n$$\n\\|x^{(m)}-x^{(n)}\\|_p<\\varepsilon.\n$$\nThus, for any $j \\in \\mathbb{N}$, it follows that\n$$\n\\left|x^{(m)}_j-x^{(n)}_j\\right|^p \\leq \\sum_{j=1}^{\\infty} \\left|x^{(m)}_j-x^{(n)}_j\\right|^p = \\|x^{(m)}-x^{(n)}\\|^p_p<\\varepsilon^p\n$$\nthat is, for any $j \\in \\mathbb{N}$ the sequence $\\left( x^{(n)}_j\\right)_{n=1}^{\\infty} \\subset \\mathbb{R}$ is a Cauchy one. Since $\\mathbb{R}$ is complete, for each $j$ there exist a $x_j \\in \\mathbb{R}$ such that\n $$\n \\lim_{n \\to \\infty} x^{(n)}_j = x_j\n$$\nLets fix $k \\in \\mathbb{N}$, then in a similar way for $m,n >N$\n \\begin{equation}\n \\sum_{j=1}^{k} \\left|x^{(m)}_j-x^{(n)}_j\\right|^p \\leq \\sum_{j=1}^{\\infty} \\left|x^{(m)}_j-x^{(n)}_j\\right|^p = \\|x^{(m)}-x^{(n)}\\|^p_p<\\varepsilon^p \\tag{1}\n\\end{equation}\nLetting $n \\to \\infty$ in (1), we get that for $m>N$ \n\\begin{equation}\n \\sum_{j=1}^{k}\\left|x^{(m)}_j-x_j\\right|^p < \\varepsilon^p \\tag{2}\n\\end{equation}\nThen by the usual triangle inecuality ( Minkowski's inequality for $\\|\\cdot\\|_p$ in $\\mathbb{R}^k$) we get that if $m>N$\n$$\n\\left( \\sum_{j=1}^{k}|x_j|^p \\right)^{1/p} \\leq \\left(  \\sum_{j=1}^{k}\\left|x^{(m)}_j-x_j\\right|^p \\right)^{1/p} + \\left( \\sum_{j=1}^{k} \\left|x^{(m)}_j \\right| \\right)^{1/p} < \\varepsilon + \\left( \\sum_{j=1}^{k} \\left|x^{(m)}_j \\right| \\right)^{1/p}\n$$\nby letting $k \\to \\infty$, we get $\\|x\\|_p\\leq \\varepsilon + \\|x^{(m)}\\|_p$, which is the same as getting that $x=\\left( x_j\\right)_{j=1}^{\\infty} \\in \\ell^p$. Again, letting $k \\to \\infty$ in (2), we obtain that if $m>N$\n$$\n\\|x^{(m)}-x\\|_p^p= \\sum_{j=1}^{\\infty}\\left|x^{(m)}_j-x_j\\right|^p < \\varepsilon^p\n$$\nthus \n$$\n\\lim_{m \\to \\infty} \\|x^{(m)}-x\\|_p= 0 \n$$\nso indeed, $\\left( x^{(m)}\\right)_{m=1}^{\\infty} \\subset \\ell^p$, is a convergent sequence who converges to $x \\in \\ell^p$. We conclude then that $\\ell^p$ is a complete metric space for $1\\leq p < \\infty$.", "meta": {"post_id": 1276470, "input_score": 15, "output_score": 34, "post_title": "Proving $\\ell^p$ is complete"}}
{"input": "Could someone possible explain the differences between each of these; \nSingularities, essential singularities, poles, simple poles.\nI understand the concept and how to use them in order to work out the residue at each point, however, done fully understand what the difference is for each of these\nAs far as i understand a simple pole is a singularity of order $1$?\nthen we have poles of order $n$ which aren't simple?\nnot too sure about essential singularity", "output": "The point $z_{0}$ is an isolated singularity of $f(z)$ if $f(z)$ is analytic in \n$0 \\lt |z-z_{0}| \\lt r$ (a circle of radius r centered at $z_{0}$ with the point $z_{0}$ punched out). If one expands a function $f(z)$ in a Laurent series about the point $z_{0}$, \n$$f(z) = \\sum\\limits_{k=-\\infty}^{\\infty} a^{k} (z-z_{0})^{k}$$\nwe can classify isolated singularties into 3 cases:\n\nIf there are no negative powers of $z-z_{0}$, then $z_{0}$ is a removable singularity and the Laurent series\nis a power series.\n\nExample: $$\\frac{\\sin(z)}{z} = 1 - \\frac{z^{2}}{3!} + \\frac{z^{4}}{5!} - ...$$ has a removable singularity\nat 0.\n\n$f(z)$ has a pole of order m at $z_{0}$ if m is the largest positive integer such that \n$a_{-m} \\ne 0$. A pole of order one is a simple pole. A pole of order two is a double pole, etc.\n\nExample: $$f(z) = \\frac{1}{(z-3i)^{7}}$$ has a pole of order 7 at $z=3i$\n\nIf there are an infinite number of negative powers of $z-z_{0}$, then $z_{0}$ is an essential singularity.\n\nExample: $$\\mathrm{e}^{1/z} = 1 + \\frac{1}{z} + \\frac{1}{2!z^{2}} + ...$$ has an essential singularity\nat 0.", "meta": {"post_id": 1284316, "input_score": 36, "output_score": 49, "post_title": "Singularities, essential singularities, poles, simple poles"}}
{"input": "We can define trace if $A =\\sum_{i} \\langle e_i, Ae_i\\rangle$ where $e_i$'s are standard column vectors, and $\\langle x, y\\rangle =x^t y$ for suitable column vectors $x, y$. With this set up, I want to prove trace of AB and BA are same, so it's enough to prove that $$\\sum_{i} \\langle e_i, ABe_i\\rangle =\\sum_{i} \\langle e_i, BAe_i\\rangle$$ but how to conclude that?", "output": "by definition $$\\begin{align}trace(AB) &= (AB)_{11}+(AB)_{22}+\\cdots+(AB)_{nn}\\\\\n&=a_{11}b_{11}+a_{12}b_{21}+\\cdots + a_{1k}b_{k1} \\\\\n&+ a_{21}b_{12}+a_{22}b_{22}+\\cdots + a_{2k}b_{k2}\\\\\n&+\\vdots  \\\\\n&+a_{n1}b_{1n}+a_{n2}b_{2n}+\\cdots + a_{nk}b_{kn}\\end{align}$$ if you view the sum according to the columns, then you see that it is the $trace(BA).$  therefore, $$trace(AB) = trace(BA). $$", "meta": {"post_id": 1314142, "input_score": 10, "output_score": 35, "post_title": "Trace of AB = Trace of BA"}}
{"input": "I have a question, to aid my understanding, about the proof that differentiability implies continutity.$\\mathstrut$ \nDifferentiability Definition\nWhen we say a function is differentiable at $x_0$, we mean that the limit:\n$$\u200ef^{\\prime} \u200e(x) = \\lim_{x\\to x_0} \\frac{f(x) - f(x_0)}{x-x_0}$$ exists.\nContinuity Definition\nWhen we say a function is continuous at $x_0$, we mean that:\n$$\\lim_{x\\to x_0} f(x) - f(x_0) = 0$$\nTheorem: Differentiability implies Continuity: If $f$ is a differentiable function at $x_0$, then it is continuous at $x_0$.\nProof:\nLet us suppose that $f$ is differentiable at $x_0$. Then\n$$ \\lim_{x\\to x_0} \\frac{f(x) - f(x_0)}{x-x_0} =  \u200ef^{\\prime} \u200e(x) $$\nand hence\n$$ \\lim_{x\\to x_0} f(x) - f(x_0) = \\lim_{x\\to x_0} \\left[ \\frac{f(x) - f(x_0)}{x-x_0} \\right] \\cdot \\lim_{x\\to x_0} (x-x_0) = 0$$\nWe have therefore shown that, using the definition of continuous, if the function is differentiable at $x_0$, it must also be continuous.\nMy Question\nThe proof seems to execute the following steps:\n\nAssume the function is continuous at $x_0$\nShow that, with little algebra, we can change this into an equivalent question about differentiability at $x_0$. With this little bit of algebra, we can show that if a function is differentiable at $x_0$ it is also continuous.\n\nWhat I am slightly unsure about is the apparent circularity. In my mind it seems to say, if a function is continuous, we can show that if it is also differentiable, then it is continuous. Rather than what I was expecting, namely, if a function is differentiable, we can show it must be continuous.\nHopefully my confusion is clear. Any help will be greatly appreciated.", "output": "Technically, there is an implicit issue of existence of limits which is being swept under the rug in the presentation you have given. The assumption of differentiability at $x_0$ says that the limit\n$$\\lim_{x \\to x_0} \\frac{f(x) - f(x_0)}{x-x_0}$$\nexists as a finite number. The limit $\\lim_{x \\to x_0} x-x_0$ exists and is zero regardless of our assumptions. Then the product rule for limits tells us both that $\\lim_{x \\to x_0} f(x)-f(x_0)$ exists, and that it is the product of the two limits above, which means it must be zero. Because the product rule also tells us that the limit exists, we do not have to assume continuity first.", "meta": {"post_id": 1314630, "input_score": 28, "output_score": 35, "post_title": "Differentiability implies continuity - A question about the proof"}}
{"input": "I am not sure how to solve this, but this is my best guess.  Since we want to know how many 12 bit strings have more 1's than 0's, start with 5 since it is one less than 12/2=6.\nThen we proceed with:\n$${12 \\choose 5} + {12 \\choose 4} + {12 \\choose 3} + {12 \\choose 2}+ {12 \\choose 1}+ {12 \\choose 0}=1586$$\nIs my reasoning correct?  I am not sure if we should use the choose function here, but I believe we do.", "output": "Your expression is correct. Here is another way that exploits the symmetry. \nLet $a$ be the number of strings with more $1$'s than $0$'s, let $b$ be the number with more $0$'s than $1$'s, and let $c$ be the number with equal numbers of $0$'s and $1$'s.\nThen $a=b$, $a+b+c=2^{12}$, and $c=\\binom{12}{6}$. So $2a=2^{12}-\\binom{12}{6}$ and therefore \n$$a=2^{11}-\\frac{1}{2}\\binom{12}{6}.$$", "meta": {"post_id": 1315712, "input_score": 13, "output_score": 34, "post_title": "How many 12-bit strings with more 1\u2019s than 0\u2019s?"}}
{"input": "$X$ is a compact metric space, then $C(X)$ is separable,\nwhere $C(X)$ denotes the space of continuous functions on $X$.\nHow to prove it?\nAnd if $X$ is just a compact Hausdorff space, then is $C(X)$ still separable?\nOr if $X$ is just a compact (not necessarily Hausdorff) space, then\nis $C(X)$ still separable?\nPlease help me. Thanks in advance.", "output": "Theorem.  If $X$ is compact Hausdorff then $C(X)$ is separable iff $X$ is metrizable.\n\nThere is a natural embedding $x\\in X\\to \\delta _x\\in \\mathcal{M}(X)$ (more precisely in the unit ball of $\\mathcal{M}(X)$). This is an homeomorphism for the weak*-topology of $\\mathcal{M}(X)$. If $C(X)$ is separable then $(\\mathcal{M}(X), w*)$ have a compact metrizable unit ball. So $X$ is metrizable.\nFor the converse, assume $X$ is a metrizable compact Hausdorff space. Let $d$ be a metric inducing the topology and $(x_n)$ a dense countable subset of $X$. Define $d_n: x\\in X \\to d_n(x):=d(x,x_n)$. It is a continuous function. It is easy to check that the algebra generated by $1$ and $(d_n)_n$ separate the points in $X$ so by Stone Weierstrass theorem this subalgebra is dense in $C(X)$. By considering linear combination with rational coefficient of element of this subalgebra it is easy to see that $C(X)$ is separable.", "meta": {"post_id": 1331321, "input_score": 27, "output_score": 34, "post_title": "$C(X)$ is separable when $X$ is compact?"}}
{"input": "The title is fairly self explanatory: I have been trying to rigorously prove that $y(x)=x^{x^{x^{\\ldots}}}$ is a strictly increasing function over the interval  $[1,e^{\\frac{1}{e}})$ for a while now, primarily by exploring various manipulations using logarithms and polylogarithms but have gotten nowhere. Although it is simple enough to show that $y(\\sqrt{2})>y(1)$ and if $y'(x)>0$ for some $x \\in [1,e^{\\frac{1}{e}})$ then $y'(x)>0$ for all $x \\in [1,e^{\\frac{1}{e}})$ (since either $y$ must be strictly increasing or strictly decreasing), I am not satisfied by the rigor of this argument, although perhaps this is me being too finicky. This lack of progress has led me to explore the possibility that it is only strictly non-decreasing but this loosening of constraints has not helped at all. When it comes to proving that it is a function I've been at a loss as to where I might even begin. Any and all insights are welcome.", "output": "It is easier to prove that the inverse function is strictly increasing. Since the inverse function is just:\n$$ g(x) = \\left(\\frac{1}{x}\\right)^{-\\frac{1}{x}}$$\nwith a change of variable everything boils down to proving that $h(x)=x^x$ is increasing over $\\left[\\frac{1}{e},1\\right]$. That is trivial since:\n$$ h'(x) = h(x)\\cdot\\frac{d}{dx}\\log h(x) = (1+\\log x)\\,h(x) \\geq 0.$$", "meta": {"post_id": 1335096, "input_score": 19, "output_score": 41, "post_title": "Proof of strictly increasing nature of $y(x)=x^{x^{x^{\\ldots}}}$ on $[1,e^{\\frac{1}{e}})$?"}}
{"input": "I'm having a hard time understanding the intuitive relationship between these three distributions. I thought that poisson is what you get when you sum n number of exponentially distributed variables, but if seems that gamma is the same...Could someone describe the relationship in layman's terms?", "output": "Poisson and exponential distributions are very strongly related but they're fundamentally different because the Poisson is discrete (a count variable) and the exponential is continuous (a waiting time).\nSo how are they related?\nIf the time between a certain type of event is exponentially distributed with rate $\\lambda$, then the number of events in a given time period of length $t$ follows a Poisson distribution with parameter $\\lambda t$.\nFor example, if shooting stars appear in the sky at a rate of $\\lambda$ per unit time, then the time you wait until you see your first shooting star is distributed exponentially with rate $\\lambda$. If you watch the night sky for $t$ units of time, then you could see $0, 1, 2, ...$ shooting stars. The number of shooting stars that you count in this time is a $\\text{Poisson}(\\lambda t)$ random variable.\nBut what if you ask, how long must I wait before I see $n$ shooting stars?\nThe answer is a sum of independent exponentially distributed random variables, and it follows a $\\text{gamma}(\\lambda, n)$ distribution (also sometimes called an Erlang distribution, to distinguish it from the general gamma distribution where $n$ is allowed to be a non-integer).", "meta": {"post_id": 1340158, "input_score": 25, "output_score": 48, "post_title": "What is the relationship between poisson, gamma, and exponential distribution?"}}
{"input": "I am interested in describing the group of special orthogonal matrices $\\operatorname{SO}(n)$ by a set of parameters, in any dimension. I would also like to obtain an expression of the density of the Haar measure in this set of parameters.\nCould anyone help me on this or indicate a good reference?\nThanks", "output": "The only explicit description of the Haar measure on $SO(n)$ that I'm aware of is inductive and based on hyperspherical coordinates on the unit $(n-1)$-sphere $S^{n-1}$.  The idea is to first perform an arbitrary rotation of the first $n-1$ coordinates, and then perform a rotation that maps $\\textbf{e}_n$ to any possible location on $S^{n-1}$.\nI will describe this parameterization using explicit inductive formulas. For convenience, we will use the following notation.  If $\\textbf{v}\\in\\mathbb{R}^n$ is a vector, let $\\textbf{v}^a\\in\\mathbb{R}^{n+1}$ be the vector obtained by augmenting $\\textbf{v}$ with a zero, i.e.\n$$\n(v_1,\\ldots,v_n)^a \\;=\\; (v_1,\\ldots,v_n,0).\n$$\nSimilarly, if $M$ is an $n\\times n$ matrix, let $M^a$ be the $(n+1)\\times(n+1)$ matrix with the following block diagonal form:\n$$\nM^a \\;=\\; \\begin{bmatrix}M & \\textbf{0} \\\\ \\textbf{0}^T & 1\\end{bmatrix}.\n$$\nWe will also use the notation $\\textbf{e}_1,\\ldots,\\textbf{e}_n$ for the  standard basis vectors in $\\mathbb{R}^n$.\n\nHyperspherical Coordinates\nThese are a coordinate system for specifying a point $\\boldsymbol{\\Sigma}_n(\\theta_1,\\ldots,\\theta_n)$ on the unit $n$-sphere $S^n$ in $\\mathbb{R}^{n+1}$ given $n$ angles $\\theta_1,\\ldots,\\theta_n$.  The first few hyperspherical coordinate systems are given by\n\\begin{align*}\n\\boldsymbol{\\Sigma}_1(\\theta_1) &\\;=\\; (\\sin\\theta_1,\\,\\cos\\theta_1), \\\\[3pt]\n\\boldsymbol{\\Sigma}_2(\\theta_1,\\theta_2) &\\;=\\; (\\sin\\theta_1\\sin\\theta_2,\\,\\cos\\theta_1\\sin\\theta_2,\\,\\cos\\theta_2), \\\\[3pt]\n\\text{and }\\boldsymbol{\\Sigma}_3(\\theta_1,\\theta_2,\\theta_3) &\\;=\\; (\\sin\\theta_1\\sin\\theta_2\\sin\\theta_3,\\,\\cos\\theta_1\\sin\\theta_2\\sin\\theta_3,\\,\\cos\\theta_2\\sin\\theta_3,\\,\\cos\\theta_3).\n\\end{align*}\nand in general the $i$th Cartesian coordinate $\\Sigma_{n,i}$ of $\\boldsymbol{\\Sigma}_n$ is given by the formula\n$$\n\\Sigma_{n,i}(\\theta_1,\\ldots,\\theta_n) \\;=\\; \\begin{cases}\\sin \\theta_1 \\cdots \\sin \\theta_n & \\text{if } i=1, \\\\[3pt] \\cos \\theta_{i-1} \\sin \\theta_i \\cdots \\sin \\theta_n & \\text{if }2\\leq i \\leq n+1.\\end{cases}\n$$\nThe function $\\boldsymbol{\\Sigma}_n$ can also be defined inductively by the formula\n$$\n\\boldsymbol{\\Sigma}_n(\\theta_1,\\ldots,\\theta_n) \\;=\\; (\\sin \\theta_n)\\,\\bigl(\\boldsymbol{\\Sigma}_{n-1}(\\theta_1,\\ldots,\\theta_{n-1})\\bigr)^a \\,+\\, (\\cos \\theta_n)\\,\\textbf{e}_{n+1}.\n$$\nwith base case $\\boldsymbol{\\Sigma}_1$.\nDomain and Volume Form\nIf we let $D_n$ be the subset of the parameter space defined by\n$$\n0\\leq\\theta_1\\leq 2\\pi\n\\qquad\\text{and}\\qquad\n0\\leq \\theta_i\\leq \\pi\\;\\; \\text{ for }2\\leq i \\leq n,\n$$\nthen $\\boldsymbol{\\Sigma}_n$ maps $D_n$ onto $S^n$, and is one-to-one on the interior of $D_n$.  The $n$-dimensional volume form with respect to $\\boldsymbol{\\Sigma}_n$ is\n$$\ndV \\;=\\; \\bigl(\\sin \\theta_2\\bigr)\\bigl(\\sin^2\\theta_3\\bigr) \\cdots \\bigl(\\sin^{n-1} \\theta_n\\bigr)\\,d\\theta_1 \\cdots d\\theta_n,\n$$\nwhich comes from the fact that the partial derivatives of $\\boldsymbol{\\Sigma}_n$ are orthogonal with\n$$\n\\left\\|\\frac{\\partial \\boldsymbol{\\Sigma}_n(\\theta_1,\\ldots,\\theta_n)}{\\partial \\theta_i}\\right\\| \\;=\\; (\\sin\\theta_{i+1})\\cdots(\\sin \\theta_n)\n$$\nfor all $1\\leq i\\leq n$.\n\nAn Orthonormal Basis\nBefore writing down the parameterization of $SO(n)$, we need to extend $\\{\\boldsymbol{\\Sigma}_n(\\theta_1,\\ldots,\\theta_n)\\}$ to an orthonormal basis of $\\mathbb{R}^{n+1}$. The basis is\n$$\n\\bigl\\{\\textbf{U}_{n,1}(\\theta_1,\\ldots,\\theta_n),\\ldots,\\textbf{U}_{n,n}(\\theta_1,\\ldots,\\theta_n),\\boldsymbol{\\Sigma}_n(\\theta_1,\\ldots,\\theta_n)\\bigr\\}\n$$\nwhere\n$$\n\\textbf{U}_{n,i}(\\theta_1,\\ldots,\\theta_n) \\;=\\; \\frac{1}{(\\sin \\theta_{i+1}) \\cdots (\\sin \\theta_n)}\\frac{\\partial \\boldsymbol{\\Sigma}(\\theta_1,\\ldots,\\theta_n)}{\\partial \\theta_i}.\n$$\nThat is, $\\textbf{U}_{n,i}$ is the unit vector tangent to $S^n$ in the direction of increasing $\\theta_i$. For example, in the case of $n=2$ we have $\\boldsymbol{\\Sigma}_2(\\theta_1,\\theta_2) = (\\sin \\theta_1\\sin\\theta_2,\\cos\\theta_1 \\sin\\theta_2,\\cos\\theta_2)$, so\n$$\n\\textbf{U}_{2,1}(\\theta_1,\\theta_2) = (\\cos \\theta_1,-\\sin\\theta_1,0),\n\\qquad\n\\textbf{U}_{2,2}(\\theta_1,\\theta_2) = (\\sin \\theta_1\\cos\\theta_2,\\cos\\theta_1 \\cos\\theta_2,-\\sin\\theta_2).\n$$\nSaying these vectors are orthonormal is the same thing as saying that hyperspherical coordinates are an orthogonal coordinate system.\nThe vectors $\\textbf{U}_{n,i}(\\theta_1,\\ldots,\\theta_n)$ can also be defined inductively by the formula\n$$\n\\textbf{U}_{n,n}(\\theta_1,\\ldots,\\theta_n) \\;=\\; (\\cos \\theta_n)\\,\\bigl(\\boldsymbol{\\Sigma}_{n-1}(\\theta_1,\\ldots,\\theta_{n-1})\\bigr)^a \\,-\\, (\\sin \\theta_n)\\,\\textbf{e}_{n+1}\n$$\nand $\\textbf{U}_{n,i}(\\theta_1,\\ldots,\\theta_n) = \\bigl(\\textbf{U}_{n-1,i}(\\theta_1,\\ldots,\\theta_{n-1})\\bigr)^a$ for $i<n$.\nLet $M_{n+1}(\\theta_1,\\ldots,\\theta_n)$ denote the $(n+1)\\times (n+1)$ matrix whose columns are the vectors of this orthonormal basis:\n$$\nM_{n+1}(\\theta_1,\\ldots,\\theta_n) \\;=\\; \\begin{bmatrix}\\textbf{U}_{n,1}(\\theta_1,\\ldots,\\theta_n) & \\cdots & \\textbf{U}_{n,n}(\\theta_1,\\ldots,\\theta_n) & \\boldsymbol{\\Sigma}_n(\\theta_1,\\ldots,\\theta_n)\\end{bmatrix}.\n$$\nSo $M_n(\\theta_1,\\ldots,\\theta_{n-1})$ is an $n\\times n$ matrix in $SO(n)$ that maps $\\textbf{e}_n$ to an arbitrary point $\\boldsymbol{\\Sigma}_{n-1}(\\theta_1,\\ldots,\\theta_{n-1})$ on the unit $(n-1)$-sphere.\n\nParameterization of $SO(n)$\nOur parameterization for $SO(n)$ will be an inductively defined function $\\Phi_n$, which will take the $\\binom{n}{2}$ angles $\\{\\phi_{ij}\\}_{1\\leq i \\leq j\\leq n-1}$ as input, and output an $n\\times n$ matrix in $SO(n)$.  It is defined inductively by the rule\n$$\n\\Phi_2(\\phi_{11}) \\;=\\; \\begin{bmatrix}\\cos \\phi_{11} & \\sin \\phi_{11} \\\\ -\\sin \\phi_{11} & \\cos \\phi_{11}\\end{bmatrix}.\n$$\nand\n$$\n\\Phi_n\\bigl(\\{\\phi_{ij}\\}_{1\\leq i \\leq j\\leq n-1}\\bigr) \\;=\\; M_n(\\phi_{1,n-1},\\ldots,\\phi_{n-1,n-1})\\, \\bigl(\\Phi_{n-1}(\\{\\phi_{ij}\\}_{1\\leq i \\leq j\\leq n-2})\\bigr)^a\n$$\nwhere the product is a matrix product. Conceptually, the $\\bigl(\\Phi_{n-1}(\\{\\phi_{ij}\\}_{1\\leq i \\leq j\\leq n-2})\\bigr)^a$ factor performs an arbitrary rotation on the first $n-1$ coordinates, and then the $M_n(\\phi_{1,n-1},\\ldots,\\phi_{n-1,n-1})$ performs a specific rotation that maps $\\textbf{e}_n$ to an arbitrary point on $S^{n-1}$.\nAgain, if we let $E_n$ be the subset of parameter space defined by $0\\leq \\phi_{1j}\\leq 2\\pi$ for $1\\leq j \\leq n-1$ and $0\\leq \\phi_{ij}\\leq \\pi$ for $2\\leq i\\leq j \\leq n-1$, then $\\Phi_n$ maps $E_n$ onto $SO(n)$ and $\\Phi_n$ is one-to-one on the interior of $E_n$.\nThe volume form on $SO(n)$ corresponding to Haar measure is\n$$\ndV \\;=\\; \\left(\\prod_{1\\leq i \\leq j \\leq n-1} \\sin^{i-1} \\phi_{ij} \\right) d\\phi_{11} \\cdots d\\phi_{n-1,n-1}.\n$$\nNote that this measure isn't normalized.  Instead, the total volume of $SO(n)$ is the product\n$$\n\\prod_{i=1}^{n-1} \\mathrm{Vol}(S^i),\n$$\nwhere $\\mathrm{Vol}(S^i)$ denotes the $i$-dimensional volume (i.e. surface area) of the unit $i$-sphere in $\\mathbb{R}^{i+1}$.\n\nSome Examples\nFor $n=3$, we are parameterizing $SO(3)$ using $3$ variables $\\phi_{11},\\phi_{12},\\phi_{22}$, where $\\phi_{11},\\phi_{12}\\in[0,2\\pi]$ and $\\phi_{22}\\in[0,\\pi]$.  The parameterization $\\Phi_3(\\phi_{11},\\phi_{12},\\phi_{22})$ is given by the following matrix product\n$$\n\\begin{bmatrix}\\cos\\phi_{12}&\\sin\\phi_{12}\\cos\\phi_{22}&\\sin\\phi_{12} \\sin \\phi_{22} \\\\ -\\sin\\phi_{12}&\\cos\\phi_{12}\\cos\\phi_{22}& \\cos\\phi_{12}\\sin\\phi_{22} \\\\ 0&-\\sin\\phi_{22}& \\cos\\phi_{22}\\end{bmatrix}\n\\begin{bmatrix}\\cos \\phi_{11} & \\sin \\phi_{11} & 0 \\\\ -\\sin \\phi_{11} & \\cos \\phi_{11} & 0 \\\\ 0 & 0 & 1\\end{bmatrix}.\n$$\nThe volume form is\n$$\ndV \\;=\\; \\sin \\phi_{22} \\,d\\phi_{11}\\,d\\phi_{12}\\,d\\phi_{22},\n$$\nand the total volume of $SO(3)$ is $(2\\pi)(4\\pi) = 8\\pi^2$.\nFor $n=4$, we are parameterizing $SO(4)$ with six parameters $\\phi_{11},\\phi_{12},\\phi_{22},\\phi_{13},\\phi_{23},\\phi_{33}$, where $\\phi_{11},\\phi_{12},\\phi_{13}\\in[0,2\\pi]$ and $\\phi_{22},\\phi_{23},\\phi_{33}\\in[0,\\pi]$. The parameterization $\\Phi_4(\\phi_{11},\\phi_{12},\\phi_{22},\\phi_{13},\\phi_{23},\\phi_{33})$ is the product of the matrix\n$$\n\\begin{bmatrix}\n\\cos\\phi_{13} & \\sin\\phi_{13}\\cos\\phi_{23} & \\sin\\phi_{13}\\sin\\phi_{23}\\cos\\phi_{33} & \\sin\\phi_{13}\\sin\\phi_{23}\\sin\\phi_{33} \\\\\n-\\sin\\phi_{13} & \\cos\\phi_{13}\\cos\\phi_{23} & \\cos\\phi_{13}\\sin\\phi_{23}\\cos\\phi_{33} & \\cos\\phi_{13}\\sin\\phi_{23}\\sin\\phi_{33} \\\\\n0 & -\\sin\\phi_{23} & \\cos\\phi_{23}\\cos\\phi_{33} & \\cos\\phi_{23}\\sin\\phi_{33} \\\\\n0 & 0 & -\\sin\\phi_{33} & \\cos\\phi_{33}\n\\end{bmatrix}\n$$\nwith $\\begin{bmatrix}\\Phi_3(\\phi_{11},\\phi_{12},\\phi_{22}) & \\textbf{0} \\\\ \\textbf{0}^T & 1\\end{bmatrix}$.  The volume form is\n$$\ndV \\;=\\; \\bigl(\\sin \\phi_{22}\\bigr) \\bigl(\\sin \\phi_{23}\\bigr) \\bigl(\\sin^2 \\phi_{33}\\bigr)\\,d\\phi_{11}\\,d\\phi_{12}\\,d\\phi_{22}\\,d\\phi_{13}\\,d\\phi_{23}\\,d\\phi_{33},\n$$\nand the total volume of $SO(4)$ is $(2\\pi)(4\\pi)(2\\pi^2) = 16\\pi^4$.", "meta": {"post_id": 1364495, "input_score": 22, "output_score": 36, "post_title": "Haar measure on $\\operatorname{SO}(n)$"}}
{"input": "Given two real positive definite (and therefore, symmetric) matrices $A$ and $B$, are all the eigenvalues of $AB$ real and positive?\n\nWikipedia says $AB$ is positive definite if $A$ and $B$ are positive definite and commute, but I don't need $AB$ to be symmetric.\nBetween the lines of this question the asking user somehow prove that yes, \"the eigenvalues of $AB$  are hence real and strictly positive\" but I couldn't understand if that is confirmed in the answer.", "output": "If we call $B^{1/2}$ the symmetric matrix such that $B^{1/2}B^{1/2}=B$ (i.e. the standard square root of a positive definite matrix) then \n$$\nAB=AB^{1/2}B^{1/2}=B^{-1/2}(B^{1/2}AB^{1/2})B^{1/2},\n$$\nthat is $AB$ is similar to the positive definite matrix $B^{1/2}AB^{1/2}$, sharing all eigenvalues. It makes the eigenvalues of $AB$ be positive.", "meta": {"post_id": 1365079, "input_score": 27, "output_score": 34, "post_title": "The product of two positive definite matrices has real and positive eigenvalues?"}}
{"input": "I know that derivative is the slope of the tangent line, and that integral is the area under the curve. \nMy question is that how these two distinct concepts are geometrically related? What is the relation between the slope of the tangent line and the area under the curve? \nIf these are inverse of each other, then there should be a relation between them, I believe.", "output": "The magic word is \"rate of change\". The slope is the rate of change, so the slope is simply \"how much the function grows when you move right\". So if you plot the integral curve (which geometrically can be interpreted as an area under some other curve), then the rate of change is \"how much the area grows when you expand it to the right\", which is exactly the value of the original function.\nSketch:\nOn the left, you have a curve of the original function $f(x)$, and the approximate area under it, shaded as red rectangles. The curve of the integral $\\int f(x)dx$ is obtained by \"adding\" the areas together (right figure, the $y$ coordinate measures the total area of the curve in the left figure). The derivative of the right curve is the slope (dashed lines across the rectangles), and obviously, the slope of a rectangle with unit width is exactly its height, and the height of the rectangle brings you back to the left figure and the value $f(x)$.", "meta": {"post_id": 1368425, "input_score": 36, "output_score": 85, "post_title": "How is the derivative geometrically inverse of integral?"}}
{"input": "One time I was bored and played around a bit with integrals and wolfram alpha and tested the following integral: \nhttp://www.wolframalpha.com/input/?i=integral_0%5E1+ceil%28x*sin%281%2Fx%29%29\nNote: The result at my laptop shows: $ \\int_0^1 \\lceil { x\\sin({1 \\over x})} \\rceil  \\,dx  = 1 - \\frac{\\log(4)}{2\\pi} $\nI was a bit surprised seeing this and wondered, if this result makes any sense and if yes, if there were an explanation, where $\\frac{\\log(4)}{2\\pi}$ exactly comes from. The numbers look too neat to be random, so there might be a way to derive those values directly without using the help of a calculator etc. \nAs always: Thanks in advance for any constructive answer/comment.", "output": "The ceiling function gives one iff $\\sin(1/x)> 0$, otherwise zero. So the integral is just a sum of interval lengths where the sin is positive, i.e. \n$$\n\\left(1-\\frac{1}{\\pi}\\right)+\\left(\\frac{1}{2\\pi}-\\frac{1}{3\\pi}\\right)+\\left(\\frac{1}{4\\pi}-\\frac{1}{5\\pi}\\right)+\\ldots=1-\\frac{1}{\\pi}\\left(1-\\frac12+\\frac13-\\frac14+\\ldots\\right)=\\\\\n=1-\\frac{1}{\\pi}\\ln(1+1)=1-\\frac{\\ln(2)}{\\pi}.\n$$", "meta": {"post_id": 1377921, "input_score": 45, "output_score": 55, "post_title": "Why does $ \\int_0^1 \\lceil { x\\sin({1 \\over x})} \\rceil \\,dx = 1 - \\frac{\\log(4)}{2\\pi} $?"}}
{"input": "Let $\\cal C$ be a circle in ${\\mathbb R}^2$ : \n$\\cal C=\\lbrace (x,y)\\in{\\mathbb R}^2 | \n(x-x_0)^2+(y-y_0)^2=r^2\\rbrace$ for some\nconstants $x_0,y_0,r$.\nWhat is the maximal number of points \nthat can be contained in ${\\cal C}\\cap {\\mathbb Z}^2$ ?\nI conjecture it is $4$, attained for the \"trivial\" case\n$x_0=y_0=0,r=1$.", "output": "The unit circle centred on the origin can be parametrised by $$x=\\frac {1-t^2}{1+t^2}; y=\\frac {2t}{1+t^2}$$\nAny rational value of $t$ gives rational values of $x$ and $y$. This can be scaled by a factor $r$ to give a circle of radius $r$.\nChoose $n$ such points, and then choose a radius which clears all the denominators - the resulting circle will have at least $n$ integer points.", "meta": {"post_id": 1379574, "input_score": 38, "output_score": 78, "post_title": "A circle in the plane contains at most four lattice points?"}}
{"input": "I am familiar with norms on vectors and functions, but do there exist norms for spaces of matrices i.e. $A$ some $n \\times m$ matrix?\nIf so, that would that imply matrices also form some sort of vector space?", "output": "First of all, yes: the matrices form some sort of vector space.  You can add any two matrices, and you can multiply matrices by a number, and you'll always get another matrix.  In a sense, that's all you need for a set to be a vector space. Matrices have a little bit more structure too: for one, you can multiply two matrices together (which you can't generally do with vectors).  Moreover, matrices are really linear maps.  I'll get back to those in a minute.\nThere are three kinds of matrix norms, each of which is useful in different circumstances.\n\nNorms (\"just\" a norm):\nSometimes a norm is just a norm.  Often, it's useful to think of a matrix as \"a box of numbers\" in the same way that you would think of a vector in $\\Bbb R^n$ as a \"list of numbers\".  A \"matrix norm\" by this definition is any function on the matrices that satisfies the usual rules that define a norm.  In particular, for any matrices $A,B \\in \\Bbb R^{n \\times m}$ and constant $\\alpha$, we need to have\n\n\n$\\|A\\| \\geq 0$, with $\\|A\\| = 0 \\iff A = 0$\n$\\|\\alpha A\\| = |\\alpha|\\|A\\|$\n$\\|A + B\\| \\leq \\|A\\| + \\|B\\|$\n\n\nYou would use these norms any time you would use an ordinary norm.  One reason we would need this kind of norm is to show that a function involving matrices is \"continuous\", or \"differentiable\".  The usual example of this kind of norm is the \"entrywise $p$-norm\", which is given by\n$$\n\\|A\\| = \\left(\\sum_{i=1}^n \\sum_{j=1}^m |a_{ij}|^p\\right)^{1/p}\n$$\nfor $1 \\leq p \\leq \\infty$.\nEvery matrix norm can be thought of in this way, i.e. as a \"general norm\".  However, sometimes we want our matrix norm to have a bit more structure.\n\nSubmultiplicative Norms (AKA \"matrix norms\")\n\nWe say that a matrix norm $\\|\\cdot\\|$ is submultiplicative if, in addition to being a norm, it also satisfies the inequality\n$$\n \\|AB\\| \\leq \\|A\\| \\cdot \\|B\\|\n$$\nFor any square matrices $A,B$ of the same size\n\nA lot of times, your everyday norm just won't cut it.  For those occasions, submultiplicative norms tend to come in handy.  These are useful for dealing with \"polynomials\" on matrices since we have inequalities like\n$$\n\\|f(A)\\| = \\left\\|\\sum_{k}a_kA^k \\right\\| \\leq \\sum_k |a_k|\\|A\\|^k\n$$\nNotably, if the $a_k$ are non-negative, $\\|f(A)\\| \\leq f(\\|A\\|)$, so that we have $\\|e^A\\| \\leq e^{\\|A\\|}$ for instance.\nMultiplicative norms are also very useful for spectral (eigenvalue) analysis.  In fact, we have some theorems involving $\\rho(A)$, the spectral radius of $A$, and any submultiplicative norm:\n\n$\\|A\\| \\geq \\rho(A)$\n$\\rho(A) = \\lim_{k \\to \\infty} \\|A^k\\|^{1/k}$\n$\\rho(A) = \\inf_{\\|\\cdot\\| \\text{ is submult.}} \\|A\\|$\n\nOne \"classic\" example of a submultiplicative norm is the Frobenius norm, AKA the entrywise $2$-norm, AKA the Schatten $2$-norm:\n$$\n\\|A\\|_F = \\sqrt{\\sum_{i = 1}^n\\sum_{j=1}^m |a_{ij}|^2} = \\sqrt{\\sum_{i = 1}^d\\sigma_i^2(A)},\n$$\nwhere d = $\\min\\{m,n\\}$ and $\\sigma_i$ denotes the $i$th singular value of $A$ (in non-increasing order). This is probably the most commonly used of all matrix norms. It is particularly useful since it is the norm derived from the Frobenius inner product (AKA Hilbert-Schmidt inner product). That is, it turns out that taking the \"dot product\" of matrices is a useful thing to do, and the Frobenius norm is the norm that results from this dot product.\nThe Frobenius norm falls into the class of matrix norms that are unitarily invariant, that is, norms $\\|\\cdot\\|$ that satisfy $\\|UAV\\| = \\|A\\|$ for compatible unitary matrices $U,V$. Equivalently, these are the matrix norms that can be expressed as the result of a vector norm applied to the vector of singular values (cf. Bhatia's Matrix Analysis). Some other commonly used unitarily invariant norms are the Schatten norms, which have the formula\n$$\n\\|A\\|_{S,p} = \\left[\\sum_{i=1}^d \\sigma_i^p(A)\\right]^{1/p}\n$$\nfor some value $1 \\leq p \\leq \\infty$, and the Ky-Fan norms\n$$\n\\|A\\|_{KF,k} = \\sum_{i=1}^k \\sigma_i(A)\n$$\nfor some value of $k$ with $1 \\leq k \\leq d$. In both of the above equations, $d = \\min\\{m,n\\}$\nThe entrywise $p$-norms from earlier only happen to be submultiplicative norms when $1 \\leq p \\leq 2$; these are easy to compute, but tend not give tight bounds.\nFinally, we might want our norms to be nicer still.\n\nOperator Norms (AKA \"induced/derived norms\")\n\nSuppose $\\|\\cdot \\|$ is a vector norm on $\\Bbb R^n$. We define the corresponding operator norm on $\\Bbb R^{m \\times n}$ to be given by\n$$\n\\|A\\| = \\sup_{\\|x\\| \\leq 1} \\|Ax\\|\n$$\n\nEvery operator norm is a submultiplicative norm.  However, not every submultiplicative norm is an operator norm.  Besides doing everything that the submultiplicative norms can do, operator norms are useful when you're thinking about how matrices act on vectors.  In particular, with operator norms, we have the inequality\n$$\n\\|Av\\| \\leq \\|A\\|\\cdot \\|v\\|\n$$\nIt follows that for every operator norm, the identity matrix $I$ has the property $\\|I\\| = 1$.  This fact turns out to have some useful consequences (e.g. inequalities involving the norm of a matrix's inverse).\nMost of the norms that we have mentioned are not operator norms.  The operator norm that you see most often is the one derived from the Euclidean norm ($2$-norm) on vectors.  In particular, we have\n$$\n\\|A\\|_2 = \\sup_{\\|x\\| \\leq 1} \\|Ax\\|_2 = \\sigma_1(A)\n$$\nThat is, this norm is equal to the largest singular value of $A$. This norm also happens to coincide with the \"Schatten $\\infty$-norm\" $\\|\\cdot\\|_{S,\\infty}$ and the Ky-Fan norm $\\|\\cdot\\|_{KF,1}$.\nA particularly useful property of this norm is that $\\|A\\|_2 = \\rho(A)$ whenever $A$ happens to be normal (i.e. whenever $A^TA = AA^T$). Because of this property, $\\|\\cdot\\|_2$ is sometimes called the \"spectral norm\".\nTwo other operator norms that are commonly used (especially in the context of numerical linear algebra) are the one derived from the $1$-norm (\"taxicab norm\") and the one derived from the $\\infty$-norm (\"max norm\"). These are straightforward to compute; in particular, we have\n$$\n\\|A\\|_1= \\max_j \\sum_{i=1}^m |A_{ij}|\\\\\n\\|A\\|_{\\infty}= \\max_i \\sum_{j=1}^n |A_{ij}|\n$$", "meta": {"post_id": 1394113, "input_score": 12, "output_score": 36, "post_title": "What are some usual norms for matrices?"}}
{"input": "Is there a direct way to evaluate the following series?\n\n$$\n\\sum_{n=1}^{\\infty}\\ln \\!\\left(1+\\frac1{2n}\\right) \\!\\ln\\!\\left(1+\\frac1{2n+1}\\right)=\\frac12\\ln^2 2. \\tag1\n$$ \n\nI've tried telescoping sums unsuccessfully. The convergence is clear. Given the simplicity of the result, I'm inclined to think it might exist an elegant way to get $(1)$.", "output": "Use:\n$$\\left(\\ln\\left(1+\\frac{1}{2n}\\right)+\\ln\\left(1+\\frac{1}{2n+1}\\right)\\right)^2=\\ln^2\\left(1+\\frac{1}{2n}\\right)+\\ln^2\\left(1+\\frac{1}{2n+1}\\right)+2\\ln\\left(1+\\frac{1}{2n}\\right)\\ln\\left(1+\\frac{1}{2n+1}\\right)$$\n$$\\Rightarrow \\ln\\left(1+\\frac{1}{2n}\\right)\\ln\\left(1+\\frac{1}{2n+1}\\right)=\\frac{1}{2}\\left(\\ln^2\\left(1+\\frac{1}{n}\\right)-\\ln^2\\left(1+\\frac{1}{2n}\\right)-\\ln^2\\left(1+\\frac{1}{2n+1}\\right)\\right)$$\nTherefore, the sum in the given problem is:\n$$\\frac{1}{2}\\left(\\sum_{n=1}^{\\infty}\\ln^2\\left(1+\\frac{1}{n}\\right)-\\sum_{n=1}^{\\infty}\\ln^2\\left(1+\\frac{1}{2n}\\right)-\\sum_{n=1}^{\\infty}\\ln^2\\left(1+\\frac{1}{2n+1}\\right)\\right)$$\nNotice that all the terms of the first sum are cancelled by the other two sums except $n=1$, hence our answer is:\n$$\\boxed{\\dfrac{1}{2}\\ln^22}$$", "meta": {"post_id": 1394623, "input_score": 21, "output_score": 37, "post_title": "Evaluating $\\sum_{n \\geq 1}\\ln \\!\\left(1+\\frac1{2n}\\right) \\!\\ln\\!\\left(1+\\frac1{2n+1}\\right)$"}}
{"input": "It's clear that Mersenne primes can't end in $9$, since $2^n$ can't end in $0$, but $2^n$ can end in $4$ and $2^{n}-1$ would end in 3. From the list at http://mathworld.wolfram.com/MersennePrime.html though, there aren't any known Mersenne primes that end in $3$ besides $3$. Is that a coincidence or is it impossible for a Mersenne prime to end in $3$?", "output": "We have $2^n\\equiv 4\\pmod{ 10}$ iff $n\\equiv 2\\pmod 4$, especially $n$ is even. As $n$ itself needs to be prime, the only candidate is with $n=2$ and in that case $2^n-1=3$ gives us the only Mersenne prime ending in $3$.", "meta": {"post_id": 1396012, "input_score": 37, "output_score": 61, "post_title": "Are there any Mersenne primes, besides 3, that end in 3"}}
{"input": "Starting from the number $1$ we write down a sequence of numbers where the next number in the sequence is obtained from the previous one either by doubling it or rearranging its digits (not allowing the first digit of the rearranged number to be $0$) for instance a sequence might begin \n$$1,2,4,8,16,61,122,212,424,\\ldots$$\nIs it possible to make a sequence that ends in $1000000000$ and a sequence that ends in $9876543210$. Please show me how and if there is any working.", "output": "We first note that we can attain $1000$ from $1$ by $$1,2,4,8,16,32,64,128,256,512,125,250,500,1000$$\nTherefore $1000^n$ can be attained by performing the same operations as above with $1$ replaced by $1000^{n-1}$.\nOn the other hand, note that rearranging the digits does not change the remainder when divided by $3$, and $2^n \\neq 0 \\pmod{3}$. Therefore the sequence would not reach any multiple of $3$.", "meta": {"post_id": 1399055, "input_score": 33, "output_score": 71, "post_title": "Is it possible to construct a sequence that ends in 1000000000?"}}
{"input": "When it comes to inner product I have thus far only dealt with vectors, and so the concept is very intuitive because one can easily visualize two vectors and how they get multiplied, and it is clear why the dot product of two vectors is defined the way it is. For $v\u2217u$ it would basically be the length projection of $v$ onto $u$ (the part of $v$ in direction of $u$) multiplied by the length of $u$. So you basically have a measure of how much the vectors move in same direction.\nBut when it comes to functions, what becomes the meaning of the inner product, why is the formula the way it is and what is the intution behind it. Basically for $f(x),g(x)$ you have....\n$\\int_{a}^{b} f(x)g(x) dx$\nas the inner product for two function $f(x),g(x)$ on the interval [a,b].", "output": "\"Only dealt with vectors\", you're dealing with vectors here too! Functions are vectors, and this is an inner product on a vector space!\nReally, the integral is exactly the same thing as with the dot product. For two vectors in $\\Bbb{R}^n$, the dot product is $(x_1,...,x_n)\\cdot (y_1,...,y_n)=x_1y_1+\\cdots+x_ny_n$. \nFor functions, you can think of the dot product being the same thing! You multiply the two components and add them up! The only difference is the measure or \"weight\" of each point. Since $\\Bbb{R}^n$ is discrete, each component has weight $1$, where in these function spaces each component has weight \"$dx$\".\nSo loosely, on $\\Bbb{R}^n$:\n$$(x_1,...,x_n)\\cdot (y_1,...,y_n)=x_1y_1 \\times1+\\cdots+x_ny_n\\times 1$$ \nOn a sequence space:\n$$(x_1,...)\\cdot (y_1,...)=x_1y_1 \\times1+\\cdots$$ \nOn these function spaces (I write $f\\cdot g$ to emphasise that I mean the dot product between the functions $f$ and $g$ and not the product $f(x)g(x)$ between the numbers $f(x)$ and $g(x)$):\n$$f \\cdot g=f(a)g(a)\\times dx+f(a+dx)g(a+dx)\\times dx+\\cdots=\\int_a^b f(x)g(x) dx$$\nThe inner product on function spaces is exactly the regular dot product, just in infinite dimensions and with a different \"weight\".\nEdit, for the issue of orthogonality. The pressing issue here is that inner products define what it means to be orthogonal. So I issue a challenge to you here. Without referencing an inner product (this includes angles, as the inner product DEFINES angles), what does it mean to be orthogonal? You can't answer this question. The entire notion of angle, orthogonality, etc. are summarized in:\n$$(x_1,...,x_n)\\cdot (y_1,...,y_n)=x_1y_1 \\times1+\\cdots+x_ny_n\\times 1$$ \nEven visualizing this is difficult. What do orthogonal vectors look like in 4 dimensions for example? I certainly don't know. Could you try picturing this in infinite dimensions? I can't. \nSo what does it mean to say that the inner product of two functions is $0$? The same thing it always does, that those two functions are orthogonal! Is there a nice visualization of this? Not really. \nFor another example, with this inner product comes the norm $\\|f(x)\\|_2=\\sqrt{\\int_a^b f^2(x) dx}$. Can you draw a unit circle? That is, all functions $f:[a,b]\\to\\mathbb R$ such that $\\|f(x)\\|_2=\\sqrt{\\int_a^b f^2(x) dx}=1$?\nCertainly you can't. Things become less geometric in infinite dimensions.", "meta": {"post_id": 1414389, "input_score": 50, "output_score": 88, "post_title": "What is the geometric meaning of the inner product of two functions?"}}
{"input": "I know that because of the birthday problem, even after 365 friends, you're going to have a lot of doubles and that there's also an infinitesimal chance that even with infinite friends that there's one day left out. But I was curious how many friends you'd need on average to have every day represented (this is ignoring leap day and assuming birthdays are equally distributed). Or to generalize it further, given n unique boxes and you're placing balls in them with an equal 1/n chance for the ball to go into any box, how many balls would you have to place on average before every box had at least one ball?", "output": "This is known as the coupon collector's problem, with the coupons in this case being birthdays.\nIf you keep adding friends one by one until you have one with each birthday, it will take 1 friend until you have the friend with one birthday. After the first friend, each new friend has a probability of $\\frac{364}{365}$ to have a birthday you haven't seen yet, and the expected number of friends to go through until you see a new birthday is the reciprocal of that probability, that is $\\frac{365}{364}$. After this, the expected number of friends until you see the third unique birthday is $\\frac{365}{363}$, and so forth.\nThe key insight here is that once you have seen, say, 42 different birthdays, the expected number of more friends you have to inspect until you see a new birthday does not depend on how many friends it took you to reach 42, so we can calculate the expectation for each next-unseen-birthday wait separately and just sum them.\nThe total expected number of friends until you have seen all birthdays is\n$$ 1+\\frac{365}{364}+\\frac{365}{363}+\\cdots+\\frac{365}{2}+\\frac{365}{1} = 365\\left(\\frac1{365}+\\frac1{364}+\\frac1{363}+\\cdots+\\frac12+1\\right)=365 H_{365} $$\nwhere $H_{365}$ is the 365th harmonic number, which is approximately $\\ln(365)+0.577$ (the added term is the Euler-Mascheroni constant).\nSo you need about $365\\cdot(\\ln(365)+0.577)=2364$ friends.\n\nIf we take leap days into account (but still assume all birthdays except February 29 are uniformly distributed and the probability of being born on Feb 29 is $\\frac{0.25}{365.25}$), things begin to get more complex.\nTo compute the expected time until all other days than February 29 have been seen, the numerators in the first sum above all become $365.25$ instead of $365$, so this expectation is $\\frac{365.25}{365}2364 \\approx 2366$. That's not much of a difference, so the main correction comes from the risk that we won't have seen any February 29 by the time all of the other days have been hit.\nWe can estimate the risk of this happening crudely as $(\\frac{365}{365.25})^{2364}\\approx 0.1982$ -- but this is not at all exact because (among other reasons) the averaging that led to 2364 does not commute with the non-linear $x\\mapsto (365/365.25)^x$ mapping.\nThe exact risk is\n$$ \\frac{365}{365.25} \\times \\frac{364}{364.25} \\times \\frac{363}{363.25} \\times \\cdots \\times \\frac{1}{1.25} = \\frac{\\Gamma(366)\\Gamma(1.25)}{\\Gamma(366.25)} \\approx 0.2073$$\nnamely, each time we wait for a new birthday the risk that the next new birthday will be non-leap.\n(To evaluate the gammas we can either cheat and use Wolfram Alpha, or use the approximation $\\Gamma(a+\\varepsilon)/\\Gamma(a)\\approx (a+\\frac\\varepsilon2-\\frac12)^{\\varepsilon}$ which gives plenty of precision here and allows us to compute the risk as $\\frac{\\Gamma(\\frac14)}{4 \\sqrt[4]{365\\frac58}}$ if only we can find $\\Gamma(\\frac14)$ in a reference work).\nSo the expected number of friends in the leap-day scenario should be\n$$ 2366 + 0.2073 \\frac{365.25}{0.25}\\approx 2669 $$", "meta": {"post_id": 1428097, "input_score": 54, "output_score": 83, "post_title": "On average, how many friends would I need to have to have at least one friend's birthday every day?"}}
{"input": "I've got an expression which is sum of products like:\n\n$$a_1 a_2 + b_1 b_2 + c_1 c_2 + \\cdots,$$\n\nbut the real problem I'm solving could be easily solved if I could convert this expression into something like :\n\n$$(a_1+b_1+c_1+\\cdots) \\cdot (a_2+b_2+c_2+\\cdots)$$\n\nIf some additional constants are added or subtracted, it's no problem, in fact, it's obvious.\nFirst query: Can we convert it into something like required form?\nSecond query: If yes, how?", "output": "In general, one can write a product of sums as a sum of a products:\n$$\\left(\\sum_{i \\in I} x_i\\right)\\left(\\sum_{j \\in J} y_j \\right) = \\sum_{i \\in I, j \\in J} x_i y_j.$$\nOne cannot, however, in general reverse this process, that is, write a sum of products, $$\\phantom{(\\ast) \\qquad} \\sum_{k \\in K} x_k y_k, \\qquad (\\ast)$$ as a product of sums. (In this answer we assume that the index sets, $I, J$, etc., are finite, though with some care we can extend them to infinite sets under suitable conditions.) Note that the factors $x_k, y_k$ of each summand in $(\\ast)$ are indexed by the same set $K$, whereas that is not (generally) the case for the sum of products in the first display equation. When they are, we can write the sum of products in terms of a product of sums with a correction term, namely as\n$$\\sum_{k \\in K} x_k y_k = \\left(\\sum_{k \\in K} x_k\\right) \\left(\\sum_{k' \\in K} y_{k'}\\right) - \\sum_{k, k' \\in K; k \\neq k'} x_k y_{k'},$$\nbut this is really just a reorganization, and not really an algebraic simplification.\nThe expression $(\\ast)$ can be factored in the sense that it is precisely the standard \"dot product\" on the space $\\oplus_{k \\in K} R$ of ordered $|K|$-tuples (with components indexed by some finite set $K$) of ${\\bf x} := (x_k)$ and ${\\bf y} := (y_k)$ with entries in some ring $R$,\n$${\\bf x} \\cdot {\\bf y} := \\sum_{k \\in K} x_k y_k,$$\nthough as the notation suggests, this is a definition and again not an simplification per se.", "meta": {"post_id": 1433059, "input_score": 28, "output_score": 40, "post_title": "Can we express sum of products as product of sums?"}}
{"input": "Suppose $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is continuous.\nIs it possible for $f$ to assume each value in its range an even number of times?\nTo clarify, some values might be taken 0 times, some 2, some 4, etc., but always an even (and therefore finite) number.\nI don't require that there be a value that is assumed any particular number of times. \nFor example, the function might be surjective, or never take on any value exactly twice.\nThis question is possibly the same as A continuous function cannot take every value an exact even number of times?.\nThat question may instead have meant, \"If $n$ is even, is it possible for $f$ to assume every value in its range exactly $n$ times?\" \nIn any event, it has not been answered.", "output": "Yes, this is possible.  Define $f$ as follows.  For $x\\leq 0$, $f(x)=-x$.  If $n\\in\\mathbb{N}$, then \n$$f(n+x)=\\begin{cases} n+3x &\\text{ if }0\\leq x\\leq 1/3 \\\\ n+2-3x &\\text{ if }1/3\\leq x\\leq 2/3 \\\\ n-2+3x &\\text{ if }2/3\\leq x\\leq 1 \\end{cases}$$\nA glance at a graph shows that $f$ is continuous and achieves every positive value four times, achieves the value $0$ twice, and is never negative.\n\nOn the other hand, it is impossible for every point in the range of $f$ to have the same even number of preimages.  For a contradiction, suppose $n$ is an even integer and $f:\\mathbb{R}\\to\\mathbb{R}$ achieves every value in its range exactly $n$ times.  Fix some value $a\\in f(\\mathbb{R})$ and let $x_1<x_2<\\dots<x_n$ be the preimages of $a$.  Let $p$ be the number of $x_i$ which are local minima of $f$, $r$ be the number of $x_i$ which are local maxima of $f$, and $q$ be the number of $x_i$ which are neither.  Then it follows from the intermediate value theorem that if $\\epsilon>0$ is sufficiently small, $f$ achieves the value $a+\\epsilon$ at least $2p+q$ times near the $x_i$ and $f$ achieves the value $a-\\epsilon$ at least $2r+q$ times near the $x_i$.  Thus $2p+q\\leq n$ and $q+2r\\leq n$.  But $p+q+r=n$, and so adding these two inequalities together we find that actually $2p+q=n=2r+q$ and thus $p=r$.  Since $n$ is even, this implies $q=n-p-r$ is also even.\nThat is, there are an even number of $x_i$ at which $f(x)-a$ changes sign.  Thus $f(x)-a$ has the same sign on both components of $\\mathbb{R}\\setminus [x_1,x_n]$.  Suppose WLOG that $f(x)-a$ is always positive on $\\mathbb{R}\\setminus [x_1,x_n]$.  It follows that $f$ has a global minimum value $b$ which it achieves somewhere on $[x_1,x_n]$.\nBut now replace $a$ by $b$ and repeat the argument above.  Every preimage of $b$ must be a local minimum, so $p=n$, which is clearly impossible.  This contradiction means that no such $f$ can exist.", "meta": {"post_id": 1439689, "input_score": 32, "output_score": 47, "post_title": "Can a continuous function from the reals to the reals assume each value an even number of times?"}}
{"input": "It is known that the set of non trivial zeros is an infinite set. But is it known if it is a countable, or uncountable infinite set?", "output": "If the set $Z$ of zeroes of $\\zeta(s)$ were uncountable, then it would have an accumulation point. Now, by certain version of identity theorem, this implies that $\\zeta(s)$ is identically zero on its domain, which is absurd.", "meta": {"post_id": 1491324, "input_score": 13, "output_score": 41, "post_title": "Are the nontrivial zeroes of the Riemann zeta function countable?"}}
{"input": "I am a little confused about what exactly are the difference(s) between simplicial complex, $\\Delta$-complex, and CW Complex.\nWhat I roughly understand is that $\\Delta$-complexes are generalisation of simplicial complexes (without the requirement that the intersection of two simplicial complexes is another simplicial complex), and CW Complex further generalises that (how?).\nAny explanation will be greatly appreciated.\nThanks for any enlightenment!", "output": "Simplicial complexes, $\\Delta$-complexes, and CW-complexes are all constructed by gluing together simplices.  However, for each one, there are different rules for what kinds of \"gluings\" you are allowed to use.\nFor CW-complexes, you are allowed to use almost any gluing.  Specifically, a CW-complex is constructed by induction, where at each step, you adjoin a new simplex by gluing its boundary to the complex you have already by any map.  More explicitly, if $Y$ is the CW-complex you have built so far and $f:\\partial \\Delta^n\\to Y$ is any continuous map, you can build a CW-complex $X=Y\\sqcup\\Delta^n/{\\sim}$, where $\\sim$ is the equivalence relation that identifies any $x\\in\\partial\\Delta^n$ with $f(x)\\in Y$.  The only restriction to this gluing process is that you have to add simplices in increasing order of dimension.  That is, you have to start with all the $0$-simplices, then glue in all the $1$-simplices, then glue in all the $2$-simplices, and so on.  You're not allowed to glue in a new $1$-simplex once you've already added a $2$-simplex.  (If you drop this ordering condition, you get a more general notion, which is sometimes called simply a \"cell complex\".)\nFor $\\Delta$-complexes, you do the same thing, except that the maps $f$ you can use when adding a new cell are highly restricted.  Specifically, for each $(n-1)$-simplex $A$ which is a face of $\\partial\\Delta^n$, the restriction of $f$ to $A$ must be equal to the inclusion of one of the $(n-1)$-simplices you already have.  That is, $f$ maps the $n$ vertices of $A$ (with their canonical ordering) to the $n$ vertices of some $(n-1)$-simplex you've already added to your complex (with the same ordering on the vertices), and $f$ extends to all of $A$ by just interpolating linearly.  Intuitively, this means that your complex is a union of simplices which are glued together by just gluing their faces together in the \"obvious\" linear way (for instance, as one encounters in a triangulation of a surface), rather than by arbitrary complicated continuous maps.  Note, however, that some faces of a single simplex might get glued to each other: the restriction on what $f$ can be only applies to each face of $\\partial\\Delta^n$ separately.  So, for instance, you can start with a single vertex, and then add an edge both of whose boundary vertices are the one vertex you started with (this gives you a circle).  You could then add a triangle such that each of its three sides are equal to the one edge you have (this gives a space which cannot be embedded in $\\mathbb{R}^3$ and is rather hard to visualize!).\nFinally, simplicial complexes are $\\Delta$-complexes which satisfy even more restrictions.  First, $f$ is required to map different faces of $\\partial\\Delta^n$ to different $(n-1)$-simplices, so the situation discussed at the end of the previous paragraph cannot happen.  In addition, you are not allowed to add two different $n$-simplices with the same set of vertices, so that a simplex in a simplicial complex is uniquely determined by its set of vertices (which, by the first requirement, you can show are all distinct).", "meta": {"post_id": 1528005, "input_score": 101, "output_score": 151, "post_title": "Simplicial Complex vs Delta Complex vs CW Complex"}}
{"input": "On the most recent Seton Hall Joseph W. Andrushkiw Competition, the final question was as follows:\n\nLet $A = (\\sqrt{3}+\\sqrt{2})^{2016}$. When A is written in decimal\n  form, what is its $31^{st}$ digit after the decimal point?\n\nBrute forcing it via wolfram alpha reveals that the answer is [edit: I found the 31st number from the start, not the 31st after the decimal point] zero, yet this competition does not allow the use of a calculator. It seems to me that as irrational numbers are in the base of the exponent, there should not be an identifiable pattern in the digits. \nSearching this site has made me think that perhaps the answer has something to do with the Euler phi function (something which I will admit up front I have never been acquainted with), but I can't find anything which I understand enough to give me a concrete way to start to approach this. Any help on this frustrating problem would be appreciated. Thanks!", "output": "Hmm. Pretty sure that the answer is $9$.\nThe key observation to this problem is noticing that $(\\sqrt{3}-\\sqrt{2})^{2016}+(\\sqrt{3}+\\sqrt{2})^{2016}$ is an integer. \nThe proof of this is expansion using Binomial Theorem. The odd powers of the square roots get canceled out.\nNow we have $(\\sqrt{3}+\\sqrt{2})^{2016} = N - (\\sqrt{3}-\\sqrt{2})^{2016}$, where $N$ is a positive integer.\nNow this is easy. Since $(\\sqrt{3}-\\sqrt{2})^{2016} < (0.4)^{2016} = (0.064)^{\\frac{2016}{3}} < (0.1)^{\\frac{2016}{3}} < (0.1)^{600}$, we have $(\\sqrt{3}+\\sqrt{2})^{2016}= (N-1)+0.99\\cdots 99$, and there are at least $500$ $9$'s there. The answer is $\\boxed{9}$.", "meta": {"post_id": 1544422, "input_score": 27, "output_score": 44, "post_title": "Calculating decimal digits by hand for extremely large numbers"}}
{"input": "Let's assume that $A\\subseteq X$ is product of $A_{i}\\subseteq X_{i}(i\\in I)$.\nThen product topology of $A$ is the same than the topology induced by $X$.\nI have proved this few different times now, and for this one I need help. I like to try all kinds of proofs.\nCollection $\\mathcal{B}_{A}$ has sets $V:=\\prod_{i\\in I} V_i$, where $V_i\\subseteq A_{i}$ by every $i\\in I$ and $V_i\\neq A_i$ finitely many $i$.\nCollection $\\mathcal{B}_{X}$ has sets $\\prod_{i}U_{i\\in I}\\cap \\prod_{i}A_{i\\in I}$, where and $U_{i}$ is element of basis of $X_{i}$. Notation $U:=\\prod_{i}U_{i\\in I}$.\nThere is theorem that says $\\mathcal{B}$ is basis for topology iff every open $U\\subseteq X$ can be shown in the form\n$$\nU=\\bigcup_{a\\in A} B_{a},\n$$\nwhere $B_{a}\\in\\mathcal{B}$ for all $a\\in A$.\nNow what I am trying to do in this proof attempt is that I want to try out the theorem above.\nThere are problems with the notion and that is one main thing where I need tips.\nI hope that you get the idea what I am after here.\nProof:\n$Z$ belongs in the product topology of $A$.\n$\\Leftrightarrow$ \n$$\nZ=\\bigcup_{i\\in I} \\big(\\prod_{i\\in I} V_{i} \\big)_{i}\\quad\\text{Where } V_{i}\\in \\mathcal{T}_{A_{i}}\\text{ for all }i\\in I.\n$$\n$\\Leftrightarrow$\n$$\nZ=\\bigcup_{i\\in I} \\big(\\prod_{i \\in I}U_{i}\\cap A_{i} \\big)\\quad\\text{Where }U_{i}\\in\\mathcal{T_{i}}\\text{ for all }i\\in I.\n$$\n$\\Leftrightarrow$\n$$\nZ=\\bigcup_{i\\in i} (\\prod_{i\\in I} U_{i}\\cap \\prod_{i\\in I} A_{i})_{i}\n$$\n$\\Leftrightarrow$\n$$\nZ=\\big(\\bigcup_{i\\in I}\\big(\\prod_{i\\in I} U_{i} \\big)_{i} \\big)\\cap \\prod_{i\\in I} A_{i}\n$$\n$\\Leftrightarrow$\n$Z$ belongs to product topology induced by $X$.", "output": "Let's start with a definition:\nLet $(X,\\mathcal{T})$ be a topological space.\nLet $I$ be an index set, and let $Y_i (i \\in I)$ be topological spaces\nand let $f_i: X \\rightarrow Y_i$ be a family of functions.\nThen $\\mathcal{T}$ is called the initial topology with respect to the maps $f_i$\niff\n\n$\\mathcal{T}$ makes all $f_i$ continuous.\nIf $\\mathcal{T}'$ is any other topology on $X$ that makes all $f_i$ continuous, then $\\mathcal{T} \\subseteq \\mathcal{T}'$.\n\nOr put more shortly: $\\mathcal{T}$ is the smallest (coarsest) topology that makes all $f_i$ continuous.\nRemark: it is not very useful to ask for the largest topology to make all\n$f_i$ continuous, then we would always get the discrete topology on $X$.\nThis is the largest topology on $X$ and it makes any map continuous.\nThis is in fact a common way to construct a topology on a set $X$, based\non functions to spaces $Y_i$ that already have a topology.\nE.g. in linear space theory we can consider an alternative topology on a linear\nspace $X$ as the smallest topology that makes all linear maps (in the original topology)\nfrom $X$ to $\\mathbb{R}$ continuous. This is called the weak topology on the linear space $X$\n(as it generally weaker (fewer open sets) than the original topology on $X$.)\nThat this works is based on the following simple:\nExistence theorem for initial topologies:\nLet $X$ be a set and $f_i : X \\rightarrow Y_i$ be a collection of topological spaces\nand maps. Then there is a topology on $X$ that is initial w.r.t. the maps $f_i$.\nMoreover, this topology is unique and a subbase of the topology is given by\n$\\mathcal{S} = \\{(f_i)^{-1}[O]: i \\in I, O \\text{ open in } Y_i\\}$.\n(a subbase is a collection $\\mathcal{S}$ of subsets of $X$ such that all finite intersections\nof elements of $\\mathcal{S}$ form a base for the topology).\nProof: Let $\\mathcal{T}$ be the topology generated by $\\mathcal{S}$ as a subbase.\nThis means that $\\mathcal{T}$ is the collection of all sets that can be written\nas unions of finite intersections from $\\mathcal{S}$.\nThen all $f_i$ are continuous, as for all open $O \\subseteq Y_i$ the inverse image under $f_i$ of $O$ is in $\\mathcal{T}$.\nAnd if $\\mathcal{T}'$ is any topology that makes all $f_i$ continuous, $\\mathcal{T}'$\nmust contain all sets of the form $(f_i)^{-1}[O]$ and so $\\mathcal{T}'$ must contain\n$\\mathcal{S}$, and as $\\mathcal{T}'$ is closed under finite\nintersections and unions, we have that $\\mathcal{T} \\subseteq \\mathcal{T}'$, as required.\nThe unicity is clear, because if $\\mathcal{T}$ and $\\mathcal{T}'$ are both initial then $\\mathcal{T} \\subseteq \\mathcal{T}'$\n(by 2) applied to $\\mathcal{T}$) and $\\mathcal{T}' \\subseteq \\mathcal{T}$ (by 2)\napplied to $\\mathcal{T}'$) and thus $\\mathcal{T} = \\mathcal{T}'$.\nExample 1: if $A$ is a subset of a topological space $X$, and $i: A \\rightarrow X$\nis the inclusion map from $A$ to $X$ (defined by $i(x) = x$ for all $x \\in A$),\nthen the subspace topology on $A$ is just the initial topology w.r.t. $i$.\nProof: the subspace topology is defined by $\\{O \\cap A: O \\text{ open in } X\\}$.\nBut $i^{-1}[O] = O \\cap A$ ($x \\in O$ and $x \\in A$, then $i(x) = x$ is in $O$, so\n$x \\in i^{-1}[O]$ and if $x \\in i^{-1}[O]$ then $x \\in A$ by definition and $x = i(x)$\nmust be in $O$, so $x$ is in $O \\cap A$).\nWe see that the subbase $\\mathcal{S}$ from the existence theorem is just equal\nto the subspace topology!\nRemark: in general, when we have just one function $f: X \\rightarrow Y$, and $X$ has the initial\ntopology w.r.t. $f$, we get the topology and not just a subbase. Also, if $f$ is moreover\ninjective (one-to-one), then $f$ is called an embedding.\nExample 2:\nIf $X_i, i \\in I$ is a family of topological spaces and $X$ is the Cartesian product of\nthe spaces $X_i$, then we have the projection maps $p_j: X \\rightarrow X_j$ defined\nby $p_j( (x_i)_{i \\in I}) = x_j$ for all $j \\in I$.\nThen the product topology on $X$ is just the initial topology w.r.t. the\nmaps $p_i$ ($i \\in I$).\nProof: the sets $(p_j)^{-1}[O]$ are just the product sets of the form $\\prod_i O_i$\nwhere all $O_i = X_i$ except $O_j = O$. So the finite intersections of the subbase\nelements are exactly all such sets $\\prod_i O_i$ where finitely many\n$O_i$ are some open set in their respective coordinate space\nand in all other coordinates $O_i$ are equal to $X_i$; this is precisely the\nstandard base for the product topology.\nSo two very common ways of making new spaces from old ones, subspaces and products,\nare special cases of initial topologies. In the following I will develop some\nbasic theory that will allow us to formulate and prove general principles that will\napply to all examples of initial topologies. Some well-known facts can then be seen\ntogether in a common framework.\n\nThe fact that a space $X$ has the initial topology w.r.t. a family of mappings, makes it\neasy to recognise continuous functions to $X$.\nWe have the following useful:\nUniversal theorem of continuity for initial topologies.\nLet $X$ be a space and $f_i : X \\rightarrow Y_i$ $(i \\in I)$ a family of mappings\nand spaces $Y_i$, such that $X$ has the initial topology with respect to the $f_i$.\nLet $W$ be any space and $g$ a function from $W$ to $X$.\nThen $g$ is continuous iff for all $i \\in I$: $f_i \\circ g$ is continuous from $W$ to $Y_i$.\nProof: if $g$ is continuous then all $f_i \\circ g$ are also continuous,\nbecause all $f_i$ are continuous and compositions of continuous maps are continuous.\nSuppose now that $f_i \\circ g$ is continuous for all $i$.\nLet $S \\subseteq X$ be any element of the subbase $\\mathcal{S}$ (from the existence theorem), so that\n$S = (f_i)^{-1}[O]$ for some open subset $O$ of $Y_i$.\nNow $$g^{-1}[S] = g^{-1}[(f_i)^{-1}[O]] = (f_i \\circ g)^{-1}[O]$$ which is open in $W$ because $f_i \\circ g$ is continuous.\nThis shows that inverse images of elements from $\\mathcal{S}$ are open.\nBut then as $g^{-1}$ preserves (finite) intersections and unions and as all open\nsubsets of $X$ are unions of finite intersections of elements from $\\mathcal{S}$,\nwe see that $g^{-1}[O]$ is open for all open subsets $O$ of $X$. Or: $g$ is continuous.\nThere is a converse to this as well:\nCharacterisation of the initial topology by the continuity theorem.\nLet $X$ be a space, and $f_i: X \\rightarrow Y_i$ be a family of spaces and functions.\nSuppose that $X$ satisfies the universal continuity theorem in the following sense:\n\n(*) for all spaces $Z$, for any function $g: Z \\rightarrow X$:  $(f_i \\circ g)$ is continuous for all $i$ iff $g$ is continuous.\n\nThen $X$ has the initial topology w.r.t. the maps $f_i$.\nProof: the identity on X is always continuous, so applying ($\\ast$) from right to left with $g = \\operatorname{id}$\ngives us that all $f_i$ are continuous. If $\\mathcal{T}'$ is another topology on $X$\nthat makes all $f_i$ continuous, then consider the map $g: (X, \\mathcal{T}') \\rightarrow (X, \\mathcal{T})$,\ndefined by $g(x) = x$. Then all maps $f_i \\circ g$ are just the maps $f_i$ as seen between\n$(X, \\mathcal{T}')$ and $Y_i$ which are by assumption continuous.\nSo by the other direction of (*) we see that $g$ is continuous,\nand thus (as $g(x) = x$, and thus $g^{-1}[O] = O$ for all $O$) we have that\n$\\mathcal{T} \\subseteq \\mathcal{T}'$,\nas required for the second property of the initial topology.\n\nApplications:\nCharacterisation of continuity into products:\na map $f$ into a product $\\prod_{i \\in I} X_i$ is continuous iff $p_i \\circ f$ is continuous\nfor all $i \\in I$.\nOr suppose that $X$ is any space, $Y'$ a subspace of a space $Y$, and $g: X \\rightarrow Y$\nis a map such that $g[X] \\subseteq Y'$.\nThen there is the \"image restriction\" $g': X \\rightarrow Y'$ of $g$, defined by $g'(x) = g(x)$.\nNote that, if $i: Y' \\rightarrow Y$ is the inclusion, then $Y'$ has the initial topology w.r.t. i,\nand moreover, $g = g' \\circ i$. So the theorem we just proved says: $g$ continuous iff $g'$ continuous.\nThis has the intuitive meaning that continuity of $g$ is only determined by $Y'$.\nSo if $g$ is an embedding (see above), then $g$ is a continuous bijection between\n$X$ and $g[X]$, which is also open as a map between these spaces, when we give $g[X]$\nthe subspace topology: let $O$ be open in $X$. Then $O = g^{-1}[O']$ for some open subset\nof $Y$ (by embedding = initial map), and then $O' \\cap g[X]$ is open in g[X],\nand $g[O] = O' \\cap g[X]$, by injectivity of $g$. The reverse is also quite easy to see\n(exercise): if $g:X \\rightarrow g[X] \\subseteq Y$ is a homeomorphism, then $g$ is an embedding from $X$\ninto Y. Many books actually define embeddings that way.\nNote that the restriction of $f$ to $A$, $f | A$, is just $f \\circ i$, where $i$ is the embedding\nof $A$ into $X$, so that $f | A$ is continuous as a composition of continuous maps.\n\nApplication: diagonal product map.\nLet $X$ be a space and let $Y_i$ ($i \\in I$) be a family of spaces, and $f_i : X \\rightarrow Y_i$\nbe a family of functions. Let $Y$ be the product of the $Y_i$, with projections $p_i$.\nDefine $f:X \\rightarrow Y$, the so-called diagonal product of the $f_i$, as follows:\n$f(x) = (f_i(x))_{i \\in I}$. Then $f$ is continuous iff for all $i \\in I$, $f_i$ is continuous.\nProof: immediate from the universal continuity theorem, because for all $i$ we have\nthat $p_i \\circ f = f_i$.\n\nApplication: product maps.\nLet $f_i : X_i \\rightarrow Y_i$ be a family of functions between spaces $X_i$ and $Y_i$,\nlet $X = \\prod_{i \\in I} X_i$, $Y = \\prod_{i \\in I} Y_i$, and let\n$f:X \\rightarrow Y$ be defined by $f((x_i)_i) = (f_i(x_i))_i$, which is called the product map of the $f_i$.\nThen $f$ is continuous iff for all $i \\in I$ we have that $f_i$ is continuous.\nProof: let $p_i$ be the projections from $Y$ to $Y_i$, and let $q_i$ be the\nprojections from $X$ to the $X_i$.\nThen for all $i$ we have $$p_i \\circ f = f_i \\circ q_i\\text{.}$$\nSuppose that all $f_i$ are continuous.\nThen, as all $q_i$ and $f_i$ are continuous, all maps $p_i \\circ f$ are continuous.\nAs $Y$ has the initial topology w.r.t. the $p_i$, we have by the universal continuity\ntheorem that $f$ is continuous.\nNow let $f$ be continuous. Fix $i$ in $I$. Also take a point $r= (r_i)_{i \\in I}$ from $X$.\nLet $(s_i)_{i \\in I}$ in $Y$ be its image $f(r)$.\nThen the map $k_i: X_i \\rightarrow X$, defined as the diagonal product of the identity\non $X_i$ and all constant maps onto the point $r_j$ for all $j \\neq i$.\nBy the previous application, this is continuous. Moreover, $q_i \\circ k_i$ is the identity on $X_i$,\nalso denoted by $\\operatorname{id}_{X_i}$. But note that\n$$ f_i = f_i \\circ \\operatorname{id}_{X_i} = f_i \\circ (q_i \\circ k_i) = \n(f_i \\circ q_i) \\circ k_i = (p_i \\circ f) \\circ k_i\\text{,}$$ which is continuous, as $f$, $p_i$ and $k_i$ are.  So $f_i$ is continuous, for all $i \\in I$.\n\nA very useful general fact is the following:\nTransitive law of initial topologies.\n\nSuppose that we have a family of spaces and maps $f_i : X \\rightarrow Y_i$\n( $i \\in I$) and for each $i \\in I$ an index set $I_i$, and a family\nof maps $g_{i,j} : Y_i \\rightarrow Z_j$ for $j$ in $I_i$.\nAssume that each $Y_i$ has the initial topology w.r.t. the $g_{i,j}$ ($j \\in I_i$).\nThen $X$ has the initial topology w.r.t. the maps $g_{i,j} \\circ f_i$ ($i \\in I, j \\in I_i$)\niff $X$ has the initial topology w.r.t. the $f_i$ ($i \\in I$).\n\nProof:\nSuppose that $X$ has the initial topology w.r.t. the $f_i$. Call this topology $\\mathcal{T}$.\nAll $g_{i,j}$ are continuous (part of being initial of the topology on $Y_i$)\nso all $g_{i,j} \\circ f_i$ are continuous. Suppose that $\\mathcal{T}'$ is another topology on $X$\nthat makes all $g_{i,j} \\circ f_i$ continuous.\nThen consider the maps $f'_i : (X,\\mathcal{T}') \\rightarrow Y_i$, defined by $f'_i(x) = f_i(x)$.\nSo we have $g_{i,j} \\circ f'_i = g_{i,j} \\circ f_i$ for all relevant indices.\nBy assumption all $g_{i,j} \\circ f'_i = g_{i,j} \\circ f_i : (X,\\mathcal{T}') \\rightarrow Z_j$\nare continuous, and as all $Y_i$\nhave the initial topology w.r.t. the $g_{i,j}$, we see that all $f'_i$ are (by the universal\ncontinuity theorem) continuous. If $\\operatorname{id}$ is the identity map from\n$(X,\\mathcal{T}') \\rightarrow (X,\\mathcal{T})$, then\n$f_i \\circ \\operatorname{id} = f'_i$ for all $i$. We have just seen that all $f'_i$ are continuous, and\nas $\\mathcal{T}$ is initial w.r.t. the $f_i$, we see that $\\operatorname{id}$\nis a continuous map by this same universal continuity theorem.\nBut the identity from $(X,\\mathcal{T}') \\rightarrow (X,\\mathcal{T})$ is continuous iff\n$\\mathcal{T} \\subset \\mathcal{T}'$\n(as $O \\in \\mathcal{T}$ means $\\operatorname{id}^{-1}[O] = O \\in \\mathcal{T}'$), so\n$\\mathcal{T}$ is indeed minimal w.r.t. the continuity\nof all maps $g_{i,j} \\circ f_i$, and $X$ has the initial topology w.r.t. these maps.\nSuppose on the other hand that $X$ has the topology $\\mathcal{T}$, which is initial w.r.t. the maps\n$g_{i,j} \\circ f_i$. Let $i$ be in $I$. For all $j \\in I_i$ we know that $g_{i,j} \\circ f_i$\nis continuous. As $Y_i$ has the initial topology w.r.t. the maps $g_{i,j}$ ($j \\in I_i$),\nwe see again by the universal continuity theorem that $f_i$ is continuous. So all $f_i$\n(from $(X,\\mathcal{T})$ to $Y_i$) are continuous.\nLet $\\mathcal{T}'$ be another topology on $X$ that makes all $f_i$ continuous.\nThis means that all $g_{i,j} \\circ f_i$ are continuous, and so by minimality of $\\mathcal{T}$\n(by the definition of initial topology w.r.t. the maps $g_{i,j} \\circ f_i$) we see that\n$\\mathcal{T} \\subseteq \\mathcal{T}'$.\nSo $\\mathcal{T}$ is the initial topology w.r.t. the $f_i$.\n\nTwo useful applications, to make all this less abstract:\nSubspaces of subspaces:\nLet $A$ be a subspace of $B$ and $B$ a subspace of $X$ ($A \\subseteq B \\subseteq X$) then\n$A$ is a subspace of $X$ (i.e. it has the subspace topology w.r.t. $X$).\nProof:\nApply the above to $i_B: B \\rightarrow X$, $i_{A,B}: A \\rightarrow B$,\n$i_A: A \\rightarrow X$, all basically the identity\nwith different domains and codomains. So by assumption $A$ has the initial topology w.r.t. $i_{A,B}$\nand $B$ has the initial topology w.r.t. $i_B$.\nNote that $i_A = i_B o i_{A,B}$, so by the transitivity theorem (right to left) we see that\n$A$ has the initial topology w.r.t. $i_A$, or $A$ has the subspace topology w.r.t. $X$.\nProducts and subspaces:\n\nLet $X_i$ ($i \\in I$) be a family of spaces, with subspaces $A_i \\subseteq X_i$.\nThen $A = \\prod_{i \\in I} A_i$ (in the product topology of the subspace topologies)\nis a subspace of $X = \\prod_{i \\in I} X_i$ (it has the initial topology w.r.t. the inclusion).\n\nProof:\nLet $k_i$ be the inclusion mapping from $A_i$ to $X_i$.\nLet $k: \\prod_i A_i \\rightarrow \\prod_i X_i$ the product mapping (as above).\nNote that $k$ is also the inclusion from $A$ into $X$.\nAgain let $p_i$ be the projections from $A$ onto the $A_i$, and $q_i$ the projections\nfrom $X$ onto the $X_i$. Then\n$$(\\ast) q_i \\circ k = k_i \\circ p_i \\text{ for all } i \\in I \\text{.} $$\n$A_i$ has the initial topology w.r.t. $k_i$, and $A$ has the initial topology w.r.t. the $p_i$.\nSo $A$ has the initial topology w.r.t. the maps $k_i \\circ p_i$ ($i \\in I$) by the\nright to left implication of the transitivity theorem. So $A$ has the initial topology w.r.t. the maps\n$q_i \\circ k$ by $(\\ast)$. But by the transitivity theorem (the other implication ) we see that $A$ has the initial topology w.r.t. the map $k$, which is, as said, the inclusion $A \\rightarrow X$. So $A$ has the subspace topology.\nSo we see that all these general considerations give a nice proof of\n\"a product of subspaces is a subspace), due to the special nature of these\ntopologies as initial topologies with respect to certain maps.\nRemark: for those who know inverse limits, which are subspaces of products\nof a certain kind, the above theorem also shows that in fact the inverse limit topology\nis itself an initial topology w.r.t. the restricted projection maps. This gives\nrise to a canonical subbase for the inverse limit, from the existence theorem, which\nis sometimes useful as well.\nAs a final remark: a similar theory can be developped for so-called \"final\" topologies.\nThis applies to situations where we have maps $f_i: X_i\\rightarrow X$ where we want $X$ to have\nthe largest topology that makes all $f_i$ continuous.\nSpecial cases include quotient topologies, disjoint sums and weak topologies induced by subspaces.\nAlso here we have an existence theorem, a universal continuity theorem and a transitive law. See here for the details.", "meta": {"post_id": 1548495, "input_score": 20, "output_score": 49, "post_title": "Proof that product topology of subspace is same as induced product topology"}}
{"input": "If $S = A \\cup B$, then $S$ is the collection of all points in $A$ and $B$\n\nWhat about $S = A \\sqcup B$?, I think disjoint union is the same as union, only $A, B$ are disjoint. So the notation is a bit misleading. Because it is not a new operation, but operation where the pair $A,B$ satisfies $A \\cap B = \\varnothing$. \nSo given $A \\cap B = \\varnothing$, $S = A \\sqcup B = A \\cup B$.\nIs my interpretation correct?", "output": "The notation $A\\sqcup B$ (and phrase \"disjoint union\") has (at least) two different meanings.  The first is the meaning you suggest: a union that happens to be disjoint.  That is, $A\\sqcup B$ is identical to $A\\cup B$, but you're only allowed to write $A\\sqcup B$ if $A$ and $B$ are disjoint.\nThe second meaning is that $A\\sqcup B$ is a union of sets that look like $A$ and $B$ but have been forced to be disjoint.  There are many ways of defining this precisely; for instance, you could define $A\\sqcup B= A\\times\\{0\\}\\cup B\\times \\{1\\}$.  This construction can also be described as the coproduct of $A$ and $B$ in the category of sets.\n(This ambiguity is similar to the ambiguity between \"internal\" and \"external\" direct sums; see for instance my answer here.)", "meta": {"post_id": 1631396, "input_score": 31, "output_score": 44, "post_title": "What is the difference between disjoint union and union?"}}
{"input": "Given $n\\in\\mathbb{N}$, I need to get just enough more than half of it. For example (you can think this is : number of games $\\rightarrow$ minimum turns to win)\n$$ 1 \\rightarrow 1 $$\n$$ 2 \\rightarrow 2 $$\n$$ 3 \\rightarrow 2 $$\n$$ 4 \\rightarrow 3 $$\n$$ 5 \\rightarrow 3 $$\n$$ 6 \\rightarrow 4 $$\n$$ 7 \\rightarrow 4 $$\n$$ \\vdots $$\n$$ 2i \\rightarrow i+1 $$\n$$ 2i+1 \\rightarrow i+1 $$\n$$ \\vdots $$\nIs it possible to create a simple formula without piecewise it into odd and even? Sorry for my bad English.", "output": "How about:\n$$\n\\frac{3+2n+(-1)^n}{4}\n$$\nor (continuous function of $n \\in \\mathbb R$ or even $\\mathbb C$):\n$$\n\\frac{3+2n+\\cos(\\pi n)}{4}\n$$", "meta": {"post_id": 1633901, "input_score": 9, "output_score": 62, "post_title": "Formula for the simple sequence 1, 2, 2, 3, 3, 4, 4, 5, 5, ..."}}
{"input": "As far as I know, mathematics is written based on a language which can be for example set theory or category theory. My concern is about the power of these languages. How can we realize which language is more powerful and we can construct more structures with them? Maybe, there is an assumption which implies all languages are equal, and actually, there is no strength preference although there is maybe the simplicity preference.   If it is the case, could you please clarify why you think this assumption is good and obvious?\np.s. : I am not an expert, so if you think my question is absolutely wrong or does not make sense at all, please let me know.", "output": "Set theory and category theory are both foundational theories of mathematics (they explain basics), but they attack different aspects of foundations. Set theory is largely concerned with \"how do we build mathematical objects (or what could we build)\" while category theory is largely concerned with \"what structure to mathematical objects have (or could have)\"?\nMathematicians work in informal set theory and informal category theory, which are immensly useful as lingua franca and as collections of universally useful concepts and techniques, but their formal versions are not actually needed by mathematicians for the most part. This is witnessed by the fact that the average mathematician is unable to list the axioms of Zermelo-Fraenkel set theory, and even of first-order logic. Yet, they are perfectly able to do complicated math.\nThe formal versions of set theory and category theory are of interest to people who study foundations of mathematics. These relationship between these two and computation has been known for a while. I highly recommend Bob Harper's blog post about the Holy Trinity for a quick read, and Steve Awodey's From Sets to Types to Categories to Sets if you would like know more about the connections and their significance.\nThe upshot is that we can mostly translate between set theory, type theory, and category theory, and that ordinary mathematicians could do their mathematics in either of the three systems, but the systems are not exclusive. In fact, a smart mathematician will be aware of their connections and will take advantage of them.", "meta": {"post_id": 1639982, "input_score": 7, "output_score": 34, "post_title": "Which is the most powerful language, set theory or category theory?"}}
{"input": "Prove the following without using L'Hospital's Rule, integration or Taylor Series:\n$$\\lim_{n \\to \\infty} \\frac{\\ln(n)}{n}=0 $$\nI began by rewriting the expression as: $$\\lim_{n \\to \\infty}{\\ln(n^{1/n})} $$\nSince the text shows $$\\lim_{n \\to \\infty}{n^{1/n} = 1} $$\nI was wondering is the proof just as simple as stating:\n$$\\lim_{n \\to \\infty}{\\ln(1) = 0} $$\nor do I need to apply the squeeze theorem,  use a   $\\varepsilon$-N proof, or etc?", "output": "Since $e^x> x$, we have $\\ln x < x$ for all $x  >0$. \nHence,\n$$0 \\leqslant \\frac{\\ln n}{n} = \\frac{2 \\ln \\sqrt{n}}{n} < \\frac{2 \\sqrt{n}}{n} = \\frac{2}{\\sqrt{n}} \\to 0$$", "meta": {"post_id": 1642671, "input_score": 7, "output_score": 36, "post_title": "Prove $\\lim_{n \\to \\infty} \\frac{\\ln(n)}{n}=0$ without L'Hospital's Rule"}}
{"input": "I was reading some topics in Homological Algebra when I came across the concepts of cone of a map of complexes and cylinder.\nMy knowledge of Algebraic Topology is pretty basic so I only used these concepts in a pure algebraic setting. What is the motivation for this? \nThe shapes of a cone for example appears only in the simplicial context of Algebraic Topology or is it possible to \"see\" the cone in algebraic terms ?", "output": "One possible motivation for the mapping cone is the fact that a morphism of chain complexes is a quasi-isomorphism iff its mapping cone has vanishing homology. So in this sense, the homology of the mapping cone of $f$ measures the default of $f$ to be a quasi-isomorphism.\nFrom an abstract homotopy theory point of view, one can first consider the mapping cylinder of $f : X_* \\to Y_*$. In algebraic topology, the mapping cylinder of $f : X \\to Y$ is $Y \\cup_{X \\times 0} X \\times I$. We'll try to see how this could be translated in homological algebra, and hopefully the picture will be clearer.\nIn homological algebra, the interval $I = [0,1]$ is replaced by the chain complex $I_*$ that has $I_0 = \\mathbb{Z} v_+ \\oplus \\mathbb{Z} v_-$ (this is a free abelian group of rank two) and  $I_1 = \\mathbb{Z} e$, $I_n = 0$ if $n \\neq 0,1$, and $d : I_1 \\to I_0$ is given by $d(e) = v_+ - v_-$. It's an acyclic chain complex that represents an interval (in some sense that can be made precise; it is a path object in the model category of chain complexes). Roughly speaking $v_+$ is the vertex $\\{1\\}$, $v_-$ is the vertex $0$, and $e$ is the edge between the two.\nThe product $X \\times I$ becomes the tensor product $X_* \\otimes I_*$, which has:\n$$(X \\otimes I)_n = X_n \\otimes v_+ \\oplus X_n \\otimes v_- \\oplus X_{n-1} \\otimes e$$\nand the differential is given by $$d(x \\otimes v_\\pm) = dx \\otimes v_\\pm, \\\\ d(x \\otimes e) = dx \\otimes e + x \\otimes v_+ - x \\otimes v_-.$$\nAnd now the mapping cylinder $Y \\cup_{X \\times 0} X \\times I$ is replaced by $\\operatorname{Cyl}(f) = Y \\oplus_{X \\otimes v_-} X \\otimes I$. It is the quotient of $Y \\oplus X \\otimes I$ where you identify $x \\otimes v_- \\in X \\otimes I$ with $f(x) \\in Y$ (recall that $v_-$ represents the vertex $0 \\in [0,1]$). So concretely we get:\n$$\\operatorname{Cyl}(f)_n = Y_n \\oplus X_n \\oplus X_{n-1} \\\\\nd(y, 0, 0) = (dy, 0, 0) \\\\\nd(0,x,0) = (0, dx, 0) \\\\\nd(0,0,x') = (-f(x'), x', dx')$$\nThe first factor is the image of $X \\otimes v_-$, which is identified with $Y$. The second factor is $X \\otimes v_+$, and the last part is $X \\otimes e$.\nNow to get the mapping cone from the mapping cylinder, in algebraic topology you collapse $X \\times 1$. The $X \\times 1$ part in homological algebra corresponds to the middle $X_n$ (really $X_n \\otimes v_+$) in $\\operatorname{Cyl}(f)_n$, so just quotient out by this ideal to get\n$$\\operatorname{Cone}(f)_n = Y_n \\oplus X_{n-1}\\\\\nd(y,0) = (dy, 0) \\\\\nd(0,x') = (-f(x'), dx)$$\nAnd this is exactly the definition of the mapping cone. There are various way to get to this result in a systematic manner. For example you can put what is called a model structure on the category of chain complexes, and then the mapping cone of $f$ becomes its homotopy cokernel. Or you can put a triangulated structure on it (though that's a bit circular, since you need to know what the mapping cone is to get the triangulated structure).\nPS: A lot of things that are true in algebraic topology are also true in homological algebra. For example, if you have $A \\subset X$, you can consider the cone on $A$ to get $X \\cup CA$, then you can cone $X$ inside it to get $(X \\cup CA) \\cup CX$, and this is homotopy equivalent to the suspension $\\Sigma A$ (the beginning of the Puppe sequence). Well, in homological algebra it's exactly the same: say you have a subcomplex $i : A_* \\to X_*$, you can take the cone $\\operatorname{Cone}(i)$, of which $X_*$ is a subcomplex; if you then take the cone of this inclusion, you get a complex homotopy equivalent to the suspension (shift in degree) of $A_*$. This is because all this can be encoded in the triangulated structure of chain complexes!\n$$ $$", "meta": {"post_id": 1667399, "input_score": 16, "output_score": 35, "post_title": "Motivation for the mapping cone complexes"}}
{"input": "I am familiar with the mechanism of proof by contradiction: we want to prove $P$, so we assume $\u00acP$ and prove that this is false; hence $P$ must be true.\nI have the following devil's advocate question, which might seem to be more philosophy than mathematics, but I would prefer answers from a mathematician's point of view:\nWhen we prove that $\u00acP$ is \"false\", what we are really showing is that it is inconsistent with our underlying set of axioms. Could there ever be a case were, for some $P$ and some set of axioms, $P$ and $\u00acP$ are both inconsistent with those axioms (or both consistent, for that matter)?", "output": "The situation you ask about, where $P$ is inconsistent with our axioms and $\\neg P$ is also inconsistent with our axioms, would mean that the axioms themselves are inconsistent.  Specifically, the inconsistency of $P$ with the axioms would mean that $\\neg P$ is provable from those axioms.  If, in addition, $\\neg P$ is inconsistent with the axioms, then the axioms themselves are inconsistent --- they imply $\\neg P$ and then they contradict that.  (I have phrased this answer so that it remains correct even if the underlying logic of the axiom system is intuitionistic rather than classical.)", "meta": {"post_id": 1719503, "input_score": 40, "output_score": 52, "post_title": "Can proof by contradiction 'fail'?"}}
{"input": "So I've been looking for a general name of this type of mathematics notation (google hasn't been very useful) so that I can learn more about it. Basically, the symbols are in the form of functions and the numbers are the inputs to the function.  + is the function taking 4 and 5, outputting 9 so, (+4 5) = 9. Formulaic  (* a b ) = a * b.  Where ' * ' can be any basic math symbol.  More complicated forms are: (+ 4 (+ 5 1)) = 4 + (5+1) = 10. 4 being input for 'a' and (5+1) being 'b' in (* a b ).\nHope it makes sense.", "output": "It is normal Polish notation or prefix notation.", "meta": {"post_id": 1727854, "input_score": 14, "output_score": 42, "post_title": "What is this type of math notation called? (+ 4 5)"}}
{"input": "Until recently, all my knowledge of measure theory and Lebesgue integration are from Rudin's book, which focuses solely on the Lebesgue measure, its construction and nothing else. I have just put my hands on a nice book \"Measure and Integration Theory\" by Heinz Bauer and I'm currently enjoying it. I have encountered the definition of a Dynkin system $\\mathcal D$, which is a family of subsets of a set $\\Omega$ satisfying\n\n1.) $\\Omega\\in\\mathcal D$.\n2.) If $A\\in\\mathcal D$, then $A^c\\in\\mathcal D$.\n3.) For $n\\in\\Bbb N$, if $A_n\\in\\mathcal D$ are pairwise disjoint then $\\bigcup_{n=1}^{\\infty}A_n\\in\\mathcal D$.\n\nI have some idea about what a $\\sigma$-algebra is, but not about a Dynkin system. I would really appreciate if someone could give me an intuition about Dynkin systems or what they're supposed to represent. What is the characteristics of a Dynkin system that let you recognize it once you see it?\nI know the $\\pi$-$\\lambda$ theorem and facts like a Dynkin system $\\mathcal D$ is a $\\sigma$-algebra if it is closed under intersection, it would be also nice if anyone could explain to me why should we expect such a result. Thank you in advance.", "output": "A motivation for Dynkin systems (and specially, the $\\pi$-$\\lambda$ Theorem) is the following problem: \n\nHow many sets should I check to be sure that two probability measures $\\mu$ and $\\nu$ are the same?\n\nFor the sake of definiteness, both $\\mu$ and $\\nu$ are defined on a measurable space $(\\Omega,\\Sigma)$. A trivial answer to the previous question is \u201cIf they coincide over $\\Sigma$, they are the same\u201d. True, and useless. So let's think about this for a moment. If I have checked that $\\mu(A) =\\nu(A)$ for some $A\\in\\Sigma$, it is not necessary to check for the complement, since\n$$\n\\mu(A^c) =1-\\mu(A) = 1-\\nu(A) = \\nu(A^c).\n$$\nIt is also immediate by $\\sigma$-additivity that if they coincide on a sequence of pairwise disjoint sets $A_n$, they must coincide on their union. Hence we conclude\n\nIf two probability measures coincide on a family $\\mathcal{A}\\subseteq\\Sigma$, then they coincide on the Dynkin system generated by $\\mathcal{A}$ (i.e., the smallest $\\lambda$-system containing it).\n\nThe above arguments can't be generalized to intersections; we can't compute $\\mu(A\\cap B)$ from $\\mu(A)$ and $\\mu(B)$. So, it would be desirable that our initial data (sets checked for coincidence) is a family of sets closed under binary intersections. Here, the $\\pi$-$\\lambda$ confirms this intuition: If $\\Sigma$ is generated by a family $\\mathcal{A}$, then it equals the smallest Dynkin system including the closure of $\\mathcal{A}$ under intersections (i.e., the $\\pi$-system generated by it). \nTherefore, to check that $\\mu$ and $\\nu$ are equal, it is enough to take a family $\\mathcal{A}$ such that of $\\Sigma =\\sigma(\\mathcal{A})$ and check that the measures coincide on every finite intersection of members of $\\mathcal{A}$.", "meta": {"post_id": 1769131, "input_score": 18, "output_score": 37, "post_title": "What is a Dynkin system? ($\\lambda$-system)"}}
{"input": "The size of the set of functions that map $\\mathbb{R}\\to \\mathbb{R}$ equals $(\\#\\mathbb{R})^{\\#\\mathbb{R}}$. How many non-differentiable functions are there in this set?", "output": "Here is a simple way to get the answer:\nSuppose a function $f:\\mathbb R \\to \\mathbb R$ is equal to the function $g$ on $\\mathbb Q$, where $g:\\mathbb Q \\to \\mathbb Q$ is defined by $g(p/q) = q$ (and we choose the representation $p/q$ so that $q$ is the smallest possible positive integer). Then $f$ is nowhere differentiable, because it is unbounded on every interval.\nAnd the number of such $f$ is $|\\mathbb R^{\\mathbb R \\setminus \\mathbb Q}| = |\\mathbb R^{\\mathbb R}|$, because $|\\mathbb R \\setminus \\mathbb Q| = |\\mathbb R|$.\nHence there as many nowhere-differentiable functions $\\mathbb R \\to \\mathbb R$ as there are functions $\\mathbb R \\to \\mathbb R$.\n(This doesn't tell you how many differentiable functions there are. The number of somewhere-differentiable functions is the same as the set of all functions; but the number of everywhere-differentiable functions is $|\\mathbb R^\\mathbb Q| = |\\mathbb R|$, because such a function is determined by its values on the rationals.)", "meta": {"post_id": 1777344, "input_score": 18, "output_score": 49, "post_title": "How many non-differentiable functions exist?"}}
{"input": "Sheaf cohomology was first introduced into algebraic geometry by Serre. He used \u010cech cohomology to define sheaf cohomology. Grothendieck then later gave a more abstract definition of the right derived functor of the global section functor.\nWhat I still don't understand what was the actual motivation for defining sheaf cohomology. What was the actual problem they were trying to solve?", "output": "Sheaves and sheaf cohomology were invented not by Serre, but by Jean Leray while he was a World War II  prisoner in Oflag XVII (Offizierlager=Officer Camp) in Austria.\nAfter the war he published his results in 1945 in the Journal de Liouville.\nHis remarkable but rather obscure results were clarified by Borel, Henri Cartan, Koszul, Serre and Weil in the late 1940's and early 1950's.    \nThe first spectacular application of Leray's new ideas was Weil's proof of De Rham's theorem: he computed the  cohomology of the constant sheaf $\\underline {\\mathbb R}$ on a manifold $M$ through its resolution by the acyclic complex of differential forms $\\Omega_M^*$ on $M$.     \nThe next success story for sheaves and their cohomology was the proof by Cartan and Serre of theorems $A$ and $B$ for Stein manifolds, which solved a whole series of difficult problems (like Cousin I and Cousin II) with the help of techniques and theorems of Oka, who can be said a posteriori to have implicitly introduced sheaves in complex analysis.\nThe German complex analysts (Behnke, Stein, Thullen,...) who had up to then be the masters of the field  were so impressed by the new cohomology techniques that they are reported to have exclaimed: \"the French have tanks and we have bows and arrows!\"     \nArmed with his deep knowledge of these weapons of complex analysis Serre took the incredibly bold step of introducing  sheaves and their cohomology on algebraic varieties endowed with their Zariski topology.\nThis was of remarkable audacity because of the coarseness of Zariski topology, which had led specialists to believe that it was  just some rather unimpressive tool allowing one for example to talk rigorously of generic properties.\nAs all algebraic geometers now know, Serre stunned his colleagues  by showing in FAC how cohomological methods yielded deep results, at the centre of which are theorems $A$ and $B$ for coherent sheaves on affine varieties.\nOther fundamental novelties obtained by Serre in FAC are his twisting sheaves $\\mathcal O(n)$, the computation of the cohomology of coherent sheaves on projective space, the vanishing of the cohomology groups $H^q(V,\\mathcal F(n))$ on a projective variety $V$ for $q\\gt0, n\\gt\\gt 0$,...     \nLast not least: the introduction of sheaves and their cohomology in FAC paved the way for Grothendieck's revolutionary introduction of schemes in algebraic geometry, as acknowledged in the preface of EGA.\nActually reading FAC was the secondary thesis of Grothendieck, accompanying his PhD on nuclear spaces in functional analysis.\nAfer his PhD defence, someone (Cartan if I remember the anecdote correctly) told him good-humouredly that he seemed not to have understood much in FAC.\nThe story goes that Grothendieck was piqued, invested much energy in understanding Serre, and the rest is history. Se non \u00e8 vero, \u00e8 ben trovato...", "meta": {"post_id": 1798667, "input_score": 43, "output_score": 59, "post_title": "Why was Sheaf cohomology invented?"}}
{"input": "Let $Q=\\mathbb Q \\cap(0,1)= \\{r_1,r_2,\\ldots\\}$ be the rational numbers in $(0,1)$ listed out so we can count them. Define $x_n=\\frac{1}{n}\\sum_{k=1}^nr_n$ to be the average of the first $n$ rational numbers from the list.\nQuestions:\n\nWhat is required for $x_n$ to converge? Certainly $0< x_n < 1$ for all $n$.\nDoes $x_n$ converge to a rational or irrational?\nHow does the behavior of the sequence depend on the choice of list? I.e. what if we rearrange the list $\\mathbb Q \\cap(0,1)=\\{r_{p(1)},r_{p(2)},\\ldots\\}$ with some one-to-one permutation $p: \\mathbb N \\to \\mathbb N$? How does the behavior of $x_n$ depend on $p$?\n\n\nMy thoughts:\nIntuitively, I feel that we might be able to choose a $p$ so that $x_n\\rightarrow y$ for any $y\\in[0,1]$. However, it also makes intuitive sense that, if each rational appears only once in the list, that the limit is required to be $\\frac{1}{2}.$ Of course, intuition can be very misleading with infinities!\nIf we are allowed to repeat rational numbers with arbitrary frequency (but still capturing every rational eventually), then we might be able to choose a listing so that $x_n\\rightarrow y$ for any $y\\in(0,\\infty)$. \nThis last point might be proved by the fact that every positive real number has a sequence of positive rationals converging to it, and every rational in that list can be expressed as a sum of positive rationals less than one. However, the averaging may complicate that idea, and I'll have to think about it more.\n\nExample I:\nNo repetition:\n$$Q=\\bigcup_{n=1}^\\infty \\bigcup_{k=1}^n \\left\\{\\frac{k}{n+1}\\right\\} =\\left\\{\\frac{1}{2},\\frac{1}{3},\\frac{2}{3},\\frac{1}{4},\\frac{3}{4},\\frac{1}{5},\\ldots\\right\\}$$\nin which case $x_n\\rightarrow\\frac{1}{2},$ a very nice and simple example. Even if we keep the non-reduced fractions and allow repetition, i.e. with\n$Q=\\{\\frac{1}{2},\\frac{1}{3},\\frac{2}{3},\\frac{1}{4},\\boxed{\\frac{2}{4},}\\frac{3}{4},\\frac{1}{5},\\ldots\\},$\nthen $x_n\\rightarrow\\frac{1}{2}.$ The latter case is easy to prove since we have the subsequence $x_{n_k}=\\frac{1}{2}$ for $n_k=\\frac{k(k+1)}{2},$ and the deviations from $1/2$ decrease. The non-repetition case, I haven't proved, but simulated numerically, so there may be an error, but I figure there is an easy calculation to show whether it's correct.\n\nExample II:\nConsider the list generated from the Stern-Brocot tree:\n$$Q=\\left\\{\\frac{1}{2},\\frac{1}{3},\\frac{2}{3},\\frac{1}{4},\\frac{2}{5},\\frac{3}{5},\\frac{3}{4},\\ldots\\right\\}.$$\nI'm sure this list could be studied analytically, but for now, I've just done a numerical simulation. The sequence of averages $x_n$ hits $\\frac{1}{2}$ infinitely often, but may be oscillatory and hence not converge. If it converges, it does so much slower than the previous examples. It appears that $x_{2^k-1}=0.5$ for all $k$ and that between those values it comes very close to $0.44,$ e.g. $x_{95743}\\approx 0.4399.$ However, my computer code is probably not very efficient, and becomes very slow past this.", "output": "Depending on how you order the rationals to begin with, the sequence $x_n$ could tend to anything in $[0,1]$ or could diverge.\nSay $y\\in[0,1]$. Start with an enumeration $r_1,\\dots$ of the rationals in $(0,1)$. When I say \"choose a rational such that [whatever]\" I mean you should choose the first rational currently on that list that satisfies [whatever], and then cross it off the list.\nStart by choosing $10$ rationals in $I_1=(y-1/10,y+1/10)$. Then choose one rational in $[0,1]\\setminus I_1$. Then choose $100$ rationals in $I_2=(y-1/100,y+1/100)$, and then choose one rational in $[0,1]\\setminus I_2$. Etc.\nFirst, note we have in fact defined a reordering of the original list. No rational appears in the new ordering more than once, because it is crossed off the original list the first time it is chosen. And every rational appears on the new list. In fact you can show by induction on $n$ that $r_n$ must be chosen at some stage: By induction you can assume that every $r_j$ for $j<n$ is chosen at some stage. So at some stage $r_n$ is the first remaining entry on the original list; hence it will be chosen soon, since either it's in $I_k$ or not.\nAnd for large $n$ the vast majority of the rationals in the first $n$ elements of the new ordering are very close to $y$, hence $x_n\\to y$.\n(Similarly, to get $x_n$ to diverge: Start with a large number of rationals near $0$. Follow with a huge number of rationals near $1$, then a stupendous number of rationals near $0$...)", "meta": {"post_id": 1803446, "input_score": 66, "output_score": 57, "post_title": "What is the average rational number?"}}
{"input": "Can path connectedness be defined without using the unit interval or more generally the real numbers?\nI.e., do we need Dedekind cuts or Cauchy convergence equivalence classes of the rational numbers (metric space completion) in order to define any object topologically equivalent to the unit interval?\n\nCompared to the definition of connectedness, which only uses open and closed sets, having to use the unit interval to define path connectedness seems somewhat like using a sledgehammer.\nI suspect that the answer might be no, since for every Hausdorff path connected space, the paths are homeomorphic to the unit interval (at least according to the relevant Wikipedia article). In particular, every locally path-connected Hausdorff space has a bunch of 1-manifolds as subsets.\nStill it is unclear to me, since it seems like it should be able to specify all of the unit interval's topological properties without having to recourse to its analytic definition.\nYour thoughts or help would both be greatly appreciated.\nEDIT: this question probably has something to do with homotopy theory: https://en.wikipedia.org/wiki/Homotopy, with which I am rudimentarily familiar at best.", "output": "There are really two separate questions here: can you define the unit interval space without talking about real numbers, and can you define path-connectedness without talking about the unit interval space?  The answer to both is yes; let me address the second question first.\nLet $P$ be a topological space and let $a,b\\in P$ be two points.  Say that a space $X$ is $(P,a,b)$-connected if for any $x,y\\in X$, there is a continuous map $f:P\\to X$ such that $f(a)=x$ and $f(b)=y$.  Of course, for $(P,a,b)=([0,1],0,1)$, this is just the usual definition of path-connectedness.\nHowever, there is a more \"universal\" characterization of path-connectedness that doesn't require you to know about the space $[0,1]$.  Namely, a space $X$ is path-connected iff it is $(P,a,b)$-connected for all compact Hausdorff spaces $P$ with two distinct points $a,b\\in P$.\nTo prove this, suppose $X$ is path-connected, $P$ is a compact Hausdorff space, $x,y\\in X$, and $a,b\\in P$ are distinct.  Since $X$ is path-connected, there is a path $g:[0,1]\\to X$ such that $g(0)=x$ and $g(1)=y$.  By Urysohn's lemma, there is a continuous map $h:P\\to [0,1]$ such that $h(a)=0$ and $h(b)=1$.  The composition $gh:P\\to X$ is then continuous and satisfies $g(a)=x$ and $g(b)=y$.\nThe idea here is that you could use any space $P$ with two chosen points $a$ and $b$ to define a notion of \"paths\" in a space.  However, if you restrict to compact Hausdorff spaces $P$, then the ordinary interval $[0,1]$ is the \"strongest possible kind of path\" you can have in a space: if you have a $[0,1]$-path between two points, then you have a $P$-path for every other compact Hausdorff space $P$ as well.\n(Of course, all we used about compact Hausdorff is that we know there is a map $P\\to [0,1]$ separating $a$ and $b$.  However, I phrased everything in terms of the compact Hausdorff condition since this is a natural condition you can define without already knowing about the space $[0,1]$.)\n\nOK, now let me say a little about the first question.  There are in fact many different ways to uniquely characterize the space $[0,1]$ up to homeomorphism without reference to the reals or anything that is essentially equivalent to constructing the reals.  In fact, you can deduce one from the answer I gave to the second question above.\nNamely, say that a compact Hausdorff space $(P,a,b)$ equipped with two distinct points is a universal path if it has the special property of $[0,1]$ noted above: whenever there is a $(P,a,b)$-path between two points $x$ and $y$ of an arbitrary space, there is also a $(Q,c,d)$-path from $x$ to $y$ for any compact Hausdorff space $Q$ with two distinct points.  Say that a universal path $(P,a,b)$ is minimal if for any other universal path $(Q,c,d)$, there is an embedding $P\\to Q$ sending $a$ to $c$ and $b$ to $d$.\nI now claim that $([0,1],0,1)$ is the unique minimal universal path (up to homeomorphism).  We know it is universal.  To show that it is minimal, let $(Q,c,d)$ be any universal path.  Since the identity map $Q\\to Q$ is a $(Q,c,d)$-path from $c$ to $d$ in $Q$, universality implies there is a $([0,1],0,1)$-path from $c$ to $d$ in $Q$.  But if there is a path between two points of a Hausdorff space, there is also a path which is an embedding (see Does path-connected imply simple path-connected?).  Thus there is an embedding $[0,1]\\to Q$ sending $0$ to $c$ and $1$ to $d$.\nNow suppose $(P,a,b)$ is any minimal universal path.  The previous paragraph shows that there is a $([0,1],0,1)$-path from $a$ to $b$ in $P$.  Now since $([0,1],0,1)$ is a universal path, minimality of $P$ says that $P$ embeds in $[0,1]$ sending $a$ to $0$ and $b$ to $1$.  But since $P$ contains a path from $a$ to $b$, the image of this embedding contains a path from $0$ to $1$, and thus contains all of $[0,1]$.  Thus the embedding is actually a homeomorphism $P\\to [0,1]$.\nAs I mentioned, this is just one of many ways of characterizing $[0,1]$.  For another characterization that also relates closely to the intuitive notion of \"paths\", see this answer by Tom Leinster on MO.", "meta": {"post_id": 1823987, "input_score": 36, "output_score": 63, "post_title": "Can path connectedness be defined without using the unit interval?"}}
{"input": "Determine whether each of these pairs of sets are equal$$A = \\{\\{1\\}\\} \\qquad \\qquad B = \\{1, \\{1\\}\\}$$\n\nI believe $A$ is equal to $B$ because all elements in $A$ are in $B$, but the answer says that it's not.", "output": "Think of $A$ as a bag which contains within it another smaller bag with a one in it.\n$A=\\underbrace{\\{~~~~~~~\\overbrace{\\{1\\}}^{\\text{second bag}}~~~~~~~~\\}}_{\\text{first bag}}$\nOn the other hand, $B$ is a bag which contains in it not only a second bag with a one in it, but also a one which is loose.\n$B=\\underbrace{\\{~~~~~~~~\\overbrace{\\{1\\}}^{\\text{second bag}}~~~~~\\overbrace{1}^{\\text{this too}}~~~~~~~\\}}_{\\text{first bag}}$\n$1\\in B$ but $1\\not\\in A$.  There is no \"loose 1\" in $A$, there is only a bag with a one in it in $A$.\nThus, $A\\neq B$", "meta": {"post_id": 1860874, "input_score": 6, "output_score": 47, "post_title": "Why is $\\{\\{1\\}\\}$ not equal to $\\{1,\\{1\\}\\}$?"}}
{"input": "So here is what I understand:\n\nIf $f(x)$ is increasing/decreasing, then its derivative $f'(x)$ is positive/negative\n\nand...\n\nIf $f(x)$ is increasing/decreasing, then the derivative of $f'(x)$ (which is $f''(x)$) is concave up/concave down\n\nSo my question is: if a graph has a vertical asymptote, the derivative must also have a vertical asymptote, too, right? Does it also work vice versa? I feel like there is a trick to it, but I'm not sure.\nI have a graph from GeoGebra here. The dotted line is the derivative.", "output": "if a graph has vertical asymptote, the derivative must also have a vertical asymptote too, right? \n\nNo. A counterexample: $$f(x)=\\frac{1}{x}+\\sin\\left(\\frac{1}{x}\\right)$$ This function is monotone and has a vertical asymptote at $x=0$. But its derivative has no limit.", "meta": {"post_id": 1863341, "input_score": 26, "output_score": 50, "post_title": "If $f(x)$ has a vertical asymptote, does $f'(x)$ have one too?"}}
{"input": "Recently, I was looking for the reviews of some Analysis books while encountered terms such as Baby/Papa/Mama/Big Rudin. Firstly, I thought that these are the names of a book! But it turned out that these are some nick names used for the books of Walter Rudin. So I was thinking that\n$1$. What are the corresponding books of these nick names?\n$2$. Why such nick names are chosen? or What are their origins?", "output": "In order to sum up the above comments, the corresponding books for these nick names are\n$1$. Baby = Principles of Mathematical Analysis; \n$2$. Papa/Big = Real and Complex Analysis; \n$3$. Grandpa = Functional Analysis;\nand it seems that the difficulty of contents of the books grows with the age of the nick names! Firstly, you are a baby and things are easy to handle. Then you grow up and become a papa and things get more complicated. Finally, when you are a grandpa you should take care of your legacy very carefully which needs a hardwork! So $1$ is a prerequisite of $2$ and $2$ is prerequisite of $3$.", "meta": {"post_id": 1863512, "input_score": 58, "output_score": 58, "post_title": "Baby/Papa/Mama/Big Rudin"}}
{"input": "I've always assumed on faulty intuition that if you have an event which occurs 1 in n chances, it will be super likely to happen at some point of that event occuring n times.  However, given some analysis, it doesn't actually seem to be all that super likely, and seems to converge at a particular value as the value of n rises.  That value is about 0.63212.\nIs this correct?  If so, is there a name for this value and is it considered significant within the field of probability?\nBelow is the Python code that I used to arrive at this value.\n>>> def p(x, r):\n...   return x + r * (1.0 - x)\n\n>>> def p_of_1(r):\n...   x = r\n...   while True:\n...     yield x\n...     x = p(x, r)\n\n>>> def p_of_n(n):\n...   g = p_of_1(1.0 / n)\n...   return [next(g) for x in range(n)]\n...\n\n>>> p_of_n(1)\n[1.0]\n>>> p_of_n(2)\n[0.5, 0.75]\n>>> p_of_n(3)\n[0.3333333333333333, 0.5555555555555556, 0.7037037037037037]\n>>> p_of_n(4)\n[0.25, 0.4375, 0.578125, 0.68359375]\n>>> p_of_n(5)\n[0.2, 0.36000000000000004, 0.488, 0.5904, 0.67232]\n\n>>> p_of_n(6)[-1]\n0.6651020233196159\n>>> p_of_n(10)[-1]\n0.6513215599000001\n>>> p_of_n(100)[-1]\n0.6339676587267709\n>>> p_of_n(10000)[-1]\n0.6321389535670703\n>>> p_of_n(10000000)[-1]\n0.6321205772225762", "output": "It's easier to work backwards.  The probability that the event does not occur on a single try is, of course, $1-\\frac 1n$.  It follows that the probability that it fails to occur in $n$ trials is $p_n=\\left(1-\\frac 1n\\right)^n$.  Therefore the probability that it occurs at least once in those $n$ trials  is $$1-p_n=1-\\left(1-\\frac 1n\\right)^n$$  If we now recall the limit definition of the exponential:  $$e^a=\\lim_{n\\to \\infty}\\left(1+\\frac an\\right)^n$$  We see that, for large $n$, $$1-p_n\\sim 1-\\frac 1e=0.632120559\\dots$$", "meta": {"post_id": 1898912, "input_score": 17, "output_score": 36, "post_title": "Why is $0.63212$ the probability of a $\\frac1n$-probability event happening in $n$ trials?"}}
{"input": "Let $G$ be the Cantor set. It is well known that:\n\n$G$ is perfect and hence closed.\n$G$ has the cardinality of the continuum.\n$G$ has measure zero.\nFor any set $S \\subset \\mathbb{R}$ (I will not keep writing that we are in $\\mathbb{R}$) we have $S \\text{ is closed} \\Leftrightarrow S^c \\text{ is open}$.\nAny open set $O$ can be written as a --- in fact unique --- countable union of disjoint open intervals.\n\n$G^c$ can thus be written as a countable union of disjoint open intervals. We now imagine this union as being superposed on the real line graphically as follows:\nR: <<<----(....)---(..)--(.)---------(...)--->>>\n\nwhere the (...) represents the open disjoint intervals (of differing size) composing $G^c$, and the ---  represents the remaining non-covered real numbers (that is those in $G$). Now we can cover $\\mathbb{R}$ in its entirety by \"collecting\" the --- into disjoint closed intervals. Any one of these closed intervals might of course consist of only a single element. We now obtain:\nR: <<<[--](....)[-](..)[](.)[-------](...)[-]>>>\n\nTake the union of these disjoint closed intervals. This must be $(G^c)^c = G$. Now it is not hard to imagine a mapping from the (...)'s to the [...]'s. Just take the next (...) in line for each [...] (and do some trivial fixing at the ends). Therefore we have written $G$ as a countable union of disjoint closed sets. However, $G$ has measure zero and therefore cannot contain any closed sets other than the single element type. Hence $G$ is countable. Contradiction.\nWhere do I go wrong?", "output": "You're imagining that the open intervals of $G^c$ are ordered discretely, like the integers, so you have alternating open intervals in $G^c$ and closed intervals in $G$.  But actually, the open intervals of $G^c$ are densely ordered, and order-isomorphic to the rationals.  As a result, there is no \"next (...) in line for each [...]\" like you claim there is.  There are uncountably many closed intervals (actually, all of them are just single points) in between these open intervals, much like how there are uncountably many irrational numbers in between the rational numbers.  There is no \"next rational number\" after each irrational number that you can use to get a bijection between rationals and irrationals, and the same thing is happening here.", "meta": {"post_id": 1928500, "input_score": 17, "output_score": 36, "post_title": "Real analysis contradiction I cannot get rid of"}}
{"input": "Definitions of positive definiteness usually look like this:\n\nA symmetric matrix $M$ is positive definite if $x^T M x > 0$ for all vectors $x \\neq 0$.\n\nWhy must $M$ be symmetric? The definition seems to make sense for general square matrices.", "output": "Let quadratic form $f$ be defined by\n$$f (\\mathrm x) := \\mathrm x^\\top \\mathrm A \\,\\mathrm x$$\nwhere $\\mathrm A \\in \\mathbb{R}^{n \\times n}$. Since $\\mathrm x^\\top \\mathrm A \\,\\mathrm x$ is a scalar, then $\\left(\\mathrm x^\\top \\mathrm A \\,\\mathrm x\\right)^\\top = \\mathrm x^\\top \\mathrm A \\,\\mathrm x$, i.e., $\\mathrm x^\\top \\mathrm A^\\top \\mathrm x = \\mathrm x^\\top \\mathrm A \\,\\mathrm x$. Hence,\n$$\\mathrm x^\\top \\left(\\frac{\\mathrm A - \\mathrm A^\\top}{2}\\right) \\mathrm x = 0$$\nThus, the skew-symmetric part of matrix $\\mathrm A$ does not contribute anything to the quadratic form. What is left is, then, the symmetric part\n$$\\frac{\\mathrm A + \\mathrm A^\\top}{2}$$\nwhich is diagonalizable and has real eigenvalues and orthogonal eigenvectors, all nice properties.\n\nAddendum\nTaking affine combinations of $\\mathrm A$ and $\\mathrm A^\\top$, we obtain\n$$\\mathrm x^\\top \\left( \\gamma \\mathrm A + (1-\\gamma) \\mathrm A^\\top \\right) \\mathrm x = f (\\mathrm x)$$\nwhich yields $f$ for all $\\gamma \\in \\mathbb{R}$. Choosing $\\gamma = \\frac{1}{2}$, we obtain the symmetric part of $\\mathrm A$.", "meta": {"post_id": 1964039, "input_score": 22, "output_score": 34, "post_title": "Why do positive definite matrices have to be symmetric?"}}
{"input": "Up till now, the only things I was able to come up/prove are the following properties:\n\n$\\prod\\lambda_i = \\pm 1$\n$ 0 \\leq \\sum \\lambda_i \\leq n$, where $n$ is the size of the matrix\neigenvalues of the permutation matrix lie on the unit circle \n\nI am curious whether there exist some other interesting properties.", "output": "A permutation matrix is an orthogonal matrix (orthogonality of column vectors and norm of column vectors = 1).\nAs such, because an orthogonal matrix \"is\" an isometry\n$$\\tag{1}\\|PV\\|=\\|V\\|$$\nIf $V$ is an eigenvector associated with eigenvalue $\\lambda$, substituting $PV=\\lambda V$ in (1) we deduce\n$$|\\lambda|=1.$$\nMoreover, as $P^p=I_n$ ($p$ is the order of the permutation) these eigenvalues are such that $\\lambda^p=1$; therefore\n$$\\lambda=e^{i k 2\\pi/p}$$\nfor some $k \\in \\mathbb{Z}$.\nLet us take an example: consider the following permutation decomposed into the product of two disjoint support cycles\na cycle $\\color{red}{(5 4 3 2 1)}$ of order $5$ and a cycle $\\color{blue}{(6 7 8)}$ of order $3$.\nIts associated matrix is:\n$$\\left(\\begin{array}{ccccc|ccc}\n0 & \\color{red}{1} & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & \\color{red}{1} & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & \\color{red}{1} & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\color{red}{1} & 0 & 0 & 0\\\\\n\\color{red}{1} & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\hline\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\color{blue}{1}\\\\\n0 & 0 & 0 & 0 & 0 & \\color{blue}{1} & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\color{blue}{1} & 0\\end{array}\\right)$$\nIts cycle structure is reflected (see picture) into the five eigenvalues $\\color{red}{e^{2i k\\pi/5}}$ and the three eigenvalues $\\color{blue}{e^{2i k\\pi/3}}$.\nPlease note that eigenvalue $1$ is - in a natural way - a double eigenvalue, and more generally with multiplicity $m$ if the permutation can be decomposed into $m$ disjoint cycles.", "meta": {"post_id": 1970702, "input_score": 23, "output_score": 39, "post_title": "What are the properties of eigenvalues of permutation matrices?"}}
{"input": "Show that a nonabelian group must have at least five distinct elements.\n\nI just learn abstract algebra by self study. I want help to solve this problem.\nJust give me a hint.", "output": "You need an instance of $ab\\ne ba$. That requires $a\\ne b$. Also $a\\ne 1$ and $b\\ne 1$ as $1$ commutes. Also, $a,b$ are not inverse of each other as those commute. Hence $1, a, b, ab, ba$ are pairwise distinct", "meta": {"post_id": 1971166, "input_score": 14, "output_score": 45, "post_title": "Show that a nonabelian group must have at least five distinct elements"}}
{"input": "How to prove that every polynomial with real coefficients is the sum of three polynomials raised to the 3rd degree? Formally the statement is:\n\n$\\forall f\\in\\mathbb{R}[x]\\quad \\exists g,h,p\\in\\mathbb{R}[x]\\quad f=g^3+h^3+p^3$", "output": "We have that the following identity holds\n$$(x+1)^3+2(-x)^3+(x-1)^3=6x.$$\nHence\n$$\\left(\\frac{f(x)+1}{6^{1/3}}\\right)^{3}+\\left(\\frac{-f(x)}{3^{1/3}}\\right)^{3}+\n\\left(\\frac{f(x)-1}{6^{1/3}}\\right)^{3}=f(x).$$", "meta": {"post_id": 1991228, "input_score": 46, "output_score": 81, "post_title": "Every polynomial with real coefficients is the sum of cubes of three polynomials"}}
{"input": "I understand that\n$Var(X) = E(X^2) - E(X)^2 $\nAnd that the second moment, variance, is\n$E(X^2)$\nHow is variance simultaneously $E(X^2)$ and $E(X^2) - E(X)^2$?", "output": "$$\n\\mathbb{E}(X^n) = \\text{raw moment}\\\\\n\\mathbb{E}\\left[\\left(X-\\mathbb{E}(X)\\right)^n\\right] = \\text{central moment}\n$$\nwhere the 2nd central moments represents the variance.\nonly equal when $\\mathbb{E}(X) = 0$ as with $\\mathcal{N}(0,1)$.", "meta": {"post_id": 2030437, "input_score": 24, "output_score": 37, "post_title": "Difference between Variance and 2nd moment"}}
{"input": "How to prove\n\n$~~ \\forall n\\in\\mathbb{N}^+$,\n\\begin{align}I_n=\\int_0^1(1+x+x^2+x^3+\\cdot\\cdot\\cdot+x^{n-1})^2 (1+4x+7x^2+\\cdot\\cdot\\cdot+(3n-2)x^{n-1})~dx=n^3.\\end{align}\n\n\nMy Try:\nDefine $\\displaystyle S(n)=\\sum_{k=0}^{n-1}x^k=1+x+x^2+x^3+\\cdot\\cdot\\cdot+x^{n-1}=\\frac{x^n-1}{x-1}$. Then,\n\\begin{align}\\frac{d}{dx}S(n)=S'(n)=1+2x+3x^2+\\cdot\\cdot\\cdot(n-1)x^{n-2}=\\sum_{k=0}^{n-1}kx^{k-1}.\\end{align}\nTherefore,\n\\begin{align}\nI_n&=\\int_0^1 S^2(n)\\left(3S'(n+1)-2S(n)\\right)~dx\\\\\n&=3\\int_0^1 S^2(n)S'(n+1)~dx-2\\int_0^1 S^3(n)~dx\\\\\n&=3\\int_0^1 S^2(n)(S'(n)+nx^{n-1})~dx-2\\int_0^1 S^3(n)~dx\\\\\n&=3\\int_0^1 S^2(n)~d(S(n))+3\\int_0^1 S^2(n)(nx^{n-1})~dx-2\\int_0^1 S^3(n)~dx\\\\ &=n^3-1+\\int_0^1 S^2(n)(3nx^{n-1}-2S(n))~dx\\\\\n&=n^3-1+\\int_0^1 \\left(\\frac{x^n-1}{x-1}\\right)^2\\left(3nx^{n-1}-2\\cdot\\frac{x^n-1}{x-1}\\right)~dx\n\\end{align}\nSo the question becomes:\n\nProve \\begin{align}I'=\\int_0^1 \\left(\\frac{x^n-1}{x-1}\\right)^2\\left(3nx^{n-1}-2\\cdot\\frac{x^n-1}{x-1}\\right)~dx=1.\\end{align}\n\n\\begin{align}I'&=\\int_0^1 \\frac{3nx^{n-1}(x^n-1)^2}{(x-1)^2}-\\frac{2(x^n-1)^3}{(x-1)^3}~dx\\\\\n&=\\int_0^1 \\frac{(x-1)^2\\left(\\frac d {dx} (x^n-1)^3\\right)-2(x^n-1)^3(x-1)}{(x-1)^4}~dx\\\\\n&=\\int_0^1 \\frac d {dx} \\left(\\frac{(x^n-1)^3}{(x-1)^2}\\right)~dx\\\\\n&=\\lim_{x \\to 1} \\frac{(x^n-1)^3}{(x-1)^2}-\\frac{(0^n-1)^3}{(0-1)^2}\\\\\n\\end{align}\n$$\\therefore I'=1.$$\n\n\\begin{align}\\therefore I_n=n^3.\\end{align}\n\nThere MUST be other BETTER ways evaluating $I_n$.\nCould anyone give me some better solutions? Thanks.", "output": "First apply the substitution $x = t^3$. Then\n\\begin{align*}\nI_n\n&= \\int_{0}^{1} (1 + t^3 + \\cdots + t^{3n-3})^2 (1 + 4t^3 + \\cdots + (3n-2)t^{3n-3}) \\cdot 3t^2 \\, dt \\\\\n&= \\int_{0}^{1} 3 (t + t^4 + \\cdots + t^{3n-2})^2 (1 + 4t^3 + \\cdots + (3n-2)t^{3n-3}) \\, dt.\n\\end{align*}\nNow let $u = u(t) = t + t^4 + \\cdots + t^{3n-2}$. Then\n$$ 3 (t + t^4 + \\cdots + t^{3n-2})^2 (1 + 4t^3 + \\cdots + (3n-2)t^{3n-3}) = 3u^2 \\frac{du}{dt}.$$\nTherefore\n$$ I_n = \\left[ u(t)^3 \\right]_{t=0}^{t=1} = u(1)^3 - u(0)^3 = n^3. $$", "meta": {"post_id": 2042986, "input_score": 21, "output_score": 49, "post_title": "An interesting definite integral $\\int_0^1(1+x+x^2+x^3+\\cdot\\cdot\\cdot+x^{n-1})^2 (1+4x+7x^2+\\cdot\\cdot\\cdot+(3n-2)x^{n-1})~dx=n^3$"}}
{"input": "What is the difference between the Taylor and the Maclaurin series? Is the series representing sine the same both ways? Can someone describe an example for both?", "output": "A Taylor series centered at $x=x_0$ is given as follows:\n$$f(x)=\\sum_{n=0}^\\infty\\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n$$\nwhile a Maclaurin series is the special case of being centered at $x=0$:\n$$f(x)=\\sum_{n=0}^\\infty\\frac{f^{(n)}(0)}{n!}x^n$$\nYou may find this very similar to a power series, which is of the form\n$$f(x)=\\sum_{n=0}^\\infty a_n(x-x_0)^n$$\nParticularly where $a_n=\\frac{f^{(n)}(x_0)}{n!}$.  If a function is equal to it's Taylor series locally, it is said to be an analytic function, and it has a lot of interesting properties.  However, not all functions are equal to their Taylor series, if a Taylor series exists.\nOne may note that most of the most famous Taylor series are a Maclaurin series, probably since they look nicer.  For example,\n$$\\sin(x)=\\sum_{n=0}^\\infty\\frac{(-1)^nx^{2n+1}}{(2n+1)!}$$\nor,\n$$\\sin(x)=\\sum_{n=0}^\\infty\\frac{(-1)^n(x-2\\pi)^{2n+1}}{(2n+1)!}$$\nWhich is trivially due to the fact that $\\sin$ is a periodic function.  So, if you had to choose, you'd probably choose the first representation.  Just a convention.\nThe geometric series is a rather beautifully known Maclaurin series, which one may derive algebraically without taking derivatives:\n$$\\frac1{1-x}=\\sum_{n=0}^\\infty x^n=1+x+x^2+x^3+\\dots$$\nHowever, it gets a little bit more involved when you try to take the Taylor series at a different point.", "meta": {"post_id": 2121695, "input_score": 30, "output_score": 39, "post_title": "What is the difference between the Taylor and Maclaurin series?"}}
{"input": "Let's define a sequence of numbers between 0 and 1. The first term, $r_1$ will be chosen uniformly randomly from $(0, 1)$, but now we iterate this process choosing $r_2$ from $(0, r_1)$, and so on, so $r_3\\in(0, r_2)$, $r_4\\in(0, r_3)$... The set of all possible sequences generated this way contains the sequence of the reciprocals of all natural numbers, which sum diverges; but it also contains all geometric sequences in which all terms are less than 1, and they all have convergent sums. The question is: does $\\sum_{n=1}^{\\infty} r_n$ converge in general? (I think this is called almost sure convergence?) If so, what is the distribution of the limits of all convergent series from this family?", "output": "The probability $f(x)$ that the result is $\\in(x,x+dx)$ is given by $$f(x) = \\exp(-\\gamma)\\rho(x)$$ where $\\rho$ is the Dickman function as @Hurkyl pointed out below. This follows from the the delay differential equation for $f$, $$f^\\prime(x) = -\\frac{f(x-1)}{x}$$ with the conditions $$f(x) = f(1) \\;\\rm{for}\\; 0\\le x \\le1 \\;\\rm{and}$$ $$\\int\\limits_0^\\infty f(x) = 1.$$ Derivation follows\n\nFrom the other answers, it looks like the probability is flat for the results less than 1. Let us prove this first.\nDefine $P(x,y)$ to be the probability that the final result lies in $(x,x+dx)$ if the first random number is chosen from the range $[0,y]$. What we want to find is $f(x) = P(x,1)$.\nNote that if the random range is changed to $[0,ay]$ the probability distribution gets stretched horizontally by $a$ (which means it has to compress vertically by $a$ as well). Hence $$P(x,y) = aP(ax,ay).$$\nWe will use this to find $f(x)$ for $x<1$.\nNote that if the first number chosen is greater than x we can never get a sum less than or equal to x. Hence $f(x)$ is equal to the probability that the first number chosen is less than or equal to $x$ multiplied by the probability for the random range $[0,x]$. That is, $$f(x) = P(x,1) = p(r_1<x)P(x,x)$$\nBut $p(r_1<x)$ is just $x$ and $P(x,x) = \\frac{1}{x}P(1,1)$ as found above. Hence $$f(x) = f(1).$$\nThe probability that the result is $x$ is constant for $x<1$.\nUsing this, we can now iteratively build up the probabilities for $x>1$ in terms of $f(1)$.\nFirst, note that when $x>1$ we have $$f(x) = P(x,1) = \\int\\limits_0^1 P(x-z,z) dz$$\nWe apply the compression again to obtain $$f(x) = \\int\\limits_0^1 \\frac{1}{z} f(\\frac{x}{z}-1) dz$$\nSetting $\\frac{x}{z}-1=t$, we get $$f(x) = \\int\\limits_{x-1}^\\infty \\frac{f(t)}{t+1} dt$$\nThis gives us the differential equation $$\\frac{df(x)}{dx} = -\\frac{f(x-1)}{x}$$\nSince we know that $f(x)$ is a constant for $x<1$, this is enough to solve the differential equation numerically for $x>1$, modulo the constant (which can be retrieved by integration in the end). Unfortunately, the solution is essentially piecewise from $n$ to $n+1$ and it is impossible to find a single function that works everywhere.\nFor example when $x\\in[1,2]$, $$f(x) = f(1) \\left[1-\\log(x)\\right]$$\nBut the expression gets really ugly even for $x \\in[2,3]$, requiring the logarithmic integral function $\\rm{Li}$.\nFinally, as a sanity check, let us compare the random simulation results with $f(x)$ found using numerical integration. The probabilities have been normalised so that $f(0) = 1$.\n\nThe match is near perfect. In particular, note how the analytical formula matches the numerical one exactly in the range $[1,2]$.\nThough we don't have a general analytic expression for $f(x)$, the differential equation can be used to show that the expectation value of $x$ is 1.\nFinally, note that the delay differential equation above is the same as that of the Dickman function $\\rho(x)$ and hence $f(x) = c \\rho(x)$. Its properties have been studied. For example the Laplace transform of the Dickman function is given by $$\\mathcal L \\rho(s) = \\exp\\left[\\gamma-\\rm{Ein}(s)\\right].$$\nThis gives $$\\int_0^\\infty \\rho(x) dx = \\exp(\\gamma).$$ Since we want $\\int_0^\\infty f(x) dx = 1,$ we obtain $$f(1) = \\exp(-\\gamma) \\rho(1) = \\exp(-\\gamma) \\approx 0.56145\\ldots$$ That is, $$f(x) = \\exp(-\\gamma) \\rho(x).$$\nThis completes the description of $f$.", "meta": {"post_id": 2130264, "input_score": 159, "output_score": 51, "post_title": "Sum of random decreasing numbers between 0 and 1: does it converge??"}}
{"input": "I'm having trouble with whether Rudin actually proves what he's tried to prove.\nProposition 1.14; (page 6)\nThe axioms of addition imply the following statements:\na) if $x + y = x + z$ then $y = z$\nThe author's proof is as follows:\n$ y = (0 + y) = (x + -x) + y = -x + (x + \\textbf{y})$\n$$ = -x + (x + \\textbf{z}) = (-x + x) + z = (0 + z) = z $$\nI emphased the section which troubles me. \nHow does Rudin prove that $ y = z $ if he substituted $y = z$?", "output": "He didn't substitute $z$ for $y$; rather, he substituted $x+z$ for $x+y$. This is legitimate based on the assumption that $x+y = x+z$.", "meta": {"post_id": 2131633, "input_score": 20, "output_score": 53, "post_title": "Problems understanding proof of if $x + y = x + z$ then $y = z$ (Baby Rudin, Chapter 1, Proposition 1.14)"}}
{"input": "We are allowed to use a calculator in our linear algebra exam. Luckily, my calculator can also do matrix calculations.\nLet's say there is a task like this:\n\nCalculate the rank of this matrix:\n$$M =\\begin{pmatrix} 5 & 6 & 7\\\\  12 &4  &9 \\\\  1 & 7 & 4\n\\end{pmatrix}$$\n\nThe problem with this matrix is we cannot use the trick with multiples, we cannot see multiples on first glance and thus cannot say whether the vectors rows / columns are linearly in/dependent.\nUsing Gauss is also very time consuming (especially in case we don't get a zero line and keep trying harder).\nEnough said, I took my calculator because we are allowed to use it and it gives me following results:\n$$M =\\begin{pmatrix} 1 & 0{,}3333 & 0{,}75\\\\  0 &1  &0{,}75 \\\\  0 & 0 & 1\n\\end{pmatrix}$$\nI quickly see that $\\text{rank(M)} = 3$ since there is no row full of zeroes.\nNow my question is, how can I convince the teacher that I calculated it? If the task says \"calculate\" and I just write down the result, I don't think I will get all the points. What would you do?\nAnd please give me some advice, this is really time consuming in an exam.", "output": "There is a very nice trick for showing that such matrix has full rank, it can be performed in a few seconds without any calculator or worrying \"moral bending\". The entries of $M$ are integers, so the determinant of $M$ is an integer, and $\\det M\\mod{2} = \\det(M\\mod{2})$. Since $M\\pmod{2}$ has the following structure\n$$ \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 0 & 1 \\\\ 1 & 1 & 0\\end{pmatrix} $$\nit is trivial that $\\det M$ is an odd integer. In particular, $\\det M\\neq 0$ and $\\text{rank}(M)=3$.", "meta": {"post_id": 2156863, "input_score": 42, "output_score": 162, "post_title": "How to efficiently use a calculator in a linear algebra exam, if allowed"}}
{"input": "Is there any rational number $r$ such that ln (r) is rational as well? \nIf so, what's the proof?\nIf proofs are too lengthy to be cointained as an answer here, I would truly appreciated any easy-to-understand references to study them.", "output": "Aside from $r=1$, no.  To prove it, suppose we had an example.  Then we'd write $$\\frac mn=e^{\\frac ab}\\implies e^a=\\left( \\frac mn \\right)^b$$  But, with $a\\neq 0$ this would tell us that $e$ was algebraic, which is not the case.", "meta": {"post_id": 2163085, "input_score": 22, "output_score": 57, "post_title": "On irrationality of natural logarithm"}}
{"input": "I don't get this mathematical joke - can someone explain?\n\nFrom Wikipedia:\nA physicist, a biologist and a mathematician are sitting in a street caf\u00e9 watching people entering and leaving the house on the other side of the street. First they see two people entering the house. Time passes. After a while they notice three people leaving the house. The physicist says, \"The measurement wasn't accurate.\" The biologist says, \"They must have reproduced.\" The mathematician says, \"If one more person enters the house then it will be empty.", "output": "I think the point of the punchline is that the mathematician simply solves the math problem as observed, and appears totally unconcerned with the impossibility of having -1 people in a house in the real world.  \nSciences like physics and biology are about explaining the real world, but in mathematics explaining the real world is not a requirement.", "meta": {"post_id": 2168558, "input_score": 36, "output_score": 38, "post_title": "Mathematical joke regarding three people leaving a house"}}
{"input": "So my prof gave me this proof:\n$f(x) = f(y) \u21d0\u21d2 f(y \u2212 x) = 0 \u21d0\u21d2 y \u2212 x \u2208 Ker f.$\nI dont see why this proof is enough, this only says $y-x \\in Ker f$", "output": "First suppose $f$ is injective.\n\nSince $f$ is linear, $f(0) = 0$, hence $0 \\in \\text{ker}(f)$.\n\nBut if $x$ is any element of $\\text{ker}(f)$, then\n\\begin{align*}\n&x \\in \\text{ker}(f)&&\\\\[4pt]\n\\implies\\; &f(x) = 0&&\\\\[4pt]\n\\implies\\; &f(x) = f(0)&&\\text{[since $f(0) = 0$]}\\\\[4pt]\n\\implies\\; &x = 0&&\\text{[since $f$ is injective]}\\\\[4pt]\n\\end{align*}\nIt follows that $\\text{ker}(f) = \\{0\\}$.\nThus, $f$ injective implies $\\text{ker}(f) = \\{0\\}$.\nNext, suppose $\\text{ker}(f) = \\{0\\}$. Then\n\\begin{align*}\n&f(x)=f(y)&&\\\\[4pt]\n\\implies\\; &f(x)-f(y) = 0&&\\\\[4pt]\n\\implies\\; &f(x-y) = 0&&\\text{[since $f$ is linear]}\\\\[4pt]\n\\implies\\; &x-y \\in  \\text{ker}(f)&&\\\\[4pt]\n\\implies\\; &x-y = 0&&\\text{[since $\\text{ker}(f) = \\{0\\}$]}\\\\[4pt]\n\\implies\\; &x=y&&\\\\[4pt]\n\\end{align*}\nhence $f$ is injective.\n\nThus, $\\text{ker}(f) = \\{0\\}$ implies $f$ is injective.\n\nHence, $f$ is injective $\\iff \\text{ker}(f) = \\{0\\}$, as was to be shown.", "meta": {"post_id": 2193333, "input_score": 15, "output_score": 54, "post_title": "Showing a linear map is injective if and only if kernel is {$ {0} $}"}}
{"input": "I'm self-studying and was doing the following integral:\n$$I = \\int \\frac{e^{\\frac{1}{x}+\\tan^{-1}x}}{x^2+x^4} dx $$\nI solved it fine by letting $ u = \\frac{1}{x} + \\tan^{-1}x$. \nMy question is about an alternative method I saw in which it seems the product rule was not applied:\n$$ I = \\int \\left(\\frac { e^{\\frac{1}{x}}} {x^2}\\right) \\left( \\frac{e^{\\tan^{-1}x}}{x^2+1}\\right) dx $$\n$$ = \\int \\frac {e^{\\frac{1}{x}}}{x^2} dx \\cdot \\int \\frac{e^{\\tan^{-1}x}}{x^2+1}dx$$\nCompleting the work following this step leads to the same solution as I originally found. \nIt is this step that has confused me. I have checked using Wolfram and the two statements are equivalent but I do not understand why.\nWhy are we able to write the integral of products as the product of integrals here, and not apply the product rule?\nThanks in advance.", "output": "Why are we able to write the integral of products as the product of integrals here?\n\nAssume you have two differentiable functions $f,g$ such that\n$$\nf'+g'=f'\\cdot g' \\tag1\n$$ by multiplying by $\\displaystyle e^{f+g}$ one gets\n$$\n(f'+g')\\cdot e^{f+g}=\\left(f'e^{f} \\right)\\cdot \\left(g'e^{g} \\right) \\tag2\n$$ then by integrating both sides\n$$\ne^{f+g}=\\int\\left(f'e^{f} \\right)\\cdot \\left(g'e^{g} \\right) \\tag3\n$$ since $\\displaystyle e^f=\\int\\left(f'e^{f} \\right) $ and $\\displaystyle e^g=\\int\\left(g'e^{g} \\right)$ we have\n\n$$\n\\int\\left(f'e^{f} \\right)\\cdot \\int\\left(g'e^{g} \\right) =\\int\\left(f'e^{f} \\right)\\cdot \\left(g'e^{g} \\right). \\tag4\n$$\n\nBy taking, $f'=-\\dfrac1{x^2}$ and $g'=\\dfrac1{1+x^2}$ we have\n$$\nf'+g'=-\\frac1{x^2}+\\frac1{1+x^2}=-\\frac1{x^2(1+x^2)}=f'g'\n$$ which leads to $(4)$ with the given example.", "meta": {"post_id": 2250993, "input_score": 40, "output_score": 44, "post_title": "When the integral of products is the product of integrals."}}
{"input": "How can I prove that a function that is its own derivative exists? And how can I prove that this function is of the form $a(b^x)$?", "output": "$f(x) = 0$ is trivially its own derivative, and is of the form $a(b^x)$ for $a=0$ and any positive $b$. That's all we need to solve the problem posed.", "meta": {"post_id": 2266951, "input_score": 20, "output_score": 42, "post_title": "How can I prove that there is a function that is its own derivative?"}}
{"input": "Let\n  $$f(x) = 2/(4^x + 2)$$\n  for real numbers $x$. Evaluate\n  $$f(1/2001) + f(2/2001) + f(3/2001) + \\cdots + f(2001/2001)$$\n\nAny idea?", "output": "We have $\\displaystyle f(1-x)=\\frac{2}{4^{1-x}+2}=\\frac{2\\cdot 4^x}{4+2\\cdot 4^x}=\\frac{4^x}{2+ 4^x}$\n$\\displaystyle f(x)+f(1-x)=\\frac{2}{4^x+2}+\\frac{4^x}{2+ 4^x}=1$\nThe required sum is \n$$\\frac{2000}{2}\\times 1+f(1)=1000+\\frac{2}{4+2}=\\frac{3001}{3}$$", "meta": {"post_id": 2327510, "input_score": 32, "output_score": 81, "post_title": "Witty functional equation"}}
{"input": "I have a basic question about the notation for writing functions. Say that I have a function defined as\n$$f(x) = g(x) - 1.$$\nIs it then correct to write\n$$ f = g -1?$$\nAre there any problems with writing functions like this?", "output": "If you use $f$ as the symbol for some function, then $f(x)$ is the image of $x$ under that function, i.e. $f$ represents a function and $f(x)$ is a number, the result of $x \\mapsto f(x)$. So it makes sense to write $f(x)-1$ as a function value where $1$ is subtracted from $f(x)$: there's no real risk on wrong interpretations here, this is standard notation.\nHowever, you can also do operations on functions such as the sum of functions $f+g$ or the product of functions $fg$. In that context, you could interpret \"$f-1$\" as a difference of functions: the difference of $f$ and a constant function $x \\mapsto 1$ (for all $x$), abbreviated simply as \"$1$\". \nNote then that the blue $1$ in $f(x)-\\color{blue}{1}$, a number, doesn't play the same role as the red $1$ in $f-\\color{red}{1}$, meant to denote a function. This can raise confusion so only do this when it's sufficiently clear from the context what you mean.", "meta": {"post_id": 2329794, "input_score": 18, "output_score": 43, "post_title": "Notation for functions"}}
{"input": "Given a $9 \\times 9$ solved Sudoku game with $3 \\times 3$ regions, is it possible that one (or more) of the regions are invalid if all rows and columns are valid (i.e. have a unique sequence of $1-9$)?", "output": "Yes, it can happen that all $3 \\times 3$ regions are invalid:\n\\begin{array}{|ccc|ccc|ccc|} \\hline\n 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\\\\n 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 1 \\\\\n 3 & 4 & 5 & 6 & 7 & 8 & 9 & 1 & 2 \\\\ \\hline\n 4 & 5 & 6 & 7 & 8 & 9 & 1 & 2 & 3 \\\\\n 5 & 6 & 7 & 8 & 9 & 1 & 2 & 3 & 4 \\\\\n 6 & 7 & 8 & 9 & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n 7 & 8 & 9 & 1 & 2 & 3 & 4 & 5 & 6 \\\\\n 8 & 9 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\\n 9 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\\\ \\hline\n\\end{array}", "meta": {"post_id": 2331022, "input_score": 15, "output_score": 48, "post_title": "Can a solved Sudoku game have an invalid region if all rows and columns are valid?"}}
{"input": "Is the closure of a compact subspace of a topological space always compact? \nI want to say no, but i can't think of/find any counterexamples. I think its probably true with added conditions, like Hausdorff property? Because the closure is always closed and closed sets in a compact hausdorff space are compact. \nAny counterexamples for a non-Hausdorff space?", "output": "Consider the space $X = \\mathbb{Z}$ equipped with the topology where the non-empty open sets are precisely those containing $0$. \nAs a finite set $\\{ 0 \\} $ is compact. Further we have that $\\overline{\\{0\\}} = X$ and $X$ is not compact since $\\bigcup_{x \\in X} \\{0,x\\}$ is an open cover with no finite subcover.\nIt is however always the case that the closure of a compact set in a Hausdorff space is compact since in Hausdorff spaces compact sets are closed. (not because of the similar fact that you give that closed sets in compact spaces are compact, even if those spaces aren't Hausdorff)", "meta": {"post_id": 2399624, "input_score": 11, "output_score": 35, "post_title": "Closure of a compact space always compact?"}}
{"input": "Why is the exterior algebra called the \"exterior algebra?\" What makes it \"exterior?\" Is it just because a module can be universally embedded into its exterior algebra, so one could view the exterior algebra as surrounding the module? Why is it not just called the \"alternating algebra?\"", "output": "It was Grassmann that called it exterior because to have a non-null product the multiplicands must be geometrically one to the exterior of the other. For instance $$\\mathbf{x}\\wedge\\mathbf{y}\\wedge\\mathbf{z}=0$$\nif $\\mathbf{x}$ lies in (is not exterior of) the subspace spanned by the $\\mathbf{y}$ and $\\mathbf{z}$. So the product is called exterior product, and consequently the algebra with this product is called exterior algebra.", "meta": {"post_id": 2421566, "input_score": 44, "output_score": 55, "post_title": "Why is the exterior algebra called the \"exterior algebra?\" What makes it \"exterior?\""}}
{"input": "Let $\\alpha$, $\\beta$, $\\gamma$, $\\delta$ be the roots of $$z^4-2z^3+z^2+z-7=0$$ then find value of $$(\\alpha^2+1)(\\beta^2+1)(\\gamma^2+1)(\\delta^2+1)$$\nAre Vieta's formulas appropriate?", "output": "There is no need to use Vieta's formulas. Let \n$$f(z)=z^4-2z^3+z^2+z-7=(z-\\alpha)(z-\\beta)(z-\\gamma)(z-\\delta).$$\nThen, since $(i-a)(-i-a)=-i^2+a^2=1+a^2$, it follows that\n$$(\\alpha^2+1)(\\beta ^2+1)(\\gamma^2+1)(\\delta^2+1)=f(i)f(-i)=|f(i)|^2=|-7+3i|^2=49+9=58.$$", "meta": {"post_id": 2463164, "input_score": 16, "output_score": 47, "post_title": "Value of $(\\alpha^2+1)(\\beta^2+1)(\\gamma^2+1)(\\delta^2+1)$ if $z^4-2z^3+z^2+z-7=0$ for $z=\\alpha$, $\\beta$, $\\gamma$, $\\delta$"}}
{"input": "Could the sum of an even number of distinct odd numbers be divisible by each of the odd numbers ?\nLet $k\\geq 4$ be an even number. Can one find $k$ distinct positive odd numbers $x_1,\\ldots,x_k$ such that each $x_i$ divides $S = \\sum_{i=1}^k x_i$ ?\nIs it possible at least for $k$ big enough ?", "output": "Yes, it is possible.  Divide your sum by $S$ and you have \n$$1=\\sum_i \\frac {x_i}S$$\nwhich is an Egyptian fraction expression of $1$ where all the denominators have the same number of factors of $2$.  This is known to be solvable with all denominators odd, but all known solutions have an odd number of terms.   A survey paper is here.  The section of interest is $9.5$.  One example is: \n$$1=\\frac 13+\\frac 15+\\frac 17 + \\frac 19+\\frac 1{11}+\\frac 1{15}+\\frac 1{35}+\\frac 1{45}+\\frac 1{231}$$ \nwhere the denominators have least common multiple $3465$ so we can write:\n$$3465=1155+693+495+385+315+231+99+77+15$$ \nwith every term dividing the sum.  Now if we add $3465$ to each side we have a solution with an even number of terms:\n$$6930=3465+1155+693+495+385+315+231+99+77+15$$\nAny Egyptian fraction decomposition of $1$ into fractions with odd denominators yields a solution to your problem.  The sum will be twice the least common multiple of the denominators in the decomposition.  The paper shows that there is such a decomposition for all odd numbers of terms $9$ or above.  You can multiply any solution by any odd number to get another.  \nWhat is happening is we are converting the Egyptian fraction decomposition of $1$ with all denominators odd into one that looks like $$1=\\frac 12+\\frac12\\left(\\text{all other terms}\\right)$$", "meta": {"post_id": 2496631, "input_score": 48, "output_score": 89, "post_title": "Could the sum of an even number of distinct positive odd numbers be divisible by each of the odd numbers?"}}
{"input": "In the definition of a ring $R$, one has\n\n$a(b+c) = ab + ac$ and \n$(a+b)c = ac + bc$ \n\nfor all $a,b,c\\in R$\nMy question is (just out of curiosity) if one really needs both of these. I can't think of an example of something that is not a ring that only satisfies one of the sides of the distributive law. So can one prove that if $a(b+c) = ab + ac$ for all $a,b,c$, then $(a+b)c = ac + bc$ for all $a,b,c$.\nEdit: I maybe should add that all rings in my definition have a unity $1$.", "output": "Here is an example that fails precisely in left distributivity.\nConsider $\\mathbb{R}[X]$ - the polynomials with coefficients from $\\mathbb{R}$ with the usual operation of pointwise addition (in fact, the ring of scalars is irrelevant here).\nThe tricky part is how we define multiplication: let $p \\cdot q$ be the composition $p \\circ q$. This multiplication is associative, and even has an identity, which is the identity polynomial $p(x)=x$.\nNow, trivially $$(p_1 + p_2) \\circ q = p_1 \\circ q + p_2 \\circ q,$$ but in general $$p \\circ (q_1 + q_2) \\color{red} \\neq p \\circ q_1 + p \\circ q_2.$$", "meta": {"post_id": 2529177, "input_score": 32, "output_score": 65, "post_title": "Question about distributive law in definition of a ring"}}
{"input": "My understanding of power series turns out to be less-well-formed than I thought.  To confess, I took my two courses in analysis in grad school (one real, one complex) and got out.  \nSince this is my Calc II class, let's keep everything in real variables, please.  It's not hard to derive the power series for $\\arctan(x)$ as\n$$\n\\arctan(x) = \\sum_{n=0}^\\infty \\frac{(-1)^n}{2n+1} x^{2n+1}, \\ -1 \\leq x \\leq 1.\n$$\nAlso not hard to work out the interval of convergence for the right-hand side.  So far, so good.\nHere's my question and why I suddenly see how naive I am.  I tend to think of $\\arctan$ as an incredibly nice function, so I expect its power/Taylor series to converge everywhere.  In short, I view $\\arctan$ as being just as nice as $f(x) = e^x$, whose power series representation converges everywhere (domain of the power series matches the domain of the function).  Same story for $\\sin(x)$ and $\\cos(x)$.  They're \"nice\" so their power series converge on their entire domain. \nWhen the power series for something like $\\ln (x)$ or $\\frac{1}{x}$ has finite radius, I'm completely fine with that as there is an obvious discontinuity that you bump into as you work your way out from the center.  But why does the power series for $\\arctan(x)$ have a finite radius?  I know that something goes wrong with Taylor's remainder and this is what prevents the series from representing $\\arctan(x)$ everywhere, but I would appreciate an explanation from the point of view of properties of $\\arctan(x)$ and not its power series:  what is it about $\\arctan(x)$ that prevents its power series from being optimally \"nice\"?", "output": "Your insistence \"let's keep everything in real variables, please\" is precisely the problem: the cause of the finite radius of convergence is due to the function's behavior in $\\mathbf C$, not $\\mathbf R$.\nA much simpler example than $\\arctan x$ is $1/(1+x^2)$, which is defined and infinitely differentiable on the whole real line but its power series at $0$ (a geometric series with $-x^2$ in place of $x$) has radius of convergence $1$, not $\\infty$. To use your language, \"there is an obvious discontinuity that you bump into as you work your way out from the center,\" namely at $x = \\pm i$ where the function blows up. In fact, if you expand $1/(1+x^2)$ into a power series at a real number $a$, not necessarily at $0$, the radius of convergence will be $\\sqrt{a^2+1} = |a-i|$ -- the distance from the center out to $i$. This phenomenon is bewildering if you refuse to use complex numbers and extremely clear if you use them. Choose wisely.\nIf $f(x)$ is a rational function in reduced form with a nonconstant denominator and its denominator does not vanish at $a$, its power series at $a$ has radius of convergence $|a-\\rho|$ where $\\rho$ is the root of the denominator in $\\mathbf C$ that is closest to $a$. This simple geometric result can not be explained in terms of real variables if the roots of the denominator are not all real. \nTo reinforce how poorly the real numbers are compared to the complex numbers as a predictive tool for the radius of convergence, there are functions $\\mathbf R \\rightarrow \\mathbf R$ that are infinitely differentiable on the whole real line but their power series at each real number $a$ has radius of convergence zero for all $a$ in $\\mathbf R$. \nStrictly speaking, the real numbers have enough information in principle to compute the radius of convergence $R$ of a power series $\\sum c_n(x-a)^n$ with all real coefficients using Hadamard's formula $1/R = \\varlimsup\\limits_{n\\to\\infty} \\sqrt[n]{|c_n|}$, but this formula is often not feasible to compute in practice.", "meta": {"post_id": 2539520, "input_score": 34, "output_score": 41, "post_title": "Power series representation of arctangent: fails to converge everywhere"}}
{"input": "I was recently asked by a student whether there exists a topological space which is not compact, but in which every proper open subset is compact. I haven't been able to give an example, or a proof that no such space exists. So far the best I've been able to show is that such a space cannot contain a closed compact subset (in particular, it cannot be T1).", "output": "Consider $\\Bbb N$, with proper open sets given by $U_n = \\{x: x\\le n\\}$ and the empty set.  Arbitrary unions of the $U_n$ are open, (either given by $U_{m}$ the maximum of the $n$ or by $\\Bbb N$ if the $n$ are unbounded), as too are finite intersections.\nHere every proper open set is finite, and thus trivially compact.  However, it is easy to see that $\\Bbb N$ itself is not compact with this topology, since it is covered by the collection of all proper open subsets, which admits no finite subcover.", "meta": {"post_id": 2551645, "input_score": 35, "output_score": 60, "post_title": "$\\exists$ a non-compact space in which every proper open subset is compact?"}}
{"input": "If you input the trig identity:\n$$\\cot (x)+\\tan(x)=\\csc(x)\\sec(x)$$\nInto WolframAlpha, it gives the following proof:\nExpand into basic trigonometric parts:\n$$\\frac{\\cos(x)}{\\sin(x)} + \\frac{\\sin(x)}{\\cos(x)} \\stackrel{?}{=} \\frac{1}{\\sin(x)\\cos(x)}$$ \nPut over a common denominator:\n$$\\frac{\\cos^2(x)+\\sin^2(x)}{\\cos(x)\\sin(x)} \\stackrel{?}{=} \\frac{1}{\\sin(x)\\cos(x)}$$ \nUse the Pythagorean identity $\\cos^2(x)+\\sin^2(x)=1$:\n$$\\frac{1}{\\sin(x)\\cos(x)}  \\stackrel{?}{=} \\frac{1}{\\sin(x)\\cos(x)}$$ \nAnd finally simplify into \n$$1\\stackrel{?}{=} 1$$\nThe left and right side are identical, so the identity has been verified.\nHowever, I take some issue with this. All this is doing is manipulating a statement that we don't know the veracity of into a true statement. And I've learned that any false statement can prove any true statement, so if this identity was wrong you could also reduce it to a true statement.\nObviously, this proof can be easily adapted into a proof by simply manipulating one side into the other, but:\nIs this proof correct on its own? And can the steps WolframAlpha takes be justified, or is it completely wrong?", "output": "It is good that you are wary of proving identities this way. Indeed, I could \"prove\" $0=1$ by saying\n\\begin{align*}\n0 &\\stackrel{?}{=}1\\\\\n0\\cdot 0 &\\stackrel{?}{=} 0 \\cdot 1\\\\\n0 &=0.\n\\end{align*}\nThe important point is that every step WolframAlpha did is reversible, while the step I took (multiplying by $0$) was not. That is what allows the proof from WolframAlpha to be rearranged into a proof that starts with one side of the identity and ends at the other:\n\\begin{align*}\n\\cot(x)+\\tan(x) &= \\frac{\\cos(x)}{\\sin(x)} + \\frac{\\sin(x)}{\\cos(x)}\\\\\n&= \\frac{\\cos^2(x)}{\\sin(x)\\cos(x)} + \\frac{\\sin^2(x)}{\\sin(x)\\cos(x)}\\\\\n&= \\frac{\\sin^2(x)+\\cos^2(x)}{\\sin(x)\\cos(x)}\\\\\n&=\\frac{1}{\\sin(x)\\cos(x)}\\\\\n&=\\csc(x)\\sec(x).\n\\end{align*}\nSo no, the WolframAlpha proof is not wrong, but it neglects to emphasize the important fact that every step is reversible. I am not a fan of that sort of proof, as it gives students the idea that they can prove an identity by manipulating both sides in any way they like to arrive at a true statement.", "meta": {"post_id": 2555645, "input_score": 21, "output_score": 35, "post_title": "Is this an incorrect proof of $\\cot (x)+\\tan(x)=\\csc(x)\\sec(x)$?"}}
{"input": "In a lecture, our professor gave an example for a ring. He took it out of another source and mentioned that he does not know the motivation for the chosen operation.\nOf course, it's likely that somebody just invented an arbitrary operation satisfying ring axioms. I'd still like to try my luck whether anyone here can decipher the operation and give any kind of motivation for that example.\nOn $\\mathbb{R}^3$ define the operations $+$ and $\\cdot$ by\n$$ \\begin{aligned} (a_1, a_2, a_3) + (b_1,b_2,b_3) &= (a_1+b_1,a_2+b_2,a_3+b_3)\n\\\\ (a_1, a_2, a_3) \\cdot (b_1, b_2, b_3) &= (a_1 \\cdot b_1, a_2 \\cdot b_2, a_1 \\cdot b_3 + a_3 \\cdot b_2).\n\\end{aligned} $$\n(The $+$ and $\\cdot$ operations on the right side are the usual addition and multiplication from $\\mathbb{R}$.)\nWith those operations, one can confirm that $\\left(\\mathbb{R}^3, +, \\cdot \\right)$ is a ring.", "output": "This is just matrix multiplication in disguise.  Specifically, if you identify $(a_1,a_2,a_3)$ with the matrix $\\begin{pmatrix}a_1 & a_3 \\\\ 0 & a_2\\end{pmatrix}$, these operations are the usual matrix operations:\n$$\\begin{pmatrix}a_1 & a_3 \\\\ 0 & a_2\\end{pmatrix}+\\begin{pmatrix}b_1 & b_3 \\\\ 0 & b_2\\end{pmatrix}=\\begin{pmatrix}a_1+b_1 & a_3+b_3 \\\\ 0 & a_2+b_2\\end{pmatrix}$$\n$$\\begin{pmatrix}a_1 & a_3 \\\\ 0 & a_2\\end{pmatrix}\\begin{pmatrix}b_1 & b_3 \\\\ 0 & b_2\\end{pmatrix}=\\begin{pmatrix}a_1b_1 & a_1b_3+a_3b_2 \\\\ 0 & a_2b_2\\end{pmatrix}$$", "meta": {"post_id": 2557479, "input_score": 33, "output_score": 55, "post_title": "Motivation for the ring product rule $(a_1, a_2, a_3) \\cdot (b_1, b_2, b_3) = (a_1 \\cdot b_1, a_2 \\cdot b_2, a_1 \\cdot b_3 + a_3 \\cdot b_2)$"}}
{"input": "I cannot figure out what is wrong:\nWe will attempt to show that $\\mathcal{P} (\\mathbb{N})$ is countable. We use the following corollary from Rudin's Principles of Mathematical Analysis, p. 29:\n\nSuppose $A$ is at most countable, and, for every $\\alpha\\in A$, $B_{\\alpha}$ is at most countable. Put\n$$T=\\bigcup_{\\alpha \\in A}B_{\\alpha}$$\nThen $T$ is at most countable.\n\n\"Proof\" 1:\nLet $A = \\mathbb{N}$ and for every $\\alpha \\in A$ let $B_{\\alpha}=\\{S \\in \\mathcal{P} (\\mathbb{N})| \\text{the sum of the elements of } S \\text{ is } \\alpha \\}$. $A$ is countable and for every $\\alpha \\in A$, $B_{\\alpha}$ is finite. Therefore\n$$\\bigcup_{\\alpha \\in A}B_{\\alpha}$$\nis countable. But $\\displaystyle \\bigcup_{\\alpha \\in A}B_{\\alpha}=\\mathcal{P} (\\mathbb{N})$, so $\\mathcal{P} (\\mathbb{N})$ is countable.\n\n\"Proof\" 2:\nLet $A= \\mathbb{N}$ and for every $\\alpha \\in A$ let $B_{\\alpha}=\\{ S \\in \\mathcal{P} (\\mathbb{N}): |S| = \\alpha \\}$. I think that I can show by induction (if requested) that for each $\\alpha \\in A$, $B_{\\alpha}$ is countable. Thus\n$$\\bigcup_{\\alpha \\in A}B_{\\alpha}$$\nis countable. But again, $\\bigcup_{\\alpha \\in A}B_{\\alpha} = \\mathcal{P} (\\mathbb{N})$", "output": "In both your \"proofs\", it is not true that $\\displaystyle \\bigcup_{\\alpha \\in A}B_{\\alpha}=\\mathcal{P} (\\mathbb{N})$.  Indeed, if $S\\in\\mathcal{P}(\\mathbb{N})$ is any infinite set, then $S$ is not in any $B_\\alpha$ (by either definition).\nWhat both your arguments show correctly is that the set of all finite subsets of $\\mathbb{N}$ is countable.", "meta": {"post_id": 2571222, "input_score": 30, "output_score": 55, "post_title": "What is wrong with my \"disproof\" of Cantor's Theorem?"}}
{"input": "I'm not looking for the definition of discrete topology given in textbooks; I'm wondering why the word 'discrete' was chosen.\nI mean, the concept of discrete topology is built up from sets, which are built from objects--which are discrete. So, if we're looking for a word to differentiate power sets as topologies from other topologies, and we use the adjective 'discrete' to accomplish that differentiation because the power set is composed of discrete objects--then, by similar reasoning, couldn't we call all topologies on sets 'discrete'.\nBecause they're built from discrete objects and compositions, too. All of them. All topologies.\nThere must be some other reason we call discrete topologies 'discrete'. What is it?", "output": "The discrete topology has a topological structure which perfectly reveals the discrete nature of the underlying set of points\nYou can consider a set to be a discrete collection of objects. To a given set $X$, you can assign a variety of topologies. Let's argue for the appropriateness of calling this particular topology \"discrete\".\n\nThe discrete topology is the finest topology\u2014it cannot be subdivided further. If you think of the elements of the set as indivisible \"discrete\" atoms, each one appears as a singleton set. You can effectively \"see\" the individual points in the topology itself.\nContrast this with the indiscrete topology, consisting only of $X$ itself and $\\varnothing$. This topology obscures everything about how many points were in the original set. It fully agglomerates the points of the set together.\nRevisiting this point, it's sometimes helpful to think of topologies as obscuring or blurring together the underlying points of the set. Topologies are all about nearness relations: points in an open set are in the vicinity of one another. If there are two points that never appear alone in an open set, those points are topologically indistinguishable. From the perspective of the topology, they are so close as to be identical.\nIt is therefore remarkable that the discrete topology has no indistinguishable points. The discrete topology is the topology that obscures nothing about the underlying set. Each point in the set is clearly highlighted and distinguishable and recoverable as an open singleton set in the topology.\nIf you think of topologies that can arise from metrics, the discrete topology arises from metrics such as $d(x,y) = \\begin{cases}0 & x=y\\\\1&x\\neq y\\end{cases}$. This metric \"shatters\" the points $X$, isolating each one within its own unit ball. In such a space, the only convergent sequences are the ones that are eventually constant; you can't find points arbitrarily close to any other points. Because points are isolated in this way, it makes sense to call the space \"discrete\".\nEvery function from a discrete space is automatically continuous. I'd argue that for this reason, the discrete topology is the one that best \"represents\" $X$ in topological space. Indeed, in many ways the nature of a set is characterized by its functions, and the nature of a topological space is characterized by its continuous functions. \nSo, note that if $T$ is any topological space, there's a natural bijective correspondence between functions $f:X\\rightarrow \\mathsf{set}(T)$ and continuous morphisms $g:\\mathsf{discrete}(X)\\rightarrow T$. For every function on $X$, you can find a continuous function on $\\mathsf{discrete}(X)$, and given any continuous function on $\\mathsf{discrete}(X)$, you can uniquely recover a function on $X$. \nThe discrete topology best represents the structure of the set $X$ which, as you say, is discretized into individual points.\nThroughout abstract algebra, isomorphisms describe which structures are \"the same\". A topological isomorphism (a homeomorphism) between two topologies says that they are essentially the same topology. An isomorphism of sets is just a bijection; it says that the sets contain the same number of elements.\nContinuing the discussion of functions above, two discrete topologies are topologically isomorphic (homeomorphic) if and only if their underlying sets are isomorphic as sets (bijective). Put casually, this means that the discrete-topology-creating process maintains the similarity and differences between the underlying sets: discrete topologies are the same if and only if their underlying sets are.\nThis is all the more important when we realize that sets are the same when they have the same number of points. Hence discrete topologies are the same when (and only when) their underlying sets have \"discrete points\" in the same quantity. You can count the points in a discrete topology through isomorphisms, and the discrete topology is the only topology for which this is possible.", "meta": {"post_id": 2614268, "input_score": 25, "output_score": 43, "post_title": "Why is a discrete topology called a discrete topology?"}}
{"input": "How would you go about finding prime factors of a number like $7999973$? I have trivial knowledge about divisor-searching algorithms.", "output": "The thing to notice here is that 7,999,973 is close to 8,000,000.  In fact it is $8000000 - 27$.  Both of these are perfect cubes.  Differences of cubes always factor: $$a^3 - b^3 = (a-b)(a^2+ab+b^2)$$\nHere we have $a=200, b=3$, so $a-b= 197$ is a factor.", "meta": {"post_id": 2615426, "input_score": 48, "output_score": 212, "post_title": "Find a prime factor of $7999973$ without a calculator"}}
{"input": "This question seems obvious, but I'm not secure of my proof.\n\nIf a compact set $V\\subset \\mathbb{R^n}$ is covered by a finite union of open balls of common radii $C(r):=\\bigcup_{i=1}^m B(c_i,r)$, then is it true that there exists $0<s<r$ such that $V\\subseteq C(s)$ as well? The centers are fixed.\n\nI believe this statement is true and this is my attempt to prove it:\nEach point of $v\\in V$ is an interior point of least one ball (suppose its index is $j_v$), that is, there exists $\\varepsilon_v>0$ such that $B(v,\\varepsilon_v)\\subseteq B(c_{j_v},r)$, so $v\\in B(c_{j_v},r-\\varepsilon_v)$. Lets consider only the greatest $\\varepsilon_v$ such that this holds. Then defining $\\varepsilon:=\\inf\\{\\varepsilon_v\\mid v\\in V\\}$ and $s=r-\\varepsilon$ we get $V\\subseteq C(s)$.\n\nBut why is $\\varepsilon$ not zero? I thought that considering the greatest $\\varepsilon_v$ was important, but still couldn't convince myself.\n\nI would appreciate any help.", "output": "Replace each open ball $B_i$ of radius $r$ in the cover by the union of concentric open balls of radii strictly smaller than $r$.  You get an infinite cover of $V$.  By compactness there is a finite subcover.  By construction the radii are smaller than before.  Finally we choose the maximal radius (for all of the finitely many balls) which is still smaller than $r$.", "meta": {"post_id": 2616280, "input_score": 20, "output_score": 37, "post_title": "If a compact set is covered by a finite union of open balls of same radii, can we always get a lesser radius?"}}
{"input": "We had this problem in exam  class  yesterday on Combinatoric  and it was supposed to be the new year gift from our teacher. The exercise was entitled A Gift Problem for the Year 2018 \n\nProblem:\nThe numbers  $1,\\frac{1}{2},\\frac{1}{3},\\frac{1}{4},\\cdots,\\frac{1}{2018} $ are written on the blackboards. John chooses any two numbers say $x$ and $y$ erases them and writes the number $x+y+xy$.\n  He continues to do so until there is only one number left on the board.\n  What are the possible value of the final number?\n\nI understood the problem as follows for instance if John take $x=1$ and $y=\\frac{1}{2}$ then $x+y+xy =2$ and the new list becomes $$2,\\frac{1}{3},\\frac{1}{4},\\cdots,\\frac{1}{2018} $$ \n continuing like this and so on.....\nPlease bear with me  that I do not want to propose my solution since I fell like it was wrong and I don't want to fail the exam before the result get out. but by the way I found, $2017$, $2018$ and $2019$ but I am still suspicious. \nYou may help is you have an idea.", "output": "Consider the multiplicative  law on $\\Bbb R$ defines by $$x*y =x+y+xy =(x+1)(y+1)-1 $$\nyou can check that it is associative and commutative on $\\Bbb R$. Therefore at the end the remaining number is \n$$\\begin{align}x_0*x_1*x_2*\\cdots x_{2018} &= 1*\\frac{1}{2}*\\frac{1}{3}*\\cdots *\\frac{1}{2018} \\\\&=\\left[\\prod_{i=1}^{2018}(1+x_i)\\right]-1\\\\\n&=\\left[\\prod_{i=1}^{2018}\\left(1+\\frac{1}{i}\\right)\\right]-1 \\\\\n&=\\frac{2}{1}\\cdot \\frac{3}{2}\\cdot \\frac{4}{3}\\cdot \\ldots \\cdot \\frac{2018+1}{2018}-1=\\color{red}{2019-1=2018.}\n\\end{align}$$", "meta": {"post_id": 2617684, "input_score": 23, "output_score": 39, "post_title": "A Gift Problem for the Year 2018"}}
{"input": "Why is that if every row of a matrix sums to $1$ then the rows of its inverse matrix sum to $1$ too?\n\nFor example, consider \n$$A=\\begin{pmatrix}\n1/3 & 2/3 \\\\\n3/4 & 1/4\n\\end{pmatrix}$$\nthen its inverse is\n$$A^{-1}=\\begin{pmatrix}\n-3/5 & 8/5 \\\\\n9/5 & -4/5\n\\end{pmatrix},$$\nwhich satisfies the condition. Is it true for every such matrix?", "output": "Let $v = (1, 1, \\ldots , 1)'$ be a column vector of all $1$s. Then the rows of $A$ adding to $1$ is equivalent to saying $Av = v$. So when $A$ is invertible, we will have $$A^{-1}v = A^{-1}Av = v$$ Thus $A^{-1}$ has rows summing to $1$ as well. (Note that $A$ will not always be invertible.)", "meta": {"post_id": 2619258, "input_score": 48, "output_score": 107, "post_title": "Why is that if every row of a matrix sums to 1, then the rows of the inverse matrix sums to 1 too?"}}
{"input": "Why is it true that if 7 divides 91 then $(2^7-1) $ divides $(2^{91}-1)$?\n1) $2^{91}-1$\n$7|91  \\implies (2^7-1)|(2^{91}-1)$\n$\\implies 2^7-1$ is factor \n2) $2^{1001}-1$\n$7|1001  \\implies (2^7-1)|(2^{1001}-1)$\n$\\implies 2^7-1$ is factor", "output": "It may be illustrative to write the numbers out in binary.  I'll use $2^{21} - 1 = (2^7)^3 - 1$ instead of $2^{91} - 1$, since it's shorter:\n$$\\begin{aligned}\n2^{21} - 1\n&= \\underbrace{111111111111111111111}_{21\\text{ digits}}\\,\\vphantom1_2 \\\\\n&= \\underbrace{1111111}_{7\\text{ digits}}\\,\\underbrace{1111111}_{7\\text{ digits}}\\,\\underbrace{1111111}_{7\\text{ digits}}\\,\\vphantom1_2 \\\\\n&= 1111111_2 \\times 100000010000001_2 \\\\\n&= (2^7 - 1) \\times (2^{14} + 2^7 + 1).\n\\end{aligned}$$", "meta": {"post_id": 2626598, "input_score": 13, "output_score": 34, "post_title": "Factor of a Mersenne number"}}
{"input": "We call a divisor of a positive integer interesting if it is a divisor of a number and differs from 1 and the number itself. We call a number $X$ very interesting if it has at least two interesting divisors and it is divisible by the difference between any two of its interesting divisors. \nHow can one find the product of all very interesting numbers?\nI experimented numerically and came to the conclusion that such numbers never exceed 1000. Is it really so?", "output": "Suppose $n$ is very interesting.\n\nThen $n$ must be composite, hence we can write, $n=ab$, where $a$ is the least prime factor of $n$, and $b > 1$.\n\nIf $b \\le a$, then by minimality of $a$, we must have $b=a$, but then $n=a^2$, which is not possible, since the square of a prime is not a very interesting number. Therefore $b > a$.\n\nIf $a$ is odd, then $n$ is odd, hence so is $b$.\n\nBut then $b-a$ is even, hence, since $(b-a){\\,\\mid\\,} n$, it follows that $2{\\,\\mid\\,}n$, contradiction.\n\nHence we must have $a=2$, so $n=2b$.\n\nThen $b-2$ must divide $n$, but then, since \n$$n = 2b = 2(b-2) + 4$$\nit follows that $(b-2){\\,\\mid\\,}4$, hence $b \\le 6$, so $n \\le 12$.\n\nFor $1 \\le n \\le 12$, the only candidates are $6,8,10,12$, since those are the only even composite numbers which are not the square of a prime. \n\nThe number $10$ is not very interesting, since $5-2 = 3$, which is not a divisor of $10$.\n\nIt's easily verified that the numbers $6,8,12$ are very interesting, hence those are the only very interesting numbers.", "meta": {"post_id": 2641461, "input_score": 12, "output_score": 38, "post_title": "About \"interesting\" numbers #2"}}
{"input": "Exactly the title: can you take the derivative of a function at infinity?\nI asked my maths teacher, and while she thought it was an original question, she didn't know the answer, and I couldn't find anything online about this.\nMaybe this is just me completely misunderstanding derivatives and functions at infinity, but to me, a high schooler, it makes sense that you can. For example, I'd imagine that a function with a horizontal asymptote would have a derivative of zero at infinity.", "output": "In a very natural sense, you can! If $\\lim_{x \\to \\infty} f(x) = \\lim_{x \\to -\\infty} f(x) = L$ is some real number, then it makes sense to define $f(\\infty) = L$, where we identify $\\infty$ and $-\\infty$ in something called the one-point compactification of the real numbers (making it look like a circle). \nIn that case, $f'(\\infty)$ can be defined as\n$$f'(\\infty) = \\lim_{x \\to \\infty} x \\big(f(x) - f(\\infty)\\big).$$\nWhen you learn something about analytic functions and Taylor series, it will be helpful to notice that this is the same as differentiating $f(1/x)$ at zero. \nNotice that this is actually not the same as $\\lim_{x \\to \\infty} f'(x)$.\nThese ideas actually show up quite a bit in analytic capacity, so this is a rather nice idea to have.\n\nI wanted to expand this answer a bit to give some explanation about why this is the \"correct\" generalization of differentiation at infinity. and hopefully address some points raised in the comments.\nAlthough $\\lim_{x \\to \\infty} f'(x)$ might feel like the natural object to study, it is quite badly behaved. There are functions which decay very quickly to zero and have horizontal asymptotes, but where $f'$ is unbounded as we tend to infinity; consider something like $\\sin(x^a) / x^b$ for various $a, b$. Furthermore, $\\lim_{x \\to \\infty} f'(x) = 0$ is not sufficient to guarantee a horizontal asymptote, as $\\sqrt{x}$ shows.\nSo why should we consider the definition I proposed above? Consider the natural change of variables interchanging zero and infinity*, swapping $x$ and $1/x$. Then if $g(x) := f(1/x)$ we have the relationship\n$$\\lim_{x \\to 0} \\frac{g(x) - g(0)}{x} = \\lim_{x \\to \\infty} x \\big(f(x) - f(\\infty)\\big).$$\nThat is to say, $g'(0) = f'(\\infty)$. Now via this change of variables, neighborhoods of zero for $g$ correspond to neighborhoods of $\\infty$ for $f$. So if we think of the derivative as a measure of local variation, we now have something that actually plays the correct role.\nFinally, we can see from this that this definition of $f'(\\infty)$ gives the coefficient $a_1$ in the Laurent series $\\sum_{i \\ge 0} a_i x^{-i}$ of $f$. Again, this corresponds to our idea of what the derivative really is.\n* This is one of the reasons why I used the one-point compactification above. Otherwise, everything that follows must be a one-sided limit or a one-sided derivative.", "meta": {"post_id": 2747593, "input_score": 44, "output_score": 65, "post_title": "Can you take the derivative of a function at infinity?"}}
{"input": "I imagine this problem is a common one, however without having any source to refer to I don't know its usual name, and am having trouble finding an answer. \nI am taking the definition: A cover $C$ of a set $S$ is a set such that $\\cup C = S$\nI want to know if every cover has a minimal subcover. \nThanks.", "output": "How about $S=\\Bbb R$ and the cover composed of the intervals $(-n,n)$?\nAny subcover of this cover remains a subcover if you omit one of its elements.", "meta": {"post_id": 2805639, "input_score": 12, "output_score": 39, "post_title": "Does every cover of a set have a minimal subcover?"}}
{"input": "What is the longest path in a square grid from one corner to the diagonally opposite corner?\nEdges in the grid may only be traversed once, but grid points can be used multiple times (in a square grid that means maximum twice). \nAfter fiddling with this a while, it seems to me the following paths are the longest:\n\nIn a $2 \\times 2$ grid, the longest path is $8$. \n\nIn a $3 \\times 3$ grid, the longest path is $18$. \n\nIn a $4 \\times 4$ grid, the longest path is $32$.\n\nIn a $5 \\times 5$ grid, the longest path is $50$.\nIn general, the rule seems to be that the longest path for an $n \\times n$ grid is $2n^2$. \nCan anyone confirm this? I've searched for a proof of this, but wasn't able to find it.", "output": "There are $2n(n+1)$ potential edges in the grid, but some of them need to be left out because the grid points on the boundary have only $3$ potential edges meeting, and only two of them can be in your path.\nAdditionally, one of the edges incident to the start and end node must be left out.\nIf we count the number of left-out half-edges we get at least\n$$ 4(n-1) + 2 = 4n-2 $$\nso at least $2n-1$ of the $2n(n+1)$ edges must be missing. So at most there are\n$$ 2n(n+1)-(2n-1) = 2n^2 +1 $$\nedges in the path.\nHowever, the length of the path must be even: Color the grid points alternately black and white in a checkerboard pattern. Two opposite corners will have the same color, but every move changes the color, so there must be an even number of moves.\nThis shows that the the length of the path is at most $2n^2$.\nOn the other hand, it should be clear that the pattern you have found achieves this number for larger $n$ too, so it is the actual maximum.", "meta": {"post_id": 2824382, "input_score": 23, "output_score": 36, "post_title": "Longest path in a square grid"}}
{"input": "I am wondering what exactly is the relationship between the three aforementioned spaced. All of them seem to show up many times in: Linear Algebra, Topology, and Analysis. However, I feel like I'm missing the bigger picture of how these spaces relate to each other. For example, in my course in multi-dimensional analysis, we started out talking about metric spaces, but later suddenly switched to normed vector spaces, without any explicit mention of this transition. In linear algebra we usually talked about inner product spaces, and in topology we talked about metric spaces and topological spaces. \nThe bigger picture of the relation between these three is still unclear to me. Which is used where, for what reason, and how do they relate?\nI do know the definitions of all three of them: \nA metric space is a pair $(S,d)$ with $S$ a set and $d: S \\times S \\to \\mathbb{R}_{\\geq 0}$ a metric:\n\n$d(x,x) = 0$ for all $x \\in S$ and $d(x,y) >0$ for $x \\neq y$, \n$d(x,y) = d(y,x)$, \n$d(x,z) \\leq d(x,y) + d(y,z)$. \n\nA (real) inner product space is a pair $(V,\\langle \\cdot \\rangle)$ where $V$ is a (real) vector space and $\\langle \\cdot \\rangle: V \\times V \\to \\mathbb{R}$ is an inner product: \n\n$\\langle v,w \\rangle = \\langle w,v \\rangle$, \n$\\langle a_1 v_1 + a_2v_2,w \\rangle = a_1\\langle v_1,w \\rangle + a_2\\langle v_2,w \\rangle$ for all $a_1,a_2 \\in \\mathbb{R}$, \n$v \\neq 0 \\Longrightarrow \\langle v,v \\rangle > 0$. \n\nA (real) normed vector space is a pair $(V,\\|\\cdot\\|)$ where $V$ is a (real) vector space and $\\|\\cdot\\|: V \\to \\mathbb{R}_{\\geq 0}: v \\mapsto \\|v\\|$ is a norm on $V$: \n\n$\\|v\\| \\geq 0$ and $\\|v\\|  = 0 \\ \\Longleftrightarrow \\ v = 0$. \nFor $t \\in \\mathbb{R}$ and $v \\in V$ we have $\\|tv\\| = |t|\\|v\\|$\n$\\|v+w\\| \\leq \\|v\\| + \\|w\\|$. \n\nI also know that an inner product gives rise to a norm by taking $\\|v\\| = \\sqrt{\\langle v,v \\rangle}$, for example the Euclidean norm derives from the standard inner product on $\\mathbb{R}^n$ in this way. And Cauchy-Schwarz: $|\\langle x,y \\rangle| \\leq \\|x\\|\\|y\\|$. \nI'm not interested in details about the definitions but in the intuition and bigger picture of these three spaces, and how they show up in Analysis.", "output": "You have the following inclusions:\n$$\\{ \\textrm{inner product vector spaces} \\} \\subsetneq \\{ \\textrm{normed vector spaces} \\} \\subsetneq \\{ \\textrm{metric spaces} \\} \\subsetneq \\{ \\textrm{topological spaces} \\}.$$\nGoing from the left to the right in the above chain of inclusions, each \"category of spaces\" carries less structure. In inner product spaces, you can use the inner product to talk about both the length and the angle of vectors (because the inner product induces a norm). In a normed vector space, you can only talk about the length of vectors and use it to define a special metric on your space which will measure the distance between two vectors. In a metric space, the elements of the space don't even have to be vectors (and even if they are, the metric itself doesn't have to come from a norm) but you can still talk about the distance between two points in the space, open balls, etc. In a topological space, you can't talk about the distance between two points but you can talk about open neighborhoods.\nBecause of this inclusion, everything that works for general topological spaces will work in particular for all other spaces, but there are some things you can do in (say) normed vector spaces which don't make sense in a general topological space. For example, if you have a function $f \\colon V \\rightarrow \\mathbb{R}$ on a normed vector space, you can define the directional derivative of $f$ at $p \\in V$ in the direction $v \\in V$ by the limit\n$$ \\lim_{t \\to 0} \\frac{f(p + tv) - f(p)}{t}. $$\nIn the definition, you are using the fact that you can add the vector $tv$ to the point $p$. If you try to mimick this definition in a topological space, then since the set itself doesn't have the structure of a vector space, you can't add two elements so this definition doesn't make sense. That's why during your studies you sometimes restrict your attention to a smaller category of spaces which has more structure so you can do more things in it. \nYou can discuss the notions of continuity, compactness only in the category (context) of topological spaces (but for reasons of simplicity it is often done in the beginning of one's studies in the category of metric spaces). However, once you want to discuss differentiability, then (in first approximation, before moving to manifolds) you need to restrict your category and work with normed vector spaces. If you also want to discuss the angle that two curves make, you will need to further restrict your category and work with inner product vector spaces in which the notion of angle makes sense, etc.", "meta": {"post_id": 2841855, "input_score": 22, "output_score": 48, "post_title": "Relation between metric spaces, normed vector spaces, and inner product space."}}
{"input": "I am currently reading various differential geometry books. From what I understand differential forms allow us to generalize calculus to manifolds and thus perform integration on manifolds. I gather that it is, in general, completely distinct from Lebesgue measure theory and is more like a generalization of Riemann integration.\nOk so here's the problem. I have always viewed Lebesgue measure theory as 'solving the issues with Riemann integration'. For example, a big problem with Riemann integration  is that the space of Riemann integral functions is not complete. The fact that $L^p$ spaces in the Lebesgue theory are complete seems like a huge improvement on the Riemann situation, and is vital for so many concepts in functional analysis, PDEs, operator theory, and numerical analysis.\nSo if we then consider differential geometry and integration via differential forms, unless I am misunderstanding something, we lose all the benefits of Lebesgue theory?\nIt seems like if do lose all those benefits we are in a very bad situation. For example, how are we supposed to rigorously define solution spaces for PDEs if we can't use $L^p$ spaces and thus can't use Sobolev spaces? How can we obtain acceptable convergence of some sequence that may arise during our work if we are operating in this generalized Riemann setting where we lack completeness?\nIn summary, if differential forms are a generalization of Riemann integration how are we supposed to perform analysis when we no longer have the power and utility of Lebesgue measure theory?", "output": "People use measure theory in tandem with differential forms all the time\u2014there's no contradiction whatsoever between the formalisms. Be aware, though, that the adjective \u201cRiemannian\u201d in the context of differential geometry refers to constructions depending on Riemannian metrics (which are \u201cRiemannian\u201d in the sense of originating in the work of Bernhard Riemann), not to Riemann integration.\nSuppose that $M$ is a smooth $n$-manifold. By definition, it's locally diffeomorphic to $\\mathbb{R}^n$, so that you can define a set $S \\subset M$ to be measurable if and only if $x(S \\cap U) \\subset \\mathbb{R}^n$ is Lebesgue measurable for every local coordinate chart $x: U \\to x(U) \\subset \\mathbb{R}^n$. This gives you a $\\sigma$-algebra of Lebesgue measurable sets on $M$ that correctly completes the Borel $\\sigma$-algebra generated by the open sets on $M$ as a topological space. At this point, you have everything you need to define measurable functions, vector fields, differential forms, tensor fields, etc., in a manner compatible with calculations in local coordinates.\nNow, suppose that $M$ is a Riemannian manifold, so that it comes equipped with a Riemannian metric $g$\u2014again, the \u201cRiemannian\u201d here does not refer to Riemann integration, but to Riemann himself and his work on differential geometry. On any local coordinate chart $x : U \\to x(U) \\subset \\mathbb{R}^n$, you can define a measure $\\lambda_{g,x}$ on $U$ by setting\n$$\n \\lambda_{g,x}(S \\cap U) := \\int_{x(S \\cap U)} \\sqrt{\\det\\left(g\\left(\\tfrac{\\partial}{\\partial x^i},\\tfrac{\\partial}{\\partial x^j}\\right)\\right)} \\,d\\lambda\n$$\nfor any Lebesgue measurable $S \\subset M$, where $\\lambda$ denotes Lebesgue measure on $\\mathbb{R}^n$. By paracompactness of the manifold $M$, one can cover $M$ by a locally finite open cover of such local coordinate charts, and hence use a smooth partition of unity subordinate to this cover to patch these local scaled pullbacks of Lebesgue measure together into a single measure $\\lambda_g$, the Riemannian measure [!] on $M$ with respect to $g$, which is a complete $\\sigma$-finite measure on the $\\sigma$-algebra of Lebesgue measurable sets in $M$.\nLet me now describe the basic properties of $\\lambda_g$.\n\nThe measure $\\lambda_g$ is compatible with calculations in local coordinates, in the precise sense that $\\lambda_g(S \\cap U) = \\lambda_{g,x}(S \\cap U)$ for any Lebesgue measurable $S$ and any local coordinate chart $x : U \\to x(U) \\subset \\mathbb{R}^n$.\n\nIf $g^\\prime$ is any another Riemannian metric, then the Riemannian measures $\\lambda_g$ and $\\lambda_{g^\\prime}$ will be mutually absolutely continuous $\\sigma$-finite measures with smooth Radon\u2013Nikodym derivative computable directly in terms of $g$ and $g^\\prime$.\n\nSuppose that $M$ is orientable, and let $\\mathrm{vol}_g \\in \\Omega^n(M)$ be the Riemannian volume form defined by $g$. Then for any Riemann integrable $f$ on $M$,\n$$\n \\int_M f \\, \\mathrm{vol}_g = \\int_M f \\,d\\lambda_g,\n$$\nso that $\\lambda_g$ really is the (completed) Radon measure on $M$ corresponding to the positive functional $C_c(M) \\ni f \\mapsto \\int_M f \\, \\mathrm{vol}_g$ via the Riesz representation theorem. In other words, integration with respect to $\\lambda_g$ really is the \u201cLebesgue-ification\u201d of integration against the top-degree form $\\mathrm{vol}_g$.\n\n\nOnce you've constructed the Riemannian measure on your Riemannian manifold $(M,g)$, the sky is now the limit\u2014you can construct $L^p$ and Sobolev spaces of functions, vector fields, differential forms, tensor fields, etc., and in particular, you can use them to study, for instance, the geometric partial differential operators (e.g., generalisations of the Laplacian and the Dirac operator) and their associated partial differential equations (e.g., heat equations) to great mathematical effect. As a mathematical researcher, I'm personally most familiar with the mathematical ecosystem centred around the Atiyah\u2013Singer index theorem, which relates quantities from algebraic topology to functional-analytic computations on Riemannian manifolds, but you should be aware, for instance, that Perelman's proof of the Poincar\u00e9 conjecture involved the detailed analysis of a certain highly non-linear PDE for the Riemannian metric itself [!]. Perhaps the most accessible example of these methods in action is Hodge theory, which basically computes the cohomology of a compact manifold in terms of solutions of the Laplace equation (with respect to some Riemannian metric) on differential forms of various degrees.\nP.S. People tend to take the extension of Lebesgue theory from $\\mathbb{R}^n$ to manifolds more or less for granted, so precise accounts of this can be oddly hard to find. However, a precise if terse account of Lebesgue theory on manifolds can be found in Dieudonn\u00e9's Treatise of Analysis, Volume 3, Section 16.22 (especially Theorem 16.22.2 and the following discussion). Dieudonn\u00e9 doesn't require a Riemannian metric, but the point is that Riemannian metric gives a canonical choice of Lebesgue measure in the sense of Dieudonn\u00e9, in exactly the same way that it gives a canonical volume form in the orientable case. In fact, Lebesgue measures in the sense of Dieudonn\u00e9 can be identified with nowhere vanishing $1$-densities, and the construction of the Riemannian measure $\\lambda_g$ is really the construction of the canonical $1$-density $\\lvert \\mathrm{vol}_g \\rvert$ associated to $g$.\n\nADDENDUM\nOne can define a measurable $k$-form on $M$ to be a map $\\omega : M \\to \\wedge^k T^\\ast M$, such that the following hold.\n\nFor every $m \\in M$, $\\omega(m) \\in \\wedge^k T^\\ast M_m$ (i.e., $\\omega$ is a set-theoretic section of $\\wedge^k T^\\ast M$).\nFor every local coordinate chart $x : U \\to x(U) \\subset \\mathbb{R}^n$, the pullback $(x^{-1})^\\ast \\omega : x(U) \\to \\wedge^k \\mathbb{R}^n$ defined by\n$$\n (x^{-1})^\\ast\\omega := \\sum_{i_1 < \\cdots < i_k} \\omega\\left(\\tfrac{\\partial}{\\partial x^{i_1}},\\dotsc,\\tfrac{\\partial}{\\partial x^{i_k}}\\right) dx^{i_1} \\wedge \\cdots \\wedge dx^{i_k}\n$$\n(with the usual abuses of notation) is measurable; this turns out to be equivalent to requiring that $\\omega(X_1,\\dotsc,X_k) : M \\to \\mathbb{R}$ be measurable (in the above sense) for any smooth vector fields $X_1,\\dots,X_k \\in \\mathfrak{X}(M)$.\n\nNow, suppose that $N$ is an oriented $k$-dimensional submanifold of $M$ (compact and without boundary, for simplicity), and let $x : U \\to x(U) \\subset \\mathbb{R}^n$ be a local coordinate chart of $M$, such that $x(N \\cap U) = V_{x,N} \\times \\{0\\}$ for some open $V_{x,N} \\subset \\mathbb{R}^k$, and such that restriction of $x$ to a diffeomorphism $N \\cap U \\to V_{x,N}$ is orientation-preserving. Then we can define\n$$\n \\int_{N \\cap U} \\omega := \\int_{V_{x,N}} \\omega\\left(\\tfrac{\\partial}{\\partial x^{1}},\\dotsc,\\tfrac{\\partial}{\\partial x^{k}}\\right) d\\lambda_{\\mathbb{R}^k}\n$$\nwhenever the Lebesgue integral on the right-hand side exists (with $\\lambda_{\\mathbb{R}^k}$ the Lebesgue measure on $\\mathbb{R}^k$). We can then define $\\omega$ to be integrable on $N$ whenever it's integrable in this way on $N \\cap U$ for any suitable local coordinate chart $x : U \\to \\mathbb{R}^n$, and then, by exactly the same arguments as in the Riemann integral case, patch these local integrals into a global Lebesgue integral $\\int_N \\omega$, which turns out to be independent of all the choices of local coordinate chart and partition of unity made along the way.", "meta": {"post_id": 2857374, "input_score": 103, "output_score": 133, "post_title": "Lebesgue measure theory vs differential forms?"}}
{"input": "I'm slightly confused on the subject of conjugates and how to define them.\nI know that for a complex number $ a - bi $ the conjugate is $ a + bi $ and similarly for $ 1 + \\sqrt 2 $ the conjugate is $ 1 - \\sqrt2 $ because when multiplied it gives a rational answer. \nBut how about for just a simple real number like 1 or 2, what would be the conjugate for this? Does a conjugate exist for a real number?\nI'm new to this topic and have tried searching Maths SE and Google in vain; any help would be appreciated.", "output": "Careful! These are two different notions of conjugate.\nFirst we have the complex conjugate, given by $\\overline{a+bi} = a-bi$. Then, since we can write a real number $x$ as $x+0i$, the complex conjugate of a real number is itself. \nThere is also a second idea of a rational conjugate, where as in your example, if $a,b$ are rational and $d$ is squarefree, the conjugate of $a+b\\sqrt{d}$ is $a-b\\sqrt{d}$. \nThere is a connection between these two ideas. In general, given a field extension $E/F$, take an algebraic element $\\alpha$ of $E$, and let $m(x)$ be it's minimal polynomial over $F$. Then we call the other roots of $m$ in $E$ the conjugates of $\\alpha$. \nIn the case of the extensions $\\mathbb{C}/\\mathbb{R}$ and  $\\mathbb{Q}(\\sqrt{d})/ \\mathbb{Q}$ this agrees with the above.", "meta": {"post_id": 2866943, "input_score": 12, "output_score": 43, "post_title": "Conjugate of real number"}}
{"input": "Consider the birthday problem. Given $N$ people, how many ways are there for there to exist some pair of people with the same birthday?\nEnumerating the possibilities quickly becomes tedious\nHowever, the complement problem (Given $N$ people, how many ways are there for no one to have the same birthday?) is trivial.\nIn fields like probability, this has obvious applications, due to the \"complement law\":\nif $A \\cup A^c = S$, where $S$ is the entire sample space, then $$P(A) + P(A^c) = 1 \\implies P(A) = 1 - P(A^c)$$\nIn general, this pattern is very common. Intuitively, I sense:\n\nsomehow, the complement problem is asking for a lot less information\nif one has something like the \"complement law\" in probability, then in some restricted scope of problems, the \"complement law\" gives in some sense, the \"same amount of information\"\n\nWhat do mathematicians call what I am getting at here? Am I overblowing how common a trend it is?", "output": "In combinatorics answering \u201cand\u201d style questions is easy because it is a multiplication. This is easy since you can remove common factors between denominators and numerators, and use the binomial/choice function.  Also any time a 1 or 0 comes up the operation becomes trivial.\nHowever asking \"or\" style questions is difficult since you have to add the numbers and then work out where you have a double count and subtract them.\nDe Morgan's laws $\\neg ( a \\vee b) = ( \\neg a \\wedge \\neg b)$ allows you to transform a \u201cor\u201d problem into a \u201cnot and\u201d problem which is easier.", "meta": {"post_id": 2869676, "input_score": 17, "output_score": 34, "post_title": "In combinatorics, why is asking the opposite problem often times easier?"}}
{"input": "I found an easy method for division and it depends on some factors.\nI wanted to find an answer for $1000/101$ with easy steps. My starting point is here. I formulated this method by 2 hours of hard work. It is an infinite series, but taking 4 or 6 units of the series we can arrive at an answer easily.\nTo find $100/11$\nWe know $100/10 = 10$, then $100/11 = 10 - 1 + 0.1 - 0.01$ (each term is the preceding term divided by $10$; division by $10$ is easy) $= 9 + 0.09$ (simplify by taking two members of the series, so easy steps) $= 9.09$\nTo find $100/12$\nWe know $100/11 = 9.09$, then $100/12 = 9.09 - 0.909 + 0.0909 - 0.00909$ (each member of the series is found by dividing by $10$ with the preceding member) giving $8.181+.08181$ (calculations are easy) or $8.29$\nI took only $4$ members of the series, and if we take $6$ members of the series, we get a better result.\nTo find $1000/101$\nWe know $1000/100 = 10$, then $1000/101 = 10 - 0.1 + 0.001 - .0001\n= 9.9 + 0.0009 = 9.9009$\nTo find $1000/102$\nWe know, $1000/101 = 9.9$ approximately, then $1000/102 = 9.9 - 0.099 + 0.00099- 0.0000099 = 9.801 + 0.0009801 = 9.801$ (approx)\nTo find $100/3$\nWe know $100/2 = 50$, then $100/3 = 50 - 25 + 12.5 - 6.25 + 3.125 - 1.5625 = 25 + 6.25 + 1.5625 = 32.8125$ or approximately $33$.\nSince they are a small number to get a perfect answer (we need to take more members of the series; for big numbers $4$ members of the series is sufficient)\nHas somebody found this before me? Where should I submit the infinite series that I have found for further evaluation?", "output": "What you are using are so called geometric series:\n$$\\frac1{1+n}=\\frac1n\\frac1{1-(-1/n)}=\\frac1n\\sum_{k=0}^{\\infty}\\left(-\\frac1n\\right)^k$$\nthus\n$$\\frac{x}{1+n}=\\frac xn\\sum_{k=0}^{\\infty}\\left(-\\frac1n\\right)^k=\\frac{x}n-\\frac{x}{n^2}+\\frac{x}{n^3}-\\dots.$$\nOf course the fact that you found the result by observation is quite impressive.", "meta": {"post_id": 2910471, "input_score": 32, "output_score": 56, "post_title": "I have found a formula for dividing numbers in easy steps"}}
{"input": "Suppose $X$ and $Y$ are two metric spaces and $f: X\\to Y$ be a continuous bijection.\nNow my question is does the completeness of $X$ and $Y$ implies $f$ to be a Homeomorphism?\n\nMy idea. First of all I try to prove $f$ to be a closed map assuming $X$ and $Y$ be a complete metric space. But this idea didn't work.\nI know if $X$ is given to be compact then whether or not $Y$ complete given initially  $f$ becomes a homeomorphism. But that is not the case here. So I try to find a counter example.\nI take $Y=\\Bbb{R}$ and try to choose $X$ to be a non compact but closed subset of $\\Bbb{R}$ (and  $\\Bbb{R}^2$) but the problem is in that situation the bijections I found was not continuous. Also I cannot found any example beyond the metric spaces $\\Bbb{R}$ or $\\Bbb{R}^2$ as my $X$.\nCan any one help me to figure out how to construct an counter example here. Thanks ...", "output": "The identity map from $\\mathbb  R$ with discrete metric into $\\mathbb  R$ with usual metric  is a continuous bijection which is not  a homeomorphism. Both spaces are complete.", "meta": {"post_id": 2912793, "input_score": 15, "output_score": 41, "post_title": "A continuous bijection between two complete metric spaces that is not a homeomorphism."}}
{"input": "I am reading this text:\n\nWhen in the real world are we checking to see if sets are vector spaces or not? The examples above seem like really specific sets...\nAre there any places where we redefined scalar multiplication like this?", "output": "You'll never have to prove something is a vector space in real life. You may want to prove something is a vector space, because vector spaces have a simply enormous amount of theory proven about them, and the non-trivial fact that you wish to establish about your specific object might boil down to a much more general, well-known result about vector spaces.\nHere's my favourite example. It's still a little artificial, but I came by it through simple curiousity, rather than any course, or the search for an example.\nConsider the logic puzzle here. It's a classic. You have a $5 \\times 5$ grid of squares, coloured black or white. Every time you press a square, it changes the square and the (up to four) adjacent squares from black to white or white to black. Your job is to press squares in such a way that you end up with every square being white.\nSo, my question to you is, can you form any configuration of white and black squares by pressing these squares? Put another way, is any $5 \\times 5$ grid of black and white squares a valid puzzle with a valid solution?\nWell, it turns out that this can be easily answered using linear algebra. We form a vector space of $5 \\times 5$ matrices whose entries are in the field $\\mathbb{Z}_2 = \\lbrace 0, 1 \\rbrace$. We represent white squares with $0$ and black squares with $1$. Such a vector space is finite, and contains $2^{25}$ vectors. Note that every vector is its own additive inverse (as is the case for any vector space over $\\mathbb{Z}_2$).\nAlso note that the usual standard basis, consisting of matrices with $0$ everywhere, except for a single $1$ in one entry, forms a basis for our vector space. Therefore, the dimension of the space is $25$.\nPressing each square corresponds to adding one of $25$ vectors to the current vector. For example, pressing the top left square will add the vector\n$$\\begin{pmatrix}\n1 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{pmatrix}.$$\nWe are trying to find, therefore, a linear combination of these $25$ vectors that will sum to the current vector (remember $-v = v$ for all our vectors $v$).\nSo, my question that I posed to you, boils down to asking whether these $25$ vectors span the $25$-dimensional space. Due to standard results in finite-dimensional linear algebra, this is equivalent to asking whether the set of $25$ vectors is linearly independent.\nThe answer is, no, they are not linearly independent. In particular, if you press the buttons highlighted in the following picture, you'll obtain the white grid again, i.e. the additive identity.\n\nTherefore, we have a non-trivial linear combination of the vectors, so they are linearly dependent, and hence not spanning. That is, there must exist certain configurations that cannot be attained, i.e. there are invalid puzzles with no solution.\nThe linear dependency I found while playing the game myself, and noticing some of the asymmetries of the automatically generated solutions, even when the problem itself was symmetric. Proving the linear dependence is as easy as showing the above picture. I still don't know of an elegant way to find an example of a puzzle that can't be solved though! So, my proof is somewhat non-constructive, and very easy if you know some linear algebra, and are willing to prove that a set is a vector space.", "meta": {"post_id": 2919706, "input_score": 19, "output_score": 68, "post_title": "Vector spaces. When in the real world are we checking if it's a vector space or not?"}}
{"input": "I'm asking this question because I was unable to find an answer elsewhere as most questions are about the summation of different irrational numbers, which is not what this question is about. Here, I'm interested in demonstrating that the result of the summation of the same irrational number is always irrational: $\\sum_{i=1}^n a$, where $n$ is a non-negative integer $>0$ and $a$ is an irrational constant.", "output": "It is trivially so. If you sum the irrational number $x$ $n$ times ($n$ being an integer \"of course\"), you end up with $nx$. \nIf $nx=\\frac ab$, with $(a,b)$ integers, then $x=\\frac{a}{nb}$, thus is rational, which is not true...", "meta": {"post_id": 2929008, "input_score": 18, "output_score": 77, "post_title": "Why is $\\sum_{i=1}^n a$ always irrational if $n>0$ and $a$ is irrational?"}}
{"input": "I was watching a video and the lecturer discusses the function\n\n$$\\frac{1}{1+x^2} = \\sum_{n=0}^{\\infty} {(-1)^n x^{2n}}$$\n$$|x| < 1$$\n\nexplaining that the radius of convergence for this Taylor series centered at $x=0$ is 1 because it is being affected by $i$ and $-i$. Then, he goes on to talk about how real analysis is a glimpse into complex analysis.\nIn the same video, the lecturer also provides the following example where a complex function is defined by using real Taylor series:\n\n\\begin{align}\ne^{ix}\n&= \\cos(x) +i \\sin(x) \\\\\n&= \\sum_{n=0}^\\infty \\frac{(ix)^n}{n!} \\\\\n&= \\sum_{n=0}^\\infty (-1)^n \\frac{x^{2n}}{2n!} + i \\sum_{n=0}^\\infty (-1)^n \\frac{x^{2n+1}}{(2n+1)!}\n\\end{align}\n\nCan someone help elaborate by what the lecturer probably meant? What is the connection between real analysis and complex analysis?\nI understand that there are two different types of analyticity: real analytic and complex analytic. Are they connected?", "output": "This is an interesting question, but one that might be hard to address completely. Let me see what I can do to help.\n\nReal Power Series:\nThe easiest way to address the connection between the two subjects is through the study of power series, as you have already had alluded to you. A power series is a particular kind of infinite sum (there are many different kinds of these) of the form\n$$\nf(x) = \\sum_{k=0}^{\\infty}a_k x^k.\n$$\nThey get their name from the fact that we are adding together powers of $x$ with different coefficients. In real analysis the argument of such a function (the \"$x$\" in $f(x)$) is taken to be a real number. And depending on the coefficients that are being multiplied with the powers of $x$, we get different intervals of convergence (intervals on which the sequence of partial sums converges.) For example, if $a_k$ is the $k$th Fibonacci number, then the radius of convergence ends up being $1/\\phi$, where $\\phi = (1+\\sqrt{5})/2$ is the \"golden ratio\".\nThe most common kind of power series that come up in calculus (and real analysis) are Taylor series or Maclaurin series. These series are created to represent (a portion) of some differentiable function. Let me try to make this a little more concrete. Pick some function $f(x)$ that is infinitely differentiable at a fixed value, say at $x=a$. The Taylor series corresponding to $f(x)$, centered at $a$, is given by\n$$\n\\sum_{k=0}^{\\infty}\\frac{f^{(k)}(a)}{k!}(x-a)^k.\n$$\nA Maclaurin series is just a Taylor series where $a=0$. After playing around with the series a bit, you may notice a few things about it.\n\nWhen $x=a$, the only non-zero term in the series is the first one: the constant $f(a)$. This means that no matter what the radius of convergence is for the series, it will at least agree with the original function at this one point.\nTaking derivatives of the power series and evaluating them at $x=a$, we see that the power series and the original function $f(x)$ have the same derivatives at $a$. This is by construction, and not a coincidence.\nIf the radius of convergence is positive, then the interval on which the series converges will converge to the original function $f(x)$. In this way, we can say that $f(x)$ \"equals\" it's Taylor series, understanding that this equality may only hold on some interval centered at $a$, and perhaps not on the entire domain that $f(x)$ is defined on. You've already seen such an example with $(1+x^2)^{-1}$. In some extreme cases, such as with $\\sin x$ and $\\cos x$ as mentioned above, this equality ends up holding for ALL real values of $x$, and so the convergence is on all of $\\mathbb{R}$ and there is no harm completely equating the function with it's Taylor series.\nEven if the radius of convergence of a particular Taylor series (centered at $a$) is finite, it does not mean you cannot take other Taylor series (centered at other values than $a$) that also have a positive radius of convergence. For example, even though the Maclaurin series for $(1+x^2)^{-1}$ does not converge outside of the interval of radius 1, you can compute the Taylor series centered at $a=1$,\n\\begin{align}\n\\frac{1}{1+x^2} &= \\sum_{k=0}^{\\infty} \\left( \\frac{1}{k!}  \\frac{d^k}{dx^k}\\left(\\frac{1}{1+x^2}\\right)\\Bigg|_{x=1}(x-1)^k \\right) \\\\\n&= \\frac{1}{2}-\\frac{1}{2}(x-1)+\\frac{1}{2}(x-1)^2-3(x-1)^4+\\cdots, \\quad \\text{for $|x-1|<\\sqrt{2}$}.\n\\end{align}\n\nYou will then find that this new power series converges on a slightly larger radius than 1 (that's the $\\sqrt{2}$ mentioned above), and that the two power series (one centered at 0, the other centered at 1) overlap and agree for certain values of $x$.\nThe big take away that you should have about power series and Taylor series is that they are one and the same. Defined a function by a power series, and then take it's Taylor series centered at the same point; you will get the same series. Conversely, any infinitely-differentiable function that has a Taylor series with positive radius of convergence is uniquely determined by that power series.\nThis is where complex analysis beings to come into play...\n\nComplex Power Series:\nComplex numbers have many similar properties to real numbers. They form and algebra (you can do arithmetic with them); the only number you still cannot divide by is zero; and the absolute value of a complex number still tells you the distance from 0 that number is. In particular there is nothing stopping you from defining power series of complex numbers, with z=x+iy:\n$$\nf(z) = \\sum_{k=0}^{\\infty}c_k z^k.\n$$\nThe only difference now is that the coefficients $c_k$ can be complex numbers, and the radius of convergence is now relating to the radius of a circle (as opposed to the radius of an interval). Things may seem exactly like in the real-valued situation, but there is more lurking beneath the surface.\nFor starters, let me define some new vocabulary terms.\n\nWe say that a complex function is complex differentiable at some fixed value $z=w$ if the following limit exists:\n$$\n\\lim_{z\\to w}\\frac{f(z)-f(w)}{z-w}.\n$$\nIf this limit exists, we denote the value of the limit by $f'(w)$. This should look familiar, as it is the same limit definition for real-valued derivatives.\nIn the same way that we could create Taylor series for real-valued functions, we can also create Taylor series  (center at $z=w$) for complex-valued functions, provided the functions have an infinite number of derivatives at $w$ (sometimes referred to as being holomorphic at $w$):\n$$\n\\sum_{k=0}^{\\infty}\\frac{f^{(k)}(w)}{k!}(z-w)^k.\n$$\nIf the Taylor series defined above has a positive radius of convergence then we say that $f$ is analytic at $w$. This vocabulary is also used in the case of real-valued Taylor series.\n\nPerhaps the biggest surprise in complex analysis is that the following conditions are all equivalent:\n\n$f(z)$ is complex differentiable at $z=w$.\n$f(z)$ is holomorphic at $w$. That is $f$ has an infinite number of derivatives at $z=w$. In real analysis this condition is sometimes referred to as being \"smooth\".\n$f(z)$ is analytic at $z=w$. That is it's Taylor series converges to $f(z)$ with some positive radius of convergence.\n\nThis means that being differentiable in the complex sense is a much harder thing to accomplish than in the real sense. Consider the contrast with the \"real-valued\" equivalents of the points made above:\n\nIn real analysis there are a number of functions with only finitely many derivatives. For example, $x^2 \\sin(1/x)$ has only one derivative at $x=0$, and $f'(x)$ is not even continuous, let alone twice differentiable.\nThere are also real-valued functions that are smooth (infinitely-differentiable) yet do not have a convergent Taylor series. An example is the function $e^{-1/x^2}$ which is smooth for every $x$, but for which every order of derivative at $x=0$ is equal to zero; this means every coefficient in the Maclaurin series is zero, and the radius of convergence is zero as well.\n\nThese kinds of pathologies do not occur in the complex world: one derivative is as good as an infinite number of derivatives; differentiability at one point translates to differentiability on a neighborhood of that point.\n\nLaurent  Series:\nA natural question that one might ask is what dictates the radius of convergence for a power series? In the real-valued case things seemed to be fairly unpredictable. However, in the complex-valued case things are much more elegant.\nLet $f(z)$ be differentiable at some point $z=w$. Then the radius of convergence for the Taylor series of $f(z)$ centered at $w$ will be the distance to the nearest complex number at which $f(z)$ fails to be differentiable. Think of it like dropping a pebble into a pool of water. The ripples will extend radially outward from the initial point of differentiability, all the way until the circular edge of the ripple hits the first \"singularity\" -- a point where $f(z)$ fails to be differentiable.\nTake the complex version of our previous example,\n$$\nf(z) = \\frac{1}{1+z^2}.\n$$\nThis is a ration function, and will be smooth for all values of $z$ where the denominator is non-zero. Since the only roots of $z^2 + 1$ are $z = i$ and $z = -i$, then $f(z)$ is differentiable/smooth/analytic at all values $w\\neq \\pm i$. This is precisely why the radius of convergence for the real-valued Maclaurin series is 1, as you've already noted: the shortest distance from $z=0$ to $z=\\pm i$ is 1. The real-valued Maclaurin series is just a \"snapshot\" or \"sliver\" of the complex-valued Taylor series centered at $z=0$. This is also why the radius of convergence for the real-valued Taylor series increases when you move away from zero; the distance to $\\pm i$ becomes greater, and so the complex-valued Taylor series can converge on a larger disk.\nSo now should come the question: when exactly does a complex function fail to be differentiable?\nWithout going into too many details from complex analysis, suffice it to say that complex functions fail to be differentiable when one of three things occurs:\n\nThe function has a \"singularity\" (think division by zero).\nThe function is defined in terms of the complex conjugate, $\\bar{z}$. For example, $f(z) = |z|^2 = z\\,\\bar{z}$.\nThe function involves logarithms or non-integer exponents (these two ideas are actually related).\n\nNumber 2. is perhaps the most egregious of the three issues, and it means that functions like $f(z) = |z|$ are actually differentiable nowhere. This is in stark contrast to the real-valued version $f(x) = |x|$, which is differentiable everywhere except at $x=0$ where there is a \"corner\".\nNumber 3. is actually not too bad, and it turns out that these kinds of functions are usually differentiable everywhere except along certain rays or line segments. To get into it further, however, will take us too far off course.\nNumber 1. is the best case scenario, and is the focus of this section of our discussion. Essentially, singularities are places where division by zero has occurred, and the extend to which something has \"gone wrong\" can be quantified. Let me try to elaborate.\nConsider the previous example of $f(z) = (1+z^2)^{-1}$. Again, since the denominator can factor into the product $(z+i)(z-i)$, then this means we could \"erase\" the singularity at $z=i$ by multiplying the function by a copy of $z-i$. In other words if\n$$\n g(z) = (z+i)f(z) = \\frac{z-i}{1+z^2},\n$$\nthen $g(z) = z+i$ for all $z\\neq i$, and $\\lim_{z\\to i}g(z) = 2i$ exists, and is no longer a singularity.\nSimilarly, if $f(z) = 1/(1+z^2)^3$, then we again have singularities at $z=\\pm i$. This time, however, multiplying $f(z)$ by only one copy of $z-i$ will not remove the singularity at $z=i$. Instead, we would need to multiply by three copies to get\n$$\n g(z) = (z-i)^3 f(z) = \\frac{(z-i)^3}{(1+z^2)^3},\n$$\nwhich again means that $g(z) = (z+i)^3$ for all $z\\neq i$, and that $\\lim_{z\\to i}g(z) = (2i)^3 = -8i$ exists.\nSingularities like this --- ones that can be \"removed\" through the use of multiplication of a finite number of linear terms --- are called poles. The order of the pole is the minimum number of liner terms need to remove the singularity.\nThe real-valued Maclaurin series for $\\sin x$ is given by\n$$\n \\sin x = \\sum_{k=0}^{\\infty} \\frac{(-1)^{k}}{(2k+1)!}x^{2k+1},\n$$\nand has a infinite radius of convergence. This means that the complex version\n$$\n \\sin z = \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{(2k+1)!}z^{2k+1} = z - \\frac{1}{3!}z^3 + \\frac{1}{5!}z^5 - \\cdots\n$$\nalso has an infinite range of convergence (such functions are called entire) and hence no singularities. From here it's easy to see that the function $(\\sin z)/z$ is analytic as well, with Taylor series\n$$\n\\frac{\\sin z}{z} = \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{(2k+1)!}z^{2k} = 1 - \\frac{1}{3!}z^2 + \\frac{1}{5!}z^4 - \\cdots\n$$\nHowever, a function like $(\\sin z)/z^3$ is not analytic at $z=0$, since dividing $\\sin z$ by $z^3$ would give us the following expression:\n$$\n \\frac{\\sin z}{z^3} = \\frac{1}{z^2} - \\frac{1}{3!} + \\frac{1}{5!}z^2 - \\cdots\n$$\nBut notice that if we were to subtract the term $1/z^2$ from both sides we would be left again with a proper Taylor series\n$$\n \\frac{\\sin z}{z^3} - \\frac{1}{z^2} = \\frac{\\sin z - z}{z^3} = \\frac{1}{3!} + \\frac{1}{5!}z^2 - \\frac{1}{7!}z^4 + \\cdots\n$$\nThis idea of extending the idea of Taylor series to include terms with negative powers of $z$ is what is referred to a Laurent series. A Laurent series is a power series in which the powers of $z$ are allowed to take on negative values, as well as positive:\n$$\n f(z) := \\sum_{k = -\\infty}^{\\infty} c_k z^k\n$$\nIn this way we can expand complex functions around singular points in a fashion similar to expanding around analytic points.\nA pole, it turns out, is a singular point for which there are a finite number of terms with negative powers, such as with $(\\sin z)/z^3$. If, however, an infinite number of negative powers are needed to fully express a Laurent series, then this type of singular point is called an essential singularity. An excellent example of such a function can be made by taking an analytic function (one with a Taylor series) and replacing $z$ by $1/z$:\n\\begin{align}\n\\sin(1/z) &= \\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k+1)!}(1/z)^k \\\\\n&= \\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k+1)!}z^{-k} \\\\\n&= \\sum_{k=-\\infty}^{0}\\frac{(-1)^{-k}}{(1-2k)!}z^{k}\n\\end{align}\nThese kinds of singularities are quite severe and the behavior of complex functions around such a point is rather erratic. This also explains why the real-valued function $e^{-1/x^2}$ was so pathological. The Taylor series for $e^z$ is given by\n$$\ne^z = \\sum_{k=0}^{\\infty}\\frac{1}{k!}z^k\n$$\nand so\n\\begin{align}\ne^{-1/z^2} &= \\sum_{k=0}^{\\infty}\\frac{1}{k!}(-1/z^2)^k \\\\\n&= \\sum_{k=0}^{\\infty}\\frac{1}{k!}(-1)^k z^{-2k} \\\\\n&= \\sum_{k=-\\infty}^{0}\\frac{1}{(-k)!}(-1)^{-k} z^{2k}.\n\\end{align}\nHence there is an essential singularity at $z=0$, and so even though the real-valued version is smooth at $x=0$, there is no hope of differentiability in a disk around $z=0$.", "meta": {"post_id": 2936156, "input_score": 29, "output_score": 44, "post_title": "Are real and complex analysis related in any way?"}}
{"input": "Hi Im fairly new to SDE theory and am struggling with the difference between a weak ( or martingale ) solution and a strong solution to an SDE : \n$$ d(X_{t})=b(t,X_{t})dt + \\sigma(t,X_{t})dW_{t}  $$\nAre these two differences and what do they really mean in detail?\n\nFor a strong solution we are given an initial value, whereas for weak solutions only a probability law?\nFor strong solutions we know what probability space we are working in and have a Brownian Motion $W$ in that space. For a weak solution we can only say that there exists some probability space where the SDE holds (with a new brownian motion in the space). \n\nAs you can tell I am confused with this topic some clarifications would be amazing.", "output": "The main difference between weak and strong solutions is indeed that for strong solutions we are given a Brownian motion on a given probability space whereas for weak solutions we are free to choose the Brownian motion and the probability space.\n\nDefinition: Let $(B_t)_{t \\geq 0}$ be a Brownian motion with admissible filtration $(\\mathcal{F}_t)_{t \\geq 0}$. A progressively measurable process $(X_t,\\mathcal{F}_t)$ is a strong solution with initial condition $\\xi$ if $$X_t-X_0 = \\int_0^t \\sigma(s,X_s) \\, dB_s + \\int_0^t b(s,X_s) \\, ds, \\qquad X_0 =\\xi \\tag{1}$$ holds almost surely for all $t \\geq 0$.\nDefinition: A stochastic process $(X_t,\\mathcal{F}_t)$ on some probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ is called a weak solution with initial distribution $\\mu$ if there exists a Brownian motion $(B_t)_{t \\geq 0}$ on $(\\Omega,\\mathcal{F},\\mathbb{P})$ such that $(\\mathcal{F}_t)_{t \\geq 0}$ is an admissible filtration, $\\mathbb{P}(X_0 \\in \\cdot) = \\mu(\\cdot)$ and $$X_t-X_0 = \\int_0^t \\sigma(s,X_s) \\, dB_s + \\int_0^t b(s,X_s) \\, ds$$ holds almost surely for all $t \\geq 0$.\n\nAs a consequence of these definitions, we have to consider different notions of uniqueness. For strong solutions we are typically looking for pathwise unique solutions, i.e. if $(X_t^{(1)})_{t \\geq 0}$ and $(X_t^{(2)})_{t \\geq 0}$ are strong solutions to $(1)$ with the same initial condition, then pathwise uniqueness means $$\\mathbb{P} \\left( \\sup_{t \\geq 0} |X_t^{(1)}-X_t^{(2)}|=0 \\right)=1.$$ As the following simple example shows it doesn't make sense to talk about pathwise uniqueness of weak solutions.\n\nExample 1: Let $(W_t^{(1)})_{t \\geq 0}$ and $(W_t^{(2)})_{t \\geq 0}$ be two Brownian motions (possibly defined on different probability spaces), then both $X_t^{(1)} := W_t^{(1)}$ and $X_t^{(2)} := W_t^{(2)}$ are weak solutions to the SDE $$dX_t = dB_t, \\qquad X_0 = 0$$ Why? According to the definition we are free choose the driving Brownian motion, so we can set $B_t^{(1)} := W_t^{(1)}$ and $B_t^{(2)} := W_t^{(2)}$, respectively, and then $$dX_t^{(i)} = dB_t^{(i)} \\quad \\text{for $i=1,2$}.$$\n\nWhat do we learn from this? Since weak solutions might be defined on different probability spaces, there is no (immediate) way to compute probabilities of the form $\\mathbb{P}(X_t^{(1)}=X_t^{(2)})$ for two weak solutions $(X_t^{(1)})_{t \\geq 0}$ and $(X_t^{(2)})_{t \\geq 0}$, and therefore we cannot even attempt to talk about pathwise uniqueness. For the same reason, it doesn't make sense to talk about pointwise initial conditions $\\xi$ for weak solutions (... for this we would need to fix some probability space on which $\\xi$ lives...); instead we only prescribe the initial distribution of $X_0$.\nThe next example shows that we cannot expect to have pathwise uniqueness even if the weak solutions are defined on the same probability space.\n\nExample 2: Let $(W_t)_{t \\geq 0}$ be a Brownian motion. It follows from Example 1 that $X_t^{(1)} := W_t$ and $X_t^{(2)} := -W_t$ are weak solutions to the SDE $$dX_t = dB_t, \\qquad X_0 =0.$$ Clearly, $\\mathbb{P}(X_t^{(1)} = X_t^{(2)}) = \\mathbb{P}(W_t=0)=0$.\n\nThe \"good\" notion of uniqueness for weak solutions is weak uniqueness, i.e. uniqueness in distribution (= the solutions have the same finite-dimensional distributions).\nTypically it is much easier to prove the existence (and/or uniqueness of) a weak solution the the existence (and/or uniqueness) of a strong solution.\n\nExample 3: The SDE $$dX_t = - \\text{sgn}\\,(X_t) \\, dB_t, \\qquad X_0 = 0 \\tag{2}$$ has a weak solution but no strong solution.\n\nLet's prove that the SDE has a weak solution. Let $(X_t,\\mathcal{F}_t)_{t \\geq 0}$ be some Brownian motion and define $$W_t := -\\int_0^t \\text{sgn} \\, (X_s) \\, dX_s.$$ It follows from L\u00e9vy's characterization that $(W_t,\\mathcal{F}_t)$ is also a Brownian motion. Since $$dW_t = - \\text{sgn} \\, (X_t) \\, dX_t$$ implies $$dX_t = - \\text{sgn} \\, (X_t) \\, dW_t$$ this means that $(X_t)_{t \\geq 0}$ is a weak solution to $(2)$. For a proof that a strong solution does not exist see e.g. Example 19.16 in the book by Schilling & Partzsch on Brownian motion.\nLet me finally mention that weak solutions are closely related to martingale problems; in this answer I tried to give some insights on the connection between the two notions.", "meta": {"post_id": 2987743, "input_score": 43, "output_score": 50, "post_title": "Difference between weak ( or martingale ) and strong solutions to SDEs"}}
{"input": "Cauchy-Schwarz inequality applied to Trace of two products $\\mathbf{Tr}(A'B)$ has the form\n$$\n\\mathbf{Tr}(A'B) \\leq \\sqrt{\\mathbf{Tr}(A'A)} \\sqrt{\\mathbf{Tr}(B'B)}\n$$\nI saw many places where people use this inequality.  But did not see a formal proof.  Is it difficult to prove ?  Anyone can give a simple proof ?", "output": "The Cauchy-Schwarz inequality is valid for any inner product, so you just need to show $\\operatorname{\\textbf{Tr}}A'B$ is an inner product. It's clearly bilinear (or sesquilinear if by $'$ you meant a complex adjoint), with $$\\operatorname{\\textbf{Tr}}A'A=\\sum_i (A'A)_{i}=\\sum_{ij}A'_{ij}A_{ji}.$$Depending on whether you're working with the real or complex case, this quantity is either $\\sum_{ij}A_{ji}^2$ or $\\sum_{ij}|A_{ji}|^2$. Either way it's non-negative, completing the proof.", "meta": {"post_id": 3006109, "input_score": 11, "output_score": 36, "post_title": "Proof for Cauchy-Schwarz inequality for Trace"}}
{"input": "When trying to solve a physics problem on decoupling a system of ODEs, I found myself needing to address the following problem:\n\nLet $A_n\\in M_n(\\mathbb R)$ be the matrix with all $1$s above its main diagonal, all $-1$s below its diagonal, and $0$s everywhere else. Is $A_n$ always diagonalisable? If so, what is its diagonalisation (equivalently: what are its eigenvalues and corresponding eigenvectors)?\n\nFor example,\n$$A_3=\\begin{bmatrix}0&1&0\\\\-1&0&1\\\\0&-1&0\\end{bmatrix},\\quad A_5=\\begin{bmatrix}0&1&0&0&0\\\\-1&0&1&0&0\\\\0&-1&0&1&0\\\\0&0&-1&0&1\\\\0&0&0&-1&0\\end{bmatrix}.$$\n\nAssuming my code is correct, Mathematica has been able to verify that $A_n$ is always diagonalisable up to $n=1000$. If we use $\\chi_n(t)\\in\\mathbb Z[t]$ to denote the characteristic polynomial of $A_n$, a straightforward evaluation also shows that\n$$\\chi_n(t)=-t\\chi_{n-1}(t)+\\chi_{n-2}(t)\\tag{1}$$\nfor all $n\\geq4$. Furthermore, note that $A_n=-A_n^t$ so that, in the case where the dimension is even,\n$$\\det(A_{2n}-\\lambda I)=\\det(A_{2n}^t-\\lambda I)=\\det(-A_{2n}-\\lambda I)=\\det(A_{2n}+\\lambda I).$$\nThis implies that whenever $\\lambda$ is an eigenvalue of $A_{2n}$, so is $-\\lambda$. In other words, $\\chi_{2n}(t)$ is always of the form $(t^2-\\lambda _1^2)(t^2-\\lambda_2^2)\\dotsm(t^2-\\lambda_n^2)$ for some $\\lambda_i$.\nAnd this is where I am stuck. In order for $A_n$ to be diagonalisable, we must have that all the eigenvalues are distinct, but trying to use the recurrence $(1)$ and strong induction, or trying to use the formula for the even case have not helped at all. It seems like the most probable line of attack would be to somehow show that\n$$\\chi_{2n}'(t)=2t\\sum_{k=1}^n\\frac{\\chi_{2n}(t)}{t^2-\\lambda_k^2}$$\nnever shares a common zero with $\\chi_{2n}$ (which would resolve the even case), though I don't see how to make this work.\n\nNote: I do not have any clue how to actually find the eigenvalues/eigenvectors even in the case where the $A_n$ are diagonalisable. As such even if someone cannot answer the second part of the question, but can prove that the $A_n$ are diagonalisable, I would appreciate that as an answer as well. Above I tried to look at the special case where the dimension is even, though of course the proof for all odd and even $n$ is more valuable. Even if this is not possible, for my purposes I just need an unbounded subset $S\\subseteq\\mathbb Z$ for which the conclusion is proven for $n\\in S$, so any such approach is welcome too.\nThank you in advance!", "output": "All those matrices are anti-symmetric and therefore they are normal matrices. And every normal matrix is diagonalizable over $\\mathbb C$, by the spectral theorem.", "meta": {"post_id": 3198918, "input_score": 14, "output_score": 36, "post_title": "Are these square matrices always diagonalisable?"}}
{"input": "Prove that there is no group $G$ s.t. $\\operatorname{Aut}(G)=\\mathbb{Q}$\nI get the feeling that we should proceed by contradiction. \nSo let $G$ be a group s.t. $\\operatorname{Aut}(G)=\\mathbb{Q}$. Then we can identify elements of $\\mathbb{Q}$ with automorphisms of $G$... and identities such as $\\frac{1}{2}*2(g)=1(g)=g$\nCan somebody help me find a contradiction?", "output": "Suppose $G$ is a group with $\\operatorname{Aut}(G)\\cong\\mathbb{Q}$.  If $G$ is abelian, then $f(x)=-x$ is an automorphism of $G$ which satisfies $f^2=1$.  But $\\mathbb{Q}$ is torsion-free, so this implies $f=1$.  But then $G$ is a vector space over $\\mathbb{Z}/(2)$, and so its automorphism group is nonabelian if its dimension is greater than $1$ and finite otherwise.\nSo, $G$ must be nonabelian; say $x,y\\in G$ do not commute.  Now note that $G$ acts on itself by conjugation, and this gives a homomorphism $\\varphi:G\\to\\operatorname{Aut}(G)\\cong\\mathbb{Q}$ whose kernel is $Z(G)$, the center of $G$.  Note that the subgroup of $\\mathbb{Q}$ generated by $\\varphi(x)$ and $\\varphi(y)$ is cyclic (since every finitely generated subgroup of $\\mathbb{Q}$ is cyclic); say it is generated by $\\varphi(a)$ for some $a\\in G$.  Then there are $m,n\\in\\mathbb{Z}$ and $z,z'\\in Z(G)$ such that $x=a^nz$ and $y=a^mz'$.  But now we see that $x$ and $y$ actually do commute (since $z$ and $z'$ commute with everything), so we have a contradiction.", "meta": {"post_id": 3210220, "input_score": 20, "output_score": 34, "post_title": "Prove that there is no group $G$ s.t. $\\operatorname{Aut}(G)=\\mathbb{Q}$"}}
{"input": "Suppose, I want to find a function such that its Taylor series expansion is \n$$f(x) = \\sum_{n=0}^{\\infty}\\frac{x^{n+1}}{(n+1)a^n}$$\nI could start with $$\\frac{1}{1-x}=\\sum_{n=0}^{\\infty}x^n$$\nIntegrate it, substitute $x\\rightarrow  \\frac{x}{a}$, multiply by $a$ and get\n$$F(x) = -\\ln|x-1| = \\sum_{n=0}^{\\infty}\\frac{x^{n+1}}{n+1}$$\n$$a F\\left(\\frac{x}{a}\\right) = -a \\ln\\left|\\frac{x}{a}-1\\right| = \\sum_{n=0}^{\\infty}\\frac{x^{n+1}}{(n+1)a^n}$$\nOn the other hand, I could start with subtituting $x \\rightarrow \\frac{x}{a}$ before integration to get\n$$\\frac{a}{a-x} = \\sum_{n=0}^{\\infty}\\frac{x^n}{a^n}$$\nand then integrate it to get\n$$-a\\ln|x-a| = \\sum_{n=0}^{\\infty}\\frac{x^{n+1}}{(n+1)a^n}$$ \nAs you can see, arguments of $\\ln$ are not equal. Where did it go wrong?", "output": "When you integrate, you should include a constant of integration. What you see here is that when integrating the functions, you get different constants of integration. This is why your answers differ by only a constant, namely $a\\ln a$ (you can see this by use of $\\log$ rules).\nIf you take care with the limits or boundary conditions in the integration step, then the answers will agree exactly.", "meta": {"post_id": 3228101, "input_score": 18, "output_score": 39, "post_title": "Taylor series leads to two different functions - why?"}}
{"input": "For all my homework in real analysis, when I've been asked to show that a function is continuous, I just found a single $x_n \\in D$ and showed that when $x_n \\rightarrow x_0$, $f(x_n) \\rightarrow f(x_0)$. Apparently, the sequence definition (as opposed to the epsilon delta definition) is (basically) only used to prove a function is not continuous, and I can't prove a function is continuous because then I'd have to show this is true for all possible sequences? Am I doing the math wrongly? Should I always use the epsilon delta definition when trying to prove that a function is continuous?", "output": "This is indeed incorrect. Take for example the function \n$$f(x) = \\begin{cases}\n1, \\text{ if } x\\in \\mathbb{Q}\\\\\n0, \\text{ otherwise}\n\\end{cases}$$ \nThis is obviously not a continuous function. However, if you look at its behavior along a sequence of rational points, it would appear to be constant (hence continuous).", "meta": {"post_id": 3240987, "input_score": 22, "output_score": 35, "post_title": "Is showing that $x_n \\rightarrow x_0\\Rightarrow f(x_n) \\rightarrow f(x_0)$ for a single sequence enough to prove continuity?"}}
{"input": "I am now confused with such problem as title goes. To be exact, the problem is\n\nDoes there exist a functor from $A:\\mathsf{Field}\\to \\mathsf{Field}$ with a natural transformation from identity functor $\\iota: \\operatorname{id}\\to A$ such that for each $F$, $A(F)$ is the algebraically closure of $F$ through $\\iota_F:F\\to A(F)$?\n\nIt is not easy rather than first glimpse. Let me explain.\nNote that, the existence of algebraic closure only ensures that there exist a map from $\\operatorname{Obj}(\\mathsf{Field})$ to itself. Since the \"extension\" property is not unique, it is not generally true that we can extend the map to $\\operatorname{Mor}(\\mathsf{Field})$ for arbitrary choice of algebraic closure.\n\nFor example, consider the fields\n$$\\begin{array}{ccc}\n\\mathbb{Q}[\\sqrt[3]{2}, \\sqrt{2}] &\\to & \\mathbb{Q}[\\omega\\sqrt[3]{2}, \\sqrt{2}]\\\\\n\\uparrow &&\\uparrow \\\\\n\\mathbb{Q}[\\sqrt[3]{2}] & \\to & \\mathbb{Q}[\\omega\\sqrt[3]{2}]\n\\end{array}$$\nIf we choose the algebraically closure of $\\left[\\begin{matrix}\\mathbb{Q}[\\sqrt[3]{2}, \\sqrt{2}] & \\mathbb{Q}[\\omega\\sqrt[3]{2}, \\sqrt{2}]\\\\ & \\mathbb{Q}[\\omega\\sqrt[3]{2}]\\end{matrix}\\right]$ by inclusion to $\\overline{\\mathbb{Q}}$, and the closure of $\\mathbb{Q}[\\sqrt[3]{2}]\\to \\overline{\\mathbb{Q}}$ by $\\sqrt[3]{2}\\mapsto \\omega\\sqrt[3]{2}$. \nWe cannot extend a well-defined functor. Similar problem exists for transcendental extension, for example, square like this\n$$\\begin{array}{ccc}\n\\mathbb{C}[X,Y] &\\to & \\mathbb{C}[X^2,Y]\\\\\n\\uparrow &&\\uparrow \\\\\n\\mathbb{C}[X] & \\to & \\mathbb{C}[X^2]\n\\end{array}$$\nA reasonable method is to avoid phenomenon above is as follow. \nFix an algebraically closed field $F$, and take all of its subfields as \"skeleton\", then fix an isomorphism to a subfields of $F$ from all fields whose algebraic closure is $F$ up to an isomorphism. The isomorphic class of algebraically closure are completely dependen by its characteristic and the transcendental dimension over prime field $\\mathbb{Q}$ or $\\mathbb{F}_p$. \nNow the problem is how to naturally chose extensions for endmorphisms. But unfortunately, the choice is fragile. For instance, consider the following diagram\n$$\\begin{array}{ccccl}\n\\mathbb{Q}[\\sqrt{3}, \\sqrt{2}] &\\to & \\mathbb{Q}[\\sqrt{3}, \\sqrt{2}] &: &\\sqrt{3}\\mapsto -\\sqrt{3},\\sqrt{2}\\mapsto \\pm \\sqrt{2}\\\\\n\\uparrow &&\\uparrow \\\\\n\\mathbb{Q}[\\sqrt{3}] & \\to & \\mathbb{Q}[\\sqrt{3}] &:&\\sqrt{3}\\mapsto -\\sqrt{3}\n\\end{array}$$\nThere is no suitable choice such that \n$$\\begin{array}{ccccl}\n\\overline{\\mathbb{Q}} &\\to & \\overline{\\mathbb{Q}} &: &\\sqrt{3}\\mapsto -\\sqrt{3},\\sqrt{2}\\mapsto \\pm \\sqrt{2}\\\\\n\\parallel &&\\parallel \\\\\n\\overline{\\mathbb{Q}}& \\to & \\overline{\\mathbb{Q}} &:&\\sqrt{3}\\mapsto -\\sqrt{3}\n\\end{array}$$\ncommutes for both $\\pm=+$ and $\\pm=-$.", "output": "No, this is not possible.  For instance, let $K$ be any field with a automorphism $f:K\\to K$ whose order is finite and greater than $2$.  Then $A(f):A(K)\\to A(K)$ would be an automorphism of the same order extending $f$.  But no such automorphism exists: by the Artin-Schreier theorem, any finite-order automorphism of an algebraically closed field has order at most $2$.\nOr without using any big theorems, you can find problems just looking at finite extensions.  For instance, if $f$ is the Frobenius automorphism of $\\mathbb{F}_{p^2}$ then $F(f)$ is an extension to an algebraic closure which still has order $2$.  Since $\\mathbb{F}_{p^4}$ is normal over $\\mathbb{F}_{p}$, $F(f)$ restricts to an automorphism of $\\mathbb{F}_{p^4}$, which must be the Frobenius squared in order to have order $2$.  But the Frobenius squared does not restrict to $f$ on $\\mathbb{F}_{p^2}$, so this is a contradiction.", "meta": {"post_id": 3300131, "input_score": 26, "output_score": 40, "post_title": "Can \"Taking algebraic closure\" be made into a functor?"}}
{"input": "From Halmos's Naive Set Theory, section 1:\n\nObserve, along the same lines, that inclusion is transitive, whereas belonging is not. Everyday examples, involving, for instance, super-organizations whose members are organizations, will readily occur to the interested reader.\n\nBelonging seems transitive. Can someone explain?", "output": "The difference between $\\subset$ and $\\in$ is that the former applies to expressions at the same level of nesting and the latter applies to expressions at one level of nesting apart from each other. So when you chain two $\\in$'s together you get something at two levels of nesting, which is not in general comparable to a single $\\in$. On the other hand, since $\\subset$ doesn't change the level of nesting it doesn't have this problem.\nThis is the idea behind the example given in other answers of\n$$\n\\varnothing\\in \\{\\varnothing\\}\\in \\{\\{\\varnothing\\}\\},\\qquad \\varnothing \\not\\in \\{\\{\\varnothing\\}\\}.\n$$", "meta": {"post_id": 3355751, "input_score": 23, "output_score": 47, "post_title": "Why is belonging not transitive?"}}
{"input": "Given some function involving two variables, $\\mathit p(x,y)$, is the formula $$\\mathit \\exists!x\\exists yp(x,y)$$ equivalent to $$\\mathit\\exists y\\exists!xp(x,y)$$\nI have tried writing out the formal definition for the unique existential quantifier, but I get stuck simplifying the statement when the unique quantifier is first. I have also tried giving $\\mathit p(x,y)$ some meaning and talking through it, but I can't decide if they mean the same thing. Any help is appreciated!", "output": "No, they do not commute.\nConsider for example the nonnegative reals as a linear order. Then $$\\exists x\\exists !y(y\\le x)$$ is true (take $x=0$), but $$\\exists!y \\exists x(y\\le x)$$ is false since for every $y$ there is some $x$ with $y\\le x$.\n\nThis becomes less surprising when we \"unpack\" $\\exists!$. There are a few ways to do this, but I think the simplest is to view \"$\\exists !x\\varphi(x,...)$\" as an abbreviation for \"$\\exists x\\forall z(\\varphi(z,...)\\leftrightarrow x=z)$.\" Then we have:\n\n\"$\\exists x\\exists!y\\varphi(x,y)$\" is an abbreviation for $$\\exists x\\exists y\\forall z(\\varphi(x,z)\\leftrightarrow y=z),$$\nwhile \"$\\exists!y\\exists x\\varphi(x,y)$\" is an abbreviation for $$\\exists y\\forall z[(\\exists x\\varphi(x,z))\\leftrightarrow y=z].$$\n\nThis has a clear instance of swapping $\\forall$ and $\\exists$ (in fact, it's even messier than that - unpack \"$\\leftrightarrow$\" ...), so we should expect them to be inequivalent in general.\n(There are of course other ways to unpack \"$\\exists!$,\" but they'll all yield the same general picture: despite the symbol suggesting only existentiality, it's hiding a very important universal quantifier, and so the usual dangers of $\\forall/\\exists$-switching carry over to $\\exists!/\\exists$-switching.)", "meta": {"post_id": 3528948, "input_score": 24, "output_score": 40, "post_title": "Does the unique existential quantifier commute with the existential quantifier?"}}
{"input": "I was sitting in analysis yesterday and, naturally, we took the limit of some expression. It occurred to me that \"taking the limit\" of some expression abides the rules of a linear transformation\n$$\\lim_{x \\rightarrow k}\\ c(f(x)+g(x)) = c \\lim_{x \\rightarrow k} f(x) + c\\ \\lim_{x \\rightarrow k} g(x),$$\nand (my group theory is virtually non existent) appears also to be a homomorphism:\n$$\\lim_{x \\rightarrow k} (fg)(x) = \\lim_{x \\rightarrow k} f(x)g(x), $$\netc.\nAnyway, my real question is, what mathematical construct is the limit?", "output": "In general, let $X, Y$ be topological spaces, and $x_0$ a non-isolated point of $X$.  Then strictly speaking, \"$\\lim_{x\\to x_0} f(x) = L$\" is a relation between functions $f : X \\to Y$ and points $L \\in Y$ (the equality notation being misleading in general).\nNow, if $Y$ is a Hausdorff topological space, it happens that this relation is what is known as a partial function: for any $f : X \\to Y$, there is at most one $L \\in Y$ such that $\\lim_{x\\to x_0} f(x) = L$.  Now, for any relation $R \\subseteq (X \\to Y) \\times Y$ which is a partial function, we can define a corresponding function $\\{ f \\in (X \\to Y) \\mid \\exists y \\in Y, (f, y) \\in R \\} \\to Y$ by sending $f$ satisfying this condition to the unique $y$ with $(f, y) \\in R$.  Then that somewhat justifies the \"equality\" in the notation $\\lim_{x\\to x_0} f(x) = L$, though you still need to keep in mind that it is a partial function where $\\lim_{x\\to x_0} f(x)$ is not defined for all $f$.  (This part relates to the answer by Jos\u00e9 Carlos Santos.)\nBuilding on top of this, in the special case of $Y = \\mathbb{R}$, we can put a ring structure on $X \\to Y$ by pointwise addition, pointwise multiplication, etc.  Then $\\{ f : X \\to \\mathbb{R} \\mid \\exists L \\in \\mathbb{R}, \\lim_{x\\to x_0} f(x) = L \\}$ turns out to be a subring of $X \\to \\mathbb{R}$, and the induced function from this subring to $\\mathbb{R}$ is a ring homomorphism.  (More generally, this will work if $Y$ is a topological ring.  Similarly, if $Y$ is a topological vector space, then the set of $f$ with a limit at $x_0$ is a linear subspace of $X \\to Y$ and the limit gives a linear transformation; if $Y$ is a topological group, you get a subgroup of $X \\to Y$ and a group homomorphism; and so on.)", "meta": {"post_id": 3530539, "input_score": 48, "output_score": 55, "post_title": "Is \"taking a limit\" a function? Is it a procedure? A ternary operation?"}}
{"input": "Evaluate $$\\int_{-a\\pi}^{a\\pi} \\frac{\\cos^5(x)+1}{e^x+1}dx, \\quad a \\in \\mathbb{N}$$\n\nIn the beginning, I didn't have any ideas of how to solve this. The one that later came to mind was to try using Feynman's technique, but I couldn't think of the proper function to use for the second variable. \nAny ideas?", "output": "Let $I$ be our integral. Substitute $t=-x \\Rightarrow dt = -dx$. Then:\n$$I=\\int_{-a\\pi}^{a\\pi} \\frac{\\cos^5 t+1}{e^{-t}+1}\\,dt=\\int_{-a\\pi}^{a\\pi} \\frac{e^t(\\cos^5 t+1)}{e^{t}+1}\\,dt=\\int_{-a\\pi}^{a\\pi} \\frac{e^x(\\cos^5 x+1)}{e^{x}+1}\\,dx$$\nTherefore:\n$$2I=\\int_{-a\\pi}^{a\\pi} \\frac{e^x(\\cos^5 x+1)}{e^{x}+1}\\,dx+\\int_{-a\\pi}^{a\\pi} \\frac{\\cos^5 x+1}{e^{x}+1}\\,dx=\\int_{-a\\pi}^{a\\pi} (\\cos^5 x+1)\\,dx$$\n$$=\\bigg[x + \\frac{5}{8} \\sin(x) + \\frac{5}{48} \\sin 3 x + \\frac{1}{80} \\sin 5 x\\bigg]_{-a\\pi}^{a\\pi}=2a\\pi$$\nThus $I=a\\pi$.", "meta": {"post_id": 3566831, "input_score": 14, "output_score": 42, "post_title": "Problem from the 2020 Latvian \"Sophomore's Dream\" competition"}}
{"input": "The diagram shows 12 small circles of radius 1 and a large circle, inside a square.\nEach side of the square is a tangent to the large circle and four of the small circles.\nEach small circle touches two other circles.\nWhat is the length of each side of the square?\nThe answer is 18\nCONTEXT: \nThis question came up in a Team Maths Challenge I did back in November. No one on our team knew how to do it and we ended up guessing the answer (please understand that time was scarce and we did several other questions without guessing!) I just remembered this question and thought I'd have a go but I am still struggling with it. \nThere are no worked solutions online (only the answer) so I reaching out to this website as a final resort. Any help would be greatly appreciated. Thank you!", "output": "It is instructive to consider the general case.  Suppose we have a circle of radius $r$ that is inscribed in a square of side length $2r$.  Suppose $n$ tangent circles of unit radius can be drawn along the inside \"corner\" of the square.  What is the relationship between $r$ and $n$?  Your question is the case $n = 2$, the third circle drawn in the corner being redundant.  The figure below illustrates the case $n = 5$:\n\nThe solution is straightforward.  The right triangle shown in the diagram has legs $r-1$ and $r-(2n-1)$, and hypotenuse $r+1$.  Therefore, $$(r-1)^2 + (r-2n+1)^2 = (r+1)^2,$$ from which it follows that $$r = (1 + \\sqrt{2n})^2.$$  For $n = 2$, this gives $r = 9$ and the side length of the square is $18$.  For $n = 5$, we have $r = 11 + 2 \\sqrt{10}$.  Whenever $n$ is twice a square, i.e. $n = 2m^2$ for a positive integer $m$, then $r = (1 + 2m)^2$ is also an integer and the circumscribing square has integer sides.\n\nAs a related but different question, given $n$ such circles, what is the total number of externally tangent unit circles that can be placed in the corner such that their centers form a square lattice and do not intersect the large circle?  So for $n = 2$, this number is $f(n) = 3$ as shown in your figure.  For $n = 5$, it is $f(5) = 12$.", "meta": {"post_id": 3691332, "input_score": 61, "output_score": 44, "post_title": "What is the size of each side of the square?"}}
{"input": "Let $M$ and $N$ be topological manifolds that admit differential structures and let $f:M\\to N$ be continuous. Can $M$ and $N$ always be given differential structures to become differentiable manifolds $\\widetilde M$ and $\\widetilde N$ such that $f:\\widetilde M\\to\\widetilde N$ is differentiable? What if we impose further restrictions, such as $\\widetilde M$ and $\\widetilde N$ being smooth manifolds, or $f$ becoming smooth? Are there certain differential structures which we can't do this with (i.e. if we want to make $f$ differentiable, we can never make $N$ diffeomorphic to $\\overline N$ where $\\overline N$ is some differential structure on $N$)? What if $M$ already has a fixed differential structure?", "output": "There is not necessarily a way to make a map smooth.  For example, suppose $M=\\mathbb{R}$, and suppose $N$ is any topological manifold of dimension $2$ or more.  Let $f:M\\rightarrow N$ be any continuous function whose image contains a non-empty open subset of $N$ (e.g., take a space filling curve onto $\\mathbb{R}^n$ and then think of this $\\mathbb{R}^n$ as a chart).\nThen there are no smooth structures on $M$ and $N$ which makes $f$ smooth.  In fact, you cannot even make $f$ continuously differentiable.  One way to see this is to use Sard's Theorem: if you could make $f$ continuously differentiable, then the set of regular values would be open and dense.  Because $M$ has a lower dimension than $N$, regular values are points of $N$ which are not in the image of $f$.  But then the rest that the image of $f$ contains an open subset of $N$ means the set of regular values of $f$ is not dense.", "meta": {"post_id": 3968922, "input_score": 21, "output_score": 36, "post_title": "Can every continuous function between topological manifolds be turned into a differentiable map?"}}
{"input": "There is apparently cutting-edge research by Dustin Clausen & Peter Scholze (and probably others) under the name Condensed Mathematics, which is meant to show that the notion of Topological Space is not so well-chosen, and that Condensed Sets lead to better behaved structures.\n\nWhat is a simple low-tech example to see the difference?\n\nI am looking for some explicit construction with quite simple topological spaces where some bad behaviour occur, and how their condensed analog fix that.\nI am aware of the nlab entry and of an introductory text by F. Deglise on this page but it goes quite far too quickly and I am missing knowledge to grasp it.", "output": "Here's a one-paragraph answer: Topological spaces formalize the idea of spaces with a notion of \"nearness\" of points. However, they fail to handle the idea of \"points that are infinitely near, but distinct\" in a useful way. Condensed sets handle this idea in a useful way.\nLet me give a few examples. Say you have a nice space like the line $X=\\mathbb R$, and acting on it you have a group $G$. If $G$ acts nicely, for example $G=\\mathbb Z$ through translations, then the quotient space $X/G$ is well-behaved as a topological space, giving the circle $S^1$ in this case. However, if $G$ has dense orbits, for example $G=\\mathbb Q$, then $X/G$ is not well-behaved as a topological space. In fact, in this example $\\mathbb R/\\mathbb Q$ has many distinct points, but they are all infinitely near: any neighborhood of one point $x$ will also contain any other point $y$. Thus, as a topological space $\\mathbb R/\\mathbb Q$ is indiscrete; in other words, it is not remembering any nontrivial topological structure.\nOne could also consider nonabelian examples, like the quotient $\\mathrm{GL}_2(\\mathbb R)/\\mathrm{GL}_2(\\mathbb Z[\\tfrac 12])$.\nSimilar things also happen in functional analysis. For example, one can embed summable sequences\n$$\\ell^1(\\mathbb N)=\\{(x_n)_n\\mid x_n\\in \\mathbb R, \\sum_n |x_n|<\\infty\\}$$\ninto square-summable sequences\n$$\\ell^2(\\mathbb N)=\\{(x_n)_n\\mid x_n\\in \\mathbb R, \\sum_n |x_n|^2<\\infty\\}$$\nand consider the quotient $\\ell^2(\\mathbb N)/\\ell^1(\\mathbb N)$. As a topological vector space, this is indiscrete, and so has no further structure than the abstract $\\mathbb R$-vector space. For this reason, quotients of this type are usually avoided, although they may come up naturally!\nWithout repeating the definition of condensed sets here, let me just advertise that they can formalize the idea of \"points that are infinitely close, but distinct\", and all the above quotients can be taken in condensed sets and are reasonable. As we have seen, this means that one must abandon \"neighborhoods\" as the primitive concept, as in these examples all neighborhoods of one point already contain all other points. Roughly, what is formalized instead is the notion of convergence, possibly allowing that one sequence converges in several different ways.\nSo in the condensed world, it becomes possible to consider quotients like $\\ell^2(\\mathbb N)/\\ell^1(\\mathbb N)$, and inside all condensed $\\mathbb R$-vector spaces they are about as strange as torsion abelian groups like $\\mathbb Z/2\\mathbb Z$ are inside all abelian groups. However, there are some surprising new phenomena: For example, there is a nonzero map of condensed $\\mathbb R$-vector spaces\n$$\\ell^1(\\mathbb N)\\to \\ell^2(\\mathbb N)/\\ell^1(\\mathbb N)$$\ninduced by the map\n$$(x_n)_n\\mapsto (x_n \\log |x_n|)_n,$$\nin other words $x\\mapsto x\\log |x|$ pretends to be a linear map! (Let me leave this as a (fun) exercise.)\n(These are liquid $\\mathbb R$-vector spaces, and the presence of such strange maps makes it hard to set up the basic theory of liquid vector spaces; but arguably makes it more interesting!)", "meta": {"post_id": 4044728, "input_score": 24, "output_score": 68, "post_title": "Examples of the difference between Topological Spaces and Condensed Sets"}}
{"input": "I have trouble understanding the difference between initial and terminal objects in category theory. I get the definitions, but when seeing examples, I am confused.\nExample 1: The empty set is the unique initial object in the category of sets. Every one-element set is a terminal object in this category.\n\nWhy there can be morphism from empty set to any other set? And why there is not morphism to empty set as well? I would find it more intuitive if one-element set would be initial object too.\n\nExample 2: Similarly, the empty space is the unique initial object in Top, the category of topological spaces and every one-point space is a terminal object in this category.\n\nWhy we can create a topological space from empty space? And why the space cannot be mapped into empty space uniquely too?\n\nI don't know why I am stuck with this. Maybe I have a wrong idea about how morphisms behave?\nThank you for your help.", "output": "Your confusion seems to be not so much about initial and terminal objects, but about what those look like in the category of sets.    Looking at the formal definition of \u201cfunction\u201d will help make clear some of the unusual cases such as functions with empty domains.\nA function from $A$ to $B$ can be understood as a set of pairs $$\\langle a,b\\rangle$$ where $a\\in A$ and $b\\in B$.  And:\n\nThere must be exactly one pair $\\langle a,b\\rangle$ for each element $a$ of $A$.\n\nExactly one, no more and no less, or the set of pairs is not a function.\nFor example, the function that takes an integer $n$ and yields its square $n^2$ can be understood as the (infinite) set of ordered pairs:\n$$\\{ \\ldots ,\\langle -2, 4\\rangle,\n\\langle -1, 1\\rangle,\n\\langle 0, 0\\rangle ,\\langle 1, 1\\rangle,\n\\langle 2, 4\\rangle\\ldots\\}$$\nAnd for each integer $n$ there is exactly one pair $\\langle n, n^2\\rangle$.  Some numbers can be missing on the right side (for example, there is no pair $\\langle n, 3\\rangle$) and some numbers can be repeated on the right (for example the function contains both $\\langle -2, 4\\rangle$\nand $\\langle 2, 4\\rangle$) but on the left each number appears exactly once.\n\nNow suppose $A$ is some set $\\{a_1, a_2, \\ldots\\}$ and $B$ is a set with only one element $\\{b\\}$.  What does a function from $A$ to $B$ look like?  There is only one possible function: it must be: $$\\{\n\\langle a_1, b\\rangle,\n\\langle a_2, b\\rangle,\n\\ldots\\}.$$ There is no choice about the left-side elements of the pairs, because there must be exactly one pair for each element of $A$.  There is also no choice about the right-side element of each pair.  $B$ has only one element, $b$, so the right-side element of each pair must be $b$.\nSo, if $B$ is a one-element set, there is exactly one function from $A$ to $B$. This is the definition of \u201cterminal\u201d, and one-element sets are terminal.\nNow what if it's $A$ that has only one element?  We have $A=\\{a\\}$ and $B=\\{b_1, b_2, \\ldots\\}$.  How many functions are there now?  Only one?\nOne function is $$\\{\\langle a, b_1\\rangle\\}$$\nanother is $$\\{\\langle a, b_2\\rangle\\}$$\nand another is $$\\{\\langle a, b_3\\rangle\\}$$\nand so on.  Each function is a set of pairs where the left-side elements come from $A$, and each element of $A$ is in exactly one pair.  $A$ has only one element, so there can only be one pair in each function.  Still, the functions are all different.\nYou said:\n\nI would find it more intuitive if one-element set would be initial object too.\n\nBut for a one-element set $A$ to be initial, there must be exactly one function $A\\to B$ for each $B$.  And we see above that usually there are many functions $A\\to B$.\n\nNow we do functions on the empty set.  Suppose $A$ is $\\{a_1, a_2, \\ldots\\}$ and $B$ is empty.  What does a function from $A\\to B$ look like?  It must be a set of pairs, it must have exactly one pair for each element of $a$, and the right-side of each pair must be an element of $B$. But $B$ has no elements, so this is impossible:  $$\\{\\langle a_1, ?\\rangle, \\langle a_2, ?\\rangle, \\ldots\\}.$$\nThere is nothing to put on the right side of the pairs.  So there are no functions $A\\to\\varnothing$.   (There is one exception to this claim, which we will see in a minute.)\nWhat if $A$ is empty and $B$ is not, say $\\{b_1, b_2, \\ldots\\}$?  A  function $A\\to B$ is a set of pairs that has exactly one pair for each element of $A$.  But $A$ has no elements.  No problem, the function has no pairs! $$\\{\\}$$\nA function is a set of pairs, and the set can be empty.  This is called the \u201cempty function\u201d.  When $A$ is the empty set, there is exactly one function from $A\\to B$, the empty function, no matter what $B$ is.  This is the definition of \u201cinitial\u201d, so the empty set is initial.\nDoes the empty set have an identity morphism?  It does; the empty function $\\{ \\}$ is its identity morphism.  This is the one exception to the claim that there are no functions from $A\\to\\varnothing$: if $A$ is also empty, the empty function is such a function, the only one.\n\nThe issue for topological spaces is exactly the same:\n\nWhen $B$ has only one element, there is exactly one continuous map  $A\\to B$ for every $A$.\nWhen $A$ is empty, there is exactly one continuous map $A\\to B$ for every $B$: the empty function is the unique map.\nWhen $A$ has only one element, there are usually many continuous maps $A\\to B$, one different one for each element of $B$.\n\nThere are categories in which the initial and terminal objects are the same.  For example, in the category of groups the trivial group (with one element) is both initial and terminal.\nI hope this was some help.", "meta": {"post_id": 4235989, "input_score": 15, "output_score": 47, "post_title": "I don't understand the difference between initial and terminal objects"}}
{"input": "Question. If I attempt to prove that space $X$ is complete by pursuing the strategy, \u201cAssume $x_n \\rightarrow x$; the space $X$ is complete if $x \\in X$,\u201d then why is that wrong?\n\nContext. I know the definition of Cauchy sequences and convergent sequences, and that the definition of completeness is that Cauchy sequences in the space converge. And so I know that if one is attempting to prove that a space is complete, then the usual proof should start, \u201cAssume that $x_n$ is a Cauchy sequence; we will show that $x_n$ converges in $X$.\u201d\nThe misconception I seem to be battling is this: It seems like a Cauchy sequence must converge to something, just that the something might not be in the space. So it seems to me like the question really is, \u201cIs the limit in the space or is it not in the space?\u201d The classic example is the sequence of rationals that converges to $\\sqrt{2}$. The sequence is Cauchy within the space of the rationals, and also the sequence does converge, just to a limit that is outside the space in consideration. So recently, I began a proof of completeness with the line, \u201cAssume $f_n \\rightarrow f$. We want to show that $f \\in X$.\u201d And the feedback was, \u201cUnclear what is being proved. Nothing related to completeness. 0/4 points.\u201d It seems to me that showing that the limit of a convergent sequence resides in the space is equivalent to saying that Cauchy sequences converge. Why is that wrong?\nThank you!", "output": "You are correct in the narrow sense that every Cauchy sequence does converge in some space.\nTo be precise, let $(X;d_1)$ be any metric space with at least two points, let $Y$ be the set of Cauchy sequences in $X$, and define $d_2:Y^2\\to\\mathbb{R}$; $$d_2(\\{x_n\\}_n,\\{y_n\\}_n)=\\lim_{n\\to\\infty}{d_1(x_n,y_n)}$$  Then it is easy (well, a decent homework problem, anyways) to verify that\n\n$Y$ is not a metric space under $d_2$; different points of $Y$ might be distance-$0$ from each other.\nFor each $y\\in Y$, there exists an equivalence class $c(y)=\\{z:d_2(y,z)=0\\}$.  Let $Z$ be the set of all equivalence classes, i.e. $Z=\\{c(y):y\\in Y\\}$.  Then $d_2$ extends to $Z^2\\to\\mathbb{R}$ in the natural way.\n$(Z;d_2)$ is a metric space.\n$(X;d_1)$ embeds homeomorphically into $(Z;d_2)$ via $x\\mapsto c(x,x,x,\\dots)$.\n$(Z;d_2)$ is complete.\n\nThus if we identify $X$ with the embedded subspace of $Z$, then any Cauchy sequence in $X$ converges in $Z$.  The end limit might be $X$, or it might not; to show $X$ complete is to show that the end limit is in fact in $X$.\nFor this reason, $Z$ is called the completion of $X$.\nWith that said, some space is much more general than you give it credit for.\nGiven your notation ($f_n\\to f$), I think you assumed that the limit of a Cauchy sequence of functions is itself a function.  Probably $X$ was the space of continuous functions under the uniform norm, and so you thought that you just had to prove $f$ was itself a continuous function.\nBut $f$ could be far worse!  For example, let $X$ be the space of continuous functions on $[0,1]$ with the following norm: $$d_1(f,g)=\\int_0^1{|f(x)-g(x)|\\,dx}$$  Then $X$ has a well-known completion: $L^1([0,1])$, the space of Lebesgue-integrable functions on $[0,1]$, up to a.e. equivalence.\nIf you haven't seen the Lebesgue integral, don't worry; the pathology carries over to Riemann-integrable functions.  In particular, one can show that any piecewise-continuous function lies in the equivalent of $(Y;d_2)$ (to borrow notation from above).[*]  So, for any $r$, the function $\\delta_r$ where $$\\delta_r(x)=\\begin{cases}r&x=0\\\\0&x\\neq0\\end{cases}$$  is in $Y$.\nBut all those functions collapse to the same point in $(Z;d_2)$.  That is, given any convergent sequence $f_n$, we can find some $f_{\\infty}$ such that $f_n\\to f_{\\infty}$\u2026and $f_n\\to f_{\\infty}+\\delta_r$ too!\nSo we can't define an evaluation map to describe $f_{\\infty}(0)$.  Indeed, for any fixed point $x$, $f_{\\infty}(x)$ is not well-defined.\n[*]: As Noiralef pointed out in comments, I'm cheating here.  I previously defined $Y$ as an equivalence set of Cauchy sequences; now I'm saying a function $\\delta_r$ is in $Y$.  What I mean here is, there exists a (uniformly bounded) Cauchy sequence of continuous functions converging pointwise to $\\delta_r$.", "meta": {"post_id": 4273341, "input_score": 80, "output_score": 91, "post_title": "Does every Cauchy sequence converge to *something*, just possibly in a different space?"}}
{"input": "I was reading Maxwell's relations and came across: $$\\oint pdV=\\oint TdS\\Rightarrow \\iint dpdV=\\iint dTdS.$$\nI know this is straightforward to see since they both represent the surface area, but I've never seen a math theorem on textbooks that indicates $$\\oint ydx=\\iint dxdy.$$ Is this just a trivial corollary?", "output": "This is Stokes' theorem,\n$$ \\int_S \\mathrm{d}\\omega = \\int_{\\partial S} \\omega$$\nfor a 1-form $\\omega = y\\mathrm{d}x$, $S$ a surface and $\\partial S$ its boundary.", "meta": {"post_id": 4392567, "input_score": 13, "output_score": 40, "post_title": "Is there a math theorem by which a contour integral is equal to a double integral?"}}
{"input": "I am reading Peter Smith's An Introduction to G\u00f6del's Theorems. In chapter 10, he defines \"baby arithmetic\" $\\mathsf{BA}$ to be the zeroth-order version of Peano arithmetic ($\\mathsf{PA}$) without induction. That is, $\\mathsf{BA}$ is the zeroth-order theory (meaning there are no quantifiers or variables) with primitive constant symbol $\\mathsf0$, unary function $\\mathsf S$, binary functions $+$ and $\\times$, and the following axiom schemas:\n\n$\\mathsf{Sn\\neq0}$\n$\\mathsf{(Sn=Sm)\\to(n=m)}$\n$\\mathsf{m+0=m}$\n$\\mathsf{m+Sn=S(m+n)}$\n$\\mathsf{m\\times0=0}$\n$\\mathsf{m\\times Sn=(m\\times n)+m}$\n\nThe symbols $\\mathsf{n}$ and $\\mathsf{m}$ appearing in the schemas are placeholders for any term of the theory, where terms are defined via the standard recursive definition for predicate logic.\nMy question is, does $\\mathsf{PA}\\vdash\\mathsf{Con(BA)}$? Note that this is a very concrete question about whether the arithmetical formula $\\mathsf{Con(BA)}$ is formally derivable in $\\mathsf{PA}$, not to be confused with more philosophical questions about whether $\\mathsf{PA}$ itself is consistent, discussed elsewhere.\nG\u00f6del's second incompleteness theorem shows that $\\mathsf{PA}$ cannot prove $\\mathsf{Con(PA)}$. Meanwhile we cannot even ask whether $\\mathsf{BA}$ can prove $\\mathsf{Con(BA)}$ because $\\mathsf{Con(BA)}$ involves quantifiers and is therefore not even in the language of $\\mathsf{BA}$. But it seems plausible that $\\mathsf{PA}$ could prove $\\mathsf{Con(BA)}$, which would be a nice, reassuring result at the very least.\n\nAs a footnote, I'll say that part of my motivation for this question is indeed wondering about whether $\\mathsf{PA}$ is consistent. A common argument goes something like: \"If you believe $\\mathsf{PA}$ formalizes valid reasoning about arithmetic, then you believe what it proves. Hence it cannot prove $\\mathsf{0=1}$, because $0$ is not really equal to $1$. Hence you believe $\\mathsf{PA}$ is consistent.\" But I think this argument would be more secure if it didn't rest on definite facts about \"true arithmetic,\" and in particular the assumption that \"$0$ is not actually equal to $1$,\" but instead referred only to $\\mathsf{BA}$. Thus, in this light, it is important to know whether $\\mathsf{BA}$ proves $\\mathsf{0=1}$, i.e., whether $\\mathsf{BA}$ is consistent.\nI'm not putting this here to start a huge philosophical discussion/argument, just to provide motivation.", "output": "$\\mathsf{PA}$ has a very interesting property, namely that it proves the consistency of each of its finitely axiomatizable subtheories. This is usually called the reflection principle for $\\mathsf{PA}$ (and incidentally the same result holds for $\\mathsf{ZFC}$). The theory $\\mathsf{BA}$ is the quantifier-free part of a finitely axiomatizable theory (just throw on \"$\\forall x$\"s everywhere) $\\mathsf{BA}'$; consequently, we do in fact have $\\mathsf{PA}\\vdash \\mathsf{Con(BA)}$.\n\nWe have to be careful here: $\\mathsf{PA}$ does not prove \"Every finite subtheory of $\\mathsf{PA}$ is consistent.\" Rather, for each finite subtheory $T$ of $\\mathsf{PA}$, $\\mathsf{PA}$ proves \"$T$ is consistent.\" So we don't get a $\\mathsf{PA}$-proof of $\\mathsf{Con(PA)}$ itself from the reflection principle. ($\\mathsf{PA}$ also proves \"$\\mathsf{PA}$ proves the consistency of each finite subtheory of $\\mathsf{PA}$,\" but again this falls short of actually being a problem.)\n\nAdmittedly, this is massive overkill, but the reflection principle is a very cute trick that's worth knowing. With more care we can prove the consistency of $\\mathsf{BA}$ in the very weak fragment $\\mathsf{I\\Sigma_1}$, or indeed much less (although when we go below $\\mathsf{I\\Sigma_1}$ things often get finicky so I usually don't).", "meta": {"post_id": 4421760, "input_score": 29, "output_score": 35, "post_title": "Can Peano arithmetic prove the consistency of \"baby arithmetic\"?"}}
{"input": "Is every $3$-dimensional vector $v$ with integer coordinates a cross product of two other vectors with integer coordinates?\n\nI have written a program to check for $v$ with entries between $-7$ and $7$. Every $v$ that small can be expressed as a cross product of two other vectors with integer coordinates.\nBut I can't come up with a general proof.\nApart from the empirical evidence from my experiment on small $v$, another reason to think this is true is that it's almost enough to find two small independent vectors, $u$ and $w$, with integer coordinates that are perpendicular to $v$. Playing around with integer relation algorithms has taught me that such $u$ and $w$ should be plentiful. The cross product of $u$ and $w$ is a scalar multiple of $v$ - call it $kv$. $k$ is an integer; in most cases $|k| = 1$. If it isn't, pick a different $u$ and $w$.\nA similar but much easier question was this: Is every vector in $\\Bbb R^3$ a cross product?.\nNote: The answer given here was used to solve Diophantine equations so the question is about number theory.", "output": "Let us write $v=(a, b, c)$, and consider the three vectors\n$$ w_1=(0, c, -b), \\quad w_2=(-c,0,a), \\qquad w_3=(b, -a, 0). $$\nNote that $w_1 \\times w_2=cv$, $w_1 \\times w_3=-bv$. This way, let $d=\\gcd(b, c)$, and write it as $d=\\lambda b+\\mu c$, so that\n$$ \\frac{w_1}{d} \\times\\left( \\mu w_2-\\lambda w_3 \\right)=v. $$\nHere $\\mu w_2-\\lambda w_3$ and $w_1/d$ have integer coordinates, as we can easily check.", "meta": {"post_id": 4428714, "input_score": 33, "output_score": 38, "post_title": "Is every vector in $\\mathbb Z^3$ a cross product?"}}
{"input": "I'm currently at a stage where I think I'm quite comfortable with the appearance of local non-archimedean fields in the maths I encounter, having seen a fair bit of technology built upon their structure and applications to connected areas, yet I somehow still feel like I have an unsatisfying understanding of why their introduction is absolutely crucial to study algebraic number theory and arithmetic geometry, especially since all these applications are hugely advanced compared to the point at which $p$-adic numbers are usually introduced in a student's career I think. If I want to motivate their definition, I sort of naturally go through the following implications in my head:\n\nto study a problem over the ring of integers $\\mathbb{Z}$ (or more generally, the ring of integers of a number field) the strategy is to localise the problem at a prime $p$, as to focus around it and worry about the problem 'one point at a time', so-to-speak;\nwe (literally, now) localise at the prime $p$, just as one would do with the ring of regular functions on a variety, and replace $\\mathbb{Z}$ with the ring $\\mathbb{Z}_{(p)} := \\{ \\frac{x}{y} \\in \\mathbb{Q} \\mid p\\nmid y \\}$ ;\nwe can then complete at the maximal ideal $(p) \\subseteq \\mathbb{Z}_{(p)}$ to obtain the $p$-adic integers $\\mathbb{Z}_p$, with the benefit that now there's access to approximation techniques such as Hensel's lemma, and the newly obtained ring has pretty much all the same algebraic properties as $\\mathbb{Z}_{(p)}$ because of its identical valuation theory.\n\nIt's this last step which still confuses me... even though I can appreciate the utility of approximation techniques, it still feels extremely arbitrary why it turns out to be so inevitable with all the theory that builds upon this measly little step. Somehow, every time I've seen the study of formal neighbourhoods in algebraic geometry it always feels like it's a tool to tackle a problem, and not the main object of study, whereas in my head the $p$-adic numbers have turned out to be the main character in many areas of mathematics, which sort-of goes against this intuition of mine I find.\nSince this intuition really doesn't come from any of my teachers and I'm not quite sure it makes much sense, I wanted to ask if it's an apt way to think about the use of non-archimedean fields in mathematics; I'd also be very interested in learning about their role in the history of algebraic number theory since I'm not quite sure I can properly place their use and introduction on a timeline in a coherent way with all of the theory I have in mind.\nI apologise if my question is very hand-wavy, and I'd be super grateful for any sort of insight :) Thank you very much for your time!!", "output": "Let me explain how I think about one way in which $\\mathbb{Q}_p$ (or more to the point $\\mathbb{Z}_p$) appears from an arithmetic-geometric perspective. Let me approach this from a vantage point that should be understandable to any advanced undergraduate. In particular, let me take 'solving Diophantine equations' as the goal.\n(For a parallel and more in-depth discussion, you can read this extended version of a talk I gave. For a perspective more in-line with that discussed by the user hunter you can look at these old notes of mine [Disclaimer: these are quite old and so I can't vouch for their correctness, mathematically of philosophically])\n\nThe general motivation\nThe basic idea stems from the following credo which one encounters ad nauseam in their mathematical education. For a 'geometric space' $X$\n$$\\left\\{\\text{Problems on }X\\right\\}=\\left\\{\\left(\\begin{matrix}\\text{Problems locally}\\\\ \\text{on }X\\end{matrix}\n\\,\\,,\\,\\,\\begin{matrix}\\text{Gluing}\\\\ \\text{data}\\end{matrix}\\right)\\right\\}.$$\nThis breaks questions on $X$ into two (hopefully) more managable pieces.\nGood examples of this are:\n\n(Diff) if $X$ is some manifold and I want to solve the differential equation $Df=0$, I can try to find solutions to $Df=0$ locally on $X$, and then glue them together to a global solution,\n(Sect) if $f\\colon Y\\to X$ is a map of 'geometric spaces' and I want to find a section $s\\colon X\\to Y$, I can try to find a section locally on $X$ and then glue these local sections together.\n\nRemark 1:\n\n The second example is really the representative one, and morally the notion of 'moduli spaces' should tell you that all examples should be of this form. For instance, it's a good exercise to think how one might put the first example in this form!\n\nOf course, this idea really only works with the following serious caveat: the local 'geometry' of $X$ is simple(r) enough to make solving the problem locally on $X$ (more) tenable.\nA geometric perspective of solving equations\nSo, how does this relate to solving Diophantine equations or, for that matter, your question?\nThe beautiful idea of Grothendieck and his collaborators (building off of the work of many others) shows that solving equations can, in fact, be put into the context of (Sect). Namely, there is associated to any ring $R$ a geometric object $\\mathrm{Spec}(R)$, a so-called locally ringed space (the category of which we shall denote $\\mathbf{LRS}$), such that\n$$\\mathbf{Ring}\\to \\mathbf{LRS},\\qquad R\\mapsto \\mathrm{Spec}(R)$$\nis a (contravariant) fully faithful embedding. This is a bit abstract, so before we explain how this relates to solving equations, let us spell this out a little more precisely.\nThe object $\\mathrm{Spec}(R)$, being a locally ringed space, means it comes with two bits of structure:\n\nan underlying topological space and,\na 'sheaf of functions' $\\mathcal{O}$ which assigns to any open subset $U$ the ring $\\mathcal{O}(U)$ of 'functions on $U$'.\n\nThe underlying topological space $\\mathrm{Spec}(R)$ can be described very explicitly. As a set we take\n$$\\mathrm{Spec}(R)=\\{\\mathfrak{p}\\subseteq R\\text{ a prime ideal}\\}.$$\nThe topology on $\\mathrm{Spec}(R)$, called the Zariski topology, has (a basis of) open subsets given by the non-vanishing loci\n$$D(f):=\\{\\mathfrak{p}\\in\\mathrm{Spec}(R):f\\notin\\mathfrak{p}\\}$$\nfor an element $f$ of $R$. To help understand the intuition behind the terminology 'non-vanishing locus', it is helpful to think about the defining property of $\\mathrm{Spec}(R)$: $R=\\mathcal{O}(\\mathrm{Spec}(R))$; in other words, the ring of functions on $\\mathrm{Spec}(R)$ is $R$.\nTo understand this, let us first think of the example $R=\\mathbb{C}[x]$. Here we have\n$$ \\{(x-p):p\\in\\mathbb{C}\\}\\cup\\{(0)\\}= \\mathrm{Spec}(\\mathbb{C}[x])\\supseteq \\mathrm{MaxSpec}(\\mathbb{C}[x])=\\{(x-p):p\\in\\mathbb{C}\\},$$\nwhere for a ring $R$ we write $\\mathrm{MaxSpec}(R)$ for the set of maximal ideals of $R$. Note then that any element $f$ in $\\mathbb{C}[x]$ can be thought of as a function\n$$f\\colon\\mathrm{MaxSpec}(\\mathbb{C}[x])\\to \\mathbb{C},\\qquad (x-p)\\mapsto (f\\bmod (x-p))=f(p),$$\nwhere this last identification is using the isomorphism $\\mathbb{C}[x]/(x-p)\\cong \\mathbb{C}$.\nNow for a general ring $R$ (especially those rings that will show up in our study of Diophantine equations), we don't have the luxury of their maximal ideals admitting such a geometric description. In fact, in general, we don't even have the luxury of having enough maximal ideals for $\\mathrm{MaxSpec}(R)$ to be a particularly useful set (e.g. for a local ring there is only one!). That said, we may use the above example as a hint of how to think of an element $f$ of $R$ as a function on $\\mathrm{Spec}(R)$: it is the assignment\n$$f\\colon \\mathrm{Spec}(R)\\longrightarrow \\bigsqcup_{\\mathfrak{p}\\in\\mathrm{Spec}(R)}k(\\mathfrak{p}),\\qquad \\mathfrak{p}\\mapsto f(\\mathfrak{p}):=(f\\bmod \\mathfrak{p})\\qquad (\\ast).$$\nHere $k(\\mathfrak{p}):=\\mathrm{Frac}(R/\\mathfrak{p})$ is the 'residue field' of $R$ at $\\mathfrak{p}$.\nThis might look quite bizarre, especially the codomain being a big disjoint union of different fields. But, the fact that this didn't occur in the case $R=\\mathbb{C}[x]$ is semi-coincidental: for every $\\mathfrak{p}$ in $\\mathrm{MaxSpec}(\\mathbb{C}[x])$ there is an isomorphism $k(\\mathfrak{p})\\cong\\mathbb{C}$ and so we (sort of carelessly) identified them all with each other. As intimated before, such happy coincidences don't happen in general, and so we're stuck with the above. But, if you buy this interpretation, then the phrase 'non-vanishing locus' makes sense, as $D(f)$ is exactly those $\\mathfrak{p}$ for which $f(\\mathfrak{p})\\ne 0$.\nBut, I told you that a locally ringed space also came with a sheaf $\\mathcal{O}$ of functions -- what is it for $\\mathrm{Spec}(R)$? For formal reasons (because they form a basis) it suffices to describe the value of $\\mathcal{O}(D(f))$, and the guess is now clear: the only new functions that should be introduced by considering the non-vanishing locus of $f$ is the function $f^{-1}$. Thus, one sets $\\mathcal{O}(D(f))=R[\\tfrac{1}{f}]$.\nThen, at least in very broad terms, the promised (contravariant) fully faithful embedding is manifested as a natural bijection\n$$\\mathrm{Hom}_\\mathbf{Ring}(R,S)=\\mathrm{Hom}_\\mathbf{LRS}(\\mathrm{Spec}(S),\\mathrm{Spec}(R)) \\quad (\\ast\\ast).$$\nThis bijection can be made very explicit, but since I am not even telling you what a morphism of locally ringed spaces is, let's just take it as a black box.\nTwo important examples to keep in mind for the discussion below are:\n\nthe natural map $R\\to R[\\tfrac{1}{f}]$ corresponds to a map $\\mathrm{Spec}(R[\\tfrac{1}{f}])\\to \\mathrm{Spec}(R)$ which is an isomorphism onto the open subset $D(f)$ (an 'open immersion'),\nfor a prime $\\mathfrak{p}$ the composition $R\\to\n   R/\\mathfrak{p}\\hookrightarrow k(\\mathfrak{p})$ corresponds to a map\n$\\mathrm{Spec}(k(\\mathfrak{p}))\\to \\mathrm{Spec}(R)$ which one should think of as picking out the point $\\mathfrak{p}$,\n\nOK, fine, how does this help us solve equations? Well, let us fix an $n$-tuple of polynomials\n$$\\mathbf{f}=(f_1,\\ldots,f_m)\\in R[x_1,\\ldots,x_n].$$\nWe may consider the solution set functor for $\\mathbf{f}$ over $S$:\n$$X_\\mathbf{f}\\colon \\{R\\text{-algebras}\\}\\to \\mathbf{Set},$$\ngiven by\n$$X_\\mathbf{f}(S):=\\{(y_1,\\ldots,y_n)\\in S^n: f_j(y_1,\\ldots,y_n)=0\\text{ for }j=1,\\ldots,m\\}.$$\nIn other words, $X_\\mathbf{f}(S)$ just spits out the solution set to $\\mathbf{f}=0$ over $S$. On the other hand, note that there is a natural bijection between $X_\\mathbf{f}(S)$ and the $S$-algebra maps\n$$\\alpha\\colon S[x_1,\\ldots,x_n]/(f_1,\\ldots,f_m)\\to S$$\ngiven by sending $\\alpha$ to the $n$-tuple $(\\alpha(x_1),\\ldots,\\alpha(x_n))$.\nSo, formally studying $(\\ast\\ast)$ shows the following beautiful fact: there is a natural bijection\n$$X_\\mathbf{f}(S)\\longleftrightarrow \\left\\{\\begin{matrix}\\text{Sections of the map}\\\\ \\mathrm{Spec}(S[x_i]/(f_j))\\to \\mathrm{Spec}(S)\\end{matrix}\\right\\}.$$\nIf you buy the above, then the upshot is that by taking $R=S=\\mathbb{Z}$, then we can squarely (if surprisingly) place solving Diophantine equations in the setting of the archetypal problem (Sect) from above with $X=\\mathrm{Spec}(\\mathbb{Z})$.\nThe ring $\\mathbb{Z}_{(p)}^h$\nBefore we get too excited though, we need to recall the aforementioned caveat: all of this formalism is useful if the local geometry of $\\mathrm{Spec}(\\mathbb{Z})$ is simple enough. Well...what is the local geometry of $\\mathrm{Spec}(\\mathbb{Z})$. Or, an even more prescient question...what does local even mean?\nWell, there is a natural guess. After all $\\mathrm{Spec}(\\mathbb{Z})$ is a topological space, so 'local' should mean studying a 'sufficiently small' open neighborhood. Now, we know what the set $\\mathrm{Spec}(\\mathbb{Z})$ looks like:\n$$\\mathrm{Spec}(\\mathbb{Z})=\\{(p):p\\text{ is a prime}\\}\\cup\\{(0)\\}.$$\nA little thought even allows us to describe the Zariski open sets. Namely, the proper non-empty open subsets of $\\mathrm{Spec}(\\mathbb{Z})$ are of the form\n$$\\mathrm{Spec}(\\mathbb{Z})-\\{p_1,\\ldots,p_n\\}$$\nfor primes $p_1,\\ldots,p_n$.\nThe Zariski topology is strange. It's quite feeble (coarse). These open subsets are all just too big to somehow serve as a 'sufficiently small' neighborhood of a point $(p)$. As an analogy, for the space $\\mathbb{C}$ (with its usual topology) and a point $p\\in\\mathbb{C}$, we think of the 'sufficiently small' open neighborhoods of $p$ to be of the form $p\\in\\mathbb{D}\\subseteq\\mathbb{C}$ where $\\mathbb{D}$ is an open disc. In particular, we very much do not think of the open subsets $\\mathbb{C}-\\{p_1,\\ldots,p_m\\}$ for points $p_1,\\ldots,p_m\\in\\mathbb{C}$ as being small.\nAgain, let me emphasize, while $\\mathrm{Spec}(\\mathbb{Z})$ is a powerful object, its topology (the Zariski topology) is somewhat feeble(=coarse).\nRemark 2:\n\nOne equational justification for why no neighborhood $D(f)$ of $(p)$ is sufficiently small: for any prime $p\\ne q\\nmid f$, the equation $qx=1$ does NOT have a solution in $\\mathbb{Z}[\\tfrac{1}{f}]$. A 'sufficiently small' neighborhood of $(p)$ should not have equational obstructions coming from other points!\n\nWell, if no one Zariski open neighborhood of $(p)$ is small enough, you can do something even more drastic: what if we intersect all the neighborhoods. In normal topology land this is not a very reasonable thing to do -- for a Hausdorff topological space you just get the point back! But, the upside of the feebleness of the Zariski topology of $\\mathrm{Spec}(\\mathbb{Z})$ is that this is a reasonable operation here. In fact, inspecting $(\\ast\\ast)$, and the fact that it reverses the direction of maps, we might guess that\n$$\\bigcap_{p\\in D(f)}D(f)=\\varprojlim_{p\\in D(f)}D(f),$$\nshould be a geometric space with ring of functions\n$$\\varinjlim_{f\\notin (p)}\\mathbb{Z}[\\tfrac{1}{f}]=\\mathbb{Z}_{(p)}.$$\nAnd so, in fact, a good model for the 'intersection of all neighborhoods of $(p)$' is $\\mathrm{Spec}(\\mathbb{Z}_{(p)})$. Maybe this is a \"neighborhood\" which is 'sufficiently small'.\nUnfortunately, this is still not small enough (a true indictment of the Zariski topology). The reason for this is more subtle, and takes a bit more of a leap-of-faith in terms of geometric philosophy. Namely, if $U$ is to be a 'sufficiently small neighborhood' of $(p)$ then we would imagine that the map\n$$\\{(p)\\}=\\mathrm{Spec}(\\mathbb{F}_p)\\to \\mathrm{Spec}(\\mathbb{Z}_{(p)}),$$\nshould be something like a (weak) homotopy equivalence. One of the main reasons that a small open disk around $p$ in $\\mathbb{C}$ is often 'sufficiently small' in complex analysis is because it's contractible. Unfortunately, this is not the case. In fact, in a way that one can make precise the map on fundamental groups isn't even an isomorphism.\nRemark 3:\n\nIt takes a lot of faith that this topological wording can be made precise and...wait a minute...even if you can, what in the world does this have to do with solving Diophantine equations? Let me try to give an example which hopefully (mildly) addresses both of these points.  Let us consider the Diophantine equation $$\\{y\\in\\mathbb{Z}:y^2+y+1=0.\\}$$ From our above discussion we know that this should correspond to studying sections of the map $$\\mathrm{Spec}(\\mathbb{Z}[x]/(x^2+x+1))\\to\\mathrm{Spec}(\\mathbb{Z}).$$ Moreover, what it should mean to study this 'locally at $(13)$', with $\\mathbb{Z}_{(13)}$ as our meaning of local, is that we want to study sections of the map $$\\mathrm{Spec}(\\mathbb{Z}_{(13)}[x]/(x^2+x+1))\\to\\mathrm{Spec}(\\mathbb{Z}_{(13)}).$$ That said, while this map does not have a section it does have a section over the point $\\mathbb{F}_{13}$: $$y^2+y+1=0\\bmod 13$$ has two distinct solutions.  Why is this indictative of the fact that $$\\{(13)\\}=\\mathrm{Spec}(\\mathbb{F}_{13})\\to \\mathrm{Spec}(\\mathbb{Z}_{(13)})$$ is not a homotopy equivalence? Well, let us note that the map $$\\mathrm{Spec}(\\mathbb{Z}_{(13)}[x]/(x^2+x+1))\\to\\mathrm{Spec}(\\mathbb{Z}_{(13)}).$$ can/should be thought of as being smooth and proper: smooth because its derivative $2x+1$ vanishes nowhere on $\\mathrm{Spec}(\\mathbb{Z}_{(13)}[x]/(x^2+x+1))$, and proper because it's finite. That said, Ehresmann's theorem says that a smooth proper map is locally trivial fibration. If $X'\\to X$ is a (weak) homotopy equivalence and $Y\\to X$ is a locally trivial fibration, then it should have a section if and only if $Y\\times_X X'\\to X'$ has a section. Thus, this indicates that $$\\{(13)\\}=\\mathrm{Spec}(\\mathbb{F}_{13})\\to \\mathrm{Spec}(\\mathbb{Z}_{(13)})$$ is indeed not a homotopy equivalence.  As mentioned above this remark, for those in the know, what I am really pointing out here is that the map $$\\pi_1^\\mathrm{et}(\\mathrm{Spec}(\\mathbb{F}_p))\\to\\pi_1(\\mathrm{Spec}(\\mathbb{Z}_{(p)})$$ is not a bijection.\n\nThe idea of Grothendieck and his collaborators/descendants is the following: because the Zariski topology on $\\mathrm{Spec}(\\mathbb{Z})$ is too feeble, one should replace it with a sort of \"generalized topology\" where one can take 'sufficiently small neighborhoods' of points. I won't attempt to define \"generalized topology\", but in our case what pops out is the Nisnevich topology on $\\mathrm{Spec}(\\mathbb{Z})$, which acts in a manner much more similar to the usual topology on $\\mathbb{C}$.\nRemark 4:\n\n For those more in the know, you might be surprised that what is showing up here is the Nisnevich topology, and not the more common etale topology. The reason is that unlike the case of $\\mathbb{C}$, our points themselves have non-trivial topology. In fact, recall that $$\\pi_1^\\mathrm{et}(\\mathrm{Spec}(\\mathbb{F}_p))=\\mathrm{Gal}(\\overline{\\mathbb{F}}_p/\\mathbb{F}_p)\\cong\\widehat{\\mathbb{Z}}.$$ The Nisnevich topology respects that, and allows you to zoom in on $(p)$ without altering the topology of the point. The etale neighborhood, on the other hand, pursues the logical conclusion by zooming in so far as to obtain a 'contractible' neighborhood, but at the cost of altering the topology of the point. Both are useful, but for our purposes here it is the Nisnevich topology playing the more crucial role.\n\nIn particular, one can again 'intersect all neighborhoods' of $(p)$ in this more robust Nisnevich topology, arriving at the Henselization of $\\mathbb{Z}_{(p)}$, denoted $\\mathbb{Z}_{(p)}^h$. One may roughly think of this as adding in the minimal number of things to $\\mathbb{Z}_{(p)}$ so that Hensel's lemma works (thus the name!). The map\n$$\\{(p)\\}=\\mathrm{Spec}(\\mathbb{F}_p)\\to \\mathrm{Spec}(\\mathbb{Z}_{(p)}^h),$$\nis like a (weak) homotopy equivalence, and so from a 'topological perspective' this does serve as a 'sufficiently small' neighborhood of $(p)$.\nRemark 5:\n\n My claim that $$\\{(p)\\}=\\mathrm{Spec}(\\mathbb{F}_p)\\to \\mathrm{Spec}(\\mathbb{Z}_{(p)}^h),$$ is like a (weak) homotopy equivalence is meant to be mostly intuitive, and probably only should be taken seriously at the pro-finite level. That said, see this, this, and Theorem 2.1.6 of this for why $\\mathrm{Spec}(A/I)$ and $\\mathrm{Spec}(A)$ are quite 'topologically similar' for a Henselian pair $(A,I)$. It is also helpful to compare this with the strict Henselization $\\mathbb{Z}_{(p)}^\\mathrm{sh}$, the local ring in the etale topology, where the results are more clear-cut: see this and Proposition 8.6 of Friedlander's Etale homotopy of schemes.\n\nOne 'equational' way this manifests itself is that if $\\mathbf{f}$ defines a 'smooth family at $p$' (i.e. that $\\mathrm{Spec}(\\mathbb{Z}[x_i]/(f_j))\\to \\mathrm{Spec}(\\mathbb{Z})$ is smooth at $p$), and so topologically well-behaved, then for any $n\\geqslant 1$ the map\n$$X_\\mathbf{f}(\\mathbb{Z}_{(p)}^h)\\to X(\\mathbb{Z}/p^n\\mathbb{Z}),$$\n(note that $\\mathbb{Z}_{(p)}^h/p^n\\mathbb{Z}^h_{(p)}=\\mathbb{Z}_{(p)}/p^n\\mathbb{Z}_{(p)}=\\mathbb{Z}/p^n\\mathbb{Z}$ by this) is surjective. In other words, for topologically well-behaved families, the only obstruction to having solutions is coming from the point $\\mathrm{Spec}(\\mathbb{F}_p)$ itself.\nThe ring $\\mathbb{Z}_p$\nNow, this Henselization step does not occur in the situation of $\\mathbb{C}$, essentially for the reason that its topology is much richer. More precisely, for a point $p$ in $\\mathbb{C}$, one has the ring\n$$\\mathbb{C}\\langle x-p\\rangle :=\\varinjlim_{p\\in U}\\left\\{\\begin{matrix}\\text{holomorphic functions}\\\\ U\\to \\mathbb{C}\\end{matrix}\\right\\}$$\nof power series $\\sum_{i=0}^\\infty a_i(x-p)^i$ which converge in a neighborhood of $p$. This is a Henselian local ring, with maximal ideal $(x-p)$! Again, we already knew the topology was rich enough to get 'sufficiently small' topological neighborhoods in this case, in the form of small open disks. But, there is one way in which these small open disks are not sufficient, and this will foretell how we will finally alter $\\mathbb{Z}_{(p)}^h$.\nTo understand this, let us go back to (Diff). Solving a differential equation $Df=0$ where $f$ is an entire function on $\\mathbb{C}$, can be studied by first trying to solve this equation locally at $p$. One way of interpreting this is as solving $Df=0$ for $f$ in $\\mathbb{C}\\langle x-p\\rangle$. But, in practice, we quite often actually break this step further into two easier pieces:\n\nfirst solve $Df=0$ for $f\\in\\mathbb{C}[\\![x-p]\\!]$ (i.e. arbitrary power series at $p$ with no convergence conditions),\nthen show that this solution has reasonable convergence properties (i.e. that the solution can be taken to lie in $\\mathbb{C}\\langle x-p\\rangle$).\n\nThe hallmark advantage of solving differential equations in $\\mathbb{C}[\\![x-p]\\!]$ is that it has a 'formal solvability criterion': a solution exists if and only if one can find a polynomial solution which approximates it arbitrarily well. We can represent this equationally as\n$$\\{f\\in\\mathbb{C}[\\![x-p]\\!] : Df=0\\}=\\varprojlim_n \\{g_n\\in\\mathbb{C}[x]_n: Dg_n=0\\},$$\nwhere $\\mathbb{C}[x]_n$ is the set of polynomials of degree at most $n-1$. This reduces us to the purely algebraic study of 'finite-like' objects $\\mathbb{C}[x]_n$. Again, the motto here is that while $\\mathbb{C}\\langle x-p\\rangle$ has no 'topological obstructions' to reducing study to $\\mathbb{C}[x]_n$, it does have analytic obstructions and $\\mathbb{C}[\\![x]\\!]$ does away with those.\nTo understand what the analagous picture is for $\\mathbb{Z}_{(p)}^h$, we observe that $\\mathbb{C}[x]_n$ is nothing but $\\mathbb{C}\\langle x-p\\rangle /(x-p)^n$: the quotient of this Henselian local ring by the $n^\\text{th}$ power of its maximal ideal. As, essentially by definition,\n$$\\mathbb{C}[\\![x-p]\\!]=\\varprojlim_n \\mathbb{C}\\langle x-p\\rangle/(x-p)^n,$$\n(mirroring what showed up in formal solvability criterion). Thus, the correct analogue for $\\mathbb{Z}_{(p)}^h$, with maximal ideal $(p)$, is\n$$\\mathbb{Z}_p=\\varprojlim_n \\mathbb{Z}_{(p)}^h/p^n\\mathbb{Z}^h_{(p)}=\\varprojlim_n \\mathbb{Z}_{(p)}/p^n\\mathbb{Z}_{(p)}=\\varprojlim_n \\mathbb{Z}/p^n\\mathbb{Z}.$$\nThe $p$-adic integers have appeared.\nWhat is the analogue of the formal solvability criterion here? Well, it is simply that one has the equality\n$$X_\\mathbf{f}(\\mathbb{Z}_p)=\\varprojlim_n X_\\mathbf{f}(\\mathbb{Z}/p^n\\mathbb{Z}),$$\nsomething that fails with $\\mathbb{Z}_p$ replaced by $\\mathbb{Z}_{(p)}^h$, essentially for 'convergence' reasons. Thus, our 'formal solvability criterion' again reduces us to studying equations in the 'finite-like' rings $\\mathbb{Z}/p^n\\mathbb{Z}$.\nSummary and original question\nTo summarize the above, we see that by using algebraic geometry to view Diophantine equations geometrically, the guiding principle given in (Sect) led us to\n\nconsider $\\mathbb{Z}_{(p)}$ as a possible 'sufficiently small neighborhood' of $(p)$, and to realize it's woefully 'too big',\nto consider the Henselization $\\mathbb{Z}_{(p)}^h$ to eliminate 'topological obstructions' (not coming from $\\mathrm{Spec}(\\mathbb{F}_p)$ itself) to finding solutions of equations and realizing that it does that job quite well,\nbut to realize that 'analytic' obstructions remain and so we can further replace it with $\\mathbb{Z}_p$ which is a more 'formal' object  (analogous to replacing convergent power series with all power series).\n\nHopefully this is satisfying, and convinces you why rings like $\\mathbb{Z}_{(p)}$ and $\\mathbb{Z}_p$ are ubiquitious as studying the 'local geometry' at $p$ of $\\mathbb{Z}$ (in different perspectives of 'local').\nI think four questions remain though:\n\nCan one actually work backward from $\\mathbb{Z}_p$ to $\\mathbb{Z}_{(p)}^h$? The answer, astoundingly, is almost always yes. This is the theory of Artin approximation.\nCan one actually work backward from $\\mathbb{Z}_{(p)}^h$ to an honest Nisnevich neighborhood? The answer again is essentially yes -- see this.\nOK, but can you finally profitably glue these solutions together to an actual Diophantine solution? This is the million dollar question, and the hardest. The answer is a resounding yes...sometimes. As the gluing data can be quite abstract/complicated, it's hard to enact this in practice (although it theoretically always works). Instead people often times consider 'local solutions' with some sort of 'weakened gluing data' (sometimes no gluing data at all!). These usually go under the heading of 'local-to-global principles', and generally have reasonable success.\nWhy use $\\mathbb{Z}_p$ instead of $\\mathbb{Z}_{(p)}^h$? This is fair question. Hopefully the above gives you some sort of idea of what the advantages of $\\mathbb{Z}_p$ are, but from a more literal gluing perspective $\\mathbb{Z}_{(p)}^h$ is superior. I can imagine a world, not so far from our own (maybe a future world?) where we do use $\\mathbb{Z}_{(p)}^h$ instead. In fact, I think that some of the work of Fujiwara and Kato (on 'Henselian rigid geometry') can be seen to point in that direction.\n\nAppendix: a description of $\\mathbb{Z}_{(p)}^h$\nA natural question you might have while reading the above: what does $\\mathbb{Z}_{(p)}^h$ even look like.\nLet me give a description of $R^h$ in a much more general setting. Suppose that $R$ is a local, Noetherian, excellent integral domain with maximal ideal $\\mathfrak{m}$. This includes many examples, but in particular includes the case when $R$ is a DVR with fraction field of characteristic $0$ (e.g. $\\mathbb{Z}_{(p)}$).\nFor an $R$-algebra $S$, let us define the algebraic closure of $R$ in $S$ to be those $s$ in $S$ which satisfy a non-zero polynomial $p(x)$ in $R[x]$ (NB: there is no assumption that $p(x)$ is monic!).\n\nFact: The Henselization of $R$ is the algebraic closure of $R$ in $\\widehat{R}$.\n\nProof: The map $R\\to R^h$ is local, and in fact $\\mathfrak{m}R^h$ is the maximal ideal of $R^h$ and the map $R/\\mathfrak{m}^nR \\to R^h/\\mathfrak{m}^n R^h$ is an isomorphism for all $n$. Thus, whe natural map $\\widehat{R}\\to \\widehat{R^h}$ is an isomorphism, and so $R^h\\to \\widehat{R^h}=\\widehat{R}$ is faithfully flat, and so injective. We claim that the image of $R^h\\to \\widehat{R}$ consists precisely of those elements of $\\widehat{R}$ which are algebraic over $R$. By definition, if $r$ is an element of $R^h$, then there exists a factorization $R\\to S\\to R^h$, with $R\\to S$ an etale map of local rings, and with $r$ in the image of $S$. Note that $S$ is a domain (combine this and this), and as $R\\to S$ is faithfully flat and so injective, the generic point of $S$ maps to that of $R$ under $\\mathrm{Spec}(S)\\to\\mathrm{Spec}(R)$. So the extension $\\mathrm{Frac}(S)/\\mathrm{Frac}(R)$ is finite, and so $r$ is algebraic. Conversely, suppose that $r$ in $\\widehat{R}$ is algebraic and let $p(x)\\in R[x]$ be a non-zero polynomial annihilating $r$. Since $\\widehat{R}$ is a domain, the polynomial can have at most $\\deg(p)$ many roots in $\\widehat{R}$. Write this finite set of roots as $r=x_1,x_2,\\ldots,x_m$. There exists some $N\\gg 0$ such that $r\\ne x_i\\bmod \\mathfrak{m}^N \\widehat{R}$ for $i\\ne 1$. By Artin approximation there exists some $s$ in $R^h$ which is also a root of $p(x)$ and such that $s=r\\bmod \\mathfrak{m}^M\\widehat{R}$ for some $M\\geqslant N$. But, by set up, we then have that $s=r$. $\\blacksquare$\nRemark 6:\n\n Let us give two applications of this.  First, it lets us give a simple example of an element of $\\mathbb{Z}_p$ not in $\\mathbb{Z}_{(p)}^h$. In particular, the element $\\sum_{i=0}^\\infty p^{i!}$. It suffices to show that it's not algebraic over $\\mathbb{Z}_{(p)}$ and this is fairly elementary (e.g. see An elementary example of a transcendental $p$-adic number by Suter). Second, it lets observe that although $\\mathbb{C}\\langle x-p\\rangle$ is Henselian, and contains $\\mathbb{C}[x]_{(x-p)}$, and the two have the same completion, that $\\mathbb{C}\\langle x-p\\rangle$ is not $\\mathbb{C}[x]_{(x-p)}^h$. Indeed, it suffices to observe that $\\mathbb{C}\\langle x-p\\rangle$ contains $\\exp(x)$ which is not algebraic over $\\mathbb{C}[x]_{(x-p)}$.", "meta": {"post_id": 4652524, "input_score": 29, "output_score": 42, "post_title": "Why are $p$-adic numbers ubiquitous in modern number theory?"}}
{"input": "I have three variables $x_1, x_2, x_3$ that can only be non-negative integers, i.e., $x_1 \\in \\mathbb{N}_0$, $x_2 \\in \\mathbb{N}_0$, and $x_3 \\in \\mathbb{N}_0$. Is it correct to write this in a tuple like $(x_1,x_2,x_3) \\in \\mathbb{N}_0^3$? If not, what is the correct way?", "output": "Unless you're actually considering them as a tuple elsewhere, you can also just write this as\n$$x_1, x_2, x_3 \\in \\mathbb N_0$$\nAnd I would argue that introducing the tuple and the product space solely for the purpose of specifying this constraint makes it more complicated for no reason.\nSo if you need the tuple, yes, use that notation. But if you don't need the tuple, use this one.", "meta": {"post_id": 4669178, "input_score": 15, "output_score": 38, "post_title": "Tuple notation for non-negative integers"}}
{"input": "There is a similar question here, but it's asking that if two matrices have the same\ncharacteristic polynomial then are they similar. If the answer were positive then the answer to my question will also be positive, but it's not.\nNow, if two matrices have the same characteristic polynomial then they are of the same order. Suppose $A$ and $B$ are matrices of order 2 and they both have a characteristic polynomial of $t^2+at+b$. We know that $t^2+at+b=t^2-\\text{trace}(A)t+\\text{det}(A)=t^2-\\text{trace}(B)t+\\text{det}(B)$, so in this case the answer to my question is affirmative.\nHowever, I don't think the answer is \"yes\" in general because then I probably will have a theorem which says this in my book. I looked for counterexamples but couldn't find any. Basically, I made up random matrices and never got a counterexample. So if the answer to the question is \"no\", can you please explain how you arrived at the counterexample.", "output": "If $\\chi_A$ denotes the characteristic polynomial of $A$, then $\\chi_A(0)=\\det(A)$, so the answer is yes.", "meta": {"post_id": 4724879, "input_score": 15, "output_score": 38, "post_title": "If two matrices have the same characteristic polynomial then do they have the same determinant?"}}
{"input": "I attempted the following integral from the 2022 MIT Integration Bee Qualifying Round:\n$$\\int\\frac{1}{1+\\sin x} + \\frac{1}{1+\\cos x}+ \\frac{1}{1+\\tan x} + \\frac{1}{1+\\cot x} + \\frac{1}{1+\\sec x} + \\frac{1}{1+\\csc x}dx$$\n\n$$\\int\\frac{1-\\sin x}{\\cos^2 x} + \\frac{1-\\cos x}{\\sin^2 x} + \\frac{\\cos x}{\\cos x + \\sin x} + \\frac{\\sin x}{\\cos x + \\sin x} + \\frac{\\cos x}{\\cos x + 1} + \\frac{\\sin x}{\\sin x + 1}dx$$\n\n$$x\\,+\\,\\int(\\sec^2 x -\\sec x\\tan x)\\,+\\,(\\csc^2 x - \\csc x\\cot x)dx + \\int\\frac{\\cos x(1 - \\cos x)}{\\sin^2 x}+ \\frac{\\sin x(1 - \\sin x)}{\\cos^2 x}dx $$\n\n$$x\\,+\\,\\tan x - \\sec x - \\cot x + \\csc x + \\int\\cot x\\csc x-\\cot^2 x + \\tan x\\sec x-\\tan^2 x dx $$\n\n$$x\\,+\\, \\tan x - \\sec x - \\cot x + \\csc x-\\csc x+\\sec x - \\tan x + x + \\cot x + x = \\fbox{3x} $$\n\nThis is the correct answer, but I have been trying to find a shorter way to compute this integral. Is there maybe a way to combine some of the terms in the original integral to make it shorter, or this essentially the best way to do it? Thank you.", "output": "The only fact from trigonometry we need are the reciprocal identities:\n$$\\sec x = 1/\\cos x, \\quad \\csc x = 1/\\sin x, \\quad \\cot x = 1/\\tan x$$\nUsing this, the integrand is recognized as the sum of three expressions of the form $$\\frac1{1+u}+\\frac1{1+u^{-1}} = \\frac1{1+u}+ \\frac{u}{u+1} = \\frac{1+u}{1+u} = 1$$\nThat is, the integral is $$\\int (1+1+1)\\,dx = \\int 3\\, dx = 3x+C$$", "meta": {"post_id": 4735687, "input_score": 15, "output_score": 43, "post_title": "2022 MIT Integration Bee, Qualifying Round, Question 17"}}
{"input": "We recently started with exponential functions, and I did this task for fun, but I apparently did everything wrong. I just don't get why it is wrong. I am aware of some logarithmic properties like $\\log(\\frac{x}{y})= \\log(x)-\\log(y)$ and $\\log(x\\cdot y)=\\log(x)+\\log(y)$ (though i don't really understand why that works and if $x$ and $y$ have to be variables or parameters. And I also don't know when to use these properties.)\nThe task:\nFind the intersection point of $g(x)=3\\cdot0.4^x$ and $f(x)=0.5\\cdot1.5^x$\nThis is how i did it (and the solution is wrong):\n$0.5\\cdot1.5^x=3\\cdot0.4^x$\n$(0.5\\cdot1.5^x)\\cdot\\frac{1}{3}\\cdot\\frac{1}{1.5^x}=(3\\cdot0.4^x)\\cdot\\frac{1}{3}\\cdot\\frac{1}{1.5^x}$\n$\\frac{1}{6}=\\frac{0.4^x}{1.5^x}$\n$\\frac{1}{6}=(\\frac{0.4}{1.5})^x$\n$\\frac{1}{6}=x\\cdot \\log(\\frac{4}{15})$\n$\\frac{1}{6\\cdot \\log(\\frac{4}{15})}=x$\n$-0,290$, which is terribly wrong.\n\nThe correct solution/approach:\n$0.5\\cdot1.5^x=3\\cdot 0.4^x$\n$\\log(0.5\\cdot 1.5^x)=\\log(3\\cdot 0.4^x)$\n$\\log(0.5)+\\log(1.5^x)=\\log(3)+\\log(0.4^x)$\n$\\log(0.5)+x\\cdot \\log(1.5)=\\log(3)+x\\cdot \\log(0.4)$\n$\\log(0.5)-\\log(3)=x\\cdot \\log(0.4)-x\\cdot \\log(1.5)$\n$\\log(0.5)-\\log(3)=x\\cdot (\\log(0.4)-\\log(1.5))$\n$\\frac{\\log(0.5)-\\log(3)}{\\log(0.4)-\\log(1.5)}=x=1,36$\nI would appreciate if someone could tell me why I wasn't able to divide and basically do the first approach.", "output": "The issue happens when you jump from\n$$\\frac{1}{6} = \\left (\\frac{4}{15} \\right )^{x}$$\nto\n$$\\frac{1}{6} = x\\ln\\left (\\frac{4}{15} \\right ).$$\nIn this step you have taken the logarithm of the right hand side and only the right hand side. What you need to do is take the logarithm of both sides\n$$\\ln \\left (\\frac{1}{6} \\right ) = x\\ln\\left (\\frac{4}{15} \\right ).$$\nand then divide to obtain\n$$x = \\frac{\\ln(1/6)}{\\ln(4/15)} \\approx 1.36$$", "meta": {"post_id": 4737223, "input_score": 11, "output_score": 39, "post_title": "Why can you not divide both sides of the equation, when working with exponential functions?"}}
{"input": "I am trying to evaluate the following integral using integration by parts:\n$$\\int\\frac{x}{1+e^x}dx$$\nHowever, using $u = x$, $du = 1$, $dv = \\frac{1}{1+e^x}$, $v = x-\\log(e^x+1)$, I keep getting that the  integral is $-\\text{Li}_2(-e^x)+\\frac{x^2}{2}-x\\log(1+e^x)$, but Wolfram Alpha says that the integral is $\\text{Li}_2(-e^{-x})-x\\log(e^{-x}+1)$.\nCan anyone tell me what I'm doing wrong here?", "output": "Who says you went wrong? Subtract the two answers\n$$-\\operatorname{Li}_2(-e^x) - \\operatorname{Li}_2(-e^{-x})+\\frac{x^2}{2}-x\\log(1+e^x) +x\\log(e^{-x}+1)$$\n$$=\\frac{\\pi^2}{6} +x^2 -x\\log(1+e^x) +x\\log(e^{-x}+1)$$\nAnd since $x^2 = x\\log e^x$, the rest of the terms collapse and we get that difference between the two answers is $\\frac{\\pi^2}{6}$, a constant.", "meta": {"post_id": 4742499, "input_score": 17, "output_score": 52, "post_title": "Why is integration by parts not working on this problem?"}}
{"input": "Given the null graph with no edges or vertices, we have a connected planar graph as no edges cross when this graph is drawn in the plane, and the fact that any two distinct vertices have a path between them is vacuously true. However, Euler's formula doesn't work: plugging into $v+f= e+2$, we have $1=2$. Why is this the case? Can we not apply Euler's formula here?", "output": "We can slightly generalize Euler's formula as: $V-E+F-C = 1$, where $C$ is the number of components. Most of the time $C=1$, which gives us the familiar formula. But it works great if $C>1$. And the question is about the \"trivial\" case where $C=0$. Now of course $V=E=0$ and $F=1$, giving us the right answer.\nThere is a standard inductive proof of Euler's formula which involves either removing an edge and a face, or removing an edge and a vertex, and observing that the invariant remains the same. See here. But these proofs are quite complicated, because they are trying to preserve connectedness.\nWith the generalized formula, the proof can be much cleaner (still skating over elementary topology). Remove any edge. There are two disjoint possibilities:\n(1) We merge two distinct faces, so: $E-1; F-1$\n(2) The edge always had the same face on both sides, so removing the edge instead splits a component into two: $E-1; C+1$\nIn either case the invariant $V-E+F-C$ remains the same. So we remove all the edges, and then have a bunch of isolated vertex/components in a single face, so $V=C$ and hence $V-0+1-C = 1$.\nAlways nice when a formula can apply all the way down to $0$.", "meta": {"post_id": 4749400, "input_score": 36, "output_score": 76, "post_title": "Euler's formula doesn't work for null graph?"}}
{"input": "In a first course in linear algebra, it is common for instructors to mostly restrict their attention to finite-dimensional vector spaces. These vector spaces are usually not assumed to be equipped with any additional structure, such as an inner product, norm, or a topology. On the other hand, it seems that when infinite-dimensional vector spaces are encountered in later courses, it is much more common to equip with them additional structure. Why is this? A partial answer might be that infinite-dimensional vector spaces are often studied in functional analysis, where extra structure is needed to properly define analytical concepts such as infinite series. However, I would have expected that \"pure\" infinite-dimensional vector spaces have a use in some area of mathematics.\nI have now also asked this on MathOverflow.", "output": "Consider the following example.\nVectors over, say, the field $\\mathbb{R}$, with finite dimension $n$, we'd like to prototypically understand as lists, or tuples:\n$$\\mathbf{v} = (a_1, a_2, \\cdots, a_n).$$\nNote here that the right hand is literally a tuple of reals; it is the \"meat\" of what the objects in the space \"$\\mathbb{R}^n$\" \"really are\". The reals $a_j$ are the components of the vector, which are not the same concept as coordinates, which refer to the values $b_j$ when $\\mathbf{v}$ is expressed in a basis $\\mathcal{B}$:\n$$\\mathbf{v} = b_1 \\mathbf{b}_1 + b_2 \\mathbf{b}_2 + \\cdots + b_n \\mathbf{b}_n$$\nwhere $\\mathcal{B} = \\{ \\mathbf{b}_1, \\mathbf{b}_2, \\cdots, \\mathbf{b}_n \\}$. Given a basis, any vector has coordinates, but not all vectors have components, because the elements of the actual vector space set need not be just tuples.\nBut here's the thing: it's quite obvious that, in this case, we have a basis where that the coordinates and components are identical, namely:\n$$\\begin{matrix}\n\\mathbf{e}_1 := (1, 0, 0, \\cdots, 0)\\\\\n\\mathbf{e}_2 := (0, 1, 0, \\cdots, 0)\\\\\n\\cdots\\\\\n\\mathbf{e}_n := (0 ,0, 0, \\cdots, 1)\\end{matrix}$$\nThen the vector $(a_1, a_2, \\cdots, a_n) = \\sum_{j=1}^{n} a_j \\mathbf{e}_j$ and the coordinates and components are identical. We call this the standard basis for $\\mathbb{R}^n$.\nNow, consider infinite dimension. It's quite natural to extend vectors with a finite tuple of $n$ reals, to an infinite tuple or perhaps as some might be more comfortable saying, a \"sequence\" (they are \"equivalent\" though one might call them different \"data types\"):\n$$\\mathbf{v} = (a_1, a_2, a_3, \\cdots)$$\nwhere there are now infinitely many components $a_j$. We'd like, then, by analogy with before, to write down a basis in the form\n$$\\begin{align}\n\\mathbf{e}_1 := (1, 0, 0, 0, \\cdots)\\\\\n\\mathbf{e}_2 := (0, 1, 0, 0, \\cdots)\\\\\n\\mathbf{e}_3 := (0, 0, 1, 0, \\cdots)\\\\\n\\cdots\\end{align}$$\nso that we could say\n$$\\mathbf{v} = \\sum_{i=1}^{\\infty} a_i \\mathbf{e}_i.$$\nThe problem though, is that by definition, an infinite summation like this requires us to take the following limit:\n$$\\mathbf{v} = \\lim_{n \\rightarrow \\infty} \\left(\\sum_{i=1}^{n} a_i \\mathbf{e}_i\\right).$$\nAnd the thing in the brackets is a vector, so what we have is a limit of vectors.\nBut limits don't make sense unless you have some way to compare vectors for notions like proximity, or \"approximate-ness\", or what have you!\nThus, if we want to be able to use the \"basis\" we have given above, we must add some extra structure to the space, which specifies how that limits or approximations are to work, i.e. \"who is within some tolerance of who\", so that the infinite sum is both defined at all, and so that it generates the result we'd like it to generate.\nSo no, the vector space itself doesn't \"need\" a topology, but suitably building one on it lets us do a lot of stuff we'd \"like\" to be able to do but otherwise wouldn't.", "meta": {"post_id": 4751895, "input_score": 24, "output_score": 48, "post_title": "Why are infinite-dimensional vector spaces usually equipped with additional structure?"}}
{"input": "Consider all the integers from 1-13. Without reusing any integers, find a set of three integers for which all numbers 1-13 may be created using only addition and subtraction. Prove that for all such sets, it is not possible to create a sum of 14.\nFor this question, I've identified that 1,3,9 is a possible set of integers that fulfill the requirements as follows:\n$1 = 1 \\\\ 2 = 3 - 1 \\\\ 3 = 3 \\\\ 4 = 1 + 3 \\\\ 5 = 9 - 1 - 3 \\\\ 6 = 9 - 3 \\\\ 7 = 9 + 1 - 3 \\\\ 10 = 9 + 1 \\\\ 11 = 9 + 3 - 1 \\\\ 12 = 9 + 3 \\\\ 13 = 1 + 3 + 9$\nThe obvious reason that this set of integers will not add up to 14 is that their sum is 13, smaller than 14. Hence, we cannot get a sum of 14. But I'm not sure if this is the only set of integers that fulfill the requirements and therefore cannot prove that all of these sets of integers will not create a sum of 14. Is there a rule or property I may be overlooking?\nAny assistance on this would be much appreciated!", "output": "Suppose your numbers are $\\{a_1,a_2,a_3\\}$.  Then the set of combinations you consider is $$\\left\\{\\sum \\delta_ia_i\\quad \\text {where}\\quad \\delta_i\\in \\{0, \\pm 1\\}\\right\\}$$\nA priori, there are $3^3=27$ such elements, but of course many of these are $\u22640$.  Indeed, you can always get $0$ (by taking $\\delta_i=0$ for all $i$) and if $n$ is in the set, so is $-n$.  Thus of the non-zero elements in the set, at most $\\frac {27-1}2=13$ of them are positive.  So if you can get all the thirteen numbers from  $1-13$ you can't get anything else.", "meta": {"post_id": 4788518, "input_score": 15, "output_score": 48, "post_title": "How to prove the following number theory puzzle"}}
