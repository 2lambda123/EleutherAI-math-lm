{"input": "Background: Many (if not all) of the transformation matrices used in $3D$ computer graphics are $4\\times 4$, including the three values for $x$, $y$ and $z$, plus an additional term which usually has a value of $1$.\nGiven the extra computing effort required to multiply $4\\times 4$ matrices instead of $3\\times 3$ matrices, there must be a substantial benefit to including that extra fourth term, even though $3\\times 3$ matrices should (?) be sufficient to describe points and transformations in 3D space.\nQuestion: Why is the inclusion of a fourth term beneficial? I can guess that it makes the computations easier in some manner, but I would really like to know why that is the case.", "output": "I'm going to copy my answer from Stack Overflow, which also shows why 4-component vectors (and hence 4\u00d74 matrices) are used instead of 3-component ones.\n\nIn most 3D graphics a point is represented by a 4-component vector (x, y, z, w), where w = 1. Usual operations applied on a point include translation, scaling, rotation, reflection, skewing and combination of these. \nThese transformations can be represented by a mathematical object called \"matrix\". A matrix applies on a vector like this:\n[ a b c tx ] [ x ]   [ a*x + b*y + c*z + tx*w ]\n| d e f ty | | y | = | d*x + e*y + f*z + ty*w |\n| g h i tz | | z |   | g*x + h*y + i*z + tz*w |\n[ p q r s  ] [ w ]   [ p*x + q*y + r*z +  s*w ]\n\nFor example, scaling is represented as\n[ 2 . . . ] [ x ]   [ 2x ]\n| . 2 . . | | y | = | 2y |\n| . . 2 . | | z |   | 2z |\n[ . . . 1 ] [ 1 ]   [ 1  ]\n\nand translation as\n[ 1 . . dx ] [ x ]   [ x + dx ]\n| . 1 . dy | | y | = | y + dy |\n| . . 1 dz | | z |   | z + dz |\n[ . . . 1  ] [ 1 ]   [   1    ]\n\nOne of the reason for the 4th component is to make a translation representable by a matrix.\nThe advantage of using a matrix is that multiple transformations can be combined into one via matrix multiplication.\nNow, if the purpose is simply to bring translation on the table, then I'd say (x, y, z, 1) instead of (x, y, z, w) and make the last row of the matrix always [0 0 0 1], as done usually for 2D graphics. In fact, the 4-component vector will be mapped back to the normal 3-vector vector via this formula:\n[ x(3D) ]   [ x / w ]\n| y(3D) ] = | y / w |\n[ z(3D) ]   [ z / w ]\n\nThis is called homogeneous coordinates. Allowing this makes the perspective projection expressible with a matrix too, which can again combine with all other transformations.\nFor example, since objects farther away should be smaller on screen, we transform the 3D coordinates into 2D using formula\nx(2D) = x(3D) / (10 * z(3D))\ny(2D) = y(3D) / (10 * z(3D))\n\nNow if we apply the projection matrix\n[ 1 . .  . ] [ x ]   [  x   ]\n| . 1 .  . | | y | = |  y   |\n| . . 1  . | | z |   |  z   |\n[ . . 10 . ] [ 1 ]   [ 10*z ]\n\nthen the real 3D coordinates would become\nx(3D) := x/w = x/10z\ny(3D) := y/w = y/10z\nz(3D) := z/w = 0.1\n\nso we just need to chop the z-coordinate out to project to 2D.", "meta": {"post_id": 336, "input_score": 57, "output_score": 54, "post_title": "Why are $3D$ transformation matrices $4 \\times 4$ instead of $3 \\times 3$?"}}
{"input": "I know that the harmonic series $1 + \\frac12 + \\frac13 + \\frac14 + \\cdots$ diverges. I also know that the sum of the inverse of prime numbers $\\frac12 + \\frac13 + \\frac15 + \\frac17 + \\frac1{11} + \\cdots$ diverges too, even if really slowly since it's $O(\\log \\log n)$.\nBut I think I read that if we consider the numbers whose decimal representation does not have a certain digit (say, 7) and sum the inverse of these numbers, the sum is finite (usually between 19 and 20, it depends from the missing digit). Does anybody know the result, and some way to prove that the sum is finite?", "output": "It is not very surprising that the sum is finite, since numbers without a 7 (or any other digit) get rarer and rarer as the number of digits increases.\nHere's a proof.\nLet $S$ be the harmonic series with all terms whose denominator contains the digit $k$ removed. We can write $S =S_1 + S_2 + S_3 + \\ldots$, where $S_i$ is the sum of all terms whose denominator contains exactly $i$ digits, all different from $k$.\nNow, the number of $i$-digit numbers that do not contain the digit $k$ is $8\\cdot9^{i-1}$ (there are $8$ choices for the first digit, excluding $0$ and $k$, and $9$ choices for the other digits). [Well, if $k=0$ there are $9$ choices for the first digit, but the proof still works.] So there are $8\\cdot9^{i-1}$ numbers in the sum $S_i$.\nNow each number in $S_i$ is of the form $\\frac1a$, where $a$ is an $i$-digit number. So $a \\geq 10^{i-1}$, which implies $\\frac1a \\leq \\frac1{10^{i-1}}$.\nTherefore $S_i \\leq 8\\cdot\\dfrac{9^{i-1} }{10^{i-1}} = 8\\cdot\\left(\\frac9{10}\\right)^{i-1}$.\nSo $S= \\sum S_i \\leq \\sum 8\\cdot\\left(\\frac9{10}\\right)^{i-1}$\nwhich is a  geometric series of ratio $\\frac9{10} < 1$, which converges. Since $S$ is a positive series bounded above by a converging series, $S$ converges.", "meta": {"post_id": 387, "input_score": 46, "output_score": 63, "post_title": "Sum of reciprocals of numbers with certain terms omitted"}}
{"input": "What's the meaning of the double turnstile symbol in logic or mathematical notation? :\n\n$\\models$", "output": "Just to enlarge on Harry's answer:\nYour symbol denotes one of two specified notions of implication in formal logic\n$\\vdash$ -the turnstile symbol denotes syntactic implication (syntactic here means  related to syntax, the structure of a sentence), where the 'algebra' of the logical system in play (for example sentential calculus) allows us to 'rearrange and cancel' the stuff we know on the left into the thing we want to prove on the right.\nAn example might be the classic \"all men are mortal $\\wedge$ socrates is a man $\\vdash$ socrates is mortal\" ('$\\wedge$' of course here just means 'and'). You can almost imagine cancelling out the 'man bit' on the left to just give the sentence on the right (although the truth may be more complex...).\n\n$\\models$ -the double turnstile, on the other hand, is not so much about algebra as meaning (formally it denotes semantic implication)- it means that any interpretation of the stuff we know on the left must have the corresponding interpretation of the thing we want to prove on the right true.\nAn example would be if we had an infinite set of sentences: $\\Gamma$:= {\"1 is lovely\", \"2 is lovely\", ...} in which all numbers appear, and the sentence A= \" the natural numbers are precisely {1,2,...}\" listing all numbers. Any interpretation would give us B=\"all natural numbers are lovely\". So $\\Gamma$, A $\\models$ B.\n\nNow, the goal of any logician trying to set up a formal system is to have $\\Gamma \\vdash A \\iff \\Gamma \\models A$, meaning that the 'algebra' must line up with the interpretation, and this is not something we can take as given. Take the second example above- can we be sure that algebraic operations can 'parse' those infinitely many sentences and make the simple sentence on the right?? (this is to do with a property called compactness)\nThe goal can be split into two distinct subgoals:\nSoundness: $A \\vdash B \\Rightarrow A \\models B$\nCompleteness: $A \\models B \\Rightarrow A \\vdash B$\nWhere the first stops you proving things that aren't true when we interpret them and the second means that everything we know to be true on interpretation, we must be able to prove.\nSentential calculus, for example, can be proved complete (and was in Godel's lesser known, but celebrated completeness theorem), but for other systems Godel's incompleteness theorem, give us a terrible choice between the two.\n\nIn summary: The interplay of meaning and axiomatic machine mathematics, captured by the difference between $\\models$ and $\\vdash$, is a subtle and interesting thing.", "meta": {"post_id": 469, "input_score": 40, "output_score": 72, "post_title": "What is the meaning of the double turnstile symbol ($\\models$)?"}}
{"input": "I just started learning model theory on my own, and I was wondering if there are any interesting examples of two structures of a language L which are not isomorphic, but are elementarily equivalent (this means that any L-sentence satisfied by one of them is satisfied by the second).\nI am using the notation of David Marker's book \"Model theory: an introduction\".", "output": "First, I'm glad you are reading my book! :)\nLet me make a couple of comments on Pete's answer--this is my first time here and I don't see how to leave comments.\n\nAny two dense linear orders without endpoints are elementarily equivalent.\nIn particular $(Q,<)$ and $(R,<)$ are elementarily equivalent.  So there is no first order way of expressing the completeness of the reals.\n\nAny two algebraically closed fields of the same characteristic are elementarily equivalent.  So the algebraic numbers\nis elementarily equivalent to the complex numbers.  This means you can prove first order things about the algebraic numbers using complex analysis or the complex numbers using Galois Theory or countability.\n\nSimilarly the reals field is elementarily equivalent to the real algebraic numbers or to the field of real Puiseux series.\nOne can for example use the Puiseux series to prove asymptotic properties of semialgebraic functions.\n\n\nFinally, Pete's comment 5) about infinite models of the theory of finite fields being elementarily equivalent isn't quite right.  This is only true if the relative algebraic closure of the prime fields are isomorphic.\nFor example,\na) take an  ultrapower of finite fields $F_p$ where the ultrafilter containing $\\{2,4,8,\\ldots\\}$ then the resulting model has\ncharacteristic $2,$ while if the ultrafilter contains the set of primes, then the ultraproduct has characteristic $0.$\nb) if the ultrapower contains the set of primes congruent to $1 \\bmod 4,$ in the ultraproduct $-1$ is a square, while if\nthe ultraproduct contains the set of primes congruent to $3 \\mod 4$ then in the ultraproduct $-1$ is not a square..", "meta": {"post_id": 1563, "input_score": 23, "output_score": 52, "post_title": "Example of non-isomorphic structures which are elementarily equivalent"}}
{"input": "I know a little bit of the theory of compact Riemann surfaces, wherein there is a very nice divisor -- line bundle correspondence.\nBut when I take up the book of Hartshorne, the notion of Cartier divisor there is very confusing. It is certainly not a direct sum of points; perhaps it is better to understand it in terms of line bundles. But Cartier divisors do not seem to be quite the same thing as line bundles. The definition is hard to figure out. Can someone clear the misunderstanding for me and explain to me how best to understand Cartier divisors?", "output": "When discussing divisors, a helpful distinction to make at the beginning is effective divisors vs. all divisors.  Normally effective divisors have a more geometric description; all divisors can then be obtained from the effective ones by allowing some minus signs to come into the picture.\nAn irreducible effective Weil divisor on a variety $X$ is the same thing as an irreducible codimension one subvariety, which in turn is the same thing as a height one point $\\eta$ of $X$.  (We get $\\eta$ as the generic point of the irred. codim'n one subvariety, and we recover the subvariety as the closure of $\\eta$.)\n An effective Weil divisor is a non-negative integral linear combination of irreducible ones, so you can think of it as a non-negative integral linear combination of height one points $\\eta$.\nTypically, one restricts to normal varieties, so that all the local rings at height one points  are DVRs.    Then, given any pure codimension one subscheme $Z$ of $X$, you can attach a Weil\ndivisor to $Z$, in the following way:\nbecause the local rings at height one points are DVRs, if $Z$ is any codimension one subscheme of $X$, cut out by an ideal sheaf $\\mathcal I_Z$,  and $\\eta$ is a height one point, then the stalk $\\mathcal I_{Z,\\eta}$ is\nan ideal in the DVR $\\mathcal O_{X,\\eta}$, thus is just some power of the maximal ideal\n$\\mathfrak m_{\\eta}$ (using the DVR property), say $\\mathcal I_{Z,\\eta} = \\mathfrak m_{\\eta}^{m_{Z,\\eta}},$ and so the multiplicity $m$ of $Z$ at $\\eta$ is well-defined.\nThus the effective Weil divisor $$div(Z) := \\sum_{\\eta \\text{ of height one}} m_{Z,\\eta}\\cdot  \\eta$$\nis well-defined.\nNote that this recipe only goes one way: starting with the Weil divisor, we can't recover $Z$, because the Weil divisor does not remember all the scheme structure (i.e. the whole\nstructure sheaf, or equivalently, the whole ideal sheaf) of $Z$, but only its behaviour at its generic points (which amounts to the same thing as remembering the irreducible components and their multiplicities). \nAn effective Cartier divisor is actually a more directly geometric object, namely, \nit is a locally principal pure codimension one subscheme, that is,\na subscheme, each component of which is codimension one, and which, locally around each point, is the zero locus of a section of the structure sheaf.   Now in order to cut out\na pure codimension one subscheme as its zero locus, a section of the structure\nsheaf should be regular (in the commutative algebra sense), i.e. a non-zero divisor.\nAlso, two regular sections will cut out the same zero locus if their ratio is a unit\nin the structure sheaf.  So if we let $\\mathcal O_X^{reg}$ denote the subsheaf of\n$\\mathcal O_X$ whose sections are regular elements (i.e. non-zero divisors in each stalk),\nthen the equation of a Cartier divisor is a well-defined global section of the quotient\nsheaf\n$\\mathcal O_X^{reg}/\\mathcal O_X^{\\times}$.\nNow suppose that we are on a smooth variety.  Then any irreducible codimension one subvariety\nis in fact locally principal, and so given a Weil divisor \n$$D = \\sum_{\\eta \\text{ of height one}} m_{\\eta} \\cdot\\eta,$$\nwe can actually canonically attach a Cartier divisor to it, in the following way:\nin a n.h. of some point $x$, let $f_{\\eta}$ be a local equation for the Zariski closure\nof $\\eta$; then if $Z(D)$ is cut out locally by $\\prod_{\\eta} f_{\\eta}^{m_{\\eta}} = 0,$\nthen $Z(D)$ is locally principal by construction, and, again by construction,\n$div(Z(D)) = D.$\nSo in the smooth setting,\nwe see that $Z \\mapsto div(Z)$ and $D \\mapsto Z(D)$ establish a bijection between\neffective Cartier divisors and effective Weil divisors.\nOn the other hand, on a singular variety, it can happen that an irreducible codimension one subvariety need not be locally principal in the neighbourhood of a singular point (e.g. a generating line on the cone $x^2 +y^2 + z^2 = 0$\nin $\\mathbb A^3$ is not locally\nprincipal in any neighbourhood of the cone point).  Thus there can be Weil divisors that\nare not of the form $div(Z)$ for any Cartier divisor $Z$.\nTo go from effective Weil divisor to all Weil divisors, you just allow negative coefficients.\nTo go from effective Cartier divisors to all Cartier divisors, you have to allow yourself \nto invert the functions $f$ that cut out the effective Cartier divisors, or equivalently, to go from the sheaf of monoids $\\mathcal O_X^{reg}/\\mathcal O_X^{\\times}$ to the associated sheaf of groups, which is $\\mathcal K_X^{reg}/\\mathcal O_X^{\\times}.$\n(Here, it helps to remember that $\\mathcal K_X$ is obtained from $\\mathcal O_X$\nby inverting non-zero divisors.)\nFinally, for the connection with line bundles: if $\\mathcal L$ is a line bundle,\nand $s$ is a regular section (i.e. a section whose zero locus is pure codimension one,\nor equivalently, a section which, when we choose a local isomorphism $\\mathcal L_{| U}\n\\cong \\mathcal O_U$, is not a zero divisor), then the zero locus $Z(s)$ of $s$\nis an effective Cartier divisor, essentially by definition.\nSo we have a map $(\\mathcal L,s) \\mapsto Z(s)$ which sends line bundles with regular sections to effective Cartier divisors.  This is in fact an isomorphism of monoids\n(where on the left we consider pairs $(\\mathcal L,s)$ up to isomorphism of pairs):\ngiven an effective Cartier divisor $D$, we can define $\\mathcal O(D)$ to be the\nsubsheaf of $\\mathcal K_X$ consisting (locally) of sections $f$ such that the locus of\npoles of $f$ (a well-defined Cartier divisor) is contained (as a subscheme)in the Cartier\ndivisor $D$ (perhaps less intuitively, but more concretely: if $D$ is locally cut out\nby the equation $g = 0$, then $\\mathcal O(D)$ consists (locally) of sections $f$\nof $\\mathcal K_X$ such that $fg$ is in fact a section of $\\mathcal O_X$).\nThe constant function $1$ certainly lies in $\\mathcal O(D)$, and (thought of as a section\nof $\\mathcal O(D)$ -- not as a function!) its zero locus is exactly $D$.\nThus $D \\mapsto (\\mathcal O(D), 1)$ is an inverse to the above map $(\\mathcal L,s) \\mapsto\nZ(s)$.\nFinally, if we choose two different regular sections of the same line bundle,\nthe corresponding Cartier divisors are linearly equivalent.  Thus we are led to the\nisomorphism \"line bundles up to isomorphism = Cartier divisors up to linear equivalence\".\nBut, just to emphasize, to understand this it is best to restrict first to line bundles\nwhich admit a regular section, and then think of the corresponding Cartier divisor as being\nthe zero locus of that section.   This brings out the geometric nature of the Cartier divisor quite clearly.", "meta": {"post_id": 1926, "input_score": 73, "output_score": 110, "post_title": "Divisor -- line bundle correspondence in algebraic geometry"}}
{"input": "Does $S_k= \\sum \\limits_{n=1}^{\\infty}\\sin(n^k)/n$ converge for all $k>0$? \nMotivation: I recently learned that $S_1$ converges. I think $S_2$ converges by the integral test. Was the question known in general?", "output": "This is a replacement for my previous answer. The sum converges, and this fact needs even more math than I believed before.\nBegin by using summation by parts. This gives \n$$\\sum_{n=1}^N \\left(\\sum_{m=1}^N \\sin(m^k) \\right) \\left( \\frac{1}{n}-\\frac{1}{n+1}\\right) + \\frac{1}{N+1} \\left(\\sum_{m=1}^N \\sin(m^k) \\right).$$ \nWrite $S_n:= \\left(\\sum_{m=1}^n \\sin(m^k) \\right)$. So this is\n$$\\sum_{n=1}^N S_n/(n(n+1)) + S_N/(N+1).$$\nThe second term goes to zero by Weyl's polynomial equidistribution theorem. So your question is equivalent to the question of whether $\\sum s_n/(n(n+1))$ converges. We may as well clean this up a little: Since $|S_n| \\leq n$, we know that $\\sum S_n \\left( 1/n(n+1) - 1/n^2 \\right)$ converges. So the question is whether\n$$\\sum \\frac{S_n}{n^2}$$\nconverges.\nI will show that $S_n$ is small enough that $\\sum S_n/n^2$ converges absolutely.\nThe way I want to prove this is to use Weyl's inequality. Let $p_i/q_i$ be an infinite sequence of rational numbers such that $|1/(2 \\pi) - p_i/q_i| < 1/q_i^2$. Such a sequence exists by a standard lemma. Weyl inequality gives that\n$$S_N = O\\left(N^{1+\\epsilon} (q_i^{-1} + N^{-1} + q_i N^{-k})^{1/2^{k-1}} \\right)$$\nfor any $\\epsilon>0$.\n\nThanks to George Lowther for pointing out the next step: According to Salikhov, for $q$ sufficiently large, we have\n$$|\\pi - p/q| > 1/q^{7.60631+\\epsilon}.$$\nSince $x \\mapsto 1/(2x)$ is Lipschitz near $\\pi$, and since $p/q$ near $\\pi$ implies that $p$ and $q$ are nearly proportional, we also have the lower bound $|1/(2 \\pi) - p/q|> 1/q^{7.60631+\\epsilon}$. \nLet $p_i/q_i$ be the convergents of the continued fraction of $1/(2 \\pi)$. By a standard result, $|1/(2 \\pi) - p_i/q_i| \\leq 1/(q_i q_{i+1})$. Thus, $q_{i+1} \\leq q_i^{6.60631 + \\epsilon}$ for $i$ sufficiently large. Thus, the intervals $[q_i, q_i^{7}]$ contain all sufficiently large integers.\nFor any large enough $N$, choose $q_i$ such that $N^{k-1} \\in [q_i, q_i^7]$. Then Weyl's inequality gives the bound\n$$S_N = O \\left( N^{1+\\epsilon} \\left(N^{-(k-1)/7} + N^{-1} + N^{-1} \\right)^{1/2^{k-1}}\\right)$$ \nSo $$S_N = \\begin{cases} O(N^{1-(k-1)/(7\\cdot 2^{k-1}) + \\epsilon}) &\\mbox{ if } \\ k\\leq 7, \\\\\nO(N^{1-1/(2^{k-1})+\\epsilon}) &\\mbox{ if } \\ k\\geq 8, \\end{cases}$$\nwhich is enough to make sure the sum converges.\n${        }{}{}{}{}$", "meta": {"post_id": 2270, "input_score": 51, "output_score": 60, "post_title": "Convergence of $\\sum \\limits_{n=1}^{\\infty}\\sin(n^k)/n$"}}
{"input": "My nephew was folding laundry, and turning the occasional shirt right-side-out. I showed him a  \"trick\" where I turned it right-side-out by pulling the whole thing through a sleeve instead of the bottom or collar of the shirt. He thought it was really cool (kids are easily amused, and so am I).\nSo he learned that you can turn a shirt or pants right-side-out by pulling the material through any hole, not just certain ones. I told him that even if there was a rip in the shirt, you could use that to turn it inside-out or right-side-out, and he was fascinated by this and asked \"why?\"\nI don't really know the answer to this. Why is this the case? What if the sleeves of a long-sleeve shirt were sewn together at the cuff, creating a continuous tube from one sleeve to the other? Would you still be able to turn it right-side-out? Why? What properties must a garment have so that it can be turned inside-out and right-side-out?\nSorry if this is a lame question, but I've always wondered. I wouldn't even know what to google for, so that is why I am asking here.\nIf you know the answer to this, could you please put it into layman's terms?\nUpdate: Wow, I really appreciate all the participation. This is a really pleasant community and I have learned a lot here. It seems that the answer is that you need at least one puncture in the garment through which to push or pull the fabric. It appears that you can have certain handles, although it's not usually practical with clothing due to necessary stretching.\nAccepted (a while ago actually -- sorry for not updating sooner) Dan's answer because among the answers that I understand, it is the highest ranked by this community.", "output": "I'm going to try to give a lighter-flavoured version of my previous answer.  I'd rather not edit the previous one anymore so here goes another response.  I want to make clear, this response is to you, not your 10-year-old nephew.  How you translate this response to any person depends more on you and that person than anything else. \nTake a look at the Wikipedia page for diffeomorphism. In particular,the lead image \nWhen I look at that image I see the standard Cartesian coordinate grid, but deformed a little. \n\nThere's a \"big theorem\" in a subject called Manifold Theory and it's name is the \"Isotopy Extension Theorem\".  Moreover, it has a lot to do with these kinds of pictures. \nThe isotopy extension theorem is roughly this construction: say you have some rubber, and it's sitting in a medium of liquid epoxy that's near-set.  Moreover, imagine the epoxy to be multi-coloured.  So when you move the rubber bit around in the epoxy, the epoxy will \"track\" the rubber object.  If your epoxy had a happy-face coloured into it originally, after you move the rubber, you'll see a deformed happy-face.  \n\n\nSo you get images that look a lot like mixed paint.  Stir various blotches of paint, and the paint gets distorted.  The more you stir, the more it mixes and it gets harder and harder to see the original image.  The important thing is that the mixed paint is something of a \"record\" of how you moved your rubber object.  And if your motion of the rubber object returns it to its initial position, there is a function\n$$ f : X \\to X $$\nwhere $X$ is all positions outside your rubber object.  Given $x \\in X$ you can ask where the particle of paint at position $x$ went after the mixing, and call that position $f(x)$. \nAll my talk about fibre bundles and homotopy-groups in the previous response was a \"high level\" encoding of the above idea.  An intermediate step in the formalization of this idea is the solution of an ordinary differential equation, and that differential equation is essentially the \"paint-mixing idea\" above, in case you want to look at this subject in more detail later. \nSo what does this mean?  A motion of an object from an initial position back to the initial position gives you an idea of how to \"mix paint\" outside the object.  Or said another way, it gives you an Automorphism of the complement, in our case that's a 1-1, continuous bijective function between 3-dimensional space without the garment and itself. \nYou may find it odd but mathematicians have been studying \"paint mixing\" in all kinds of mathematical objects, including \"the space outside of garments\" and far more bizarre objects for well over 100 years.  This is the subject of dynamical systems.  \"Garment complements\" are a very special case, as these are subsets of 3-dimensional euclidean space and so they're 3-manifolds.  Over the past 40 years our understanding of 3-manifolds has changed and seriously altered our understanding of things.  To give you a sense for what this understanding is, let's start with the basics.  3-manifolds are things that on small scales look just like \"standard\" 3-dimensional Euclidean space.   So 3-manifolds are an instance of \"the flat earth problem\".  Think about the idea that maybe the earth is like a flat sheet of paper that goes on forever.  Some people (apparently) believed this at some point. And superficially, as an idea, it's got some things going for it.  The evidence that the earth isn't flat requires some build-up. \n\nAnyhow, so 3-manifolds are the next step.  Maybe all space isn't flat in some sense.  That's a tricky concept to make sense of as space isn't \"in\" anything -- basically by definition whatever space is in we'd call space, no?  Strangely, it's not this simple.  A guy named Gauss discovered that there is a way to make sense of space being non-flat without space sitting in something larger. Meaning curvature is a relative thing, not something judged by some exterior absolute standard.  This idea was a revelation and spawned the idea of an abstract manifold.  To summarize the notion, here is a little thought experiment. \nImagine a rocket with a rope attached to its tail, the other end of the rope fixed to the earth.  The rocket takes off and goes straight away from the earth.  Years later, the rocket returns from some other direction, and we grab both loose ends of the rope and pull.  We pull and pull, and soon the rope is tight.  And the rope doesn't move, it's taut. as if it was stuck to something.  But the rope isn't touching anything except your hands.  Of course you can't see all the rope at one time as the rope is tracing out the (very long) path of the rocket. But if you climb along the rope, after years you can verify: it's finite in length, it's not touching anything except where it's pinned-down on the earth.  And it can't be pulled in.  \nThis is what a topologist might call a hole in the universe.  We have abstract conceptions of these types of objects (\"holes in the universe\") but by their nature they're not terribly easy to visualize -- not impossible either, but it takes practice and some training.\nIn the 1970's by the work of many mathematicians we started to achieve an understanding of what we expected 3-manifolds to be like.  In particular we had procedures to construct them all, and a rough idea of how many varieties of them there should be.  The conjectural description of them was called the geometrization conjecture. It was a revelation in its day, since it implied that many of our traditional notions of geometry from studying surfaces in 3-dimensional space translate to the description of all 3-dimensional manifolds.   The geometriztion conjecture was recently proven in 2002. \nThe upshot of this theory is that in some sense 3-dimensional manifolds \"crystalize\" and break apart in certain standard ways.  This forces any kind of dynamics on a 3-manifold (like \"paint mixing outside of a garment\") to respect that crystalization. \nSo how do I find a garment you can't turn inside-out?  I manufacture one so that its exterior crystalizes in a way I understand.  In particular I find a complement that won't allow for this kind of turning inside-out.  The fact that these things exist is rather delicate and takes work to see.  So it's not particularly easy to explain the proof.  But that's the essential idea. \nEdit: To say a tad more, there is a certain way in which this \"crystalization\" can be extremely beautiful.  One of the simplest types of crystalizations happens when you're dealing with a finite-volume hyperbolic manifold.  This happens more often than you might imagine -- and it's the key idea working in the example in my previous response.  The decomposition in this case is very special as there's something called the \"Epstein-Penner decomposition\" which gives a canonical way to cut the complement into convex polytopes.  Things like tetrahedra, octahedra, icosahedra, etc, very standard objects.  So understanding the dynamics of \"garments\" frequently gets turned into (ie the problem \"reduces to\") the understanding of the geometry of convex polytopes -- the kind of things Euclid was very comfortable with.  In particular there's software called \"SnapPea\" which allows for rather easy computations of these things. \n\n(source: utk.edu) \n\nImages taken from Morwen Thistlethwaite's webpage.  These are images of the closely-related notion of a \"Dirichlet domain\". \nHere is an image of the Dirichlet domain for the complement of $8_{17}$, the key idea in the construction of my previous post.\nDirichlet domain for the complement of $8_{17}$\nTechnically, this in the Poincare model for hyperbolic space, which gives it the jagged/curvy appearance.", "meta": {"post_id": 2755, "input_score": 626, "output_score": 192, "post_title": "Why can you turn clothing right-side-out?"}}
{"input": "Is there a comprehensive resource listing binomial identities? I am more interested in combinatorial proofs of such identities, but even a list without proofs will do.", "output": "The most comprehensive list I know of is H.W. Gould's Combinatorial Identities.  It is available directly from him if you contact him.  He also has some pdf documents available for download from his web site.  Although he says they do \"NOT replace [Combinatorial Identities] which remains in print with supplements,\" they still contain many more binomial identities even than in Concrete Mathematics.  In general, Gould's work is a great resource for this sort of thing; he has spent much of his career collecting and proving combinatorial identities. \nAdded: Another useful reference is John Riordan's Combinatorial Identities.  It's hard to pick one of its 250 pages at random and not find at least one binomial coefficient identity there.  Unfortunately, the identities are not always organized in a way that makes it easy to find what you are looking for.  Still it's a good resource.", "meta": {"post_id": 3085, "input_score": 69, "output_score": 47, "post_title": "A comprehensive list of binomial identities?"}}
{"input": "Why don't $3$-cycles generate the symmetric group? was asked earlier today. The proof is essentially that $3$-cycles are even permutations, and products of even permutations are even.\nSo: do the $3$-cycles generate the alternating group? Similarly, do the $k$-cycles generate the alternating group when $k$ is odd?\nAnd do the $k$-cycles generate the symmetric group when $k$ is even? I know that transpositions ($2$-cycles) generate the symmetric group.", "output": "If $n\\geq5$, then the only normal subgroups of the symmetric group $S_n$ are the trivial group, the alternating group and the symmetric group itself. Since the $k$-cycles form a full conjugacy class, it follows that the subgroup they generate is normal. This determines everything if $n \\geq 5$.\nMore specifically: the $k$-cycles in $S_n$ generate the alternating group if $k$ is odd and $k \\ne 1$; they generate the full symmetric group if $k$ is even.", "meta": {"post_id": 3667, "input_score": 20, "output_score": 36, "post_title": "What do all the $k$-cycles in $S_n$ generate?"}}
{"input": "I know that $1$ is not a prime number because $1\\cdot\\mathbb Z=\\mathbb Z$ is, by convention, not a prime ideal in the ring $\\mathbb Z$.\nHowever, since $\\mathbb Z$ is a domain, $0\\cdot\\mathbb Z=0$ is a prime ideal in $\\mathbb Z$. Isn't $(p)$ being a prime ideal the very definition of $p$ being a prime element?\n(I know that this would violate the Fundamental Theorem of Arithmetic.)\n\nEdit:\nApparently the answer is that a prime element in a ring is, by convention a non-zero non-unit (see wikipedia).\nThis is strange because a prime ideal of a ring is, by convention, a proper ideal but not necessarily non-zero (see wikipedia).\nSo, my question is now: Why do we make this awkward convention?", "output": "You have a point here: absolutely we want to count $(0)$ as a prime ideal in $\\mathbb{Z}$ -- because $\\mathbb{Z}$ is an integral domain -- whereas we do not want to count $(1)$ as being a prime ideal -- because the zero ring is not an integral domain (which, to me, is much more a true fact than a convention: e.g., every integral domain has a field of fractions, and the zero ring does not).\nI think we do not want to call $0$ a prime element because, in practice, we never want to include $0$ in divisibility arguments.  Another way to say this is that we generally want to study factorization in integral domains, but once we have specified that a commutative ring $R$ is a domain, we know all there is to know about factoring $0$: $0 = x_1 \\cdots x_n$ iff at least one $x_i = 0$.\nHere is one way to make this \"ignoring $0$\" convention look more natural: the notions of factorization, prime element, irreducible element, and so forth in an integral domain $R$ depend entirely on the multiplicative structure of $R$.  Thus we can think of factorization questions as taking place in the cancellative monoid $(R \\setminus 0,\\cdot)$.  (Cancellative means: if $x \\cdot y = x \\cdot z$, then $y = z$.)  In this context it is natural to exclude zero, because otherwise the monoid would not be cancellative.  Contemporary algebraists often think about factorization as a property of monoids rather than integral domains per se.  For a little more information about this, see e.g. Section 4.1 of http://alpha.math.uga.edu/~pete/factorization2010.pdf.", "meta": {"post_id": 3698, "input_score": 49, "output_score": 41, "post_title": "Why doesn't $0$ being a prime ideal in $\\mathbb Z$ imply that $0$ is a prime number?"}}
{"input": "If memory serves, ten years ago to the week (or so), I taught first semester freshman calculus for the first time.  As many calculus instructors do, I decided I should ask some extra credit questions to get students to think more deeply about the material.  The first one I asked was this:\n1) Recall that a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is said to have a removable discontinuity at a point $x_0 \\in \\mathbb{R}$ if $\\lim_{x \\rightarrow x_0} f(x)$ exists but not does not equal $f(x_0)$.  Does there exist a function $f$ which has a removable discontinuity at $x_0$ for every $x_0 \\in \\mathbb{R}$?\nCommentary: if so, we could define a new function $\\tilde{f}(x_0) = \\lim_{x \\rightarrow x_0} f(x)$ and it seems at least that $\\tilde{f}$ has a fighting chance to be continuous on $\\mathbb{R}$.  Thus we have successfully \"removed the discontinuities\" of $f$, but in so doing we have changed the value at every point!  \nRemark: Lest you think this is too silly to even seriously contemplate, consider the function $f: \\mathbb{Q} \\rightarrow \\mathbb{Q}$ given by $f(0) = 1$ and for a nonzero \nrational number $\\frac{p}{q}$, $f(\\frac{p}{q}) = \\frac{1}{q}$.  It is easy to see that this function has limit $0$ at every (rational) point!\nSo I mentioned this problem to my students.  A week later, the only person who asked me about it at all was my Teaching Assistant, who was an older undergraduate, not even a math major, I think.  (I hasten to add that this was not in any sense an honors calculus class, i.e., I was pretty clueless back then.)  Thinking about it a bit, I asked him if he knew about uncountable sets, and he said that he didn't.  At that point I realized that I didn't have a solution in mind that he would understand (so still less so for the freshman calculus students) and I advised him to forget all about it.\n\nSo my actual question is: can you solve this problem using only the concepts in a non-honors freshman calculus textbook?  (In particular, without using notions of un/countability?)\n[Addendum: Let me say explicitly that I would welcome an answer that proceeds directly in terms of the least upper bound axiom.  Most freshman calculus books do include this, albeit somewhere hidden from view of the casual readers, i.e., actual freshman calculus students.]\n\nIf you can't figure out how to answer the question at all, I think the following related question helps.\n2) Define a function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ to be precontinuous if the limit exists at every point.  For such a function, we can define $\\tilde{f}$ as above.  Prove/disprove that, as suggested above, $\\tilde{f}$ is indeed continuous.  [Then think about $f - \\tilde{f}$.]\nNow that I think about it, there is an entire little area here that I don't know anything about, e.g.\n3) The set of discontinuities of an arbitrary function is known -- any $F_{\\sigma}$ set inside $\\mathbb{R}$ can serve.  What can we say about the set of discontinuities of a \"precontinuous function\"?  [Edit: from the link provided in Chandru1's answer, we see that it is countable.  What else can we say?  Note that taking the above example and extending by $0$ to the irrationals, we see that the set of points of discontinuity of a precontinuous function can be dense.]", "output": "I think the following works:\nHere is a sketch, I will fill in the details later if required.\nLet $g(x) = \\lim_{t\\rightarrow x} f(t)$. Then we can show that $g(x)$ is continuous.\nLet $h(x) = f(x) - g(x)$. Then $\\lim_{t \\rightarrow x} h(t)$ exists and is $0$ everywhere.\nWe will now show that $h(c) = 0$ for some $c$. \nThis will imply that $f(x)$ is continuous at $c$ as then we will have $f(c) = g(c) = \\lim_{t->c} f(t)$.\nConsider any point $x_0$.\nBy limit of $h$ at $x_0$ being $0$, there is a closed interval $I_0$ (of length > 0) such that $|h(x)| < 1$ for all $x \\in I_0$.\nThis is because, given an $\\epsilon > 0$ there is a $\\delta > 0$ such that $|h(x)| < \\epsilon$ for all $x$ such that $0 < |x - x_{0}| < \\delta$. Pick $\\epsilon = 1$ and pick $I_{0}$ to be any closed interval of non-zero length in $(x_{0}, x_{0} + \\delta)$.\nNow pick any point $x_1$ in $I_0$.\nBy limit of $h$ at $x_1$ being $0$, there is a closed interval $I_1 \\subset I_0$ (of length > 0) such that $|h(x)| < 1/2$ for all $x \\in I_1$, by argument similar to above.\nContinuing this way, we get a sequence of closed intervals $I_n$ such that\n$|h(x)| < \\frac{1}{n+1}$ for all $x \\in I_n$. We also have that $I_{n+1} \\subset I_n$ for each $n$, and that length $I_n$ > 0. We could also arrange so that length $I_n \\rightarrow 0$.\nNow there is a point $c$ (by completeness of $\\mathbb{R}$) such that $c \\in \\bigcap_{n=0}^{\\infty}I_{n}$.\nThus we have that $|h(c)| < \\frac{1}{n+1}$ for all $n$ and so $h(c) = 0$ and $f(c) = g(c)$.", "meta": {"post_id": 3777, "input_score": 83, "output_score": 41, "post_title": "Is there a function with a removable discontinuity at every point?"}}
{"input": "In the comments to the question: If $(a^{n}+n ) \\mid (b^{n}+n)$ for all $n$, then $ a=b$, there was a claim that $5^n+n$ is never prime (for integer $n>0$).\nIt does not look obvious to prove, nor have I found a counterexample.\nIs this really true?\nUpdate: $5^{7954} + 7954$ has been found to be prime by a computer: http://www.mersenneforum.org/showpost.php?p=233370&postcount=46\nThanks to Douglas (and lavalamp)!", "output": "A general rule-of-thumb for \"is there a prime of the form f(n)?\" questions is, unless there exists a set of small divisors D, called a covering set, that divide every number of the form f(n), then there will eventually be a prime.  See, e.g. Sierpinski numbers.\nRunning WinPFGW (it should be available from the primeform yahoo group http://tech.groups.yahoo.com/group/primeform/), it found that $5^n+n$ is 3-probable prime when n=7954.  Moreover, for every n less than 7954, we have $5^n+n$ is composite.\nTo actually certify that $5^{7954}+7954$ is a prime, you could use Primo (available from http://www.ellipsa.eu/public/misc/downloads.html).  I've begun running it (so it's passed a few more pseudo-primality tests), but I doubt I will continue until it's completed -- it could take a long time (e.g. a few months).\nEDIT: $5^{7954}+7954$ is officially prime.  A proof certificate was given by lavalamp at mersenneforum.org.", "meta": {"post_id": 4125, "input_score": 62, "output_score": 59, "post_title": "$5^n+n$ is never prime?"}}
{"input": "Given $f(x) = \\frac{1}{2}x^TAx + b^Tx + \\alpha $ \nwhere A is an nxn symmetric matrix, b is an n-dimensional vector, and alpha a scalar. Show that\n$\\bigtriangledown _{x}f(x) = Ax + b$ \nand\n$H = \\bigtriangledown ^{2}_{x}f(x) = A$ \nIs this simply a matter of taking a derivative with respect to X, how would you attack this one?", "output": "$\\nabla f = (\\partial f/\\partial x_1, \\ldots, \\partial f/\\partial x_n)^t$ denotes the vector of partial derivatives of $f$ and is a completely standard notation.\nOn the other hand, $\\nabla^2 f$ seems to be used here in an unusual way, namely to denote the Hessian (the matrix of all second order partial derivatives), $(\\partial^2 f/\\partial x_i \\partial x_j)_{i,j=1}^n$.\n(The usual meaning of $\\nabla^2 f$ is the Laplacian, $\\partial^2 f/\\partial x_1^2 + \\ldots + \\partial^2 f/\\partial x_n^2$.)", "meta": {"post_id": 5076, "input_score": 39, "output_score": 42, "post_title": "what does \u2207 (upside down triangle) symbol mean in this problem"}}
{"input": "What are the benefits of using a conjugate linear inner product in a complex vector space vs a simple linear inner product?  That is, why do we demand that $(y,x) = \\overline{(x,y)}$ as opposed to $(y,x)=(x,y)$?  Of course, this ensures that $(x,x)$ is real and thus makes an easy definition of norm, but is that necessary?", "output": "It is in fact necessary. The inner product axioms without the conjugation are inconsistent:\n(Here $u$, $v$, $w$ are vectors and $c$ is a scalar)\n\n$\\langle cu, v\\rangle = c\\langle u, v\\rangle$\n$\\langle u,v\\rangle = \\langle v,u\\rangle$\nIf $u \\neq 0$, then $\\langle u,u\\rangle$ is a positive real number\n$\\langle u+v,w\\rangle = \\langle u,w\\rangle + \\langle v,w\\rangle$\n\nIn fact, 1-3 alone are inconsistent. Indeed, let $u$ be any nonzero vector, so $\\langle u,u\\rangle > 0$ by condition 3. But if $i = \\sqrt{-1}$, then $\\langle iu, iu\\rangle = i\\langle u, iu\\rangle$ (by 1) $ = i\\langle iu, u\\rangle$ (by 2) $ = i^2\\langle u, u\\rangle$ (by 1) $ = -\\langle u, u\\rangle < 0$, contradicting condition 3. \nThe upshot is that you can choose: either conjugate one side of condition 2, giving you the axioms for an inner product, or get rid of condition 3, giving you the axioms for a symmetric bilinear form. You could also consider a weaker version of 3, like requiring that if $u\\neq 0$, then $\\langle u, v\\rangle \\neq 0$ for some $v$. That gives you nondegenerate symmetric bilinear forms.\nNote that there's nothing wrong with bilinear forms on complex vector spaces; they're just not inner products. They're disjoint concepts, unlike in real vector spaces, where inner products are just special symmetric bilinear forms. In some ways, bilinear forms are nicer than inner products, since you don't have to worry about complex conjugation. However, bilinear forms over the complex numbers do not give rise to norms, which means they don't endow vector spaces with good geometry. Inner products do, hence their ubiquity.", "meta": {"post_id": 5179, "input_score": 26, "output_score": 38, "post_title": "Is complex conjugation needed for valid inner product?"}}
{"input": "Say you have two groups $G = \\langle g \\rangle$ with order $n$ and $H = \\langle h \\rangle$ with order $m$. Then the product $G \\times H$ is a cyclic group if and only if $\\gcd(n,m)=1$.\nI can't seem to figure out how to start proving this. I have tried with some examples, where I pick $(g,h)$ as a candidate generator of $G \\times H$. I see that what we want is for the cycles of $g$ and $h$, as we take powers of $(g,h)$, to interleave such that we do not get $(1,1)$ until the $(mn)$-th power. However, I am having a hard time formalizing this and relating it to the greatest common divisor.\nAny hints are much appreciated!", "output": "$\\begin{align}{\\bf Hint}\\ \\ \\  \n      & \\Bbb Z_m \\times \\mathbb Z_n\\ \\text{is noncyclic}\\\\[.2em]\n\\iff\\ & \\Bbb Z_m \\times \\Bbb Z_n\\ \\text{has all elts of order} < mn\\\\[.2em]\n\\iff\\ &  {\\rm lcm}(m,n) < mn\\\\[.2em]\n\\iff\\ & \\!\\gcd(m,n) > 1\n\\end{align}$", "meta": {"post_id": 5969, "input_score": 35, "output_score": 40, "post_title": "Product of two cyclic groups is cyclic iff their orders are co-prime"}}
{"input": "So, from what I understand, the axiom of choice is equivalent to the claim that every set can be well ordered. A set is well ordered by a relation, $R$ , if every subset has a least element. My question is: Has anyone constructed a well ordering on the reals?\nFirst, I was going to ask this question about the rationals, but then I realised that if you pick your favourite bijection between rationals and the integers, this determines a well ordering on the rationals through the natural well order on $\\mathbb{Z}$ . So it's not the denseness of the reals that makes it hard to well order them. So is it just the size of $\\mathbb{R}$ that makes it difficult to find a well order for it? Why should that be?\nTo reiterate:\n\nIs there a known well order on the Reals?\nIf there is, does a similar construction work for larger cardinalities?\nIs there a largest cardinality for which the construction works?", "output": "I assume you know the general theorem that, using the axiom of choice, every set can be well ordered. Given that, I think you're asking how hard it is to actually define the well ordering. This is a natural question but it turns out that the answer may be unsatisfying.\nFirst, of course, without the axiom of choice it's consistent with ZF set theory that there is no well ordering of the reals. So you can't just write down a formula of set theory akin to the quadratic formula that will \"obviously\" define a well ordering. Any formula that does define a well-ordering of the reals is going to require a nontrivial proof to verify that it's correct.\nHowever, there is not even a formula that unequivocally defines a well ordering of the reals in ZFC.\n\nThe theorem of \"Borel determinacy\" implies that there is no well ordering of the reals whose graph is a Borel set. This is provable in ZFC.  The stronger hypothesis of \"projective determinacy\" implies there is no well ordering of the reals definable by a formula in the projective hierarchy. This is consistent with ZFC but not provable in ZFC. \nWorse, it's even consistent with ZFC that no formula in the language of set theory defines a well ordering of the reals (even though one exists). That is, there is a model of ZFC in which no formula defines a well ordering of the reals. \n\nA set theorist could tell you more about these results.  They are in the set theoretic literature but not in the undergraduate literature. \nHere is a positive result. If you work in $L$ (that is, you assume the axiom of constructibility) then a specific formula is known that defines a well ordering of the reals in that context.  However, the axiom of constructibility is not provable in ZFC (although it is consistent with ZFC), and the formula in question does not define a well ordering of the reals in arbitrary models of ZFC. \nA second positive result, for relative definability. By looking at the standard proof of the well ordering principle (Zermelo's proof), we see that there is a single formula $\\phi(x,y,z)$ in the language of set theory such that if we have any choice function $F$ on the powerset of the reals then the formula $\\psi(x,y) = \\phi(x,y,F)$ defines a well ordering of the reals, in any model of ZF that happens to have such a choice function. Informally, this says that the reason the usual proof can't explicitly construct a well ordering is because we can't explicitly construct the choice function that the proof takes as an input.", "meta": {"post_id": 6501, "input_score": 142, "output_score": 134, "post_title": "Is there a known well ordering of the reals?"}}
{"input": "How does one prove the following limit?\n$$\n  \\lim_{n \\to \\infty}\n  \\sqrt{1 + 2 \\sqrt{1 + 3 \\sqrt{1 + \\cdots \\sqrt{1 + (n - 1) \\sqrt{1 + n}}}}}\n= 3.\n$$", "output": "Let me provide a full and simple proof here (6 years later)\nSet, for $m<n$\n$$\na_{m,n}=\\sqrt{1+m\\sqrt{1+(m+1)\\sqrt{1+\\cdots+(n-1)\\sqrt{1+n}}}}.\n$$\nWe shall first show that $$a_{m,n}<m+1,\\tag{1}$$ in the following way using\nBackwards induction. Clearly,\n$a_{n,n}=\\sqrt{1+n}<1+n$, and if $a_{k+1,n}<k+2$, for some $k<n$, then\n$$\na_{k,n}=\\sqrt{1+ka_{k+1,n}}<\\sqrt{1+k(k+2})=\\sqrt{k^2+2k+1}=k+1.\n$$\nIn particular, in the same way we can show that\n$$\nm+1=\\sqrt{1+m\\sqrt{1+(m+1)\\sqrt{1+\\cdots+(n-1)\\sqrt{1+{\\color{red}{n^2+2n}}}}}}\n$$\nNext, observe that\n$$\n0<m+1-a_{m,n}= \\\\\n=\\sqrt{1+m\\sqrt{1+\\cdots+(n-1)\\sqrt{1+{\\color{red}{n^2+2n}}}}}\n-\\sqrt{1+m\\sqrt{1+\\cdots+(n-1)\\sqrt{1+{n}}}} \\\\\n=\\frac{m\\Big(\\sqrt{1+(m+1)\\sqrt{1+\\cdots+(n-1)\\sqrt{1+{\\color{red}{n^2+2n}}}}}\n-\\sqrt{1+(m+1)\\sqrt{1+\\cdots+(n-1)\\sqrt{1+{n}}}}\\Big)}{\\sqrt{1+m\\sqrt{1+\\cdots+(n-1)\\sqrt{1+{\\color{red}{n^2+2n}}}}}\n+\\sqrt{1+m\\sqrt{1+\\cdots+(n-1)\\sqrt{1+{n}}}}} \\\\\n<\\frac{m}{m+2}(m+2-a_{m+1,n}) <\\cdots < \\frac{m(m+1)\\cdots (n-1)}{(m+2)(m+3)\\cdots(n+1)}(n+1-a_{n,n})=\\frac{m(m+1)(n+1-\\sqrt{n+1})}{n(n+1)}<\\frac{m(m+1)}{n}.\n$$\nThus\n$$\n\\lim_{n\\to\\infty}a_{m,n}=m+1.\n$$", "meta": {"post_id": 7204, "input_score": 55, "output_score": 35, "post_title": "Evaluating the nested radical $ \\sqrt{1 + 2 \\sqrt{1 + 3 \\sqrt{1 + \\cdots}}} $."}}
{"input": "Firstly, you guys are awesome, and I learn quite a bit just from reading the questions of others.\nSecondly, a friend asked me recently why large primes are important for data security, and I was unable to give him an answer with which I myself was satisfied.  Various wikipedia articles have mostly pointed out an embarrassing paucity in mathematical knowledge on my part, and since this happens to be a very math-related question (and not a programming-related question) I was hoping someone could shed some light.\ntl;dr: question reads as title.", "output": "There is a whole class of cryptographic/security systems which rely on what are called \"trap-door functions\". The idea is that they are functions which are generally easy to compute, but for which finding the inverse is very hard (here, \"easy\" and \"hard\" refer to how quickly we know how to do it), but such that if you have an extra piece of information, then finding the inverse is easy as well. Primes play a very important role in many such systems.\nOne such example is the function that takes two integers and multiplies them together (something we can do very easily), versus the \"inverse\", which is a function that takes an integer and gives you proper factors (given $n$, two numbers $p$ and $q$ such that $pq=n$ and $1\\lt p,q\\lt n$).  If $n$ is the product of two primes, then there is one and only one such pair.\nAnother example is the discrete logarithm. To consider a simple example, look at the integers modulo, say, $7$. The integers between $1$ and $6$, inclusively, form a group under multiplication, and in fact every number between $1$ and $6$ is a power of $3$. The \"discrete logarithm problem\" would be, given a number $x$ between $1$ and $6$, to find a number $a$ such that $3^a$ equals $x$ modulo $7$. In this case, you can just try powers of $3$ until you hit the right answer. But if the modulo is very large, then this would take too much time.\nOne method for exchanging information over an open channel relies on the fact that we do not have very good methods of finding discrete logarithms in general, but we do have very good methods for computing modular powers. The idea is: suppose you and I need to exchange information. We want to use some very secure cryptographic system that relies on a complicated key. But, how can we agree on a key? If we have some secure way of communicating so that when we agree on the key nobody will overhear us, then why bother with the entire exercise? We should just communicate using that secure way. So instead we need to communicate at a place where we can be overhead. How can we agree on a secret key if everyone can hear us? Well, Diffie and Hellman proposed the following method:\nPick a very large prime $p$, and a number $r$ such that every number between $1$ and $p-1$ is a power of $r$ modulo $p$ (such numbers $r$ are known to exist for every prime; they are called primitive roots). Everyone knows $p$ and everyone knows $r$. Then I pick a secret number $a$, and you pick a secret number $b$. I cannot tell you my secret number (it's secret). But I tell you what $r^a \\mod p$ is. Because computing modular powers is easy, I can do this computation easy enough; but because we don't know how to do discrete logarithms easily, we are hoping that nobody will be able to figure out $a$ just from knowing $r^a$... at least, not very quickly. Likewise, you tell me $r^b \\mod p$. Now, you know $r^a$, and you know what $b$ is, so you compute $(r^a)^b \\mod p$. By the laws of exponent, you now know (secretly!) the number $r^{ab} \\mod p$. I, on the other hand, know $r^b$ (because you told me that number) and I know what $a$ is. So I compute $(r^b)^a\\mod p$. But this is the same as $r^{ab} \\mod p$. So now we both have a piece of information, namely the number $r^{ab}\\mod p$.  This is going to be our \"secret key\".\nNow, if someone can figure out either $a$ or $b$, then since they also know $r^a$ and $r^b$, they'll be able to figure out our secret key. We hope this is hard, but we certainly need $p$ to be very big: otherwise, they can just try all powers of $r$ until they hit the right one. We need the \"search space\" to be very big, so we need $p$ to be very big. Added: As jug points out, having $p$ big is not sufficient. There are algorithms for computing discrete logarithms that are particularly good with certain kinds of primes, so we generally also require that $p$ satisfy some additional \"good\" properties relative to the cryptographic application. You generally want $p$ and $(p-1)/2$ to be both primes, for example. On the other hand, in practice one does not really need $r$ to be a primitive root. Instead, it is enough that it generate a \"large\" subgroup of the multiplicative group, which one generally wants to be of prime order.\n(Note: figuring out $a$ or $b$ is just one way in which they could figure out our secret key $r^{ab}$, since everyone knows $p$, $r$, $r^a$, and $r^b$. It is not known whether this is essentially the only way to break this \"key exchange\" method; the method really relies on whether one can figure out $r^{ab}$ from knowing $r$, $p$, $r^a$, and $r^b$; this is called the Diffie-Hellman problem; the Diffie-Hellman problem is at most as hard as the Discrete Logarithm Problem, but we do not know if it is just as hard (it could be easier); and we don't know just how hard the Discrete Logarithm Problem is, we just know that we don't have any easy ways of doing it yet).\nSo key exchange is one place where big primes are very important. (Diffie-Hellman is not the only way to do key exchanges). Another place where big primes play a big role is in RSA which is a cryptosystem that also relies on big primes (this time, two big primes $p$ and $q$, and we do arithmetic modulo $n=pq$).\nAdded: Might as well add a quick overview of RSA and how the primes come into play. Here, once again modular exponentiation is part of the process. This is an \"public key\" system: I will tell everyone how to send me secret messages, which hopefully only I can decode. (In Diffie-Hellman, we did not exchange a message; we agreed on a secret key that we will use with a separate system that requires a secret key; for example, AES). I pick two large primes $p$ and $q$, and compute $n=pq$. I also pick a number $e$ that is relatively prime to $(p-1)(q-1)$ (I can do that because I know $p$ and $q$). Then I use the Euclidean algorithm, which is pretty quick, to find a $d$ such that $ed\\equiv 1 \\pmod{(p-1)(q-1)}$. Finally, I tell everyone what $n$ and $e$ are. If you want to send me a message, you first convert it to a number $M$ using some standard mechanism. Then you compute $M^e \\mod n$, and you tell me what $M^e\\mod n$ is. I will take $M^e$ and compute $(M^e)^d = M^{ed}\\mod n$. Because $ed\\equiv 1 \\pmod{(p-1)(q-1)}$, then $M^{ed}\\equiv M\\pmod{n}$, so that is how I recover $M$. The security of the system relies in hoping that from knowing $n$ and $e$, it is difficult to figure out $d$ (it is easy if I know $p$ and $q$; this is why this is believed to be a \"trap-door function\" as described in the first paragraph). The problem is at most as hard as factoring $n$, because if you can factor $n$ then you can find $d$ the same way I did; it is not known if the problem of finding $M$ from $n$, $M^e$, and $e$  is at least as hard as factoring (it has been shown that some variants are at least as hard as factoring), and again we don't know just how hard factoring is.  But: because we know that if you can factor $n$ then you can read the message, then we want to make $n$ difficult to factor. It only has two factors, but you don't want them to be easy to find, so you want $p$ and $q$ to be large for sure. (Again, there are other conditions one usually puts on $e$, $p$, and $q$ to make sure that certain special attacks do not succeed easily, but at least we need $p$ and $q$ to be very big).", "meta": {"post_id": 7377, "input_score": 41, "output_score": 54, "post_title": "Why are very large prime numbers important in cryptography?"}}
{"input": "Hi I have just found the proof that 90 equals 95 and was wondering if I have made some mistake. If so, which step in my proof is not true?\nDefinitions:\n 1. $\\angle ABC=90^{\\circ}$\n 2. $\\angle BCD=95^{\\circ}$\n 3. $|AB|=|CD|$\n 4. $M:=$ the center of $BC$\n 5. $N:=$ the center of $AD$\n 6. $l:=$ a line perpendicular to $BC$ passing through $M$\n 7. $m:=$ a line perpendicular to $AD$ passing through $N$\n 8. $S:=$ is the cross-section of $l$ and $m$  \nBased on definitions 1 through 8 we can draw the following image:\n\nBased on the definitions we can derive the following:\n 9. $\\triangle BSC$ is isosceles (follows from 4, 6 and 8)\n 10. $\\triangle ASD$ is isosceles (follows from 5, 7 and 8)\n 11. $|BS|=|CS|$ (follows from 9)\n 12. $|AS|=|DS|$ (follows from 10)\n 13. $\\triangle ABS\\cong\\triangle DCS$ (follows from 3, 11 and 12)\n 14. $\\angle ABS=\\angle DCS$ (follows from 13)\n 15. $\\angle CBS=\\angle BCS$ (follows from 9)\n 16. $\\angle ABC=\\angle ABS - \\angle CBS=\\angle DCS-\\angle BCS=\\angle BCD$ (follows from 14 and 15)\n 17. $90^{\\circ}=95^{\\circ}$ (follows from 1, 2 and 16)   \nNote: point $S$ is indeed lying above $BC$. If however it would be below $BC$, then the 'minus' in step 16 would simply have to be changed into a 'plus'. \nAlso note: The image is not drawn to scale. It only serves as to provide the reader with a intuitive view of the proof.\nAlso also note: before posting, I have first investigated if this type of question would be appropriate. Based on How come 32.5 = 31.5? and the following meta Questions about math jokes I have decided to post this question.", "output": "$$\r\n\\begin{array}{l}\r\n0\\in\\mathbb N\\\\\r\n\\forall n\\in \\mathbb N : n'\\in \\mathbb N\\\\\r\n\\hline\r\n0'\\in\\mathbb N\\\\\r\n\\forall n\\in \\mathbb N : n'\\in \\mathbb N\\\\\r\n\\hline\r\n0''\\in\\mathbb N\\\\\r\n\\forall n\\in \\mathbb N : n'\\in \\mathbb N\\\\\r\n\\hline\r\n0'''\\in\\mathbb N\\\\\r\n\\forall n\\in \\mathbb N : n'\\in \\mathbb N\\\\\r\n\\hline\r\n0''''\\in\\mathbb N\\\\\r\n\\forall n\\in \\mathbb N : n'\\neq0\\\\\r\n\\hline\r\n0'''''\\neq0\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''\\neq0'\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''\\neq0''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''\\neq0'''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''\\neq0''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''\\neq0'''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''\\neq0''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''\\neq0'''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''\\neq0''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''\\neq0'''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''\\neq0''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''\\neq0'''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''\\neq0''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''\\neq0'''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''\\neq0''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''\\neq0'''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''\\neq0''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''\\neq0'''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''\\neq0''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''\\neq0'''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''\\neq0''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''\\neq0'''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''\\neq0''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n\\phantom{0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''}\r\n\\end{array}\r\n$$\n$$\r\n\\begin{array}{l}\r\n0'''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n\\phantom{0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''}\r\n\\end{array}\r\n$$\n$$\r\n\\begin{array}{l}\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n\\phantom{0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''}\r\n\\end{array}\r\n$$\n$$\r\n\\begin{array}{l}\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\forall m,n\\in \\mathbb N:m\\neq n\\rightarrow m'\\neq n'\r\n\\\\\r\n\\hline\r\n0'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\neq0''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\\\\\r\n\\\\\r\n\\square\r\n\\end{array}\r\n$$", "meta": {"post_id": 7497, "input_score": 15, "output_score": 57, "post_title": "What is wrong in my proof that 90 = 95? Or is it correct?"}}
{"input": "I know it is possible for a group $G$ to have normal subgroups $H, K$, such that $H\\cong K$ but $G/H\\not\\cong G/K$, but I couldn't think of any examples with $G$ finite. What is an illustrative example?", "output": "Take $G = \\mathbb{Z}_4 \\times \\mathbb{Z}_2$, $H$ generated by $(0,1)$, $K$ generated by $(2,0)$.  Then $H \\cong K \\cong \\mathbb{Z}_2$ but $G/H \\cong \\mathbb{Z}_4$ while $G/K \\cong \\mathbb{Z}_2 \\times \\mathbb{Z}_2$.", "meta": {"post_id": 7720, "input_score": 30, "output_score": 39, "post_title": "Finite group with isomorphic normal subgroups and non-isomorphic quotients?"}}
{"input": "So, I don't like proofs.\nTo me building a proof feels like constructing a steel trap out of arguments to make true what you're trying to assert.\nOftentimes the proof in the book is something that I get if I study, but hard to come up with on my own.  In other words I can't make steel traps, but I feel fine buying them from others.\nHow does one acquire the ability to create steel traps with fluency and ease?  Are there any particular reference books that you found helped you really get how to construct a proof fluently?  Or is it just practice?", "output": "I'd like to second one part of Qiaochu Yuan's answer: the recommendation to read Polya's book.  Unlike many other books I've seen (albeit none of the others recommended above), it actually does contain guidance on how to construct a proof \"out of nothing\".\nAnd that's one problem with the \"practise, practise, practise\" mantra.  Practise what?  Where are the lists of similar-but-not-quite-identical things to prove to practise on?  I can find lists of integrals to do and lists of matrices to solve, but it's hard coming up with lists of things to prove.\nOf course, practise is correct.  But just as with anything else in mathematics, there's guidelines to help get you started.\nThe first thing to realise is that reading others proofs is not guaranteed to give you any insight as to how the proof was developed.  A proof is meant to convince someone of a result, so a proof points to the theorem (or whatever) and knowing how the proof was constructed does not (or at least, should not) lend any extra weight to our confidence in the theorem.  Proofs can be written in this way, and when teaching we should make sure to present some proofs in this way, but to do it every time would be tedious.\nSo, what are the guidelines for constructing a proof?  You'll probably get different answers from different mathematicians so these should be construed as being my opinion and not a(n attempt at a) definitive answer.\nMy recommendation is that you take the statement that you want to prove and apply the following steps to it as often as you can:\n\nExpand out unfamiliar terms.\nReplacing generic statements by statements about generic objects.\nIncluding implicit information.\n\nOnce you've done all that, the hope is that the proof will be much clearer.\nHere's an example.\n\nOriginal statement:\n\nThe composition of linear transformations is again linear.\n\nReplace generic statements:\n\nIf $S$ and $T$ are two composable linear transformations then their composition, $S T$, is again linear.\n\nIt is important to be precise here.  The word \"composable\" could have been left out, as the statement only makes sense if $S$ and $T$ are composable, but until you are completely familiar with this kind of process, it is better to be overly precise than otherwise.  In this case, leaving in the word \"composable\" reminds us that there is a restriction on the domains and codomains which will be useful later.  (However, one has to draw the line somewhere: even the word \"composable\" is not quite enough since it leaves open the question as to whether it is $S T$ or $T S$!)\nInclude implicit information:\n\nIf $S \\colon V \\to W$ and $T \\colon U \\to V$ are linear transformations then $S T \\colon U \\to W$ is again linear.\n\nHere's where remembering that $S$ and $T$ are composable in the previous step helps keep things clear.  As $S$ and $T$ are composable, we only need $3$ vector spaces.  Then, since we explicitly have the vector spaces the fact that $S$ and $T$ are composable is plain, though some may prefer to keep that fact in the statement.  Also, some may like to have the fact that $U$, $V$, and $W$ are vector spaces explicitly stated.\nExpand out definitions:\n\nIf $S \\colon V \\to W$ and $T \\colon U \\to V$ are such that $S(v_1 + \\lambda v_2) = S(v_1) + \\lambda S(v_2)$ and $T(u_1) + \\mu T(u_2)$ for all $v_1, v_2 \\in V$, $u_1, u_2 \\in U$, and $\\lambda, \\mu \\in \\mathbb{R}$, then $S T(x_1 + \\eta x_2) = S T(x_1) + \\eta S T(x_2)$ for all $x_1, x_2 \\in U$ and $\\eta \\in \\mathbb{R}$.\n\nNote that I have been careful not to repeat myself with the newly introduced symbols.  It would be technically alright to reuse $u_1$ and $u_2$ in place of $x_1$ and $x_2$ since these are local declarations (restricted by the phrases \"for all ...\").  However, humans are not good at differentiating between local and global declarations so it is best not to reuse symbols unless the scope is very clear.\nReplace generic statements:\n\nIf $S \\colon V \\to W$ and $T \\colon U \\to V$ are such that $S(v_1 + \\lambda v_2) = S(v_1) + \\lambda S(v_2)$ and $T(u_1) + \\mu T(u_2)$ for all $v_1, v_2 \\in V$, $u_1, u_2 \\in U$, and $\\lambda, \\mu \\in \\mathbb{R}$, then whenever  $x_1, x_2 \\in U$ and $\\eta \\in \\mathbb{R}$,  $S T(x_1 + \\eta x_2) = S T(x_1) + \\eta S T(x_2)$.\n\nUp to now, the rephrasing has not taken into account the fact that there is a conclusion and a hypothesis.  This rephrasing modifies a part of the conclusion to turn it from a generic statement \"$P(p)$ is true for all $p \\in Q$\" to a statement about a generic object \"whenever $p \\in Q$ then $P(p)$ is true\".  We do not do this for the similar statements in the hypothesis.  This is because these two pieces are treated differently in the proof.\nReplace generic statements, and reorganise to bring choices to the fore:\n\nLet $S \\colon V \\to W$ and $T \\colon U \\to V$ be such that $S(v_1 + \\lambda v_2) = S(v_1) + \\lambda S(v_2)$ and $T(u_1) + \\mu T(u_2)$ for all $v_1, v_2 \\in V$, $u_1, u_2 \\in U$, and $\\lambda, \\mu \\in \\mathbb{R}$.  Let  $x_1, x_2 \\in U$ and $\\eta \\in \\mathbb{R}$.  Then  $S T(x_1 + \\eta x_2) = S T(x_1) + \\eta S T(x_2)$.\n\nIn this form, the distinction between hypothesis and conclusion is all the clearer.  Parts of the hypothesis use the word \"Let\", parts of the conclusion use the word \"Then\".\n\nWith this formulation, the proof essentially writes itself.  With all it's gory details:\n\nProof\n\nLet $S \\colon V \\to W$ and $T \\colon U \\to V$ be such that $S(v_1 + \\lambda v_2) = S(v_1) + \\lambda S(v_2)$ and $T(u_1) + \\mu T(u_2)$ for all $v_1, v_2 \\in V$, $u_1, u_2 \\in U$, and $\\lambda, \\mu \\in \\mathbb{R}$.  Let  $x_1, x_2 \\in U$ and $\\eta \\in \\mathbb{R}$.[^quick]  Then:\n$$\nS T(x_1 + \\eta x_2) = S \\big( T(x_1) + \\eta T(x_2)\\big)\n$$\nusing the hypothesis on $T$ as $x_1, x_2 \\in U$ and $\\eta \\in \\mathbb{R}$.  So:\n$$\nS T(x_1 + \\eta x_2) = S T(x_1) + \\eta S T(x_2)\n$$\nusing the hypothesis on $S$ as $T(x_1), T(x_2) \\in V$ and $\\eta \\in \\mathbb{R}$.  Hence the conclusion is true.\nNotes:\n1. This could be condensed, but the important thing here is how to find it, not what the final form should be.\n2. Notice that I wrote \"as $x_1, x_2 \\in U$\" rather than \"with $u_1 = x_1$ and $u_2 = x_2$\".  This is partly style, and partly because in the statement of linearity, $u_1$ and $u_2$ are placeholders into which we put $x_1$ and $x_2$.   So saying $u_1 = x_1$ is semantically incorrect as it equates a virtual vector with an actual vector.  This is a very minor point, though.\n\nFinally, I would like to disagree with one part of Qiaochu's answer.  I actually like the imagery of a steel trap.  A proof is a bit like a trap: we want to capture the theorem in a trap so that it can't wriggle out.  We construct the proof so that there is no possibility of escape.  Eventually, yes, we want the proof to be beautiful but when it's first constructed we just want it to do the job.  Only once the theorem is caught can we spend a little time decorating the cage to make it look pretty and set it off to its best advantage.  So build the trap because theorems can be dangerous!  An escaped theorem can do untold damage, rampaging across the countryside, laying waste like an unchecked viking.\n(Okay, not quite finally.  The step-by-step proof above was taking from a page I wrote for my students on the nature of proof.  The original can be found here.)", "meta": {"post_id": 7743, "input_score": 131, "output_score": 143, "post_title": "Getting better at proofs"}}
{"input": "Ok. This question may sound very easy, but actually I am in great need of an answer. I have been facing trouble in constructing functions, which are only continuous at some particular sets. \nFor example, the standard example of a function which is only continuous at one point, is the function,  $f(x) = x, \\ x \\in \\mathbb{Q}$ and $f(x) = -x, x  \\in \\mathbb{R} \\setminus \\mathbb{Q}$. Similarly, I would like to know how to construct a function which is:\n\nContinuous at exactly $2,3,4$ points.\nContinuous exactly at integers\nContinuous exactly at Natural numbers\nContinuous exactly at Rationals. \n\nI would like to see many examples (with proof!), so that I won't struggle when somebody asks me to construct such functions.", "output": "One simple way of constructing a function which is continuous only at a finite number of points, $x=a_1,\\ldots,a_n$, is to do a slight modification to the function you give: take a polynomial $p(x)$ that has roots exactly at $x=a_1,\\ldots,a_n$ (e.g., $p(x) = (x-a_1)\\cdots(x-a_n)$) , and then define\n$$ g(x) = \\left\\{\\begin{array}{ll}\np(x) & \\text{if $x\\in\\mathbb{Q}$;}\\\\\n0 & \\text{if $x\\notin\\mathbb{Q}$.}\n\\end{array}\\right.$$\nThe function is continuous at $a_1,\\ldots,a_n$, and since $p(x)\\neq 0$ for any $x\\notin\\{a_1,\\ldots,a_n\\}$ then $g(x)$ is not continuous at any point other than $a_1,\\ldots,a_n$. Other possibilities should suggest themselves easily enough.\nA function that is continuous exactly at the integers: a similar idea will work: find a function that has zeros exactly at the integers, for example $f(x)=\\sin(\\pi x)$, and then take\n$$g(x) = \\left\\{\\begin{array}{ll}\n\\sin(\\pi x) & \\text{if $x\\in\\mathbb{Q}$;}\\\\\n0 & \\text{if $x\\notin\\mathbb{Q}$.}\n\\end{array}\\right.$$\nA function continuous exactly in the natural numbers: take a function that is continuous at the integers, and redefine it as the characteristic function of the rationals in appropriate places(what happens at $0$ depends on whether you believe $0$ is in the natural numbers or not). Assuming that $0\\in\\mathbb{N}$, one possibility is:\n$$g(x) = \\left\\{\\begin{array}{ll}\n\\sin(\\pi x)&\\text{if $x\\in\\mathbb{Q}$ and $x\\geq 0$;}\\\\\nx & \\text{if $x\\in\\mathbb{Q}$ and $-\\frac{1}{2}\\lt x\\leq 0$;}\\\\\n1 & \\text{if $x\\in\\mathbb{Q}$ and $x\\leq -\\frac{1}{2}$;}\\\\\n0 & \\text{if $x\\notin\\mathbb{Q}$.}\n\\end{array}\\right.$$\nA function continuous exactly on the rationals. This one is a bit trickier. There is no such function. This follows because the set of discontinuities of a real valued function must be a countable union of closed sets. \nPerhaps then, we might anticipate the next question:\nA function that is continuous exactly on the irrationals. An example is the following: let $s\\colon\\mathbb{N}\\to\\mathbb{Q}$ be an enumeration of the rationals (that is, a bijection from $\\mathbb{N}$ to $\\mathbb{Q}$. Define $f(x)$ as follows:\n$$f(x) = \\sum_{\\stackrel{n\\in\\mathbb{N}}{s_n\\leq x}} \\frac{1}{2^n}.$$\nThe function has a jump at every rational, so it is not continuous at any rational. However, if $x$ is irrational, let $\\epsilon\\gt 0$. Then there exists $N$ such that $\\sum_{k\\geq N}\\frac{1}{2^k}\\lt \\epsilon$. Find a neighborhood of $x$ which excludes every $q_m$ with $m\\leq N$, and conclude that the difference between the value of $f$ at $x$ and at any point in the neighborhood is at most $\\sum_{k\\geq N}\\frac{1}{2^k}$. \nEdit: As I was reminded in the comments by jake, in fact the \"standard example\" of a function that is continuous at every rational and discontinuous at every rational is Thomae's function. The example I give is a monotone function, and although it is discontinuous at every rational, it is continuous from the right at every number.", "meta": {"post_id": 7821, "input_score": 25, "output_score": 34, "post_title": "Constructing Continuous functions at given points"}}
{"input": "One observes that \n\\begin{equation*}\n4!+1 =25=5^{2},~5!+1=121=11^{2} \n\\end{equation*}\nis a perfect square. Similarly for $n=7$ also we see that $n!+1$ is a perfect square. So one can ask the truth of this question:\n\nIs $n!+1$ a perfect square for infinitely many $n$? If yes, then how to prove.", "output": "This is Brocard's problem, and it is still open.\nhttp://en.wikipedia.org/wiki/Brocard%27s_problem", "meta": {"post_id": 7938, "input_score": 53, "output_score": 45, "post_title": "$n!+1$ being a perfect square"}}
{"input": "Let $(X, d)$ be a metric space. Is the function $x\\mapsto d(x, z)$ continuous? Is it uniformly continuous?", "output": "As Qiaochu points out $d(x,y)$ is continuous for fixed $x$. You may like to see this as well, as this is a familiar result in Topology:\n\nIf $A$ is a non empty subset of a metric space $(X,d)$ then the function $f$ on $X$ given by \n  $$f(x)=d(x,A):= \\inf_{y\\in A} d(x, y)$$ \n  is continuous. Indeed, \n  $$| f(x) - f(y) | = | d(x,A) - d(y,A) | \\leq d(x,y),$$\n  and thus $f$ is uniformly continuous (use $\\delta = \\epsilon$ in any point).\n\nTo show this, let $x$ and $y$ be points in $X$, and $p$ any point in $A$.\nThen \n$$d(x,p) \\leq d(x,y) + d(y,p)\\ \\ \\ \\ \\text{ (triangle inequality)}$$\nand so \n$$d(x,A) \\leq d(x,y) + d(y,p)$$ \nas $d(x,A)$ is the infimum. But then $d(y,p) \\geq d(x,A) - d(x,y)$ (for all $p$, obtained by subtracting from the previous inequality), so that $d(y,A) \\geq d(x,A) - d(x,y)$ (as $d(y,A)$ is the infimum).\nSo : $d(x,A) - d(y,A) \\leq d(x,y)$.\nNow reverse the roles of $x$ and $y$ to get\n$d(y,A) - d(x,A) \\leq d(x,y)$.\nThis is taken from http://at.yorku.ca/cgi-bin/bbqa?forum=homework_help_2004;task=show_msg;msg=1323.0001", "meta": {"post_id": 8066, "input_score": 31, "output_score": 47, "post_title": "Is the distance function in a metric space (uniformly) continuous?"}}
{"input": "Is the localization of a reduced ring (no nilpotents) still reduced?", "output": "Let $A$ be a ring, $S\\subset A$ a multiplicatively closed subset, and suppose that $0\\neq a/b\\in A_S$ is nilpotent. Then there exists $n$ such that $(a/b)^n=0$, i.e., such that there exists $t\\in S$ with $ta^n=0$. But then $ta$ is nilpotent in $A$. If it is zero, then $a/b=0$ in $A_S$, which it isn't.", "meta": {"post_id": 8150, "input_score": 19, "output_score": 38, "post_title": "Does localization preserve reducedness?"}}
{"input": "Why is every group the quotient of a free group by a normal subgroup?", "output": "This is one of the most intuitive observations in all of group theory, and it illustrates the quotient operation in the most fundamental way.\nI'll provide two separate answers. The first is fully intuitive; the second is a formalized version of the first.\nFirst answer: Take a group $G$. A relation on $G$ is an equation satisfied by some of the elements. For instance, $eg = g$ where $e$ is the identity is a relation satisfied by all group elements $g \\in G$. Because we can always multiply by inverses in a group, we can rewrite this relation as $egg^{-1} = gg^{-1} = e$, i.e., $e = e$. This can be applied to any relation. If $G$ is abelian, then $ab = ba$ for all $a,b \\in G$, and we can rewrite this as $aba^{-1}b^{-1} = e$.\nIn other words, a relation asserts that some product of group elements coincides with the identity, so the only information we need to understand the relation is the product which occurs on the left side of the equals sign.\nNow every group has a few relations which are implied directly by the group axioms. $aa^{-1} = e$ is one of them. We can ask whether the group has any extra relations which are not implied by the group axioms. If no such relations exist, i.e., if the only relations which hold are those which must hold by virtue of the group axioms, then the group is said to be free; the group is \"free of additional relations.\"\nIf you have a group $G$, one natural thing to do is to introduce new relations into it and to thereby create new groups. But you can't just introduce completely random relations because (a) the relations can't contradict each other or pre-exising relations and (b) the resulting structure must again be a group. Now we saw earlier that a relation can be specified as a product of group elements. In order that the relations satisfy (a) and (b), it turns out it is necessary and sufficient that the corresponding products form a normal subgroup $N$. The result of introducing the collection of relations $N$ into the group $G$ is the quotient $G/N$.\nAny group $G$ can be obtained in this manner. You start with the free group $F$ whose generators are elements of $G$ considered as a set. And then you look at all the additional relations satisfied by elements of $G$ and assemble them into a normal subgroup $N$. Then $G = F/N$ by the above.\nSecond answer: Given any set $S$, the free group on $S$ is that group $F(S)$ for which every function $f : S \\rightarrow G$ from $S$ to an arbitrary group $G$ extends to a unique homomorphism $\\tilde{f} : F(S) \\rightarrow G$. There are various ways of constructing $F(S)$ explicitly. For instance, you may take $F(S)$ to consist of words over the alphabet whose letters are elements of $S$ and $S'$, where $S'$ has the letter $s^{-1}$ (a symbol at the moment) for each symbol $s \\in S$. It's important to notice that $F(S)$ actually contains equivalence classes of words, because we introduce the obvious cancellation rules; e.g., $abb^{-1}c$ can be reduced via cancellation to $ac$. It must be proved that all possible algorithms for reduction yield the same reduced word; I'll omit that step.\nYou also have to prove that this group $F(S)$ satisfies the stated universal property. I won't prove this in detail, but it is more or less intuitive. Since $\\tilde{f}$ has to be a homomorphism, we find, for instance, that $\\tilde{f}(ab) = \\tilde{f}(a) \\tilde{f}(b) = f(a)f(b)$. In general, since $f$ is defined for all elements of $S$, $\\tilde{f}$ is thereby defined uniquely for all elements of $F(S)$. [It is via similar reasoning that you may determine that it is sufficient to know the values of a linear operator on the elements of a basis of a vector space.]\nSo we start with our group $G$ which we would like to write as a quotient of a free group. Which free group? That free group whose generators are the symbols from $G$. So we pick $F(G)$. Now we need to introduce the needed relations in order to collapse $F(G)$ into $G$. How do we carry it out? By the first answer, we could easily accomplish this if only we knew the normal subgroup $N$ of relations, but it seems that in this general case we don't really know $N$ concretely.\nIn fact, we can figure out $N$ as follows. We can take the identity map $f : G \\rightarrow G$ and extend it to a homomorphism $\\tilde{f} : F(G) \\rightarrow G$. The extension $\\tilde{f}$ is in general not injective, and its kernel is precisely the group of relations $N$! (Formally this is an application of one of the standard theorems on homomorphisms.) Then $G = F(G)/N$ as before.", "meta": {"post_id": 9446, "input_score": 23, "output_score": 39, "post_title": "Every group is the quotient of a free group by a normal subgroup"}}
{"input": "I am learning the basics of category theory, so this question is probably obvious to anyone who knows the subject.\nThe resources I've seen all take the following approach:\n0) A category is a collection of objects and morphisms between those objects that satisfy some rules.\n1) A functor is a morphism in the category of categories.\n2) A natural transformation is a morphism in the category of functors.\nBut they all stop right there. What about:\n3) the morphisms in the category of natural transformations? \n4) Or the \"morphisms in the category of the morphisms in the category of natural transformations\" \n5) ... \nAre these uninteresting? Why does the \"meta-ness\" stop at 2 levels deep?", "output": "I want to point out something potentially misleading about Marek's answer. The n-categories he mentions are not categories, but generalizations of them, so the question still remains, why do categories only form a 2-category, that is, why do people stop after categories, functors, and natural transformations? Why don't people define modifications of natural transformations?\nI think it is good to realize that categories really are in an essential way only 2-categorical, if you want interesting higher morphisms you do need to define something like a higher category. One way to think about it is this: natural tranformations are basically homotopies. To make this precise, take I to be the category with two objects, 0 and 1, one morphism from 0 to 1 and the identity morphisms. Then, it is easy to check that to specify a natural tranformation between two functors F and G (both functors C \u2192 D) is the same as specifying a functor H : C \u00d7 I \u2192 D which agrees with F on C \u00d7 {0} and with G on C \u00d7 {1}.\nSo then we could get higher morphisms by saying they are homotopies of homotopies, i.e., functors C \u00d7 I \u00d7 I with appropriate restrictions. This works, and we indeed get some definition of modification, but it is not interesting as it reduces to just a commuting square of natural transformations, i.e., it can be described simply in terms of the structure we already had.\nThis is similar to what happens for, say, groups: you can think of a group as a category with a single object where all the morphisms are invertible (the morphisms are the group elements and the composition law is the group product). Then group homomorphisms are simply functors. This makes it sound as if groups now magically have a higher sort of morphisms: natural tranformations between functors! And indeed they do, they are even useful in certain contexts, but they're not terribly interesting: a natural transformation between to group homomorphisms f and g is simply a group element y such that f(x) = y g(x) y -1 . Again, this is described in terms of things we already knew about (the group element y and conjugation), and is not really a brand new concept.", "meta": {"post_id": 9890, "input_score": 27, "output_score": 44, "post_title": "Morphisms in the category of natural transformations?"}}
{"input": "I'm just reviewing for my exam tomorow looking at old exams, unfortunately I don't have solutions.  Here is a question I found : determine if the series converges or diverges.  If it converges find it's limit.  \n$$\\displaystyle \\sum\\limits_{n=1}^{\\infty}\\dfrac{\\sin(n-\\sqrt{n^2+n})}{n}$$\r\nI've ruled down possible tests to the limit comparison test, but I feel like I've made a mistake somewhere.\ndivergence test - limit is 0 by the squeeze theorem\nintegral test - who knows how to solve this\ncomparison test - series is not positive\nratio root tests - on the absolute value of the series, this wouldn't work out\nalternating series test - would not work, the series is not decreasing or alternating  \nAny ideas what to compare this series here with or where my mistake is on my reasoning above?", "output": "The key here is that $n - \\sqrt{n^2 + n}$ converges to $-{1 \\over 2}$ as $n$ goes to infinity:\n$$n - \\sqrt{n^2 + n}=  (n - \\sqrt{n^2 + n}) \\times  {n + \\sqrt{n^2 + n} \\over n + \\sqrt{n^2 + n}}$$\r\n$$= {n^2 - (n^2 + n) \\over n + \\sqrt{n^2 + n}} = -{n \\over  n + \\sqrt{n^2 + n}}$$\r\n$$= -{1 \\over 1 + \\sqrt{1 + {1 \\over n}}}$$\r\nTake limits as $n$ goes to infinity to get $-{1 \\over 2}$.\nHence $\\sin(n - \\sqrt{n^2 + n})$ converges to $\\sin(-{1 \\over 2})$, and the series diverges similarly to ${1 \\over n}$, using the limit comparison test for example.", "meta": {"post_id": 10264, "input_score": 32, "output_score": 39, "post_title": "Does the series $\\sum\\limits_{n=1}^{\\infty}\\frac{\\sin(n-\\sqrt{n^2+n})}{n}$ converge?"}}
{"input": "If someone could give answers and explain, it would be greatly appreciated. Help required studying for a final.\n\nOne hundred tickets, numbered $1,2,3,\u2026,100,$ are sold to $100$ different people for a drawing.\nFour different prizes are awarded, including a grand prize (a trip to Tahiti).\n\nA) How many ways are there to award the prizes?\nB) How many ways are there to award the prizes if the person holding ticket $47$ wins the grand prize?\nC) How many ways are there to award the prizes if the person holding ticket $47$ wins one of the prizes?\nD) How many ways are there to award the prizes if the person holding ticket $47$ does not win a prize?\nE) How many ways are there to award the prizes if the people holding tickets $19$ and $47$ both win prizes?\nF) How many ways are there to award the prizes if the people holding tickets $19, 47, 73,$ and $97$ all win prizes?\nG) How many ways are there to award the prizes if none of the people holding tickets $19, 47, 73,$ and $97$ wins a prize?\nH) How many ways are there to award the prizes if the grand prize winner is a person holding ticket $18, 47, 73,$ or $97?$\nI) How many ways are there to award the prizes if the people holding tickets $19$ and $47$ win prizes, but the people holding tickets $73$ and $97$ do not win prizes?", "output": "The two basic rules of counting are:\n\nSum rule: If one event can occur in $n$ different ways, and another, independent, event can occur in $m$ different ways, then the number of different ways in which either one or the other event can occur is $n+m$.\nProduct rule: If one event can occur in $n$ different ways, and another, independent, event can occur in $m$ different ways, then the number of different ways in which both evens can occur is $nm$.\n\nWhen making choices, you have two basic kinds: if the order in which you make the choices matters, they are called permutations. (For example, choosing the Caesar salad as an appetizer and the shrimp cocktail as the main course is different from choosing the shrimp cocktail as an appetizer and the Caesar salad as a main course). If the order in which you make the choices does not matter, you have combinations. (For example, when choosing teams, it doesn't matter if Bill is chosen first to join team A and Clara is chosen second also for team A, or if Clara is chosen first to join team A and Bill is chosen second also for team A; all that matters is that both Bill and Clara are in team A). \nCombining these, you get some basic formulas:\n\nThe number of ways in which you can make $n$ choices, with $m$ options for each, allowing repetitions but where the order of the choice matters (permutations with repetitions), is $m^n$.\nThe number of ways in which you can make $n$ choices, with $m$ possibilities, where the order matters but with repetitions not allowed (permutations without repetitions) is $m(m-1)(m-2)\\cdots(m-n+1) = \\frac{m!}{(m-n)!}$.\nThe number of ways in which you can make $n$ choices out of $m$ options, if the order does not matter and repetitions are not allowed, is $\\binom{m}{n} = \\frac{m!}{n!(m-n)!}$ (called \"$m$-choose-$n$\", because you are choosing $n$ out of $m$). These are \"combinations\".\nThe number of ways in which you can make $n$ choices, with $m$ options, if the order does not matter and repetitions are allowed is $\\binom{m+n-1}{n}$. (Combinations with repetitions).\n\nYou probably knew all that, but still...\nThat said:\nA. You need to choose 4 tickets, out of 100 possibilities; you are not allowed to choose the same ticket twice (no repetitions). Since the prizes are all different, the order in which you make the choices matters (draw for the 4th prize, then the 3rd prize, then the 2nd prize, then the Grand Prize; or in whichever order you want). So, which formula above applies, and what is the answer?\nB. If ticket 47 will win the Grand Prize, you still need to assign the remaining three prizes. They have to be awarded to people other than ticket 47, no repetitions, but order still matters. How many options do you have, and how many choices to do you need to make? Order matters, no repetitions.\nC. This is similar to the above, but this time, ticket 47 may win the Grand Prize, the second, the third, or the fourth place prize. Count each of the four outcomes separately, then apply the Sum rule. \nD. Well, if you are going to exclude ticket 47, you have to choose from among the remaining 99 tickets. Order still matters, repetitions are not allowed. So the only difference is the number of options you have.\nE. Well, you have two prizes awarded, and two more to award. First award the other two prizes (i.e., count how many ways you can do that). Then decide which prizes go to tickets 19 and 47: you have four prizes you can award, so you need to choose two prizes (to give to 19 and 47); order matters (first prize chosen goes to 19, second to 47), no repetitions. Count that. In total, you need to (i) pick to other winners; pick them in order, so first winner gets the top prize not awarded to 19 or 47, second gets the lower prize not awarded); and (ii) pick which prizes to give to 19 and to 47. You need both things to happen, so you should then use the Product rule.\nF. Now you know who the four winners are; you just need to figure out which prizes they get. Count how many ways you can distribute the prizes among the four winners.\nG. Similar to D, but now you have to exclude four people instead of one.\nH. First decide who wins the Grand Prize. Then pick the other three winners. Then use the Product rule.\nI. This is a combination of the ideas from E and D. Use the same method as in E to figure out how many ways there are to award prizes to 19 and 47; then figure out how many ways there are to assign the remaining two prizes to the remaining people if you exclude 73 and 97. Then combine the two answers using an appropriate rule mentioned above.", "meta": {"post_id": 11307, "input_score": 7, "output_score": 35, "post_title": "One hundred tickets sold for different people, four different prizes are awarded, including a grand prize,\u2026"}}
{"input": "What is the limit of the series $1 \\over (2n)!$ for n in $[0, \\infty)$ ?  \n$$ \\sum_{n = 0}^{\\infty}{1 \\over (2n)!}$$\nI've ground out the sum of the 1st 1000 terms to 1000 digits using Python,\n(see here ), but how would a mathematician calculate the limit? And what is it? \nNo, this isn't homework. I'm 73. Just curious. \nThanks", "output": "It's half the sum of $e^1=\\sum 1/n!$ and $e^{-1}=\\sum (-1)^{n}/n!$ (or $\\cosh 1$, in other words).", "meta": {"post_id": 12340, "input_score": 25, "output_score": 36, "post_title": "What is limit of $\\sum \\limits_{n=0}^{\\infty}\\frac{1}{(2n)!} $?"}}
{"input": "I heard that using some relatively basic differential geometry, you can show that the only spheres which are Lie groups are $S^0$, $S^1$, and $S^3$.  My friend who told me this thought that it involved de Rham cohomology, but I don't really know anything about the cohomology of Lie groups so this doesn't help me much.  Presumably there are some pretty strict conditions we can get from talking about invariant differential forms -- if you can tell me anything about this it will be a much-appreciated bonus :)\n(A necessary condition for a manifold to be a Lie group is that is must be parallelizable, since any Lie group is parallelized (?) by the left-invariant vector fields generated by a basis of the Lie algebra.  Which happens to mean, by some pretty fancy tricks, that the only spheres that even have a chance are the ones listed above plus $S^7$.  The usual parallelization of this last one comes from viewing it as the set of unit octonions, which don't form a group since their multiplication isn't associative; of course this doesn't immediately preclude $S^7$ from admitting the structure of a Lie group.  Whatever.  I'd like to avoid having to appeal to this whole parallelizability business, if possible.)", "output": "Here is the sketch of the proof.\nStart with a compact connected Lie group G.  Let's break into 2 cases - either $G$ is abelian or not.\nIf $G$ is abelian, then one can easily show the Lie algebra is abelian, i.e., $[x,y]=0$ for any $x$ and $y$ in $\\mathfrak{g}$.  Since $\\mathbb{R}^n$ is simply connected and has the same Lie algebra as $G$, it must be the universal cover of $G$.\nSo, if $G$ is a sphere, it's $S^1$, since all the others are simply connected, and hence are their own universal covers.\nNext, we move onto the case where $G$ is nonabelian.  For $x,y,$ and $z$ in the Lie algebra, consider the map $t(x,y,z) = \\langle [x,y], z\\rangle$.  This map is clearly multilinear.  It obviously changes sign if we swap $x$ and $y$.  What's a bit more surprising is that it changes sign if we swap $y$ and $z$ or $x$ and $z$.  Said another way, $t$ is a 3 form!  I believe $t$ is called the Cartan 3-form.  Since $G$ is nonabelian, there are some $x$ and $y$ with $[x,y]\\neq 0$.  Then $t(x,y,[x,y]) = ||[x,y]||^2 \\neq 0$ so $t$ is not the 0 form.\nNext, use left translation on $G$ to move $t$ around: define $t$ at the point $g\\in G$ to be $L_{g^{-1}}^*t$, where $L_{g^{-1}}:G\\rightarrow G$ is given by $L_{g^{-1}}(h) = g^{-1}h$.\nThis differential 3-form is automatically left invariant from the way you've defined it everywhere.  It takes a bit more work (but is not too hard) to show that it's also right invariant as well.\nNext one argues that a biinvariant form is automatically closed.  This means $t$ defines an element in the 3rd de Rham cohomology of $G$.  It must be nonzero, for if $ds = t$, then we may assume wlog that $s$ is biinvariant in which case $ds = 0 = t$, but $t$ is not $0$ as we argued above.\nThus, for a nonabelian Lie group, $H^3_{\\text{de Rham}}(G)\\neq 0$.  But this is isomorphic to singular homology.  Hence, for a sphere to have a nonabelian Lie group structure, it must satisfy $H^3(S^n)\\neq 0$.  This tells you $n=3$.", "meta": {"post_id": 12453, "input_score": 87, "output_score": 136, "post_title": "Is there an easy way to show which spheres can be Lie groups?"}}
{"input": "in functional analysis, you encounter the terms 'adjoint' and 'formal adjoint'.\nWhat does 'formal' in that case mean? It Sounds like a hint that 'formal adjoints' lack a certain property to make them a 'true' adjoint.\nI have nowhere found a definition, and would be eager to know.", "output": "Are you talking about differential operators on functions over a domain in $\\mathbb{R}^d$? (This is the context in which the phrase \"formal adjoint\" usually comes up.)\nThe idea is that working with, say, smooth functions with compact support, we have the integration by parts formula\n$$ \\int Du\\cdot v ~dx + \\int u\\cdot Dv~ dx = 0 $$\nSo if a linear partial differential operator is defined as $P = \\sum A_\\alpha D^\\alpha$ where $\\alpha$ are multi-indices, you can write $P'$ as a linear partial differential operator $P'\\phi = \\sum (-1)^{|\\alpha|} D^\\alpha(A_\\alpha \\phi)$ and generalize the integration by parts formula\n$$ \\int Pu \\cdot v~dx = \\int u \\cdot P'v~ dx $$\nwhich looks, in form, suspiciously like the adjoint with respect to the $L^2$ inner product. That is, writing $\\langle,\\rangle$ for the $L^2$ inner product of real valued functions, \n$$ \\langle Pu,v\\rangle = \\langle u, P'v\\rangle $$\nThe reason that we call this a formal adjoint is because, technically, to take an adjoint (in the Hilbert space sense, there is also a different notion for Banach spaces) of an operator, you need to specify which Hilbert space you are working over. In the case of the formal adjoint, it is left unspecified: indeed, the formula only really hold for sufficiently smooth function decaying sufficiently fast at infinity, and not in general for arbitrary functions $u,v\\in L^2$. \nIn general for differential operators, the operator itself will not be bounded on an $L^2$ Hilbert space, and so the operator is only densely defined on your Hilbert space. Therefore the adjoint can only be defined on another subset of the Hilbert space, the domain of the adjoint. (In the most general cases, the domain of the adjoint can be a much, much smaller set [even finite dimensional], so does not make much sense as an operator on the original Hilbert space. For differential operators, the adjoint is still densely defined using the density of $C^\\infty_0$ in $L^2$.) (Note that also if the spatial domain has a boundary, the integration by parts formula picks up a boundary term in general, so you pick up a further problem with the notion of adjoints, related to the fact that $C^\\infty_0(\\Omega)$ is not dense in the Sobolev space $W^{1,2}(\\Omega)$ when $\\Omega$ has boundary.)\nWhile the word \"formal\" is, I think, not mentioned explicitly, a lot of the problems that can arise when you deal with unbounded operators are discussed in chapter 8 of Reed-Simon, \"Methods of mathematical physics\".", "meta": {"post_id": 12894, "input_score": 29, "output_score": 48, "post_title": "Distinction between 'adjoint' and 'formal adjoint'"}}
{"input": "I am wondering how much a smooth function may be non-analytic, because in proofs, whilst there non-analytic smooth functions, it would suffice if a smooth function were analytic on only a \"small set\". More exactly:\nLet $U \\in \\mathbb R^n$ be open, $C^\\infty = C^\\infty(U,\\mathbb R)$. A smooth function in $C^\\infty$ is analytic in $a \\in U$, iff there exists $\\epsilon > 0$, s.t. the function is equal to its own Taylor series in $B_\\epsilon(a)$. There exist smooth functions that are non-analytic, i.e. there exists $f \\in C^\\infty, b \\in U, \\epsilon > 0$ s.t. the function is not its taylor series at $x$ in $B_\\epsilon (b)$.\nLet $A$ be the union of all $\\epsilon$-Balls in $U$ where $f$ is analytic. By definition, $A$ is open. It's complement $C = A^c$is the closed set of points where $f$ is non-analytic.\nDoes $C$ have an interior?", "output": "Yes, that can happen. The canonical example (as far as I know) is the Fabius function which is smooth and nowhere analytic.\n\nI never really liked this answer because it's hard \u2014 if not outright impossible \u2014 to find references online on how to prove the nonanalyticity of $Fb$. So here is a short exposition on a different example adapted from the discussion archived in this text file mirrored on archive.org.\n\nIntroduction\nStudents often see\n$$ f(x) = \\begin{cases}\n    \\exp(-\\tfrac{1}{x}) & \\text{for } x > 0 \\\\\\\\\n    0                  & \\text{for } x \\leq 0\n\\end{cases}$$\nas an example of a smooth function that's not analytic at $0$, but this as well as the other usual examples makes it very easy to intuit that smooth functions are \u201cmostly analytic\u201d, i.e. everywhere analytic except possibly at some isolated points. And given that this is the only example that one typically sees this is in fact a reasonable thing to believe. But nonetheless there are plenty of examples out there \u2014 for example the following: \nExample\nDefine $F: \\mathbb{R} \\to \\mathbb{C}$ by\n$$ F(x) := \\sum_{n=0}^\\infty \\frac{\\exp(i2^nx)}{n!}. $$\nTheorem: $F$ and $\\Re F$, the real part of $F$, are smooth nowhere analytic functions.\nProof: Computing the derivatives of $F$ we get\n$$ F^{(k)}(x) = \\sum_{n=0}^\\infty \\frac{\\exp(i2^nx)(i2^n)^k}{n!}, $$\nwhich is uniformly convergent everywhere, so it's continuous and there are no issues with moving the differentiation in under the summation. In other words $F$ and $\\Re F$ are smooth. Furthermore we have that \n$$ F^{(k)}(0) = \\sum_{n=0}^\\infty \\frac{(i2^n)^k}{n!} = i^k \\exp(2^k). $$\nThe Cauchy-Hadamard formula for the radius of convergence, $r$, for the Taylor series for $F$ gives at $x=0$ with the usual conventions about infinities that \n$$ \\frac{1}{r} = \\limsup_{k \\to \\infty} \\left\\lvert \\frac{F^{(k)}(0)}{k!}\\right\\rvert^{1/k} = \\limsup_{k \\to \\infty} \\left(\\frac{\\exp(2^k)}{k!} \\right)^{1/k} = \\infty, $$\nand so $r = 0$. The same will be true for $\\Re f$ as the even-indexed parts of $\\Re F$ have the same magnitude as those of $F$. $F$ is $2\\pi$-periodic, so the same is true for every $x$ which is an integer multiple of $2\\pi$. Moreover we see that if we throw away the first $p$ terms, we get a function with period $\\omega = \\frac{2\\pi}{2^p}$ and with points of non-analyticity at all integer multiples of $\\omega$. Hence $F$ and $\\Re F$ must also be nonanalytic at these points and we conclude that the set of points where the two functions are nonanalytic is dense in $\\mathbb{R}$. \nFinally observe that if a function is analytic in a point it must be analytic on a neighbourhood of that point, thus there are no possible open sets where $F$ and $\\Re F$ can be analytic, i.e. they are smooth nowhere analytic functions. $\\;\\square$\nGood Questions to Ponder\nSuppose $\\Omega \\subseteq \\mathbb{R}^n$ is open.\nIs there an $f \\in C^\\infty(\\mathbb{R}^n)$ such that $\\Omega$ is its locus of analyticity?\nAre the functions that are analytic at some point of the first category in $C^\\infty(\\Omega)$?", "meta": {"post_id": 12989, "input_score": 26, "output_score": 36, "post_title": "A smooth function's domain of being non-analytic"}}
{"input": "I was reading the following article on Ultrafinitism, and it mentions that one of the reasons ultrafinitists believe that N is not infinite is because the floor of $e^{e^{e^{79}}}$ is not computable. I was wondering if that's the case because of technological limitations, or whether there is another reason we cannot find a floor of this number.", "output": "In the formal meaning of \"computable\" the floor of that number is indeed computable. This is to say that a patient immortal human with access to unlimited paper and pencil could, in principle, work out the answer. (Here I assume, for technical reasons, that the number in question is not an integer - I assume someone who knows enough number theory will be able to cite a result that implies this.)\nThe article linked makes the weaker claim that the value has not yet been calculated, which seems likely to me. The issue they are concerned about is that humans are not immortal and that our supply of paper is very limited. If the number of decimal digits in the value is too large, it would be impossible to actually represent it in any physical way within our universe. \nIn general, I think it is more accurate to say that ultrafinitists don't accept that the set of all natural numbers is a coherent entity - not that they think it is finite. However, as the article you linked alludes, it is very difficult to find a coherent but non-arbitrary way to say what natural numbers are without accepting that there are an infinite number of them.  \nAddendum Here is why I am worried whether $e^{e^{e^{79}}}$ is an integer. It's certainly correct that no matter what, the floor of that number is an integer and is therefore computable. That part of my argument is fine.\nOn the other hand, if $e^{e^{e^{79}}}$ is not an integer, then I can tell you a specific algorithm to use to compute it. Namely, compute better and better upper and lower bounds until they fall strictly between two consecutive integers (which they must, since their limit is not an integer) and then pick the smaller of those two integers.  \nIf $e^{e^{e^{79}}}$ is an integer, then that algorithm won't work, because it will never stop. But if we knew that $e^{e^{e^{79}}}$ was an integer then we could take better and better upper and lower approximations until they straddle a single integer, and then pick that. \nSo the reason that I am interested whether the number is an integer is that, beyond merely knowing that the floor is an integer, I'd like to know which algorithm could be used to compute it. \nIn any case, I don't think that the point of the example was to pick a number that is not known to be integer or known to not be an integer. The point of the example should be to pick a number which is simply too large to represent physically.  I was hoping that someone would have a quick answer that confirms $e^{e^{e^{79}}}$ is not an integer, so I could edit my response with that info. But the non-integer property seems more difficult than I thought.", "meta": {"post_id": 13050, "input_score": 41, "output_score": 49, "post_title": "$e^{e^{e^{79}}}$ and ultrafinitism"}}
{"input": "In this question, I needed to assume in my answer that $e^{e^{e^{79}}}$ is not an integer. Is there some standard result in number theory that applies to situations like this? \nAfter several years, it appears this is an open problem. As a non-number theorist, I had assumed there would be known results that would answer the question. I was aware of the difficulty in proving various constants to be transcendental -- such as $e + \\pi$, which is not known to be transcendental at present. \nHowever, I was looking at a question that seems simpler, naively: whether a number is an integer, rather than whether it is transcendental.  It seems that what appeared to be possibly simpler is actually not, with current techniques. \nThe main motivation for asking about this particular number is that it is very large. It is certainly possible to find a pair of very large numbers, at least one of which is transcendental. But the current lack of knowledge about this particular number is even an integer shows just how much progress remains to be made, in my opinion.  Any answers that describe techniques that would suffice to solve the problem (perhaps with other, unproven assumptions) would be very welcome.", "output": "The paper Chuangxun Cheng, Brian Dietel, Mathilde Herblot, Jingjing Huang, Holly Krieger, Diego Marques, Jonathan Mason, Martin Mereb, and S. Robert Wilson, Some consequences of Schanuel\u2019s conjecture, Journal of Number Theory 129 (2009) 1464\u20131467, shows that $e,e^e,e^{e^e},\\dots$ is an algebraically independent set, on the assumption of Schanuel's Conjecture. Maybe a close reading of that paper will suggest a way of applying the result to the $79-$question.", "meta": {"post_id": 13054, "input_score": 203, "output_score": 47, "post_title": "How to show $e^{e^{e^{79}}}$ is not an integer"}}
{"input": "There is a formula given in my module:\n$$\n\\sqrt[n]{a^n} = \\begin{cases}\n \\, a  &\\text{ if $n$ is odd } \\\\\n |a| &\\text{ if $n$ is even }\n\\end{cases}\n$$\nI don't really understand the differences between them, kindly explain with an example.", "output": "$\\sqrt{(-1)^2} =\\sqrt{1} = 1 = |-1|$ because in order to make the square root an unambiguous operation, we agree that the square root of a nonnegative number $x$ is always the (unique) nonnegative number $r$ such that $r^2=x$.  But with cubic roots there is no problem: $\\sqrt[3]{-1}=-1$, because every real number has a unique cubic root.\nThe same is true with 4th, 6th, 8th, 10th, etc. powers, since $a^n = (-a)^n$, and the 4th, 6th, 8th, 10th, etc. roots are defined to be the unique nonnegative real number that \"works\", so that they are unambiguous.\nThat is, there are two numbers which when squared will give you the value $2^2$: both $2$ and $-2$. There are two numbers that when taken to the fourth power will give you $(-6)^4$: both $-6$ and $6$. And so on. Generally, both $a$ and $-a$ will, when raised to an even $n$th power, give the same answer: $a^n = (-a)^n$. And we agree that a square root (fourth root, sixth root, etc.) will always be the nonnegative answer, so the $n$th root of $a^n$ will be $|a|$ when $n$ is even. (Don't let the big $-$ in \"$-a$\" fool you; that does not mean that $-a$ is negative, it just means the additive inverse of whatever $a$ is; if $a$ is positive, then $-a$ is negative, but if $a$ is negative, say $a=-3$, then $-a$ is positive, $-a = -(-3) = 3$. Repeat after me: the proper way to pronounce \"-a\" is not \"negative a\", the proper pronunciation is \"minus a\").\nBut if $n$ is odd, then every number has a unique $n$th root. In particular, the only number that when cubed gives $2^3$ is $2$; the only number which, when raised to the fifth power, gives $(-6)^5$, is $-6$. There is no longer the problem that both $6$ and $-6$ are possible answers, so we can simply say that the cubic root of $(-2)^3$ is $-2$, the fifth root of $7^5$ is $7$, etc.", "meta": {"post_id": 13094, "input_score": 33, "output_score": 47, "post_title": "Significance of $\\sqrt[n]{a^n} $?!"}}
{"input": "My undergraduate number theory class decided to dip into a bit of algebraic geometry to finish up the semester. I'm having trouble understanding this bit of information that the instructor presented in his notes. \nHere it is in paraphrase (assume we are over an abstract field k)\nWe take a polynomial in k, $f  =Y^2 - X^3 -aX -b$ and homogenize the polynomial to $F = Y^2Z - X^3 -aXZ^2 - bZ^3$. Note that the points at infinity of V(F) consist of triples $[\\alpha : \\beta : 0]$ s.t $ -\\alpha^3 = 0$, hence the only point at infinity is $[0 : 1 :0]$\r\nThe part I'm confused about is in italics. He introduces the terms \"points at infinity\" without defining it. After some google time, I understand what a point at infinity means in the context of a projective space/projective line but am having trouble understanding how the professor came to his conclusion about the point at infinity in this particular example \nHere is my question. In general, are all points in the locus of vanishing points for a homogeneous polynomials considered points at infinity? If not, is there a general procedure for calculating these point if we are given an arbitrary polynomial?\nMore abstractly, How do I understand that a finite point in the projective space is a \"point at infinity\" for this polynomial.", "output": "Here's another way to think about the \"line at infinity\" and the \"points at infinity\"...\nThink of the usual $XY$-plane as sitting inside of $3$-space, but instead of it sitting in its usual place, $\\{(x,y,0) : x,y\\in\\mathbb{R}\\}$, shift it up by $1$ so that it sits as the $z=1$ plane.\nNow, you are sitting at the origin with a very powerful laser pointer. Whenever you want a point on the $XY$-plane, you shine your laser pointer at that point. So, if you want the point $(x,y)$, you are actually pointing your laser pointer at the point $(x,y,1)$; since you are sitting at the origin, the laser beam describes a (half)-line, joining $(0,0,0)$ to $(x,y,1)$. \nNow, for example, look at the point $(x,0,1)$, and imagine $x$ getting larger. The angle your laser pointer makes with the $z=0$ plane gets smaller and smaller, until \"as $x$ goes to infinity\", your laser pointer is just pointing along the line $x$ axis (at the point $(1,0,0)$), and the same thing happens if you let $x$ go to $-\\infty$. More generally, if you start pointing to points that are further and further away from the \"origin\" in your plane (away from $(0,0,1)$), the laser beam's angle with $Z=0$ gets smaller and smaller, until, \"at the limit\" as $||(x,y)||\\to\\infty$, you end up with the laser beam pointing along the $z=0$ plane in some direction. We can represent the direction with the slope of the line, so that we are pointing at $(1,m,0)$ for some $m$ (or perhaps to $(-1,-m,0)$, but that's the same direction), or perhaps to the point $(0,1,0)$. So we \"add\" these \"points at infinity\" (so called because we get them by letting the point we are shining the laser beam on \"go to infinity\"), one for each direction away from the \"origin\": $(1,m,0)$ for arbitrary $m$ for nonvertical lines, and $(0,1,0)$ corresponding to the direction of $x=0$, $y\\to\\pm\\infty$. \nSo: the \"usual\", affine points, are the ones in the $z=1$ plane, and they correspond to laser beams coming from the origin; they are each of the form $(x,y,1)$ for some $x,y$ in $\\mathbb{R}$. In addition, for each \"direction\" we want to include that limiting laser beam which does not intersect the plane $z=1$; those correspond to points $(1,m,0)$, or the point $(0,1,0)$ when you do it with the line $x=0$. So we get one point for every real $m$, $(1,m,0)$, and another for $(0,1,0)$. You are adding one point for every direction of lines through the origin; these points are the \"points at infinity\", and together they make the \"line at infinity\".\nNow, put your elliptic curve/polynomial $F=Y^2 - X^3 - aX-b$, and draw the points that correspond to it on the $z=1$ plane; that's the \"affine piece\" of the curve. But do you also get any of those \"points at infinity\"?\nWell, even though we are thinking of the points as being on the $XY$-plane, they \"really\" are in the $Z=1$ plane; so our equation actually has a \"hidden\" $Z$ that we lost sight of when we evaluated at $Z=1$. We use the homogenization $f = Y^2Z - X^3 - aXZ^2 - bZ^3$ to find it. Why that? Well, for any fixed point $(x,y,1)$ in our \"$XY$-plane\", the laser pointer points to all points of the form $(\\alpha x,\\alpha y,\\alpha)$. If we were to shift up our copy of the plane from $Z=1$ to $Z=\\alpha$, we'll want to scale everything so that it still corresponds to what I'm tracing from the origin; this requires that every monomial have the same total degree, which is why we put in factors of $Z$ to complete them to degree $3$, the smallest we can (making it bigger would give you the point $(0,0,0)$ as a solution, and we do need to stay away from that because we cannot point the laser pointer in our eye). \nOnce we do that, we find the \"directions\" that also correspond to our curve by setting $Z=0$ and solving, to find those points $(1,m,0)$ and $(0,1,0)$ that may also lie in our curve. But the only one that works is $(0,1,0)$, which is why the elliptic curve $F$ only has one \"point at infinity\".", "meta": {"post_id": 13763, "input_score": 29, "output_score": 50, "post_title": "Elliptic Curves and Points at Infinity"}}
{"input": "The Collatz Conjecture is a famous conjecture in mathematics that has lasted for over 70 years. It goes as follows:\nDefine $f(n)$ to be as a function on the natural numbers by:\n$f(n) = n/2$ if $n$ is even and\n$f(n) = 3n+1$ if $n$ is odd\nThe conjecture is that for all $n \\in \\mathbb{N}$, $n$ eventually converges under iteration by $f$ to $1$.\nI was wondering if the \"5n+1\" problem has been solved. This problem is the same as the Collatz problem except that in the above one replaces $3n+1$ with $5n+1$.", "output": "You shouldn't expect this to be true. Here is a nonrigorous argument. Let $n_k$ be the sequence of odd numbers you obtain. So (heuristically), with probability $1/2$, we have $n_{k+1} = (5n_k+1)/2$, with probability $1/4$, we have $n_{k+1} = (5 n_k+1)/4$, with probability $1/8$, we have $n_{k+1} = (5 n_k+1)/8$ and so forth. Setting $x_k = \\log n_k$, we approximately have $x_{k+1} \\approx x_k + \\log 5 - \\log 2$ with probability $1/2$, $x_{k+1} \\approx x_k + \\log 5 - 2 \\log 2$ with probability $1/4$, $x_{k+1} \\approx x_k + \\log 5 - 3 \\log 2$ with probability $1/8$ and so forth.\nSo the expected change from $x_{k}$ to $x_{k+1}$ is \n$$\\sum_{j=1}^{\\infty} \\frac{ \\log 5 - j \\log 2}{2^j} = \\log 5 - 2 \\log 2.$$\nThis is positive! So, heurisitically, I expect this sequence to run off to $\\infty$. This is different from the $3n+1$ problem, where $\\log 3 - 2 \\log 2 <0$, and so you heurisitically expect the sequence to decrease over time. \nHere is a numerical example. I started with $n=25$ and generated $25$ odd numbers. Here is a plot of $(k, \\log n_k)$, versus the linear growth predicted by my heuristic. Notice that we are up to 4 digit numbers and show no signs of dropping down.", "meta": {"post_id": 14569, "input_score": 31, "output_score": 49, "post_title": "The $5n+1$ Problem"}}
{"input": "I have an HP 50g graphing calculator and I am using it to calculate the standard deviation of some data. In the statistics calculation there is a type which can have two values:\nSample\nPopulation\nI didn't change it, but I kept getting the wrong results for the standard deviation. When I changed it to \"Population\" type, I started getting correct results!\nWhy is that? As far as I know, there is only one type of standard deviation which is to calculate the root-mean-square of the values!\nDid I miss something?", "output": "There are, in fact, two different formulas for standard deviation here: The population standard deviation $\\sigma$ and the sample standard deviation $s$.\nIf $x_1, x_2, \\ldots, x_N$ denote all $N$ values from a population, then the (population) standard deviation is \n$$\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2},$$\nwhere $\\mu$ is the mean of the population.  \nIf $x_1, x_2, \\ldots, x_N$ denote $N$ values from a sample, however, then the (sample) standard deviation is \n$$s = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N (x_i - \\bar{x})^2},$$\nwhere $\\bar{x}$ is the mean of the sample.\nThe reason for the change in formula with the sample is this: When you're calculating $s$ you are normally using $s^2$ (the sample variance) to estimate $\\sigma^2$ (the population variance).  The problem, though, is that if you don't know $\\sigma$ you generally don't know the population mean $\\mu$, either, and so you have to use $\\bar{x}$ in the place in the formula where you normally would use $\\mu$.  Doing so introduces a slight bias into the calculation: Since $\\bar{x}$ is calculated from the sample, the values of $x_i$ are on average closer to $\\bar{x}$ than they would be to $\\mu$, and so the sum of squares $\\sum_{i=1}^N (x_i - \\bar{x})^2$ turns out to be smaller on average than $\\sum_{i=1}^N (x_i - \\mu)^2$.  It just so happens that that bias can be corrected by dividing by $N-1$ instead of $N$.  (Proving this is a standard exercise in an advanced undergraduate or beginning graduate course in statistical theory.)  The technical term here is that $s^2$ (because of the division by $N-1$) is an unbiased estimator of $\\sigma^2$. \nAnother way to think about it is that with a sample you have $N$ independent pieces of information.  However, since $\\bar{x}$ is the average of those $N$ pieces, if you know $x_1 - \\bar{x}, x_2 - \\bar{x}, \\ldots, x_{N-1} - \\bar{x}$, you can figure out what $x_N - \\bar{x}$ is.  So when you're squaring and adding up the residuals $x_i - \\bar{x}$, there are really only $N-1$ independent pieces of information there.  So in that sense perhaps dividing by $N-1$ rather than $N$ makes sense.  The technical term here is that there are $N-1$ degrees of freedom in the residuals $x_i - \\bar{x}$.\nFor more information, see Wikipedia's article on the sample standard deviation.", "meta": {"post_id": 15098, "input_score": 58, "output_score": 105, "post_title": "Sample Standard Deviation vs. Population Standard Deviation"}}
{"input": "Hopefully without getting too complicated, how is a norm different from an absolute value?\nIn context, I am trying to understand relative stability of an algorithim:\nUsing the inequality $\\frac{|(x_0)- \\tilde{f}(\\epsilon, x_0)|}{|f(x_0)|} \\leq \\sigma_{rel} ||\\epsilon|| + o(||\\epsilon||)$\nFor which $\\sigma_{rel}$ describes the relative stability.\nAnd $||\\epsilon||$ is the maximum rounding error of an elementary function: $||\\epsilon|| = \\mbox{max} |\\epsilon_i|, i=1,...n$,  $\\epsilon = (\\epsilon_1,...,\\epsilon_n)$.\nAfter reading some of the wikipedia article on norms, I could not take away much other than a norm is positive and a length. I am confused about the statement that it is a function...\nAny help trying to understand this would be great, thanks!", "output": "The absolute value is a particular instance of a norm. Or perhaps, you can think of norms as functions $\\mathbf{V}\\to\\mathbb{R}$ where $\\mathbf{V}$ is a vector space over a field $\\mathbf{F}$, and \"absolute values\" are \"norms on the base field\". \nThe absolute value is a function $|\\>|\\colon\\mathbb{R}\\to[0,\\infty)$; given any real number $r$, you get a nonnegative real number that we write $|r|$, and which satisfies the following properties:\n\n$|x|\\geq 0$ for all $x\\in\\mathbb{R}$; $|x|=0$ if and only if $x=0$.\n$|rx| = |r||x|$ for all $r,x$.\n$|x+y| \\leq |x|+|y|$.\n\nIt's a real valued function of real variable, because it takes a real number as an input, and gives a (nonnegative) real number as an output. It's just that instead of calling the function, say, $f$, and writing the output as $f(x)$, we call the function \"$|\\>|$\" and write the output as $|x|$.\nSimilarly, we have the modulus function for complex numbers, $|\\>|\\colon\\mathbb{C}\\to[0,\\infty)$, defined by $|z| = \\sqrt{z\\overline{z}}$, and which also satisfies the three conditions above.\nIf $\\mathbf{V}$ is a vector space over $\\mathbb{R}$, then a norm on $\\mathbf{V}$ is a function $||\\>||\\colon \\mathbf{V}\\to[0,\\infty)$ that generalizes the absolute value; it must satisfy:\n\n$||\\mathbf{x}|| \\geq 0$ for all $\\mathbf{x}\\in\\mathbf{V}$; $||\\mathbf{x}||=0$ if and only if $\\mathbf{x}=\\mathbf{0}$.\n$||r\\mathbf{x}|| = |r|\\,||\\mathbf{x}||$ for all $r\\in\\mathbb{R}$, all $\\mathbf{x}\\in\\mathbf{V}$.\n$||\\mathbf{x}+\\mathbf{y}|| \\leq ||\\mathbf{x}|| + ||\\mathbf{y}||$ for all $\\mathbf{x},\\mathbf{y}\\in\\mathbf{V}$.\n\nAgain, this is a function: given any vector $\\mathbf{x}$ in the domain, $||\\mathbf{x}||$ is the output of the function (a nonnegative real number). \nIn particular, if you view $\\mathbb{R}$ as a vector space over itself, then the absolute value gives a norm on $\\mathbb{R}$. \nFor vector spaces over $\\mathbb{C}$, you replace the absolute value of $r$ in the second property with the modulus of $r$. \nThe norm you describe in your post, $||\\epsilon||=\\max|\\epsilon_i|$ is a particular norm that can be placed on $\\mathbb{R}^n$; there are many norms that can be defined on $\\mathbb{R}^n$.  \nThe notion of norm on a vector space can be done with any field that is contained in $\\mathbb{C}$, by restricting the modulus to that field.\nAdded. One can also extend the notion of norm by starting with any field $\\mathbf{F}$, such as the rationals, simply asking for an \"absolute value\" function $|\\>|\\colon \\mathbf{F}\\to [0,\\infty)$ that satisfies properties 1, 2, and 3 above. Then you extend the notion of norm for any vector space over that field. It is common to refer to such \"absolute values\" as \"norms\" so as not to confuse them with the usual absolute value. One classical example, mentioned by Asaf, are the $p$-adic norms on the rationals. First, fix a prime $p$; given an integer $p$, we define the $p$-order of $a$ to be the largest power of $p$ that divides $a$: that is, $\\mathrm{ord}_p(a) = n$ if and only if $p^n$ divides $a$ and $p^{n+1}$ does not divide $a$. We formally set $\\mathrm{ord}_p(0)=\\infty$. We then extend this to the rationals: given a rational $\\frac{a}{b}$, we let $\\mathrm{o}_p(\\frac{a}{b}) = \\mathrm{ord}_p(a) - \\mathrm{ord}_p(b)$. Finally, we define the $p$-adic norm on the rationals, $||\\>||_p\\colon\\mathbb{Q}\\to[0,\\infty)$ by $||\\frac{a}{b}||_p = p^{-o_p(a/b)}$. (You can use bases other than $p$; they amount to what are called \"equivalent norms\"). \nHere you really want to think of $||\\>||_p$ as a kind of \"non-standard absolute value\" on the rationals; it leads to a lot of interesting mathematics, beginning with the $p$-adic numbers, if you use $||\\>||_p$ to define \"Cauchy-sequences\" instead of $|\\>|$ and do the same construction that leads to the reals in the latter case.", "meta": {"post_id": 16163, "input_score": 23, "output_score": 44, "post_title": "How are norms different from absolute values?"}}
{"input": "Many courses and books assume that rings have an identity. They say there is not much loss in generality in doing so as rings studied usually have an identity or can be embedded in a ring with an identity. What then are the major applications of rings without an identity occurring naturally in mathematics?", "output": "The most common example of rings without identity occurs in functional analysis, when one considers rings of functions. A typical example is to consider the ring of all functions of compact support on a non-compact space. Obviously, as these rings of functions are very important in $C^*$-algebras and in studying the properties of the space, knowledge about rings without identity is very important for studying these spaces.\nArbitrary direct sums of rings with unity are not rings with unity, which can also be fairly annoying. \nIt is true that one can always embed a ring (as an ideal, even) into a ring with identity. The most common such embedding is the Dorroh embedding, in which we start with a ring $R$, and consider the ring with underlying set $\\mathbb{Z}\\times R$ and operations given by $(n,a)+(m,b) = (n+m,a+b)$ and $(n,a)(m,b) = (nm, nb+ma+ab)$. It is not hard to verify that $r\\mapsto (0,r)$  embeds $R$ into the Dorroh extension as an ideal. You can preserve the characteristic of $R$ if necessary: if $R$ is of characteristic $n$, then replace $\\mathbb{Z}$ with $\\mathbb{Z}/n\\mathbb{Z}$ in the construction. The extension has other nice properties (ideals of $R$ remain ideals of the extension, for example).\n(Luckily, I am currently going over a thesis about embedding rings as ideals into rings with identity, so I can give you some other classical results.)\nHowever, the Dorroh extension does not preserve all ring properties that may be of interest in $R$. For example, a ring is entire if it has no nonzero zero divisors; a ring is prime if whenever $A$ and $B$ are ideals and $AB=0$, then either $A=0$ or $B=0$ (that is, \"prime\" is the ideal version of \"entire\"; an entire ring is necessarily prime). For example, if you perform the Dorroh extension on $\\mathbb{Z}$ itself (perhaps not realizing it already had a $1$) then $(1,-1)(0,r)=(0,0)$ even though $\\mathbb{Z}$ is entire. There are nontrivial examples of this situation as well. Another property not necessarily preserved by the Dorroh extension is being semiprime.\nThere are other standard embeddings of rings into rings with identity, such as the Szendrei extension (a quotient of the Dorroh extension). But even so there are ring-theoretic properties that may be very hard to maintain in these kinds of embeddings. Among the more difficult ones are simplicity (if $R$ is simple, can we embed $R$ into a simple ring with identity? Yes; Anne Vakarietis, a student of a colleague, just finished putting together the pieces for this in her dissertation). It's known that every commutative $n$-root ring (rings in which every element has an $n$th root) can be embedded in a commutative $n$-root ring with identity, but it is not known if this is possible for noncommutative rings. Likewise, it is not known if every semiprimary ring can be embedded in a semiprimary ring with identity.\nAnd worse, there are some properties that we know cannot be respected by such embeddings. For example, Fuchs and Rangaswamy proved that not every $\\pi$-regular ring can be embedded as an ideal in a $\\pi$-regular ring with identity (a ring is $\\pi$-regular if every element is $n$-regular for some natural number $n$; an element $x$ is $n$-regular if there exists some $y$ such that $x^nyx^n=x^n$; this is a generalization of von Neumann regularity).\nSo, in summary: yes, rings without identity arise very naturally, and as such they show up when investigating other mathematical objects. And while it is true that one can always embed a ring without identity as an ideal into a ring with identity, this may not be a good thing from the point of view of studying some ring-theoretic properties of these rings.", "meta": {"post_id": 16168, "input_score": 28, "output_score": 38, "post_title": "Applications of rings without identity"}}
{"input": "I'm doing the exercises in \"Introduction to commutive algebra\" by Atiyah&MacDonald. In chapter two, exercises 24-26 assume knowledge of the Tor functor.\nI have tried Googling the term, but I don't find any readable sources. Wikipedia's explanation use the the term \"take the homology\", which I don't understand (yet).\nAre there any good explanations of what the Tor functor is available online not assuming any knowledge about homology?\nThe first exercise:\n\"If $M$ is an $A$-module, TFAE:\n\n1) $M$ is flat\n2) $\\operatorname{Tor}_n^A (M,N)=0$ for all $n>0$ and all $A$-modules $N$.\n3) $\\operatorname{Tor}_1^A (M,N)=0$ for all $A$-modules $N$.\"\n\nThanks in advance.", "output": "You will be a lot more motivated to learn about Tor once you observe closely how horribly tensor product behaves. \nLet us look at the simplest example possible. Consider the ring $R=\\mathbb C[x,y]$ and the ideal $I=(x,y)$. These are about the most well-understood objects, right? What is the tensor product $I\\otimes_RI$? This is quite nasty, it has torsions: the element $u = x\\otimes y - y\\otimes x$ is non-zero, but $xu=yu=0$!\nTor gives you a black box to understand this kind of things. Take the short exact sequence $0 \\to I \\to R \\to R/I \\to 0$ and tensor with $I$ we get:\n$$0 \\to \\text{Tor}_1(R/I,I) \\to I\\otimes I \\to I \\to I/I^2 \\to 0$$ \nfrom which you can extract:\n$$0 \\to \\text{Tor}_1(R/I,I) \\to  I\\otimes I \\to I^2 \\to 0$$\nBut $\\text{Tor}_1(R/I,I) = \\text{Tor}_2(R/I, R/I) = \\mathbb C$ by standard homological algebra.  So now everything fits nicely: the map from $I\\otimes I \\to I^2$ takes $f\\otimes g$ to $fg$, and the kernel is generated by the element $u$, which is killed by $I$, so it is isomorphic to $R/I \\cong \\mathbb C$. \nTo summarize: tensor product, despite being a fundamental operation, is actually quite bad, and Tor helps you to understand it.", "meta": {"post_id": 16310, "input_score": 85, "output_score": 45, "post_title": "What is the Tor functor?"}}
{"input": "Question: Show that $\\pi_{1}({\\mathbb R}^{2} - {\\mathbb Q}^{2})$ is uncountable.\nMotivation: This is one of those problems that I saw in Hatcher and felt I should be able to do, but couldn't quite get there.\nWhat I Can Do:  There are proofs of this being path connected (though, I'm not exactly in love with any of those proofs) and this tells us we can let any point be our base-point.  Now, let $p$ be some point in ${\\mathbb R}^{2} - {\\mathbb Q}^{2}$ and let's let this be our base-point.  We can take one path from $p$ to $q$ and a second one from $q$ to $p$, and it's not hard to show that if these paths are different then there is at least one rational on the \"inside\" of it.  Since there are uncountably many $q$, this would seem to imply uncountably many different elements of the fundamental group; the problem I'm having is showing that two loops like we've described are actually different!  For example, a loop starting at $p$ and passing through $q$ should be different from a loop starting at $p$ and passing through $q'$ for none of these points the same, for at least an uncountable number of elements $q'$.  Is there some construction I should be using to show these elements of the fundamental group are different?", "output": "In case anyone doesn't like the arguments from uncountability, here's a more concrete argument:\nFix a base-point $(p,p)$ for an arbitrary $p\\in\\mathbb{R}-\\mathbb{Q}$. For any $q\\in\\mathbb{R}-\\mathbb{Q}$, $q\\not=p$, consider the loop $L_q$ consisting of the following line segments: $(p,p)\\rightarrow(p,q)\\rightarrow(q,q)\\rightarrow(q,p)\\rightarrow(p,p)$\nWe claim that for any $q_1<q_2$, the loops $L_{q_1}$ and $L_{q_2}$ are non-homotopic. To see this, pick a rational $r$ such that $q_1<r<q_2$. Embed $\\mathbb{R}^2-\\mathbb{Q}^2$ into $\\mathbb{R}^2-(r,r)$ (aka, the punctured plane). Clearly, $L_{q_1}\\equiv 0$ while $L_{q_2}\\equiv 1$, so the loops are non-homotopic, as desired.", "meta": {"post_id": 16948, "input_score": 45, "output_score": 43, "post_title": "$\\pi_{1}({\\mathbb R}^{2} - {\\mathbb Q}^{2})$ is uncountable"}}
{"input": "Let $G$ be a group where every non-identity element has order 2.   \nIf |G| is finite then $G$ is isomorphic to the direct product $\\mathbb{Z}_{2} \\times \\mathbb{Z}_{2} \\times \\ldots  \\times \\mathbb{Z}_{2}$.\nIs the analogous result \n $G= \\mathbb{Z}_{2} \\times \\mathbb{Z}_{2} \\times \\ldots $.\ntrue for the case |G| is infinite?", "output": "Perhaps the best way to look at the problem is to establish the following more precise result:\nFor a group $G$, the following are equivalent:\n(i) Every non-identity element of $G$ has order $2$.\n(ii) $G$ is commutative, and there is a unique $\\mathbb{Z}/2\\mathbb{Z}$-vector space structure on $G$ with the group operation as addition.\nI guess you probably already know how to show that if every nonidentity element has order $2$, $G$ is commutative: for all $x,y \\in G$, $e = (xy)^2 = xyxy$.  Multiplying on the left by $x$ and on the right by $y$ gives $xy = yx$. \nHaving established the commutativity, it is convenient to write the group law additively.  Then there is only one possible $\\mathbb{Z}/2\\mathbb{Z}$-vector space structure on $G$, since it remains to define a scalar multiplication and of course we need $0 \\cdot x = 0, \\ 1 \\cdot x = x$ for all $x \\in G$.  But you should check that this actually works: i.e., defines an $\\mathbb{Z}/2\\mathbb{Z}$-vector space structure, just by checking the axioms: the key point is that for all $x \\in G$, $(1+1)x = x + x = 0 = 0x$.\nSo now your question is equivalent to: is every $\\mathbb{Z}/2\\mathbb{Z}$ vector space isomorphic to a product of copies of $\\mathbb{Z}/2\\mathbb{Z}$?  Well, the only invariant of a vector space is its dimension.  It is clear that every finite-dimensional vector space is of this form.  Every infinite dimensional space is isomorphic to a direct sum $\\bigoplus_{i \\in I} \\mathbb{Z}/2\\mathbb{Z}$, the distinction being that in a direct sum, every element has only finitely many nonzero entries.  (In other words, the allowable linear combinations of basis elements are finite linear combinations.)  Moreover, for any infinite index set $I$, the direct sum $\\bigoplus_{i \\in I} \\mathbb{Z}/2\\mathbb{Z}$ has dimension $I$ and also cardinality $I$.  \nFinally, it is not possible for a direct product of two element sets to have countably infinite cardinality: if $I$ is infinite, it is at least countable, and then the infinite direct product has the same cardinality of the real numbers (think of binary expansions).  So the answer to your question is \"yes\" for direct sums, but \"no\" for direct products.", "meta": {"post_id": 17054, "input_score": 44, "output_score": 43, "post_title": "Group where every element is order 2"}}
{"input": "I'm studying graphs in algorithm and complexity,\n(but I'm not very good at math) as in title:\n\nWhy a complete graph has $\\frac{n(n-1)}{2}$ edges?\n\nAnd how this is related with combinatorics?", "output": "A simpler answer without binomials: A complete graph means that every vertex is connected with every other vertex. If you take one vertex of your graph, you therefore have $n-1$ outgoing edges from that particular vertex. \nNow, you have $n$ vertices in total, so you might be tempted to say that there are $n(n-1)$ edges in total, $n-1$ for every vertex in your graph. But this method counts every edge twice, because every edge going out from one vertex is an edge going into another vertex. Hence, you have to divide your result by 2. This leaves you with $n(n-1)/2$.", "meta": {"post_id": 17747, "input_score": 59, "output_score": 94, "post_title": "Why a complete graph has $\\frac{n(n-1)}{2}$ edges?"}}
{"input": "I would assume the answer to my question is yes, but I want to make sure because my book uses both terminologies.  Please also indicate where zero falls into the mix.\nUPDATE:\nHere is an excerpt from my book:\n\nThe definition of $\\Theta(g(n))$ requires\n  that every member $f(n) \\in \\Theta(g(n))$ be\n  asymptotically non-negative, that is,\n  that $f(n)$ be non-negative whenever n\n  is sufficiently large. (An\n  asymptotically positive function is\n  one that is positive for all\n  sufficiently large $n$.)", "output": "The real numbers can be partitioned into the positive real numbers, the negative real numbers, and zero.  A real number is one and only one of those three possibilities.  This is called \"trichotomy.\"  Non-negative (or, correspondingly, non-positive) means not negative (not positive), so zero or positive (zero or negative).\nThat is, non-negative includes zero whereas positive does not.\nEdit for clarity:\nNon-negative means zero or positive.\nNon-positive means zero or negative.\nThat is, non-negative includes zero whereas positive does not and vice versa.", "meta": {"post_id": 18464, "input_score": 40, "output_score": 49, "post_title": "Is positive the same as non-negative?"}}
{"input": "I am trying to prove a result, for which I have got one part, but I am not able to get the converse part.\nTheorem. Let $R$ be a commutative ring with $1$. Then $f(X)=a_{0}+a_{1}X+a_{2}X^{2} + \\cdots + a_{n}X^{n}$ is a unit in $R[X]$ if and only if $a_{0}$ is a unit in $R$ and $a_{1},a_{2},\\dots,a_{n}$ are all nilpotent in $R$.\nProof. Suppose $f(X)=a_{0}+a_{1}X+\\cdots +a_{n}X^{n}$ is such that $a_{0}$ is a unit in $R$ and $a_{1},a_{2}, \\dots,a_{r}$ are all nilpotent in $R$. Since $R$ is commutative, we get that $a_{1}X,a_{2}X^{2},\\cdots,a_{n}X^{n}$ are all nilpotent and hence also their sum is nilpotent. Let $z = \\sum a_{i}X^{i}$ then $a_{0}^{-1}z$ is nilpotent and so $1+a_{0}^{-1}z$ is a unit. Thus $f(X)=a_{0}+z=a_{0} \\cdot (1+a_{0}^{-1}z)$ is a unit since product of two units in $R[X]$ is a unit. \nI have not been able to get the converse part and would like to see the proof for the converse part.", "output": "Let $f=\\sum_{k=0}^n a_kX^k$ and $g= \\sum_{k=0}^m b_kX^k$. If $f g=1$, then clearly $a_0,b_0$ are units and:\n$$a_nb_m=0 \\tag1$$\n$$a_{n-1}b_m+a_nb_{m-1}=0$$\n(on multiplying both sides by $a_n$)\n$$\\Rightarrow (a_n)^2b_{m-1}=0 \\tag2$$\n$$a_{n-2}b_m+a_{n-1}b_{m-1}+a_nb_{m-2}=0$$\n(on multiplying both sides by $(a_n)^2$)\n$$\\Rightarrow (a_n)^3b_{m-2}=0 \\tag3$$\n$$.....$$\n$$.....+a_{n-2}b_2+a_{n-1}b_1+a_nb_0=0$$\n(on multiplying both sides by $(a_n)^m$)\n$$\\Rightarrow (a_n)^{m+1}b_{0}=0 \\tag{m+1}$$\nSince $b_0$ is an unit, it follows that $(a_n)^{m+1}=0$.\nHence, we proved that $a_n$ is nilpotent, but this is enough. Indeed, since $f$ is invertible, $a_nx^n$ being nilpotent implies that $f-a_nX^n$ is unit and we can repeat (or more rigorously, perform induction on $\\deg(f)$).", "meta": {"post_id": 19132, "input_score": 69, "output_score": 76, "post_title": "Characterizing units in polynomial rings"}}
{"input": "If a function is a combination of other functions whose derivatives are known via composition, addition, etc., the derivative can be calculated using the chain rule and the like. But even the product of integrals can't be expressed in general in terms of the integral of the products, and forget about composition! Why is this?", "output": "I guess the OP asks about the symbolic integration. Other answers already dealt with the numeric case where integration is easy and differentiation is hard.\nIf you recall the definition of the differentiation, you can see it's just a subtraction and division by a constant. Even if you can't do any algebraic changes, it won't get any more complex than that. But usually you can do many simplifications due to the zero limit, as many terms fall out as being too small. From this definition it can be shown that if you know the derivative of $f(x)$ and $g(x)$, then you can use these derivatives to express the derivative of $f(x) \\pm g(x)$, $f(x)g(x)$ and $f(g(x))$. \nThis makes symbolic differentiation easy as you just need to apply the rules recursively.\nNow about integration. Integration is basically an infinite sum of small quantities. So if you see an $\\int f(x) \\, dx$. You can imagine it as an infinite sum of $(f_1 + f_2 + ...) \\, dx$ where $f_i$ are consecutive values of the function. \nThis means if you need to calculate integral of $\\int (a f(x) + b g(x)) \\,d x$. Then you can imagine the sum $((af_1 + bg_1) + (af_2 + bg_2) + ...) \\,d x$. Using the associativity and distributivity, you can transform this into: $a(f_1 + f_2 +...)\\,d x + b(g_1 + g_2 + ...)\\,d x$.\nSo this means $\\int (a f(x) + b g(x)) \\, d x = a \\int f(x) \\,d x + b \\int g(x) \\, dx$.\nBut if you have $\\int f(x) g(x) \\, d x$, you have the sum $(f_1 g_1 + f_2 g_2 + ...) \\,d x$. From which you cannot factor out the sum of $f$s and $g$s. This means there is no recursive rule for multiplication. \nSame goes for $\\int f(g(x)) \\,d x$. You cannot extract anything from the sum $(f(g_1) + f(g_2) + ...) \\,d x$ in general.\nSo far, only linearity is the useful property. What about the  analogues of the Differentiation rules? We have the product rule:\n$$\\frac{d f(x)g(x) }{\\, d x} = f(x) \\frac{d g(x)}{\\, d x} + g(x) \\frac{d f(x)}{\\, d x}.$$ Integrating both sides and rearranging the terms, we get the well-known integral by parts formula:\n$$\\int f(x) \\frac{d g(x)}{\\, d x} \\, d x = f(x)g(x) - \\int g(x) \\frac{d f(x)}{\\, d x} \\, d x.$$\nBut this formula is only useful if $\\frac{d f(x)}{dx} \\int g(x) \\, d x$ or \n$\\frac{d g(x)}{dx} \\int f(x) \\, d x$ is easier to integrate than $f(x)g(x)$.\nAnd it's often hard to see when this rule is useful. For example, when you try to integrate $\\mathrm{ln}(x)$, it's not obvious to see that it's $1 \\mathrm{ln}(x)$. The integral of $1$ is $x$ and the derivative of $\\mathrm{ln}(x)$ is $\\frac{1}{x}$, which lead to a very simple integral of $x\\frac{1}{x} = 1$, whose integral is again $x$.\nAnother well-known differential rule is the chain rule $$\\frac{d f(g(x))}{\\, d x} = \\frac{d f(g(x))}{d g(x)} \\frac{d g(x)}{\\, d x}.$$\nIntegrating both sides, you get the reverse chain rule:\n$$f(g(x)) = \\int \\frac{d f(g(x))}{d g(x)} \\frac{d g(x)}{\\, d x} \\, d x.$$\nBut again it's hard to see when it is useful. For example what about the integration of $\\frac{x}{\\sqrt{x^2 + c}}$? Is it obvious to  you that $\\frac{x}{\\sqrt{x^2 + c}} = 2x \\frac{1}{2\\sqrt{x^2 + c}}$ and this is the derivative of $\\sqrt{x^2 + c}$? I guess not, unless someone showed you the trick.\nFor differentiation, you can mechanically apply the rules. For integration, you need to recognize patterns and even need to introduce cancellations to bring the expression into the desired form and this requires lot of practice and intuition.\nFor example how would you integrate $\\sqrt{x^2 + 1}$? \nFirst you turn it into a fraction:\n$$\\frac{x^2 + 1}{\\sqrt{x^2+1}}$$\nThen multiply and divide by 2:\n$$\\frac{2x^2 + 2}{2\\sqrt{x^2+1}}$$\nSeparate the terms like this:\n$$\\frac{1}{2}\\left(\\frac{1}{\\sqrt{x^2+1}}+\\frac{x^2+1}{\\sqrt{x^2+1}}+\\frac{x^2}{\\sqrt{x^2+1}} \\right)$$\nPlay with 2nd and 3rd term:\n$$\\frac{1}{2}\n\\left(\n\\frac{1}{\\sqrt{x^2+1}}+\n1\\sqrt{x^2+1}+\nx2x\\frac{1}{2\\sqrt{x^2+1}} \n\\right)$$\nNow you can see the first bracketed term is the derivative of $\\mathrm{arsinh(x)}$. The second and third term is the derivative of the $x\\sqrt{x^2+1}$. Thus the integral will be:\n$$\\frac{\\mathrm{arsinh}(x)}{2} + \\frac{x\\sqrt{x^2+1}}{2} + C$$\nWere these transformations obvious to you? Probably not. That's why differentiation is just a mechanic while integration is an art.", "meta": {"post_id": 20578, "input_score": 140, "output_score": 37, "post_title": "Why is integration so much harder than differentiation?"}}
{"input": "I'd like to calculate a standard deviation for a very large (but known) number of sample values, with the highest accuracy possible.  The number of samples is larger than can be efficiently stored in memory.\nThe basic variance formula is:\n$\\sigma^2 = \\frac{1}{N}\\sum (x - \\mu)^2$\n... but this formulation depends on knowing the value of $\\mu$ already.  \n$\\mu$ can be calculated cumulatively -- that is, you can calculate the mean without storing every sample value.  You just have to store their sum.\nBut to calculate the variance, is it necessary to store every sample value?  Given a stream of samples, can I accumulate a calculation of the variance, without a need for memory of each sample?  Put another way, is there a formulation of the variance which doesn't depend on foreknowledge of the exact value of $\\mu$ before the whole sample set has been seen?", "output": "I'm a little late to the party, but it appears that this method is pretty unstable, but that there is a method that allows for streaming computation of the variance without sacrificing numerical stability.\nCook describes a method from Knuth, the punchline of which is to initialize $m_1 = x_1$, and $v_1 = 0$, where $m_k$ is the mean of the first $k$ values. From there,\n$$\r\n\\begin{align*}\r\nm_k & = m_{k-1} + \\frac{x_k - m_{k-1}}k \\\\\r\nv_k & = v_{k-1} + (x_k - m_{k-1})(x_k - m_k)\r\n\\end{align*}\r\n$$\nThe mean at this point is simply extracted as $m_k$, and the variance is $\\sigma^2 = \\frac{v_k}{k-1}$. It's easy to verify that it works for the mean, but I'm still working on grokking the variance.", "meta": {"post_id": 20593, "input_score": 34, "output_score": 60, "post_title": "Calculate variance from a stream of sample values"}}
{"input": "I'm just working through Conway's book on complex analysis and I stumbled across this lovely exercise:\n\nUse Cauchy's Integral Formula to prove the Cayley-Hamilton Theorem: If $A$ is an $n \\times n$ matrix over $\\mathbb C$ and $f(z)  = \\det(z-A)$ is the characteristic polynomial of $A$ then $f(A) = 0$. (This exercise was taken from a paper by C. A. McCarthy, Amer. Math. Monthly, 82 (1975), 390-391)\n\nUnfortunately, I was not able to find said paper. I'm completely lost with this exercise. I can't even start to imagine how one could possibly make use of Cauchy here...\nThanks for any hints.\nRegards, S.L.", "output": "The idea is to use holomorphic functional calculus and to show that for a matrix $A$ and a polynomial $p(z)$ we have for $r \\gt \\|A\\|$ \n\\begin{equation}\\tag{$\\ast$}\np(A) = \\frac{1}{2\\pi i} \\int_{|z| = r} p(z) \\cdot (z - A)^{-1}\\ \\,dz\n\\end{equation}\nin complete analogy with the Cauchy formula for complex numbers. The integral of a matrix of holomorphic functions is defined by integrating each entry separately.\nBy Cramer's rule, the $(k,l)$-entry of $(z-A)^{-1}$ is $\\displaystyle ((z-A)^{-1})_{k,l} = \\frac{1}{\\det(z-A)} c_{k,l}(z)$ where $c_{k,l}(z)$ is some polynomial in $z$. Let $p(z) = \\det(z-A)$ be the characteristic polynomial of $A$. Conclude using\u00a0$(\\ast)$ by applying Cauchy's integral theorem to $c_{k,l}$.\n\nTo see that the identity\u00a0$(\\ast)$ holds, proceed as follows (this is a slight variant of McCarthy's argument):\n\nThe usual matrix norm induced by the Euclidean norm on $\\mathbb{C}^{n}$ satisfies $\\|A^{n}\\| \\leq \\|A\\|^{n}$.\nUse this to show that $(z - A)^{-1} = \\sum_{n = 0}^{\\infty} \\frac{A^{n}}{z^{n+1}}$, where the right hand side converges uniformly on $\\{|z| \\gt \\|A\\| + \\varepsilon\\}$.\nIt follows that we can interchange integration and summation. Conclude that $$ A^{k} = \\int_{|z| = r} z^{k} (z - A)^{-1}\\,dz$$ and $(\\ast)$ follows by linearity.\n\n\nHere's a link to McCarthy's article (you need a university subscription to download it, but the first page is almost the entire article).", "meta": {"post_id": 20677, "input_score": 44, "output_score": 42, "post_title": "Cauchy's integral formula for Cayley-Hamilton Theorem"}}
{"input": "Consider the following diagram:\n\nWhat does it mean precisely to say \"$f$ factors through $G/\\text{ker}(f)$\"?\nDoes it mean $f = \\tilde{f} \\circ \\pi$, for some $\\tilde{f}$?\nI've seen texts use the phrase, but never a definition of this notion.", "output": "It means exactly what you write: that you can express $f$ as  \"product\" (composition) of two functions, with the first function going through $G/\\mathrm{ker}(f)$; by implication, that map will be the \"natural\" map into the quotient, i.e., $\\pi$. Under more general circumstances, you would also indicate the map in question.\nThe reason for the term \"factors\" is that if you write composition of functions by juxtaposition, which is fairly common, then the equation looks exactly as if you \"factored\" $f$: $f=\\tilde{f}\\pi$.", "meta": {"post_id": 21932, "input_score": 56, "output_score": 48, "post_title": "What does it mean to say a map \"factors through\" a set?"}}
{"input": "Denote by $M_{n \\times n}(k)$ the ring of $n$ by $n$ matrices with coefficients in the field $k$.  Then why does this ring not contain any two-sided ideal?  \nThanks for any clarification, and this is an exercise from the notes of Commutative Algebra by Pete L Clark, of which I thought as simple but I cannot figure it out now.", "output": "A faster, and more general result, which Arturo hinted at, is obtained via following proposition from Grillet's Abstract Algebra, section \"Semisimple Rings and Modules\", page 360:\n\nConsequence: if $R:=D$ is a division ring, then $M_n(D)$ is simple.\nProof: Suppose there existed an ideal of $M_n(D)$. By the proposition, it'd be of the form $M_n(I)$, for $I\\unlhd D$, but division rings do not have any ideals (other than $0$ and $D$), so this is a contradiction. $\\blacksquare$", "meta": {"post_id": 22629, "input_score": 55, "output_score": 47, "post_title": "Why is the ring of matrices over a field simple?"}}
{"input": "I'm having trouble getting my head arround the concept. \nCan someone explain it to me?", "output": "I think that understanding comes through examples. The most fundamental example I believe to be the rotation group. Consider the sphere $S^2\\subset \\mathbb{R}^3$. The sphere has rotational symmetries. If we rotate the sphere by any angle, the sphere doesn't change. \nThe collection of all rotations forms a Lie group. The group property basically means that\nif we rotate the sphere over any angle $\\alpha$, after this over an angle $\\beta$, it is the same if we would have rotated it in one go (over some different angle). Also any rotation has an inverse (rotating it over the opposite angle). This makes the rotations a group. The \"Lie\" in Lie group means that these rotations can be done arbitrary small. Many small rotations makes for a big rotation.\nLie groups capture the concept of \"continuous symmetries\".", "meta": {"post_id": 22967, "input_score": 28, "output_score": 50, "post_title": "What is a Lie Group in layman's terms?"}}
{"input": "I hope this isn't off topic - sorry if I'm wrong.\nIn 1976, Kenneth Appel and Wolfgang Haken proved the claim (conjecture) that a map can always be coloured with four colours, with no adjacent regions given the same colour. This was controversial because the proof process required a computer to evaluate many different cases.\nFrom what I've read, the controversy was (1) that no human could reasonably confirm the proof, and (2) that being a computer proof, all it did was give a yes/no answer to the question - it didn't contribute to the understanding of the problem.\nThe first issue seems to be a non-issue now - the proof has been replicated using different software. But the second issue seems to stand - and I don't really understand that issue.\nQuite a few proofs require a number of particular cases of the problem to be separately checked. Having eliminated every possible case where the answer might have been \"no\" is a routine way of proving that the answer is \"yes\". Does it really make any difference in principle whether the number of cases is 2 or 2 million? Does the scale of that number make any difference to the degree of human understanding of the problem?\nAnd, given that people wrote the software that evaluated all the cases, all the ideas underlying the proof seem to be understandable by, and to have been understood by, people.\nTo me, having evaluated a thousand different variations on the same kind of a problem shows no more understanding than evaluating one. Solving a thousand quadratic equations, for instance, is much the same as solving one - all the repetitions are just plugging different numbers into the same formula, or repeating the same completing-the-square procedure, or whatever.\nTherefore, I am very much impressed that Appel and Haken were able to understand the problem in sufficient depth that they could write a program to derive what all the special cases are and check them. Writing software to reliably determine all the cases often shows even deeper understanding than manual derivation of all the cases, where deeper understanding can be bypassed to a degree by trial-and-error.\nGetting the computer to run the program, once written, seems irrelevant to me. The program presumably could have been (eventually) executed by a person, in the same way that it could have been executed by a Turing machine, but doing the mechanical follow-the-steps stuff seems irrelevant to depth of understanding.\nIs there something I'm missing?", "output": "When people say that Appel-Haken did not really contribute understanding, they aren't necessarily talking about the four-color problem itself. They mean that the proof did not really contribute any understanding of mathematics.\nA famous example when the proof of a long-standing conjecture really did contribute a lot of understanding is Andrew Wiles' proof of Fermat's Last Theorem. People weren't excited about this proof because they cared directly about the answer to FLT; they were excited because the proof brought together and integrated ideas from many disciplines and represented progress on the much larger Langlands program. Similarly, Perelman's proof of the Poincare Conjecture introduced important new ideas and tools. This was why many experts were convinced that Perelman's proof was valid even before it was formally checked; the high-level summary contained enough nontrivial ideas that they already saw that there was something extremely interesting going on.\nIn other words, the problem with Appel-Haken is that it is boring. It was more or less an application of an already-understood technique, just on a larger scale, and so has led to very little interesting new mathematics. People are still looking for a conceptual proof of the Four-Color Theorem analogous to the two proofs above, for example people working in quantum topology; see this blog post by Noah Snyder and Kainen's Quantum interpretations of the four-color theorem. A proof along these lines would be much more interesting, as it would likely shed light on a number of other issues in quantum topology. \n\nThis principle applies to more than just long-standing open problems. Many problems you might encounter in an undergraduate course might be solvable by a tedious computation along the lines of the Appel-Haken proof, but often there exist much more interesting conceptual proofs along the lines of Wiles' proof. If you stopped after finding the tedious calculation you might never find the conceptual argument, which often turns out to be much more interesting (e.g. it naturally suggests generalizations, interesting concepts, ties to other branches of mathematics...). \nI will take your comment about quadratic equations as an example. It's true that the quadratic formula allows you to solve quadratic equations mechanistically. The question, then, is whether the quadratic formula leads to any significant conceptual understanding of polynomials. For example, does it suggest a natural route to the cubic formula?\nIf you think of the quadratic equation in terms of completing the square, then you quickly run into a problem: you cannot, in general, complete the cube. So completing the square does not generalize to cubic equations. If you want to understand cubic equations, it follows that you need to think about the quadratic formula more conceptually.\nThe conceptual breakthrough is the following: what the quadratic formula really shows you is that there is a symmetry to the roots of a quadratic polynomial. The roots\n$$x_1 = \\frac{-b + \\sqrt{b^2 - 4ac}}{2a}$$\nand\n$$x_2 = \\frac{-b - \\sqrt{b^2 - 4ac}}{2a}$$\nare conjugate: they are related by a symmetry which flips the sign of the square root. This symmetry manifests itself concretely in the fact that the sum\n$$x_1 + x_2 = - \\frac{b}{a}$$\nis completely invariant under flipping the sign of the square root, whereas the sum\n$$x_1 - x_2 = \\frac{ \\sqrt{b^2 - 4ac} }{a}$$\nis completely negated under flipping the sign of the square root. Taking this idea seriously leads you to the method of Lagrange resolvents, and now if you were born in the right century you would be well on your way to inventing group theory, Galois theory, and (if you were really observant) representation theory. \nIsn't that way more interesting than using the quadratic formula?", "meta": {"post_id": 23409, "input_score": 21, "output_score": 34, "post_title": "Did the Appel/Haken graph colouring (four colour map) proof really not contribute to understanding?"}}
{"input": "So the question states that the intersection of two prime ideals is always a prime ideal. Well this is false but I need an example to counter it. I looked online and found one \"For example, inside $\\mathbb Z, 2 \\mathbb Z$ and $3\\mathbb Z$ are prime, but there intersection, $6\\mathbb Z$ is not prime\"\nso I just need some explanation to what a prime ideal is and how you can determine that an ideal is prime. The definition I know of is\n\nLet $R$ be a comm. ring with identity. An ideal P is prime iff $R \\neq P$ and whenever $bc \\in P$ then $b \\in P$ or $c \\in P$\n\nI dont know how to apply this definition to the example above.", "output": "I'm going to concentrate on commutative rings, because those are the ones closest to what you might be familiar with. To deal with the notions in noncommutative rings takes a bit more work, but is certainly doable.\nIdeals, at least at first, are meant to generalize the notion of \"is a multiple of\" (turns out that there is a different motivation for singling out ideals among the subrings, which is essentially the same reason for singling out normal subgroups among all subgroups of a group; this is not relevant right now, but you might be interested in taking a look at this answer about normal subgroups later). \nIf you consider the integers, you can characterize a number $n$ (up to sign) by describing all elements that are multiples of $n$. If you know exactly who the multiples of $n$ are, then you know exactly who $n$ is (except that you might confuse it with $-n$). So, instead of looking at the number $n$, we can look at the collection of all its multiples, \n$$n\\mathbb{Z} = (n) = \\{a\\in\\mathbb{Z}\\mid a\\text{ is a multiples of }n\\}.$$\nWhat properties do the collections of \"all multiples of a given number\" have? Well:\n\nThe collection always contains $0$.\nIf $a$ and $b$ are in the collection, so are $a+b$ and $a-b$.\nIf $a$ is in the collection, and $r$ is any element of the ring, then $ra$ is also in the collection.\n\nIn the integers, and also in many other rings (for example, $\\mathbb{R}[x]$, the polynomials over $\\mathbb{R}$), every collection that satisfies these three properties is in fact the collection of all multiples of some $a\\in R$. But there are other rings where this does not happen. For example, if you consider the ring $\\mathbb{Z}[x]$ of all polynomials with integer coefficients, you can take\n$$I = \\{ p(x)\\in\\mathbb{Z}[x]\\mid p(0)\\text{ is even}\\}.$$\nThis collection satisfies all three properties: $0$ is in $I$; if $p(x)$ and $q(x)$ are in $I$, then so is $p(x)+q(x)$, because $(p+q)(0) = p(0)+q(0)$ is a sum of two even numbers, hence even; and if $p(x)$ is in the collection and $q(x)$ is any polynomial with integer coefficients, when $pq(0) = p(0)q(0)$ is even, because $p(0)$ is even and $q(0)$ is an integer. So $I$ is an ideal. \nIs this $I$ the collection of \"all multiples of $a$\" for some $a\\in\\mathbb{Z}[x]$? No. If there were such an $a$, then since $2\\in I$, then $2$ would have to be a multiple of $a$. That means that $a$ must be a constant polynomial, and must be either $\\pm 1$ or $\\pm 2$ (the only elements of $\\mathbb{Z}[x]$ that divide $2$). It can't be either $1$ or $-1$, because \"multiples of $\\pm 1$\" is everything, and not everything is in our $I$. But $x\\in I$ as well, since evaluating $x$ at $0$ is even; and neither $2$ nor $-2$ divide $x$ in $\\mathbb{Z}[x]$. So even though $I$ is an ideal, it is not \"all multiples of\" someone. So the notion of \"ideal\", even though it starts up as \"all multiples of\" someone, is actually more general. This is the distinction between principal ideals (ideals which are \"all multiples of $a$\" for some $a$), and more general ideals (which need not be made up of \"all multiples of $a$\" for some $a$).\nNonetheless, ideals are closely connected to the notions of divisibility; as Dedekind noted when he introduced them in the 19th century, if you want to try to do \"modulo arithmetic\" as in the integers (working modulo $n$ is \"really\" working in $\\mathbb{Z}/(n)$) then the conditions you need on a collection are precisely the conditions that are needed to have ideals. That is, ideals are exactly the things for which you can do \"modulo arithmetic\". And modulo arithmetic is all about divisibility (after all, $a\\equiv b\\pmod{n}$ means that $n$ divides $a-b$). \nSo we want to also keep track of a few of the other special properties that some numbers have, and \"translate\" them into what they mean for ideals. \nPrime numbers play a major role in divisibility issues in the integers. How does the \"prime\" property translate into the setting of ideals? A prime is a number $p$ such that:\n\n$p\\neq \\pm 1$; and\nIf $p$ divides a product, then it divides at least one of the factors.\n\nOkay, how does that translate into ideals? If you think of an ideal $I$ as \"the collection of all multiples of some number\" (again, not really that in the general setting, but that's where the intuition and some of the definitions come from), then when do the multiples correspond to a prime? We need the prime not to divide everything; so we require the ideal to not be the entire ring, $I\\neq R$. And the second condition: if $ab$ is a multiple, then either $a$ or $b$ is a multiple. In other words: if $ab\\in I$, then either $a\\in I$ or $b\\in I$. So we define:\n\nAn ideal $I$ is a prime ideal if and only if $I\\neq R$, and whenever $ab\\in I$, either $a\\in I$ or $b\\in I$.\n\nGoing back to the intuition for ideals: what does the intersection of ideals correspond to? If $I$ is sort of like \"all multiples of $a$\", and $J$ is sort of like \"all multiples of $b$\", then what is $I\\cap J$? All things that are multiples of both $a$ and $b$!\nSo, if $P$ and $Q$ are both prime ideals, would $P\\cap Q$ be a prime ideal? Generally no: in general, you don't expect things that are multiples of two different primes to be themselves prime. And so you get to your example. $(2)$ is a prime ideal in $\\mathbb{Z}$, precisely because $2$ is a prime number: if $ab\\in(2)$, then $ab$ is a multiple of $2$, so either $a$ is a multiple of $2$ or $b$ is a multiple of $2$ (because $2$ is a prime number), so either $a\\in(2)$ or $b\\in(2)$. Similarly with $(3)$. But $(2)\\cap(3)$ will be all numbers that are multiples of both $2$ and $3$; this corresponds to \"all multiples of $6$\", as we know from elementary number theory: $(2)\\cap(3)=(6)$. But $6$ is not a prime number, so there is no reason to expect $(6)$ to be a prime ideal. In fact, a witness to the fact that $6$ is not a prime number should also work as a witness to the fact that $(6)$ is not a prime ideal. And indeed it does.\nCaveat. The analogy of ideals as \"set of all multiples of something\" works reasonably well in very familiar settings, but breaks down very quickly once you get beyond the most basic of rings. For instance, in the integers, you cannot have two nonzero prime ideals $(p)$ and $(q)$ with $p\\neq 0$, $q\\neq 0$, $p\\neq \\pm q$, and $(p)\\subseteq (q)$: that would mean that $p$ is a multiple of $q$, and with prime numbers that can only happen if $p=\\pm q$. But in other rings it is certainly possible for it to happen. For instance, in $R=\\mathbb{R}[x,y]$, the ring of polynomials in two variables, both\n\\begin{align*}\n(x) &= \\{ p(x,y)\\in R\\mid p(0,y) = 0\\text{ for all }y\\};\\\\\n(x,y) &= \\{ p(x,y)\\in R\\mid p(0,0) = 0\\}\n\\end{align*}\nare ideals; clearly $(x)\\subseteq (x,y)$, $(x)\\neq (0)$, $(x,y)\\neq (0)$, and $(x)\\neq (x,y)$. Yet both $(x)$ and $(x,y)$ are prime ideals. \nSo the analogy can only take you so far, and it can be misleading if you try to take it all the way. But at least at first you might find it a useful hook for thinking about possible examples and possible counterexamples.", "meta": {"post_id": 23790, "input_score": 8, "output_score": 34, "post_title": "Prime ideals: definition, verification, and examples"}}
{"input": "Could give some examples of nonnegative measurable function $f:\\mathbb{R}\\to[0,\\infty)$, such that its integral over any bounded interval is infinite?", "output": "The easiest example I know is constructed as follows. Let $q_{n}$ be an enumeration of the rational numbers in $[0,1]$. Consider $$g(x) = \\sum_{n=1}^{\\infty} 2^{-n} \\frac{1}{|x-q_{n}|^{1/2}}.$$\nSince each function  $\\dfrac{1}{|x-q_{n}|^{1/2}}$ is integrable on $[0,1]$, so is $g(x)$ [verify this!]. Therefore $g(x) < \\infty$ almost everywhere, so we can simply set $g(x) = 0$ in the points where the sum is infinite.\nOn the other hand, $f = g^{2}$ has infinite integral over each interval in $[0,1]$. Indeed, if $0 \\leq a \\lt b \\leq 1$ then $(a,b)$ contains a number $q_{n}$, so $$\\int_{a}^{b} f(x)\\,dx \\geq \\int_{a}^{b} \\frac{1}{|x-q_{n}|}\\,dx = \\infty.$$ Now in order to get the function $f$ defined at every point of $\\mathbb{R}$, simply define $f(n + x) = f(x)$ for $0 \\leq x \\lt 1$.", "meta": {"post_id": 24413, "input_score": 40, "output_score": 47, "post_title": "Is there a function with infinite integral on every interval?"}}
{"input": "I've just sat through several lectures that proved most of the results in Tate's thesis: the self-duality of the adeles, the construction of \"zeta functions\" by integration, and the proof of the functional equation. However, while I was able to follow at least some of the arguments in the individual steps,  I understand almost nothing about the big picture. My impression so far is that Tate invented a new and fancier way of proving the functional equation that the Hecke analytic approach. But is there more to the story than \"this is a neat way of proving something already known\"?\nI'm under the impression that Tate's thesis laid the foundations for the Langlands program, but I don't understand this properly yet. \nCan someone explain to me what's the real significance and meaning of Tate's thesis?", "output": "Tate's thesis introduces the concept, ubiquitous now, of doing analysis, and especially Fourier analysis, on the locally compact ring of adeles.    In this setting, the discrete subgroup $\\mathbb Z \\subset \\mathbb R$ is replaced\nby the discrete subgroup $\\mathbb Q \\subset \\mathbb A$.\nThis has a number of implications, some of which are:\n\n$\\mathbb Q$ is a field, and $\\mathbb A$ is essentially a product of fields.\nIt is technically almost always easier to work with fields rather than more general rings (such as $\\mathbb Z$).  The adelic formalism allows one to have one's cake and eat it too (in some sense): one is working with the field $\\mathbb Q$, not the ring $\\mathbb Z$, but the primes are still present, in the factorization of $\\mathbb A$ as a product.  (And this product structure of \n$\\mathbb A$, which is formally very simple, captures in some subtle way the\ndeeper sense of \"product\" in the statement of the fundamental theorem of arithmetic, i.e. that any natural number is a unique product of prime powers.)\nTate writes zeta-functions, or more generally, Hecke $L$-series, as integrals over $\\mathbb A^{\\times}$.  The Euler product structure of the $L$-series then becomes simply a factorization of this integral according to the product structure of $\\mathbb A^{\\times}$.  (This is a manifestation of the parenthetical remark at the end of point (1).)\nThe proof of the functional equation becomes (more-or-less) just an application of Poisson summation (in the adelic context).  \nIt is worth comparing this with the classical proof (which one can read in Lang's book, among other places, if memory serves).   Classically, one takes\nthe sum over ideals representation of the $L$-function, and decomposes it first\ninto a finite collection of sums, indexed by the ideal class group, each sum taking place over all the integral ideals in a given ideal class.  These individual series are then described as Mellin transforms of theta series, \nand the functional equation is derived from the transformation properties of the\ntheta series, the latter being proved by an application of Poisson summation in the classical setting.   \nOnce one unpacks all the details, Tate's proof and Hecke's proof don't look so different; but the difference in packaging is enormous!  In Tate's approach there is no need to unpack everything (for example, the ideal class group is just lurking around in the background implicitly, and there is no need to bring it out explicitly), while in the classical arguments such unpacking is key to the whole thing.   \nAs another example of the conceptual clarity and simplification that Tate's approach gives, you might consider the way he derives the formula for the residue at $s = 1$ of the zeta function of a number field (i.e. the general class number formula) and compare it with the classical derivation.\nWorking in the case of a function field over a finite field, Tate derives the Riemann--Roch formula (in the form $\\dim H^0(C,\\mathcal O(D)) - \\dim\nH^0(C,\\mathcal O(K - D)) = 1 + \\deg D - g$) as a straightforward consequence of Poisson summation.  Among other things, this provides a rather striking unification of (what we now call) Serre duality and Fourier duality.  (Although I don't know the precise history, this probably has antecedents in the literature: the original proof of the functional equation of the $\\zeta$-function for a curve over a finite field, by  Schmidt, proceeded by applying Riemann--Roch; so Tate is essentially reversing this argument.)\nTate's explication of the functional equation of $L$-series in terms of local functional equations shows that the global root number --- i.e. the constant that appears in the functional equation --- is a product of local numbers.  As far as I understand, this wasn't known (and perhaps not even suspected) prior to Tate's proof.  \nThis may seem slightly esoteric, but experience shows that one should regard global root numbers, and their factorization into a product of local root numbers (or $\\epsilon$-factors), to be of essentially equal importance to global $L$-series, and their (Euler product) factorization into local $L$-factors.\n\nSummary/Conclusion: The aim of the above list is just to highlight some of the points to watch out for while studying Tate's thesis.  Let me now make some remarks at a more general level.\nIn the classical theory of zeta and $L$-functions, there is a tension between the analytic tools, which are essentially additive Fourier theory (e.g. Poissson summation) and the multiplicative aspects of the theory (exemplified by the Euler product).  Tate's thesis resolves these tensions by moving to the adelic context.\nIn the general theory of automorphic forms (say on a quotient $\\Gamma\n\\backslash G(\\mathbb R)$) for some congruence subgroup $\\Gamma$ of the integral points $G(\\mathbb Z)$ of a semi-simple or reductive Lie group $G(\\mathbb R)$) there is the same tension between the harmonic analysis and Lie theory (which $\\Gamma \\backslash G(\\mathbb R)$ is well set-up to accommodate) and the theory of Hecke operators (which pertain to the finite primes, which are not particular visible in this classical description), which is resolved by moving to the adelic picture $G(\\mathbb Q)\\backslash G(\\mathbb A)$.\nAnother thing to bear in mind is that the theories of $L$-series and of automorphic forms are quite technical in nature, and so conceptual and aesthetic simplifications (as in Tate's thesis) go hand in hand with technical simplifications.  (See e.g. points (1) and (3) above.)  One instance of this in the automorphic forms context is that conjugacy classes in $G(\\mathbb Q)$ are much easier to understand than in a congruence subgroup $\\Gamma$ of $G(\\mathbb Z)$.  (Another instance of the technical superiority of fields over more general rings.)  One might also consider the Tamagawa number one theorem, which gives an elegant reformulation and generalization of a myriad of classical results.\nSo, to finish, Tate's thesis is significant because it improves the classical point of view in a number of ways, achieving conceptual, technical, and aesthetic simplifications.  At the same time, it suggests a way of unifying harmonic analytic and arithmetic considerations in the general context of automorphic forms, by working in the adelic context.\nFinally, I strongly suggest working through the details of Tate's thesis in the particular case of the Riemann zeta function, and seeing how his arguments and construction compare with the classical ones.  If you haven't already done this, it should be quite enlightening.  (In particular, it will illuminate points (1), (2), and (3) above.)", "meta": {"post_id": 25090, "input_score": 162, "output_score": 199, "post_title": "What's the significance of Tate's thesis?"}}
{"input": "My son was busily memorizing digits of $\\pi$ when he asked if any power of $\\pi$ was an integer.  I told him: $\\pi$ is transcendental, so no non-zero integer power can be an integer.\nAfter tiring of memorizing $\\pi$, he resolved to discover a new irrational whose expansion is easier to memorize.  He invented (probably re-invented) the number $J$:\n$$J = 6.12345678910111213141516171819202122\\ldots$$\nwhich clearly lets you name as many digits as you like pretty easily.  He asked me if $J$ is transcendental just like $\\pi$, and I said it must be but I didn't know for sure. Is there an easy way to determine this?  \nI can show that $\\pi$ is transcendental (using Lindemann-Weierstrass) but it doesn't work for arbitrary numbers like $J$, I don't think.", "output": "This is a transcendental number, in fact one of the best known ones, it is $6+$ Champernowne's number.\nKurt Mahler was first to show that the number is transcendental, a proof can be found on his \"Lectures on Diophantine approximations\", available through Project Euclid. The argument (as typical in this area) consists in analyzing the rate at which rational numbers can approximate the constant (see the section on \"Approximation by rational numbers: Liouville to Roth\" in the Wikipedia entry for Transcendence theory).\nAn excellent book to learn about proofs of transcendence is \"Making transcendence transparent: an intuitive approach to classical transcendental number theory\", by Edward Burger and Robert Tubbs.", "meta": {"post_id": 25205, "input_score": 58, "output_score": 47, "post_title": "Is $6.12345678910111213141516171819202122\\ldots$ transcendental?"}}
{"input": "Let $A \\in \\mathbb{R}^{n\\times n}$ be a real symmetric matrix. Please help me clear up some confusion about the relationship between the singular value decomposition of $A$ and the eigen-decomposition of $A$. \nLet $A = U\\Sigma V^T$ be the SVD of $A$. Since $A = A^T$, we have $AA^T = A^TA = A^2$ and:\n$$A^2 = AA^T = U\\Sigma V^T V \\Sigma U^T = U\\Sigma^2 U^T$$\n$$A^2 = A^TA = V\\Sigma U^T U\\Sigma V^T = V\\Sigma^2 V^T$$\nBoth of these are eigen-decompositions of $A^2$. Now consider some eigen-decomposition of $A$\n$$A = W\\Lambda W^T$$\nThen\n$$A^2 = W\\Lambda W^T W\\Lambda W^T = W\\Lambda^2 W^T$$\nSo $W$ also can be used to perform an eigen-decomposition of $A^2$. \nSo now my confusion:\nIt seems that $A = W\\Lambda W^T$ is also a singular value decomposition of A. But singular values are always non-negative, and eigenvalues can be negative, so something must be wrong.\nWhat is going on?", "output": "If $A = U \\Sigma V^T$ and $A$ is symmetric, then $V$ is almost $U$ except for the signs of columns of $V$ and $U$.\n$$A = W \\Lambda W^T = \\displaystyle \\sum_{i=1}^n w_i \\lambda_i w_i^T = \\sum_{i=1}^n w_i \\left| \\lambda_i \\right| \\text{sign}(\\lambda_i) w_i^T$$ where $w_i$ are the columns of the matrix $W$.\nThe left singular vectors $u_i$ are $w_i$ and the right singular vectors $v_i$ are $\\text{sign}(\\lambda_i) w_i$. (You can of course put the sign term with the left singular vectors as well.)The singular values $\\sigma_i$ are the magnitude of the eigen values $\\lambda_i$.\nHence, $A = U \\Sigma V^T = W \\Lambda W^T$\nand $$A^2 = U \\Sigma^2 U^T = V \\Sigma^2 V^T = W \\Lambda^2 W^T$$\nNote that the eigenvalues of $A^2$ are positive.", "meta": {"post_id": 28036, "input_score": 41, "output_score": 35, "post_title": "Relationship between eigendecomposition and singular value decomposition"}}
{"input": "A classical exercise in group theory is \"Show that if a group has a trivial automorphism group, then it is of order $1$ or $2$.\" I think that the straightforward solution uses that a exponent two group is a vector space over $\\operatorname{GF}(2)$, and therefore has nontrivial automorphisms as soon as its dimension is at least $2$ (simply transposing two basis vectors).\nMy question is now natural:\n\nIs it possible, without the axiom of choice, to construct a vector space $E$ over $\\operatorname{GF}(2)$, different from $\\{0\\}$ or $\\operatorname{GF}(2)$, whose automorphism group $\\operatorname{GL}(E)$ is trivial?", "output": "Nov. 6th, 2011 After several long months a post on MathOverflow pushed me to reconsider this math, and I have found a mistake. The claim was still true, as shown by L\u00e4uchli $\\small[1]$, however despite trying to do my best to understand the argument for this specific claim, it eluded me for several days. I then proceeded to construct my own proof, this time errors free - or so I hope. While at it, I am revising the writing style.\nJul. 21st, 2012 While reviewing this proof again it was apparent that its most prominent use in generating such space over the field of two elements fails, as the third lemma implicitly assumed $x+x\\neq x$. Now this has been corrected and the proof is truly complete.\n$\\newcommand{\\sym}{\\operatorname{sym}}\n\\newcommand{\\fix}{\\operatorname{fix}}\n\\newcommand{\\span}{\\operatorname{span}}\n\\newcommand{\\im}{\\operatorname{Im}}\n\\newcommand{\\Id}{\\operatorname{Id}}\n$\n\nI got it! The answer is that you can construct such vector space.\nI will assume that you are familiar with ZFA and the construction of permutation models, references can be found in Jech's Set Theory $\\small[2, \\text{Ch}. 15]$ as well The Axiom of Choice $\\small{[3]}$. Any questions are welcomed.\nSome notations, if $x\\in V$ which is assumed to be a model of ZFC+Atoms then:\n\n$\\sym(x) =\\{\\pi\\in\\mathscr{G} \\mid \\pi x = x\\}$, and\n$\\fix(x) = \\{\\pi\\in\\mathscr{G} \\mid \\forall y\\in x:\\ \\pi y = y\\}$\n\nDefinition: Suppose $G$ is a group, $\\mathcal{F}\\subseteq\\mathcal{P}(G)$ is a normal subgroups filter if:\n\n$G\\in\\mathcal{F}$;\n$H,K$ are subgroups of $G$ such that $H\\subseteq K$, then $H\\in\\mathcal{F}$ implies $K\\in\\mathcal{F}$;\n$H,K$ are subgroups of $G$ such that $H,K\\in\\mathcal{F}$ then $H\\cap K\\in\\mathcal{F}$; \n${1}\\notin\\mathcal{F}$ (non-triviality);\nFor every $H\\in\\mathcal{F}$ and $g\\in G$ then $g^{-1}Hg\\in\\mathcal{F}$ (normality).\n\nNow consider the normal subgroups-filter $\\mathcal{F}$ to be generated by the subgroups $\\fix(E)$ for $E\\in I$, where $I$ is an ideal of sets of atoms (closed under finite unions, intersections and subsets).\nBasics of permutation models: \nA permutation model is a transitive subclass of the universe $V$ that for every ordinal $\\alpha$, we have $x\\in\\mathfrak{U}\\cap V_{\\alpha+1}$ if and only if $x\\subseteq\\mathfrak{U}\\cap V_\\alpha$ and $\\sym(x)\\in\\mathcal{F}$. \nThe latter property is known as being symmetric (with respect to $\\mathcal{F}$) and $x$ being in the permutation model means that $x$ is hereditarily symmetric. (Of course at limit stages take limits, and start with the empty set)\nIf $\\mathcal{F}$ was generated by some ideal of sets $I$, then if $x$ is symmetric with respect to $\\mathcal{F}$ it means that for some $E\\in I$ we have $\\fix(E)\\subseteq\\sym(x)$. In this case we say that $E$ is a support of $x$.\nNote that if $E$ is a support of $x$ and $E\\subseteq E'$ then $E'$ is also a support of $x$, since $\\fix(E')\\subseteq\\fix(E)$.\nLastly if $f$ is a function in $\\mathfrak{U}$ and $\\pi$ is a permutation in $G$ then $\\pi(f(x)) = (\\pi f)(\\pi x)$.\n\nStart with $V$ a model of ZFC+Atoms, assuming there are infinitely (countably should be enough) many atoms. $A$ is the set of atoms, endow it with operations that make it a vector space over a field $\\mathbb{F}$ (If we only assume countably many atoms, we should assume the field is countable too. Since we are interested in $\\mathbb F_2$ this assertion is not a big hassle). Now consider $\\mathscr{G}$ the group of all linear automorphisms of $A$, each can be extended uniquely to an automorphism of $V$.\nNow consider the normal subgroups-filter $\\mathcal{F}$ to be generated by the subgroups $\\fix(E)$ for $E\\in I$, where $E$ a finite set of atoms. Note that since all the permutations are linear they extend unique to $\\span(E)$. In the case where $\\mathbb F$, our field, is finite then so is this span.\nLet $\\mathfrak{U}$ be the permutation model generated by $\\mathscr{G}$ and $\\mathcal{F}$. \nLemma I: Suppose $E$ is a finite set, and $u,v$ are two vectors such that $v\\notin\\span(E\\cup\\{u\\})$ and $u\\notin\\span(E\\cup\\{v\\})$ (in which case we say that $u$ and $v$ are linearly independent over $E$), then there is a permutation which fixes $E$ and permutes $u$ with $v$.\nProof: Without loss of generality we can assume that $E$ is linearly independent, otherwise take a subset of $E$ which is. Since $E\\cup\\{u,v\\}$ is linearly independent we can (in $V$) extend it to a base of $A$, and define a permutation of this base which fixes $E$, permutes $u$ and $v$. This extends uniquely to a linear permutation $\\pi\\in\\fix(E)$ as needed. $\\square$\nLemma II: In $\\mathfrak{U}$, $A$ is a vector space over $\\mathbb F$, and if $W\\in\\mathfrak{U}$ is a linear proper subspace then $W$ has a finite dimension.\nProof: Suppose $W$ is as above, let $E$ be a support of $W$. If $W\\subseteq\\span(E)$ then we are done. Otherwise take $u\\notin W\\cup \\span(E)$ and $v\\in W\\setminus \\span(E)$ and permute $u$ and $v$ while fixing $E$, denote the linear permutation with $\\pi$. It is clear that $\\pi\\in\\fix(E)$ but $\\pi(W)\\neq W$, in contradiction. $\\square$\nLemma III: If $T\\in\\mathfrak{U}$ is a linear endomorphism of $A$, and $E$ is a support of $T$ then $x\\in\\span(E)\\Leftrightarrow Tx\\in\\span(E)$, or $Tx=0$.\nProof: First for $x\\in \\span(E)$, if $Tx\\notin\\span(E)$ for some $Tx\\neq u\\notin\\span(E)$ let $\\pi$ be a linear automorphism of $A$ which fixes $E$ and $\\pi(Tx)=u$. We have, if so:\n$$u=\\pi(Tx)=(\\pi T)(\\pi x) = Tx\\neq u$$\nOn the other hand, if $x\\notin\\span(E)$ and $Tx\\in\\span(E)$ and if $Tx=Tu$ for some $x\\neq u$ for $u\\notin\\span(E)$, in which case we have that $x+u\\neq x$ set $\\pi$ an automorphism which fixes $E$ and $\\pi(x)=x+u$, now we have: $$Tx = \\pi(Tx) = (\\pi T)(\\pi x) = T(x+u) = Tx+Tu$$ Therefore $Tx=0$. \nOtherwise for all $u\\neq x$ we have $Tu\\neq Tx$. Let $\\pi$ be an automorphism fixing $E$ such that $\\pi(x)=u$ for some $u\\notin\\span(E)$, and we have: $$Tx=\\pi(Tx)=(\\pi T)(\\pi x) = Tu$$ this is a contradiction, so this case is impossible.  $\\square$\nTheorem: if $T\\in\\mathfrak{U}$ is an endomorphism of $A$ then for some $\\lambda\\in\\mathbb F$ we have $Tx=\\lambda x$ for all $x\\in A$.\nProof:\nAssume that $T\\neq 0$, so it has a nontrivial image. Let $E$ be a support of $T$. If $\\ker(T)$ is nontrivial then it is a proper subspace, thus for a finite set of atoms $B$ we have $\\span(B)=\\ker(T)$. Without loss of generality, $B\\subseteq E$, otherwise $E\\cup B$ is also a support of $T$.\nFor every $v\\notin\\span(E)$ we have $Tv\\notin\\span(E)$. However, $E_v = E\\cup\\{v\\}$ is also a support of $T$. Therefore restricting $T$ to $E_v$ yields that $Tv=\\lambda v$ for some $\\lambda\\in\\mathbb F$.\nLet $v,u\\notin\\span(E)$ linearly independent over $\\span(E)$. We have that: $Tu=\\alpha u, Tv=\\mu v$, and $v+u\\notin\\span(E)$ so $T(v+u)=\\lambda(v+u)$, for $\\lambda\\in\\mathbb F$.\n$$\\begin{align}\n0&=T(0) \\\\ &= T(u+v-u-v)\\\\\n&=T(u+v)-Tu-Tv \\\\ &=\\lambda(u+v)-\\alpha u-\\mu v=(\\lambda-\\alpha)u+(\\lambda-\\mu)v\n\\end{align}$$ Since $u,v$ are linearly independent we have $\\alpha=\\lambda=\\mu$. Due to the fact that for every $u,v\\notin\\span(E)$ we can find $x$ which is linearly independent over $\\span(E)$ both with $u$ and $v$ we can conclude that for $x\\notin E$ we have $Tx=\\lambda x$. \nFor $v\\in\\span(E)$ let $x\\notin\\span(E)$, we have that $v+x\\notin\\span(E)$ and therefore:\n$$\\begin{align}\nTx &= T(x+u - u)\\\\\n&=T(x+u)-T(u)\\\\\n&=\\lambda(x+u)-\\lambda u = \\lambda x\n\\end{align}$$\nWe have concluded, if so that $T=\\lambda x$ for some $\\lambda\\in\\mathbb F$. $\\square$\n\nSet $\\mathbb F=\\mathbb F_2$ the field with two elements and we have created ourselves a vector space without any nontrivial automorphisms. However, one last problem remains. This construction was carried out in ZF+Atoms, while we want to have it without atoms. For this simply use the Jech-Sochor embedding theorem $\\small[3, \\text{Th}. 6.1, \\text p. 85]$, and by setting $\\alpha>4$ it should be that any endomorphism is transferred to the model of ZF created by this theorem.\n(Many thanks to t.b. which helped me translating parts of the original paper of L\u00e4uchli.\nAdditional thanks to Uri Abraham for noting that an operator need not be injective in order to be surjective, resulting a shorter proof.)\n\nBibliography\n\nL\u00e4uchli, H. Auswahlaxiom in der Algebra. Commentarii Mathematici Helvetici, vol 37, pp. 1-19.\nJech, T. Set Theory, 3rd millennium ed., Springer (2003).\nJech, T. The Axiom of Choice. North-Holland (1973).", "meta": {"post_id": 28145, "input_score": 59, "output_score": 44, "post_title": "Axiom of choice and automorphisms of vector spaces"}}
{"input": "$X \\sim \\mathcal{N}(0,1)$, then to show that for $x > 0$,\n$$\r\n\\mathbb{P}(X>x) \\leq \\frac{\\exp(-x^2/2)}{x \\sqrt{2 \\pi}} \\>.\r\n$$", "output": "Integrating by parts,\n$$\\begin{align*}\nQ(x) &= \\int_x^{\\infty} \\phi(t)\\mathrm dt = \\int_x^{\\infty} \\frac{1}{\\sqrt{2\\pi}}\\exp(-t^2/2) \\mathrm dt\\\\\n&= \\int_x^{\\infty} \\frac{1}{t} \\frac{1}{\\sqrt{2\\pi}}t\\cdot\\exp(-t^2/2) \\mathrm dt\\\\\n&= - \\frac{1}{t}\\frac{1}{\\sqrt{2\\pi}}\\exp(-t^2/2)\\biggr\\vert_x^\\infty \n- \\int_x^{\\infty} \\left( - \\frac{1}{t^2} \\right ) \\left ( - \\frac{1}{\\sqrt{2\\pi}} \\exp(-t^2/2) \\right )\\mathrm dt\\\\\n&= \\frac{\\phi(x)}{x} - \\int_x^{\\infty} \\frac{\\phi(t)}{t^2} \\mathrm dt.\n\\end{align*}\n$$\nThe integral on the last line above has a positive integrand and so \nmust have positive value.  Therefore we have that \n$$\nQ(x) < \\frac{\\phi(x)}{x} = \\frac{\\exp(-x^2/2)}{x\\sqrt{2\\pi}}~~ \\text{for}~~ x > 0.\n$$\nThis argument is more complicated than @cardinal's elegant proof of the\nsame result.  However, note that by repeating the above trick of \nintegrating by parts and the \nargument about the value of an integral with positive integrand, we get that\n$$\nQ(x) > \\phi(x) \\left (\\frac{1}{x} - \\frac{1}{x^3}\\right ) = \\frac{\\exp(-x^2/2)}{\\sqrt{2\\pi}}\\left (\\frac{1}{x} - \\frac{1}{x^3}\\right )~~ \\text{for}~~ x > 0.\n$$\nIn fact, for large values of $x$, a sequence of increasingly tighter upper and lower bounds can be developed via this argument. Unfortunately all the bounds diverge to $\\pm \\infty$ as $x \\to 0$.", "meta": {"post_id": 28751, "input_score": 44, "output_score": 41, "post_title": "Proof of upper-tail inequality for standard normal distribution"}}
{"input": "How I can compute cohomology de Rham of the projective plane $P^{2}(\\mathbb{R})$ using Mayer vietoris or any other methods?", "output": "If you remove a point from $P^2$ you are left with something which looks like a Moebius band. You can use this to compute $H^\\bullet(P^2)$.\nLet $p\\in P^2$, let $U$ be a small open neighborhood of $p$ in $P^2$ diffeomorphic to an open disc centered at $p$, and let $V=P^2\\setminus\\{p\\}$. Now use Mayer-Vietoris.\nThe cohomology of $U$ you know. The open set $V$ is diffeomorphic to an open moebious band, so that tells you the cohomology; alternatively, you can check that it deformation-retracts to the $P^1\\subseteq P^2$ consiting of all lines orthogonal to the line corresponding to $p$ (with respect to any inner product in the vector space $\\mathbb R^3$ you used to construct $P^2$), and the intersection $U\\cap V$ has also the homotopy type of a circle. The maps in the M-V long exact sequence are not hard to make explicit; it does help to keep in mind the geometric interpretation of $U$ and $V$.\nLater: alternatively, one can do a bit of magic. Since there is a covering $S^2\\to P^2$ with $2$ sheets, we know that the Euler characteristics of $S^2$ and $P^2$ are related by $\\chi(S^2)=2\\chi(P^2)$. Since $\\chi(S^2)=2$, we conclude that $\\chi(P^2)=1$. Since $P^2$ is of dimension $2$, we have $\\dim H^p(P^2)=0$ if $p>2$; since $P^2$ is non-orientable, $H^2(P^2)=0$; finally, since $P^2$ is connected, $H^0(P^2)\\cong\\mathbb R$. It follows that $1=\\chi(P^2)=\\dim H^0(P^2)-\\dim H^1(P^2)=1-\\dim H^1(P^2)$, so that $H^1(P^2)=0$. \nEven later: if one is willing to use magic, there is lot of fun one can have. For example: if a finite group $G$ acts properly discontinuously on a manifold $M$, then the cohomology of the quotient $M/G$ is the subset $H^\\bullet(M)^G$ of the cohomology $H^\\bullet(M)$ fixed by the natural action of $G$. In this case, if we set $M=S^2$, $G=\\mathbb Z_2$ acting on $M$ so that the non-identity element is the antipodal map, so that $M/G=P^2$: we get that $H^\\bullet(P^2)=H^\\bullet(S^2)^G$.\nWe have to compute the fixed spaces: \n\n$H^0(S^2)$ is one dimensional, spanned by the constant function $1$, which is obviously fixed by $G$, so $H^0(P^2)\\cong H^0(S^2)^G=H^0(S^2)=\\mathbb R$. \nOn the other hand, $H^2(S^2)\\cong\\mathbb R$, spanned by any volume form on the sphere; since the action of the non-trivial element of $G$ reverses the orientation, we see that it acts as multiplication by $-1$ on $H^2(S^2)$ and therefore $H^2(P^2)\\cong H^2(S^2)^G=0$. \nFinally, if $p\\not\\in\\{0,2\\}$, then $H^p(S^2)=0$, so that obviously $H^p(P^2)\\cong H^p(S^2)^G=0$.\n\nLuckily, this agrees with the previous two computations.", "meta": {"post_id": 29404, "input_score": 30, "output_score": 43, "post_title": "Cohomology of projective plane"}}
{"input": "I came across the following definition:\nGiven a ring $A$, with a unit $1 \\in A$, and $A$-modules $M$ and $N$, we denote by $Hom(M, N)$ or $Hom_A(M, N)$ the space of $A$-linear maps from $M$ to $N$.\nMy question is: what exactly is the difference between homomorphism and a linear map? I can see that linearity is defined in terms of a vector space or module and homomorphism in terms of groups.\nBut every linear map is a homomorphism and when treating a group as a one dimensional vector space over itself, every homo. is also a linear map. This makes me think they are kind of the same.\nIs it ok to think of it that way? Or am I confused? Because I feel confused. Thanks once again for your help!", "output": "\"Homomorphism\" comes from the greek homo (same) and morphus (form or shape).\nSo a \"homomorphism\" is a map that \"preserves the shape\" or \"preserves the structure.\"\n\nIf you are working with groups, you want $f\\colon G\\to H$ to preserve the group structure: identity, inverses, and products. So a homomorphism is a map $f$ such that $f(1)=1$, $f(a^{-1}) = (f(a))^{-1}$, and $f(ab) = f(a)f(b)$ (though it turns out that the latter is enough to guarantee all of them, so we only check the latter). \nIf you are working with rings, you want $f\\colon R\\to S$ to preserve the ring structure (addition and multiplication; if the rings have unity, then you want it to preserve unity). So you want $f(a+b) = f(a)+f(b)$, $f(ab)=f(a)f(b)$ (and if both rings have unity, you often want $f(1_R) = 1_S$).\nIf you are working with partially ordered sets, you want $f$ to preserve the order structure. So you want that if $a\\leq b$, then $f(a)\\leq f(b)$.\nIf you are working with graphs, you want the homomorphisms to preserve the graph structure, which is adjacency: if $v$ is adjacent to $w$, you want $f(v)$ to be adjacent to $f(w)$.\nIf you are working with topological spaces, you want homomorphisms to preserve the topological space structure; it turns out that the way to do this is to ask that the inverse image of an open set be open.\nIf you are working with \"pointed sets\" (sets with a distinguished object), then you want a homomorphism $f\\colon S\\to T$ to \"preserve the structure\", so you require it to map the distinguished object of $S$ to the distinguished object of $T$.\nAnd if you are working with vector spaces over a field $F$, you want a homomorphism $f\\colon V\\to W$ to \"preserve the vector space structure\"; so you want it to preserve the additive structure, $f(x+y) = f(x)+f(y)$; and the scalar multiplication structure, $f(av) = af(v)$. \nSimilarly, if you are working with $R$-modules, a homomorphism will be a map $f\\colon M\\to N$ that preserves \"the $R$-module structure\", $f(m+m') = f(m)+f(m')$ and $f(rm) = rf(m)$.\n\nSo the meaning of \"homomorphism\" will depend on the context. It is often clear. If I say \"Let $G$ and $H$ be groups, and let $f\\colon G\\to H$ be a homomorphism\", then it's pretty clear I'm talking about a group homomorphism. \nBut sometimes it isn't clear. What if I say \"Let $f\\colon\\mathbb{Z}\\to\\mathbb{R}$ be a homomorphism\"? Am I talking about a homomorphism of additive groups, or a homomorphism of rings? How about \"$f\\colon\\mathbb{R}\\to\\mathbb{C}$\"? Am I talking about additive groups, rings, topological spaces,  $\\mathbb{R}$-vector spaces, $\\mathbb{Q}$-vector spaces, inner product spaces? Which?\nSo we often specify what kind of homomorphism we mean. This is especially important when a particular set has many different structures (such as $\\mathbb{R}$, which is an additive group, a field, a vector space over $\\mathbb{Q}$, a vector space over $\\mathbb{R}$, etc). So we will say things like \"let $f\\colon M\\to N$ be an $R$-module homomorphism\", or \"let $f\\colon\\mathbb{R}\\to\\mathbb{C}$ be an additive homomorphism\" to specify which kind we are thinking about.\nAnd, historically, some terminology precedes the generic \"homomorphism.\" Homomorphisms of vector spaces have long been called \"linear transformations\", so we often call them that instead of \"vector space homomorphism\". When a vector space has several structures as a vector space (e.g., $\\mathbb{C}^2$ can be thought of as a complex vector space or as a real vector space), we often specify the field, so we may say things like \"let $f\\colon\\mathbb{C}^2\\to\\mathbb{C}$ be an $\\mathbb{R}$-linear transformation\" or just \"$\\mathbb{R}$-linear\", to specify we are looking at the structure as a real vector space.\nBecause modules are a direct generalization of vector spaces, we often say \"$R$-linear function\" or \"$R$-linear\" to refer to homomorphisms of $R$-modules, by analogy to $\\mathbb{R}$-linear or $\\mathbb{C}$-linear for homomorphisms of real or complex vector spaces. Note that a module over a field is the same thing as a vector space.", "meta": {"post_id": 29944, "input_score": 28, "output_score": 72, "post_title": "Difference between linear map and homomorphism"}}
{"input": "I know that different people follow different conventions, but whenever I see $\\arcsin(x)$ written as $\\sin^{-1}(x)$, I find myself thinking it wrong, since $\\sin^{-1}(x)$ should be $\\csc(x)$, and not possibly confused with another function.\nDoes anyone say it's bad practice to write $\\sin^{-1}(x)$ for $\\arcsin(x)$?", "output": "The notation for trigonometric functions is \"traditional\", which is to say that it is not the way we would invent notation today.\n\n$\\sin^{-1}(x)$ means the inverse sine, as you mentioned, rather than a reciprocal.  So $\\sin^{-1}(x)$ is not an abbreviation for $(\\sin(x))^{-1}$. Instead it's notation for $(\\sin^{-1})(x)$, in the same way that $f^{-1}(x)$ means the inverse function of $f$, applied to $x$. \nBut $\\sin^2(x)$ means $(\\sin(x))^2$, rather than $\\sin(\\sin(x))$. In other contexts, like dynamical systems, if I have a function $f$, the notation $f^2$ means $f \\circ f$. This is compatible with the $f^{-1}$ notation, if we take juxtaposition of functions to mean composition: $f^{-1}f^{3}$ will be $f^{2}$ as desired. \n\nSo the traditional notation for sine is actually a mixture of two different systems: $-1$ denotes an inverse, not a power, while positive integer exponents denote powers, not iterated compositions. \nThis is simply a fact of life, like an irregular conjugation of a verb. As with other languages, the things that we use most often are the ones that are likely to remain irregular. That doesn't mean that they are incorrect, however, as long as other speakers of the language know what they mean. \nMoreover, if you wanted to reform the system, there would be an equally strong argument for changing $\\sin^2$ to mean $\\sin \\circ \\sin$. This is already slowly happening with $\\log$; I think that the usage of $\\log^2(x)$ to mean $(\\log(x))^2$ is slowly decreasing, because people tend to confuse it with $\\log(\\log(x))$. That confusion is less likely with $\\sin$ because $\\sin(\\sin(x))$ arises so rarely in practice, unlike $\\log(\\log(x))$.", "meta": {"post_id": 30317, "input_score": 21, "output_score": 39, "post_title": "$\\arcsin$ written as $\\sin^{-1}(x)$"}}
{"input": "I need to find a way of proving that the square roots of a finite set \n  of different primes are linearly independent over the field of \n  rationals. \n\nI've tried to solve the problem using elementary algebra \nand also using the theory of field extensions, without success. To \nprove linear independence of two primes is easy but then my problems \narise. I would be very thankful for an answer to this question.", "output": "Assume that there was some linear dependence relation of the form\n$$ \\sum_{k=1}^n c_k \\sqrt{p_k} + c_0 = 0 $$\nwhere $ c_k \\in \\mathbb{Q} $ and the $ p_k $ are distinct prime numbers. Let $ L $ be the smallest extension of $ \\mathbb{Q} $ containing all of the $ \\sqrt{p_k} $. We argue using the field trace $ T =  T_{L/\\mathbb{Q}} $. First, note that if $ d \\in \\mathbb{N} $ is not a perfect square, we have that $ T(\\sqrt{d}) = 0 $. This is because $ L/\\mathbb{Q} $ is Galois, and $ \\sqrt{d} $ cannot be a fixed point of the action of the Galois group as it is not rational. This means that half of the Galois group maps it to its other conjugate $ -\\sqrt{d} $, and therefore the sum of all conjugates cancel out. Furthermore, note that we have $ T(q) = 0 $ iff $ q = 0 $ for rational $ q $.\nTaking traces on both sides we immediately find that $ c_0 = 0 $. Let $ 1 \\leq j \\leq n $ and multiply both sides by $ \\sqrt{p_j} $ to get\n$$ c_j p_j + \\sum_{1 \\leq k \\leq n, k\\neq j} c_k \\sqrt{p_k p_j} = 0$$\nNow, taking traces annihilates the second term entirely and we are left with $ T(c_j p_j) = 0 $, which implies $ c_j = 0 $. Since $ j $ was arbitrary, we conclude that all coefficients are zero, proving linear independence.", "meta": {"post_id": 30687, "input_score": 163, "output_score": 40, "post_title": "The square roots of different primes are linearly independent over the field of rationals"}}
{"input": "Let $f(\\bar x)$ be a multivariable polynomial with integer coefficients.\nThe zeros of that polynomial are in bijection with the homomorphisms $\\mathbb Z[\\bar x] \\rightarrow \\mathbb Z$ that factor through $\\mathbb{Z}[\\bar x]/(f)$.\nAs I understand it this viewpoint leads to the contrafunctor $\\text{Spec}$ and schemes and such.\nCan you show any concrete examples of Diophantine equations that we can solve using this viewpoint?", "output": "As far as I know, the first Diophantine problem (over a number field) that was solved using Spec and other tools of algebraic geometry was the following result (proved by Mazur and Tate in a paper from Inventiones in the early 1970s):\n\nIf $E$ is an elliptic curve over $\\mathbb Q$, then $E$ has no rational point of order 13.\n\nThe proof as it's written uses quite a bit more than you can learn just from reading Hartshorne; I don't know if there is any way to significantly simplify it.  [Added: Rereading the first page of the Mazur--Tate paper, I see that they\nrefer to another proof of this fact by Blass, which I've never read, but which\nseems likely to be of a more classical nature.]\nThere is another result, which goes back to Billing and Mahler, of the same nature:\n\nIf $E$ is an elliptic curve over $\\mathbb Q$, then $E$ has no rational point\nof order $11$.\n\nThis was proved by elementary (if somewhat complicated) arguments.  An analogous\nresult with $11$ replaced by $17$ was\nproved by Ogg again\nusing elementary arguments.\nThese results were all generalized by Mazur (in the mid 1970s) as follows:\n\nIf $E$ is an elliptic curve over $\\mathbb Q$, then $E$ has no rational point of any order other than $2,\\ldots,10$, or $12$.\n\nMazur's paper doing this (the famous Eisenstein ideal paper) was the one which\nreally established the effectiveness of Grothendieck's algebro-geometric tools for solving classical number theory problems.  For example, Wiles's work on Fermat's Last Theorem fits squarely in the tradition established by Mazur's paper.\nAs far as I know, no-one has found an elementary proof of Mazur's theorem; the\nelementary techniques of Billing--Mahler and Ogg don't seem to be extendable to the general case.  So this is an interesting Diophantine problem which seems to require modern algebraic geometry to solve.\n\nOften when a Diophantine problem is solved by algebro-geometric methods, it is not as simple as the way you suggest in your question.\nFor example, in the results described above, one does not work with one particular elliptic curve at a time.  Rather, for each $N \\geq 1$, there is a Diophantine equation, whose solutions over $\\mathbb Q$ correspond to elliptic\ncurves over $\\mathbb Q$ with a rational solution of order $N$.\nThis is the so-called modular curve $Y_1(N)$; although it was in some sense known to Jacobi, Kronecker, and the other 19th century developers of the theory of elliptic and automorphic functions, its precise interpretation as a Diophantine equation over $\\mathbb Q$ is hard to make precise without modern\ntechniques of algebraic geometry.  (As its name suggests, it is a certain moduli space.)\nAn even more important contribution of modern theory is that this Diophantine equation even has a canonical model over $\\mathbb Z$, which continues to have\na moduli-space interpretation.  (Concretely, this means that one starts with\nsome Diophantine equation --- or better, system of Diophantine equations --- over $\\mathbb Q$, and then clears the denominators in a canonical fashion,\nto get a particular system of Diophantine equations with integral coefficients\nwhose solutions have a conceptual interpretation in terms of certain data related\nto elliptic curves.)\nThe curve $Y_1(N)$ is affine, not projective, and it is more natural to study projective curves.  One can naturally complete it to a projective curve,\ncalled $X_1(N)$.  It turns out that $X_1(N)$ can have rational solutions --- some of the extra points we added in going from $Y_1(N)$ to $X_1(N)$\nmight be rational --- and so we can rephrase Mazur's theorem as saying that\nthe only rational points of $X_1(N)$ (for any $N \\neq 2,\\ldots,10,12$) lie in\nthe complement of $Y_1(N)$.\nIn fact, there are related curves $X_0(N)$, and what he proves is that $X_0(N)$ has only finitely many rational points for each $N$.  He is then able to deduce the result about $Y_1(N)$ and $X_1(N)$ by further arguments.\n\nThe reason for giving the preceding somewhat technical details is that I want\nto say something about how Mazur's proof works in the particular case $N = 11$\n(recovering the theorem of Billing and Mahler).\nThe curve $X_0(11)$ is an elliptic curve.  One can write down its\nexplicit equation easily enough; it is (the projectivization of)\n$$y^2 +y = x^3 - x^2 - 10 x - 20.$$\n(There is one point at infinity, which serves as the origin of the group law.)\nMazur wants to show it has only finitely many solutions.   It's not clear how the explicit equation will help.  (In the sense that if you begin with this equation, it's not clear how to directly show that it has only finitely many solutions over $\\mathbb Q$.)\nInstead, he first notes that it has a subgroup of rational points of order $5$:\n$$\\{\\text{ the point at infinity}, (5,5), (16,-61), (16,60), (5,-6) \\}.$$\nOne knows from the general theory of elliptic curves that the full $5$-torsion subgroup of $X_0(11)$ is of order $25$, a product of two cyclic groups of order $5$.\nWe have one of them above, while the other factor is not given by\nrational points.\nIn fact, the other $5$-torsion points have coordinates in the field $\\mathbb Z[\\zeta_5]$.  (I don't know their explicit coordinates, unfortunately.)\nMazur doesn't need to know their exact values; instead, what is important for\nhim is that he is able to show (by conceptual, not computational, arguments)\nthat the full $5$-torsion subgroup of $X_0(11)$, now thought of not just as a Diophantine over $\\mathbb Q$ but as a scheme over Spec $\\mathbb Z$,\nis a product of two group schemes of order $5$: namely\n$$\\mathbb Z/ 5\\mathbb Z \\times \\mu_5.$$\nThe first factor is the subgroup of order $5$ determined by the points with\ninteger coordinates; the second factor is a subgroup of order $5$ generated by\na $5$-torsion point with coefficients in Spec $\\mathbb Z[\\zeta_5]$.\nWhat does it mean that this second factor is $\\mu_5$?\nWell, $X^5 - 1$ is a Diophantine equation, whose solutions are defined over\n$\\mathbb Z[\\zeta_5]$, and  have a natural (multiplicative) group structure, and this is what $\\mu_5$ is.\nWhat Mazur says is that an isomorphic copy of this \"Diophantine group\" (more precisely, this group scheme) lives inside $X_0(11)$.\nNote that the classical theory of Diophantine equations is not very well set up\nto deal with concepts like \"isomorphisms of Diophantine equations whose solutions admits a natural group structure\".  (One already sees this if one tries to develop the theory of elliptic curves, including the group structure, in an elementary way.)   So this is already a place where scheme theory provides new and important expressive power.\nIn any event, once Mazur has this formula for the $5$-torsion, he can make an infinite descent to prove that there are no other rational points besides the $5$ that we already wrote down.   He doesn't phrase this infinite descent in\nthe naive way, with equations, as Fermat did with his descents (although it\nis the same underlying idea): rather, he argues as follows:\nThe curve $X_0(11)$ stays non-singular modulo every prime except $11$ (as you can check directly from the above equation).  Modulo $11$ it becomes singular:\nyou can check directly that reduced modulo $11$, the above equation becomes\n$$(y-5)^2 = (x-2)(x-5)^2,$$\nwhich has a singular point (a node) at $(5,5)$.\nNote now that all our rational solutions $(5,5), (16,-61),$ etc. (other than\nthe point at infinity) reduce\nto the node when you reduce them modulo $11$.\nUsing this (plus a little more argument) what you can show is that if\n$(x,y)$ is any rational point of $X_0(11)$, then after subtracting off\n(in the group law) a suitable choice of one of our $5$ known points, you obtain\na point which does not reduce to the node upon reduction modulo $11$.\nSo what we have to show is that if $(x,y)$ is any rational solution on $X_0(11)$\nwhich does not map to the node mod $11$, it is trivial (i.e. the point at\ninfinity).\nSuppose it is not: then Mazur considers a point $(x',y')$ (no longer necessarily rational,\njust defined over some number field) which maps to $(x,y)$ under multiplication\nby $5$ (in the group law).  (This is the descent argument.)\nNow this point is not uniquely determined, but it is determined up to\naddition (in the group law) of a $5$-torsion point.  Because we know the precise\nstructure of the $5$-torsion (even over Spec $\\mathbb Z$) we see that this\npoint would have to have coordinates in some compositum of fields of the following type: (a) an everywhere unramified cyclic degree $5$ extension of $\\mathbb Q$ (this relates to the $\\mathbb Z/5\\mathbb Z$ factor); and (b) an everywhere\nunramifed extension of $\\mathbb Q$ obtained by extracting the $5$th root of\nsome number (this relates to the $\\mu_5$ factor). Now no such extension of $\\mathbb Q$ exist (e.g. because\n$\\mathbb Q$ admits no non-trivial everywhere unramified extension), and hence\n$(x',y')$ again has to be defined over $\\mathbb Q$.  Now we repeat the above\nprocedure ad infitum, to get a contradiction (via infinite descent).\n\nI hope that the above sketch gives some idea of how more sophisticated methods\ncan help with the solution of Diophantine equations.  It is not just that one writes down Spec and magically gets new information.  Rather, the introduction of a more conceptual way of thinking gives whole new ways of transferring information around and making computations which are not accessible when working in a naive manner.\nA good high-level comparison would be the theory of solutions of algebraic equations before and after Galois's contributions.\nA more specific analogy would be the difference between studying surfaces in space (say) with the tools of an undergraduate multi-variable calculus class,\ncompared to the tools of manifold theory.  In undergraduate calculus, one has to\nat all times remember the equation for the surface, work with explicit coordinates, make explicit coordinate changes to reduce computations from the curved surface to the plane, and so on.  In manifold theory, one has a conceptual apparatus which lets one speak of the surface as an object independent of the equation cutting it out; one can say \"consider a chart\nin the neighbourhood of the point $p$\" without having to explicitly write\ndown the functions giving rise to the chart.  (The implicit function theorem\nsupplies them, and that is often enough; you don't have to concretely determine the output\nof that theorem every time you want to apply it.)\nSo it goes with the scheme-theoretic point of view.  One can use the modular\ninterpretation to write down points of $X_0(11)$ without having to give their\ncoordinates.  In fact, one can show that it has a node when reduced modulo $11$\nwithout ever having to write down an equation.  The determination of the $5$-torsion group is again made by conceptual arguments, without having to write down the actual solutions in coordinates.  And as the above sketch of the infinite descent (hopefully) makes clear, it is any case the abstract nature\nof the $5$-torsion points (the fact that they are isomorphic to\n$\\mathbb Z/5\\mathbb Z \\times \\mu_5$) which is important for the descent, not any information about their explicit coordinates.\nI hope this answer, as long and technical as it is, gives some hint as to the utility of the scheme-theoretic viewpoint.\n\nReferences: A nice introduction to $X_0(11)$ is given in this expository article of Tom Weston.\nAs for Mazur's theorem, I don't know of any expositions which are not at a\nmuch higher level of sophistication.  (There are simpler proofs of his main technical results now, e.g. here,\nbut these are simpler only in a relative sense; they are still not accessible\nto non-experts in this style of number theory.)", "meta": {"post_id": 30866, "input_score": 39, "output_score": 71, "post_title": "Diophantine applications of Spec?"}}
{"input": "Find all irreducible monic polynomials in $\\mathbb{Z}/(2)[x]$ with degree equal or less than $5$.\n\nThis is what I tried:\nIt's evident that $x,x+1$ are irreducible. Then, use these to find all reducible polynomials of degree 2. There ones that can't be made are irreducible. Then use these to make polynomials of degree 3, the ones that can't be made are irreducible. Repeat until degree 5.\nDoing this way takes way too long and I just gave up during when I was about to reach degree 4 polynomials.\nMy question is: is there any easier way to find these polynomials?\nP.S.: this is not exactly homework, but a question which I came across while studying for an exam.", "output": "Extrapolated Comments converted to answer:\nFirst, we note that there are $2^n$ polynomials in $\\mathbb{Z}_2[x]$ of degree $n$.\nA polynomial $p(x)$ of degree $2$ or $3$ is irreducible if and only if it does not have linear factors.  Therefore, it suffices to show that $p(0) = p(1) = 1$.  This quickly tells us that $x^2 + x + 1$ is the only irreducible polynomial of degree $2$.  This also tells us that $x^3 + x^2 + 1$ and $x^3 + x + 1$ are the only irreducible polynomials of degree $3$.\nAs hardmath points out, for a polynomial $p(x)$ of degree $4$ or $5$ to be irreducible, it suffices to show that $p(x)$ has no linear or quadratic factors.  To rule out the linear factors, we can again throw out any polynomial not satisfying $p(0) = p(1) = 1$.  That is, we can throw out any polynomial with constant term $0$, and we can throw out any polynomial with an even number of terms.  This rules out $3/4$ of the polynomials.  For example, the $4^{th}$ degree polynomials which do not have linear factors are:\n\n$ x^4 + x^3 + x^2 + x + 1 $\n$ x^4 + x^3 + 1 $\n$ x^4 + x^2 + 1 $\n$ x^4 + x + 1 $\n\nThe $5^{th}$ degree polynomials which do not contain linear factors are:\n\n$x^5 + x^4 + x^3 + x^2 + 1$\n$x^5 + x^4 + x^3 + x + 1$\n$x^5 + x^4 + x^2 + x + 1$\n$x^5 + x^3 + x^2 + x + 1$\n$x^5 + x^4  + 1$\n$x^5 + x^3 + 1$\n$x^5 + x^2 + 1$\n$x^5 + x + 1$\n\nIt still remains to check whether $x^2 + x + 1$ (which is the only quadratic irreducible polynomial in $\\mathbb{Z}_2[x]$) divides any of these polynomials.  This can be done by hand for sufficiently small degrees.  Again, as hardmath points out, since $x^2 + x + 1$ is the only irreducible polynomial of degree $2$, it follows that $(x^2 + x + 1)^2 = x^4 + x^2 + 1$ is the only polynomial of degree $4$ which does not have linear factors and yet is not irreducible.  Therefore, the other $3$ polynomials listed must be irreducible.  Similarly, for degree $5$ polynomials, we can rule out \n$$\n(x^2 + x + 1)(x^3 + x^2 + 1) = x^5 + x + 1\n$$\nand \n$$\n(x^2 + x + 1)(x^3 + x + 1) = x^5 + x^4 + 1.\n$$\nThe other $6$ listed polynomials must therefore be irreducible.\nNotice that this trick of throwing out polynomials with linear factors, then quadratic factors, etc. (which hardmath called akin to the Sieve of Eratosthenes) is not efficient for large degree polynomials (even degree $6$ starts to be a problem, as a polynomial of degree $6$ can factor as a product of to polynomials of degree $3$).  This method, therefore only works for sufficiently small degree polynomials.\nTo recap, the irreducible polynomials in $\\mathbb{Z}_2[x]$ of degree $\\leq 5$ are:\n\n$x$\n$x+1$\n$x^2 + x + 1$\n$x^3 + x^2 + 1$\n$x^3 + x + 1$\n$ x^4 + x^3 + x^2 + x + 1 $\n$ x^4 + x^3 + 1 $\n$ x^4 + x + 1 $\n$x^5 + x^4 + x^3 + x^2 + 1$\n$x^5 + x^4 + x^3 + x + 1$\n$x^5 + x^4 + x^2 + x + 1$\n$x^5 + x^3 + x^2 + x + 1$\n$x^5 + x^3 + 1$\n$x^5 + x^2 + 1$", "meta": {"post_id": 32197, "input_score": 51, "output_score": 74, "post_title": "Find all irreducible monic polynomials in $\\mathbb{Z}/(2)[x]$ with degree equal or less than 5"}}
{"input": "The definitions do not seem easy to me for computation. For example,  Lebesgue(-Stieltjes) integral is a measure theory concept, involving construction for from step function, simple functions, nonnegative function till general function.\nI was wondering, in practice, what common ways for computing Lebesgue(-Stieltjes) integral are?\n\nIs it most desirable, when possible,\nto convert Lebesgue(-Stieltjes)\nintegral to Riemann(-Stieltjes)\nintegral, and Riemann-Stieltjes)\nintegral to Riemann integral, and\nthen apply the methods learned from\ncalculus to compute the equivalent\nRiemann integral?\nWhat about the cases when the\nequivalence/conversion is not\npossible? Is definition the only way\nto compute Riemann-Stieltjes or\nLebesgue(-Stieltjes) integrals?\n\nMy questions come from a previous reply by Gortaur\n\nUsually only Lebesgue\n  (Lebesgue-Stieltjes) integrals are\n  used in the probability theory. On the\n  other hand to calculate them you can\n  use an equivalence of\n  Lebesgue-Stieltjes and\n  Riemann-Stieltjes integrals (provided\n  necessary conditions).\n\nThanks and regards!", "output": "Even with the Riemann Integral, we do not usually use the definition (as a limit of Riemann sums, or by verifying that the limit of the upper sums and the lower sums both exist and are equal) to compute integrals. Instead, we use the Fundamental Theorem of Calculus, or theorems about convergence. The following are taken from Frank E. Burk's A Garden of Integrals, which I recommend. One can use these theorems to compute integrals without having to go down all the way to the definition (when they are applicable).\nTheorem (Theorem 3.8.1 in AGoI; Convergence for Riemann Integrable Functions) If $\\{f_k\\}$ is a sequence of Riemann integrable functions converging uniformly to the function $f$ on $[a,b]$, then $f$ is Riemann integrable on $[a,b]$ and\n$$R\\int_a^b f(x)\\,dx = \\lim_{k\\to\\infty}R\\int_a^b f_k(x)\\,dx$$\n(where \"$R\\int_a^b f(x)\\,dx$\" means \"the Riemann integral of $f(x)$\").\nTheorem (Theorem 3.7.1 in AGoI; Fundamental Theorem of Calculus for the Riemann Integral) If $F$ is a differentiable function on $[a,b]$, and $F'$ is bounded and continuous almost everywhere on $[a,b]$, then:\n\n$F'$ is Riemann-integrable on $[a,b]$, and\n$\\displaystyle R\\int_a^x F'(t)\\,dt = F(x) - F(a)$ for each $x\\in [a,b]$.\n\n\nLikewise, for Riemann-Stieltjes, we don't usually go by the definition; instead we try, as far as possible, to use theorems that tell us how to evaluate them. For example:\nTheorem (Theorem 4.3.1 in AGoI) Suppose $f$ is continuous and $\\phi$ is differentiable, with $\\phi'$ being Riemann integrable on $[a,b]$. Then the Riemann-Stieltjes integral of $f$ with respect to $\\phi$ exists, and\n$$\\text{R-S}\\int_a^b f(x)d\\phi(x) = R\\int_a^b f(x)\\phi'(x)\\,dx$$\nwhere $\\text{R-S}\\int_a^bf(x)d\\phi(x)$ is the Riemann-Stieltjes integral of $f$ with respect to $d\\phi(x)$. \nTheorem (Theorem 4.3.2 in AGoI) Suppose $f$ and $\\phi$ are bounded functions with no common discontinuities on the interval $[a,b]$, and that the Riemann-Stieltjes integral of $f$ with respect to $\\phi$ exists. Then the Riemann-Stieltjes integral of $\\phi$ with respect to $f$ exists, and\n$$\\text{R-S}\\int_a^b \\phi(x)df(x) = f(b)\\phi(b) - f(a)\\phi(a) - \\text{R-S}\\int_a^bf(x)d\\phi(x).$$\nTheorem. (Theorem 4.4.1 in AGoI; FTC for Riemann-Stieltjes Integrals) If $f$ is continuous on $[a,b]$ and $\\phi$ is monotone increasing on $[a,b]$, then $$\\displaystyle \\text{R-S}\\int_a^b f(x)d\\phi(x)$$\nexists. Defining a function $F$ on $[a,b]$ by\n$$F(x) =\\text{R-S}\\int_a^x f(t)d\\phi(t),$$\nthen\n\n$F$ is continuous at any point where $\\phi$ is continuous; and\n$F$ is differentiable at each point where $\\phi$ is differentiable (almost everywhere), and at such points $F'=f\\phi'$. \n\nTheorem. (Theorem 4.6.1 in AGoI; Convergence Theorem for the Riemann-Stieltjes integral.) Suppose $\\{f_k\\}$ is a sequence of continuous functions converging uniformly to $f$ on $[a,b]$ and that $\\phi$ is monotone increasing on $[a,b]$. Then\n\nThe Riemann-Stieltjes integral of $f_k$ with respect to $\\phi$ exists for all $k$; and\nThe Riemann-Stieltjes integral of $f$ with respect to $\\phi$ exists; and\n$\\displaystyle \\text{R-S}\\int_a^b f(x)d\\phi(x) = \\lim_{k\\to\\infty} \\text{R-S}\\int_a^b f_k(x)d\\phi(x)$.\n\nOne reason why one often restricts the Riemann-Stieltjes integral to $\\phi$ of bounded variation is that every function of bounded variation is the difference of two monotone increasing functions, so we can apply theorems like the above when $\\phi$ is of bounded variation.\n\nFor the Lebesgue integral, there are a lot of \"convergence\" theorems: theorems that relate the integral of a limit of functions with the limit of the integrals; these are very useful to compute integrals. Among them:\nTheorem (Theorem 6.3.2 in AGoI) If $\\{f_k\\}$ is a monotone increasing sequence of nonnegative measurable functions converging pointwise to the function $f$ on $[a,b]$, then the Lebesgue integral of $f$ exists and\n$$L\\int_a^b fd\\mu = \\lim_{k\\to\\infty} L\\int_a^b f_kd\\mu.$$\nTheorem (Lebesgue's Dominated Convergence Theorem; Theorem 6.3.3 in AGoI) Suppose $\\{f_k\\}$ is a sequence of Lebesgue integrable functions ($f_k$ measurable and $L\\int_a^b|f_k|d\\mu\\lt\\infty$ for all $k$) converging pointwise almost everywhere to $f$ on $[a,b]$. Let $g$ be a Lebesgue integrable function such that $|f_k|\\leq g$ on $[a,b]$ for all $k$. Then $f$ is Lebesgue integrable on $[a,b]$ and\n$$L\\int_a^b fd\\mu = \\lim_{k\\to\\infty} L\\int_a^b f_kd\\mu.$$\nTheorem (Theorem 6.4.2 in AGoI) If $F$ is a differentiable function, and the derivative $F'$ is bounded on the interval $[a,b]$, then $F'$ is Lebesgue integrable on $[a,b]$ and\n$$L\\int_a^x F'd\\mu = F(x) - F(a)$$\nfor all $x$ in $[a,b]$.\nTheorem (Theorem 6.4.3 in AGoI) If $F$ is absolutely continuous on $[a,b]$, then $F'$ is Lebesgue integrable and\n$$L\\int_a^x F'd\\mu = F(x) - F(a),\\qquad\\text{for }x\\text{ in }[a,b].$$\nTheorem (Theorem 6.4.4 in AGoI) If $f$ is continuous and $\\phi$ is absolutely continuous on an interval $[a,b]$, then the Riemann-Stieltjes integral of $f$ with respect to $\\phi$ is the Lebesgue integral of $f\\phi'$ on $[a,b]$:\n$$\\text{R-S}\\int_a^b f(x)d\\phi(x) = L\\int_a^b f\\phi'd\\mu.$$\n\nFor Lebesgue-Stieltjes Integrals, you also have an FTC:\nTheorem. (Theorem 7.7.1 in AGoI; FTC for Lebesgue-Stieltjes Integrals) If $g$ is a Lebesgue measurable function on $R$, $f$ is a nonnegative Lebesgue integrable function on $\\mathbb{R}$, and $F(x) = L\\int_{-\\infty}^xd\\mu$, then\n\n$F$ is bounded, monotone increasing, absolutely continuous, and differentiable almost everywhere with $F' = f$ almost everywhere;\nThere is a Lebesgue-Stieltjes measure $\\mu_f$ so that, for any Lebesgue measurable set $E$, $\\mu_f(E) = L\\int_E fd\\mu$, and $\\mu_f$ is absolutely continuous with respect to Lebesgue measure.\n$\\displaystyle \\text{L-S}\\int_{\\mathbb{R}} gd\\mu_f = L\\int_{\\mathbb{R}}gfd\\mu = L\\int_{\\mathbb{R}} gF'd\\mu$. \n\n \nThe Henstock-Kurzweil integral likewise has monotone convergence theorems (if $\\{f_k\\}$ is a monotone sequence of H-K integrable functions that converge pointwise to $f$, then $f$ is H-K integrable if and only if the integrals of the $f_k$ are bounded, and in that case the integral of the limit equals the limit of the integrals); a dominated convergence theorem (very similar to Lebesgue's dominated convergence); an FTC that says that if $F$ is differentiable on $[a,b]$, then $F'$ is H-K integrable and\n$$\\text{H-K}\\int_a^x F'(t)dt = F(x) - F(a);$$\n(this holds if $F$ is continuous on $[a,b]$ and has at most countably many exceptional points on $[a,b]$ as well); and a \"2nd FTC\" theorem.", "meta": {"post_id": 32217, "input_score": 24, "output_score": 42, "post_title": "How to compute Riemann-Stieltjes / Lebesgue(-Stieltjes) integral?"}}
{"input": "This is a simple question I came across in reviewing.  I am wondering if I got the correct answer.  \nThe question is simple.  You have $n$ balls and $m$ bins.  Each ball has an equal probability of landing in any bin.  I want to know what the probability that exactly $1$ bin is empty.\nMy answer seems simple enough, but I don't think it's sufficient.  It is $(\\frac{m-1}{m})^n$ since for each ball, it can go in any of the other bins.  I think, however, that this is just the probability that some arbitrary bin $A$ is empty, not exactly one bin.  What else should I consider?", "output": "Let's count configurations, and then divide by $m^n$. \nThere are $m$ choices for the empty bin. Then the other bins are occupied. We can count the ways to place $n$ balls in $m-1$ bins so that no bin is empty by inclusion-exclusion: It is \n$$\\sum_{k ~\\text {bins known to be empty}} (-1)^k {m-1 \\choose k} (m-1-k)^n.$$\nAnother way to get this is to label the parts of a set partition of size $n$ with $m-1$ parts. The number of set partitions with a given number of parts is a Stirling number of the second kind, and we want $(m-1)! S(n,m-1)$.\nMultiply this by $m$ and then divide by $m^n$ to get the probability exactly $1$ bin is empty. \nWe can use the same techniques to compute the probability exactly $e$ bins are empty for other values of $e$. For example, suppose there are $4$ bins and $6$ balls. Then there are $1560$ ways for there to be $0$ empty bins, $2160$ ways for there to be exactly $1$ empty bin, $372$ ways for there to be exactly $2$ empty bins, and $4$ ways for there to be exactly $3$ empty bins. The total is $4096 = 4^6$. Dividing by this gives a probability of $\\frac{135}{256} = 0.52734375$ that exactly $1$ bin is empty.", "meta": {"post_id": 32444, "input_score": 5, "output_score": 35, "post_title": "Simple probability question, balls and bins"}}
{"input": "I'm thinking about a circle rolling along a parabola. Would this be a parametric representation?\n$(t + A\\sin (Bt)  , Ct^2 + A\\cos (Bt) )$\nA gives us the radius of the circle, B changes the frequency of the rotations, C, of course, varies the parabola.  Now, if I want the circle to \"match up\" with the parabola as if they were both made of non-stretchy rope, what should I choose for B?\nMy first guess is 1. But, the the arc length of a parabola from 0 to 1 is much less than the length from 1 to 2. And, as I examine the graphs, it seems like I might need to vary B in order to get the graph that I want. Take a look:\n\nThis makes me think that the graph my equation produces will always be wrong no matter what constants I choose. It should look like a cycloid:\n\nBut bent to fit on a parabola. [I started this becuase I wanted to know if such a curve could be self-intersecting. (I think yes.) When I was a child my mom asked me to draw what would happen if a circle rolled along the tray of the blackboard with a point on the rim tracing a line ... like most young people, I drew self-intersecting loops and my young mind was amazed to see that they did not intersect!]\nSo, other than checking to see if this is even going in the right direction, I would like to know if there is a point where the curve shown (or any curve in the family I described) is most like a cycloid-- \nThanks.\n\"It would be really really hard to tell\" is a totally acceptable answer, though it's my current answer, and I wonder if the folks here can make it a little better.", "output": "(I had been meaning to blog about roulettes a while back, but since this question came up, I'll write about this topic here.)\nI'll use the parametric representation\n$$\\begin{pmatrix}2at\\\\at^2\\end{pmatrix}$$\nfor a parabola opening upwards, where $a$ is the focal length, or the length of the segment joining the parabola's vertex and focus. The arclength function corresponding to this parametrization is $s(t)=a(t\\sqrt{1+t^2}+\\mathrm{arsinh}(t))$.\nuser8268 gave a derivation for the \"cycloidal\" case, and Willie used unit-speed machinery, so I'll handle the generalization to the \"trochoidal case\", where the tracing point is not necessarily on the rolling circle's circumference.\nWillie's comment shows how you should consider the notion of \"rolling\" in deriving the parametric equations: a rotation (about the wheel's center) followed by a rotation/translation. The first key is to consider that the amount of rotation needed for your \"wheel\" to roll should be equivalent to the arclength along the \"base curve\" (in your case, the parabola).\nI'll start with a parametrization of a circle of radius $r$ tangent to the horizontal axis at the origin:\n$$\\begin{pmatrix}-r\\sin\\;u\\\\r-r\\cos\\;u\\end{pmatrix}$$\nThis parametrization of the circle was designed such that a positive value of the parameter $u$ corresponds to a clockwise rotation of the wheel, and the origin corresponds to the parameter value $u=0$.\nThe arclength function for this circle is $ru$; for rolling this circle, we obtain the equivalence\n$$ru=s(t)-s(c)$$\nwhere $c$ is the parameter value corresponding to the point on the base curve where the rolling starts. Solving for $u$ and substituting the resulting expression into the circle equations yields\n$$\\begin{pmatrix}-r\\sin\\left(\\frac{s(t)-s(c)}{r}\\right)\\\\r-r\\cos\\left(\\frac{s(t)-s(c)}{r}\\right)\\end{pmatrix}$$\nSo far, this is for the \"cycloidal\" case, where the tracing point is on the circumference. To obtain the \"trochoidal\" case, what is needed is to replace the $r$ multiplying the trigonometric functions with the quantity $hr$, the distance of the tracing point from the center of the rolling circle:\n$$\\begin{pmatrix}-hr\\sin\\left(\\frac{s(t)-s(c)}{r}\\right)\\\\r-hr\\cos\\left(\\frac{s(t)-s(c)}{r}\\right)\\end{pmatrix}$$\nAt this point, I note that $r$ here can be a positive or a negative quantity. For your \"parabolic trochoid\", negative $r$ corresponds to the circle rolling outside the parabola and positive $r$ corresponds to rolling inside the parabola. $h=1$ is the \"cycloidal\" case; $h > 1$ is the \"prolate\" case (tracing point outside the rolling circle), and $0 < h < 1$ is the \"curtate\" case (tracing point within the rolling circle).\nThat only takes care of the rotation corresponding to \"rolling\"; to get the circle into the proper position, a further rotation and a translation has to be done. The further rotation needed is a rotation by the tangential angle $\\phi$, where for a parametrically-represented curve $(f(t)\\quad g(t))^T$, $\\tan\\;\\phi=\\frac{g^\\prime(t)}{f^\\prime(t)}$. (In words: $\\phi$ is the angle the tangent of the curve at a given $t$ value makes with the horizontal axis.)\nWe then substitute the expression for $\\phi$ into the anticlockwise rotation matrix\n$$\\begin{pmatrix}\\cos\\;\\phi&-\\sin\\;\\phi\\\\\\sin\\;\\phi&\\cos\\;\\phi\\end{pmatrix}$$\nwhich yields\n$$\\begin{pmatrix}\\frac{f^\\prime(t)}{\\sqrt{f^\\prime(t)^2+g^\\prime(t)^2}}&-\\frac{g^\\prime(t)}{\\sqrt{f^\\prime(t)^2+g^\\prime(t)^2}}\\\\\\frac{g^\\prime(t)}{\\sqrt{f^\\prime(t)^2+g^\\prime(t)^2}}&\\frac{f^\\prime(t)}{\\sqrt{f^\\prime(t)^2+g^\\prime(t)^2}}\\end{pmatrix}$$\nFor the parabola as I had parametrized it, the tangential angle rotation matrix is\n$$\\begin{pmatrix}\\frac1{\\sqrt{1+t^2}}&-\\frac{t}{\\sqrt{1+t^2}}\\\\\\frac{t}{\\sqrt{1+t^2}}&\\frac1{\\sqrt{1+t^2}}\\end{pmatrix}$$\nThis rotation matrix can be multiplied with the \"transformed circle\" and then translated by the vector $(f(t)\\quad g(t))^T$, finally resulting in the expression\n$$\\begin{pmatrix}f(t)\\\\g(t)\\end{pmatrix}+\\frac1{\\sqrt{f^\\prime(t)^2+g^\\prime(t)^2}}\\begin{pmatrix}f^\\prime(t)&-g^\\prime(t)\\\\g^\\prime(t)&f^\\prime(t)\\end{pmatrix}\\begin{pmatrix}-hr\\sin\\left(\\frac{s(t)-s(c)}{r}\\right)\\\\r-hr\\cos\\left(\\frac{s(t)-s(c)}{r}\\right)\\end{pmatrix}$$\nfor a trochoidal curve. (What those last two transformations do, in words, is to rotate and shift the rolling circle appropriately such that the rolling circle touches an appropriate point on the base curve.)\nUsing this formula, the parametric equations for the \"parabolic trochoid\" (with starting point at the vertex, $c=0$) are\n$$\\begin{align*}x&=2at+\\frac{r}{\\sqrt{1+t^2}}\\left(ht\\cos\\left(\\frac{a}{r}\\left(t\\sqrt{1+t^2}+\\mathrm{arsinh}(t)\\right)\\right)-t-h\\sin\\left(\\frac{a}{r}\\left(t\\sqrt{1+t^2}+\\mathrm{arsinh}(t)\\right)\\right)\\right)\\\\y&=at^2-\\frac{r}{\\sqrt{1+t^2}}\\left(h\\cos\\left(\\frac{a}{r}\\left(t\\sqrt{1+t^2}+\\mathrm{arsinh}(t)\\right)\\right)+ht\\sin\\left(\\frac{a}{r}\\left(t\\sqrt{1+t^2}+\\mathrm{arsinh}(t)\\right)\\right)-1\\right)\\end{align*}$$\nA further generalization to a space curve can be made if the rolling circle is not coplanar to the parabola; I'll leave the derivation to the interested reader (hint: rotate the \"transformed\" rolling circle equation about the x-axis before applying the other transformations).\nNow, for some plots:\n\nFor this picture, I used a focal length $a=1$ and a radius $r=\\frac34$ (negative for the \"outer\" ones and positive for the \"inner\" ones). The curtate, cycloidal, and prolate cases correspond to $h=\\frac12,1,\\frac32$.\n\n(added 5/2/2011)\nI did promise to include animations and code, so here's a bunch of GIFs I had previously made in Mathematica 5.2:\nInner parabolic cycloid, $a=1,\\;r=\\frac34\\;h=1$\n\nCurtate inner parabolic trochoid, $a=1,\\;r=\\frac34\\;h=\\frac12$\n\nProlate inner parabolic trochoid, $a=1,\\;r=\\frac34\\;h=\\frac32$\n\nOuter parabolic cycloid, $a=1,\\;r=-\\frac34\\;h=1$\n\nCurtate outer parabolic trochoid, $a=1,\\;r=-\\frac34\\;h=\\frac12$\n\nProlate outer parabolic trochoid, $a=1,\\;r=-\\frac34\\;h=\\frac32$\n\nThe Mathematica code (unoptimized, sorry) is a bit too long to reproduce; those who want to experiment with parabolic trochoids can obtain a notebook from me upon request.\nAs a final bonus, here is an animation of a three-dimensional generalization of the prolate parabolic trochoid:", "meta": {"post_id": 32629, "input_score": 62, "output_score": 77, "post_title": "A circle rolls along a parabola"}}
{"input": "I was trying to learn a little about the Dedekind zeta function. The first place I looked at was obviously the Wikipedia article above. So my question comes from a sentence by the end of the article in the section on relations to other L-functions. That section states that if you have an abelian extension $K/\\mathbb{Q} \\,$, then the Dedekind zeta function of $K$ is a product of Dirichlet L-functions. \nIn particular it states that if $K$ is a quadratic field, then the ratio\n$$\\frac{\\zeta_K (s)}{\\zeta(s)} = L(s, \\chi)$$\nor equivalently, that $$\\zeta_K (s) = \\zeta(s) L(s, \\chi)$$\nwhere $\\zeta(s)$ is the Riemann zeta function and $L(s, \\chi) \\,$ is the Dirichlet L-function associated to the Dirichlet character defined by the Jacobi symbol as follows. If $K = \\mathbb{Q}(\\sqrt{d}) \\,$, and $D$ is the discriminant of this number field, then \n$$\\chi(n) := \\left ( \\frac{D}{n} \\right )$$\nThen the Wikipedia article says the following:\n\n\nThat the zeta function of a quadratic field is a product of the Riemann zeta function and a certain Dirichlet L-function is an analytic formulation of the quadratic reciprocity law of Gauss. \n\n\nThis is were I was absolutely amazed, because even though I've seen in the past that the existence of the Euler product for the Riemann zeta function is equivalent to the unique factorization property of the integers, which apparently is also reflected more generally in the context of Dedekind zeta functions and number fields, this time realized as the unique factorization of ideals into products of prime ideals, I just find marvelous that these two things can be equivalent (in some sense which I don't know yet).\n\n\nSo my question is why is this analytic fact about the Dedekind zeta function of a quadratic number field, \n$$\\zeta_K (s) = \\zeta(s) L(s, \\chi)$$\nan analytic reformulation of the quadratic reciprocity law?\n\n\nAnd maybe if it is possible, to push it a little bit further, are there analogues of this, say for higher reciprocity laws, like for cubic or biquadratic reciprocity? Or is this a \"peculiarity\" that occurs just for quadratic fields?\nThank you very much for any help.", "output": "This is not an answer to all your questions, but anyway.\n$\\zeta_K(s)=\\sum N(\\mathfrak{a})^{-s}=\\prod_\\mathfrak{p} \\frac{1}{1-N(\\mathfrak{p})^{-s}}$. In the product we have $\\frac{1}{1-p^{-s}}$ for every prime dividing $D$, $\\frac{1}{(1-p^{-s})^2}$ for every prime that splits in $K$, i.e. such that $D$ is a square mod $p$, and $\\frac{1}{1-p^{-2s}}= \\frac{1}{(1-p^{-s})(1+p^{-s})}$ for every prime which doesn't split. \nFor every prime, the factor is thus $\\frac{1}{1-p^{-s}}\\frac{1}{1-(\\frac{D}{p})p^{-s}}$. The product over all primes is thus $$\\zeta(s)\\,\\prod_p \\frac{1}{1-(\\frac{D}{p})p^{-s}}$$ and your equation becomes \n$$L(s,\\chi)=\\prod_p \\frac{1}{1-(\\frac{D}{p})p^{-s}}.$$\nThe function $\\chi$ in $L(s,\\chi)=\\sum_n \\chi(n) n^{-s}$ is a (quadratic) Dirichlet character modulo $D$, i.e. it is a group morphism $(\\mathbb{Z}/D\\mathbb{Z})^\\times\\to\\{+1,-1\\}$, which is then extended to a function $\\mathbb{Z}\\to \\{0,+1,-1\\}$ by $\\chi(n)=0$ if $(n,D)\\neq1$. Since $\\chi$ is multiplicative, we have\n$$L(s,\\chi)=\\prod_p \\frac{1}{1-\\chi(p)p^{-s}} .$$\nWe therefore indeed have $$\\chi(p)=\\left(\\frac{D}{p}\\right).$$\nThis implies that $(\\frac{D}{p})$ depends on $p$ only modulo $D$ - something not evident at all from its definition, but an easy consequence of quadratic reciprocity. If in particular $D=(-1)^{(q-1)/2} q$  (where $q$ is a prime) then there is only one quadratic Dirichlet character, namely $\\chi(n)=(\\frac{n}{q})$. We therefore have\n$$\\left(\\frac{p}{q}\\right)=\\left(\\frac{(-1)^{(q-1)/2}\\, q}{p}\\right)$$\ni.e. quadratic reciprocity.", "meta": {"post_id": 33006, "input_score": 46, "output_score": 34, "post_title": "Relation between the Dedekind Zeta Function and Quadratic Reciprocity"}}
{"input": "Given a matrix, is the Frobenius norm of that matrix always equal to the 2-norm of it, or are there certain matrices where these two norm methods would produce different results?\nIf they are identical, then I suppose the only difference between them is the method of calculation, eh?", "output": "The 2-norm (spectral norm) of a matrix is the greatest distortion of the unit circle/sphere/hyper-sphere. It corresponds to the largest singular value (or |eigenvalue| if the matrix is symmetric/hermitian). \nThe Forbenius norm is the \"diagonal\" between all the singular values.\ni.e. $$||A||_2 = s_1 \\;\\;,\\;\\;||A||_F = \\sqrt{s_1^2 +s_2^2 + ... + s_r^2}$$ \n(r being the rank of A).  \nHere's a 2D version of it: $x$ is any vector on the unit circle. $Ax$ is the deformation of all those vectors. The length of the red line is the 2-norm (biggest singular value). And the length of the green line is the Forbenius norm (diagonal).", "meta": {"post_id": 33083, "input_score": 77, "output_score": 41, "post_title": "What is the difference between the Frobenius norm and the 2-norm of a matrix?"}}
{"input": "There is a huge debate on the internet on the value of $48\\div2(9+3)$.\nI believe the answer $2$ as I believe it is part of the bracket operation in BEDMAS. Mathway yields the same answer. I also believe that if $48\\div2\\times(9+3)$ was asked it would be $288$ which Mathway agrees with as well.\nHowever, WolframAlpha says it is $288$ either way.\nA friend of mine (who is better at math) told me that there is no such thing as 'implicit multiplication', only shorthand so that is in fact done after the division (going left to right, not necessarily because division occurs before multiplication. But he didn't explicitly give a reason)\nWhat is the answer and why?", "output": "There is no Supreme Court for mathematical notation; there were no commandments handed down on Sinai concerning operational precedence; all there is, is convention, and different people are free to adhere to different conventions. Wise people will stick in enough parentheses to make it impossible for anyone to mistake the meaning. If they mean, $(48\\div2)(9+3)$, they'll write it that way; if they mean $48\\div\\bigl(2(9+3)\\bigr)$, they'll write it that way.", "meta": {"post_id": 33215, "input_score": 46, "output_score": 179, "post_title": "What is $48\\div2(9+3)$?"}}
{"input": "From a bank of previous masters exams:\n\nLet $G$ be a finite group such that its automorphism group $\\operatorname{Aut}(G)$ is cyclic. Prove that $G$ is abelian.\n\nHere's what I was thinking. Let $\\phi:G \\to G$ be the generator of $\\operatorname{Aut}(G)$, with order $n$. Assume that $G$ is not abelian. Then there is a nontrivial inner automorphism $\\psi_g(x) = g^{-1}xg$. Since $\\phi$ generates all automorphisms, then $\\psi_g = \\phi^k$ for some $k$. This also implies that $\\psi_g^n(x) = g^{-n}xg^n$ is the identity map.\nAfter that, no luck. Any ideas?", "output": "If the automorphism group is cyclic, then the Inner automorphism group is cyclic. But the inner automorphism group is isomorphic to $G/Z(G)$, and if $G/Z(G)$ is cyclic, then it is trivial. Therefore, $G=Z(G)$ so $G$ is abelian. (The argument does not require $G$ to be finite, by the by.)\nFor the latter:\nProp. If $H\\subseteq Z(G)$ and $G/H$ is cyclic, then $G$ is abelian.\nSuppose $G/H$ is cyclic, with $H\\subseteq Z(G)$. Let $g\\in G$ be such that $gH$ generates $G/H$. Then every $x\\in G$ can be written as $x=g^kh$ for some integer $k$ and some $h\\in H$. Given $x,y\\in G$, we have $x=g^kh$ and $y=g^{\\ell}h'$, so\n$$\\begin{align*}\nxy &= (g^kh)(g^{\\ell}h')\\\\\n   &= g^kg^{\\ell}hh' &\\quad&\\text{since }h\\in Z(G)\\\\\n   &= g^{\\ell}g^kh'h\\\\\n   &= g^{\\ell}h'g^kh &&\\text{since }h'\\in Z(G)\\\\\n   &= (g^{\\ell}h')(g^kh)\\\\ \n   &= yx,\n\\end{align*}$$\nhence $G$ is abelian. QED\nFor more on what groups can occur as central quotients, see this previous question.\nAdded. Since you mention you did not know that $\\mathrm{Inn}(G)\\cong G/Z(G)$, let's do that too:\nDefine a map $G\\to \\mathrm{Aut}(G)$ by mapping $g\\mapsto \\varphi_g$, where $\\varphi_g$ is \"conjugation by $g$\". That is, for all $x\\in G$,\n$\\varphi_g(x) = gxg^{-1}$. \nThis map is a group homomorphism: if $g,h\\in G$, then we want to show that $\\varphi_{gh} = \\varphi_g\\circ\\varphi_h$. To that end, let $x\\in G$ be any element, and we show that $\\varphi_{gh}(x) = \\varphi_g(\\varphi_h(x))$.\n$$\\varphi_{gh}(x) = (gh)x(gh)^{-1} = ghxh^{-1}g^{-1}= g(hxh^{-1})g^{-1} = \\varphi_g(hxh^{-1}) = \\varphi_g(\\varphi_h(x)).$$\nTherefore, the map $g\\mapsto\\varphi_g$ is a homomorphism from $G$ onto $\\mathrm{Inn}(G)$. By the Isomorphism Theorem, $\\mathrm{Inn}(G)$ is isomorphic to $G/N$, where $N$ is the kernel of this homomorphism.\nWhat is $N$? $g\\in N$ if and only if $\\varphi_g$ is the identity element of $\\mathrm{Aut}(G)$, which is the identity; that is, if and only if $\\varphi_g(x)=x$ for all $x\\in G$. But $\\varphi_g(x)=x$ if and only if $gxg^{-1}=x$, if and only if $gx = xg$. So $\\varphi_g(x) = x$ if and only if $g$ commutes with $x$. Thus, $\\varphi_g(x)=x$ for all $x$ if and only if $g$ commutes with all $x$, if and only if $g\\in Z(G)$. Thus, $N=Z(G)$, so $\\mathrm{Inn}(G)\\cong G/Z(G)$, as claimed.", "meta": {"post_id": 33254, "input_score": 19, "output_score": 35, "post_title": "Showing that a cyclic automorphism group makes a finite group abelian"}}
{"input": "$1$ has to map to $1$, right?  So $n$ has to map to $n$ (for $n \\in \\mathbb{Z}$), and $\\frac{1}{n}$ maps to $\\frac{1}{n}$, so $\\frac{n}{m}$ maps to itself, so the only possible automorphism is the identity.  Is this true or am I deceiving myself?  Because I feel like there should definitely be more automorphisms of $\\mathbb{Q}$.\nAlso, if you have some extension of $\\mathbb{Q}$ (call it $E$), does every automorphism of $E$ automatically fix $\\mathbb{Q}$?", "output": "When one says \"automorphisms\", it is important to specify automorphisms of what. There are a lot of automorphisms of $\\mathbb{Q}$ as an abelian group (equivalently, as a $\\mathbb{Q}$-vector space). \nHowever, there is one and only one field automorphism (equivalently, one and only one ring automorphism). Indeed: If you are doing a ring automorphism, then $1$ must map to an idempotent (an element equal to its square); there are only two idempotents in $\\mathbb{Q}$, $1$ and $0$; but if you map $1$ to $0$, then you map everything to $0$ and the map is not an automorphism. So $1$ must map to $1$ (you can skip this step if your definition of \"homomorphism of rings\" requires you to map $1$ to $1$). \nSince $1$ maps to $1$, by induction you can show that for every natural number $n$, $n$ maps to $n$. Therefore, $-n$ must map to $-n$ (since the map sends additive inverses to additive inverses), and must sent $\\frac{1}{n}$ to $\\frac{1}{n}$ (because it maps $1 = n(\\frac{1}{n})$ to $n$ times the image of $\\frac{1}{n}$, and the only solution to $nx = 1$ in $\\mathbb{Q}$ is $x=\\frac{1}{n}$. And from here you get that any field automorphism of $\\mathbb{Q}$ must be the identity.\nAs to your second question, yes: if $E$ is an extension of $\\mathbb{Q}$, then any field automorphism of $E$ restricts to the identity automorphism of $\\mathbb{Q}$. More generally, if $E$ is any field, then any automorphism of $E$ restricts to the identity of its prime field (which is $\\mathbb{Q}$ in the case of characteristic 0, and $\\mathbb{F}_p$ in the case of characteristic $p$).", "meta": {"post_id": 34217, "input_score": 31, "output_score": 36, "post_title": "Field automorphisms of $\\mathbb{Q}$ - shouldn't there be only one?"}}
{"input": "I know a sketch of the proof.\nM. A. Armstrong 's Basic Topology says that\n\nSuppose $X$ has a universal covering space, and denote it by $\\tilde{X}$. Then the covering transformations form a group isomorphic to the fundamental group of $X$. Given any subgroup $H$ of $\\pi_1(X)$, it acts on $\\tilde{X}$ and the associated orbit space $\\tilde{X}/H$ is a covering space of $X$ whose fundamental group is isomorphic to $H$. \n\n$\\mathbb{R}^2$ is the universal covering space of $T$. \nThe fundamental group of  a torus $T=S^1\\times S^1$ is isomorphic to\n$\\mathbb{Z}\\times \\mathbb{Z}=\\mathbb{Z}^2$. Any subgroup of $\\mathbb{Z}^2$ is isomorphic either to the trivial group, $\\mathbb{Z}$ or $\\mathbb{Z}^2$.\nSince $\\mathbb{R}^2/\\mathbb{Z}$ can be $S^1\\times\\mathbb{R}$, $\\mathbb{R}^2/\\mathbb{Z}^2$ can be $T$, we get the conclusion we want to prove.\nHowever, Basic Topology also says:\n\nA given group may act in many different ways on the same space.\n\nIndeed, it gives an example that $T/\\mathbb{Z}^2$ can be sphere, torus and Klein bottle.\nSo my problem is: Is it possible that $\\mathbb{R}^2/\\mathbb{Z}$ or $\\mathbb{R}^2/\\mathbb{Z}^2$ can be other spaces?\nCan you please help? Thank you.", "output": "To expand on what Alexander said, we aren't looking at group actions of $\\pi_1(X)$ on $\\widetilde{X}$, but we're looking at the group actions that are compatible with the covering map.  Let's go over the construction to see what is happening.\nIf we have a covering map $p:E\\to B$ of spaces, where $b\\in B$ is a fixed basepoint, then for any path $\\gamma:[0,1] \\to B$, with $\\gamma(0)=\\gamma(1)=b$,  $x\\in p^{-1}(b)$, there is a unique lift $\\gamma_x:[0,1]\\to E$ of $\\gamma$ with $\\gamma_x(0)=x$.  Note that by a lift of $\\gamma$, we mean that $p(\\gamma_x(t))=\\gamma(t)$.  Let us assume that $\\gamma$ is a loop, that is $\\gamma(0)=\\gamma(1)=b$, \nWe assert the following facts:\n(1) The map $x\\mapsto \\gamma_x(1)$ is in fact an automorphism of $p^{-1}(b)$.\n(2) This map depends only on the class of $\\gamma$ in $\\pi_1(B)$.\n(3) This map $\\pi_1(B) \\to \\operatorname{Aut}(p^{-1}(b))$ is actually a group homomorphism.\nGiven an automorphism of $\\phi$ of $p^{-1}(b)$, there is at most one automorphism $\\psi$ of $E$ which both restricts to $\\phi$ and satisfies $p(\\psi(e))=p(e)$ for all $e\\in E$.  Of course, not every automorphism lifts.  For example, if $\\mathbb{R}\\to S^1$ is the standard covering map, the automorphism of $\\mathbb{Z}$ (viewed as a space, not a group) defined by $z\\mapsto (-1)^{z} z$ doesn't extend to a continuous automorphism of $\\mathbb{R}$.  However, by using the path lifting property, we can show that all the automorphisms we've generated actually do extend (prove this!).  \nSo what we have shown is that there is a unique action of $\\pi_1(B)$ on $E$ which is compatible with the projection map $p$, in the sense that the action comes from path lifting.  There are probably other characterizations of the action in terms of an action of $\\pi_1(B)$ on $\\pi_1(E)$.\nOf course, there are lots of other actions.  For example, given any homomorphism from $\\pi_1(B)$ to itself we can compose these with our given action to get a new one\n$\\pi_1(B) \\to \\pi_1(B) \\to \\operatorname{Aut}(E)$\nFor example, composing with the trivial group homomorphism, we get the trivial action (all loops act trivially).  That doesn't take away from the fact that there is a canonical action.\nIn the case of the universal covering space, there is a very simple description of the action.  One of the constructions of $\\widetilde{X}$ is as the space of homotopy classes (preserving the endpoints) of paths in $X$ which start at the basepoint.  The covering map sends a path to its endpoint.  The action of $\\pi_1(X)$ takes a loop $\\gamma$ and a path $\\rho$ and gives the path obtained by following $\\gamma$ and then following $\\rho$.\nThe point is that all of this fits together very nicely:  For every covering of a space $X$, we have a canonical action of $\\pi_1(X)$, and if we have a map of two covering spaces of $X$ (a continuous map which is compatible with the projection maps), then this map is compatible with the action of $\\pi_1(X)$.  If $X$ is nice (so that the universal exists), and if we look only at connected covering spaces, then there is always a unique map of covering space from the universal covering to any other given cover.  This map is the unique map which has the property you stated in your question.\nTo go back to the original question, we have a natural action of $\\mathbb{Z}^2$ on $\\mathbb{R}^2$.  Namely, we view both as groups under addition, and the action comes from the natural inclusion.\nEvery subgroup of $\\mathbb{Z}^2$ is either isomorphic to $\\{0\\}$, $\\mathbb{Z}$, or $\\mathbb{Z}^2$, although there are many different subgroups of the latter two types, which up to automorphism correspond to orbits of points or pairs of points in $\\mathbb{Z}^2$ under the action of $\\operatorname{SL}_2(\\mathbb{Z})$.  While the quotient of any two different subgroups gives a different covering map, we have that (in this special case, with these particular actions) the  quotient of $\\mathbb{R}^2$, up to homeomorphism, by the subgroup depends only on the isomorphism class of the subgroup.  For isomorphic but non-equivalent subgroups, even though the space on top is the same, the action of $\\mathbb{Z}^2$ is different.  So in fact, this gives an example of how a group can act on a space in multiple ways.\nBut what if we don't want to use this action?  What spaces can you get? Well, then depends on what kind of action you want to take.  If you act by $(m,n).(x,y)=(-1)^m x, (-1)^n y)$, you will something that looks like the first quadrant of the plane.  If you act by $\\mathbb{Z}$ with $n.(a,b)=(2^n a,2^n b)$, the quotient is almost the torus: every point other than the original can be moved inside the the annulus between the unit circle and the circle of radius $2$, and so the quotient of the action on $\\mathbb{R}^2\\setminus \\{0\\}$ is the torus, but then you have an extra point.  You can get all sorts of interesting things if you are okay with the action not treating every point equally.  I am not sure what the classification of such quotients looks like, or even what familiar things you can get from such quotients.\nFor a nice action, where each point has the same stabilizer, and modulo this stabalizer the action is free, you will get some surface whose fundamental group is a quotient of $\\mathbb{Z}^2$, and whose universal cover is $\\mathbb{R}^2$.  The first condition rules out getting things like higher genus surfaces and the Klein bottle, whose fundamental groups are not abelian..  The second condition rules out the projective plane, whose universal cover is $S^2$.\n\nUpdate in response to the clarification request:\nWhen you have a group acting on a space, quotienting out by a subgroup will generally depend on the specific subgroup, and not just the isomorphism type of the subgroup.  This is easy to see for non-covering-space examples, where the group doesn't act in a uniform way: consider $\\mathbb{Z}^2$ acting on $\\mathbb{R}$ where the second copy of $\\mathbb{Z}$ acts trivially.  Quotienting out by one copy of $\\mathbb{Z}$ does nothing, and by the other gives you the circle.\nHowever, covering maps are nice (the action of the fundamental group of the base is uniform).  Let's just consider the case of the universal cover.\nSuppose that $H_1,H_2\\subset G=\\pi_1(X,x)$, are subgroups of the fundamental group which are abstractly isomorphic, but which are not mapped into each other by any automorphism of $G$.  For example, we could have $4\\mathbb{Z}\\subset 2\\mathbb{Z} \\subset \\mathbb{Z}$.  The bundles $\\widetilde{X}/H_i \\to X$, $i=1,2$ will usually be non-isomorphic.  For example, if $H_i$ has finite index, then $\\widetilde{X}/H_i \\to X$ will be a finite covering of degree equal to that index.  For the example $4\\mathbb{Z}\\subset 2\\mathbb{Z} \\subset \\mathbb{Z}$, the two subgroups have index $2$ and $4$.\nHowever, what if we ignore the bundle structure?  I believe that $\\widetilde{X}/H$ spaces need not be determined by the isomorphism type of $H$.  Unfortunately, in one or two dimensions, we can't get any interesting counterexamples: $\\widetilde{X}/H$ has fundamental group isomorphic go $H$, and surfaces are determined by their fundamental groups.  Additionally, up to homotopy, there are no counterexamples if $\\widetilde{X}$ is contractible,  as we would have an Eilenberg-Maclane space, which is up to homotopy determined by its fundamental group.  See this  wikipedia article  for more information on EM-spaces.\nSo is there a conterexample?  Probably, but I do not know it.  It will require at least $3$ dimensions and a covering space which is not contactable.  Still, I have no good a priori reason to believe that quotienting the universal cover by a subgroup depends only on the isomorphism type of the subgroup, and you shouldn't bu lulled into believing this is true in general until you see a theorem asserting it.  I don't even know where to begin constructing a counterexample, as the obvious algebraic invariant that might help, the quotient $G/H$, is lost when you forget the bundle structure.", "meta": {"post_id": 34238, "input_score": 44, "output_score": 40, "post_title": "Why is a covering space of a torus $T$ homeomorphic either to $\\mathbb{R}^2$, $S^1\\times\\mathbb{R}$ or $T$?"}}
{"input": "Let $\\mathbb{F}_3$ be the field with three elements. Let $n\\geq 1$. How many elements do the following groups have?\n\n$\\text{GL}_n(\\mathbb{F}_3)$\n$\\text{SL}_n(\\mathbb{F}_3)$\n\nHere GL is the general linear group, the group of invertible n\u00d7n matrices, and SL is the special linear group, the group of n\u00d7n matrices with determinant 1.", "output": "Determinant function is a surjective homomorphism from $GL(n, F)$ to $F^*$ with kernel $SL(n, F)$. Hence by the fundamental isomorphism theorem $\\frac{GL(n,F)}{SL(n,F)}$ is isomorphic to $F^*$, the multiplicative group of nonzero elements of $F$. \nThus if $F$ is finite with $p$ elements then $|GL(n,F)|=(p-1)|SL(n, F)|$.", "meta": {"post_id": 34271, "input_score": 66, "output_score": 39, "post_title": "Order of general- and special linear groups over finite fields."}}
{"input": "I'm a bachelor student of Math major. The question is about the better way to select and read maths text- or non-text- books, without considering the course and exam. (Because we follow the lecture notes during the course, and the exam is totally covered in lecture notes, so it's not an issue, but just about how to really learn)\nQuestion: \n(1) Reading the book one by one, or just focus and re-read lots of times on one certain book, and then continue with little others ? Because recently I've tried the first way, for example, to learn Calculus, I took one book, the read it from first page to the last and finish some exercises, and then immediately read another calculus book, doing the same thing, and then the third one, and so forth. After this, I realize it is really bad result, it seems that I don't master the calculus pretty well, just memorize lots of things. So, I'm thinking about whether the second way is better. Because some friends told me before, said that there're lots of good maths textbooks, I should read as much as possible. \n(2): If the second way is better, could I just use the UTM and GTM series(published by Springer, Undergraduate Texts in Mathematics) for all the maths courses ? Because there're too many, tons of books in the market, I'm not sure how to select the perfect books. But lots of people said that the UTM & GTM are two perfect series in Maths. And my teacher told me that it is better to spend more time on the books written by the famous mathematicians who are the masters in their areas. Less time on the so-called popular textbooks, like thousand-page widely-used textbooks. He's not saying that the latter one is bad, it just means that the latter one is too specified for the general course in university, passing exam , or something like that. Although it's organized well, ton's of exercises, examples and so on,  the former one is better to grab the deep insight of the field and focus more on thinking, not just knowledge. So, briefly, is it good to just use to the whole UTM series for every course in math bachelor program?", "output": "In my opinion it's much better to pick some book and read it in depth, solving many of the exercises while reading. If you read a 100 books without actually concentrating on what you're doing, it will be of no help. Of course, reading a book thoroughly and attentively is slow, so don't expect it to be quick.\nYou seem to be somewhat biased by your experience so far. Apart from the first few courses, there are no expensive, shiny, popular textbooks - only the dense, terse, substantial ones. If you really want to understand math (rather than to be just able to apply it), then the shiny books don't help you at all.\nFinally, I have no idea whether UTM/GTM books are \"enough\", but if for some reason they're all that you have access to, you're probably fine. But why limit yourself? It might happen that in a particular subject, none of the \"standard\" books are UTM/GTM, e.g. Kunen's Set Theory is Elsevier and Jech's is Springer but neither UTM/GTM. Instead of committing to UTM/GTM, just pick whatever friends or professors recommend and is actually available.", "meta": {"post_id": 35105, "input_score": 47, "output_score": 35, "post_title": "Math Major: How to read textbooks in better style or method ? And how to select best books?"}}
{"input": "The following mathematical equation was shown during the television show Fringe which aired on Friday, April 22nd. Any idea what it is?\n\n(edit by J.M.: for reference, this was Sam Weiss scribbling formulae in his notebook.)", "output": "The last formula seems to be an integral expression for the Dirichlet $\\eta$ function:\n$$\\eta(s)=\\sum_{k=1}^\\infty \\frac{(-1)^{k-1}}{k^s}\\quad \\Re(s) > 0$$\nUsing the relationship with the usual Riemann $\\zeta$ function\n$$\\eta(s)=(1-2^{1-s})\\zeta(s)$$\nand this integral expression, you get that integral in the notebook:\n$$\\eta(s) = \\frac1{\\Gamma(s)} \\int_0^{\\infty}\\frac{x^{s-1}}{e^x+1}\\mathrm dx$$\nThere is also this closely related integral (which Arturo mentions in his answer).\nThe (first part of the) second line looks to be the chain of relations relating Riemann $\\zeta$, Dirichlet $\\eta$, and Dirichlet $\\lambda$:\n$$\\frac{\\zeta(s)}{2^s}=\\frac{\\lambda(s)}{2^s-1}=\\frac{\\eta(s)}{2^s-2}$$\nIn the second part, the expressions look to be the differentiation of Dirichlet $\\eta$, but the screenshot is fuzzy around that region...", "meta": {"post_id": 35547, "input_score": 35, "output_score": 35, "post_title": "What is this mathematical equation from Fringe?"}}
{"input": "Let $f:R^n \\rightarrow R^m$ be any function. Will the graph of $f$ always have Lebesgue measure zero?\n$(1)$ I could prove that this is true if $f$ is continuous.\n$(2)$ I suspect it is true if $f$ is measurable, but I'm not sure. (My idea was to use Fubini's Theorem to integrate the indicator function of the graph, but I don't know if I'm using the Theorem properly).\nIf $(2)$ is incorrect, what would be a counterexample where the graph of $f$ has positive measure?\nIf $(2)$ is correct, can we prove the existence of a non-measurable function whose graph has positive outer measure?", "output": "No function can have a graph with  positive measure or even positive inner measure, since every function graph has uncountably many disjoint vertical translations, which cover the plane. \nMeanwhile, using the axiom of choice, there is a function whose graph has positive outer measure. The construction is easiest to see if one assumes that the Continuum Hypothesis is true, so let me assume that.\nTo begin, note first that there are only continuum many open sets in\nthe plane, since every such set is determined by a\ncountable union of basic open balls with rational center\nand rational radius. Next, it follows that the number of\n$G_\\delta$ sets is also continuum, since any such set is\ndetermined by a countable sequence of open sets, and\n$(2^{\\aleph_0})^{\\aleph_0}=2^{\\aleph_0}$.\nThus, we may enumerate the $G_\\delta$ sets in the plane as\n$A_\\alpha$ for $\\alpha\\lt \\aleph_1$ (using CH). Build a\nfunction $f:\\mathbb{R}\\to\\mathbb{R}$ by transfinite\ninduction. At any stage $\\alpha\\lt \\aleph_1$, we have\nthe approximation $f_\\alpha$ to $f$, and we assume that it\nhas been defined on only $\\alpha$ many points. Given\n$f_\\alpha$, consider the $G_\\delta$ set $A_\\alpha$. If we\ncan extend $f_\\alpha$ to a function $f_{\\alpha+1}$ by\ndefining it on one more point $x$, so that\n$(x,f_{\\alpha+1}(x))$ is outside $A_\\alpha$, then do so.\nOtherwise, $A_\\alpha$ contains the complement of\ncountably many vertical lines in the plane, and thus has\nfull measure.\nAfter this construction, extend the resulting function if\nnecessary to a total function $f:\\mathbb{R}\\to\\mathbb{R}$.\nIt now follows that the graph of $f$ is not contained in\nany $G_\\delta$ set with less than full measure. Thus, the\ngraph has full outer measure.\nNow, finally, the same construction works without CH, once you realize that any $G_\\delta$ set containing the complement of fewer than continuum many vertical lines has full measure.", "meta": {"post_id": 35606, "input_score": 42, "output_score": 51, "post_title": "Lebesgue Measure of the Graph of a Function"}}
{"input": "What can be said about the dual space of an infinite-dimensional real vector space?\nAre both spaces isomorphic? Or shall we have something like the dual of the dual is isomorphic to the initial vector space (same as with the perpendicular subspace in a Hilbert space)?", "output": "In the abstract vector space case, where \"dual space\" is the algebraic dual (the vector space of all linear functionals), a vector space is isomorphic to its (algebraic) dual if and only if it is finite dimensional.\nBill Dubuque gives a nice argument in a sci.math post (see Google Groups or MathForum)\nIf $\\mathbf{V}$ is an infinite dimensional vector space over $\\mathbf{F}$ of dimension $d$, then the cardinality of $\\mathbf{V}$ as a set is equal to $d|\\mathbf{F}|=\\max\\{d,|\\mathbf{F}|\\}$, and $\\mathbf{V}$ is isomorphic to $\\mathbf{F}^{(d)}$ (functions from a set of cardinality $d$ to $\\mathbf{F}$ with finite support), and the dual $\\mathbf{V}^*$ is isomorphic to $\\mathbf{F}^d$ (all functions from a set of cardinality $d$ to $\\mathbf{F}$), so $|\\mathbf{V}^*| = |\\mathbf{F}|^d$.\nIf the dimension of $\\mathbf{V}^*$ is $d'$, we want to show that $d'\\gt d$. Note that, as with $\\mathbf{V}$, we have $|\\mathbf{V}^*|=d'|\\mathbf{F}| = \\max\\{d',|\\mathbf{F}|\\}$.\nNow let $\\{\\mathbf{e}_n\\}$ be a countable linearly independent subset of $\\mathbf{V}$, and extend to a basis. For each $c\\in \\mathbf{F}$, $c\\neq 0$, define $\\mathbf{f}_c\\colon \\mathbf{V}\\to\\mathbf{F}$ by $\\mathbf{f}_c(\\mathbf{e}_n) = c^n$, and making $\\mathbf{f}_c$ equal to $0$ on the rest of the basis. Thet set of all $\\mathbf{f}_c$, $c\\neq 0$, is linearly independent, so we can conclude that the dimension if $\\mathbf{V}^*$ must be at least equal to $|\\mathbf{F}|$ (in the finite case, we know the dimension is at least $d\\gt |\\mathbf{F}|$). \nThat means that \n$$|\\mathbf{V}^*| = d'|\\mathbf{F}| = \\max\\{d',|\\mathbf{F}|\\} = d'.$$\nBut we also know that $|\\mathbf{V}^*| = |\\mathbf{F}|^d$. Since $d< |\\mathbf{F}|^{d}$ (since $|\\mathbf{F}|\\geq 2$), then $d' = |\\mathbf{F}|^d\\gt d$, proving that the dimension of $\\mathbf{V}^*$ is strictly larger (in the sense of cardinality) than that of $\\mathbf{V}$. \nThe isomorphism in the finite dimensional case is standard.\nSo for the algebraic dual, there is never an isomorphism in the infinite dimensional case.\nIn the Hilbert space case (or in a Banach space, or more generally a topological vector space), one usually restricts to the continuous (or bounded) functionals, so that $\\mathbf{V}^*$ denotes the bounded functionals rather than the regular functions. In that case, some spaces are topological-vector-space isomorphic to their double duals, and some not, as AD shows in his answer. The ones that are isomorphic are important enough to get their own name (reflexive). Hilbert spaces are always reflexive, and there are other classes of topological vector spaces that are always reflexive (see Wikipedia's page on reflexive spaces).", "meta": {"post_id": 35779, "input_score": 31, "output_score": 35, "post_title": "What can be said about the dual space of an infinite-dimensional real vector space?"}}
{"input": "It is changing the coordinate from one coordinate to another. There is an angle and radius on the right side. What is it? And why?\nI got:\n$2\\,\\mathrm dy\\,\\mathrm dx = r(\\cos^2\\alpha-\\sin^2\\alpha)\\,\\mathrm d\\alpha \\,\\mathrm dr$,\nwhere $x = r \\cos(\\alpha)$ and $y = r \\sin(\\alpha)$.\nbut cannot understand and get the right side. The problem emerged when trying to integrate $\\displaystyle \\int_0^\\infty e^{\\frac{-x^2}{n^2}}\\,\\mathrm dx$ where I tried to change the problem knowing $r^2=x^2+y^2$ but stuck to this part. What is the change in the title called and why is it so?", "output": "Let us approach this problem like some physicists would, that is, let us consider the differential $\\mathrm{d}$ as an operator which obeys the rules of derivation, only with a sign. \nIn the present case, $x=r\\cos\\alpha$ and $y=r\\sin\\alpha$ hence\n$$\n\\mathrm{d}x=\\cos\\alpha\\mathrm{d}r-r\\sin\\alpha\\mathrm{d}\\alpha,\n\\quad\n\\mathrm{d}y=\\sin\\alpha\\mathrm{d}r+r\\cos\\alpha\\mathrm{d}\\alpha.\n$$\nNow, there are some magic rules which allow to multiply $\\mathrm{d}r$ and $\\mathrm{d}\\alpha$ elements. These are\n$$\n\\mathrm{d}r\\mathrm{d}r=0,\\quad\n\\mathrm{d}\\alpha\\mathrm{d}r=-\\mathrm{d}r\\mathrm{d}\\alpha,\\quad\n\\mathrm{d}\\alpha\\mathrm{d}\\alpha=0.\n$$\nHence,\n$$\n\\mathrm{d}x\\mathrm{d}y=(\\cos\\alpha\\mathrm{d}r-r\\sin\\alpha\\mathrm{d}\\alpha)(\\sin\\alpha\\mathrm{d}r+r\\cos\\alpha\\mathrm{d}\\alpha).\n$$\nThe $\\mathrm{d}r\\mathrm{d}r$ and $\\mathrm{d}\\alpha\\mathrm{d}\\alpha$ terms disappear and the terms which interest us are the $\\mathrm{d}r\\mathrm{d}\\alpha$ and $\\mathrm{d}\\alpha\\mathrm{d}r$ ones. One gets\n$$\n\\mathrm{d}x\\mathrm{d}y=(\\cos\\alpha\\cdot r\\cos\\alpha-r\\sin\\alpha\\cdot (-1)\\sin\\alpha)\\mathrm{d}r\\mathrm{d}\\alpha,\n$$\nthat is,\n$$\n\\color{green}{\\mathrm{d}x\\mathrm{d}y=r\\mathrm{d}r\\mathrm{d}\\alpha},\n$$\na formula which yields\n$$\n\\color{red}{\\iint f(x,y)\\mathrm{d}x\\mathrm{d}y=\\iint f(r\\cos\\alpha,r\\sin\\alpha)r\\mathrm{d}r\\mathrm{d}\\alpha}.\n$$\nOne can also transform integrals the other way round, all there is to do is to compute a formula for $\\mathrm{d}r\\mathrm{d}\\alpha$ as a multiple of $\\mathrm{d}x\\mathrm{d}y$. Our formula for $\\mathrm{d}x\\mathrm{d}y$ in terms of $\\mathrm{d}r\\mathrm{d}\\alpha$ yields\n$$\n\\mathrm{d}r\\mathrm{d}\\alpha=\\frac1r\\mathrm{d}x\\mathrm{d}y=\\frac1{\\sqrt{x^2+y^2}}\\mathrm{d}x\\mathrm{d}y,\n$$\nhence\n$$\n\\color{blue}{\\iint f(r,\\alpha)\\mathrm{d}r\\mathrm{d}\\alpha=\\iint f(x,y)\\frac1{\\sqrt{x^2+y^2}}\\mathrm{d}x\\mathrm{d}y}.\n$$\nOnce again, this only describes the computational side of the story, but this recipe is supported by a well established theory of differential forms which we omitted.", "meta": {"post_id": 37044, "input_score": 47, "output_score": 36, "post_title": "Explain $\\iint \\mathrm dx\\,\\mathrm dy = \\iint r \\,\\mathrm \\,d\\alpha\\,\\mathrm dr$"}}
{"input": "While going through Probability: Theory and Examples by Rick Durrett (4th edition, p.9), I came across the familiar definition of $\\sigma$-algebras where, if $A_i \\in \\mathcal{F}$ is a countable sequence of sets for some $\\sigma$-algebra $\\mathcal{F}$ and $\\cup_i A_i \\in \\mathcal{F}$ by definition, then it follows that $\\cap_i A_i^C \\in \\mathcal{F}$ by de Morgan's law.\nThat's when it occurred to me that I had never seen a proof that de Morgan's law holds over a countably infinite number of sets. I don't have my measure theory/probably theory books with me right now, but I'm quite sure that I've never seen any of them prove this before extending $\\sigma$-algebras to countable union or intersection, depending on which definition it started with.\nOn the one hand, it seems obvious that it would hold. On the other hand, seeming obvious is not a proof, especially when it comes to something involving infinity.\nI can imagine an inductive proof where I\n\nassume de Morgan's law holds for an index set of size $n$\nThen prove that it holds for an index set of size $n+1$\n\nand wrap it up by $n \\rightarrow \\infty$ but I'm not convinced that's right. For example, an argument like that doesn't work for countable intersection being closed on a collection of open sets.\nSo what's a good proof that can extend de Morgan's law to an infinite collection of sets.", "output": "The result holds for every family, countable or not, of sets $A(i)$ and it is a simple matter of logic.\nTo wit, the assertion \"$x$ belongs to the union\" means \"There exists $i$ such that $x$ belongs to $A(i)$\" hence its negation \"$x$ belongs to the complement of the union\" is also \"For all $i$, $x$ does not belong to $A(i)$\", that is, \"For all $i$, $x$ belongs to the complement of $A(i)$\". We are done.", "meta": {"post_id": 37045, "input_score": 43, "output_score": 51, "post_title": "De Morgan's law on infinite unions and intersections"}}
{"input": "I know little about number theory even after reading its Wikipedia article. \n\nI was wondering if the subjects\nstudied by number theory is only\nabout integers or even more\nrestrictedly natural numbers or\nprime numbers, and how they are related to other types of numbers such as rational and irrational numbers?\nIn number theory, are the real\nnumbers also studied? If yes, what aspects of real numbers are studied in number theory? \nAre real numbers studied just as\ngeneralization from integers and\nrational numbers? Or are they also\nstudied standalone independently of\nintegers and rational numbers?\nIf they are also studied\nindependently of integers and\nrational numbers, is the set of real\nnumbers, $\\mathbb{R}$, regarded as a\n1-dim Hilbert space with dot product\nbeing inner product and the induced\ntopology, just as how it is studied\nin real analysis? Is $\\mathbb{R}^n,\r\n    n \\in \\mathbb{N}, n>1$ studied in\nnumber theory? Can I say what is studied in real analysis about real numbers is part of number theory?\nWhen studied independently of\nintegers and rational numbers, is\n$\\mathbb{R}$ regarded  as a special\nkind of algebraic structure studied\nin abstract algebra?\nSimilar questions for complex\nnumbers, to those for real numbers\nin 2. Especially are there differences between how complex numbers are studied in complex analysis, in algebra, and in number theory? \n\nThanks and regards!", "output": "Number theorists study a range of different questions that are loosely inspired by questions related to integers and rational numbers.\nHere are some basic topics:\n\nDistribution of primes: The archetypal result here is the prime number theorem,\nstating that the number of primes $\\leq x$ is asymptotically $x/\\log x$.  Another basic result is Dirichlet's theorem on primes in arithmetic progression.  More recently, one has the results of Ben Green and Terry Tao on solving linear equations (with $\\mathbb Z$-coefficients, say) in primes.   Important open problems are Goldbach's conjecture, the twin prime conjecture, and questions about solving non-linear equations in primes (e.g. are there infinitely many primes of the form $n^2 + 1$).  The Riemann hypothesis (one of the Clay Institute's Millennium Problems) also fits in here.\nDiophantine equations: The basic problem here is to solve polynomial equations (e.g. with $\\mathbb Z$-coefficients) in integers or rational numbers.\nOne famous problem here is Fermat's Last Theorem (finally solved by Wiles).  The theory of elliptic curves over $\\mathbb Q$ fits in here. The Birch-Swinnerton-Dyer conjecture (another one of the Clay Institute's Millennium Problems) is a famous open problem about elliptic curves.  Mordell's conjecture, proved by Faltings (for which he got the Fields medal) is a famous result. One can also study Diophantine equations mod $p$ (for a prime $p$).  The Weil conjectures were a famous problem related to this latter topic, and both Grothendieck and Deligne received Fields medals in part for their work on proving the Weil conjectures.\nReciprocity laws: The law of quadratic reciprocity is the beginning result here, but there were many generalizations worked out in the 19th century, culminating in the development of class field theory in the first half of the 20th century.  The Langlands program is in part about the development of non-abelian reciprocity laws.  \nBehaviour of arithmetic functions: A typical question here would be to investigate behaviour of functions such as $d(n)$ (the function which counts the number of divisors of a natural number $n$).  These functions often behave quite irregularly, but one can study their asymptotic behaviour, or the behaviour on average.    \nDiophantine approximation and transcendence theory: The goal of this area is to establish results about whether certain numbers are irrational or transcendental, and also to investigate how well various irrational numbers can be approximated by rational numbers. (This latter problem is the problem of Diophantine approximation).  Some results are Liouville's construction of the first known transcendental number, transcendence results about $e$ and $\\pi$, and Roth's theorem on Diophantine approximation (for which he got the Fields medal).  \nThe theory of modular (or more generally automorphic) forms: This is an area which grew out of the development of the theory of elliptic functions by Jacobi, but which has always had a strong number-theoretic flavour. The modern theory is highly influenced by ideas of Langlands.\nThe theory of lattices and quadratic forms: The problem of studying quadratic forms goes back at least to the four-squares theorem of Lagrange, and binary quadratic forms were one of the central topics of Gauss's Disquitiones.  In its modern form, it ranges from questions such as representing integers by quadratic forms, to studying lattices with good packing properties. \nAlgebraic number theory: This is concerned with studying properties and invariants of algebraic number fields (i.e. finite extensions of $\\mathbb Q$) and their rings of integers.\n\nThere are more topics than just these; these are the ones that came to mind.  Also, these topics are all interrelated in various ways.  For example, the prime counting function is an example of one of the arithmetic functions mentioned in (4), and so (1) and (4) are related.    As another example, $\\zeta$-functions and $L$-functions are basic tools in the study of primes, and also in the study of Diophantine equations, reciprocity laws, and automorphic forms; this gives a common link between (1), (2), (3), and (6).  As a third, a basic tool for studying quadratic forms is the associated theta-function; this relates (6) and (7).   And reciprocity laws, Diophantine equations, and automorphic forms are all related, not just by their common use of $L$-functions, but by a deep web of conjectures (e.g. the BSD conjecture, and Langlands's conjectures).  As yet another example, Diophantine approximation can be an important tool in studying and solving Diophantine equations; thus (2) and (5) are related.  Finally, algebraic number theory was essentially invented by Kummer, building on old work of Gauss and Eisenstein, to study reciprocity laws, and also Fermat's Last Theorem.  Thus there have always been, and continue to be, very strong relations between topics (2), (3), and (8).  \nA general rule in number theory, as in all of mathematics, is that it is very difficult to separate important results, techniques, and ideas neatly into distinct areas.  For example, $\\zeta$- and $L$-functions are analytic functions, but they are basic tools not only in traditional areas of analytic number theory such as (1), but also in areas thought of as being more algebraic, such as (2), (3), and (8).  Although some of the areas mentioned above are more closely related to one another than others, they are all linked in various ways (as I have tried to indicate).\n[Note: There are Wikipedia entries on many of the topics mentioned above, as well as quite a number of questions and answers on this site.  I might add links at some point, but they are not too hard to find in any event.]", "meta": {"post_id": 37648, "input_score": 45, "output_score": 54, "post_title": "Subjects studied in number theory"}}
{"input": "Here it is :\n$$\r\n\\frac{\\mathrm d}{\\mathrm dx}\\left( \\int_{\\cos x}^{\\sin x}{\\sin \\left( t^3 \\right)\\mathrm dt} \\right)\r\n$$\nI've got the answer but I don't know how to start , what to do ?\nHere is the answer : \n$\r\n\\sin \\left( \\sin^3 x \\right)\\cos x + \\sin \\left( \\cos ^{3}x \\right)\\sin x\r\n$\nSo first I calculate the primitive and then I derivate it. But I don't know how to integrate. Should I use 'substitution' method ? I tried but then i was blocked...", "output": "I understand from the comments that you are not completely pleased with the answers so far. That's why I try it (with a bit delay). Note that there is nothing new in this answer ...\nAll you need to know is the fundamental theorem of calculus\n$$f(x) = \\frac{d}{dx} F(x)$$\nwith\n$$F(x) = \\int^x_a f(t) dt$$\nand the chain rule\n$$\\frac{d}{dx} f[g(x)] = f'[g(x)] g'(x).$$\nYour integral is given by\n$$ \\int_{\\cos x}^{\\sin x}{\\sin ( t^3) \\,dt} =F(\\sin x) - F(\\cos x)$$\nwith $$F(x) = \\int_a^x f(t) dt$$\nand $f(t)=\\sin(t^3)$.\nTherefore,\n$$ \\frac{d}{dx}\\left[ \\int_{\\cos x}^{\\sin x}{\\sin ( t^3 ) dt} \\right]\n = \\frac{d}{dx} [F(\\sin x) - F(\\cos x)]\n = F'(\\sin x) \\sin' x - F'(\\cos x) \\cos' x$$\n$$ = f(\\sin x) \\cos x + f(\\cos x) \\sin x = \\sin ( \\sin^3 x) \\cos x + \\sin (\\cos^3 x) \\sin x.$$", "meta": {"post_id": 37656, "input_score": 23, "output_score": 38, "post_title": "How to calculate the derivative of this integral?"}}
{"input": "This is a follow-up question on this one. The answers to my questions made things a lot clearer to me (Thank you for that!), yet there is some point that still bothers me.\nThis time I am making things more concrete: I am esp. interested in the difference between a metric and a norm. I understand that the metric gives the distance between two points as a real number. The norm gives the length of a a vector as a real number (see def. e.g. here). I further understand that all normed spaces are metric spaces (for a norm induces a metric) but not the other way around (please correct me if I am wrong).\nHere I am only talking about vector spaces. As an example lets talk about Euclidean distance and Euclidean norm. Wikipedia says:\n\nA vector can be described as a\n  directed line segment from the origin\n  of the Euclidean space (vector tail),\n  to a point in that space (vector tip).\n  If we consider that its length is\n  actually the distance from its tail to\n  its tip, it becomes clear that the\n  Euclidean norm of a vector is just a\n  special case of Euclidean distance:\n  the Euclidean distance between its\n  tail and its tip.\n\nWhat confuses me is that they seem to be having it backwards: The Euclidean metric induces the Euclidean norm: You measure the distance between tip and tail and get the length out of that. What makes my confusion complete is that $L^2$ distance is also called the Euclidean norm (see here).\nI would very much appreciate it if somebody could clear the haze.", "output": "The metric $d(u,v)$ induced by a vector space norm has additional properties that are not true of general metrics. These are:\nTranslation Invariance: $d(u+w,v+w)=d(u,v)$\nScaling Property: For any real number $t$, $d(tu,tv)=|t|d(u,v)$.\nConversely, if a metric has the above properties, then $d(u,0)$ is a norm.\nMore informally, the metric induced by a norm \"plays nicely\" with the vector space structure.  The usual metric on $\\mathbb{R}^n$ has the two properties mentioned above.  But there are metrics on $\\mathbb{R}^n$ that are topologically equivalent to the usual metric, but not translation invariant, and so are not induced by a norm.", "meta": {"post_id": 38634, "input_score": 88, "output_score": 108, "post_title": "Difference between metric and norm made concrete: The case of Euclid"}}
{"input": "What does it mean to talk about the \"irreducible representatives of SO(3)\"?  I'm struggling to understand the concept of irreducible representations.  Could someone give a concrete example for someone unfamiliar with group theory or representation theory?", "output": "A representation of the group $G$ means a homomorphism from $G$ into the group of automorphisms of a vector space $\\mathbf{V}$. Essentially, you are trying to interpret each element of $G$ as an invertible linear transformation $\\mathbf{V}\\to\\mathbf{V}$, in order to try to understand the group $G$ by how it \"acts on $\\mathbf{V}$.\" \nIf you have an action $\\rho_1$ of $G$ on a vector space $\\mathbf{W}$ (that is, one representation), and you have some other action $\\rho_2$ of $G$ on another vector space $\\mathbf{Z}$ (another representation), then you can use these two actions to construct an action of $G$ on the vector space $\\mathbf{W}\\oplus\\mathbf{Z}$: just let $G$ act on the first coordinate using the old action on $\\mathbf{W}$, and let it act on the second coordinate using the old action on $\\mathbf{Z}$. \nThe point to observe, however, is that the action of $G$ on $\\mathbf{W}\\oplus \\mathbf{Z}$ defined this way does not give you any new insights into the structure of $G$: anything you can glean about $G$ from this action, you can learn about $G$ by considering the original actions $\\rho_1$ and $\\rho_2$. So this new action does not give us anything new.\nConversely, suppose you have one representation $\\rho$, with $G$ acting on $\\mathbf{V}$, and that there are proper subspaces $\\mathbf{W}$ and $\\mathbf{Z}$ of $\\mathbf{V}$ that satisfy the following properties:\n\n$\\mathbf{V}=\\mathbf{W}\\oplus\\mathbf{Z}$; and\nThe action of every $g\\in G$ on $\\mathbf{V}$ maps $\\mathbf{W}$ to itself; and\nThe action of every $g\\in G$ on $\\mathbf{V}$ maps $\\mathbf{Z}$ to itself.\n\nThen you can look at the restriction of the action of $G$ on $\\mathbf{W}$ to get a representation, and the restriction on $\\mathbf{Z}$ to get another representation; and these two representations will give you all the information from the original representation, the same way we had before. The advantage being that since $\\mathbf{W}$ and $\\mathbf{Z}$ are proper subspaces of $\\mathbf{V}$, they have smaller dimension and, presumably, it's easier to understand a subgroup of linear automorphisms for them than for $\\mathbf{V}$.\nSo the moral is that we want to find representations that cannot be \"broken up\" into smaller ones, because there's no point in trying to understand ones that do break up, we can focus our attention on those that don't, because all the other representations can be built up in terms of the ones that cannot be broken up.\nThe irreducible representations are precisely the ones that cannot be broken up into smaller pieces (at least for finite groups). Maschke's Theorem says that if you have a representation $\\rho$ of a finite group $G$ acting on $\\mathbf{V}$, and $\\mathbf{W}$ is a subspace of $\\mathbf{V}$ such that for all $g\\in G$, the image of $\\mathbf{W}$ under the action of $g$ is $\\mathbf{W}$ itself, then you can find a subspace $\\mathbf{Z}$ of $\\mathbf{V}$ such that $\\mathbf{V}=\\mathbf{W}\\oplus\\mathbf{Z}$ and every $g\\in G$ maps $\\mathbf{Z}$ to itself (that is, in order to break up $\\rho$ into two smaller pieces, it is enough to find a single proper piece on which $\\rho$ acts; then you can find a complement for it). \nWith this in mind, we say:\n\nLet $\\rho\\colon G\\to \\mathrm{Aut}(\\mathbf{V})$ be a representation of $G$. We say that $\\rho$ is irreducible if and only if $\\mathbf{V}$ is not the zero vector space, and the only subspaces of $\\mathbf{V}$ that are mapped to themselves under the action of every $g\\in G$ are $\\{\\mathbf{0}\\}$ and $\\mathbf{V}$ itself.\n\nAn irreducible representation of $SO(3)$ will be a representation of $SO(3)$ that is irreducible. $SO(3)$ acts naturally on the vector space $\\mathbb{R}^3$: it consists of all automorphisms of $\\mathbb{R}^3$ that respect the inner product, so this is itself a representation of $SO(3)$ (which is irreducible, because no proper subspace of $\\mathbb{R}^3$ is sent to itself by all elements of $SO(3)$). \n(Maschke's Theorem holds if the vector space is over a field of characteristic $0$, or if the characteristic does not divide the order of the group; there are similar theorems for certain kinds of infinite groups, but it does not hold for arbitrary infinite groups in general.)", "meta": {"post_id": 38958, "input_score": 37, "output_score": 85, "post_title": "What is the meaning of an \"irreducible representation\"?"}}
{"input": "Let's say I have one point that will be taken randomly from a normal distribution with mean $\\mu_1$ and standard deviation $\\sigma_1$.  Let's say I have another point that is taken much in the same way from another normal distribution with mean $\\mu_2$ and standard deviation $\\sigma_2$;.\nHow can I compute the probability, given $\\mu_1$, $\\mu_2$, $\\sigma_1$, and $\\sigma_2$, that my first point will be larger than the second?\nI am sort of interested in the reasoning behind an \"analytic\" answer (or as analytic as you can possibly get with the normal distribution, which isn't that much), but I am more importantly looking for an algorithm of computing this probability, as it will be used in a simulation/model.\nDoes anyone know where I could get started on reasoning through this?\nNote: For actual computation, having a table of values of the % of the curve within a given multiple of the standard deviation is feasible in my situation.", "output": "Suppose that $X_1 \\sim {\\rm N}(\\mu_1,\\sigma_1^2)$ and $X_2 \\sim {\\rm N}(\\mu_2,\\sigma_2^2)$ are independent. Then,\n$$\r\n{\\rm P}(X_1  > X_2 ) = {\\rm P}(X_1  - X_2  > 0) = 1 - {\\rm P}(X_1  - X_2  \\le 0).\r\n$$\nNow, by independence, $X_1 - X_2$ is normally distributed with mean\n$$\r\n\\mu := {\\rm E}(X_1 - X_2) = \\mu_1 - \\mu_2\r\n$$\nand variance\n$$\r\n\\sigma^2 := {\\rm Var}(X_1 - X_2) = \\sigma_1^2 + \\sigma_2^2.\r\n$$\nHence, \n$$\r\n\\frac{{X_1  - X_2  - \\mu}}{{\\sigma}} \\sim {\\rm N}(0,1),\r\n$$\nand so\n$$\r\n{\\rm P}(X_1  - X_2  \\le 0) = {\\rm P}\\bigg(\\frac{{X_1  - X_2  - \\mu }}{\\sigma } \\le \\frac{{0 - \\mu }}{\\sigma }\\bigg) = \\Phi \\Big(  \\frac{-\\mu }{\\sigma }\\Big),\r\n$$\nwhere $\\Phi$ is the distribution function of the ${\\rm N}(0,1)$ distribution. Thus,\n$$\r\n{\\rm P}(X_1  > X_2 )  = 1 - {\\rm P}(X_1  - X_2  \\le 0) = 1 - \\Phi \\Big(  \\frac{-\\mu }{\\sigma }\\Big).\r\n$$", "meta": {"post_id": 40224, "input_score": 32, "output_score": 42, "post_title": "Probability of a point taken from a certain normal distribution will be greater than a point taken from another?"}}
{"input": "Suppose $F$ is a field s.t $\\left|F\\right|=q$. Take $p$ to be some prime. How many monic irreducible polynomials of degree $p$ do exist over $F$?\n\nThanks!", "output": "The number of such polynomials is exactly $\\displaystyle \\frac{q^{p}-q}{p}$ and this is the proof:\nThe two main facts which we use (and which I will not prove here) are that $\\mathbb{F}_{q^{p}}$ is the splitting field of the polynomial $g\\left(x\\right)=x^{q^{p}}-x$, and that every monic irreducible polynomial of degree $p$ divides $g$.\nNow: $\\left|\\mathbb{F}_{q^{p}}:\\mathbb{F}_{q}\\right|=p$ and therefore there could be no sub-extensions. Therefore, every irreducible polynomial that divides $g$ must be of degree $p$ or 1. \nSince each linear polynomial over $\\mathbb{F}_{q}$ divides $g$ (since for each $a\\in \\mathbb{F}_{q}$, $g(a)=0$), and from the fact that $g$ has distinct roots, we have exactly $q$ different linear polynomials that divide $g$. \nMultiplying all the irreducible monic polynomials that divide $g$ will give us $g$, and therefore summing up their degrees will give us $q^{p}$.\nSo, if we denote the number of monic irreducible polynomials of degree $p$ by $k$  (which is the number we want), we get that $kp+q=q^{p}$, i.e $\\displaystyle k=\\frac{q^{p}-q}{p}$.", "meta": {"post_id": 40811, "input_score": 65, "output_score": 76, "post_title": "Number of monic irreducible polynomials of prime degree $p$ over finite fields"}}
{"input": "[This question involves mostly math papers, and may be relevant to graduate students learning to write and cite papers, although this is my only justification for this being a math question.]\nUsually papers start out with the title and then the author. Sometimes near the author's name there is the phrase \"Communicated by John Doe\". Does this mean John Doe told the author the essentials of the paper and then the author wrote down the details? If so, why isn't John Doe just a co-author? Or does this mean something else?\nI could not find the answer using Google.\nThank you.", "output": "I'm posting my comment as an answer as Zev and amWhy asked me to do so:\nIt usually means that John Doe was the editor in charge. That is: John Doe received the submission by the author(s), contacted the referees and informed the board of editors about his and the referees' opinion. John Doe thus takes some sort of responsibility on the paper. His name is associated with it and it's his choice of the referees and his judgment of their opinion that ultimately led to the paper's publication. However, John Doe doesn't contribute a single line to the paper itself. Usually, the communicator is a rather senior and well-established mathematician.\nTwo journals that systematically use the \"communicated by ...\" stamp are, among many others:\n\nJournal of Algebra\nComptes Rendus de l'Acad\u00e9mie des sciences\n\nThe CRAS are a journal that (at least historically) belongs to the category that Zev mentions in his answer. However, the Journal of Algebra shows that this is not necessarily the case. The explanation I'm giving tries to cover both cases.", "meta": {"post_id": 41871, "input_score": 66, "output_score": 70, "post_title": "What does \"communicated by\" mean in math papers?"}}
{"input": "So, given a topological space $S$ we can construct its Borel sigma-algebra $\\mathcal{B}(S)$. Does it mean that we can construct a measure $\\mu$ on this sigma-algebra as well? Say, discrete topology on the circle implies $\\mathcal{B}(S) = 2^S$, and hence we know that we cannot construct a measure. \nThis leads to the idea that certainly we need some restrictions on the topological space. What are useful sufficient conditions?\nI am especially interested in the conditions like\n- separable;\n- second countable;\n- metrizable;", "output": "This question is an oldie, but I feel that it deserves a more elaborate answer, so here goes.\nAs you said, to every topological space $X$ one can associate the Borel $\\sigma$-algebra $\\mathcal{B}_X$, which is the $\\sigma$-algebra generated by all open sets in $X$. Now $(X,\\mathcal{B}_X)$ is a measurable space and it is desirable to find a natural Borel measure on it. By Borel measure I simply mean a measure defined on $\\mathcal{B}_X$ and by \"natural\" I mean that it should be compatible with the topology of $X$ in some sense (otherwise, $X$ is just an abstract set). There are several compatibility conditions one can impose, which are motivated from the fact that the Euclidean spaces (with Lebesgue measure) satisfy all of them. The following are most often encountered:\n\nStrict positivity: this means that the measure of every nonempty open set is positive.\nLocal finiteness: this means that every point has some open neighborhood $V$ with $\\mu(V) < \\infty$.\nFiniteness on compacta: this means that $\\mu (K) < \\infty$ for every compact set $K \\subset X$.\nOuter regularity: This means that the measure of every Borel set $E \\subset X$ is equal to the infimum of $\\mu (G)$ over all open sets $G \\subset X$ which contain $E$.\n(Weak) inner regularity: This means that the measure of every Borel set $E \\subset X$ is equal to the supremum of $\\mu (C)$ over all closed sets $C \\subset X$ which are contained in $E$.\n(Strong) inner regularity, also known as tightness: The measure of every Borel set $E \\subset X$ is equal to the supremum of $\\mu (K)$ over all compact sets $K \\subset X$ which are contained in $E$.\nRegularity: this is just outer regularity + (strong) inner regularity.\nRadon: $\\mu$ is Radon if it is satisfies a certain combination of the above properties. Different texts often use different combinations, which often coincide if the topological space $X$ is nice enough (certainly if it is, say, $[0,1]$). One popular combination is \"Radon = locally finite + inner regular\" but some texts omit the inner regularity hypothesis, some replace it with outer regularity, some add outer regularity, etc. . The thing to keep in mind is that a Radon measure is a Borel measure which has some nice relation to the topology, but not necessarily every relation you want it to have (especially if the topology is somewhat ill-behaved).\n\nIn addition, there are some general measure-theoretical conditions one can impose on $\\mu$ which ensure that it obeys to some general measure-theoretical theorems (e.g. Fubini). The most useful ones are:\n\n$\\sigma$-finiteness: this means that $X$ can be written as a countable union of Borel sets such that each of these sets has finite measure.\nFiniteness:  $\\mu (X) < \\infty$.\n\nAlso, you probably want $\\mu$ to be nondegenerate ($\\mu \\ne 0$).\nThe above conditions are certainly not mutually independent and even for the most general topological spaces there are some obvious implications. For instance:\n\nStrict positivity implies nondegeneracy.\nIf $X$ is Hausdorff then strong inner regularity implies weak inner regularity.\nIf $X$ is Hausdorff and locally compact then finiteness on compacta implies local finiteness.\nIf $X$ is $\\sigma$-compact (i.e. equal to a countable union of compact subsets) and $\\mu$ is finite on compacta, then $\\mu$ is $\\sigma$-finite.\nIf $\\mu$ is finite then outer regularity is equivalent to (weak) inner regularity.\netc.\n\nIn the highest generality (arbitrary topological spaces), there is almost nothing nontrivial one can say, and one may not be able to find any nice Borel measure on the space. Therefore one restricts to some nice class of topological spaces, in which an interesting theory can be developed. Among them we have:\n\n$\\sigma$-compact, locally compact Hausdorff spaces. On such spaces there are strictly positive, finite on compacta, regular measures (which are then also $\\sigma$-finite, and finite if the space is compact). In fact, there are plenty of them. A theorem of Riesz asserts that in this case, Borel measures on $X$ are in 1-1 correspondence with positive linear functionals on $C_c (X)$(the normed space of all continuous real-valued functions on $X$ with compact support) and Borel measures satisfying the above regularity conditions. Every such functional acts as integration against a measure on $X$ satisfying the above.\nPolish spaces, which are topological spaces homeomorphic to separable, complete metric spaces, e.g. infinite dimensional Banach spaces and countably infinite products of finite discrete spaces. In such spaces it is often difficult to find a geometrically appealing Borel measure, especially if you want the measure to be invariant under many isometries of a given metric. But as Tim mentioned, they frequently appear in functional analysis and probability theory, and so find their uses.\n\nAlmost any space which shows up in applications belongs to one of these two classes. For instance, real (and p-adic?) Lie groups and their homogeneous spaces always belong to the first class.\nHere are some references: for the theory of measures on locally compact topological spaces, there's a book called \"Measure and Integration\" by K\u00f6nig. It is somewhat technical but very general. For Polish spaces, there's a very nice book by Parthasarathy called \"Probability Measures on Metric Spaces\". \"Handbook of measure theory\" probably discusses these subjects in length too.", "meta": {"post_id": 43601, "input_score": 25, "output_score": 39, "post_title": "Measure on topological spaces"}}
{"input": "I'm working on an exercise from Atiyah and MacDonald's Commutative Algebra, and have hit a bump on Exercise 14 of Chapter 1. \n\nIn a ring $A$, let $\\Sigma$ be the set of all ideals in which every element is a zero-divisor. Show that set $\\Sigma$ has maximal elements and that every maximal element of $\\Sigma$ is a prime ideal. Hence the set of zero-divisors in $A$ is a union of prime ideals.\n\nI see by an application of Zorn's Lemma that $\\Sigma$ has maximal elements. I take $\\mathfrak{m}$ to be maximal in $\\Sigma$, with $xy\\in\\mathfrak{m}$. Since $xy$ is a zero divisor, $xyz=0$ for some $z\\neq 0$. If $yz=0$, then $y$ is a zero divisor, otherwise $x$ is a zero divisor. So I guess I then want to show $x\\in\\Sigma$ or $y\\in\\Sigma$. If neither is, then $\\mathfrak{m}$ is properly contained in both $(\\mathfrak{m},x)$ and $(\\mathfrak{m},y)$. However, I'm not sure how to show either of these ideals is again in $\\Sigma$. \nIf my understanding is correct, elements of $(\\mathfrak{m},x)$ are finite sums of the form $\\sum_ia_im_i+bx$ for $a_i\\in A$, $m_i\\in\\mathfrak{m}$ and $b\\in A$. To show this sum is a zero divisor, my hunch is that if $c_im_i=0$ for $c_i\\neq 0$ and $dx=0$ for $d\\neq 0$, then \n$$\r\n\\left(\\sum_ia_im_i+bx\\right)(d\\prod_ic_i)=0.\r\n$$\nMy concern is that perhaps $d\\prod_ic_i=0$, so the above wouldn't show that $(\\mathfrak{m},x)$ consists of only zero divisors. How can I get around this? Or is there perhaps a better approach? Thank you for your help.", "output": "Let $\\frak{m}$ be a maximal element in $\\Sigma$. We want to show it is prime, i.e. that if $x\\notin\\frak{m}$ and $y\\notin\\frak{m}$, then $xy\\notin\\frak{m}$. \nIf $x\\notin\\frak{m}$ and $y\\notin\\frak{m}$, then ${\\frak{m}}+(x)$ and ${\\frak{m}}+(y)$ are both ideals of $A$ that strictly contain $\\frak{m}$, and therefore each must contain non-zero-divisors ($\\frak{m}$ is maximal among ideals consisting only of zero-divisors, so any ideal strictly containing $\\frak{m}$ cannot consist only of zero-divisors). Thus the ideal $({\\frak{m}}+(x))({\\frak{m}}+(y))\\subseteq{\\frak{m}}+(xy)$ contains non-zero-divisors (because there is at least one non-zero-divisor in each of ${\\frak{m}}+(x)$ and ${\\frak{m}}+(y)$, and the product of two non-zero-divisors is a non-zero-divisor). But the fact that ${\\frak{m}}+(xy)$ contains non-zero-divisors implies that ${\\frak{m}}+(xy)$ strictly contains $\\frak{m}$, hence $xy\\notin\\frak{m}$. Thus $\\frak{m}$ is prime.", "meta": {"post_id": 44481, "input_score": 45, "output_score": 47, "post_title": "Showing the set of zero-divisors is a union of prime ideals"}}
{"input": "Let's fix some terminology first. A category $\\mathcal{C}$ is preabelian if:\n1) $Hom_{\\mathcal{C}}(A,B)$ is an abelian group for every $A,B$ such that composition is biadditive,\n2) $\\mathcal{C}$ has a zero object,\n3) $\\mathcal{C}$ has binary products,\n4) $\\mathcal{C}$ has kernels and cokernels.\nA category $\\mathcal{C}$ is abelian if it is preabelian and satisfies:\n5) every monomorphism is a kernel and every epimorphism is a cokernel.\nDefine the coimage of a map to be the cokernel of its kernel, and the image to be the kernel of its cokernel. We have the following commutative diagram:\n\nwhere $\\overline{f}$ is the only existing map (because of universality of kernel and cokernel).\nI'm having trouble proving the following:\n\nA preabelian category $\\mathcal{C}$ is abelian iff $\\overline{f}$ is an isomorphism.\n\nThe converse is easily shown, I'm having trouble proving $\\Rightarrow$...", "output": "Here is an argument for $\\Rightarrow$. There is not much more to it than chasing diagrams (as it should be). Also, I didn't really bother to check which (parts of the) axioms are actually needed:\n\nIn presence of 1),2),3) we have that $\\mathcal{C}$ has biproducts as well: every binary coproduct is also a binary product. (This is not used below but I added it for the sake of completeness)\nAssuming 1)-5), an epi $e:B \\to C$ is the cokernel of its kernel. \nIndeed, let $f$ be a morphism such that $e = \\operatorname{coker}\\,{f}$ and let $k = \\operatorname{ker}{e}$. Since $ef = 0$, we see that $f = kf'$. If $y$ is such that $yk =0$ then $ykf' = yf = 0$ and hence $y = y'e$,  and thus $e$ is a cokernel of $k$. Dually, a mono is the kernel of its cokernel.\nAssuming 1)-5) a morphism which is both an epimorphism and a monomorphism is an isomorphism. I leave that as an easy exercise (I gave the argument in the comments above).\nLet $f: A \\to B$. The morphism $i: \\operatorname{Coim}{f} \\to B$ is monic. To this end, let $x: X \\to \\operatorname{Coim}{f}$ be such that $ix = 0$. Let $q = \\operatorname{coker}{x}$ and let $j: \\operatorname{Coker}{x} \\to B$ be the unique map such that $i = jq$. Since $qp$ is epi we have a morphism $h: H \\to A$ such that $qp = \\operatorname{coker}{{h}}$. Now $fh = iph = jqph = 0$ so $h = kh'$. This gives that $ph = pkh' = 0$, \n\nso $p$ factors as $p = p'(qp) = (p'q)p$. But $p$ is epi, so $p'q = 1_{\\operatorname{Coim}{f}}$. This implies that $q$ is a monomorphism and finally $qx = 0$ implies that $x = 0$. We have shown that $ix = 0$ implies $x = 0$ and thus $i$ is a monomorphism.\nDually $j: A \\to \\operatorname{Im}f$ is an epimorphism.\nConsider the factorization of $f$:\n\nBy step 4 we have that $A \\to \\operatorname{Im}{f}$ and $\\operatorname{Coim}{f} \\to B$ are epi and mono, respectively. Therefore $\\bar{f}$ is both epi and mono and we're done by step 3.", "meta": {"post_id": 45008, "input_score": 45, "output_score": 48, "post_title": "Equivalent conditions for a preabelian category to be abelian"}}
{"input": "I recall reading in an abstract algebra text two years ago (when I had the pleasure to learn this beautiful subject) that there exists a division ring $D$ that is not isomorphic to its opposite ring. However, the author noted that the construction of such a ring \"would take us too far afield\". My question is: please give an example of a division ring $D$ that is not isomorphic to its opposite ring.\nLet me recall that a division ring is a ring $A$ such that every non-zero element of $A$ is a unit, i.e., has a multiplicative inverse in $A$. However, division rings are not required to be commutative. Let me also recall that if $A$ is a ring, the opposite ring of $A$ is the ring $A^{\\text{op}}$ that has the same underlying set as $A$, the same additive structure as $A$, but that the multiplication $*$ on $A^{\\text{op}}$ is defined by the rule $a*b=ba$ where \"$ba$\" denotes the product of $b$ and $a$ with respect to the multiplication in $A$. \nIt is not hard to find examples of rings which are not isomorphic to their opposite ring and it is trivial to find examples of noncommutative rings that are isomorphic to their opposite rings. (E.g., the $n\\times n$ matrix ring over a field $F$ ($n>1$) is isomorphic to its opposite ring via the transpose map $A\\to A^{T}$ where $A$ is a matrix and $A^{T}$ denotes its transpose.) Of course, every commutative ring is isomorphic to its opposite ring via the identity map. However, this particular question appears to be difficult.", "output": "Let $F$ be the center of the division ring $D$. Then $D$ represents its class in the Brauer group $Br(F)$. The opposite ring $D^{opp}$ represents the inverse element. The reason why for example the quaternions are isomorphic to their opposite algebra is that the quaternions are an element of order 2 in $Br(\\mathbf{R})$, and hence equal to its own inverse in the Brauer group. \nTo get a division algebra that is not isomorphic to its opposite algebra we can use an element of order 3 in the Brauer group. One method for constructing those is to start with a (cyclic) Galois extension of number fields $E/F$ such that $[E:F]=3$. Let $\\sigma\\in Gal(E/F)$ be the generator. Let $\\gamma\\in F$ be an element that cannot be written in the form $\\gamma=N(x)$, where $N:E\\rightarrow F, x\\mapsto x\\sigma(x)\\sigma^2(x)$ is the relative norm map. Consider the set of matrices\n$$\n\\mathcal{A}(E,F,\\sigma,\\gamma)=\\left\\{\n\\left(\\begin{array}{rrr}\nx_0&\\sigma(x_2)&\\sigma^2(x_1)\\\\\n\\gamma x_1&\\sigma(x_0)&\\sigma^2(x_2)\\\\\n\\gamma x_2&\\gamma\\sigma(x_1)&\\sigma^2(x_0)\n\\end{array}\\right)\\mid x_0,x_1,x_2\\in E\\right\\}.\n$$\nA theorem of A. Albert tells us that this forms a division algebra with center $F$, and its order in $Br(F)$ is 3, so it will not be isomorphic to its opposite algebra. The theory is described for example in ch. 8 of Jacobson's Basic Algebra II. The buzzword 'cyclic division algebra' should give you some hits.\nFor a concrete example consider the following. Let $F=\\mathbf{Q}(\\sqrt{-3})$ and let\n$E=F(\\zeta_9)$, with $\\zeta_9=e^{2\\pi i/9}$. Then $E/F$ is a cubic extension of cyclotomic fields, $\\sigma:\\zeta_9\\mapsto\\zeta_9^4$. I claim that the element $2$ does not belong the image of the norm map. This follows from the fact 2 is totally inert in the extension tower $E/F/\\mathbf{Q}$. Basically because $GF(2^6)$ is the smallest finite field of characteristic 2 that contains a primitive ninth root of unity. Now, if $2=N(x)$ for some $x\\in E$, then 2 must appear as a factor (with a positive coefficient) in the fractional ideal generated by $x$. But the norm map then multiplies that coefficient by 3, and as there were no other primes above 2, we cannot cancel that. Sorry, if this is too sketchy.\nAnyway (see Jacobson again), the product of the $\\gamma$ elements modulo $N(E^*)$ is the operation in the Brauer group $Br(E/F)\\le Br(F).$ Therefore the opposite algebra should correspond to the choice $\\gamma=1/2$, (or to the choice $\\gamma=4$, as $2\\cdot4=N(2)$). So\n$$\n\\mathcal{A}(E,F,\\sigma,2)^{opp}\\cong\n\\mathcal{A}(E,F,\\sigma,1/2).\n$$\nand the choices $\\gamma=2$ and $\\gamma=1/2$ yield non-isomorphic division algebras, as their ratio $=4$ is not in the image of the norm map.\n======================================================\nEdit (added more details here, because another answer links to this answer): In general the cyclic division algebra construction works much the same for any cyclic extension $E/F$. When $[E:F]=n$ we get a set of $n\\times n$ matrices with entries in $E$. The number $\\gamma$ appears in the lower diagonal part of the matrix. The condition for this to be a division algebra is that $\\gamma^k$ should not be a norm for any integer $k, 0<k<n$. Obviously it suffices to check this for (maximal) proper divisors of $n$. In particular, if $n$ is a prime, then it suffices to check that $\\gamma$ itself is not a norm.\nEdit^2: Matt E's answer here linear algebra over a division ring vs. over a field gives a simpler cyclic division algebra of $3\\times3$ matrices with entries in the real subfield of the seventh cyclotomic field.", "meta": {"post_id": 45085, "input_score": 52, "output_score": 67, "post_title": "An example of a division ring $D$ that is **not** isomorphic to its opposite ring"}}
{"input": "I need your help with evaluating this limit:\n$$ \\lim_{n \\to \\infty }\\underbrace{\\sin \\sin \\dots\\sin}_{\\text{$n$ compositions}}\\,n,$$\ni.e. we apply the $\\sin$ function $n$ times.\nThank you.", "output": "The first sine is in $I_1=[-1,1]$ hence the $n$th term of the sequence is in the interval $I_n$ defined recursively by $I_1=[-1,1]$ and $I_{n+1}=\\sin(I_n)$. One sees that $I_n=[-x_n,x_n]$ where $x_1=1$ and $x_{n+1}=\\sin(x_n)$. The sine function is such that $0\\le\\sin(x)\\le x$ for every nonnegative $x$ hence $(x_n)$ is nonincreasing and bounded below by zero hence it converges to a limit $\\ell$. The sine function is continuous hence $\\ell=\\sin(\\ell)$. The only fixed point of the sine function is zero hence $\\ell=0$. This proves that $x_n\\to0$, that the sequence $(I_n)$ is nonincreasing and that its intersection is reduced to the point zero and finally, that the sequence considered in the post converges to zero.\nEdit: The argument above shows that for every sequence $(z_n)$, the sequence $(s_n)$ defined by $s_n=\\sin\\sin\\cdots\\sin(z_n)$ (the sine function being iterated $n$ times to define $s_n$) converges to zero. In other words, there is nothing particular about the choice $z_n=n$.", "meta": {"post_id": 45283, "input_score": 81, "output_score": 164, "post_title": "Compute $ \\lim\\limits_{n \\to \\infty }\\sin \\sin \\dots\\sin n$"}}
{"input": "A function by definition is a set of ordered pairs, and also according the Kuratowski, an ordered pair $(x,y)$ is defined to be $$\\{\\{x\\}, \\{x,y\\}\\}.$$\nGiven $A\\neq \\varnothing$, and $\\varnothing\\colon \\varnothing \\rightarrow A$. I know $\\varnothing \\subseteq \\varnothing \\times A$, but still an empty set is not an ordered pair. How do you explain that an empty function is a function?", "output": "I believe that the answer lies hidden within the depth of \"vacuously true argument\".\nAn argument of the form $\\forall x\\varphi$ is true if and only if there is no $x$ such that $\\lnot\\varphi(x)$.\nFor example if our universe is the natural numbers with the usual $\\ge$ order, then $\\forall x(x\\ge 0)$ is true because there are no negative numbers.\nOn the other hand, $\\forall x(x\\ge 0 \\land x\\neq 0)$ is false, simply because setting $x=0$ is a counterexample.\nMore generally, a sentence \"If $p$ then $q$\" ($p\\implies q$, or $p\\rightarrow q$) is true whenever the assumption is false, i.e. $p$ never occurs. \nAn example I often use is \"If I am standing upside down from the ceiling right now, then you are all unicorns\". It does not matter that I am talking to people, and not to unicorns, because I never stand upside down from the ceiling (it gives me a huge headache, you see).\nThe next point in our journey towards the empty function, is the bounded quantification. When we write $\\forall x\\in A(P(x))$ we actually write $\\forall x(x\\in A\\rightarrow P(x))$, this means that we quantify over all the possible $x$, but if $x\\notin A$ then we do not care about it anymore (the proposition is true since the assumption is false).\nAnd lastly, the definition of a function $F$ is this: \n$$\\begin{align}\r\n\\forall z & (z\\in F\\rightarrow\\exists x\\exists y(z=\\{\\{x\\},\\{x,y\\}\\})\\land\\\\ & \\forall x(\\exists z\\exists y(z\\in F\\land z=\\{\\{x\\},\\{x,y\\}\\})\\rightarrow \\\\ &\\qquad(\\forall u\\forall v(\\exists z\\exists w((z\\in F\\land w\\in F\\land z= \\{\\{x\\},\\{x,v\\}\\}\\land w=\\{\\{x\\},\\{x,u\\}\\})\\rightarrow u=v)\r\n\\end{align}$$\nLet's read this long formula. It says that $F$ is such that every element of $F$ is an ordered pair, and for every $x$, if there is an ordered pair $z$ with $x$ for left coordinate, then there is only one such pair (i.e. given two pairs, if their right coordinate is equal then they are equal).\nInformally, $F$ is a function if it is a set of ordered pairs, that for every $x\\in Dom(f)$ there is a unique $y$ such that $\\langle x,y\\rangle\\in F$.\nAn example is $F=\\{\\langle 1,2\\rangle\\}$ is a function, all its members are ordered pairs, and since there is only one member it automatically satisfies the requirement that the left-coordinate determines the pair.\nOn the other hand $R=\\{\\langle 1,2\\rangle,\\langle 1,3\\rangle\\}$ is a set that indeed all its members are ordered pairs, but there are two distinct ordered pairs with $1$ in the left coordinate, so it is not a function.\nAnd even more $A=\\{3,\\langle 1,2\\rangle\\}$ is clearly not a function, since $3$ is not an ordered pair!\nThe definition of a function is vacuously true when it is applied to the empty set, let us see why:\n\nFor all $z$ if $z\\in\\emptyset$ then $z$ is an ordered pair is vacuously true, as no $z$ is a member of the empty set. \nNext we have, that for all $x$, if there is some ordered pair in the empty set, with $x$ as left coordinate then the right coordinate is unique, this is also vacuously true since there is no ordered pair in the empty set with $x$ in the left coordinate. This is exactly the case $p\\rightarrow q$ and $p$ is false.\n\nThe conjunction of two true statement is true as well, therefore the empty set satisfies the requirement that every element of it is an ordered pair, and if two ordered pairs have the same left coordinate then they are equal. Therefore, $\\emptyset$ is a function.", "meta": {"post_id": 45625, "input_score": 52, "output_score": 41, "post_title": "Why is an empty function considered a function?"}}
{"input": "Let $M=\\{(x,|x|): x \\in (-1, 1)\\}$. Then there is an atlas with only one coordinate chart $(M, (x, |x|) \\mapsto x)$ for $M$. We don't need any coordinate transformation maps to worry about differentiablity. So I thought $M$ is a differentiable manifold. However my teacher says it is not. He says the sharp corner at $x = 0$ is a problem. I can't understand why it is a problem.", "output": "To add to what joriki wrote and what some people expressed in the comments: the real problem lies in certain implicit information/assumption you are making. The problem is that you have only specified $M$ as a set explicitly, and to ask whether an object is a differentiable manifold requires also specifying the topological and smooth structures! There are generally two definitions of smooth manifolds that I see in textbooks:\n\nIntrinsic definition A smooth manifold is a topological manifold equipped with an atlas whose transition maps are smooth. In this case the smooth structure is explicitly given by saying that a function defined on the manifold is smooth if it is smooth in each chart belonging to the atlas. \nExtrinsic definition A smooth manifold is defined as a smooth submanifold of some ambient Euclidean space (usually through a defining function). In this case the smooth structure is given by saying that a function defined on the manifold is smooth if it is the restriction of a smooth function on the ambient space. \n\nYour response uses the intrinsic definition: you construct an atlas based on the homeomorphism of your set (as a topological subspace of $\\mathbb{R}^2$) to the open unit interval. Your teacher's response implicitly depends on inheriting not only the topological structure from $\\mathbb{R}^2$, but also the smooth structure, so is based on the extrinsic definition. \nTo be really pedantic, as stated the question cannot be answered, since no candidate for a smooth structure was given. In essence you interpreted the question to mean that \"Given this topological space $M$, can we give it a smooth structure?\" while your teacher interpreted the question to mean \"Given this topological subspace $M\\subset \\mathbb{R}^2$, is it a smooth submanifold?\" Therefore two difference answers are expected since two different questions are treated. \n\nTo illustrate the problem further: a common exam question is to ask \n\nIs the punctured closed square $[-1,1]\\times[-1,1] \\setminus \\{(0,0)\\} \\subset \\mathbb{R}^2$ a smooth manifold with boundary?\n\nObserving that if you only induce on the square the topology from the ambient Euclidean space, then you can reason thus: you can explicitly construct a homeomorphism from our punctured closed square to the upper half plane, so we can take that as a single coordinate chart and give the set the structure of a smooth manifold compatible with the induced topology. But if you want the smooth structure to also be inherited.... \n\nBut unlike joriki, I would say that in context your teacher's response is more reasonable. This is because by using the local homeomorphism of any topological manifold to Euclidean spaces, you can always prescribe local differentiable structures on any topological manifold. The obstruction for a topological manifold to be smooth is very essentially global (and somewhat tangentially related is the fact that the obstruction is on the level of the first derivative only), and the existence of nondifferentiable topological manifolds is actually not that easy to show. Furthermore, it is also known that any topological manifold in dimensions 1, 2 and 3 admits a (unique) compatible smooth structure, so in view of that (also not too easy) result, the question becomes vacuous if you choose to interpret it in the way you did.", "meta": {"post_id": 45673, "input_score": 43, "output_score": 36, "post_title": "Is $M=\\{(x,|x|): x \\in (-1, 1)\\}$ not a differentiable manifold?"}}
{"input": "I've been learning about Galois theory recently on my own, and I've been trying to solve tests from my university. Even though I understand all the theorems, I  seem to be having some trouble with the technical stuff. A specific example would be how to find the Galois group of a given polynomial. I know some tricks, and I manage to solve some of those questions, but some not.\nFor example, one of the tests asks to find the Galois group of $x^{4}-4x+2$.\nI can see that it is irreducible over $\\mathbb Q$ (Eisenstein), but I have no clue as to how to find its Galois group over $\\mathbb Q$. Can someone tell me how to do this? General techniques concerning this sort of problems are also welcome :).\nThanks!", "output": "This is an attempt to write a canonical answer listing techniques to compute Galois groups of explicit polynomials, primarily over $\\mathbb{Q}$, as described in this meta thread. This answer is CW to encourage others to add techniques.\nComputability: Over any reasonable field $K$, such as $\\mathbb{Q}$, $\\mathbb{F}_p$, $\\mathbb{Q}(t)$, etc, Galois groups are computable. See Computable fields and Galois theory by Russel Miller for a good summary of this subject. Logicians can construct countable fields $K$ where the basic field operations $+$, $-$, $\\times$, $\\div$ are computable but the question \"is $D$ square?\" is not computable. In such a field, we cannot compute the Galois group of the splitting field of $x^2-D$. However, this is not the issue that this answer seeks to address and, as described in the linked article, there are very good criteria for showing that fields encountered in ordinary practice do not exhibit this phenomenon.\nAlthough Galois groups are computable, computation of Galois groups, both by computer systems and by students in Galois theory courses, does not proceed along a single algorithm, but rather involves a smorgasboard of methods which are used to prove facts about the group $G$, until enough facts are accumulated to single out a particular group. The main goal of this entry is to list such techniques. I've listed the methods roughly in the order I would pull them out. Some methods are left blank for now; feel free to fill them in before I do!\nSeveral of these methods are either primarily used to prove $G$ is large, or primarily used to prove $G$ is small. In this case, I have put \"LARGE\" or \"SMALL\" after the subject heading.\nI also plan to put some useful lemmas about permutation subgroups at the end, since the more you know about subgroups of $S_n$, the fewer facts you need about a group $G$ in order to identify it.\n\nAs I assume you understand already, if $f$ is a separable polynomial of degree $n$, with roots $\\theta_1$, $\\theta_2$, ..., $\\theta_n$, and $L$ is the splitting field of $f$ over $K$, then $Gal(L/K)$ is a subgroup of $S_n$, which can be thought of as permutations of the roots.\nNoticing obvious additive and multiplicative relations between roots (SMALL) Let $f(x) = x^4+2 x^2 + 3$. Since all the exponents in $f$ are even, if $\\theta$ is a root then so is $-\\theta$. Let the roots be $(\\theta_1, -\\theta_1, \\theta_2, -\\theta_2)$. So any Galois symmetry must take the (unordered) pair $\\{ \\theta_1, - \\theta_1 \\}$ either to $\\{ \\theta_1, - \\theta_1 \\}$ or $\\{ \\theta_2, - \\theta_2 \\}$.\nSimilarly, let $f(x) = x^4 + 2 x^3 + 5 x^2 + 2 x + 1$. Since this polynomial is palindromic,\nif $\\theta$ is a root, then so is $\\theta^{-1}$. Writing the roots as $(\\theta_1, \\theta_1^{-1}, \\theta_2, \\theta_2^{-1})$, one obtains similar restrictions on $G$. One can also see more complicated multiplicative relations: If $\\theta_1$ and $\\theta_2$ are two roots of $x^5-2=0$, then so is $\\theta_2^2/\\theta_1$; relations of the form $\\theta_3 = \\theta_2^2/\\theta_1$ quickly restrict the Galois group to be a subgroup of a $20$ element group.\nIrreducibility of polynomials and transitive group actions The polynomial $f$ is irreducible if and only if the Galois group is transitive. Wolfram alpha, Mathematica, Maple, SAGE, etc. will factor polynomials for you.\nTo prove polynomials are irreducible by hand, one can use Eisenstein's criterion or can try to find a prime $p$ such that $f$ does not factor modulo $p$. However, there are polynomials which are irreducible but for which neither of these tests will prove them so.\nComplex conjugation (LARGE) Suppose that $K$ is a subfield of $\\mathbb{R}$ (such as the rationals). Let $\\theta_1$, $\\theta_2$, ..., $\\theta_n$ be the roots of $f$, thought of as complex numbers. Again, Wolfram alpha or any computer algebra system will find the $\\theta$'s for you.\nIf there are $r$ real roots and $2s$ complex roots among the $\\theta$'s, then $G$ contains a permutation of conjugacy class $(1\\ 2) (3\\ 4) \\cdots (2s-1 \\ 2s)$\nDiscriminants Define $\\beta = \\prod_{i<j} (\\theta_i - \\theta_j)$ and $\\Delta = \\beta^2$. Since $\\Delta$ is symmetric in the $\\theta$'s, it is in the ground field $K$ and is quite reasonable to compute. The number $\\Delta$ is called the discriminant of $f$.\nThe Galois group $G$ is contained in $A_n$ if and only if $\\beta$ is in $K$; in other words, if and only if $\\Delta$ is square in $K$.\nHere is a sample Wolfram alpha session, verifying that the Galois group of the splitting field of $-1 - 2 x + x^2 + x^3$ is contained in $A_3$.\nRecognizing $S_n$ and $A_n$ (LARGE) A random polynomial over $\\mathbb{Q}$ has very high probability of having Galois group $S_n$. (See this paper of Cohen for a precise bound.) So it is worth being able to recognize the Galois group $S_n$ early in the process. Keith Conrad has good notes on how to do so. Galois groups appearing on problem sets in Galois theory courses have a lower probability of being $S_n$. :-)\nReduction modulo $p$/Chebotarev's density theorem (LARGE, but useful both ways) Suppose that $p$ does not divide the discriminant of $f$. Let $f$ factor modulo $p$ into factors of degrees $(h_1, h_2, \\ldots, h_d)$. Then $G$ contains an element with cycles of length $h_1$, $h_2$, ..., $h_d$. Conversely, by the Cebatarov density theorem, if $G$ contains an element of cycle type $(h_1, h_2, \\ldots, h_d)$, then there is a prime $p$ such that $f$ factors modulo $p$ with factors of that size. See David Speyer's blogpost,  Hendrik Lenstra's exposition or Keith Conrad's notes.\nHere is an example:\nTable[Factor[X^4 + 4 X^3 + 12 X^2 + 24 X + 24, Modulus -> Prime[n]], {n, 3, 10}]\n\nproduces\n{(4 + X) (1 + 2 X + X^3), (5 + X) (2 + 3 X + 6 X^2 + X^3), \n (3 + X) (8 + 9 X + X^2 + X^3), (12 + X) (2 + 4 X + 5 X^2 + X^3), \n (16 + X + X^2) (10 + 3 X + X^2), (14 + 11 X + X^2) (18 + 12 X + X^2), \n (17 + X) (19 + 3 X + 10 X^2 + X^3), (22 + X) (9 + 2 X + 11 X^2 + X^3)}\n\nAll the polynomials have cycle type $(3,1)$ or $(2,2)$, proving that $G$ contains those cycle types, and producing strong evidence that $G$ is contained in a group which only has cycles of that type. That can't quite be right, because $G$ contains the identity with cycle type $(1,1,1,1)$; searching up to $p=71$ finds such a factorization. But even running the search that far, we only see cycle types $(1,1,1,1)$, $(3,1)$, $(2,2)$, suggesting that $G = A_4$. Indeed,\nDiscriminant[X^4 + 4 X^3 + 12 X^2 + 24 X + 24, X]\n\noutputs $331776=576^2$ so we know $G \\subseteq A_4$, and the above data shows that $G$ contains a $3$-cycle and contains an element of type $(2,2)$, so $G = A_4$.\nDistinguishing $D_4$ and $C_4$ Let $f(x) = x^4 - 5 x^2 + 5$. Since $f(x)$ is even, if $\\theta$ is a root, so is $- \\theta$. This shows that $G$ is a subgroup of $D_4$, the dihedral symmetries of a square. Since the polynomial is irreducible, the action on a $4$ element set is transitive, which makes $G$ either $D_4$, the cyclic group $C_4$, or $C_2 \\times C_2$ with the generators $(12)(34)$ and $(13)(24)$. Since the discriminant is $2000$, which is not square, it is not contained in $A_4$, which rules out $C_2 \\times C_2$.\nFactoring modulo $p$ finds no factors with cycle type $(1,1,2)$, which strongly suggests $G$ is $C_4$. But that isn't a proof. This particular case comes up enough (especially on exams!) that there are special methods for it, see Keith Conrad's notes or my answer here.\nTesting for $k$-transitivity (LARGE)\nMore generally, let $\\beta = \\theta_1 + 2 \\theta_2 + \\cdots + k \\theta_k$ and let $g(x)$ be the minimal polynomial of $\\beta$. If $g(x)$ has degree $n(n-1) \\cdots (n-k+1)$, then $G$ is $k$-transitive on $(\\theta_1, \\ldots, \\theta_n)$. If there is some number of the form $\\theta_{s_1} + 2 \\theta_{s_2} + \\cdots + k \\theta_{s_k}$ which is not a root of $g$, then the group $G$ is not $k$-transitive. Since the $k$-transitive groups have been classified for $k \\geq 3$ (see previous link), this can be very useful in identifying $G$.\nHere is a small example. Take $f(x) = x^4-4 x^3+2 x^2+4 x-6$. We check that $f$ is irreducible:\nf = x^4 - 4 x^3 + 2 x^2 + 4 x - 6;\nFactor[f]\n\nWe store the roots of $f$ in variables t1, t2, t3, t4. (Mathematica can actually solve this polynomial exactly, but this is just meant as an example.)\n{t1, t2, t3, t4} = x /. Solve[f == 0, x];\n\nb = t1 + 2 t2;\n\ng = MinimalPolynomial[b, x] \n(* Output is 42 - 84 x + 50 x^2 - 12 x^3 + x^4 *)\n\nSince $g$ has degree $4$, we conclude that the group $G$ probably has an orbit of size $4$ acting on ordered pairs of distinct $\\theta_i$'s. For a careful proof, we need to make sure that we don't have accidental equalities of the form $\\theta_i+2 \\theta_j = \\theta_k + 2 \\theta_{\\ell}$. In particular, we check that $g(\\theta_1+2 \\theta_3) \\neq 0$, so $(1,2)$ and $(1,3)$ are in different orbits for the $G$-action. This example is small enough for Mathematica to do exactly, but I'll do the computation numerically because that is good practice for larger examples:\nN[g /. x -> t1 + 2 t3]\n(* Output is 198.996 + 220.833 I *)\n\nRelative resultants\nInstalling PARI, GAP or MAGMA This section is about which computer algebra systems have implemented general algorithms to do this, and where to get them.\n\nUseful lemmas about subgroups of $S_n$", "meta": {"post_id": 45893, "input_score": 140, "output_score": 177, "post_title": "How to find the Galois group of a polynomial?"}}
{"input": "Is it the right point of view to say, that topology is nonfirstorderizable (only) because the union of arbitrarily many open sets has to be open? And if \"arbitrarily many\" was relaxed to \"finitely many\", topology would be firstorderizable?\nI can imagine two versions of the relaxed theory: one whose models must be powersets, and another one whose models may be arbitrary boolean lattices (both with an extra predicate \"open\").\nHas one of these relaxed first-order theories been found worth being studied in its own right?", "output": "Nonfirstorderizability is a concept from philosophy in which some argue that particular natural-language sentences cannot be captured in particular first-order theories. One example sentence they look at is \"Some critics admire only one another\".  As a concept, nonfirstorderizability doesn't really apply to mathematical theories, only to English sentences.  Virtually all of mathematics can be directly formalized in first-order set theory, and in particular this is how topology is usually formalized.  \nThere are a few difficulties with the argument about nonfirstorderizability as it is often presented, including the argument on the Wikipedia page [2]. The main one is that that page refers to a particular sentence as a second-order sentence. However, there is nothing in the syntax of a sentence that ties it to any particular semantics: the same sentence could be studied in one-sorted second-order logic or two-sorted first-order logic. I would argue that anything that can be expressed in second-order logic can be expressed in two-sorted first-order logic. (This leads to a discussion of what it means to \"express\" something formally, which is for another day.) My point is that you have to take these claims critically, because the philosophical issues that are being discussed may or may not have much relevance to a working mathematician. \n* First-order theory of topological spaces *\nBefore I make more comments about nonfirstorderizability, I want to explain how to make a first-order theory of topological spaces. The technique I am describing here is familiar to many logicians - it's routine, not novel. \nThe signature of the language includes three sorts of variables, no function symbols, no constant symbols, and the following relation symbols:\n\n$=^0$, $=^1$, $=^2$: equality relation symbols for each of the three sorts\nA unary relation $U(x^1)$ that takes an object of sort 1\n$R(x^0,x^1)$: a binary relation that takes an object of sort 0 and an object of sort 1\n$S(x^1,x^2)$: a binary relation that takes an object of sort 1 and an object of sort 2\n\nNote that this is, so far, just an uninterpreted signature $\\sigma$ which is unarguably a first-order three-sorted signature. We could use it to study various things, but in particular it is useful for topology.  The use of three sorts is not essential; many-sorted first-order logic can be interpreted in one-sorted first-order logic, so there is no gain in expressiveness. It's simply more concise to use many-sorted logic. \nEvery topological space $X$ gives rise to a $\\sigma$-structure $M_X$, as follows:\n\nThe objects of $M_X$ of sort 0 are the points of $X$\nThe objects of $M_X$ of sort 1 are the sets of points of $X$\nThe objects of $M_X$ of sort 2 are the sets of sets of points of $X$\n$U$ is interpreted so that $U(a^1)$ holds in $M_X$ if and only if $a^1$ is an open set in $X$\n$R$ is interpreted so that $R(a^0, a^1)$ holds in $M_X$ if and only if $a^0 \\in a^1$. \n$S$ is interpreted so that $S(a^1,a^2)$ holds in $M_X$ if and only if $a^1 \\in a^2$. \nAll the equality relation symbols are interpreted in $M_X$ as the actual equality relations on the three sorts\n\nLet $T$ be the set of all $\\sigma$-sentences $\\phi$ such that for every topological space $X$, $\\phi$ is true in $M_X$. Then, in the usual jargon, $T$ is the first order theory of topological spaces in signature $\\sigma$.  $T$ will include, for example, this first-order sentence which expresses the fact that the union of an arbitrary collection of open sets is open: \n$$\r\n(\\forall x^2)((\\forall z^1)(S(z^1,x^2)\\to U(z^1)) \\to (\\exists x^1)(U(x^1) \\land (\\forall x^0)(x^0 \\in x^1 \\leftrightarrow (\\exists z^1)(R(x^0,z^1)\\land S(z^1,x^2)))\r\n$$\nMore examples: $T$ will contain sentences expressing the axiom of extensionality for sorts 1 and 2; it will contain a sentence saying that the empty set is an open set, and it will contain a sentence saying the intersection of two open sets is open. All of these can be naturally expressed as first-order sentences in the signature $\\sigma$. \nAs I mentioned, the use of three sorts is not necessary here, they can be simulated by adding three additional unary predicates to the structure.  So we could do the same thing in one-sorted first-order logic. \n* Analysis *\nAny argument that something is \"not first order\" has to be made with the awareness of this sort of construction. There may be some sense in which the theory $T$ I have just constructed is a not a \"first order theory\", but that sense appears to be very subtle, and disagrees with the usual terminology.  It  seems more natural to accept that $T$ is indeed a first-order theory and ask what \"nonfirstorderizability\" could mean given the existence of examples such as this. I am confident that philosophers who look at nonfirstorderizability must be aware of examples such as this, so when we read their remarks we have to take into account that \"cannot be expressed in first-order logic\" might not have its literal meaning. \nA key fact to note is there is no set of first-order axioms, in the signature $\\sigma$, such that every model of these axioms is of the form $M_X$ for some topological space $X$. The model-theoretic way of saying this is that the collection of models that correspond to actual topological spaces is not an \"elementary class\". But there are many other objects that we study in first-order logic that do not form elementary classes; the question of which classes of structures are elementary was a central motivation of classical model theory. If all that we meant by \"nonfirstorderizable\" was \"not an elementary class\" then we would just use the established terminology. So I think that \"nonfirstorderizable\" has to be read as meaning something other than \"not an elementary class\". \nIn light of examples like the one above, though, I'm not sure what a rigorous definition of \"nonfirstorderizable\" would be. It seems to be related to the ability to express a theory in a particularly limited signature. But, for example, the axioms of a topology mention points, sets of points, and sets of sets of points, and the theory I constructed above seems like a pretty minimal theory, syntactically, for expressing propositions about these three sorts of objects.  \nIn the case of the sentence \"Some critics admire only one another\", the philosophical question is whether the sentence necessarily involves sets (in which case we would expect to use sets to formalize it) or whether it can be understood without reference to sets. This is a good philosophical question. But mathematical theories such as topology or the theory of complete ordered fields explicitly mention sets, so it is not obvious to me why we would expect to be able to axiomatize them without sets.\nThe question that Terence Tao described in his blog post [2] is a little different. The issue there is not with sets directly, it's with tracking the dependence of quantified variables deep within a formula on variables quantified farther out in the formula. I think this is also qualitatively different than the question about topology. \n1: http://en.wikipedia.org/wiki/Nonfirstorderizability\n2: http://terrytao.wordpress.com/2007/08/27/printer-friendly-css-and-nonfirstorderizability/", "meta": {"post_id": 46656, "input_score": 28, "output_score": 37, "post_title": "(Why) is topology nonfirstorderizable?"}}
{"input": "I have two questions, actually. The first is as the title says: how do we know there exists an infinite cardinal such that there exists no other cardinals between it and $ \\aleph_0 $? (We would have to assume or derive the existence of such an object before we label it something like $ \\aleph_1 $.)\nMy second question is, can we say for certain if there's any limit to the number of cardinals existing between $ \\aleph_0 $ and continuum (i.e. $ 2^{\\aleph_0} $)? I mean, how could we know that there's not an infinite number of cardinals between the two - perhaps even more than $ \\aleph_0 $?", "output": "To prove the existence of $\\aleph_1$ we use the concept of Hartogs number. The question asks, really, why are there uncountable ordinals, since $\\aleph_1$ is by definition the least ordinal which is not countable.\nTake a set of cardinality $\\aleph_0$, say $\\omega$. Now consider all the orders on $\\omega$ which are well-orders, and consider the order isomorphism as an equivalence relation. The collection of all equivalence classes is a set.\n\nFact: If $(A,R)$ is a well-ordered set, then there exists a unique ordinal $\\alpha$ such that $(A,R)\\cong(\\alpha,\\in)$.\n\nMap every equivalence class to the unique ordinal which is order isomorphic to the members of the class. We now have a set and all its members are ordinals which correspond to possible well-ordering of $\\omega$.\n\nFact: The union of a set of ordinals is an ordinal, it is in fact the supremum of the elements in the union.\n\nLet $\\alpha$ be the union of the set defined above. We have that $\\alpha$ is an ordinal, and that every ordinal below $\\alpha$ is a possible well-ordering of $\\omega$ (and therefore countable). \nSuppose $\\alpha$ was countable too, then $\\alpha+1$ was also countable (since $\\alpha+1=\\alpha\\cup\\{\\alpha\\}$), and therefore a possible well ordering of $\\omega$. This would contradict the above fact that $\\alpha$ is greater or equal to all the ordinals which correspond to well-orderings of $\\omega$, since $\\alpha<\\alpha+1$.\nThis means that $\\alpha$ is uncountable, and that it is the first uncountable ordinal, since if $\\beta<\\alpha$ then $\\beta$ can be injected into $\\omega$, and so it is countable. Therefore we have that $\\alpha=\\omega_1=\\aleph_1$. \nNote that the above does not require the axiom of choice and holds in $\\sf ZF$. The collection of all well-orders is a set by power set and replacement, so is the set of equivalence classes, from this we have that the collection of ordinals defined is also a set (replacement again), and finally $\\alpha$ exists by the axiom of union. There was also no use of the axiom of choice because the only choice we had to do was of \"a unique ordinal\" which is a definable map (we can say when two orders are isomorphic, and when a set is an ordinal - without the axiom of choice).\nWith the axiom of choice this can be even easier:\nFrom the axiom of choice we know that the continuum is bijectible with some ordinal. Let this order type be $\\alpha$, now since the ordinals are well-ordered there exists some $\\beta\\le\\alpha$ which is the least ordinal which cannot be injected into $\\omega$ (that is there is no function whose domain is $\\beta$, its range is $\\omega$ and this function is injective).\nFrom here the same argument as before, since $\\gamma<\\beta$ implies $\\gamma$ is countable, $\\beta$ is the first uncountable ordinal, that is $\\omega_1$.\nAs to why there is no cardinals strictly between $\\aleph_0$ and $\\aleph_1$ (and between any two consecutive $\\aleph$-numbers) also stems from this definition.\n\n$\\aleph_0 = |\\omega|$, the cardinality of the natural numbers,\n$\\aleph_{\\alpha+1} = |\\omega_{\\alpha+1}|$, the cardinality of the least ordinal number which cannot bijected with $\\omega_\\alpha$,\n$\\aleph_{\\beta} = \\bigcup_{\\alpha<\\beta}\\aleph_\\alpha$, at limit points just take the supremum.\n\nThis is a function from the ordinals to the cardinals, and this function is strictly increasing and continuous. Its result is well-ordered, i.e. linearly ordered, and every subset has a minimal element.\nThis implies that $\\aleph_1$ is the first $\\aleph$ cardinal above $\\aleph_0$, i.e. there are no others between them.\nWithout the axiom of choice, however, there are cardinals which are not $\\aleph$-numbers, and it is consistent with $\\sf ZF$ that $2^{\\aleph_0}$ is not an $\\aleph$ number at all, and yet there are not cardinals strictly between $\\aleph_0$ and $2^{\\aleph_0}$ - that is $\\aleph_0$ has two distinct immediate successor cardinals.\n\nFor the second question, there is no actual limit. Within the confines of a specific model, the continuum is a constant, however using forcing we can blow up the continuum to be as big as we want.\nThis is the work of Paul Cohen. He showed that you can add $\\omega_2$ many subsets of $\\omega$ (that is $\\aleph_2\\le 2^{\\aleph_0}$), and the proof is very simple to generalize to any higher cardinal.\nIn fact Easton's theorem shows that if $F$ is a function defined on regular cardinals, which has a very limited set of constraints, then there is a forcing extension where $F(\\kappa) = 2^\\kappa$, so we do not only violate $\\sf CH$ but we violate $\\sf GCH$ ($2^{\\aleph_\\alpha}=\\aleph_{\\alpha+1}$) in a very acute manner.", "meta": {"post_id": 46833, "input_score": 118, "output_score": 102, "post_title": "How do we know an $ \\aleph_1 $ exists at all?"}}
{"input": "The Fundamental Theorem of Calculus says the following:\n\nTheorem. If $f$ is the derivative of $F$ at every point on $[a,b]$, then under suitable hypotheses we have that $$\\int_{a}^{b} f(t) \\ dt = F(b)-F(a)$$\nTheorem. If $f$ is integrable on $[a,b]$, then under suitable hypotheses we have that $$\\frac{d}{dx} \\int_{a}^{x} f(t) \\ dt = f(x)$$\n\nI am trying to put myself in the shoes of Poisson, Cauchy and Riemann. The first theorem is basically saying that to find the area under a curve, we need to find any anti-derivative and evaluate it at the endpoints?\nThe second theorem is saying that we can view the integral as a function of $x$ and take its derivative to get $f(x)$.\nWasn't the goal of Poisson, Cauchy and Riemann to find the area under a curve? So they hypothesized the first theorem and then only later proposed the second theorem? Both theorems deal with finding the area under a curve (i.e. they are equivalent)?\nDo these theorems still hold under other types of integration (i.e. the Lebesgue integral)?", "output": "Added.\nThe relationship between the definite integral and the total change of an accumulation function goes back to well before Poisson, Cauchy, or Riemann. There's a nice historical overview in a recent article by David M. Bressoud, Historical reflections on teaching the Fundamental Theorem of Integral Calculus published in the Monthly last February. You can find one version in Leibniz's work in 1693, where he writes:\n\n\"I shall now show that the general problem of quadratures can be reduced to the finding of a line that has a given law of tangency, that is, for which the sides of the characteristic triangle have a given mutual relation. Then I shall show how this line can be described by a motion that I have invented.\" \n\n\"The problem of quadratures\" is the problem of finding areas. Leibniz's proof, which is entirely geometric (you can find it in Bressoud's article) follows from the understanding of areas and tangents as certain sums and differences. But it does not originate with Leibniz: Isaac Barrow gave a proof in his Lectiones geometricae (1670); and James Gregory gives one in his Geometriae pars universalis (1668). Gregory shows that finding the length of a curve is equivalent to finding the area under a related curve: he shows that there is a constant $c$, chosen depending on certain given ratios, the length of the curve $y=f(x)$ from $x=a$ to $x=b$ equals the area under the curve $y=c\\sqrt{1+(f'(x))^2}$ (though of course, not expressed that way). He then deals with the converse: given $y=g(x)$ on $[a,b]$, finding a curve $y=u(x)$ so that the area under the $y=g(x)$ is equal to the length of $y=u(x)$. He proves that if\n$$u(x) = \\frac{1}{c}\\int_a^x z(t)\\,dt$$\nthen $z/c$ describes the slope of the tangent to $u$; this \"contains\" the second FTC. \nEven earlier, the first part of what Gregory did had been done by Hendrick van Heureat, published in 1659 in van Schooten's edition of Descartes' Geometry.\nNewton, by contrast, gives a kind of \"dinamic proof\" of the FTC; it has its roots in Oresme's Tractatus de configurationibus qualitatum et motuum (1350), in which he shows that if you represent velocity by a curve, then the area under the curve corresponds to the distance traveled (that is, the integral of the derivative equals the total change of the function, the first part of the FTC). \nSo by the time Cauchy and Riemann gave their definitions of integrals, the FTC (both parts) was already on \"the table\"; they had the onus of showing that their definitions implied the FTC. So the FTC was already \"visible\" to them (just like it was to Lebesgue), they didn't need to hypothesize the first or second theorem, nor propose them. They only had to show that their definitions were such that the theorems held for their integrals. Much like Lebesgue needed to show that his definition of integral agreed with that of Riemann where they were both defined, but that didn't mean he had to come up with Riemann's definition of integral from scratch: it was already there, he just needed to show his definition did not change the old properties. \n\nYes, there are versions of the Fundamental Theorem of Calculus that hold for other types of integrals. A good resource is A Garden of Integrals, by Frank E. Burke. The following statements are taken from there.\nThe Cauchy Integral\nThe Cauchy definition of integral (from 1823) is the following:\nGiven a bounded function $f$ on $[a,b]$, divide $[a,b]$ into a finite number of contiguous subintervals $[x_{k-1},x_k]$, $a= x_0\\lt x_1\\lt\\cdots\\lt x_n=b$. The Cauchy sum of $f$ is\n$$\\sum_{k=1}^n f(x_{k-1})(x_k-x_{k-1}).$$\n(This is the equivalent of a left-hand sum evaluation in today's parlance). We say that $f$ is Cauchy-integrable on $[a,b]$ if and only if there exists a number $A$ such that for every $\\epsilon\\gt 0$ there exists $\\delta\\gt 0$ such that for any partition $P$ of $[a,b]$ whose subintervals have length less than $\\delta$, we have\n$$\\left|\\sum_{P} f(x_{k-1})(x_k-x_{k-1}) - A\\right| \\lt \\epsilon.$$\nCauchy proved that continuous functions are Cauchy-integrable, though this does not exhaust the class. We have the following \"FTC\"s for the Cauchy integral:\n\nFTC for the Cauchy Integral. If $F$ is a differentiable function on $[a,b]$, and $F'$ is continuous on $[a,b]$, then $F'$ is Cauchy integrable on $[a,b]$ and\n  $$C\\int_a^x F'(t)\\,dt = F(x)-F(a)$$\n  for each $x$ in $[a,b]$.\n\nHere, $C\\int$ denotes the Cauchy integral. \n\nFTC part 2 for the Cauchy Integral. If $f$ is a continuous function on the interval $[a,b]$, and we define a function $F$ on $[a,b]$ by $F(x) = C\\int_a^xf(t)\\,dt$, then $F$ is differentiable on $[a,b]$, $F' = f$ on $[a,b]$, and $F$ is absolutely continuous on $[a,b]$. \n\nWe also have a convergence theorem:\n\nConvergence for Cauchy Integrable Functions. If $\\{f_k\\}$ is a sequence of continuous functions converging uniformly to $f$ on $[a,b]$, then $f$ is Cauchy integrable on $[a,b]$ and $C\\int_a^bf(x)\\,dx = \\lim C\\int_a^b f_k(x)\\,dx$. \n\nThe Riemann Integral\nThe definition of the Riemann integral is the usual one. Lebesgue proved in 1902 that a bounded function on $[a,b]$ is Riemann integrable on $[a,b]$ if and only if it is continuous almost everywhere.\n\nFTC for the Riemann Integral. If $F$ is a differentiable function on the interval $[a,b]$, and $F'$ is bounded and continuous almost everywhere on $[a,b]$, then $F'$ is Riemann integrable on $[a,b]$, and $$R\\int_a^x F'(t)\\,dt = F(x) - F(a)$$ for each $x$ in the interval $[a,b]$.\nFTC part 2 for the Riemann Integral. Suppose $f$ is a bounded and continuous almost everywhere function on the interval $[a,b]$. Let $F$ on $[a,b]$ be defined by $F(x)=R\\int_a^x f(t)\\,dt$. Then $F$ is absolutely continuous on $[a,b]$; if $f$ is continuous at $x_0\\in[a,b]$, then $F$ is differentiable at $x_0$ and $F'(x_0)=f(x_0)$; and $F'=f$ almost everywhere.\nConvergence for Riemann Integrable Functions. If $\\{f_k\\}$ is a sequence of Riemann integrable functions converging uniformly to $f$ on $[a,b]$, then $f$ is Riemann integrable and $R\\int_a^b f(x)\\,dx = \\lim R\\int_a^b f_k(x)\\,dx$.\n\nRiemann-Stieltjes Integral\nLet $f$ and $\\phi$ be two bounded functions on $[a,b]$. We say that $f$ is Riemann-Stieltjes integrable with respect to $\\phi$ if and only if there exists a number $A$ such that for every $\\epsilon\\gt 0$ there exists a $\\delta\\gt 0$ such that\n$$\\left|\\sum_{k=1}^n f(c_k)\\bigl(\\phi(x_k) - \\phi(x_{k-1})\\bigr) - A\\right|\\lt \\epsilon$$\nwhere $x_{k-1}\\leq c_k\\leq x_k$, for every partition $P$ of $[a,b]$ whose subintervals has length less than $\\delta$. We write\n$$RS\\int_a^b f(x)d\\phi(x) = A.$$\n\nFTC for Riemann Stieltjes Integrals. If $f$ is continuous and $\\phi$ is differentiable, with $\\phi'$ Riemann integrable on $[a,b]$, then \n  $$RS\\int_a^b f(x)\\,d\\phi(x) = R\\int_a^b f(x)\\phi'(x)\\,dx.$$\nTheorem. If $f$ and $\\phi$ are bounded functions with no common discontinuities on $[a,b]$, and the Riemann-Stieltjes integral of $f$ with respect to $\\phi$ exists, then the Riemann-Stieltjes integral of $\\phi$ with respect to $f$ exists, and\n  $$RS\\int_a^b\\phi(x)\\,df(x) = f(b)\\phi(b) - f(a)\\phi(a) - RS\\int_a^bf(x)\\,d\\phi(x).$$\nFTC part two for Riemann-Stieltjes Integrals. If $f$ is continuous on $[a,b]$ and $\\phi$ is monotonic increasing on $[a,b]$, then $f$ is Riemann-Stieltjes integrable with respect to $\\phi$. If we define $F$ on $[a,b]$ by\n  $$F(x) = RS\\int_a^x f(t)d\\phi(t),$$\n  then $F$ is continuous at any point where $\\phi$ is continuous; and $F$ is differentiable at each point where $\\phi$ is differentiable (almost everywhere) and at such points, $F' = f\\phi'$.\nConvergence Theorem for Riemann-Stieltjes Integrals. If $\\{f_k\\}$ is a sequence of functions that converge uniformly to $f$ on $[a,b]$ and $\\phi$ is monotone increasing on $[a,b]$, then the $f_k$ is Riemann-Stieltjes integrable with respect to $\\phi$ for each $k$, $f$ is Riemann-Stieltjes integrable with respect to $\\phi$, and $$RS\\int_a^bf(x)\\,d\\phi(x) = \\lim RS\\int_a^b f_k(x)\\,d\\phi(x).$$\n\nLebesgue Integral\n\nFTC for the Lebesgue Integral. If $F$ is differentiable, and the derivative $F'$ is bounded on $[a,b]$, then $F'$ is Lebesgue integrable on $[a,b]$ and \n  $$\\int_{[a,x]}F'\\,d\\mu = F(x)-F(a)$$\n  for $x$ in $[a,b]$. \nLebesgue's FTC. If $F$ is absolutely continuous on $[a,b]$, then $F'$ is Lebesgue integrable and\n  $$\\int_{[a,x]} F'\\,d\\mu = F(x)- F(a)$$\n  for $x$ in $[a,b]$. \nFTC Part 2 for the Lebesgue Integral. If $f$ is Lebesgue integrable on $[a,b]$, and we define $F$ on $[a,b]$ by $F(x) = \\int_{[a,x]}f\\,d\\mu$, then $F$ is absolutely continuous on $[a,b]$ and $F'=f$ almost everywhere on $[a,b]$.\nDominated Convergence Theorem. If $\\{f_k\\}$ is a sequence of Lebesgue integrable functions converging pointwise almost everywhere to $f$ on $[a,b]$, and $g$ is a Lebesgue integrable function such that $|f_k|\\leq g$ on $[a,b]$, then $f$ is Lebesgue integrable on $[a,b]$ and\n  $$\\int_{[a,b]}f\\,d\\mu = \\lim \\int_{[a,b]} f_k\\,d\\mu.$$\n\nHenstock-Kurzweil Integral\nA function $\\delta\\colon [a,b]\\to (0,\\infty)$ is called a gauge on $[a,b]$. A tagged partition of $[a,b]$ is a finite collection of pointed intervals $(c_k,[x_{k-1},x_k])$, where $x_{k-1}\\leq c_k\\leq x_k$, $a=x_0\\lt x_1\\lt x_2\\lt\\cdots\\lt x_n=b$. We say a tagged partition of $[a,b]$ is $\\delta$ fine if $c_k-\\delta(c_k) \\lt x_{k-1}\\leq c_k \\leq x_k \\lt c_k+\\delta(c_k)$. \nA function $f$ on $[a,b]$ is said to be Henstock-Kurzweil integrable on $[a,b]$ if there is a number $A$ such that for every $\\epsilon\\gt 0$ there exists a positive function $\\delta_{\\epsilon}\\colon [a,b]\\to (0,\\infty)$ such that for any $\\delta_{\\epsilon}$-fine partition on $[a,b]$ with $c_{k}-\\delta_{\\epsilon}(c_k) \\lt x_{k-1}\\leq c_k \\leq x_{k}\\lt c_k+\\delta_{\\epsilon}(c_k)$, we have:\n$$\\left|\\sum_{k=1}^n f(c_k)(x_k-x_{k-1}) - A\\right|\\lt \\epsilon.$$\nIn that case, we write $HK\\int_a^b f(x)\\,dx = A$.\n\nFTC for the Henstock-Kurzweil Integral. If $F$ is continuous on $[a,b]$ and $F$ is differentiable on $[a,b]$ with at most a countable number of exceptional points, then $F'$ is Henstock-Kurzweil integrable on $[a,b]$, and\n  $$HK\\int_a^x F'(t)\\,dt = F(x) - F(a)$$\n  for each $x$ in $[a,b]$. \nFTC part two for the Henstock-Kurzweil Integral. If $f$ is Henstock-Kurtzweil integrable on $[a,b]$, and we define $F$ by $F(x) = HK\\int_a^x f(t)\\,dt$, then $F$ is continuous on $[a,b]$, $F'=f$ almost everywhere, and $f$ is Lebesgue measurable.\nDominated Convergence for Henstock-Kurzweil Integral. If $\\{f_k\\}$ is a sequence of Henstock-Kurzweil integrable functions that converge pointwise to $f$ on $[a,b]$, and there exist Henstock-Kurzweil integrable functions $\\phi$ and $\\psi$ such that $\\phi\\leq f_k\\leq \\psi$ for all $k$, then $f$ is Henstock-Kurzweil integrable and\n  $$HK\\int_a^b f(x)\\,dx = \\lim HK\\int_a^b f_k(x)\\,dx.$$\n\n\nThe integrals above are given in increasing order of strength, in the following sense: if $f$ is a function on $[a,b]$, then:\n$$\\begin{align*}\r\nf\\text{ is Cauchy integrable on }[a,b] &\\Longrightarrow f\\text{ is Riemann integrable on }[a,b]\\\\\r\n&\\Longrightarrow f\\text{ is Lebesgue integrable on }[a,b]\\\\\r\n&\\Longrightarrow f\\text{ is Henstock-Kurzweil integrable on }[a,b]\r\n\\end{align*}$$\nand none of the implications are reversible. The Riemann-Stieltjes (and its variant, the Lebesgue-Stieltjes) integral is not included in the chain of implications, because integrability there depends on both the function $f$ and the function $\\phi$.", "meta": {"post_id": 47285, "input_score": 26, "output_score": 40, "post_title": "Fundamental Theorem of Calculus"}}
{"input": "Can we identify the dual space of $l^\\infty$ with another \"natural space\"?  If the answer is yes, what can we say about $L^\\infty$?\nBy the dual space I mean the space of all continuous linear functionals.", "output": "This is perhaps not such a nice characterization as finitely additive measures on $\\mathbb N$, but it might be worth mentioning. (You can look it as follows: We obtain  nicer measures - $\\sigma$-additive instead of finitely additive - on a more complicated space - $\\beta\\mathbb N$ instead of $\\mathbb N$.)\nThe space $\\ell_\\infty$ is isometrically isomorphic to $C(\\beta\\mathbb N)$, hence the dual is isomorphic to $C^*(\\beta\\mathbb N)$.\nMore details about the correspondence between $\\ell_\\infty$ and the Stone-Cech compactification of integers can be found at wikipedia or in chapter 15 of Carothers' book A short course on Banach space theory.\nNow, $C^*(\\beta\\mathbb N)$ is the space of regular Borel measures on $\\beta\\mathbb N$ by Riesz representation theorem.\nIn fact, Carothers goes the other way round. First, the dual of $C^*(\\beta\\mathbb N)$ is described via finitely additive measures. And then he uses this to prove Riesz representation theorem for compact spaces. (This proof of Riesz' representation theorem  is due to Garling.)", "meta": {"post_id": 47395, "input_score": 88, "output_score": 39, "post_title": "The Duals of $l^\\infty$ and $L^{\\infty}$"}}
{"input": "If \n  $$\n0 \\to A \\to B\\to C,$$ \n  is a left exact sequence of $R$-module, then for any $R$-module $M$, \n  $$\n0 \\to \\operatorname{Hom}_R(M,A)\\to \\operatorname{Hom}_R(M,B)\\to \\operatorname{Hom}_R(M,C),\n$$ \n  is left exact.\n\nI proved the above, and highlighted what I'm a little unfamiliar with: Let $$\n0 \\to A\\ \\xrightarrow{i}\\ B\\ \\xrightarrow{f}\\ C,\n$$ \nand \n$$\n0 \\to \\operatorname{Hom}(M,A)\\ \\xrightarrow{\\operatorname{Hom}(M,i)}\\ \\operatorname{Hom}(M,B)\\ \\xrightarrow{\\operatorname{Hom}(M,f)}\\ \\operatorname{Hom}(M,C).\n$$ \nWe need to show that \n$$\n\\ker\\left(\\operatorname{Hom}(M,f)\\right)=\\operatorname{Hom}(M,i)(\\operatorname{Hom}(M,A)).\n$$ \nLet $i \\circ \\varphi \\in \\operatorname{RHS}$. Then $f \\circ i \\circ \\varphi : M \\to C$ is $0$ since $f \\circ i \\circ \\varphi(M) \\subseteq f( i(A)) = f(\\ker(f))=0$.\nConversely, let $\\psi \\in \\operatorname{LHS}$. Then $f \\circ \\psi = 0$ so that $ f(\\psi(M))=0$. Hence $\\psi(M) \\subseteq \\ker(f)=i(A)$. Since the image of $\\psi$ is contained in the image of $i$, we may factor $\\psi$ as $\\psi=i \\varphi$ with $\\varphi : M \\to A$. (Here is my trial, but I'm not fully understanding this: Since $i$ is injective, $i(A)$ is isomorphic with $A$. So $i^{-1}(\\psi (M)) \\subseteq A$ and if we let $\\varphi=i^{-1} \\psi$, then $\\psi = i \\varphi$.)\nAnd I have one more question: The above looks very messy, especially the notation. Is there a better proof/understanding about it?", "output": "Well, your proof is okay. Let me suggest a slightly different way of looking at it:\nConsider a sequence\n$$ 0\\; \\xrightarrow{\\phantom{ij}} \\; A \\; \\xrightarrow{i\\phantom{j}}\\; B\\; \\xrightarrow{j\\phantom{i}} \\; C$$\nand look at\n$$ 0\\; \\xrightarrow{\\phantom{j_{\\ast}}}\n\\operatorname{Hom}{(M,A)}\\; \\xrightarrow{i_{\\ast}}\\operatorname{Hom}{(M,B)} \\;\\xrightarrow{j_{\\ast}}\\;\n\\operatorname{Hom}{(M,C)},$$\nwhere I write $i_{\\ast} = \\operatorname{Hom}(M,i)$ and $j_{\\ast} =  \\operatorname{Hom}(M,j)$. \nSaying that the first sequence is exact amounts to saying $i = \\ker{j}$, that is $ji = 0$ and $i$ has the universal property as depicted in the first diagram below: If $g: M \\to B$ is such that $jg = 0$ then there exists a unique $f: M \\to A$ such that $if = g$. In other words if $j_\\ast g = 0$ then $g = i_{\\ast}f$, or yet again $\\operatorname{Ker}{j_\\ast} \\subset \\operatorname{Im}{i_{\\ast}}$ and $i_{\\ast}$ is injective.\n\nOn the other hand, the second diagram says: if $g: M \\to B$ is of the form $g = if = i_{\\ast}f$ then $j_{\\ast}g = 0$ (because $j_\\ast g = j_{\\ast}i_{\\ast} f = (ji)_{\\ast}f = 0f = 0$). In other words, $\\operatorname{Im}{i_{\\ast}} \\subset \\operatorname{Ker}{j_{\\ast}}$.\nSumming up, we have shown that for all $M$ the sequence\n$$ 0\\; \\xrightarrow{\\phantom{j_{\\ast}}}\n\\operatorname{Hom}{(M,A)}\\; \\xrightarrow{i_{\\ast}}\\operatorname{Hom}{(M,B)} \\;\\xrightarrow{j_{\\ast}}\\;\n\\operatorname{Hom}{(M,C)}$$\nis exact both at $\\operatorname{Hom}{(M,A)}$ ($i_{\\ast}$ is injective) and at $\\operatorname{Hom}{(M,B)}$ ($\\operatorname{Im}{i_{\\ast}} = \\operatorname{Ker}{j_{\\ast}}$)\u2014you seem to have forgotten about the first point here. \n\nAdded: As witnessed by the argument above, left exactness of $\\operatorname{Hom}$ is essentially the definition of left exactness in the abelian category of $R$-modules. As the comments try to point out, the importance of this fact cannot be overemphasized.\nI would like to add two further points:\n\nA functor $F$ is left exact in your definition if and only if $0 \\to F(A) \\to F(B) \\to F(C)$ is left exact for every short exact sequence $0 \\to A \\to B \\to C \\to 0$.  Indeed, in a left exact sequence $0 \\to A \\to B \\to C$, we may factor $j: B \\to C$ over its image as $B \\twoheadrightarrow \\operatorname{Im}{j} \\rightarrowtail C$ and obtain two exact sequences $$0 \\to A \\to B \\to \\operatorname{Im}{j} \\to 0 \\qquad \\text{and} \\qquad 0 \\to \\operatorname{Im}{j} \\to C \\to \\operatorname{Coker}{j} \\to 0.$$ Applying $F$ to these two exact sequences, we obtain the left exact sequences $$0 \\to F(A) \\to F(B) \\to F(\\operatorname{Im}{j}) \\qquad \\text{and} \\qquad 0 \\to F(\\operatorname{Im}{j}) \\to F(C ) \\to F(\\operatorname{Coker}{j}).$$ Since the kernel of a map is not changed by postcomposing the map with a monomorphism (check this!), we have $$\\operatorname{Ker}{(F(B) \\to F(\\operatorname{Im}{j}))} = \\operatorname{Ker}{(F(B) \\to F(\\operatorname{Im}{j}) \\to F(C))},$$ so by functoriality of $F$ we get a left exact sequence $0 \\to F(A) \\to F(B) \\to F(C)$ as desired.\nA natural question is: When does $\\operatorname{Hom}(M,-)$ send short exact sequences to short exact sequences? In other words, when is $j_\\ast = \\operatorname{Hom}{(M,j)}$ an epimorphism for all short exact sequences $0\\; \\xrightarrow{\\phantom{ij}} \\; A \\; \\xrightarrow{i\\phantom{j}}\\; B\\; \\xrightarrow{j\\phantom{i}} \\; C \\to 0$? \nIn view of left exactness of $\\operatorname{Hom}{(M,-)}$ the question is: Given any morphism $h: M \\to C$ and any epimorphism $j: B \\to C$, when is $h$ of the form $h = j_\\ast g$ for some morphism $g: M \\to B$? \n\n\n\nAs you certainly know, this is precisely the definition of projective modules: $M$ is called projective if and only if $g$ always exists, for all epimorphisms $j: B \\twoheadrightarrow C$ and all $h: M \\to C$. For emphasis: \n\n\nA module $M$ is projective if and only if $\\operatorname{Hom}{(M,-)}$ is exact, that is: it sends short exact sequences to short exact sequences.", "meta": {"post_id": 47401, "input_score": 32, "output_score": 51, "post_title": "Hom is a left-exact functor"}}
{"input": "I didn't know how to phrase the question properly so I am going to explain how this came about.\nI know Math is a very rigorous subject and there are proofs for everything we know and use.  In fact, I am sure that if there was anything that we couldn't prove Mathematically, then we wouldn't use it any where in Math or Science.\nAnyway, here is the story:\nNot to long ago I was having a conversation with someone who insisted in telling me that any human knowledge based on Math is a sham, this of course includes Science.  He then proceed to tell me that he thought this was the case because according to him there is no proof that zero is a number and second he insisted even if we were to say that zero is a number,  that there is no proof that the distance from 0 to 1 is the same as the distance from 1 to 2.\nAccording to this guy, there is no way to measure the distance from something that doesn't exist to something that does.  If I understood him correctly he said that is because he just can't see how we can measure from non-existence to something that does exist.  In this case, he said that there is now way to know for sure that the length or distance from zero or non-existent number to one would be the same as the distance or length from 1 to 2.\nNo matter what I said to him, he just disregarded offhandedly.  I am so upset that I want to punch him in the face, but I would rather get the proofs and show him that he is completely wrong.  To me this would be the equivalent of slapping him on the face with a gauntlet and I would have the satisfaction of knowing that Math and Science are in solid ground and that Mathematicians and/or Scientist haven't pulled a fast one on us.\nSo please post links to where I can read the Mathematical Proofs for these.  Or links that show that to be a fallacy.\nThanks.\n\nThe question was, is there a place where I can get proofs for both of those things.  First, for Zero being a number whether an integer, complex or any other kind.  And second, for a proof that would show that the distance from 0 to 1 is equal to the distance from 1 to 2.  \nBut I guess what I should really have asked is whether or not these proof exist and where I can find them.", "output": "Balthus: \"Zero is a fictional concept!\"\nRobespierre: \"You are right, friend! Mathematicians are fools! By the way, when are you going to pay me the money you owe me?\"\nBalthus: \"I don't owe you any money!\"\nRobespierre: \"Why, certainly; but that's as much as to say, you owe me zero francs; and we just determined that there's no such thing as zero.\"\nBalthus: \"Um...\"\nRobespierre: \"Therefore, since you cannot owe me zero francs, you must owe me some other amount. I demand that you pay me!\"\n\nIn short, rather than arguing from the essential truths (which your friend will ignore anyway), argue from utility. Mathematicians choose axioms based on utility for their current purpose, after all.\nMore than likely, this person just wanted an all-purpose counterargument for science he doesn't want to believe. If you get rid of that motivation to disbelieve, the person would doubtless abandon their arguments.", "meta": {"post_id": 47714, "input_score": 62, "output_score": 46, "post_title": "I need mathematical proof that the distance from zero to 1 is the equal to the distance from 1 to 2"}}
{"input": "Note the $ p < x $ in the sum stands for all primes less than $ x $. I know that for $ s=1 $,\n$$ \\sum_{p<x} \\frac{1}{p} \\sim \\ln \\ln x , $$\nand for $ \\mathrm{Re}(s) > 1 $, the partial sums actually converge to a finite limit called the prime zeta function, which has an analytic continuation to the whole right-half plane but the actual series diverges in the critical strip. So anyway, I'm wondering what the asymptotic behavior of the partial sums are in the limit as $ x \\to \\infty $ for a given value of $ s $ with $ \\mathrm{Re}(s) < 1 $. At first I intuitively conjectured it might be something vaguely like the following \n$$ \\sum_{p<x} \\frac1{p^s} \\sim f(s) \\pi(x)^{1-s} , \\quad f(s) = \\lim_{n\\to\\infty} \\int_0^1 g_n(u) u^{-s} du $$\nbut after some thought I'm not sure if this kind of formula will work after all. Any ideas?\nNote again: I'm asking about asymptotics when $ \\mathrm{Re}(s) < 1 $.", "output": "Asymptotic: For $k>-1$ we have\n$$\\sum_{p\\leq x}p^{k}=\\text{li}\\left(x^{k+1}\\right)+O\\left(x^{k+1}e^{-c\\sqrt{\\log x}}\\right).$$\nProof:\nWe want to sum $\\sum_{p\\leq x}p^{-s}.$ Write this as a Riemann Stieltjes integral and use partial integration. The infinite series converges absolutely if $\\text{Re}(s)>1$, so we assume that $\\text{Re}(s)< 1.$  Then this is\n$$\\sum_{p\\leq x}p^{-s}=\\int_{2}^{x}t^{-s}d\\left(\\pi(t)\\right)=t^{-s}\\pi(t)\\biggr|_{2}^{x}+s\\int_{2}^{x}t^{-s-1}\\pi(t)dt.$$\nWe expect this to be close to  $\\int_{2}^{x}t^{-s}d\\left(\\text{li}(t)\\right)$, so consider\n$$\\int_{2}^{x}t^{-s}d\\left(\\pi(t)\\right)-\\int_{2}^{x}t^{-s}d\\left(\\text{li}(t)\\right)=t^{-s}\\left(\\pi(t)-\\text{li}(t)\\right)\\biggr|_{2}^{x}+s\\int_{2}^{x}t^{-s-1}\\left(\\pi(t)-\\text{li}(t)\\right)dt$$\nwhich by the quantitative prime number theorem is\n$$=O\\left(|s|xe^{-c\\sqrt{\\log x}}\\int_2^x t^{-\\text{Re(s)}-1}dt\\right)=O\\left(\\frac{|s|}{\\text{Re}(s)}x^{1-\\text{Re}(s)}e^{-c\\sqrt{\\log x}}\\right).$$  Notice if rewritten for real $s$, it appears much nicer.\nHence\n$$\\sum_{p\\leq x}p^{-s}=\\int_{2}^{x}\\frac{t^{-s}}{\\log t}dt+O\\left(\\frac{|s|}{\\text{Re}(s)}x^{1-\\text{Re}(s)}e^{-c\\sqrt{\\log x}}\\right).$$\nIf we let $t=u^{\\alpha}$, the integral term becomes $\\int_{2^{1/\\alpha}}^{x^{1/\\alpha}}\\frac{u^{-\\alpha s}u^{\\alpha-1}}{\\log u}du+O(1).$ Because we want the exponent to be zero, we need $-\\alpha s+\\alpha-1=0$ so let $\\alpha=\\frac{1}{1-s}$. Then we see that\n$$\\int_{2}^{x}\\frac{t^{-s}}{\\log t}dt=\\int_{2^{1-s}}^{x^{1-s}}\\frac{1}{\\log u}du=\\text{li}\\left(x^{1-s}\\right)+O(1).$$\n(The $O(1)$ comes from the starting point of the integral)  Consequently, for $\\text{Re}(s)\\neq 1$, we have that\n$$\\sum_{p\\leq x}p^{-s}=\\text{li}\\left(x^{1-s}\\right)+O\\left(\\frac{|s|}{\\text{Re}(s)}x^{1-\\text{Re}(s)}e^{-c\\sqrt{\\log x}}\\right).$$\nIn particular for fixed $s$,\n$$\\sum_{p\\leq x}p^{-s}\\sim\\frac{x^{1-s}}{(1-s)\\log x}.$$\nWhen $\\text{Re}(s)=1$, things are special, and only when $s=1$ do we get $\\log\\log x$.  Also, when $s=-k$ is real, we obtain\n$$\\sum_{p\\leq x}p^{k}=\\text{li}\\left(x^{k+1}\\right)+O\\left(x^{k+1}e^{-c\\sqrt{\\log x}}\\right).$$\nHope that helps,\nEdit: I edited as previously the answer only applied to real $s$.  Now it applies to all $s$ in the complex plane we $\\text{Re}(s)<1$.\nEdit: This question gets asked a lot on math.stackexchange, here are just some of the duplicates:\n\nFinding an asymptotic for the sum $\\sum_{p\\leq x}p^m$\nGeneralization of the Prime number theorem to $\\sum_{p\\leq x}p^{m+1}$\nWhat is the sum of the prime numbers up to a prime number $n$?\nEstimate for sum of negative powers of primes", "meta": {"post_id": 49383, "input_score": 41, "output_score": 34, "post_title": "How does $ \\sum_{p<x} p^{-s} $ grow asymptotically for $ \\text{Re}(s) < 1 $?"}}
{"input": "Let  $ X_1, ... X_n $ a sample of independent random variables with uniform distribution $(0,$$\n\\theta \n$$\r\n) $\nFind a $ $$\n\\widehat\\theta \n$$\r\n $ estimator for theta using the maximun estimator method more known as MLE", "output": "First note that $f\\left({\\bf x}|\\theta\\right)=\\frac{1}{\\theta}$ ,\nfor $0\\leq x\\leq\\theta$ and $0$ elsewhere.\nLet $x_{\\left(1\\right)}\\leq x_{\\left(2\\right)}\\leq\\cdots\\leq x_{\\left(n\\right)}$\nbe the order statistics. Then it is easy to see that the likelihood\nfunction is given by \n$$L\\left(\\theta|{\\bf x}\\right) = \\prod^n_{i=1}\\frac{1}{\\theta}=\\theta^{-n}\\,\\,\\,\\,\\,(*)$$\n  for $0\\leq x_{(1)}$ and $\\theta \\geq x_{(n)}$ and $0$ elsewhere.\nNow taking the derivative of the log Likelihood wrt $\\theta$ gives:\n$$\\frac{\\text{d}\\ln L\\left(\\theta|{\\bf x}\\right)}{\\text{d}\\theta}=-\\frac{n}{\\theta}<0.$$\n So we can say that $L\\left(\\theta|{\\bf x}\\right)=\\theta^{-n}$ is\na decreasing function for $\\theta\\geq x_{\\left(n\\right)}.$ Using\nthis information and (*) we see that $L\\left(\\theta|{\\bf x}\\right)$\nis maximized at $\\theta=x_{\\left(n\\right)}.$ Hence the maximum likelihood\nestimator for $\\theta$ is given by $$\r\n\\hat{\\theta}=x_{\\left(n\\right)}.$$", "meta": {"post_id": 49543, "input_score": 36, "output_score": 70, "post_title": "maximum estimator method more known as MLE of a uniform distribution"}}
{"input": "Is it true that quotient space of a Hausdorff space is necessarily Hausdorff?\n\nIn the book Algebraic Curves and Riemann Surfaces, by Miranda, the author writes:\n\n$\\mathbb{P}^2$ can be viewed as the quotient space of $\\mathbb{C}^3-\\{0\\}$ by the multiplicative action of $\\mathbb{C}^*$. In this way, $\\mathbb{P}^2$ inherits a Hausdorff topology, which is the quotient topology from the natural map from  $\\mathbb{C}^3-\\{0\\}$ to $\\mathbb{P}^2$\n\nIt is true that the complex projective plane $\\mathbb{P}^2$ is Hausdorff, but the above reasoning by Miranda will be true if the statement in the question is true.", "output": "I thought I'd better make my comments into an answer.\nShort answer: No, the quotient space of a Hausdorff space need not be Hausdorff.\nHere are some of the canonical examples (in view of the comments I formulate them using group actions):\n\nLet $\\mathbb{Z}/2$ act on $\\mathbb{R} \\times \\{0\\} \\cup \\mathbb{R} \\times \\{1\\}$ by $g\\cdot(t,0) = (t,1)$ and $g\\cdot (t,1) = (t,0)$ if $t \\neq 0$ and $g\\cdot(0,0) = (0,0)$ and $g\\cdot(1,1) =(1,1)$. Then the quotient space is the line with two origins which is certainly not Hausdorff.\n\nOne could object that this is not a particularly good example because the action is not by homeomorphisms, and I'd have to agree with that. So here's a better example: Let $\\mathbb{Z}$ act on $\\mathbb{R}^2\\smallsetminus\\{0\\}$ via the matrix $\\begin{bmatrix}2&0\\\\0&1/2\\end{bmatrix}$ (more precisely, define $n \\cdot \\begin{bmatrix}x\\\\y\\end{bmatrix} = A^{n}\\begin{bmatrix}x\\\\y\\end{bmatrix}$). Then it is easy to see that the images of $\\begin{bmatrix}1\\\\0\\end{bmatrix}$ and $\\begin{bmatrix}0\\\\1\\end{bmatrix}$ in the quotient don't have disjoint neighborhoods.\n\nA more drastic example is the action of the additive subgroup $\\mathbb{Q}$ on $\\mathbb{R}$. The quotient $\\mathbb{R}/\\mathbb{Q}$ carries the trivial topology (because $\\mathbb{Q}$ is dense in $\\mathbb{R}$).\n\n\nIn a positive direction, I'd like to make the following remarks:\n\nIf $G$ acts by homeomorphisms the quotient map $p: X \\to X / G$ is always open (contrary to general quotient maps): this is because $V \\subset X/G$ is open if and only if $p^{-1}(V) \\subset X$ is open and $p^{-1}(p(U)) = \\bigcup_{g \\in G}gU$ is a union of open sets if $U \\subset X$ is open. Therefore $X/G$ is Hausdorff if and only if the orbit equivalence relation is a closed subset of $X \\times X$.\n\nIf a group $G$ acts properly on the Hausdorff space $X$ then $X/G$ is Hausdorff. See e.g. my post on MO for some quick facts on proper actions and some references.\n\nIf $G$ is a Lie group acting smoothly, properly and freely on a manifold $M$ then $M/G$ is a manifold. This can be found e.g. in Duistermaat-Kolk Lie groups, or Montgomery-Zippin, Topological transformation groups. Unfortunately, I can't give you more precise references, as Google doesn't let me look at the relevant pages. Update: Olivier B\u00e9gassat recommends J.M.\u00a0Lee, Introduction to smooth manifolds for this (which I can only second, thanks!)\n\n\nThe last fact is pretty difficult to prove in this generality (and I hope I haven't forgotten a hypothesis).\nOf course, your question about $P^2 = (\\mathbb{C}^3 \\smallsetminus \\{0\\}) / \\mathbb{C}^{\\ast}$ being a Hausdorff space is covered by remark\u00a02, while remark\u00a03 shows that $P^2$ is even a manifold.", "meta": {"post_id": 50044, "input_score": 26, "output_score": 85, "post_title": "Quotient Space of Hausdorff space"}}
{"input": "Every proof I've seen of Euler's Theorem (that $\\gcd(a,m) = 1 \\implies a^{\\phi(m)} \\equiv 1 \\pmod m$) involves the fact that the units of $\\mathbb{Z}/m\\mathbb{Z}$ form a group of order $\\phi(m)$.  While this is a perfectly good proof, I have to wonder if it was the one that Euler used.  I know that there are fairly old precursors to group theory, but it still seems incongruous.\nThus, my question is: Are there proofs of Euler's Theorem that do not use group/ring theory?  In particular, what proof (if any; I don't know whether Euler actually discovered the theorem) did Euler use himself?", "output": "Consider the set of all numbers less than $n$ and relatively prime to it. Let $\\{a_1,a_2,...,a_{\\varphi(n)}\\}$ be this set.\nConsider a number $c < n$ and relatively prime to it i.e. $c \\in \\{a_1,a_2,\\ldots,a_{\\varphi(n)}\\}$.\nFirst observe that for any $a_i$, $c a_{i} \\equiv a_{j} \\pmod{n}$ for some $j$.\n(True since $c$ and $a_i$ are themselves relatively prime to $n$, their product has to be relatively prime to $n$. This follows immediately from the definition).\nIf $c a_{i} \\equiv c a_{j} \\pmod{n}$ then $a_i = a_j$.\n(True as cancellation can be done since $c$ is relatively prime to $n$).\nHence, if we now consider the set $\\{ca_1,ca_2,...,ca_{\\varphi(n)}\\}$ this is just a permutation of the set $\\{a_1,a_2,...,a_{\\varphi(n)}\\}$.\nThereby, we have $\\displaystyle \\prod_{k=1}^{\\varphi(n)} ca_k \\equiv \\prod_{k=1}^{\\varphi(n)} a_k \\pmod{n}$.\nHence, we get $\\displaystyle c^{\\varphi(n)} \\prod_{k=1}^{\\varphi(n)} a_k \\equiv \\prod_{k=1}^{\\varphi(n)} a_k \\pmod{n}$.\nNow, note that $\\displaystyle \\prod_{k=1}^{\\varphi(n)} a_k$ is relatively prime to $n$ and hence you can cancel them on both sides to get\n$$c^{\\varphi(n)} \\equiv 1 \\pmod{n}$$ whenever $(c,n) = 1$.", "meta": {"post_id": 50542, "input_score": 31, "output_score": 35, "post_title": "Proof of Euler's Theorem without abstract algebra?"}}
{"input": "I've been reading Hatcher's Algebraic Topology, specifically the paragraph about reduced homology $\\tilde{H}_*$ (for singular homology of topological spaces). Can someone please provide reasons why reduced homology is defined and studied?\nI understand the following facts, which are all found in Hatcher's book :\n$-$ The reduced homology of a point is $0$.\n$-$ The reduced homology is the same in all degrees $*$ as the usual singular homology for pairs of spaces $(X,A)$ with $A\\neq \\emptyset$ : $\\tilde{H}_*(X,A)= H_*(X,A)$, and in positive degrees $(*=n>0)$ for single spaces $X$ (that is when $A=\\emptyset$). There is the same long exact sequence in reduced homology for a pair of spaces as in standard homology.\n$-$ In degree $0$, one has $\\tilde{H}_0(X)\\oplus\\mathbb{Z}\\cong H_0(X)$, with the coefficients for homology in $\\mathbb{Z}$.\n$-$ For any space $X$, and any point $\\mathrm{pt}\\in X$, there is an isomorphism $\\tilde{H}_*(X)\\cong H_*(X,\\lbrace \\mathrm{pt}\\rbrace)$\n$-$ This in turn implies that when $A\\subset  U\\subset X$ is such that $A$ is closed, $U$ is open, and $A$ is a strong deformation retract of $U$, then there is an exact sequence in reduced homology (that stems from an exact sequence for standard singular homology)\n$$\\cdots\\rightarrow\\tilde{H}_*(A)\\rightarrow\\tilde{H}_*(X)\\rightarrow\\tilde{H}_*(X/A)\\rightarrow\\cdots$$\nAll of this is straightforward to prove, but it doesn't tell me why reduced homology is defined and when it is used. Can someone please shed some light on this matter?", "output": "The essential reason for preferring reduced homology (as experts do)\nis that the suspension axiom holds in all degrees, as it must when\none generalizes from spaces to spectra and studies generalized \nhomology theories.  Also, when using reduced homology, one need not \nexplicitly use pairs of spaces since $H_*(X,A)$ is the reduced \nhomology of the cofiber $Ci$ of the inclusion $i\\colon A\\to X$.\nThe Eilenberg-Steenrod axioms for homology theories have a variant\nversion for reduced theories, and the reduced and unreduced theories\ndetermine each other.  (See for example my book ``A concise course \nin algebraic topology'').", "meta": {"post_id": 51142, "input_score": 43, "output_score": 37, "post_title": "Use of Reduced Homology"}}
{"input": "Let $1\\leq p < \\infty$. Suppose that\n\n$\\{f_k, f\\} \\subset L^p$ (the domain here does not necessarily have to be finite),\n$f_k \\to f$ almost everywhere, and\n$\\|f_k\\|_{L^p} \\to \\|f\\|_{L^p}$.\n\nWhy is it the case that $$\\|f_k - f\\|_{L^p} \\to 0?$$\nA statement in the other direction (i.e. $\\|f_k - f\\|_{L^p} \\to 0 \\Rightarrow \\|f_k\\|_{L^p} \\to \\|f\\|_{L^p}$ ) follows pretty easily and is the one that I've seen most of the time. I'm not how to show the result above though.", "output": "This is a theorem by Riesz.\nObserve that\n$$|f_k - f|^p \\leq 2^p (|f_k|^p + |f|^p),$$\nNow we can apply Fatou's lemma to\n$$2^p (|f_k|^p + |f|^p) - |f_k - f|^p \\geq 0.$$\nIf you look well enough you will notice that this implies that\n$$\\limsup_{k \\to \\infty} \\int |f_k - f|^p \\, d\\mu = 0.$$\nHence you can conclude the same for the normal limit.", "meta": {"post_id": 51502, "input_score": 104, "output_score": 86, "post_title": "If $f_k \\to f$ a.e. and the $L^p$ norms converge, then $f_k \\to f$ in $L^p$"}}
{"input": "I am struggling to understand why it should be that the $\\sigma$-algebra of subsets of $X$ generated by $\\mathcal{A}$ should be the smallest $\\sigma$-algebra of subsets of $X$ including $\\mathcal{A}$.\n\nLet me try to elucidate my understanding of the topic, in the hope that somebody patient and kind might be able to fill in the gaps.\nIf $X$ is a set, and $\\mathcal{G}$ is any non-empty family of $\\sigma$-algebras of subsets of $X$, then I am very happy that \n$$ \\bigcap \\mathcal{G} := \\left\\{ E : E \\in \\Sigma, \\forall \\Sigma \\in \\mathcal{G} \\right\\},$$\nthe intersection of all the $\\sigma$ algebras belonging to $\\mathcal{G}$ is a $\\sigma$-algebra of subsets of $X$.  \nNow, if $\\mathcal{A}$ is any any of subsets of $X$, then defining\n$$ \\mathcal{G} := \\left\\{ \\Sigma : \\Sigma \\ \\textrm{is a } \\sigma \\textrm{-algebra of subsets of } X, \\mathcal{A} \\subseteq \\Sigma \\right\\},$$\nthen we have by definition that $\\mathcal{G}$ is a family of $\\sigma$-algebras of subsets of $X$; also, since $\\mathcal{P} X \\in \\mathcal{G}$ we have that it is non-empty. So $\\Sigma_{\\mathcal{A}} := \\bigcap \\mathcal{G}$, called the $\\sigma$-algebra of subsets of $X$ generated by $\\mathcal{A}$, is a $\\sigma$-algebra of subsets of $X$. Because $\\mathcal{A} \\subseteq \\Sigma$ for every $\\Sigma \\in \\mathcal{G}$, we have $\\mathcal{A} \\subseteq \\Sigma_{\\mathcal{A}}$; thus $\\Sigma_{\\mathcal{A}}$ itself belongs to $\\mathcal{G}$.\nHowever, I cannot get my head around why it should be that $\\Sigma_{\\mathcal{A}}$ should be the smallest $\\sigma$-algebra of subsets of $X$ including $\\mathcal{A}$, perhaps because I am not entirely sure what this statement means explicitly (namely, I have problems interpreting 'smallest' and 'including')! I'd be very relieved if someone could try to explain this to me as it has been bugging me for a week now; I have a feeling that it might rely heavily on the $\\bigcap$, but I'm not sure exactly how...", "output": "Let me make a general comment rather than a specific one, because the construction that you are having trouble with is one that is very common and very useful (though it does have its limitations; see below) so it is important and good to have it \"down\" properly.\nYou have the following situation: you are considering a certain type of object of interest. For simplicity, let's look at the earliest example that most students encounter, which is vector spaces. So, you are looking at vector spaces. Specifically, you are looking at a particular vector space $\\mathbf{V}$.\nThe objects have sub-objects (subspaces). These are subsets of your original $\\mathbf{V}$, which are also objects (vector spaces) in their own right. Not every subset is a subobject, but every subobject is a subset.\nIn this situation, it is often fruitful to consider the following problem:\n\nGiven a subset $S$ of $\\mathbf{V}$, what is the smallest subspace of $\\mathbf{V}$ that contains $S$? \n\nThat is, we want to find a $\\mathbf{W}$ with the following properties:\n\n$\\mathbf{W}$ is a subspace of $\\mathbf{V}$;\n$S$ is contained in $\\mathbf{W}$ (\"...that contains $S$\");\nIf $\\mathbf{Z}$ is any subspace of $\\mathbf{V}$ that contains $S$, then $\\mathbf{W}\\subseteq\\mathbf{Z}$ (\"... smallest ...\")\n\nThis is the situation you have at hand, and it's also a very common situation that we encounter over and over again. Some examples:\n\nGiven a group $G$ and a subset $S$, to find the smallest subgroup of $G$ that contains $S$ (the \"subgroup generated by $S$\");\nGiven a group $G$ and a subset $S$, to find the smallest normal subgroup of $G$ that contains $S$;\nGiven a subset $S$ of the plane $\\mathbb{R}^2$, to find the smallest convex set that contains $S$ (the \"convex hull of $S$\");\nGiven a set $X$ and a collection of subsets $\\mathcal{S}\\subseteq \\mathcal{P}(X)$, find the smallest $\\sigma$-algebra on $X$ that contains $\\mathcal{S}$ (the case you have);\nGiven a set $X$ and a relation $R$ on $X$, find the smallest transitive relation on $X$ that extends $R$ (the \"transitive closure\");\nGiven a topological space $X$ and a subset $S$, find the smallest closed subset of $X$ that contains $S$ (the \"closure of $S$\").\n\nand so on and so forth.\nNow, in general, such a thing may not exist; or there may be minimal objects but no minimum object. For example, if in the last example above you replace \"closed\" with \"open\", there may be no such object: if $X=\\mathbb{R}$ and $S=[0,1]$, there is no \"smallest open set that contains $S$\". \nBut in many situations, there is one single observation that lets you conclude that such as \"smallest subobject\" must exist. Namely, if you can show that the intersection of any collection of \"subobjects\" is again a \"subobject\". For the example with vector spaces: is the intersection of an arbitrary family of subspaces of $\\mathbf{V}$ itself a subspace of $\\mathbf{V}$? For the above examples:\n\nIs the intersection of an arbitrary family of subgroups of $G$, itself a subgroup of $G$?\nIs the intersection of an arbitrary family of normal subgroups of $G$ itself a normal subgroup of $G$?\nIs the intersection of an arbitrary family of convex subsets of $\\mathbb{R}^2$ itself a convex subset of $\\mathbb{R}^2$?\nIs the intersection of an arbitrary family of $\\sigma$-algebras on $X$ itself a $\\sigma$-algebra on $X$?\nIs the intersection of an arbitrary family of transitive relations on $X$ itself a transitive relation on $X$?\nIs the intersection of an arbitrary family of closed subsets of $X$ itself a closed subset of $X$?\n\nWhen the answer is \"yes\", then the following construction will always show that there is such a thing as the \"smallest subobject that contains $S$\":\n\nTake the family of all subobjects that contain $S$; then take the intersection of the family. That's the smallest subobject that contains $S$.\n\nWhy does this work?\nBecause:\n(i) There is at least one subobject that contains $S$, (namely the original object itself; for $\\sigma$-algebras, this would be $\\mathcal{P}(X)$; for the transitive closure example, you would take the \"total relation\" $X\\times X$). \n(ii) Since the intersection of an arbitrary family of subobjects is a subobject (this is our assumption), then this intersection is a subobject.\n(iii) Since each thing being intersected contains $S$, the intersection contains $S$.\nThis means that the intersection is indeed a subobject that contains $S$. Finally:\n(iv) The intersection is always contained in each and every element of the family being intersected. So if $\\mathbf{Z}$ is any subobject that contains $S$, then it is a member of the family being intersected, so the intersection is contained in $\\mathbf{Z}$. This shows the intersection is indeed the \"smallest subobject\" with the desired properties.\nSo:\n\nTo find the smallest subspace of $\\mathbf{V}$ that contains $S$, intersect all subspaces that contain $S$.\nTo find the smallest subgroup of $G$ that contains $S$, intersect all subgroups that contain $S$.\nTo find the smallest normal subgroup of $G$ that contains $S$, intersect all normal subgroups that contain $S$.\nTo find the smallest convex set that contains $S$, intersect all convex subsets of $\\mathbb{R}^2$ that contain $S$.\nTo find the smallest $\\sigma$-algebra that contains $S$, intersect all $\\sigma$-algebras that contain $S$.\nTo find the smallest transitive relation that contains $R$, intersect all transitive relations that contain $R$.\nTo find the smallest closed subset that contains $S$, intersect all closed subsets that contain $S$.\n\nAnd this works like magic. Voil\u00e1! You have shown that this object exists. It necessarily has the properties you want.\nThis is a \"top-down\" approach. Imagine yourself looking at the \"big object\", and you are \"paring it down\" until you get \"just enough\" for the object you want (intersections make things smaller; you are paring down stuff that may not be needed).\nThe problem? Like most magic spells, it doesn't really tell you much about the end product. The fact that the end product appeared \"as if by magic\" means that you are likely to be as clueless about the actual nature of the \"smallest object\" in question as you were when you started. You now know that there is such a thing, but you don't really know what it \"looks like\".\nThat is why in almost every situation like this, you also want a \"bottom-up\" description of this \"smallest subobject that contains $S$\". You want an explicit description of what it actually looks like. For the examples above:\n\nThe smallest subspace of $\\mathbf{V}$ that contains $S$ is the set of all linear combinations of vectors in $S$.\nThe smallest subgroup of $G$ that contains $S$ is the set of all finite products of elements of $S$ and their inverses.\nThe smallest normal subgroup of $G$ that contains $S$ is the set of all finite products of conjugates of elements of $S$ and their inverses.\nThe smallest convex subset of $\\mathbb{R}^2$ that contains $S$ is the set of all convex combinations of elements of $S$.\nAsaf gives an explicit description of the smallest $\\sigma$-algebra on $X$ that contains $S$ in his answer, described by starting from $S$.\nThe smallest transitive relation on a set $X$ that contains a given relation $R$ is the set of all pairs $(a,b)$ such that there exist a finite sequence  $x_0,x_1,\\ldots,x_n$ of elements of $X$ such that $x_0=a$, $x_n=b$, and $(x_i,x_{i+1})\\in R$ for $i=0,\\ldots,n-1$.\nThe smallest closed subset of a topological space $X$ that contains a given set $S$ is equal to $S\\cup\\partial S$ or to $S\\cup S'$.\n\nIn each of these cases, one would have to show that the given description actually has the desired properties. This is a \"bottom-up\" approach.\nThe \"top-down\" description has the benefit of simplicity, that the \"universal properties\" that define the object are very clearly satisfied, and that they make proving results about how the \"smallest object\" relates to other objects easy. However, the \"top-down\" description is usually very hard to actually use to prove things about the specific smallest object. The \"bottom-up\" construction has the benefit of (usually) being a very concrete way of getting your hands on the object itself, making it easy to prove things about the object itself, but proving the universal properties is usually difficult. Thus, for example, the top-down definition of \"subspace $\\mathbf{V}$ generated by $S$\" in the linear algebra setting makes it very hard to figure out things like the dimension of the subspace, or a basis, while the \"bottom-up\" approach makes that very easy, but then proving that the collection of all linear combinations forms a subspace is more difficult than simply taking an intersection of subspaces.\nIn most books or presentations, when discussing \"the smallest X that contains S\", you will see one of two approaches:\n\nDefine it as a big intersection, then prove a theorem that gives the \"bottom-up\" description; or\nGive a \"bottom-up\" description; then prove the object described has the desired properties of being a subobject, containing S, and being the \"smallest\".\n\nWhenever possible, you want both descriptions because they have complementary strengths and weaknesses.", "meta": {"post_id": 54172, "input_score": 66, "output_score": 118, "post_title": "The $\\sigma$-algebra of subsets of $X$ generated by a set $\\mathcal{A}$ is the smallest sigma algebra including $\\mathcal{A}$"}}
{"input": "A classical Banach limit is very useful concept for me, but there is a problem with the integration and even with the measurability, this means for a sequence $(f_n)_{n\\in \\mathbb{N}}$ of measurable (eg borel) and uniformly bounded functions, the function $x \\mapsto L((f_n(x))_{n\\in\\mathbb{N}})$, where $L$ is Banach limit, is not measurable in general.\nI heard about a stronger concept of Banach limit of Mokobodzki called \"medial limit\", that exists, assuming the continuum hypothesis. This functional apparently has the same properties like Banach limit and additionally, which is important for me, commutes with integration in appropriate settings, this means it would preserve the measurability and satisfied condition $M((\\int f_n(x)\\mu(dx))_{n\\in\\mathbb{N}}))=\\int M((f_n(x))_{n\\in\\mathbb{N}})\\mu(dx)$, with appropriate assumptions.\nUnfortunately, I found only a brief mention of this notion in the available literature. Could somebody explain to me this idea more precisely or point me to available literature on this subject?", "output": "You are talking about what is often called a measure-linear mean or limite m\u00e9diale in the sense of Mokobodzki. $\\newcommand{mypart}[1]{\\unicode{x2014}\\text{ #1 }\\unicode{x2014}}$\nEdit: Given my recent revisiting the literature on this topic, it seems like medial limit (when thought of as an ultrafilter on $\\mathbb{N}$) or medial mean (when thought of as linear functional on $\\ell^{\\infty}(\\mathbb{N})$) are the most wide-spread terms nowadays.\n\nEdit: Since this answer grew longer than I ever would have imagined, I thought a short overview might be in order. I wanted to keep the chronology visible. The text roughly grew in the order it is displayed below:\n\nDefinition\nSimple comments on the definition\nSome pointers to the literature\nA Birkhoff-type result with a one-line proof\nReference to Fremlin's book, weakening and elaborating on (seemingly necessary) hypotheses\nLarson's result on the necessity of assuming more than ZFC\n\n[Meta: A big thank you to Vadim\u00a0Ka\u012dmanovich, Freddy\u00a0Delbaen and Andreas\u00a0Blass.]\n\n$$\\mypart{1}$$\nA medial mean is a linear functional $\\mathfrak{m}: \\ell^{\\infty}(\\mathbb{N}) \\to \\mathbb{R}$ satisfying the following properties:\n\nIt is positive, i.e., $\\mathfrak{m}{(f)} \\geq 0$ if $f \\geq 0$.\n\nIt is normalized, i.e., $\\mathfrak{m}{(1_{\\mathbb{N}})} = 1$.\n\nIt is shift-invariant: if $Tf(x) = f(x+1)$ then $\\mathfrak{m}{(Tf)} = \\mathfrak{m}{(f)}$.\n\nIt is universally measurable on the unit ball $B = [0,1]^{\\mathbb{N}}$ of $\\ell^{\\infty}$: More precisely, if $\\mu$ is a Borel probability measure with respect to the weak$^{\\ast}$-topology (= product topology) on $B$ then $\\mathfrak{m}$ is $\\mu$-measurable.\n\nIt is measure-linear: If $f_n : [0,1] \\to [0,1]$ is a sequence of Borel measurable functions then the function $f = \\mathfrak{m}((f_n))$ is measurable (!) with respect to any probability measure $\\mu$ on $B$, and we have the identity:\n$$\\int f \\,d\\mu = \\int \\mathfrak{m}(f_n)\\,d\\mu = \\mathfrak{m}\\left[\\int f_n\\,d\\mu\\right].$$\nEquivalently, this holds for a sequence of functions defined on any probability space (this is quite non-trivial).\n\n\n\n$$\\mypart{2}$$\n\nThe first three properties are saying that $\\mathfrak{m}$ is a Banach limit. In particular, $\\mathfrak{m}$ has norm one and gives the usual limit for convergent sequences.\nThe measurability property is essential and subtle. One can show that a weak$^{\\ast}$-measurable linear functional is automatically weak$^{\\ast}$-continuous on $\\ell^{\\infty}$, and this would mean that $\\mathfrak{m}$ is representable by an $\\ell^1$ function, which it obviously can't be in view of 1,2,3. Thus weak$^{\\ast}$-measurability must be weakened. If you take just any Hahn-Banach extension from the convergent sequences to the bounded sequences of the limit functional, there is no reason for this extension to be measurable.\nProperty 5. can be seen as a version of Fubini involving one finitely additive measure. Another way of looking at it would be to see it as a very strong version of the bounded convergence theorem. Indeed, write $\\mathrm{LIM}$ instead of $\\mathfrak{m}$. Edit: Note that\u00a05. does not follow immediately from 4. as I carelessly asserted earlier (we're composing measurable maps \"the wrong way around\").\nThe existence of a measurable mean is (as far as I know) not provable in ZFC. The usual assumption people make is the continuum hypothesis, but Martin's Axiom MA is sufficient. MA is in some very na\u00efve sense \"half the continuum hypothesis\": G\u00f6del, in What is Cantor's Continuum Problem  famously listed 6 reasons why the continuum hypothesis should be considered implausible, and MA \"only\" implies 3 of them.\n\n\n$$\\mypart{3}$$\nThese surprising gadgets were discovered by Gabriel Mokobodzki, Ultrafiltres rapides sur N. Construction d'une densit\u00e9 relative de deux potentiels comparables.,  S\u00e9minaire de Th\u00e9orie du Potentiel, dirig\u00e9 par M. Brelot, G. Choquet et J. Deny: 1967/68, Exp.\u00a012, 22\u00a0pp. Secr\u00e9tariat math\u00e9matique, Paris 1969. The construction was simplified and clarified by Paul-Andr\u00e9 Meyer Limites m\u00e9diales, d'apr\u00e8s Mokobodzki, S\u00e9minaire de Probabilit\u00e9s, VII (Univ. Strasbourg, ann\u00e9e universitaire 1971\u20131972), pp. 198\u2013204. Lecture Notes in Math., Vol. 321, Springer, Berlin, 1973.\nThese medial limits were studied rather in-depth by J.P.R. Christensen, but I don't remember in which papers exactly. One nice paper that makes use of them is Ka\u012dmanovich-Fisher, A Poisson formula for harmonic projections, Ann. Inst. H. Poincar\u00e9 Probab. Statist. 34 (1998), no. 2, 209\u2013216. See also A.\u00a0Fisher, Convex-invariant means and a pathwise central limit theorem, Adv. in Math. 63 (1987), no. 3, 213\u2013246.\n\n$$\\mypart{4}$$\nAdded: Here's one classic sample application of medial limits (taken fom Ka\u012dmanovich-Fisher):\nLet $T: X \\to X$ be an ergodic and measure-preserving transformation on a probability space $(X,\\mu)$. If $f$ is a bounded and measurable function then we can consider its time translates $(T^{\\ast n}f)(x) = f(T^nx)$. If we fix a medial limit $\\mathfrak{m}$ then the time average $\\mathcal{T}\\,f = \\mathfrak{m}((T^{\\ast n}f))$ is $T$-invariant and measurable, hence constant by ergodicity of $T$. By measure-linearity and our assumptions on $T$ we have\n$$\\mathcal{T}\\,f = \\int \\mathcal{T}\\,f\\,d\\mu = \\int \\mathfrak{m}{(T^{\\ast n}f)}\\,d\\mu = \\mathfrak{m}\\left[\\int T^{\\ast n}f\\,d\\mu\\right] = \\mathfrak{m}\\left[\\int f\\,d\\mu\\right] = \\int f\\,d\\mu = \\mathcal{S}_\\mu f$$\nwhere $\\mathcal{S}_\\mu$ denotes the space average with respect to $\\mu$. This is at least morally the content of the classical Birkhoff ergodic theorem. See also this related MO-thread.\n\n$$\\mypart{5}$$\nAdded Later: An English reference giving a careful proof of the existence of a medial limits is volume\u00a05\u00a0Part\u00a0I of Fremlin's Measure Theory compendium. In fact, he proves the existence in Theorem\u00a0538S on page\u00a0277 under the quite a bit weaker assumption than Martin's axiom that $\\mathbb{R}$ can't be covered by strictly less than $\\mathfrak{c}$ meager sets. A rather detailed discussion of medial limits is contained from\u00a0538P (page\u00a0272) on where you'll find variants of Fubini, dominated convergence, Vitali and similar things (in case you'll find yourself needing them).\nIn fact, the consistency needed for set-theoretic measure theory (about which I don't know much) depends on certain cardinal numbers to coincide. These are subsumed in what is called Cicho\u0144's diagram amalgamated with Martin's diagram by Fremlin (most of the entries in the picture below are not of interest here):\n\nThe entries appearing here are cardinal numbers which increase from bottom up and from left to right. Assuming the continuum hypothesis, which is equivalent to $\\omega_1 = \\mathfrak{c}$ (the continuum is equal to the first uncountable ordinal), all these cardinalities collapse to one. Martin's axiom can be expressed as $\\mathfrak{m} = \\mathfrak{c}$, where $\\mathfrak{m}$ is Martin's number (this is of course a different $\\mathfrak{m}$ than the one in the main body of this answer). Now the assertion that $\\mathbb{R}$ can't be covered by less than continuum many meager sets is equivalent to $\\operatorname{cov}{\\mathcal{M}} = \\mathfrak{c}$. This should give some impression on how much weaker this assumption is. In volume\u00a05 Fremlin collects facts that are (usually) undecidable in ZFC and proves them with reasonably weak assumptions beyond.\nEven Later: The question of necessity of going beyond ZFC seems to be open if you're willing to take Fremlin's word for it: Apparently, it is unknown whether it is relatively consistent with ZFC that there are no medial limits at all. In fact, this is asked as problem in 538Z. I'm quoting the very last paragraph of 538 (page\u00a0281):\n\nFor most of the classes of filter [sic] here, there is a question concerning their existence. Subject to the continuum hypothesis, there are many Ramsey ultrafilters, and refining the argument we find that the same is true if $\\mathfrak{p = c}$ (538Yb). There are many ways of forcing non-existence of Ramsey ultrafilters, of which one of the simplest is in 553H below. With more difficulty, we can eliminate $p$-point ultrafilters (Wimmers\u00a082) or rapid filters (Miller\u00a080) or nowhere dense filters and therefore measure-centering ultrafilters (538Hd, Shelah\u00a098). It is not known for sure that we can eliminate medial limits or measure-converging filters (538Z).\n\nThe relevant statement mentioned here is 538Z on page\u00a0280:\n\n\n538Z Problem Show that it is relatively consistent with ZFC to suppose that there are no measure-converging filters on $\\mathbb{N}$. (Note that in this case there are no medial limits, by\u00a0538Rd.)\n\n\n\n$$\\mypart{6}$$\nUpdate: Andreas Blass kindly pointed out to me on meta.MO that the problem was settled in\u00a02009 by Paul Larson in The filter dichotomy and medial limits, J. Math. Logic, vol.\u00a09, no. 2\u00a0(2009) pp.\u00a0159-165 (see also the ESI preprint\u00a02173). Indeed, based on results by Blass and Laflamme concerning models constructed by Miller and Blass\u2013Shelah, Paul Larson establishes\n\nCorollary 3.3 If ZFC is consistent, then so is ZFC + \u201cthere exist no medial limits.\u201d\n\nI'd like to point out that Larson mentions\u2014reference\u00a0[10] is Fremlin's book I mentioned above:\n\nAs far as we know, the weakest hypothesis known to be sufficient [for the existence of medial limits] (see 538S of [10]) is the statement that the reals are not a union of fewer than continuum many meager sets (i.e., that the covering number for the meager ideal is the continuum).\n\nAs a final remark: Larson's paper gives quite a few references on results concerning medial limits (I already mentioned quite a few of them above), he lists in particular numerous papers by Christensen and attributes the weakening to Martin's Axiom to Dag Norman.", "meta": {"post_id": 54554, "input_score": 20, "output_score": 36, "post_title": "Medial Limit of Mokobodzki (case of Banach Limit)"}}
{"input": "What is the relation between normal extension and separable extension?\nLet F be the algebraic extension of K, if F is a separable extension of K,if and only if F is a normal extension of K? is this correct?", "output": "Neither implies the other. So, there exist separable extensions that are not normal, and normal extensions that are not separable.\n$\\mathbb{Q}(\\sqrt[3]{2})/\\mathbb{Q}$ is a separable extension that is not normal.\nIt is separable because any extension of characteristic zero fields is separable, but it is not normal because (for example) not all algebraic conjugates of $\\sqrt[3]{2}$ lie in the field $\\mathbb{Q}(\\sqrt[3]{2})$ (its conjugates are $\\zeta_3\\sqrt[3]{2}$ and $\\zeta_3^2\\sqrt[3]{2}$ where $\\zeta_3$ is a cube root of unity; these are complex numbers, while $\\mathbb{Q}(\\sqrt[3]{2})\\subset\\mathbb{R}$).\n$\\mathbb{F}_p(\\sqrt[p]{T})/\\mathbb{F}_p(T)$ is a normal extension that is not separable.\nIt is normal because $\\mathbb{F}_p(\\sqrt[p]{T})$ is the splitting field of $x^p-T\\in \\mathbb{F}_p(T)[x]$, but it is not separable because the minimal polynomial of $\\sqrt[p]{T}$ is $x^p-T=(x-\\sqrt[p]{T})^p$ which does not have distinct roots.\nA field extension $L/K$ that is both normal and separable is called a Galois extension.", "meta": {"post_id": 54565, "input_score": 15, "output_score": 37, "post_title": "What is the relation between normal extension and separable extension?"}}
{"input": "In http://en.wikipedia.org/wiki/Manifold_(mathematics)#Construction, it says that 6 charts can be used to make an atlas for a sphere. But the text shows that you have a chart for the northern hemisphere, and you can make a similar chart for the southern hemisphere. Hence, these two charts cover the entire sphere.\nWhat am I doing wrong?", "output": "The northern hemisphere and southern hemisphere don't cover the entire sphere. The sphere is \n$$\\mathbb{S}^2=\\{(x,y,z)\\in\\mathbb{R}^3\\mid x^2+y^2+z^2=1\\}.$$\nThe northern and southern hemispheres are, respectively,\n$$\\mathbb{S}_N^2=\\{(x,y,z)\\in\\mathbb{S}^2\\mid z>0\\},\\qquad\\mathbb{S}_S^2=\\{(x,y,z)\\in\\mathbb{S}^2\\mid z<0\\}.$$\nThese miss the equator $\\{(x,y,z)\\in\\mathbb{S}^2\\mid z=0\\}$. Adding \"east\" and \"west\" hemispheres \n$$\\mathbb{S}_W^2=\\{(x,y,z)\\in\\mathbb{S}^2\\mid x>0\\},\\qquad\\mathbb{S}_E^2=\\{(x,y,z)\\in\\mathbb{S}^2\\mid x<0\\}$$\nstill doesn't get everything: we are missing the points on the equator $(0,1,0)$ and $(0,-1,0)$. Finally, adding the last two hemispheres (east and west, only rotated 90 degrees) covers the entire sphere.\n\nThis raises the question, why are we defining our hemispheres with $>$ and $<$? Perhaps we could instead use $\\leq $ and $\\geq$, and this would let us cover the sphere with two hemispheres?\nThe answer is that a chart of a manifold needs to be a homeomorphism between an open subset of the manifold with an open subset of $\\mathbb{R}^n$. The sets\n$$\\{(x,y,z)\\in\\mathbb{S}^2\\mid z\\geq 0\\},\\qquad\\{(x,y,z)\\in\\mathbb{S}^2\\mid z\\leq 0\\}$$\nare not open in the topology of $\\mathbb{S}^2$ (which is the subspace topology inherited from $\\mathbb{R}^3$). So we can't use them as coordinate neighborhoods in the manifold structure of $\\mathbb{S}^2$.\n\nIt does warrant mentioning, however, that we can cover the sphere using only two charts, via stereographic projection. The two open subsets of $\\mathbb{S}^2$ acting as our coordinate domains are \n$$\\mathbb{S}^2-\\{(0,0,1)\\},\\qquad\\mathbb{S}^2-\\{(0,0,-1)\\}$$\nand for each, we project a line from the removed point to the plane, which one can check gives a continuous map. It is a tedious (but important) exercise to demonstrate that the smooth structure determined by stereographic projection is the same as that of the hemispheres (i.e., they are compatible atlases).", "meta": {"post_id": 54643, "input_score": 24, "output_score": 43, "post_title": "why not just 2 charts to make atlas for sphere?"}}
{"input": "This about the famous article \nZermelo, E., Beweis, da\u00df jede Menge wohlgeordnet werden kann, Math. Ann.\u00a059\u00a0(4), 514\u2013516\u00a0(1904), \navailable here. Edit: Springer link to the original (OCR'ed, may be behind a paywall)\nAn English translation can be found in the book Collected Works/Gesammelte Werke by Ernst Zermelo. Alternative source: the book From Frege to G\u00f6del: a source book in mathematical logic, 1879-1931, by Jean Van Heijenoort.\n[See also this interesting text by Dan Grayson.]\nI don't understand the paragraph on the last page whose English translation is\n\nAccordingly, to every covering $\\gamma$ there corresponds a definite well-ordering of the set $M$, even if the well-orderings that correspond to two distinct coverings are not always themselves distinct. There must at any rate exist at least one such well-ordering, and every set for which the totality of subsets, and so on, is meaningful may be regarded as well-ordered and its cardinality as an \"aleph\". It therefore follows that, for every transfinite cardinality, \n  $$\\mathfrak m=2\\mathfrak m=\\aleph_0\\,\\mathfrak m=\\mathfrak m^2,\\mbox{and so forth;}$$\n  and any two sets are \"comparable\"; that is, one of them can always be mapped one-to-one onto the other or one of its parts.\n\nIt seems to me Zermelo says that the fact that any set can be well-ordered immediately implies that any infinite cardinal equals its square. Is this interpretation correct? If it is, what is the argument?\nSide question: What is, in a nutshell, the history of the statement that any infinite cardinal equals its square? Where was it stated for the first time? Where was it proved for the first time? (In a similar vein: where was the comparability of any two cardinal numbers proved for the first time?)\n\nEdit: German original of the passage in question:\n\nSomit entspricht jeder Belegung $\\gamma$ eine ganz bestimmte Wohlordnung der Menge $M$, wenn auch nicht zwei verschiedenen Belegungen immer verschiedene. Jedenfalls mu\u00df es mindestens eine solche Wohlordnung geben, und jede Menge, f\u00fcr welche die Gesamtheit der Teilmengen usw. einen Sinn hat, darf als eine wohlgeordnete, ihre M\u00e4chtigkeit als ein \u201eAlef\u201c betrachtet werden. So folgt also f\u00fcr jede transfinite M\u00e4chtigkeit\n  $$\\mathfrak m=2\\mathfrak m=\\aleph_0\\,\\mathfrak m=\\mathfrak m^2\\text{ usw.,}$$\n  und je zwei Mengen sind miteinander \u201evergleichbar\u201c, d.\u00a0h. es ist immer die eine ein-eindeutig abbildbar auf die andere oder einen ihrer Teile.", "output": "The following is a theorem of $ZF$:\u00a0\nThe axiom of choice holds if and only if for every infinite set $A$, there exists a bijection of $A$ with $A\\times A$. (i.e. $|A|=|A|^2$)\u00a0\n\nLet us overview the theorem of Zermelo, namely if the axiom of choice holds then $\\kappa=\\kappa^2$ for every infinite $\\kappa$.\nThis is fairly simple, by the canonical well ordering of pairs.\nConsider $\\alpha\\times\\beta$, this can be well ordered as ordinal multiplication (that is $\\beta$ copies of $\\alpha$, i.e. lexicographical ordering), or it can be ordered as following:\n$$(x,y)<(w,z)\\iff\\begin{cases} \\max\\{x,y\\}<\\max\\{w,z\\}\\\\ \\max\\{x,y\\}=\\max\\{w,z\\}\\land x<w\\\\ \\max\\{x,y\\}=\\max\\{w,z\\}\\land x=w\\land y<z\\end{cases}$$\nThis is a well-ordering (can you see why?). Now we will prove that $\\kappa\\times\\kappa$ has the same order type as $\\kappa$, this is a proof that the two sets have the same cardinality, since similar order types induce a bijection.\nFirstly, it is obvious that $\\kappa$ is at most of the order type of $\\kappa\\times\\kappa$ since the order type of $\\kappa$ can be simply be written as $\\alpha\\mapsto (\\alpha,\\alpha)$. The other direction we prove by induction on $\\alpha$ that for the initial ordinal $\\omega_\\alpha$ it is true: $\\omega_\\alpha=\\omega_\\alpha\\times\\omega_\\alpha$.\nFact: If $\\delta<\\omega_\\alpha$ (where $\\omega_\\alpha$ is the $\\alpha$-th initial ordinal) then $|\\delta|<\\aleph_\\alpha$.\nThe claim is true for $\\omega_0=\\omega$ since for any $k$ the set $\\{(n,m)\\mid (n,m)<(k,k)\\}$ is finite. Therefore the order type of $\\omega\\times\\omega$ is the supremum of $\\{k_n\\mid n\\in\\omega\\}$ and $k_n$ are finite. Simply put, the order type is $\\omega$.\nNow assume (by contradiction) $\\alpha$ was the least ordinal such that $\\omega_\\alpha$ was a counterexample to this claim, i.e. $\\omega_\\alpha$ is strictly less than the order type of $\\omega_\\alpha\\times\\omega_\\alpha$. \nLet $(\\gamma,\\beta)<\\omega_\\alpha\\times\\omega_\\alpha$ be the pair of ordinals such that the order type of $\\{(\\xi,\\zeta)\\mid (\\xi,\\zeta)<(\\gamma,\\beta)\\}$ is $\\omega_\\alpha$.\nTake $\\delta$ such that $\\omega_\\alpha>\\delta>\\max\\{\\gamma,\\beta\\}$ then $(\\gamma,\\beta)<(\\delta,\\delta)$ and in particular $\\{(\\xi,\\zeta)\\mid (\\xi,\\zeta)<(\\delta,\\delta)\\}$ has cardinality of at least $\\omega_\\alpha$, as it extends a well order of the type $\\omega_\\alpha$.\nHowever, $\\delta<\\omega_\\alpha$ by the fact above it is of smaller cardinality, and thus that set has the cardinality $|\\delta|\\times |\\delta|=|\\delta|<\\omega_\\alpha$ by our induction assumption. Hence, a contradiction.\n\nThe other direction, also known as Tarski's theorem (I managed to find that it was published around 1923, but I could not find a proper reference.) is as follows:\nSuppose that for all infinite $A$, there exists a bijection of $A$ with $A\\times A$ then the axiom of choice holds.\nThe proof (which I will not bring here, as it would require a few more notations and definitions - I did give it here) uses the concept of Hartogs number (the least ordinal which cannot be injected into $A$). The proof in its essence is:\nIf $\\aleph(A)$ is the Hartog of $A$, \n$$A+\\aleph(A)=(A+\\aleph(A))^2=A^2+2\\cdot A\\cdot\\aleph(A)+\\aleph(A)^2\\ge A\\cdot\\aleph(A)\\ge A+\\aleph(A)$$\nWe then use (or prove) a theorem that if $A+\\aleph(A)=A\\cdot\\aleph(A)$ then $A$ can be well ordered.\nHistorically, Tarski came to publish this theorem. It was rejected at first. Polish-American mathematician Jan Mycielsi relates in his article A System of Axioms of Set Theory for the Rationalists Notices AMS, February\u00a02006,\u00a0p.209:\n\nTarski told me the following story. He tried to publish his theorem (stated above) in the Comptes Rendus Acad. Sci. Paris but Fr\u00e9chet and Lebesgue refused to present it. Fr\u00e9chet wrote that an implication between two well known propositions is not a new result. Lebesgue wrote that an implication between two false propositions is of no interest. And Tarski said that after this misadventure he never tried to publish in the Comptes Rendus.\n\nFound via Wikipedia article on the axiom of choice.", "meta": {"post_id": 54892, "input_score": 26, "output_score": 35, "post_title": "About a paper of Zermelo"}}
{"input": "This seems to be a common result. I've been trying to follow the bijective proof of it, which can be found easily online, but the explanations go over my head. It would be wonderful if you could give me an understandable explanation of the proof and let me know how I'd go about finding such a bijection.", "output": "I'll give a sketch of the bijective proof; ask me if there's some part you don't understand and need fleshed out (or maybe someone else will post a detailed version).\nThe important idea is that every number can be expressed uniquely as a power of 2 multiplied by an odd number. (Divide the number repeatedly by 2 till you get an odd number.) For instance, $120 = 2^3 \\times 15$ and $68 = 2^2 \\times 17$ and $81 = 2^0 \\times 81$. \nOdd parts -> Distinct parts\nSuppose you are given a partition of n into odd parts. Count the number of times each odd number occurs: suppose $1$ occurs $a_1$ times, similarly $3$ occurs $a_3$ times, etc. So\n$n = a_11 + a_33 + a_55 + \\cdots$.\nNow write each $a_i$ \"in binary\", i.e., as a sum of distinct powers of two. So you have\n$n =  (2^{b_{11}}+2^{b_{12}}+\\cdots)1 + (2^{b_{31}}+2^{b_{32}}+\\cdots)3 + \\cdots$.\nNow just get rid of the brackets, and note that all terms are distinct. (Why?)\nE.g. for $20 = 5+3+3+3+1+1+1+1+1+1$, which is a partition of $20$ into odd parts, we do\n$20 = (1)5 + (3)3 + (6)1$\n$20 = (1)5 + (2+1)3 + (4+2)1$, so you get the partition\n$20 = 5 + 6 + 3 + 4 + 2$ in which all parts are distinct. \nDistinct parts -> Odd parts\nGiven a partition into distinct parts, we can write each part as a power of 2 multiplied by an odd number, and collect the coefficients of each odd number, and write the odd number those many times, to get a partition into odd parts.\nSo for example with $20 = 5+6+3+4+2$ which is a partition of $20$ into distinct parts, we write \n$20 = 5 + (2)3 + 3 + (4)1 + (2)1$, and then collect coefficients of the odd numbers $5$, $3$ and $1$:\n$20 = 5 + (2+1)3 + (4+2)1$\n$20 = 5+3+3+3+1+1+1+1+1+1$, which was our original odd partition.\n\nAside: you asked about the bijective proof, but I cannot resist mentioning that when Euler proved this theorem about partitions, it was with a proof that used generating functions. (When you understand both, maybe you can think about whether this proof is related to the bijective proof.)\nSketch: Let $D(n)$ denote the number of partitions into distinct parts, and $O(n)$ denote the number of partitions into odd parts. Then we have:\n$$\n\\begin{align}\n\\sum_{n\\ge0}D(n)x^n &= (1+x)(1+x^2)(1+x^3)(1+x^4)(1+x^5)\\cdots \\\\\n&= \\frac{1-x^2}{1-x}\\frac{1-x^4}{1-x^2}\\frac{1-x^6}{1-x^3}\\frac{1-x^8}{1-x^4}\\frac{1-x^{10}}{1-x^5}\\cdots \\\\\n &= \\frac{1}{(1-x)(1-x^3)(1-x^5)\\cdots} \\\\\n &= (1+x+x^{1+1}+\\cdots)(1+x^3+x^{3+3}+\\cdots)(1+x^5+x^{5+5}+\\cdots) \\\\\n &= \\sum_{n\\ge0}O(n)x^n\n\\end{align}\n$$\nwhich proves the theorem.", "meta": {"post_id": 54961, "input_score": 27, "output_score": 57, "post_title": "The number of partitions of $n$ into distinct parts equals the number of partitions of $n$ into odd parts"}}
{"input": "Is it true that every contractible open subset of $\\mathbb{R}^n$ is homeomorphic to $\\mathbb{R}^n$?", "output": "The answer to your general question is \"no\".\nA contractible open subset of $\\Bbb R^n$ need not be \"simply connected at\ninfinity\". ( \"$X$ is simply connected at infinity\" means that for each\ncompact $K$ there is a larger compact $L$ such that the induced map on\n$\\pi_1$ from $X - L$ to $X - K$ is trivial.)\nA contractible open subset of $\\Bbb R^n$ which is simply connected\nat infinity is homeomorphic to $\\Bbb R^n$\na) if $n > 4$: by J. Stallings, The piecewise linear structure of Euclidean space, Proc Camb Phil Soc 58(1962) (481-88)\nb) $n = 4$: by M. Freedman - see Topology of 4-Manifolds by Freedman and Quinn.\nc) For $n = 3$ this is a standard exercise - I don't know who gets the credit,\nbut you oould refer to  AMS memoir 411  by Brin and Thickstun.\nThe ingredients are\n\nthe Loop theorem and\nAlexander's theorem\n\nthat a PL sphere in $\\Bbb R^3$ bounds a 3-ball - you could even get around that by using the generalized Schoenfliess theorem of Morton Brown.", "meta": {"post_id": 55114, "input_score": 42, "output_score": 49, "post_title": "Are contractible open sets in $\\mathbb{R}^n$ homeomorphic to $\\mathbb R^n$?"}}
{"input": "In short, my question is:\n\nWas Grothendieck familiar with Stone's work on Boolean algebras?\n\nBackground:\nIn an answer to Pierre-Yves Gaillard's question Did Zariski really define the Zariski topology on the prime spectrum of a ring? I let myself get carried away and explained a result of Grothendieck that (for me) implies that Grothendieck certainly was familiar with Stone's work on spectra and even proved theorems with it. Qiaochu suggested that I ask a question and answer myself (apparently officially encouraged, see his remark), so I'm doing that in order to avoid an off-topic answer to Pierre-Yves's question. \nQiaochu's accepted answer quotes excerpts from Johnstone's Stone spaces that seem to imply that Grothendieck never quoted Stone. Precisely I'm having the following passage in mind:\n\nBut again, one will not find any reference to Stone in the work of Grothendieck, even though his use of the word 'spectrum' is an obvious echo of [Stone 1940], and Grothendieck, with his background in functional analysis, must have been familiar with Stone's work in that field.\n\nI did not seriously try to verify or falsify the first part of the sentence (and please do provide references if you happen to know of them). My own long answer addresses the second part of the sentence and tries to make a point that must have been should be replaced by was.\nNow fire away and complain about this being a nitpick, but I'm trying to explain a nice and interesting piece of mathematics and both Jonas Meyer and Qiaochu Yuan said I should post this answer, so: that's what I'm doing here.", "output": "[...] Grothendieck, with his background in functional analysis, must have been familiar with Stone's work in that field.\n\ncited by Qiaochu reveals a gap in the knowledge of Grothendieck's work (which is of course nothing to be blamed for, given its vastness and ramifications in so many fields) but it also misses a link to one of Grothendieck's beautiful theorems in functional analysis. I'd like to make the point that\n\nnot only was Grothendieck well aware of that work, he even exploited it!\n\nThe crucial reference is his article Une caract\u00e9risation vectorielle\u2013m\u00e9trique des espaces $L^{1}$. While Grothendieck does not cite Stone, he relies crucially on Nachbin's paper A theorem of the Hahn\u2013Banach type for linear transformations, improves on that paper and exploits the main results in a serious way. This is only possible if you understand these results profoundly. Nachbin in turn cites three works of Stone, see references 4,5,6 below.\nTo prevent a possible criticism of that argument, let me note that Nachbin and Grothendieck knew each other well. Grothendieck spent some time in Brazil and  even planned to write a book on topological vector spaces with Nachbin:\n\nGrothendieck planned to write a book on topological vector spaces with Leopoldo Nachbin, who was in Rio de Janeiro, but the book never materialized. However, Grothendieck taught a course in S\u00e3o Paulo on topological vector spaces and wrote up the notes, which were subsequently published by the university.\n\nsee Allyn Jackson's Notices article on Grothendieck, part\u00a01, p.1044. It could seem conceivable that the results were orally transmitted and that Grothendieck never looked at that paper. However, to such an objection I'd respond: you have to take into account that Grothendieck asserted the main results of that work earlier but he later discovered that his arguments had a serious gap. Given that he had to work rather hard to fix it, it seems quite implausible to me that he never took a closer look at Nachbin's work and missed the work of Stone cited in there entirely.\nSecondly Grothendieck refers to two works of Kakutani (references 8 and 9) below, which in turn refer to Stones works\u00a04 and\u00a05 and rely on it, too.\nIn conclusion I think it is safe to say that Grothendieck not only \"must have been\" but actually \"was\" aware of Stone's work (Edit: As Didier Piau pointed out in a comment this is still inconclusive, but please see the update at the end of this post).\nI'm now parting from the historical ramblings and make an attempt at a description of Grothendieck's result in a way that I hope is appealing to algebraists. For the actual work, please consult the MathSciNet review and the paper itself.\nFurther References: It don't know of many self-contained expositions of the results I mention below. The classification of \"injective\" Banach spaces (called $P_1$-spaces in the classical literature) can be found in Day's booklet Normed Linear Spaces as well as in H.\u00a0Elton Lacey's The isometric theory of classical Banach spaces. For a categorical view on Banach spaces please consult the Book Banach modules and functors on categories of Banach spaces by Cigler\u2013Losert\u2013Michor. I take the liberty and refer to Part\u00a02 (Chapter\u00a0IV) of my thesis for further facts, references and details.\n\nRecall that Grothendieck's thesis was called Produits tensoriels topologiques et espaces nucl\u00e9aires which is a masterpiece of its own and even mentioning its ramifications to the theory of distributions would lead us too far astray\u2014the first line of the MathSciNet review reads: Le grand nombre de r\u00e9sultats importants contenus dans ce m\u00e9moire (Th\u00e8se), rend difficile d'en mettre en \u00e9vidence les lignes essentielles, m\u00eame si l'on a recours au r\u00e9sum\u00e9 des r\u00e9sultats publi\u00e9 auparavant\u00a0[...]. Incidentally, one characteristic of that work is that quite often the statement of theorems span more than a page while the proofs only take a couple of lines or are dismissed with a mere \"Preuve: \u00e9vidente.\"\nThe simplest instance of a topological tensor product is due to Murray (and von Neumann)'s student Schatten and is called the projective tensor product $\\otimes$ of Banach spaces (usually denoted $\\hat{\\otimes}$ or $\\hat{\\otimes}_{\\pi}$). Given two Banach spaces, the space of bounded linear operators equipped with the operator norm yields an (internal) Hom-bifunctor and by design the projective tensor product is its left adjoint. From that point of view this is the most algebraically sane tensor product of Banach spaces to look at. (Yes, it is part of a monoidal closed structure etc etc)\nLet us call a Banach space flat if $F \\otimes {-}$ preserves short exact sequences of Banach spaces (here short exact means isometric inclusion of subspace + corresponding quotient map\u2014we're working in the category of Banach spaces and linear contractions).\nA Banach space $I$ will be called injective if every contractive map $E \\to I$ from a subspace $E \\leq F$ extends to a contractive map $F \\to I$.\nNote: The scalar field is of course flat (it is the tensor unit) and it is injective (that's equivalent to Hahn\u2013Banach).\nUsing this language we have:\n\nTheorem (Grothendieck). A Banach space $F$ is flat if and only if  $F$ is isomorphic to a space $L^1(\\Omega)$ where $\\Omega$ is some measure space.\n\nNow how does Grothendieck prove that? In his thesis he observed that $L^1(\\Omega)$ is flat. This is not very hard to prove, as from an analytic perspective the projective tensor product $L^1(\\Omega) \\otimes E$ really is the space of Bochner integrable functions $L^1(\\Omega, E)$ and from that identification (due to Grothendieck as well, as far as I know) the exactness of $L^1(\\Omega) \\otimes {-}$ is rather obvious .\nThe other direction needed much more insight. I won't elaborate on precisely what Grothendieck did, but here's what it comes down to morally, from the skewed perspective I adopted in this part of the answer so far:\n\n\"Lambek's theorem\": A Banach space is flat if and only if its dual space $F^{\\ast}$ is injective (rather trivial, see see also Lam Theorem\u00a04.9, p.125).\nA Banach space is injective if and only if it is isomorphic to a space of the form $C(K)$ with $K$ Stonean (compact Hausdorff and extremally disconnected).\nGrothendieck's theorem can now be phrased as: An injective Banach space is a dual space if and only if it is of the form $L^{\\infty}(\\Omega)$; its pre-dual  $L^1(\\Omega)$ is unique (contrary to general Banach spaces).\n\nThe theorem in 2. (due to various efforts by Akilov-Goodner-Kelley-Nachbin) is seriously deep. A feeling for this can be obtained by noting that the easiest example of an infinite Stonean space is $K = \\beta \\mathbb{N}$, the Stone\u2013\u010cech compactification of the natural numbers. Suffice it to say that the space $K$ arises from the geometry of the unit ball of an injective Banach space, which is strongly related to lattice theory and Boolean algebras, hence the link. Note also that Stonean spaces are called thus because they arose from Stones study of spectra of Boolean algebras.\nNote that by the Riesz-Kakutani representation theorem the space $C(K)^\\ast$ is the space of measures on $K$. Now given a flat Banach space $F$ it embeds into $F^{\\ast\\ast} = C(K)^{\\ast}$ and Grothendieck needed to recover it from the fact that he knew only that $C(K)$ is a dual space. He did that by identifying the space $F$ with the normal measures on $K$ and this does involve a deep understanding of the Stonean spaces.\n\nAdded: I forgot to mention one further punchline: Grothendieck also proves and makes use of the fact that a $C(K)$ space which is a dual space can be realized as a von Neumann algebra (in the spatial definition, of course). In remarque\u00a03 he states: \n\nOn peut se demander si le th\u00e9or\u00e8me\u00a02 se g\u00e9n\u00e9ralise \u00e0 toute $C^{\\ast}$-alg\u00e8bre (non n\u00e9cessairement ab\u00e9lienne comme dans notre \u00e9nonc\u00e9): Si une telle alg\u00e8bre $C$ est isomorphe (avec sa norme) au dual d'un sous-espace $L$ de $C$, est-il vrai que $C$ est isomorphe \u00e0 une alg\u00e8bre de von Neumann, et que $L$ est exactement l'ensemble des formes normales sur $C$? [...] La seule difficult\u00e9 est dans la question si $L$ est engendr\u00e9 par sa partie positive, le raisonnement donn\u00e9 dans le cas commutatif ne vaut pas tel quel. Il semble probable cependant que la technique des $C^{\\ast}$-alg\u00e8bres jointe au th\u00e9or\u00e8me de Banach-Dieudonn\u00e9 doive permettre de donner une r\u00e9ponse affirmative \u00e0 notre question.\n\nOf course, this is nothing but giving the outline of the proof of the celebrated Sakai theorem that characterizes von Neumann algebras as precisely those $C^{\\ast}$-algebras which are dual spaces as a Banach space. Those acquainted with Sakai's theorem will of course recognize that this is exactly how Sakai proceeded and indeed the \"seule difficult\u00e9\" in the proof... but I digressed enough. Let me stop by remarking that Sakai's paper was submitted while Grothendieck proofread his paper and I'll finish by quoting him:\n\nAjout\u00e9 pendant la correction des \u00e9preuves. Monsieur Lowdenslager m'a fait observer les faits suivants. La g\u00e9n\u00e9ralisation du th\u00e9or\u00e8me\u00a02 conjectur\u00e9e dans la remarque\u00a03 a \u00e9t\u00e9 prouv\u00e9e r\u00e9cemment par Sakai, dans un papier qui sera publi\u00e9 dans le Pacific Journal of Mathematics. Le th\u00e9or\u00e8me de Nachbin cit\u00e9 au d\u00e9but de ce travail a \u00e9t\u00e9 prouv\u00e9 ind\u00e9pendamment par D.\u00a0B.\u00a0Goodner; la plus jolie preuve connue semble \u00eatre celle de Kelley (Banach spaces with the extension property, Trans. Amer. Math. Soc, 72 (1952), 323-326), qui ne suppose pas que la boule unit\u00e9 admette un point extremal.\n\n\nReferences:\n\nMR0075539:\nGrothendieck, Alexandre, \nProduits tensoriels topologiques et espaces nucl\u00e9aires.\nMem. Amer. Math. Soc. (1955), no.\u00a016, 140\u00a0pp.\nMR0076301:\nGrothendieck, A.\nUne caract\u00e9risation vectorielle-m\u00e9trique des espaces $L^1$.\nCanad. J. Math.7\u00a0(1955),\u00a0552\u2013561. \nMR0032932: Leopoldo Nachbin, A theorem of the Hahn-Banach type for linear transformations, Trans. Amer. Math. Soc.\u00a068\u00a0(1950), 28-46. \nMR1501905: Stone, M. H.,\nApplications of the theory of Boolean rings to general topology. \nTrans. Amer. Math. Soc. 41 (1937), no. 3, 375\u2013481. \nMR2023: Stone, M. H., A general theory of spectra. I., \nProc. Nat. Acad. Sci. U. S. A. 26, (1940). 280\u2013283.\nMR4092: Stone, M. H., A general theory of spectra. II. \nProc. Nat. Acad. Sci. U. S. A. 27, (1941). 83\u201387. \nMR29091: M. H. Stone, Boundedness properties in function-lattices, Canadian Journal of Mathematics vol. 1 (1949) pp. 176-186.\nMR0004095: Kakutani, Shizuo. Concrete representation of abstract $(L)$-spaces and the mean ergodic theorem.\nAnn. of Math. (2) 42, (1941). 523--537.\nMR0005778: Kakutani, Shizuo. Concrete representation of abstract (M)-spaces. (A characterization of the space of continuous functions.)\nAnn. of Math. (2) 42, (1941). 994--1024.\nMR0084115: Sakai, Sh\u00f4ichir\u00f4,\nA characterization of $W^{\\ast}$-algebras. Pacific J. Math.\u00a06\u00a0(1956), 763\u2013773. \nMR0007568: Robert Schatten, On the direct product of Banach spaces, Trans. Amer. Math. Soc.\u00a053 (1943), 195-217.\nT. B\u00fchler, On the algebraic foundation of bounded cohomology, Mem. Amer. Math. Soc., posted on March\u00a011, 2011,\nPII S\u00a00065-9266(2011)00618-0  (to appear in print). Also available here.\nMR533819: Cigler, Johann; Losert, Viktor; Michor, Peter\nBanach modules and functors on categories of Banach spaces. \nLecture Notes in Pure and Applied Mathematics,\u00a046. Marcel Dekker, Inc., New York, 1979. xv+282 pp.\nMR0344849: Day, Mahlon M. Normed linear spaces. \nThird edition. Ergebnisse der Mathematik und ihrer Grenzgebiete, Band\u00a021. Springer-Verlag, New York-Heidelberg, 1973. viii+211\u00a0pp. \nMR0493279:\nLacey, H. Elton. The isometric theory of classical Banach spaces.\nDie Grundlehren der mathematischen Wissenschaften, Band\u00a0208. Springer-Verlag, New York-Heidelberg, 1974. x+270\u00a0pp.\nMR1653294: T.\u00a0Y.\u00a0Lam, Lectures on modules and rings, Graduate Texts in Mathematics, vol.\u00a0189, Springer-Verlag, New York,\u00a01999. \nMR58866: A. Grothendieck, Sur les applications lin\u00e9aires faiblement compactes d'espaces du type $C(K)$., Canadian J. Math.5,(1953). 129\u2013173. \n\n\nUpdate:\nIn the paper Sur les applications lin\u00e9aires faiblement compactes d'espaces du type $C(K)$ we find Stone's paper Applications of the theory of Boolean rings to general topology listed as reference\u00a014. It is mentioned only once as a mere reference for the Stone\u2013\u010cech compactification at the very end of the proof of Proposition\u00a06\u00a0(b): \n\nHere's the bibliography of that paper:\n\nI'd like to emphasize that my intention of this post was by no means to debunk anything Johnstone writes (and as a matter of fact: I didn't!): the excerpt provided by Qiaochu explicitly speaks of Grothendieck's work on Algebraic Geometry a bit before. I merely wanted to point out that there was an interesting link that may not be as widely known as it deserves to be.\nFor me the above settles the matter whether Grothendieck was aware of Stone's work. Whether he was familiar with it, particularly with the aspect of spectra, it is probably something he alone could answer, unless a more specific reference is provided. However, I hope I have succeeded in making the point that he was familiar enough with the contents that he could prove a beautiful theorem with it.", "meta": {"post_id": 55386, "input_score": 48, "output_score": 54, "post_title": "Was Grothendieck familiar with Stone's work on Boolean algebras?"}}
{"input": "As I understand it (If I'm imprecise, as I will likely be, please correct me), Langlands says roughly as follows:\nFor every representation $Gal(\\mathbb{Q}) \\rightarrow GL_n(\\mathbb{C})$ we can form a function, called the associated $L$-function.\nThere is such a thing called an automorphic form, which in the $n=2$ case is just a modular form (is this right? I have a feeling that there's a subtlety missing here, but I will continue, as this is not yet the gist of my question). For each automorphic form we may associate a function, confusingly also called the associated $L$-function (except now it's associated to an automorphic form rather than a Galois representation).\nLanglands says that we get the same set of functions from both of these constructions. (although, as I understand it, it doesn't really say much about what this bijection is)\nHowever, Taniyama-Shimura is phrased in terms of elliptic curves! I'm trying to relate it to something I would recognize as being my rough sketch for Langlands in $n=2$ case.\nIt is true if an elliptic curve is defined over $\\mathbb{Q}$ then one would get a representation of $Gal(\\mathbb{Q})$ by its action on the vector space $H^1(E,\\mathbb{Q}_l)$ (in the etale sense, for various prime $l$). But this is confusing to me: are all $2$-dimensional representations of this form? And what about this weird varying $l$ (shouldn't the representations ultimately be over $\\mathbb{C}$ as in my statement of Langlands, or was that wrong?)?\nAnd how does any of this relate to the phrasing of Taniyama-Shimura that says that any elliptic curve over $\\mathbb{Q}$ has a rational map into it from some $X_0(N)$ with integer coefficients?\nI hope you can help my confusion...\nEdit:\nLet me focus my question: in what sense does an elliptic curve give a 2-dimensional representation? (do you look at the first etale cohomology with coefficients in $\\mathbb{Q}_l$ and then basechange to $\\mathbb{C}$ and it turns out to be indep. of both $l$ and the embedding into $\\mathbb{C}$?)\nAnd secondly, do all two dimension (do I need the word \"irreducible\" here?) representations of the Galois group of the rationals arise from an elliptic curve in this way?", "output": "Here are some answers to your various questions:\nFor the group $GL_2$, every cuspidal automorphic representation of $GL_2(\\mathbb A)$ (here $\\mathbb A$ is the adele ring of $\\mathbb Q$) is generated by a uniquely determined newform, which (when normalized in a suitable fashion) is either a classical newform in the sense of Atkin and Lehner, i.e. a holomorphic cuspform of some weight $k \\geq 1$ which is an eigenform for all the Hecke operators, and of minimal possible level for its associated system of Hecke eigenvalues, or else is a Maass newform (same as before but replace holomorphic cuspform of weight $k \\geq 1$ by Maass form, which is an eigenform for the Laplacian with some eigenvalue $\\lambda$).  \nThere is a conjecture of Selberg that in the Maass form case, $\\lambda \\geq 1/4$.  This is still open (although there are known lower bounds that are close); it is an analogue \"at infinity\" of the Ramanujan--Petersson conjecture.  (More precisely, Langlands explained how to unify Selberg's conjecture with the Ramanujan--Petersson conjecture using the language of automorphic representations and the concept of \"tempered\" local factors. The local factor at infinity for a holomorphic newform is discrete series (or limit of discrete series in the weight one case), hence automatically tempered, but for Maass forms the local factor at infinity is unitary principal series, and then temperedness is a non-trivial additional condition.) \nIt has been proved (essentially by Eichler--Shimura when $k = 2$, by Deligne for the general case of $k \\geq 2$, and by Deligne and Serre when $k = 1$) that holomorphic newforms give rise to two-dimensional Galois representations, and it is conjectured (but not proved in general) that $\\lambda = 1/4$ Maass forms also give rise to two-dimensional Galois representations.  The Maass forms with $\\lambda \\neq 1/4$ (i.e. $\\lambda > 1/4$ if Selberg's conjecture is true) are not expected to correspond to Galois representations.\nWhat does \"give rise to two-dimensional Galois representations\" mean exactly? Let me explain.  (Note: from now on, to avoid circumlocutions, I will write affirmative statements, but in the Maass form $\\lambda = 1/4$ case, these remain conjectural in general; in the holomorphic case they are all proved theorems.)\nActually what will happen is that the newform will correspond to a two-dimensional motive.  This motive has $\\ell$-adic cohomology, for each $\\ell$, and these will form a compatible system of two-dimensional $\\ell$-adic representations of $G_{\\mathbb Q}$.  \nThis motive also has a Hodge structure, which has Hodge numbers $(0,0), (0,0)$ in the $\\lambda = 1/4$ case, and $(k-1,0), (0,k-1)$ in the holomorphic case.\nIn the $\\lambda = 1/4$ case, or in the holomorphic weight $1$ case, since the Hodge numbers are $(0,0), (0,0)$, the motive will come from a zero-dimensional variety, which is to say it will be a two-dimensional Artin motive, and so the\n$\\ell$-adic Galois representations can be consolidated into a single Artin\nrepresentation $G_{\\mathbb Q} \\to GL_2(\\mathbb C)$.  However, in the holomorphic case when $k \\geq 2$, the motive will come from a positive-dimensional variety, and so the $\\ell$-adic representations will not be able to be simplified into a single Artin representation (e.g. because they will have infinite image, not finite image).\nE.g. in the case $k = 2$, the Hodge numbers are $(1,0), (0,1)$, which says that the motive comes from a certain abelian variety.  The $\\ell$-adic representations are then the $\\ell$-adic Tate modules (or rather, their duals, if you want to be careful about the distinction betweeh cohomology and homology) of this abelian variety.\nI now should say something about coefficients.  The Hecke eigenvalues of the newform will be algebraic numbers, and collectively they will generate a finite extension $E$ of $\\mathbb Q$.  The motive attached to the newform will have $E$ acting as endomorphisms, and when I say it it two-dimensional, I mean it is two-dimensional over $E$.  Thus the $\\ell$-adic representations will actually be two\ndimensional $\\lambda$-adic representations for each prime $\\lambda$ of $E$, i.e. they will be representations $G_{\\mathbb Q} \\to GL_2(E_{\\lambda})$.\nE.g. if $k = 2$ and the newform $f$ has level $N$, then we can form the Jacobian\n$J_1(N)$ of $X_1(N)$, and this has an action of $\\mathbb T$, the Hecke algebra at level $N$.  The action of $\\mathbb T$ on $f$ gives rise to a homomorphism\n$\\mathbb T \\to E$ (sending $T_p$ to the $p$th Hecke eigenvalue of $f$), and\nwe may form $A_f:= E\\otimes_{\\mathbb T} J_1(N)$, which is an abelian variety well-defined up to isogeny.  It has dimension $[E:\\mathbb Q]$ and endomorphisms by $E$, and so for each $\\lambda$ we form its $\\lambda$-adic Tate module,\nand (the dual of) this is two-dimensional over $E_{\\lambda}$, and is the $\\lambda$-adic Galois representation attached to $f$.\nIn particular, if $k = 2$ and $E = \\mathbb Q$ (i.e. all the Hecke eigenvalues of $f$ are rational numbers) then $A_f$ is an elliptic curve, and this is the association of elliptic curves to weight two newforms with rational Hecke eigenvalues.  (Note that in this case $f$ automatically has level $\\Gamma_0(N)$ rather than $\\Gamma_1(N)$, and so in the above we replace $X_1(N)$ and $J_1(N)$ by $X_0(N)$ and $J_0(N)$, which may be a little more familiar.)\nNow suppose conversely that $M$ is a motive with endomorphisms by a number field $E$, irreducible and of dimension two over $E$.  It is conjectured that $M$ is attached to a newform.  In fact, we can determine the newform precisely (assuming it exists):\nthe ramification of $M$ will determine the level, and counting points on $M$ mod primes will determine the Hecke eigenvalues of the newform.   The Hodge numbers of $M$ (possibly after a Tate twist) will be $(i,0),(0,i)$ for some $i \\geq 0$.\nIf $i > 0$, then the newform will have to be a weight $i + 1$ holomorphic newform.  If $i = 0$, then the newform will be either holomorphic weight one,\nor a $\\lambda = 1/4$ Maass form; to tell which, you have to look at the action of complex conjugation on the Hodge strucuture: if it is non-scalar (this is usually called \"odd\"), you are in the holomorphic case, while if it is scalar\n(this is usually called \"even\") you are in the Maass case.  (The reason for \"odd\" and \"even\" is that in these case the determinant of complex conjugation on the $\\ell$-adic cohomology is $-1$ resp. $+1$.)\nLet's suppose that the motive $M$ comes from an elliptic curve $C$.  (I use $C$ rather than $E$ because $E$ was my notation for the field of coefficients.)\nThen the Hodge numbers are $(1,0),(0,1)$, hence we expect there to be weight two newform $f$ such that $C = A_f$.  (Since $A_f$ is only defined up to isogeny, this equality should be understood as an isogeny.)  But if $C$ is isogenous to a factor of the Jacobian of $X_0(N)$ (which is what we are saying), then this implies (indeed is equivalent to) $C$ being a quotient of $X_0(N)$.\nSometimes things are phrased in terms of $L$-functions rather than motives (because motives are subtle to think about, and parts of the theory of motives remain conjectural).  In the elliptic curve case, though, everything is known, because Faltings proved the Tate conjecture, namely that two elliptic curves with the same $L$-function are isogenous.  Since you can read off the $L$-function from the $\\ell$-adic Tate module, this says that even knowing that the $\\ell$-adic Tate module of $C$ coincides with the $\\ell$-adic Galois representation attached to $f$ (even for a single $\\ell$) is enough to get\n$C = A_f$.  (This is is why things are sometimes formulated in terms of $L$-functions, sometimes in terms of Galois reps., and sometimes in terms of motives --- the formulations are conjecturally all equivalent, and are known to be equivalent in the elliptic curve case.)\nThe preceding discussion shows that the general conjecture about going from two-dimensional motives to newforms is a generalization of Shimura--Taniyama.  It is known in the odd case (it follows from Serre's conjecture, proved by Khare, Wintenberger, and Kisin; see Cor. 0.5 of Kisin's paper).  It is open in general in the even case (just as the construction of motives, or Galois representations, from $\\lambda = 1/4$ Maass forms is wide open).  (But note that it is proved in the case of solvable image, by Langlands--Tunnel --- see below.)\n\nAs I already noted: one can avoid talking about motives, and instead talk about compatible families of $\\ell$-adic Galois reps. (more precisely, $\\lambda$-adic Galois reps., if we fix our coefficient field $E$), or even $\\ell$-adic (or $\\lambda$-adic) reps. for a single choice of $\\ell$ or $\\lambda$. \nThere are arrows\n$$\\text{motives } \\to \\text{ compatible families of $\\ell$-adic reps. } \\to \\text{ $\\ell$-adic rep. for a fixed $\\ell$} $$\ngiven by passing to $\\ell$-adic cohomology for all $\\ell$, and then by picking out a particular $\\ell$.  These maps are injective (the first conjecturally in general --- this is the Tate conjecture), but not surjective.  Not all compatible families come from a motive, and not every individual $\\ell$-adic rep. sits in a compatible family.  \nThere is a conjecture of Fontaine and Mazur which (conjecturally!) describes those $\\ell$-adic reps. which come from motives. (Here $\\ell$ is a fixed prime, and I could just as well be talking about $\\lambda$-adic reps. here, but it is more traditional to say $\\ell$-adic rather than $\\lambda$-adic, even when the latter is what is meant.) In the two-dimensional case, for representations which are supposed to come from holomorphic newforms of weight $k \\geq 2$, it is largely proved: see e.g. Thm. 1.2.4 (2), and also the discussion in Section 1.4, of this paper, as well as this paper of Kisin, and these papers\nof Calegari.  (These papers all build on the work of Wiles and Taylor--Wiles.)\nIn the case of representations which are supposed to come from either weight one holomorphic forms or Maass forms with $\\lambda = 1/4$, the situation is more complicated.  If you assume that the Galois rep. is odd and has finite image, then it is known to come from a weight one form; this follows from Serre's conjecture, as noted above.  However, it is supposed to be enough (according to Fontaine and Mazur) to assume that the $\\ell$-adic representation has finite image on inertia at $\\ell$ --- this should then imply that the whole image is finite.  However, this is not proved in general; I believe this result of Buzzard, which handles certain odd cases, is the best general result currently.\nIn the even case, if you assume that the image is finite and solvable, then Langlands--Tunnel give a $\\lambda = 1/4$ Maass form giving rise to your Galois rep., but (still in the even case now), if the image is finite but non-solvable, or (even worse) if you just assume that the image of inertia at $\\ell$ is finite, then the conjecture is wide-open.\n\nNote that $2$-dimensional Artin (i.e. finite image) reps. are just a special case of all two-dimensional $\\ell$-adic reps. (corresponding to $\\lambda = 1/4$ Maass form or weight one forms --- at least conjecturally, in the even case), and the $\\ell$-adic Tate modules of elliptic curves are just another special case of such reps.  Even if you write down all the $\\ell$-adic reps. coming from all newforms, there are uncountably manner other irreducible two-dimensional $\\ell$-adic reps. that don't come from motives at all.", "meta": {"post_id": 55449, "input_score": 41, "output_score": 50, "post_title": "In what sense is Taniyama-Shimura the $n=2$ case of Langlands?"}}
{"input": "Let $V$ be a vector space of finite dimension and let $T,S$ linear diagonalizable  transformations from $V$ to itself. I need to prove that if $TS=ST$ every eigenspace $V_\\lambda$ of $S$ is $T$-invariant and the restriction of $T$ to $V_\\lambda$ ($T:{V_{\\lambda }}\\rightarrow V_{\\lambda }$) is diagonalizable. In addition, I need to show that there's a base $B$ of $V$ such that $[S]_{B}^{B}$, $[T]_{B}^{B}$ are diagonalizable if and only if $TS=ST$.\nOk, so first let $v\\in V_\\lambda$. From $TS=ST$ we get that $\\lambda T(v)= S(T(v))$ so $T(v)$ is eigenvector of $S$ and we get what we want. I want to use that in order to get the following claim, I just don't know how. One direction of the \"iff\" is obvious, the other one is more tricky to me.", "output": "This answer is basically the same as Paul Garrett's. --- First I'll state the question as follows. \nLet $V$ be a finite dimensional vector space over a field $K$, and let $S$ and $T$ be diagonalizable endomorphisms of $V$. We say that $S$ and $T$ are simultaneously diagonalizable if (and only if) there is a basis of $V$ which diagonalizes both. The theorem is \n\n$S$ and $T$ are simultaneously diagonalizable if and only if they commute. \n\nIf $S$ and $T$ are simultaneously diagonalizable, they clearly commute. For the converse, I'll just refer to Theorem 5.1 of The minimal polynomial and some applications\nby Keith Conrad. \nEDIT. The key statement to prove the above theorem is Theorem 4.11 of Keith Conrad's text, which says: \n\nLet $A: V \\to V$ be a linear operator. Then $A$ is diagonalizable if and only if its minimal polynomial in $F[T]$ splits in $F[T]$ and has distinct roots. \n\n[$F$ is the ground field, $T$ is an indeterminate, and $V$ is finite dimensional.] \nThe key point to prove Theorem 4.11 is to check the equality \n$$V=E_{\\lambda_1}+\u00b7\u00b7\u00b7+E_{\\lambda_r},$$ \nwhere the $\\lambda_i$ are the distinct eigenvalues and the $E_{\\lambda_i}$ are the corresponding eigenspaces. One can prove this by using Lagrange's interpolation formula: put \n$$f:=\\sum_{i=1}^r\\ \\prod_{j\\not=i}\\ \\frac{T-\\lambda_j}{\\lambda_i-\\lambda_j}\\ \\in F[T]$$ and observe that $f(A)$ is the identity of $V$.", "meta": {"post_id": 56307, "input_score": 52, "output_score": 59, "post_title": "Simultaneous diagonalization of commuting linear transformations"}}
{"input": "Let $\\mathcal{A}$ be an abelian category. \nWe say that $\\mathcal{A}$ satisfies (AB5) if $\\mathcal{A}$ is cocomplete and filtered colimits are exact.\nIn Weibel's Introduction to homological algebra, he states (without proof) that $\\mathcal{A}$ satisfies axiom (AB5) iff $\\mathcal{A}$ is cocomplete and for all lattices $\\{ A_i \\}$ of subobjects of $A \\in \\mathcal{A}$ and all subobjects $B$ of $A$, we have $$ \\sum (A_i \\cap B) = B \\cap \\sum A_i.$$\nI have been thinking about this for a few days but have been unable to come up with a proof. In the forward direction I can't seem to relate the sum of subobjects and filtered colimits. I have no idea about the backward direction. Could anyone give me a hint?\nNote: This is not actually an exercise in Weibel's book, he states it in the appendix on category theory when he is defining axiom (AB5). It is stated without proof in Grothendieck's T\u014dhoku paper. Also, It is an exercise in Freyd's abelian categories.", "output": "I borrowed the ideas from the following books.\nAbelian categories with application to rings and modules by Popescu, 1973.\nTheory of categories by Mitchell, 1964.\nNotations and Conventions\nWe fix a Grothendieck universe $\\mathcal{U}$.\nWe consider only categories which belong to $\\mathcal{U}$.\nLet $\\mathcal{C}$ be a category.\nWe denote by Ob($\\mathcal{C}$) the set of objects of $\\mathcal{C}$.\nOften, by abuse of notation, we use $\\mathcal{C}$ instead of Ob($\\mathcal{C}$). \nWe denote by Mor($\\mathcal{C}$) the set of morphisms of $\\mathcal{C}$.\nLet $f:X \\rightarrow Y$ be a morphism of $\\mathcal{C}$.\nWe denote by dom($f$) the domain of $f$, i.e. $X$ = dom($f$).\nWe denote by codom($f$) the codomain of $f$, i.e. $Y$ = codom($f$).\nDefinition\nLet $\\mathcal{C}$ be a category.\nLet $X$ be an object of $\\mathcal{C}$.\nLet $I$ be a small set.\nLet $(X_i)_I$ be a family of subobjects of $X$.\nIf $(X_i)_I$ satisfies the following condition, $(X_i)_I$ is called a directed family of subobjects of $X$.\nFor any $i, j \\in I$, there exists $k \\in I$ such that $X_i \\subset X_k$ and $X_j \\subset X_k$.\nLemma 1\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nLet $I$ be a small category.\nLet $F: I \\rightarrow \\mathcal{A}$ be a functor.\nLet $A$ = colim $F$.\nFor each $i \\in I$, let $f_i:F(i) \\rightarrow A$ be the canonical morphism.\nFor each $i \\in I$, let $A_i$ = Im($f_i$).\nSince $\\mathcal{A}$ is cocomplete, $\\sum A_i$ exists.\nThen $A = \\sum A_i$.\nProof:\nLet $B = \\sum A_i$.\nLet $m:B \\rightarrow A$ be the canonical monomorphism.\nSince $A_i$ = Im($f_i$) for each $i \\in I$,\nthere exists $g_i:F(i) \\rightarrow B$ such that $f_i = mg_i$.\nLet $u: i \\rightarrow j$ be a morphism of I.\nSince $f_i = f_jF(u)$, $mg_i = mg_jF(u)$.\nSince $m$ is a monomorphism, $g_i = g_jF(u)$.\nHence there exists $g:A \\rightarrow B$ such that $g_i = gf_i$ for each $i$.\nHence $mgf_i = mg_i = f_i$ for each $i$.\nHence $mg = 1_A$.\nHence $A \\subset B$.\nHence $A = B$.\nQED\nLemma 2\nLet $\\mathcal{C}$ be a cocomplete category.\nLet I be a small category.\nLet $\\mathcal{C}^I$ be the category of functors: $I \\rightarrow \\mathcal{C}$.\nThen colim$: \\mathcal{C}^I \\rightarrow \\mathcal{C}$ preserves colimits.\nProof:\nLet $\\Delta: \\mathcal{C} \u2192 \\mathcal{C}^I$ be the diagonal functor,\ni.e. for each $X \\in \\mathcal{C}$ and for each $i \\in I$, $\\Delta(X)(i) = X$.\nSince colim is a left adjoint functor of $\\Delta$, it preserves colimits(MacLane: Categories for the working mathematician, Chapter V, Section 5, Theorem 1, p.114).\nQED\nLemma 3\nLet $\\mathcal{A}$ be a cocomplete abelian category which satisfies (AB5).\nLet $X$ be an object of $\\mathcal{A}$.\nLet I be a small filtered category.\nLet Sub($X$) be the category of subobjects of $X$.\nLet $F: I \\rightarrow$ Sub($X$) be a functor.\nThen $\\sum F(i)$ = colim $F$.\nProof:\nFor each $i \\in I$, Let $u_i\uff1aF(i) \\rightarrow$ colim $F$ be the canonical morphism.\nFor each $i \\in I$, Let $m_i\uff1aF(i) \\rightarrow X$ be the canonical monomorphism.\nSince $(m_i)_I$ is a cocone, it induces a morphism $f$: colim $F \\rightarrow X$.\nBy (AB5), $f$ is mono.\nHence we can regard colim $F$ as a subobject of $X$.\nSince $fu_i = m_i$ for each $i$, $F(i) \\subset$ colim $F$.\nLet $Z$ be a subobject of $X$.\nLet $r: Z \\rightarrow X$ be the canonical monomorphism.\nSuppose $F(i) \\subset Z$ for each $i$.\nLet $k_i: F(i) \\rightarrow Z$ be the canonical monomorphism.\nSince $(k_i)_I$ is a cocone, it induces a morphism $g$: colim $F \\rightarrow Z$.\nFor each $i \\in I$, $rgu_i = rk_i = m_i$.\nHence $f = rg$.\nHence colim $F \\subset Z$.\nQED\nLemma 4\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nLet $f:X \\rightarrow Y$ be a morphism of $\\mathcal{A}$.\nLet $I$ be a small set.\nLet $(X_i)_I$ be a family of subobjects of $X$.\nThen $\\sum f(X_i) = f(\\sum X_i)$.\nProof:\nFor each $i \\in I$, $X_i \\subset \\sum X_i$.\nHence $f(X_i) \\subset f(\\sum X_i)$.\nLet $Z$ be a subobject of $X$.\nSuppose $f(X_i) \\subset Z$ for each $i$.\nThen $f^{-1}(f(X_i)) \\subset f^{-1}(Z)$.\nSince $X_i \\subset f^{-1}(f(X_i))$, $X_i \\subset f^{-1}(Z)$.\nHence $\\sum X_i \\subset f^{-1}(Z)$.\nHence $f(\\sum X_i) \\subset f(f^{-1}(Z)) \\subset Z$.\nQED\nLemma 4.5\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nLet $I$ be a small category.\nLet $F: I \\rightarrow \\mathcal{A}$ be a functor.\nLet $X$ = colim $F$.\nLet $(s_i: F(i) \\rightarrow Y)_I$ be a cocone.\nLet $f:X \\rightarrow Y$ be the morphism induced by the cocone.\nThen $f(X) = \\sum s_i(F(i))$.\nProof:\nFor each $i$, let $u_i: F(i) \\rightarrow X$ be the canonical morphism.\nFor each $i$, $fu_i = s_i$.\nHence $f(u_i(F(i)) = s_i(F(i)) \\subset f(X)$.\nLet $Z$ be a subobject of $Y$.\nSuppose $s_i(F(i)) \\subset Z$ for each $i$.\nFor each $i$, $s_i$ induces $t_i: F(i) \\rightarrow Z$.\nSince $(t_i: F(i) \\rightarrow Z)_I$ is a cocone,\nit induces $g:X \\rightarrow Z$.\nLet $m: Z \\rightarrow Y$ be the canonical monomorphism.\n$mgu_i = mt_i = s_i$ for each i.\nHence $f = mg$.\nHence $f(X) \\subset Z$.\nQED\nLemma 5\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nLet $X$ be an object of $\\mathcal{A}$.\nLet $I$ be a small set.\nLet $(X_i)_I$ be a family of subobjects of $X$.\nThen $\\bigoplus X/X_i$ = $X/(\\sum X_i)$.\nProof:\nFor each $i \\in I$, the following sequence is exact.\n$0 \\rightarrow X_i \\rightarrow X \\rightarrow X/X_i \\rightarrow 0$.\nBy Lemma 2, colim preserves cokernels.\nHence, colim $X_i \\rightarrow X \\rightarrow$ colim $X/X_i \\rightarrow 0$ is exact.\nBy Lemma 4.5, Im(colim $X_i \\rightarrow X$) = $\\sum X_i$.\nHence colim $X/X_i$ = $X/(\\sum X_i)$.\nQED\nLemma 5.4\nSuppose the following is a pullback diagaram in an abelian category.\n$$\\begin{matrix}\nA&\\stackrel{f}{\\rightarrow}&B\\\\\n\\downarrow&&\\downarrow\\\\\nC&\\stackrel{h}{\\rightarrow}&D\n\\end{matrix}\n$$\nSuppose the following sequence is exact.\n$0 \\rightarrow C \\stackrel{h}{\\rightarrow}D \\rightarrow E$\nThen $0 \\rightarrow A \\stackrel{f}{\\rightarrow}B \\rightarrow E$ is exact.\nProof: Left to the readers.\nLemma 5.5\nConsider the following commutative diagram with two horizontal exact sequences in an abelian category.\n$X \\rightarrow Y \\rightarrow Z \\rightarrow 0$\n$0 \\rightarrow X' \\rightarrow Y' \\rightarrow Z' \\rightarrow 0$\nSuppose the left square is a pullback.\nThen $Z \\rightarrow Z'$ is mono.\nProof:\nWe call s the above morphism $Z \\rightarrow Z'$.\nLet $r:T \\rightarrow Z$ be a morphism such that sr = 0.\nThere exists the following pullback diagaram.\n$$\\begin{matrix}\nP&\\stackrel{u}{\\rightarrow}&T\\\\\n\\downarrow&&\\downarrow{r}\\\\\nY&\\stackrel{}{\\rightarrow}&Z\n\\end{matrix}\n$$\nBy Lemma 5.4,\n$0 \\rightarrow X \\rightarrow Y \\rightarrow Z\u2019$ is exact.\nHence there exists $P \\rightarrow X$ such that $P \\rightarrow Y = P \\rightarrow  X \\rightarrow Y$.\nHence $ru$ = 0.\nOn the other hand, since a pullback of an epimorphism in an abelian category is epi(MacLane Proposition 2, p.199), $u$ is epi.\nHence $r$ = 0.\nQED\nLemma 6\nLet $\\mathcal{A}$ be an abelian category.\nLet $f:X \\rightarrow Y$ be a morphism of $\\mathcal{A}$.\nLet $Z \\subset Y$.\nThen $X/f^{-1}(Z)$ is canonically isomorphic to $f(X)/(f(X) \\cap Z)$.\nProof:\nConsider the following commutative diagram with two horizontal exact sequences.\n\nBy Lemma 5.5, $X/f^{-1}(Z) \\rightarrow f(X)/(f(X) \\cap Z)$ is mono.\nSince $X \\rightarrow f(X)$ is epi,\n$X \\rightarrow X/f^{-1}(Z) \\rightarrow f(X)/(f(X) \u2229 Z)$ is epi.\nHence $X/f^{-1}(Z) \\rightarrow f(X)/(f(X) \\cap Z)$ is epi.\nHence $X/f^{-1}(Z) \\rightarrow f(X)/(f(X) \\cap Z)$ is an isomorphism.\nQED\nNote\nIf you are willing to accept Mitchell's embedding theorem, Lemma 6 will be trivial.\nLemma 7\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nSuppose $\\mathcal{A}$ has the following property.\n\nLet $A$ be an object of $\\mathcal{A}$.\n  Let $(A_i)_I$ be a directed family of subobjects of $A$.\n  Then, for every subobject $B$ of $A$, $(\\sum A_i) \\cap B = \\sum (A_i \\cap B)$.\n\nLet $f:Y \\rightarrow X$ be a morphism of $\\mathcal{A}$.\nLet $(X_i)_I$ be a directed family of subobjects of $X$.\nThen,\n$f^{-1}(\\sum X_i) = \\sum f^{-1}(X_i)$.\nProof:\nBy Lemma 6, for each $i$, $Y/f^{-1}(X_i)$ is canonically isomorphic to $f(Y)/(f(Y) \\cap X_i)$.\nHence $\\bigoplus Y/f^{-1}(X_i)$ is canonically isomorphic to $\\bigoplus f(Y)/(f(Y) \\cap  X_i)$.\nBy Lemma 5, $Y/\\sum f^{-1}(X_i)$ = $\\bigoplus Y/f^{-1}(X_i)$.\nHence $Y/\\sum f^{-1}(X_i)$ = $\\bigoplus f(Y)/(f(Y) \\cap X_i)$.\nBy Lemma 5, $\\bigoplus f(Y)/(f(Y) \\cap X_i)$ = $f(Y)/\\sum (f(Y) \\cap X_i)$.\nBy the assumption, $f(Y)/\\sum (f(Y) \\cap  X_i)$ = $f(Y)/((\\sum X_i) \\cap f(Y))$.\nBy Lemma 6, $Y/f^{-1}(\\sum X_i)$ is canonically isomorphic to $f(Y)/((\\sum X_i) \\cap  f(Y))$.\nHence $Y/\\sum f^{-1}(X_i)$ is canonically isomorphic to $Y/f^{-1}(\\sum X_i)$.\nHence $f^{-1}(\\sum X_i)$ = $\\sum f^{-1}(X_i)$.\nQED\nLemma 7.3\nLet $\\mathcal{C}$ be a category.\nLet $X$ be an object of $\\mathcal{C}$.\nLet Sub($X$) be the category of subobjects of $X$.\nLet $I$ be a small set.\nLet $(X_i)_I$ be a directed family of subobjects of $X$.\nThen there exists a preorder on $I$ making $I$ a filtered category and a functor $F: I \\rightarrow$ Sub($X$) such that\n$F(i) = X_i$ for each $i \\in I$.\nProof: Define $i \\leq j$ if and only if $X_i \\subset X_j$.\nQED \nLemma 7.5\nLet $\\mathcal{A}$ be an abelian category.\nLet $I$ be a small category.\nLet $F: I \\rightarrow \\mathcal{A}$ be a functor.\nLet $i \\in I$.\nLet $(i\\downarrow I)$ be the coslice category under i.\nLet Sub($F(i)$) be the category of subobjects of $F(i)$.\nThen there exists a functor $G$: $(i\\downarrow I) \\rightarrow$ Sub($F(i)$)\nsuch that $G(u)$ = Ker($F(u)$) for each $u \\in (i\\downarrow I)$.\nProof:Clear.\nLemma 8\nLet $\\mathcal{A}$ be an abelian category.\nLet $I$ be a small filtered category.\nLet $F: I \\rightarrow \\mathcal{A}$ be a functor.\nLet $i \\in I$.\nLet $J$ = {$u \\in$ Mor($I$); $i$ = dom($u$)}.\nThen (Ker($F(u))$)$_J$ is a directed family of subobjects of $F(i)$.\nProof:\nLet $(i\\downarrow I)$ be the coslice category under i.\n$(i\\downarrow I)$ is clearly a filtered category.\nSince $J$ = Ob($(i\\downarrow I)$), the assertion follows immediately from Lemma 7.5.\nQED\nLemma 8.5\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nLet $I$ be a small category.\nLet $F: I \\rightarrow \\mathcal{A}$ be a functor.\nLet $S = \\bigoplus_i F(i)$, where $i$ runs over every object of $I$.\nLet $m_i: F(i) \\rightarrow S$ be the canonical monomorphism for each $i \\in I$.\nLet $M$ = $\\sum_u$ Im($m_i - m_jF(u)$), where $u$ runs over every morphism of $I$ and $i$ = dom($u$), $j$ = codom($u$).\nLet $\\pi:S \\rightarrow S/M$ be the canonical epimorphism.\nLet $f_i = \\pi m_i$ for each $i \\in I$.\nThen $S/M$ = colim $F$ with canonical morphisms $f_i: F(i) \\rightarrow S/M$ for each $i \\in I$.\nProof:Left to the readers.\nLemma 8.6\nLet $I$ be a filtered category.\nLet $V$ be a non-empty finite subset of Ob($I$).\nLet $T$ be a finite subset of Mor($I$) such that dom($u$) $\\in V$ and codom($u$) $\\in V$ whenever $u \\in T$.\nThen there exists $p \\in$ Ob($I$) and a morphism $f_i: i \\rightarrow p$ for each $i \\in V$ with the following property.\n\nFor each $u:i \\rightarrow j$ in $T$, $f_i = f_ju$.\n\nProof:\nThere exists $q \\in I$ such that there exists a morphism $g_i:i \\rightarrow q$ for each $i \\in V$.\nLet $u:i \\rightarrow j$ in $T$.\nThere exists $r_u \\in I$ and a morphism $h_u:q \\rightarrow r_u$ such that\n$h_ug_i = h_ug_ju$.\nThere exists $r \\in I$ such that there exists a morphism $r_u \\rightarrow r$ for each $u \\in T$.\nHence, for each $u:i \\rightarrow j$ in $T$ there exist a morphism $g_{u, i}: i \\rightarrow r$ and a morphism $h_{u, j}: j \\rightarrow r$ such that $g_{u, i} = h_{u, j}u$.\nFor each $i \\in V$, let $G_i$ be the set {$g_{u, i}: i$ = dom($u$), $u \\in T$},\nand let $H_i$ be the set {$h_{u, i}: i$ = codom($u$), $u \\in T$}.\nLet $S_i = G_i \\cup H_i$ for each $i \\in V$.\nBy the properties of a filtered category, we can assume that $S_i$ consists of one morphism $f_i$ with a common codomain $p$ for each $i \\in V$.\nIf $S_i$ is empty, we can assume that there exists a morphism $f_i:i \\rightarrow p$ which has no condition.\nQED\nLemma 9\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nSuppose $\\mathcal{A}$ has the following property.\n\nLet $A$ be an object of $\\mathcal{A}$.\n  Let $(A_i)_I$ be a directed family of subobjects of $A$.\n  Then, for every subobject $B$ of $A$, $(\\sum A_i) \\cap B = \\sum (A_i \\cap B)$.\n\nLet I be a small filtered category.\nLet $F: I \\rightarrow \\mathcal{A}$ be a functor.\nFor each $i$, let $f_i: F(i) \\rightarrow$ colim($F$) be the canonical morphism.\nThen, for each $i$, Ker($f_i$) = $\\sum$ Ker($F(u)$), where $u$ runs over every morphism such that $i$ = dom($u$).\nProof:\nWe use the notations of Lemma 8.5.\nLet $T$ be a subset of Mor($I$).\nLet $M_T$ = $\\sum_{u \\in T}$ Im($m_i - m_jF(u)$), where $i$ = dom($u$), $j$ = codom($u$).\nThen $M = \\sum_T M_T$, where T runs through all finite subsets of Mor($I$).\nHence, by Lemma 8.5 and Lemma 7, Ker($f_i$) = $m_i^{-1}(M)$ = $\\sum_T m_i^{-1}(M_T)$,\nwhere T runs through all finite subsets of Mor($I$).\nIt suffices to prove:\nFor each finite subset $T$ of Mor($I$), $m_i^{-1}(M_T) \\subset$ Ker($F(u)$) for some $u \\in$ Mor($I$) such that  $i$ = dom($u$).\nLet $V$ be the set of $k \\in I$ such that $k$ = $i$ or $k$ = dom($u$) or $k$ = codom($u$) for some $u \\in T$.\nSince $I$ is filtered, by Lemma 8.6, there exists $p \\in$ Ob($I$) and a morphism $v_k: k \\rightarrow p$ for each $k \\in V$ with the following property.\n\nFor each $u \\in$ Mor($I$) such that k = dom($u$) $\\in V$ and $j$ = codom($u$) $\\in V$, $v_k = v_ju$.\n\nWe define $f:S \\rightarrow F(p)$ as follows.\nLet $k$ be any object of $I$.\nIf $k \\in V$, $fm_k = F(v_k)$, otherwise $fm_k = 0$.\nFor each $u \\in T$, let $k$ = dom($u$), $j$ = codom($u$).\nThen $f(m_k - m_jF(u))$ = $F(v_k) - F(v_j)F(u)$ = $0$.\nHence, by Lemma 4, $f(M_T)$ = $0$.\nSince $m_i(m_i^{-1}(M_T)) \\subset M_T$, $0$ = $f(m_i(m_i^{-1}(M_T)))$ = $F(v_i)(m_i^{-1}(M_T))$.\nHence $m_i^{-1}(M_T) \\subset$ Ker($F(v_i)$) as required.\nQED\nProposition 1\nLet $\\mathcal{A}$ be a cocomplete abelian category.\nSuppose $\\mathcal{A}$ has the following property.\n\nLet $A$ be an object of $\\mathcal{A}$.\n  Let $(A_i)_I$ be a directed family of subobjects of $A$.\n  Then, for every subobject $B$ of $A$, $(\\sum A_i) \\cap B = \\sum (A_i \\cap B)$.\n\nThen $\\mathcal{A}$ satisfies (AB5).\nProof:\nLet $I$ be a small filtered category.\nBy Lemma 2, colim$: \\mathcal{A}^I \\rightarrow \\mathcal{A}$ preserves colimits.\nIn particular, it preserves cokernels.\nHence it is right exact.\nIt suffices to prove that it preserves monomorphisms.\nLet $f: F \\rightarrow G$ be a monomorphism of $\\mathcal{A}^I$.\nLet $K$ = Ker(colim($f$)).\nFor each i, let $u_i: F(i) \\rightarrow$ colim $F$ be the canonical morphism.\nLet $A_i$ = $u_i(F(i))$ for each i.\nSince $I$ is a filtered category, $(A_i)_I$ is a directed family of subobjects of colim $F$.\nBy Lemma 1, colim $F$ = $\\sum A_i$.\nBy the assumption, $K$ = $(\\sum A_i) \\cap K = \\sum (A_i \\cap K)$.\nSuppose $K \\neq 0$.\nThere exists $k \\in I$ such that $A_k \\cap K \\neq 0$.\nSince $A_k$ = Im($u_k$), $u_k^{-1}(A_k \\cap K) \\neq 0$.\nLet $M = u_k^{-1}(A_k \\cap K)$.\nThen $u_k(M) \\neq 0$.\nFor each i, let $v_i: G(i) \\rightarrow$ colim $G$ be the canonical morphism.\n$v_k(f_k(M))$ = (colim $f$)($u_k(M)$) = (colim $f$)($A_k \\cap K$) $\\subset$ (colim f)($K$) = $0$.\nHence $f_k(M) \\subset$ Ker($v_k$).\nBy Lemma 9, $f_k(M) \\subset \\sum$ Ker($G(t)$), where $t$ runs over every morphism such that $k$ = dom($t$).\nBy Lemma 8 and the assumption, $f_k(M)$ = $\\sum$ (Ker($G(t)$) $\\cap f_k(M)$).\nSince $f_k$ is mono,  $M$ = $f_k^{-1}(f_k(M))$.\nBy Lemma 7, $M$ = $f_k^{-1}(f_k(M))$ = $f_k^{-1}(\\sum$ (Ker($G(t)$) $\\cap f_k(M)))$ = $\\sum f_k^{-1}$(Ker($G(t)$ $\\cap f_k(M))$.\nFor each morphism $t: k \\rightarrow j$, Let $N_t = f_k^{-1}$(Ker($G(t)$) $\\cap f_k(M))$.\nThen $G(t)f_k(N_t) = 0$.\nSince $G(t)f_k = f_jF(t)$, $f_jF(t)(N_t) = G(t)f_k(N_t) = 0$.\nSince $f_j$ is mono, $F(t)(N_t) = 0$.\nHence, by Lemma 9, $u_k(N_t) = 0$.\nHence, by Lemma 4, $u_k(M) = u_k(\\sum N_t) = \\sum u_k(N_t) = 0$.\nThis is a contradiction.\nQED\nProposition 2\nLet $\\mathcal{A}$ be a cocomplete abelian category satisfying (AB5).\nLet $A$ be an object of $\\mathcal{A}$. \nLet $(A_i)_I$ be a directed family of subobjects of $A$.\nThen, for every subobject $B$ of $A$, $(\\sum A_i) \\cap B = \\sum (A_i \\cap B)$.\nProof:\nLet $C = \\sum A_i$.\nFor each i, we have the following exact sequence.\n$0 \\rightarrow A_i \\cap B \\rightarrow A_i \\rightarrow C/(C \\cap B)$\nBy (AB5), Lemma 7.3 and Lemma 3, we get the following exact sequence.\n$0 \\rightarrow \\sum (A_i \\cap B) \\rightarrow C \\rightarrow C/(C \\cap B)$\nHence $(\\sum A_i) \\cap B = \\sum (A_i \\cap B)$.\nQED", "meta": {"post_id": 56454, "input_score": 31, "output_score": 61, "post_title": "Abelian categories and axiom (AB5)"}}
{"input": "In fact, it is an exercise on Hartshorne, Ex 2.4 to the second chapter of it (p.79):\n\nLet $A$ be a ring and $(X,\\mathcal{O}_X)$ be a scheme. Given a morphism $f:X\\longrightarrow \\operatorname{Spec} A$, we have an associated map on sheaves $f^\\sharp :\\mathcal{O}_{\\operatorname{Spec} A}\\longrightarrow f_*\\mathcal{O}_X$. Taking global sections we obtain a homomorphism $A\\longrightarrow \\Gamma (X,\\mathcal{O}_X)$. Thus there is a natural map $\\alpha:\\operatorname{Hom}_{\\mathcal{Sch}}(X,\\operatorname{Spec} A)\\longrightarrow \\operatorname{Hom}_{\\mathcal{Rings}}(A,\\Gamma (X,\\mathcal{O}_X))$. How to show that $\\alpha$ is bijective?\n\nMy approach is to construct an inverse for $\\alpha$. Starting from a ring hom $\\phi :A\\longrightarrow \\Gamma (X,\\mathcal{O}_X)$, and an affine covering $X=\\bigcup_i \\operatorname{Spec} B_i$,restricting $\\Gamma (X,\\mathcal{O}_X)$ to $\\mathcal{O}_X (\\operatorname{Spec}B_i)$, we get maps $\\phi_i:A\\longrightarrow B_i$, thus inducing $(f_i,f_{i}^{\\sharp}):\\operatorname{Spec}B_i\\longrightarrow \\operatorname{Spec} A$. Then I want to glue these $\\operatorname{Spec}B_i$ together to get a morphism $(f,f^\\sharp):(X,\\mathcal{O}_X)\\longrightarrow (\\operatorname{Spec}A,\\mathcal{O}_{\\operatorname{Spec}A})$. I get stuck here: I don't know how to glue them together and verify that $(f,f^\\sharp)$ is independent of the covering $\\{ \\operatorname{Spec}B_i\\}_i$. Is my idea right? Will someone be kind enough to give me some hints on this problem? Thank you very much!", "output": "If you want to prove the statement in question using a gluing argument, then you need to prove the result in the special case that $X=\\mathrm{Spec}(B)$ is also affine. This will allow you to conclude that your maps on intersections of affine opens agree on overlaps (because $\\mathrm{Hom}(\\mathrm{Spec}(B),\\mathrm{Spec}(A))\\rightarrow\\mathrm{Hom}(A,B)$ is a bijection).\nThe result for $X=\\mathrm{Spec}(B)$ is proved in Hartshorne if I remember correctly. In any case, the idea is that a ring map $\\varphi:A\\rightarrow B$ induces a continuous map $\\alpha:\\mathrm{Spec}(B)\\rightarrow\\mathrm{Spec}(A)$ under which the inverse image of a standard open $D(f)\\subseteq\\mathrm{Spec}(A)$ is $D(\\varphi(f))$ ($f\\in A$). The homomorphism $\\varphi$ naturally induces, for each $f\\in A$, a ring map $A_f\\rightarrow B_{\\varphi(f)}$, i.e., a map $\\mathcal{O}_{\\mathrm{Spec}(A)}(D(f))\\rightarrow\\mathcal{O}_{\\mathrm{Spec}(B)}(D(\\varphi(f))$, compatible with restriction, and since the sets $D(f)$ give a base for the topology of $\\mathrm{Spec}(A)$, this is extends uniquely to a sheaf map $\\mathcal{O}_{\\mathrm{Spec}(A)}\\rightarrow\\alpha_*\\mathcal{O}_{\\mathrm{Spec}(B)}$. For $\\mathfrak{q}\\in\\mathrm{Spec}(B)$, $\\mathfrak{p}=\\alpha(\\mathfrak{q})=\\varphi^{-1}(\\mathfrak{q})$, the stalk map $A_\\mathfrak{p}\\rightarrow B_\\mathfrak{q}$ is also induced (via the universal property of localization) by $\\varphi$, and is easily seen to be local (by construction). This morphism $\\alpha$ also recovers $\\varphi$ on global sections.\nUniqueness follows ultimately because of the requirement that the stalk maps of a morphism of locally ringed spaces are local and because morphisms of sheaves are determined by the morphisms on stalks. The requirement that the stalk maps be local forces $\\alpha(\\mathfrak{q})=\\varphi^{-1}(\\mathfrak{q})$, and then, again using the universal property of localization, there is a unique map of stalks $A_{\\varphi^{-1}(\\mathfrak{q})}\\rightarrow B_\\mathfrak{q}$ compatible with $\\varphi$. In summary, if you want to recover $\\varphi$ on global sections and you want your stalk maps to be local, you only have one choice (everything is induced by the universal property of localization). \nI want to point out that this argument actually extends to prove the result you're after with $X$ replaced not just by an arbitrary scheme, but by an arbitrary locally ringed space. This means the result actually has nothing to do with gluing (since an arbitrary locally ringed space needn't be built from affine schemes). The crucial thing is (as mentioned above) that for a morphism $\\alpha:X\\rightarrow\\mathrm{Spec}(A)$ the stalk map $\\alpha_x^\\sharp:A_{f(x)}\\rightarrow\\mathcal{O}_{X,x}$ is local for any $x\\in X$. Note that here $f(x)$ is a prime ideal of $A$ and $A_{f(x)}$ is the localization at that prime (the stalk of the structure sheaf of $\\mathrm{Spec}(A)$ at $f(x)$). If $\\alpha^\\sharp:A\\rightarrow\\mathcal{O}_X(X)$ is the map on global sections of the morphism $\\alpha$, then its compatibility with the stalk map $\\alpha_x^\\sharp$ and the fact that $\\alpha_x^\\sharp$ is local actually implies that $f(x)$ is the inverse image of the maximal ideal $\\mathfrak{m}_x\\subseteq\\mathcal{O}_{X,x}$ under the ring map $A\\rightarrow\\mathcal{O}_X\\rightarrow\\mathcal{O}_{X,x}$ (the first arrow is $\\alpha^\\sharp$ and the second is taking the stalk at $x$). So the map on global sections determines the morphism $\\alpha$ on the underlying topological spaces. Once you know this, it also follows that the stalk maps are uniquely determined. \nThe point of all this is that the map you want to prove is a bijection is injective. To prove that it is surjective, you basically run the above argument backwards. Given a ring map $\\varphi:A\\rightarrow\\mathcal{O}_X(X)$, you can define $\\alpha:X\\rightarrow\\mathrm{Spec}(A)$ on topological spaces by taking $f(x)$ to the prime ideal that is the inverse image of $\\mathfrak{m}_x\\subseteq\\mathcal{O}_{X,x}$ under the map mentioned in the previous paragraph. You can prove then that for any $f\\in A$, $\\alpha^{-1}(D(f))$ is $X_{\\varphi(f)}$, defined as the set of all $x\\in X$ such that $\\varphi(f)_x$ is not in the maximal ideal $\\mathfrak{m}_x$ of $\\mathcal{O}_{X,x}$. This is an open set of $X$ (the analogue of a standard open in an affine scheme), so $\\alpha$ is continuous. The universal property of localization together with the fact that $\\varphi(f)\\vert_{X_{\\varphi(f)}}\\in\\mathcal{O}_X(X_{\\varphi(f)})$ is a unit (this follows from the definition of $X_{\\varphi(f)}$) shows that the ring map $A\\rightarrow\\mathcal{O}_X(X)\\rightarrow\\mathcal{O}_X(X_{\\varphi(f)})$ ($\\varphi$ followed by restriction from $X$ to $X_{\\varphi(f)})$ induces a unique map $A_f\\rightarrow\\mathcal{O}_X(X_f)$ compatible with restriction. This data is what you need for a map of sheaves $\\mathcal{O}_{\\mathrm{Spec}(A)}\\rightarrow\\alpha_*\\mathcal{O}_X$. This $\\alpha$ recovers $\\varphi$ on global sections and proves surjectivity.\nJust to explain, the reason I went through all this was to illustrate that the adjunction between the global sections functor and $\\mathrm{Spec}$ actually works on the entire category of locally ringed spaces (not just the full subcategory of schemes) and so really has nothing to do with gluing maps on affine patches. I personally (when learning algebraic geometry) found this fact really helped me and shaped the way I think of affine schemes among locally ringed spaces (or just schemes). For example, any locally ringed space $X$ admits a unique morphism to $\\mathrm{Spec}(\\mathbb{Z})$, and the method of proof I've described shows that it sends a point $x\\in X$ to the prime ideal generated by the characteristic of the residue field $ \\kappa(x)$.", "meta": {"post_id": 56854, "input_score": 15, "output_score": 37, "post_title": "on the adjointness of the global section functor and the Spec functor"}}
{"input": "If I choose two open sets $A$ and $B$ as depicted on Wikipedia here:\n\nthen I have an isomorphism between $H_n(A \\cap B)$ and $H_n(A) \\oplus H_n(B)$ because the two tubes in $A \\cap B$ are disjoint.\nOK, so far so good. Then I write down the Mayer-Vietoris sequence and try to compute $H_n(T^2)$ but it seems to me I don't have enough information to do so. Can you confirm this?\nI then computed the reduced MVS instead and used that to compute $H_n(T^2)$. What I would like to know is if this is the only way or if there is a way to do it with MVS directly, without the reduced MVS. \nMany thanks for your help, I appreciate it.", "output": "Maybe I can add some details to Dylan's answer.\nFirst of all, with the open cover you have chosen, you have\n$$\nA, B \\quad \\cong \\quad \\text{cylinder} \\quad \\simeq \\quad S^1 \n$$\nand\n$$\nA \\cap B \\quad \\cong \\quad \\text{disjoint union of two cylinders} \\quad \\simeq \\quad S^1 \\sqcup S^1  \\ .\n$$\nHence\n$$\nH_n(A) = H_n(B) = \n\\begin{cases}\n\\mathbb{Z} & \\text{if}\\ n=0,1 \\\\\n0          & \\text{otherwise}\n\\end{cases}\n$$\nand\n$$\nH_n(A\\cap B) = \n\\begin{cases}\n\\mathbb{Z}\\oplus \\mathbb{Z} & \\text{if}\\ n=0,1 \\\\\n0          & \\text{otherwise}\n\\end{cases}\n$$\nHence, in particular, for $n\\geq 2$, $H_n(A) = H_n(B) = H_n(A\\cap B) = 0$. Thus, from the MVS,\n$$\n\\dots \\longrightarrow H_n(A)\\oplus H_n(B) \\longrightarrow H_n(\\mathbb{T}^2) \\longrightarrow H_{n-1}(A \\cap B) \\longrightarrow \\dots\n$$\nwhich, for $n > 2$, is just\n$$\n\\dots \\longrightarrow 0  \\longrightarrow H_n(\\mathbb{T}^2) \\longrightarrow 0 \\longrightarrow \\dots\n$$\nfollows that \n$$\nH_n(\\mathbb{T}^2) = 0, \\quad \\text{for} \\quad n> 2 .\n$$\nSo, again we can focus on what happens for $n=0,1,2$. For $n=0$ there is no big deal, because, since $\\mathbb{T}^2$ is connected, \n$$\nH_0 (\\mathbb{T}^2) = \\mathbb{Z} \\ .\n$$\nFor $n= 2$, we look at this piece of the MVS:\n$$\n\\dots \\longrightarrow H_2(A)\\oplus H_2(B) \\longrightarrow H_2(\\mathbb{T}^2) \\stackrel{\\partial}{\\longrightarrow} H_1(A\\cap B) \\stackrel{(i_*, j_*)}{\\longrightarrow} H_1(A) \\oplus H_1(B) \\longrightarrow \\dots \\ ,\n$$\nof which we know all the groups, except the torus' one:\n$$\n\\dots \\longrightarrow 0 \\longrightarrow H_2(\\mathbb{T}^2) \\stackrel{\\partial}{\\longrightarrow} \\mathbb{Z}\\oplus\\mathbb{Z} \\stackrel{(i_*, j_*)}{\\longrightarrow} \\mathbb{Z}\\oplus\\mathbb{Z} \\longrightarrow \\dots\n$$\nNow, as Steve D. pointed you out, despite the two last groups being isomorphic, it doesn't mean that the morphism $(i_*, j_*)$ between them in the MVS, induced by the inclusions $i: A\\cap B \\longrightarrow A$ and $j: A\\cap B \\longrightarrow B$, is an isomorphism. And actually it is not. (If it was, then $H_2(\\mathbb{T}^2) $ would be zero.)\nAt this stage, you need to compute who this $(i_*, j_*)$ is. For this, you choose $1$-cicles generating the homologies of $A, B $ and $A\\cap B$ as follows: for each cylinder of the intersection $A\\cap B$, take an equatorial circumference. Name their homology classes $\\alpha$ and $\\beta$. So, actually, those $\\mathbb{Z}$ in the piece of MVS depicted above are the free abelian groups generated by $\\alpha$ and $\\beta$:\n$$\n(i_*, j_*) : \\mathbb{Z}\\langle \\alpha \\rangle \\oplus \\mathbb{Z}\\langle \\beta \\rangle \\longrightarrow \\mathbb{Z}\\langle \\alpha \\rangle \\oplus \\mathbb{Z}\\langle \\beta \\rangle\n$$\nAnd now we compute:\n$$\n(i_* , j_*) (\\alpha , 0) = (i_* , j_*) (0, \\beta )  = (\\alpha , \\beta)\n$$\nsince $\\alpha = \\beta$ in $H_1(A)$ and $H_1(B)$.\nHence, in terms of these basis, our morphism $(i_* , j_*)$ can be represented by the matrix\n$$\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix} :\n\\mathbb{Z} \\oplus \\mathbb{Z} \\longrightarrow \\mathbb{Z} \\oplus \\mathbb{Z}\n$$\nHence,\n$$\nH_2(\\mathbb{T}^2 ) = \\mathrm{im}\\ \\partial = \\mathrm{ker}\\\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix}\n= \\mathbb{Z}\\langle \\alpha - \\beta \\rangle = \\mathbb{Z}\n$$\nAs for $H_1(\\mathbb{T}^2)$, let's focus on the following piece of the MVS:\n$$\n\\dots \\longrightarrow H_1(A\\cap B) \\stackrel{(i_*, j_*)}{\\longrightarrow} H_1(A) \\oplus H_1(B) \\stackrel{k_* - l_*}{\\longrightarrow} H_1(\\mathbb{T}^2) \\stackrel{\\partial}{\\longrightarrow} H_0 (A\\cap B) \\stackrel{(i_* , j_*)}{\\longrightarrow} H_0(A) \\oplus H_0(B) \\longrightarrow \\dots \\ ,\n$$\nwhere $k_* - l_*$ is the morphism induced by the inclusions $k: A \\longrightarrow \\mathbb{T}^2$ and $l : B \\longrightarrow \\mathbb{T}^2$.\nAgain, we know all the groups except $H_1(\\mathbb{T}^2)$:\n$$\n\\dots \\longrightarrow \\mathbb{Z}\\oplus\\mathbb{Z} \\stackrel{(i_*, j_*)}{\\longrightarrow} \\mathbb{Z}\\oplus\\mathbb{Z} \\stackrel{k_* - l_*}{\\longrightarrow} H_1(\\mathbb{T}^2) \\stackrel{\\partial}{\\longrightarrow} \\mathbb{Z}\\oplus\\mathbb{Z}  \\stackrel{(i_* , j_*)}{\\longrightarrow}\\mathbb{Z}\\oplus\\mathbb{Z}   \\longrightarrow \\dots\n$$\nAnd again, we need some knowledge of the morphisms involved. I claim that, taking as generators for $H_0(A)$ and $ H_0(B)$ two points $p,q$, one in each component of $A\\cap B$, you have also generators for $H_0(A\\cap B)$ and, with these generators and similar computations that the ones we have already done, the second morphism $(i_*, j_*)$ can also be represented by the matrix\n$$\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix} :\n\\mathbb{Z} \\oplus \\mathbb{Z} \\longrightarrow \\mathbb{Z} \\oplus \\mathbb{Z}\n$$\nNow, you get a short exact sequence from that piece of the MVS in the standard way:\n$$ 0\\longrightarrow \\mathrm{ker}\\ \\partial \\longrightarrow H_1(\\mathrm{T}^2) \\longrightarrow\n\\mathrm{im}\\ \\partial \n\\longrightarrow 0\n$$\nBut, on one hand, \n$$\n \\mathrm{im}\\ \\partial = \\mathrm{ker}\\ (i_*, j_*) = \\mathrm{\\ker}\\\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1\n\\end{pmatrix} \n= \\mathbb{Z}\n$$\nOn the other hand,\n$$\n\\mathrm{ker}\\ \\partial = \\mathrm{im}\\ (k_*-l_*) =(\\mathbb{Z}\\oplus \\mathbb{Z}) / \\mathrm{ker}(k_*-l_* ) = (\\mathbb{Z}\\oplus \\mathbb{Z})/\\mathrm{im}(i_*, j_*) = \\mathbb{Z}\n$$\nHence, we have the following short exact sequence:\n$$\n0 \\longrightarrow \\mathbb{Z} \\longrightarrow H_1(\\mathbb{T}^2) \\longrightarrow \\mathbb{Z} \\longrightarrow 0 \\ .\n$$\nThus\n$$\nH_1(\\mathbb{T}^2) = \\mathbb{Z} \\oplus \\mathbb{Z} \\ .\n$$\nEDIT. Some more details. Inclusions \n$$\nA \\stackrel{i}{\\longleftarrow} A\\cap B \\stackrel{j}{\\longrightarrow} B\n$$\ninduce morphims\n$$\nH_1(A) \\stackrel{i_*}{\\longleftarrow} H_1(A\\cap B ) \\stackrel{j_*}{\\longrightarrow} H_1(B) \\ .\n$$\nHence a morphism\n$$\n(i_*, j_*) : H_1(A\\cap B) \\longrightarrow H_1(A) \\times H_1(B) = H_1(A) \\oplus H_1(B) \\ ,\n$$\nwhose value on $1$-cycles $\\sigma : \\Delta^1 \\longrightarrow A\\cap B$ is\n$$\n(i_*, j_*) [\\sigma] = (i_*[\\sigma], j_* [\\sigma]) = ([i\\circ \\sigma],[j\\circ \\sigma]) .\n$$\nHence, if $a: \\Delta^1 \\longrightarrow A\\cap B$ and $b: \\Delta^1 \\longrightarrow A\\cap B$ are $1$-cycles representatives of the homology classes $\\alpha$ and $\\beta$, respectively, you have\n$$\n(i_*,j_*) (\\alpha, 0) = (i_*,j_*) [a] = ([i\\circ a],[j\\circ a]) = (\\alpha, \\alpha) = (\\alpha , \\beta)\n$$\nand analogously for $\\beta$.\nEDIT2. More details. To break a long exact sequence\n$$\n\\dots \\stackrel{f_{i-2}}{\\longrightarrow} A_{i-1} \\stackrel{f_{i-1}}{\\longrightarrow} A_i \\stackrel{f_i}{\\longrightarrow} A_{i+1} \\stackrel{f_{i+1}}{\\longrightarrow} \\dots\n$$\nin \"the standard way\" means that, at any point, you can get short exact sequences like\n$$\n0 \\longrightarrow \\mathrm{ker}\\ f_i \\longrightarrow A_i \\longrightarrow \\mathrm{im}\\ f_i \\longrightarrow 0\n$$\nwhere the arrow on the left is the inclusion and that on the right just $f_i$.", "meta": {"post_id": 58311, "input_score": 39, "output_score": 56, "post_title": "The homology groups of $T^2$ by Mayer-Vietoris"}}
{"input": "I got stuck with a problem that pop up in my mind while learning limits. I am still a high school student. \nDefine $P(m)$ to be the statement: $\\quad \\lim\\limits_{n\\to\\infty}(\\underbrace{\\frac{1}{n}+\\frac{1}{n}+\\cdots+\\frac{1}{n}}_{m})=0$\nThe statement holds for $m = 1$: $\\quad \\lim\\limits_{n\\to\\infty}\\frac{1}{n}=0$.\nAssume that $P(k)$ holds for some $k$. So put $m = k$: $\\quad \\lim\\limits_{n\\to\\infty}(\\underbrace{\\frac{1}{n}+\\frac{1}{n}+\\cdots+\\frac{1}{n}}_{k})=0$.\nWe prove $P(k + 1)$: $\\quad \\lim\\limits_{n\\to\\infty}(\\underbrace{\\frac{1}{n}+\\frac{1}{n}+\\cdots+\\frac{1}{n}}_{k+1}) =\\lim\\limits_{n\\to\\infty}(\\underbrace{\\frac{1}{n}+\\frac{1}{n}+\\cdots+\\frac{1}{n}}_{k}+\\frac{1}{n})$\n$=\\lim\\limits_{n\\to\\infty}(\\underbrace{\\frac{1}{n}+\\frac{1}{n}+\\cdots+\\frac{1}{n}}_{k}) +\\lim\\limits_{n\\to\\infty}\\frac{1}{n}$\n$=0+0=0$.\nIt has now been proved by mathematical induction that statement holds for all natural m.\nIf we let $m=n$, then $\\lim\\limits_{n\\to\\infty}(\\underbrace{\\frac{1}{n}+\\frac{1}{n}+\\cdots+\\frac{1}{n}}_{n})=0 \\tag{*}$.\nHowever, $\\underbrace{\\frac{1}{n}+\\frac{1}{n}+\\cdots+\\frac{1}{n}}_{n}=1 \\implies \\lim\\limits_{n\\to\\infty}(\\underbrace{\\frac{1}{n}+\\frac{1}{n}+\\cdots+\\frac{1}{n}}_{n})=1 \\tag{$\\dagger$}$.\nThen $(*) \\, \\& \\, (\\dagger)$ yield $1=0$?\nCan anybody explain this? thanks.", "output": "The problem is that the statement you proved was for fixed $m$, and then you let it vary.\nWhat follows is one way of looking at this problem:\nRewrite things as: $$\\underbrace{\\frac{1}{n}+\\frac{1}{n}+\\cdots+\\frac{1}{n}}_{m}=\\frac{m}{n}$$\nThen what you proved by induction is that for any fixed $m$ $$\\lim_{n\\rightarrow \\infty}\\frac{m}{n}=\\left(\\lim_{n\\rightarrow \\infty}m\\right)\\cdot\\left(\\lim_{n\\rightarrow \\infty}\\frac{1}{n}\\right)=m\\cdot 0=0.$$  This is fine since when the limits exist we can split them up like above.  However in the second deduction you try to do the same thing\n$$\\lim_{n\\rightarrow \\infty}\\frac{n}{n}=\\left(\\lim_{n\\rightarrow \\infty}n\\right)\\cdot\\left(\\lim_{n\\rightarrow \\infty}\\frac{1}{n}\\right)=n\\cdot 0=0.$$ This doesn't make any sense now, because $n$ is no longer fixed, and the one limit does not exist.  (We only have the multiplicative property when both limits exist)\nHope that helps,", "meta": {"post_id": 59795, "input_score": 48, "output_score": 48, "post_title": "Proof of 1 = 0 by Mathematical Induction on Limits?"}}
{"input": "Let's say I have two simple vectors: $[0, 1]$ and $[1, 0]$.\nTheir Kronecker product would be $[0, 0, 1, 0]$.\nLet's say I have only the Kronecker product. How can I find the two initial vectors back?\nIf my two vectors are written as : $[a, b]$ and $[c, d]$, the (given) Kronecker product is:\n$$[ac, ad, bc, bd] = [k_0, k_1, k_2, k_3]$$\nSo I have a system of four non linear equations that I wish to solve:\n$$\\begin{align*}\r\nac &= k_0\\\\\r\nad&= k_1\\\\\r\nbc&= k_2\\\\\r\nbd &=k_3.\r\n\\end{align*}$$\nI am looking for a general way to solve this problem for any number of initial vectors in $\\mathbb{C}^2$ (leading my number of variables to $2n$ and my equations to $2^n$ if I have $n$ vectors).\nSo here are a few specific questions:\nWhat is the common name of this problem? \nIf a general solution is known, what is its complexity class?\nDoes the fact that I have more and more equations when $n$ goes up compared to the number of variables help?\n(Note: I really didn't know what to put as a tag.)", "output": "This problem (Reverse kronecker product) has a known solution called \"Nearest Kronecker Product\" and it is generalized to matrices as well.\nGiven $A\\in  \\mathbb R^{m\\times n} $ with $m = m_1m_2$ and $n = n_1n_2$, find $B\\in  \\mathbb R^{m_1\\times n_1}$ and $C\\in \\mathbb R^{m_2\\times n_2}$ so\n$\\phi(B,C)$ = min $|| A- B\\otimes C||_F$, where $F$ denotes Frobenius norm. \nThis is reformulated as:\n$\\phi(B,C)$ = min $|| R- vec(B)\\otimes vec(C)'||_F$\n$vec$ is the vectorization operator which stacks columns of a matrix on top of each other. A is rearranged into $R \\in  \\mathbb R^{m_1n_1\\times m_2n_2}$ such that the sum of squares in $|| A- B\\otimes C||_F$ is exactly the same as $|| R- vec(B)\\otimes vec(C)'||_F$.\nExample for arrangement where $m_1=3,n_1=m_2=n_2=2$: \n$$ \n\\phi(B,C)  =  \\left|\n\\left[\n\\begin{array}{cc|cc}\na_{11}& a_{12}  & a_{13} & a_{14} \\\\\na_{21}& a_{22} & a_{23} & a_{24} \\\\ \\hline\na_{31}& a_{32} & a_{33} & a_{34} \\\\\na_{41}& a_{42} & a_{43} & a_{44} \\\\  \\hline\na_{51}& a_{52} & a_{53} & a_{54} \\\\\na_{11}& a_{62} & a_{63} & a_{64} \n\\end{array}\n\\right]\n - \n\\begin{bmatrix}\nb_{11}& b_{12}  \\\\\nb_{21}& b_{22}  \\\\\nb_{31}& b_{32}  \n\\end{bmatrix}\n\\otimes\n\\begin{bmatrix}\nc_{11}& c_{12}  \\\\\nc_{21}& c_{22}  \n\\end{bmatrix} \\right|_F \\\\\n\\phi(B,C) =\n\\left|\n\\begin{bmatrix}\na_{11}& a_{21} & a_{12} & a_{22} \\\\ \\hline\na_{31}& a_{41} & a_{32} & a_{42} \\\\ \\hline\na_{51}& a_{61} & a_{52} & a_{62} \\\\ \\hline\na_{13}& a_{23} & a_{14} & a_{24} \\\\ \\hline \na_{33}& a_{43} & a_{34} & a_{44} \\\\ \\hline\na_{53}& a_{63} & a_{54} & a_{64} \n\\end{bmatrix}\n - \n\\begin{bmatrix}\nb_{11}  \\\\\nb_{21}  \\\\\nb_{31}  \\\\\nb_{12} \\\\\nb_{22} \\\\\nb_{32}\n\\end{bmatrix}\n\\begin{bmatrix}\nc_{11}&c_{21} & c_{12} & c_{22}\n\\end{bmatrix} \\right|_F\n$$\nNow the problem has turned into rank 1 approximation for a rectangular matrix. The solution is given by the singular value decomposition of $R  = USV^T$ in [1,2]. \n$$\nvec(B) = \\sqrt{\\sigma_1}u_1, \\quad vec(C) = \\sqrt{\\sigma_1}v_1\n$$   \nIf $R$ is a rank 1 matrix solution will be exact i.e. $A$ is full seperable.[3]\n[1] Golub G, Van Loan C. Matrix Computations, The John Hopkins University Pres. 1996\n[2] Van Loan C., Pitsianis N., Approximation with Kronecker Products, Cornell University, Ithaca, NY, 1992 \n[3] Genton MG. Separable approximations of space\u2013time covariance matrices. Environmetrics 2007; 18:681\u2013695.", "meta": {"post_id": 60399, "input_score": 22, "output_score": 41, "post_title": "Method to reverse a Kronecker product"}}
{"input": "In my answer to the recent question Nested Square Roots, @GEdgar correctly raised the issue that the proof is incomplete unless I show that the intermediate expressions do converge to a (finite) limit. One such quantity was the nested radical \n$$\n\\sqrt{1 + \\sqrt{1+\\sqrt{1 + \\sqrt{1 + \\cdots}}}} \\tag{1}\n$$\nTo assign a value $Y$ to such an expression, I proposed the following definition. Define the sequence $\\{ y_n \\}$ by: \n$$\ny_1 = \\sqrt{1}, y_{n+1} = \\sqrt{1+y_n}.\n$$\nThen we say that this expression evaluates to $Y$ if the sequence $y_n$ converges to $Y$. \nFor the expression (1), I could show that the $y_n$ converges to $\\phi = (\\sqrt{5}+1)/2$. (To give more details, I showed, by induction, that $y_n$ increases monotonically and is bounded by $\\phi$, so that it has a limit $Y < \\infty$. Furthermore, this limit must satisfy $Y = \\sqrt{1+Y}$.) Hence we could safely say (1) evaluates to $\\phi$, and all seems to be good.\nMy trouble. Let us now test my proposed idea with a more general expression of the form\n$$\\sqrt{a_1 + \\sqrt{a_2 + \\sqrt{a_3 + \\sqrt{a_4+\\cdots}}}} \\tag{2}$$\n (Note that the linked question involves one such expression, with $a_n = 5^{2^n}$.) How do we decide if this expression converges? Mimicking the above definition, we can write:\n$$\ny_1 = \\sqrt{a_1}, y_{n+1} = \\sqrt{a_{n+1}+y_n}.\n$$\nHowever, unrolling this definition, one get the sequence\n$$\n\\sqrt{a_1}, \\sqrt{a_{2}+ \\sqrt{a_1}}, \\sqrt{a_3 + \\sqrt{a_2 + \\sqrt{a_1}}}, \\sqrt{a_4+\\sqrt{a_3 + \\sqrt{a_2 + \\sqrt{a_1}}}}, \\ldots\n$$\nbut this seems little to do with the expression (2) that we started with. \nI could not come up with any satisfactory ways to resolve the issue. So, my question is:\n\nHow do I rigorously define when an expression of the form (2) converges, and also assign a value to it when it does converge?\n\nThanks.", "output": "Vijayaraghavan proved that a sufficient criterion for the convergence of the following sequence $\\ \\sqrt{a_1 + \\sqrt{a_2 +\\:\\cdots\\: +\\sqrt{a_n}}}\\ \\ $ is that $\\displaystyle\\ \\ {\\overline {\\lim_{n\\to\\infty}}}\\ \\frac{\\log\\:{a_n}}{2^n}\\: < \\:\\infty\\:.\\: $\nFor references see see this 1935 Monthly article, Herschfeld: On infinite radicals, and Raoa and Berghe: On Ramanujan's nested roots expansion 1, 2005 and see this prior answer.", "meta": {"post_id": 61048, "input_score": 40, "output_score": 37, "post_title": "Definition of convergence of a nested radical $\\sqrt{a_1 + \\sqrt{a_2 + \\sqrt{a_3 + \\sqrt{a_4+\\cdots}}}}$?"}}
{"input": "This is a question that has been bothering me for quite a while. Let me put between quotation marks the terms that are used informally.\n\"Quotient objects\" are always the same. Take groups, abelian groups, rings, topological vector spaces for example. Inside every object there are certain \"subobjects\" that we can divide by: normal subgroups, subgroups, ideals and subspaces with the subspace topology, respectively.\nI (think I) know from an universal algebra viewpoint (of which I know nothing) that in the case of \"algebraic\" objects, the \"subobjects\" we take the quotient by are determined by congruences: equivalence relations on the cartesian product that respect the operation(s).\nThe first problem is:\n1) How to define categorically the \"subobjects\" by which we take quotients?\nNow, the resulting \"quotient object\" $G/N$ always satisfies the same universal property: it comes with a morphism $\\pi$ such that $\\pi:G\\to G/N$ and $N\\subset ker \\,\\pi$, which is universal with respect to this: any other such morphism factors through $\\pi$.\nThe second (intimately linked) problem is:\n2) How to define categorically these \"quotient objects\" and express in fancy terms the universal property they satisfy?\nNow, in \"algebraic\" categories there are well-known isomorphism theorems. In the Wikipedia link, it is explained that they are all a special case of the universal algebra statement.\n3) How can we express them in categorical terms?\nAny insightful answer, even if partial, will be very welcome.", "output": "First of all, let me say that there already well-defined notions of subobjects and quotient objects in category theory. I'll state them here for reference:\nDefinition. A subobject of an object $A$ in a category $\\mathbf{C}$ is an equivalence class of monomorphisms with codomain $A$, where we identify two monomorphisms $m : B \\to A$, $m' : B' \\to A$ as equivalent just if there is an isomorphism $f : B \\to B'$ such that $m = m' \\circ f$.\nDefinition. A quotient object of an object $A$ in a category $\\mathbf{C}$ is a subobject of $A$ in the opposite category $\\mathbf{C}^{\\textrm{op}}$. Explicitly, it is an equivalence class of epimorphisms with domain $A$, where we identify two epimorphisms $e : A \\to B$ and $e' : A \\to B'$ as equivalent just if there is an isomorphism $f : B' \\to B$ such that $e = f \\circ e'$.\nBut we shall see by means of examples that these aren't necessarily what we want them to be.\nExample. In the category of topological spaces, subspaces are subobjects. But so are other things. Indeed, if $B$ has a finer topology than $A$, then $B$ is also a subobject of $A$.\nExample. In the category of monoids, $\\mathbb{Z}$ is a quotient of $\\mathbb{N}$, because the natural inclusion map $\\mathbb{N} \\hookrightarrow \\mathbb{Z}$ is epic.\nThe problem is that the notions of monomorphism and epimorphism are a bit too general to capture the properties we want. So we should begin by defining some \u2018stronger\u2019 notions of mono/epi.\nDefinition. A regular monomorphism is an monomorphism which is an equaliser of some pair of parallel morphisms. A regular epimorphism is a coequaliser of some pair of parallel morphisms.\nExample. The kernel of a group/ring/module/etc. homomorphism is a regular monomorphism: indeed, $\\ker f$ is the equaliser of $f$ and the zero morphism. \nExample. Conversely, in an abelian category, every regular monomorphism is a kernel: indeed, the equaliser of $f, g : A \\to B$ is precisely the same thing as $\\ker (f-g)$, by the above observation.\nExample. In the category of topological spaces, the inclusion of a subspace $B \\hookrightarrow A$ is a regular monomorphism: it is the equaliser of its characteristic map $A \\to 2$ (where $2$ is given the indiscrete topology) and a constant map. Conversely, every regular monomorphism is (isomorphic to) the inclusion of a subspace.\nThis suggests we're on the right track. Let's look now at the epimorphisms.\nExample. The coimage of a group/ring/module/etc. homomorphism is a regular epimorphism: indeed, $\\operatorname{coim} f$ is the coequaliser of $\\ker f$ and the zero morphism. In particular, the projection maps onto quotient groups/rings/modules/etc. are regular epimorphisms. The converse is true in abelian categories by an argument similar to the one before.\nExample. Let $A / R$ be the quotient of a topological space $A$ by an equivalence relation $R$. Then the projection map $\\pi : A \\to A / R$ is a regular epimorphism. Recall that $R$ is a subset of the cartesian product $A \\times A$; so let us take $A \\times A$ with the induced product topology and topologise $R$ with subspace topology. Then we have reified $R$ as an object in $\\textbf{Top}$. Let $p_1, p_2 : R \\to A$ be the projections. It is not hard to verify that $\\pi : A \\to A / R$ is the coequaliser of $p_1$ and $p_2$, so it is indeed a regular epimorphism.\nUnfortunately, it turns out that in the category of groups, monomorphisms and regular monomorphisms are the same thing. (For a proof, see here.) So perhaps regular monomorphisms aren't a perfect fit for what we're looking for. Nonetheless, in sufficiently nice categories, we do have the following analogue of the first isomorphism theorem:\nTheorem. In any regular category, (the domain of) the image of a morphism is isomorphic (the codomain of) its regular coimage (the coequaliser of its kernel pair).\nExamples. Any abelian category is regular, as is any semiabelian category and any topos. \nSo I think this is probably the right level of generality, even though it doesn't cover the category of groups or the category of topological spaces. (In fact, the conclusion of the above theorem is blatantly false in the category of topological spaces: intuitively, the regular coimage is topologised by the domain, while the image is topologised by the codomain; so, for example, the map from a discrete space to an indiscrete space will have non-isomorphic image and regular coimage.)", "meta": {"post_id": 61062, "input_score": 46, "output_score": 41, "post_title": "Quotient objects, their universal property and the isomorphism theorems"}}
{"input": "I've done some search in Internet and other sources about this question. Why the name ring to this particular object? Just curiosity.\nThanks.", "output": "The name \"ring\" is derived from Hilbert's term \"Zahlring\" (number ring), introduced in his Zahlbericht for certain rings of algebraic integers. As for why Hilbert chose the name \"ring\", I recall reading speculations that it may have to do with cyclical (ring-shaped) behavior of powers of algebraic integers. Namely, if $\\:\\alpha\\:$ is an algebraic integer of degree $\\rm\\:n\\:$  then $\\:\\alpha^n\\:$ is a $\\rm\\:\\mathbb Z$-linear combination of lower powers of $\\rm\\:\\alpha\\:,\\:$ thus so too are all higher powers of $\\rm\\:\\alpha\\:.\\:$ Hence all powers cycle back onto  $\\rm\\:1,\\:\\alpha,\\:,\\ldots,\\alpha^{n-1}\\:,\\:$ i.e. $\\rm\\:\\mathbb Z[\\alpha]\\:$ is a finitely generated $\\:\\mathbb Z$-module. Possibly also the motivation for the name had to do more specifically with rings of cyclotomic integers. However, as plausible as that may seem, I don't recall the existence of any historical documents that provide solid evidence in support of such speculations.\nBeware that one has to be very careful when reading such older literature. Some authors mistakenly read modern notions into terms which have no such denotation in their original usage. To provide some context I recommend reading Lemmermeyer and Schappacher's Introduction to the English Edition of Hilbert\u2019s Zahlbericht. Below is a pertinent excerpt.\n\nBelow is an excerpt from Leo Corry's Modern algebra and the rise of mathematical structures, p. 149.\n\n\n\nBelow are a couple typical examples of said speculative etymology of the term \"ring\" via the \"circling  back\" nature of integral dependence, from Harvey Cohn's Advanced Number Theory, p. 49.\n\n$\\quad$The designation of the letter $\\mathfrak D$ for the integral domain has some historical importance going back to Gauss's work on quadratic forms. Gauss $\\left(1800\\right)$ noted that for certain quadratic forms $Ax^2+Bxy+Cy^2$ the discriminant need not be square-free, although $A$, $B$, $C$ are relatively prime. For example, $x^2-45y^2$ has $D=4\\cdot45$. The $4$ was ignored for the reason that $4|D$ necessarily by virtue of Gauss's requirement that $B$ be even, but the factor of $3^2$ in $D$ caused Gauss to refer to the form as one of \"order $3$.\" Eventually, the forms corresponding to a value of $D$ were called an \"order\" (Ordnung). Dedekind retained this word for what is here called an \"integral domain.\"\n$\\quad$The term \"ring\" is a contraction of \"Zahlring\" introduced by Hilbert $\\left(1892\\right)$ to denote (in our present context) the ring generated by the rational integers and a quadratic integer $\\eta$ defined by $$\\eta^2+B\\eta+C=0.$$ It would seem that module $\\left[1,\\eta\\right]$ is called a Zahlring because $\\eta^2$ equals $-B\\eta-C$ \"circling directly back\" to an element of $\\left[1,\\eta\\right]$ . This word has been maintained today. Incidentally, every Zahlring is an integral domain and the converse is true for quadratic fields.\n\nand from Rotman's Advanced Modern Algebra, p. 81.", "meta": {"post_id": 61497, "input_score": 252, "output_score": 243, "post_title": "Why are rings called rings?"}}
{"input": "I would like to prove that the equation $ 3^x+4^x=5^x $ has only one real solution ($x=2$)\nI tried to study the function $ f(x)=5^x-4^x-3^x $ (in order to use the intermediate value theorem) but I am not able to find the sign of $ f'(x)= \\ln(5)\\times5^x-\\ln(4)\\times4^x-\\ln(3)\\times3^x $ and I can't see any other method to solve this exercise...", "output": "One direct method is to divide directly by $5^x$ and get $1=(3/5)^x+(4/5)^x$. From here it is clear that the RHS is strictly decreasing, and there is a unique solution. Almost all exponential equations can be treated this way, by transforming them to\n\none increasing function equal to one decreasing function\none increasing/decreasing function equal to a constant.", "meta": {"post_id": 61812, "input_score": 25, "output_score": 43, "post_title": "Proving that $ 2 $ is the only real solution of $ 3^x+4^x=5^x $"}}
{"input": "I am trying to figure out how to prove that all rational numbers are either terminating decimal or repeating decimal numerals, but I am having a great difficulty in doing so. Any help will be greatly appreciated.", "output": "Recall that in long division, one gets a remainder at each step:\n$$\n\\begin{array}\n& & & 0 & . & 2 & 2 & 7 & 2  \\\\\n\\hline\n22 & ) & 5&.&0&0&0&0&0 \\\\\n& & 4 & &  4 \\\\\n& & & & 6 & 0 & \\leftarrow \\\\\n& & & & 4 & 4 \\\\\n& & & & 1 & 6 & 0 \\\\\n& & & & 1 & 5 & 4 \\\\\n& & & & & & 6 & 0 &  \\leftarrow & \\text{repeating}\\\\\n\\end{array}\n$$\n6 is a remainder.  The next remainder is 16.  Then the next is 6.  This brings us back to where we were at an earlier step: Dividing 60 by 22.  We have to get the same answer we got the previous time.  Hence we have repetition of \"27\".  The answer is $0.2272727\\overline{27}\\ldots$, where \"27\" keeps repeating.\nThe question then is: Why must we always return to a remainder that we saw earlier?  The answer is that the only possible remainders are $0, 1, 2, 3, \\ldots, 21$ (if $22$ is what we're dividing by) and there are only finitely many.  If we get 0, the process terminates.  If we never get 0, we have only 21 possibilities, so we can go at most 21 steps without seeing one that we've seen before.  As soon as we get one that we've seen before, the repetition begins.\nA related question worth asking is how you know that every repeating decimal corresponds to a rational number.  E.g., if you're handed $0.2272727\\overline{27}\\ldots$ with \"27\" repeating forever, how do you figure out that it's exactly $5/22$?  There's a simple algorithm for that too.", "meta": {"post_id": 61937, "input_score": 37, "output_score": 36, "post_title": "How can I prove that all rational numbers are either terminating decimal or repeating decimal numerals?"}}
{"input": "Let $f(x)\\in F[x]$ be a polynomial of degree $n$. Let $K$ be a splitting field of $f(x)$ over $F$. Then [K:F] must divides $n!$.\n\nI only know that $[K:F] \\le n!$, but how can I show that $[K:F]$ divides $n!$?", "output": "Hint: Try induction on $n$. The base case is clear; in the inductive step, we will want to start with a degree $n+1$ polynomial $f$, and somehow reduce to the case of a degree $\\leq n$ polynomial. There are two cases: $f$ is irreducible, and $f$ is reducible.\nSuppose $f$ is reducible. Let $p$ be an irreducible factor of $f$, so that $1\\leq \\deg(p)\\leq n$, and let $L$ be the splitting field of $p$ over $F$. Then $K$ is the splitting field of $\\frac{f}{p}$ over $L$, and $\\deg(\\frac{f}{p})=\\deg(f)-\\deg(p)$. Note that $a!\\times b!$ always divides $(a+b)!$  (this is equivalent to the binomial coefficients being integers).\nSuppose $f$ is irreducible. Then letting $L=F[x]/(f)\\cong F(\\alpha)$ for some root $\\alpha$ of $f$, we have that $[L:F]=n+1$. Now consider $\\frac{f}{x-\\alpha}$ (which is of degree $n$) as a polynomial over $L$.", "meta": {"post_id": 62762, "input_score": 23, "output_score": 37, "post_title": "the degree of a splitting field of a polynomial"}}
{"input": "I've read the proof for why $\\int_0^\\infty P(X >x)dx=E[X]$ for nonnegative random variables (located here) and understand its mechanics, but I'm having trouble understanding the intuition behind this formula or why it should be the case at all. Does anyone have any insight on this? I bet I'm missing something obvious.", "output": "For the discrete case, and if $X$ is nonnegative, $E[X] = \\sum_{x=0}^\\infty x P(X = x)$.  That means we're adding up $P(X = 0)$ zero times, $P(X = 1)$ once, $P(X = 2)$ twice, etc.  This can be represented in array form, where we're adding column-by-column:\n$$\\begin{matrix} P(X=1) & P(X = 2) & P(X = 3) & P(X = 4) & P(X = 5) & \\cdots \\\\ & P(X = 2) & P(X = 3) & P(X = 4) & P(X = 5) & \\cdots \\\\ & & P(X = 3) & P(X = 4) & P(X = 5) & \\cdots \\\\ & & & P(X = 4) & P(X = 5) & \\cdots \\\\ & &  &  & P(X = 5) & \\cdots\\end{matrix}.$$\nWe could also add up these numbers row-by-row, though, and get the same result.  The first row has everything but $P(X = 0)$ and so sums to $P(X > 0)$.  The second row has everything but $P(X =0)$ and $P(X = 1)$ and so sums to $P(X > 1)$.  In general, the sum of row $x+1$ is $P(X > x)$, and so adding the numbers row-by-row gives us $\\sum_{x = 0}^{\\infty} P(X > x)$, which thus must also be equal to $\\sum_{x=0}^\\infty x P(X = x) = E[X].$\nThe continuous case is analogous.\nIn general, switching the order of summation (as in the proof the OP links to) can always be interpreted as adding row-by-row vs. column-by-column.", "meta": {"post_id": 64186, "input_score": 88, "output_score": 144, "post_title": "Intuition behind using complementary CDF to compute expectation for nonnegative random variables"}}
{"input": "This is a really natural question for which I know a stunning solution. So I admit I have a solution, however I would like to see if anybody will come up with something different. The question is\n\nWhat is the probability that two numbers randomly chosen are coprime?\n\nMore formally, calculate the limit as $n\\to\\infty$ of the probability that two randomly chosen numbers, both less than $n$ are coprime.", "output": "Here is a fairly easy approach.\nLet us start with a basic observation:\n$\\bullet$ Every integer has the probability \"1\" to be divisible by 1.\n$\\bullet$ A given integer is either even or odd hence has probability $\"1/2\"$ to be divisible by 2\n$\\bullet$ Similarly, an integer has a probability $\"1/3\"$ to be divisible by 3. Because any interger is either the form $3k, 3k+1$ or $3k+2$.\n\nConjecture More generally, one integer chosen amongst $\"p\"$ other integers has one chance to be divisible by $p$\n\n\nFrom this we infer that the probability that  an integer is divisible by $p$ is $\\frac{1}{p}$. This is having one chance over $p-$chances to be divisible by $p$.\nTherefore the probability that two different integers are both simultaneously divisible by a prime $p$ is $\\frac{1}{p^2}$\nThis means that the probability that two different integers are  not simultaneously divisible by a prime $p$ is $$1-\\frac{1}{p^2}$$\n\n\nConclusion: The probability that two different integers are  never simultaneously divisible by a prime (meaning that they are co-prime)\nis therefore given by\n$$ \\color{blue}{\\prod_{p, prime}\\left(1-\\frac{1}{p^2} \\right) =\n\\left(\\prod _{p, prime}\\frac {1}{1-p^{-2}}\\right)^{-1}=\\frac {1}{\\zeta (2)}=\\frac {6}{\\pi ^{2}} \\approx  0,607927102 \u2248 61 \\%}$$\n\n\n\n\nWhere we should recall that, from the Basel problem we have the following Euler identity\n$$\\frac{\\pi^2}{6}=\\sum_{n=1}^{\\infty} \\frac{1}{n^2} = \\zeta(2)=\\prod _{p, prime}\\frac {1}{1-p^{-2}}.$$\nBy a similar token, the probability that $m$ integers are co-prime is given by\n$$ \\color{red}{\\prod_{p, prime}\\left(1-\\frac{1}{p^m} \\right) =\n\\left(\\prod _{p, prime}\\frac {1}{1-p^{-m}}\\right)^{-1}=\\frac {1}{\\zeta (m)}}.$$\nHere $\\zeta$ is the Riemann zeta function. $$\\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s} $$", "meta": {"post_id": 64498, "input_score": 43, "output_score": 45, "post_title": "Probability that two random numbers are coprime is $\\frac{6}{\\pi^2}$"}}
{"input": "In the definition of smooth manifolds, complex manifolds, and similar constructions, one starts by defining a property on neighborhoods in the space, specifying how they relate on overlapping neighborhoods. An atlas is a set of such neighborhoods that covers the space. Some books (Lee, Warner) define the structure as the maximal atlas. Others define it as the equivalence class of compatible atlases.\nI was under the impression that the advantage of using the equivalence class definition instead of the maximal atlas definition was that the proof of the existence of such a maximal atlas requires Zorn's lemma, which some prefer not to use if not absolutely necessary.\nBut Lee and Warner's books both contain existence proofs for this maximal atlas; they start with any atlas, and then just take the set of all compatible charts. If that argument somehow relies on Zorn's lemma (or some other variant of choice), I can't see how. So what do you say? Is choice required, assumed for convenience but not required, or just not necessary at all?", "output": "Zorn's lemma is not required to prove the existence of a maximal atlas, though it is convenient. For one thing, we don't have to prove that compatibility of atlases is an equivalence relation. On the other hand, the obvious proof using Zorn's lemma requires some extra work to show that there is a unique maximal atlas containing any atlas. So let's do it without Zorn's lemma.\nDefinition. Two atlases on a manifold are compatible if their union is an atlas.\nLemma. Compatibility of atlases is an equivalence relation.\nProof. It is clear that compatibility is symmetric and reflexive, and it remains to be shown that compatibility is transitive. Let $\\mathcal{A}_1, \\mathcal{A}_2, \\mathcal{A}_3$ be three atlases on a $k$-manifold $M$, and suppose $\\mathcal{A}_1 \\cup \\mathcal{A}_2$ and $\\mathcal{A}_2 \\cup \\mathcal{A}_3$ are atlases. We wish to show $\\mathcal{A}_1 \\cup \\mathcal{A}_3$ is an atlas. So let $\\varphi_1 : U_1 \\to \\mathbb{R}^k$ be a chart in $\\mathcal{A}_1$, $\\varphi_3 : U_3 \\to \\mathbb{R}^k$ be a chart in $\\mathcal{A}_3$. $\\mathcal{A}_2$ is an atlas, so for each point $x$ in $U_1 \\cap U_3$ there is a chart $\\varphi_2 : U_2 \\to \\mathbb{R}^k$ in $\\mathcal{A}_2$ such that $x \\in U_2$; but $\\varphi_1$ and $\\varphi_2$ are compatible and $\\varphi_2$ and $\\varphi_3$ are compatible so we see that $\\varphi_1$ and $\\varphi_3$ are locally compatible at $x$. (Here, \u2018compatible\u2019 means that the transition map satisfies the relevant regularity condition. There may well be invocations of the axiom of choice hidden here, but I will assume that there are not.) Moreover, $x$ is arbitrary in $U_1 \\cap U_3$ so this shows $\\varphi_1$ and $\\varphi_3$ are compatible; and $\\varphi_1$ and $\\varphi_3$ are also arbitrary, so $\\mathcal{A}_1 \\cup \\mathcal{A}_3$ is an atlas.\nLemma. The class of atlases on a manifold is a set.\nProof. The class of atlases is a subclass of the set\n$$\\mathscr{P} \\left( \\bigcup_{U \\in \\mathscr{P}(M)} \\{ U \\to \\mathbb{R}^k \\} \\right)$$\nwhere $\\{ U \\to \\mathbb{R}^k \\}$ denotes the set of all functions $U \\to \\mathbb{R}^k$, so by the axiom of separation,  the class of atlases is a set.\nLemma. The union of arbitrarily many pairwise compatible atlases is an atlas.\nProof. Immediate.\nTheorem. Every atlas is contained in a unique maximal atlas.\nProof. From the above, it is clear that every atlas $\\mathcal{A}$ is contained in some equivalence class of atlases, and this equivalence class is a set of compatible atlases. Let $\\overline{\\mathcal{A}}$ be the union of all those atlases. Then $\\mathcal{A} \\subseteq \\overline{\\mathcal{A}}$, and $\\overline{\\mathcal{A}}$ is the unique maximal atlas containing $\\mathcal{A}$: for if $\\mathcal{A} \\subseteq \\mathcal{A}'$, then $\\mathcal{A}$ and $\\mathcal{A}'$ are compatible, so $\\mathcal{A}' \\subseteq \\overline{\\mathcal{A}}$ by construction.\n\nFor the sake of completeness I sketch a proof using Zorn's lemma.\nTheorem. Every atlas is contained in a maximal atlas.\nProof. The set of all atlases containing $\\mathcal{A}$ partially ordered by inclusion is a chain-complete poset: indeed, it is clear that if we have a chain $\\{ \\mathcal{A}_\\alpha \\}$, then $\\bigcup_\\alpha \\mathcal{A}_\\alpha$ is also an atlas. Thus, the hypotheses of Zorn's lemma are satisfied and there is some maximal atlas containing $\\mathcal{A}$.", "meta": {"post_id": 66554, "input_score": 49, "output_score": 47, "post_title": "Is Zorn's lemma required to prove the existence of a maximal atlas on a manifold?"}}
{"input": "I know that convexity does not imply differentiability, for example f(x)=|x| is convex but not differentiable. However, |x| is not strictly convex. So I wonder whether strict convexity imply differentiability.\nI did some search and found out the Wikipedia implicitly gives the negative answer: http://en.wikipedia.org/wiki/Convex_function#Strongly_convex_functions\nIt says that \"a strongly convex function is also strictly convex\" and \"a function doesn't have to be differentiable in order to be strongly convex\".\nCan anyone provide a concrete example? Thanks in advance.", "output": "$$f(x)=x^2+|x|$$ is strictly convex because of the $x^2$ term but not differentiable at $0$ because of the $|x|$ term", "meta": {"post_id": 67853, "input_score": 26, "output_score": 34, "post_title": "Does strict convexity imply differentiability?"}}
{"input": "I'm trying to tie some loose ends here. My lecturer didn't bother to go into details, so I have to work it out myself. I usually hate to be pedantic, but these questions have been bugging me for a while.\nFirst, what's the proper definition of the natural numbers? The one given to me is: \"the smallest set such that $\\emptyset\\in N$ and $x\\in N\\implies x\\cup\\lbrace x\\rbrace\\in N$\". This definition does not seem rigorous to me. What does \"smallest\" mean here? Does it mean that no proper subset have the same property? And how to prove the existence of such set (under ZFC)? Lastly, how to prove this set satisfies Peano's axioms, especially the last axiom (about induction)?\nNext, how to prove that the relation $<$ is a total order? The definition is $x<y\\Leftrightarrow x\\subset y$. I was thinking of changing the definition into \"the transitive closure of $\\lbrace(n,n')|n\\in N\\rbrace$\" (to avoid the set theory stuffs), but it's still difficult to prove that it's a total order.\nLastly, how to prove the well ordering principle? I suspect this would be easy after proving the previous statement though.", "output": "Proper definition of the natural numbers.\nHere's one taken (from memory) from Halmos's Naive Set Theory.\nDefinition. Given a set $x$, we define $x^+$, the successor of $x$, to be the set $x\\cup\\{x\\}$.\nIf $x$ is a set, $x^+$ is a set: $\\{x\\}$ is an element of the power set of $X$, so $x^+$ is a subset of $x\\cup\\mathcal{P}(x)$, hence a set (by the Axiom of Powers, Axiom of Unions, and Axiom of Separation).\nDefinition. A set $S$ is said to be inductive if and only if $\\emptyset\\in S$ and for every $x$, if $x\\in S$ then $x^+\\in S$.\nAxiom of Infinity. There exists at least one inductive set.\nNow, let $S$ be any inductive set. Let\n$$\\mathbb{N}_S = \\bigcap\\{A\\subseteq S\\mid A\\text{ is inductive}\\}.$$\nThis is a set, since the family is a subsets of $\\mathcal{P}(S)$, and so its intersection is a set.\nLemma. The intersection of any nonempty collection of inductive sets is inductive.\nProof. Suppose $\\{S_i\\}$ is a nonempty family of inductive sets, and let $S=\\cap S_i$  Then $\\emptyset\\in S_i$ for each $i$, hence $\\emptyset\\in S$; and if $x\\in S$, then $x\\in S_i$, for each $i$; hence (since each $S_i$ is inductive), $x^+\\in S_i$ for each $i$, and thus $x^+\\in S$. Thus, $S$ is inductive. $\\Box$\nCorollary. If $S$ is inductive, then $\\mathbb{N}_S$ is inductive. Moreover, if $B$ is any inductive subset of $S$, then $\\mathbb{N}_S\\subseteq B$. \nTheorem. If $S$ and $T$ are two inductive sets, then $\\mathbb{N}_S = \\mathbb{N}_T$.\nProof. Consider $\\mathbb{N}_T\\cap S$; this is an inductive subset of $S$, hence $\\mathbb{N}_S\\subseteq \\mathbb{N}_T\\cap S\\subseteq \\mathbb{N}_T$. Symmetrically, since $\\mathbb{N}_S\\cap T$ is inductive, then $\\mathbb{N}_T\\subseteq \\mathbb{N}_S\\cap T\\subseteq\\mathbb{N}_S$. Thus, $\\mathbb{N}_S=\\mathbb{N}_T$. $\\Box$\nDefinition. $\\mathbb{N}$ is the set $\\mathbb{N}_S$, where $S$ is any inductive set.\nThis set is well defined, since by the theorem above it does not matter what inductive set we start with, we always get the same set $\\mathbb{N}$; and there is at least one inductive set by the Axiom of Infinity.\nTheorem. If $S$ is any inductive set, then $\\mathbb{N}\\subseteq S$; that is, $\\mathbb{N}$ is the \"smallest\" inductive set, in the sense of set inclusion.\nProof. If $S$ is any inductive set, then $\\mathbb{N}=\\mathbb{N}_S\\subseteq S$. $\\Box$\nNow let $0=\\emptyset$, and let $s(x) = x^+$ for every $x$.\nTheorem. (The Peano Axioms) $\\mathbb{N}$ satisfies the following:\n\n$0\\in \\mathbb{N}$.\nIf $n\\in \\mathbb{N}$, then $s(n)\\in\\mathbb{N}$.\nFor all $n\\in\\mathbb{N}$, $s(n)\\neq 0$.\nIf $s(n)=s(m)$, then $n=m$.\nIf $S\\subseteq \\mathbb{N}$ is such that $0\\in S$ and if $n\\in S$ then $s(n)\\in S$, then $S=\\mathbb{N}$.\n\nProof. Since $\\mathbb{N}$ is inductive, 1 and 2 are satisfied. 3 is satisfied because by definition, $n\\in s(n)$, hence $s(n)\\neq\\emptyset=0$. 5 is satisfied because the given conditions imply that $S$ is inductive, and therefore $\\mathbb{N}\\subseteq S$. Since we already have $S\\subseteq \\mathbb{N}$, then $S=\\mathbb{N}$ holds.\nThe only difficulty is property 4. This requires some auxiliary results:\nLemma 1. If $n\\in\\mathbb{N}$, and $m\\in n$, then $m\\subseteq n$. \nProof. Let $S=\\{n\\in\\mathbb{N}\\mid \\text{for all }m\\in n, m\\subseteq n\\}$.\nThen $0\\in S$, since $0=\\emptyset$ satisfies the property by vacuity. Assume that $k\\in S$, and let $m\\in s(k) = k\\cup\\{k\\}$. If $m\\in k$, then since $k\\in S$ we have $m\\subseteq k\\subseteq s(k)$. If $m\\notin k$, then $m\\in\\{k\\}$, hence $m=k$, and $m=k\\subseteq k\\cup\\{k\\}=s(k)$. Thus, $s(k)\\in S$. \nWe conclude that $S$ is an inductive subset of $\\mathbb{N}$, and therefore that $S=\\mathbb{N}$. $\\Box$\nLemma 2. If $n\\in\\mathbb{N}$ and $m\\in n$, then $n\\not\\subseteq m$.\nProof.  Let $S=\\{n\\in\\mathbb{N}\\mid\\text{for all }m\\in n, n\\not\\subseteq m\\}$. Then $0\\in S$, since it satisfies the condition by vacuity.\nAssume that $k\\in S$. If $m\\in s(k)=k\\cup\\{k\\}$, then either $m\\in k$, in which case $k\\not\\subseteq m$, hence $s(k)\\not\\subseteq m$ (since $k\\subseteq s(k)$); or else $m=k$. But since $k\\in S$, then $k\\notin k$ (for $k\\subseteq k$, so $k\\in k$ would contradict that $k\\in S$). Since $k\\in s(k)$, then $s(k)\\not\\subseteq k=m$. Thus, if $k\\in S$ then $s(k)\\in S$. Hence $S$ is an inductive subset of $\\mathbb{N}$, so $S=\\mathbb{N}$. $\\Box$\nWe are now ready to prove property 4. Assume that $s(n)=s(m)$. Since $m\\in s(m)=s(n) = n\\cup \\{n\\}$, then either $m=n$, or $m\\in n$. If $m=n$, we are done. So assume that $m\\in n$. Since $n\\in s(n)=s(m)$, then $n\\in m\\cup \\{m\\}$; since $n\\neq m$, then $n\\in m$. Thus, $n\\in m$, hence $n\\subseteq m$; but $m\\in n$, contradicting Lemma 2. This contradiction arises from assuming that $m\\in n$. Therefore, $m\\notin n$, so $m=n$, hence $\\mathbb{N}$ satisfies the fourth Peano axiom. $\\Box$\nProof that $\\lt$ is a total order on $\\mathbb{N}$.\n(I'm more used to the definition $n\\lt m\\Longleftrightarrow n\\in m$, but I'll use your definition with proper inclusion)\nWe want to prove that if $n,m\\in\\mathbb{N}$, then either $n=m$, $n\\subset m$, or $m\\subset n$.\nLemma. Let $n\\in \\mathbb{N}$. Then either $n=0$, or there exists $k\\in\\mathbb{N}$ such that $n=s(k)$.\nProof. Let $S=\\{n\\in\\mathbb{N}\\mid n=0\\text{ or }n=s(k)\\text{ for some }k\\in\\mathbb{N}\\}$. \nThen $0\\in S$; if $k\\in S$, $s(k)\\in S$ (since $s(k)$ is a successor). Thus, $S$ is inductive, hence $S=\\mathbb{N}$. $\\Box$\nLemma. Let $k,n\\in\\mathbb{N}$. If $k\\subset n$, then $s(k)\\subseteq n$.\nProof. Let $S=\\{n\\in\\mathbb{N}\\mid \\text{if }k\\subset n\\text{ then }s(k)\\subseteq n\\}$. \nThen $0\\in S$ by vacuity. If $n\\in S$, let $k\\subset s(n)=n\\cup\\{n\\}$. \nIf $k\\subset n$, then $s(k)\\subseteq n\\subset s(n)$. If $k=n$, then $s(k)=s(n)$. The only other possibility is that $n\\in k$. But since $k$ is a natural number, then $n\\in k$ implies $n\\subseteq k$. But $n\\subseteq k\\subset n\\cup\\{n\\}$ is impossible. Hence, $k\\subset s(n)$ simplies $s(k)\\subseteq s(n)$. This proves $S$ is inductive, hence $S=\\mathbb{N}$. \nFinally, fix $m$, and let $S=\\{n\\in\\mathbb{N}\\mid m\\subseteq n\\text{ or }n\\subset m\\}$. \nThen $0\\in S$: if $m=\\emptyset$, then $m\\subseteq 0$; if $m\\neq\\emptyset$, then $\\emptyset\\subset m$. \nAssume that $k\\in S$. If $m\\subseteq k$, then $m\\subseteq s(k)$. If $k\\subset m$, then by the Lemma $s(k)\\subseteq m$. If $s(k)=m$, then $s(k)\\in S$. If $s(k)\\subset m$, then $s(k)\\in S$. Either way, $S$ is inductive, hence $S=\\mathbb{N}$. Therefore, $\\lt$ is a trichotomic relation on $S$.\nProof of the Well Ordering Principle\nLemma. If $n\\in\\mathbb{N}$, then there does not exist $k\\in\\mathbb{N}$ such that $n\\lt k\\lt s(n)$.\nProof. It is impossible to have $n\\subset k\\subset n\\cup\\{n\\}$. $\\Box$\nWell Ordering Principle. If $A\\subseteq\\mathbb{N}$ and $A\\neq\\emptyset$, then $A$ has a smallest element.\nProof. Let $A\\neq\\emptyset$, $A\\subseteq \\mathbb{N}$. Let $S=\\mathbb{N}-A$. \nLet $S'= \\{n\\in\\mathbb{N}\\mid n\\in S\\text{ and for all }k,\\text{ if }k\\lt n\\text{ then }k\\in S\\}$. Since $S'\\subseteq S\\neq\\mathbb{N}$, then $S'$ is not inductive. Therefore, either $0\\notin S'$, or else there exists $k\\in S'$ such that $s(k)\\notin S'$.\nIf $0\\notin S'$, then since for all $k\\lt 0$, $k\\in S$, it follows that $0\\notin S$. Therefore, $0\\in A$. Since $0$ is the smallest element of $\\mathbb{N}$, it is also the smallest element of $A$ and we are done.\nAssume then that there exist $k\\in \\mathbb{N}$ such that $k\\in S'$ but $s(k)\\notin S'$. Then $k\\in S$ and for all $n\\in\\mathbb{N}$, if $n\\lt k$ then $n\\in S$. That means that for all $n\\in\\mathbb{N}$, if $n\\lt s(k)$, then $n\\in S$ (for $n\\lt s(k)$ implies $n\\leq k$ by the Lemma, and so either $n=k\\in S$ or $n\\lt k$ hence $n\\in S$ since $k\\in S'$). That means that because $s(k)\\notin S'$, it must be because $s(k)\\notin S$. Thus, $s(k)\\in A$. However, as we have already seen, if $n\\lt s(k)$, then $n\\in S$, hence $n\\lt s(k)\\Rightarrow n\\notin A$. Thus, $n\\in A\\Rightarrow s(k)\\leq n$, which proves that $s(k)$ is the smallest element of $A$. $\\Box$", "meta": {"post_id": 68659, "input_score": 17, "output_score": 41, "post_title": "Set theoretic construction of the natural numbers"}}
{"input": "When I read the definitions of material and logical implications, they seem to me pretty much equivalent.  Could someone give me an example illustrating the difference?\n(BTW, I have no problem with the equivalence between $\\lnot p \\vee q$ and $p \\to q$, aka \"if $p$ then $q$\".  My confusion is with the idea that there are two different forms of implication, material and logical.)\nThanks!", "output": "There is one level at which they can be distinguished. The following definitions are relatively common.\n\nMaterial implication is a binary connective that can be used to create new sentences; so $\\phi \\to \\psi$ is a compound sentence using the material implication symbol $\\to$. Alternatively, in some contexts, material implication is the truth function of this connective.\nLogical implication is a relation between two sentences $\\phi$ and $\\psi$, which says that any model that makes $\\phi$ true also makes $\\psi$ true.  This can be written as $\\phi \\models \\psi$, or sometimes, confusingly, as $\\phi \\Rightarrow \\psi$, although some people use $\\Rightarrow$ for material implication. \n\nIn this distinction, material implication is a symbol at the object level, while logical implication is a relation at the meta level. In other words, material implication is a function of the truth value of two sentences in one fixed model, but logical implication is not directly about the truth values of sentences in a particular model, it is about the relation between the truth values of the sentences when all models are considered. \nThere is a close relationship between the two notions in first-order logic. It is somewhat immediate from the definitions that if $\\phi \\to \\psi$ holds in every model then $\\phi \\models \\psi$, and conversely if $\\phi \\models \\psi$ then $\\phi \\to \\psi$ is true in every model.  This relationship becomes more fuzzy when we begin to look at other logics, and in particular it can be quite fuzzy when philosophers talk about material conditionals and logical implication independent of any formal system.", "meta": {"post_id": 68932, "input_score": 40, "output_score": 49, "post_title": "What's the difference between material implication and logical implication?"}}
{"input": "Suppose that a sequence $x=(x_n)$ belongs both to $\\ell^p$ and $\\ell^q$ ($p,q>1$, $p\\neq q$).\nIs there any inequality between $\\|x\\|_p$ and $\\|x\\|_q$. Can one $\\ell^p$ be continuously embedded into another $\\ell^q$?", "output": "If $1 \\leq p \\leq q \\lt \\infty$ then $\\|x\\|_{q} \\leq \\|x\\|_{p}$ and clearly $\\|x\\|_p \\geq \\|x\\|_\\infty$. In particular, $\\ell^p$ embeds continuously into $\\ell^q$ whenever $p \\leq q$.\nTo see this, note that both sides of the inequality $\\|x\\|_{q} \\leq \\|x\\|_{p}$ are homogeneous in $x$ (multiplying $x$ with a positive real number multiplies both sides with the same positive factor), so we may take without loss of generality an $x$ with $\\|x\\|_{p} = 1$. Then $\\|x\\|_{q}^{q} = \\sum_{j = 1}^{\\infty} |x_{j}|^{q} \\leq \\sum_{j = 1}^{\\infty} |x_{j}|^{p} = 1$, and this is because for $t \\leq 1$ and $p \\leq q$ we have $t^{q} \\leq  t^{p}$.\nThis means that $p \\mapsto \\|x\\|_p$ is decreasing. In terms of spaces, we have the inclusions $\\ell^1 \\subset \\ell^p \\subset \\ell^q \\subset \\ell^{\\infty}$ whenever $1 \\lt p \\lt q \\lt \\infty$ and it is not hard to show that the inclusions are all strict.\nNote that this is opposite to the case of finite measure spaces $(\\Omega,\\mu)$, where the inclusions go the other way around: $L^1(\\Omega,\\mu) \\supset L^p(\\Omega,\\mu) \\supset L^{q}(\\Omega,\\mu) \\supset L^{\\infty}(\\Omega,\\mu)$.\nSee also the Wikipedia page on $L^p$-spaces.", "meta": {"post_id": 69125, "input_score": 23, "output_score": 55, "post_title": "Inequality between $\\ell^p$-norms"}}
{"input": "Could you give me An example of a function uniformly continuous on $\\mathbb{R}$ but not Lipschitz continuous?", "output": "There are many examples. Here are two of them and a simple method to produce many more:\n\n$f(x) = \\sqrt{|x|}$\n$g(x) = |x|\\sin{\\frac{1}{|x|}}$, where $g(0) = 0$ is understood\nIf $f:[0,\\frac{1}{2}] \\to \\mathbb{R}$ is a continuous function, define $\\phi_f:[0,1] \\to \\mathbb{R}$\n$$\\phi_f(x) = \\begin{cases} f(x), & \\text{ if } 0 \\leq x \\leq \\frac{1}{2}, \\\\ f(1-x), & \\text{ if } \\frac{1}{2} \\leq x \\leq 1.\\end{cases}$$ Note that the graph of $\\phi_f$ is simply the reflection of the graph of $f$ at the vertical line $x = \\frac{1}{2}$. Then $\\phi_f$ is continuous, satisfies $\\phi_f(0)= \\phi_f(1)$ and can thus be extended to a continuous periodic function by setting $\\varphi_{f}(x) = \\phi_f(x-\\lfloor x \\rfloor)$ for $x \\in \\mathbb{R}$. The function $\\varphi_f$\n\nis uniformly continuous\ncoincides with $f$ on $[0,\\frac{1}{2}]$\nis not Lipschitz continuous if $f$ fails to be Lipschitz continuous.\n\n\n\nFive key observations to prove the above claims and an aside:\n$1$. A continuous function on a compact interval is uniformly continuous, see the Wikipedia page on the Heine-Cantor theorem.\n$2$. If $f:(a,b) \\to \\mathbb{R}$ has the property that $f$ is continuous at $c \\in (a,b)$ and if $f$ is uniformly continuous on both intervals $(a,c)$ and $(c,b)$, respectively, then $f$ is uniformly continuous on all of $(a,b)$.\n$3$. If $f:(a,b) \\to \\mathbb{R}$ is differentiable and $f'$ is bounded then $f$ is Lipschitz continuous. This follows from the mean value theorem: if $|f'(\\xi)| \\leq L$ for all $\\xi \\in (a,b)$ then $|f(x)-f(y)| \\leq |f'(\\xi)|\\,|x-y| \\leq L|x-y|$ for all $x,y \\in (a,b)$.\n$4$.  If $f$ is Lipschitz continuous and differentiable at $x$ then $f'(x)$ is bounded by the Lipschitz constant:\n$$|f'(x)| = \\lim_{h \\to 0} \\frac{|f(x+h)-f(x)|}{|h|} \\leq \\frac{L|x+h-x|}{|h|} = L.$$\n$5$. Finally, a Lipschitz continuous function is differentiable almost everywhere \u2014 that's not easy: it's a consequence of Lebesgue's differentiation theorem and has a beautiful generalization as Rademacher's theorem.\n\nEdit: Why are $f(x) = \\sqrt{|x|}$ and $g(x)=|x|\\sin{\\frac{1}{|x|}}$ uniformly continuous on all of $\\mathbb{R}$?\nLet $C \\gt 0$. By $1$. above both $f$ and $g$ are uniformly continuous on $[-C,C]$ since $[-C,C]$ is compact. On $[C,\\infty)$ and $(-\\infty,-C]$ both $f$ and $g$ have a bounded derivative, hence they are uniformly continuous there by $3$. above. Now apply $2$. to the intervals $(-C,C)$ and $(C,\\infty)$ to see that $f$ and $g$ are uniformly continuous on $(-C,\\infty)$ and apply it again to $(-\\infty,-C)$ and $(-C,\\infty)$ to see that they are uniformly continuous on all of $\\mathbb{R}$.", "meta": {"post_id": 69457, "input_score": 51, "output_score": 69, "post_title": "An example of a function uniformly continuous on $\\mathbb{R}$ but not Lipschitz continuous"}}
{"input": "I read that there is a one-one correspondence between the ideals of $R/I$ and the ideals containing $I$. ($R$ is a ring and $I$ is any ideal in $R$)\nIs this bijection obvious? It's not to me. Can someone tell me what the bijection looks like explicitly? Many thanks for your help!", "output": "This is just one of the Isomorphism Theorems. It holds for groups, rings, modules, and in general any algebra (in the sense of universal algebra). The proofs are all the same; in fact, you can take the proof for groups and it will become a proof for rings mutatis mutandis.  Here it is, explicitly, for rings. \nLet $R$ be a ring, let $I$ be an ideal. The one-to-one correspondence between subrings of $R/I$ and subrings of $R$ that contain $I$ (which in fact also makes ideals correspond to ideals) is given as follows:\nLet $\\pi\\colon R\\to R/I$ be the canonical projection sending $r$ to the class $r+I$.\nGiven a subring $S$ of $R$ with $I\\subseteq S\\subseteq R$, we let \n$$\\pi(S) = \\{\\pi(s)\\mid s\\in S\\} = \\{s+I\\mid s\\in S\\}\\subseteq R/I.$$\nGiven a subring $T$ of $R/I$, we make it correspond to\n$$\\pi^{-1}(T) = \\{r\\in R\\mid \\pi(r)\\in T\\}.$$\n\n$\\pi(S)$ is a subring of $R/I$ whenever $S$ is a subring of $R$ that contains $I$. If $S$ is a (left, right, two-sided) ideal, then $\\pi(S)$ is a (left, right, two-sided) ideal of $R/I$.\nProof. $0\\in S$, so $\\pi(0) = 0+I \\in \\pi(S)$, hence $\\pi(S)$ is not empty. Also, if $(s+I),(t+I)\\in \\pi(S)$, with $s,t\\in S$, then $s-t\\in S$, so $(s+I)-(t+I) = (s-t)+I = \\pi(s-t)\\in \\pi(S)$. Thus, $\\pi(S)$ is a subgroup of $R/I$. And if $s+I,t+I\\in\\pi(S)$, with $s,t\\in S$, then $(s+I)(t+I) = st+I = \\pi(st)\\in \\pi(S)$ (since $S$ is a subring of $R$), so $\\pi(S)$ is a subring.\nIf in addition $S$ is a (left) ideal of $R$, then given $(s+I)\\in \\pi(S)$ and $(a+I)\\in R/I$, with $s\\in S$, we have $(a+I)(s+I) = as+I = \\pi(as)$; since $S$ is a (left) ideal, $s\\in S$ and $a\\in R$, then $as\\in S$, so $\\pi(as)\\in \\pi(S)$. Similar arguments establish the right and two-sided cases.\nIf $T$ is a subring of $R/I$, then $\\pi^{-1}(T)$ is a subring of $R$ that contains $I$. If $T$ is a (left, right, two-sided) ideal, then so is $\\pi^{-1}(T)$.\nProof. $0+I\\in T$, and since for all $a\\in I$, $\\pi(a)=a+I = 0+I\\in T$, then $a\\in \\pi^{-1}(T)$. Thus, $\\pi^{-1}(T)$ contains $I$. If $r,s\\in \\pi^{-1}(T)$, then so are $r-s$ and $rs$, since $\\pi(r-s) = (r-s)+I = (r+I)-(s+I)\\in T$ (since $r+I,s+I\\in T$ and $T$ is a subring) and $\\pi(rs) = rs+I = (r+I)(s+I)\\in T$ (since $T$ is closed under products and $r+I,s+I\\in T$). Thus, $\\pi^{-1}(T)$ is a subring of $R$.\nIf $T$ is a left ideal of $R/I$, and $s\\in\\pi^{-1}(T)$, $a\\in R$, then $\\pi(s)\\in T$, so $\\pi(as) = \\pi(a)\\pi(s)\\in T$ (since $T$ is a left ideal), so $as\\in\\pi^{-1}(T)$. Thus, $\\pi^{-1}(T)$ is a left ideal of $R$. Similar arguments establish the right and two-sided cases.\nThe correspondences are inverses of each other, hence they are bijections.\nProof.  If $(\\pi^{-1}\\circ\\pi)$ and $(\\pi\\circ\\pi^{-1})$ are both the identity, then $\\pi$ is an isomorphism.\nLet $S$ be an ideal of $R$ that contains $I$. Then $S\\subseteq \\pi^{-1}(\\pi(S))$ holds, because it holds for any subset and any function. Now let $a\\in \\pi^{-1}(\\pi(S))$. then $\\pi(a)\\in \\pi(S)$, so there exists $s\\in S$ such that $\\pi(a)=\\pi(s)$; hence $\\pi(a-s)\\in\\mathrm{ker}(\\pi) = I$. Thus, $a-s\\in I\\subseteq S$. Since $a-s,s\\in S$, and $S$ is a subring of $R$, then $a=(a-s)+s\\in S$. Thus, $\\pi^{-1}(\\pi(S))\\subseteq S$, proving $(\\pi^{-1}\\circ\\pi)=\\text{id}$.\nConversely, if $T$ is an ideal of $R/I$, then $\\pi(\\pi^{-1}(T))=T$, because $\\pi$ is onto and this equality holds for any surjective function. $\\Box$\nThe correspondences are inclusion-preserving.\nProof. For any function $f\\colon X\\to Y$ and subsets $A,B\\subseteq X$, if $A\\subseteq B$ then $f(A)\\subseteq f(B)$; and for any subsets $C,D$ of $Y$, if $C\\subseteq D$ then $f^{-1}(C)\\subseteq f^{-1}(D)$, so this follows from purely set-theoretic considerations.", "meta": {"post_id": 69578, "input_score": 22, "output_score": 41, "post_title": "Bijection between ideals of $R/I$ and ideals containing $I$"}}
{"input": "A number is an \"algebraic integer\" if it is the root to a monic polynomial with integer coefficients. Artin says (Algebra, p. 411):\n\nThe concept of algebraic integer was one of the most important discoveries of number theory. It is not easy to explain quickly why it is the right definition to use, but roughly speaking, we can think of the leading coefficient of the primitive irreducible polynomials $f(x)$ as a \"denominator.\" If $\\alpha$ is the root of an integer polynomial $f(x)=dx^n+a_{n-1}x^{n-1}...$ then $d\\alpha$ is an algebraic integer, because it is a root of the monic integer polynomial $x^n + a_{n-1}x^{n-1} + ... + d^{n-1}a_0$.\nThus we can \"clear the denominator\" in any algebraic number by multiplying it with a suitable integer to get an algebraic integer.\n\nWhen I first learned of algebraic integers, I looked online and saw some hints that maybe they were used to prove the Abel-Ruffini theorem. So I put off questioning their usage for a while; I now think I understand one proof of this theorem (the one at the end of Artin's Algebra) and it has nothing to do with algebraic integers (that I can tell).\nSo basically: why is it important if a number is an algebraic integer? I think I understand what he's saying about the relationship between roots of integer polynomials and algebraic integers, but I fail to see why this is \"one of the most important discoveries of number theory.\"", "output": "Suppose that we desire to consider as \"integers\" some subring $\\:\\mathbb I\\:$ of the field of all algebraic numbers. To be a purely algebraic notion, it cannot distinguish between conjugate roots, so if $\\rm\\:\\alpha,\\alpha'$ are roots of the same polynomial irreducible over $\\rm\\:\\mathbb Q\\:,\\:$ then $\\rm\\:\\alpha\\in\\mathbb I\\iff \\alpha'\\in\\mathbb I\\:.\\:$ Also we desire $\\rm\\:\\mathbb I\\cap \\mathbb Q = \\mathbb Z\\ $ so that our notion of algebraic integer is a faithful extension of the notion of a rational integer. Now suppose that $\\rm\\:f(x)\\:$ is the monic minimal polynomial over $\\rm\\:\\mathbb Q\\:$ of an algebraic \"integer\" $\\rm\\:\\alpha\\in \\mathbb I\\:.\\:$ Then $\\rm\\:f(x) = (x-\\alpha)\\:(x-\\alpha')\\:(x-\\alpha'')\\:\\cdots\\:$ has coefficients in $\\rm\\:\\mathbb I\\cap \\mathbb Q = \\mathbb Z\\:.\\:$ Therefore the monic minimal polynomial of elements $\\in\\mathbb I\\:$ must have coefficients $\\in\\mathbb Z\\:.\\:$ Conversely, one easily shows that the set of all such algebraic numbers contains $1$ and is closed under both difference and multiplication, so it forms a ring. Moreover, as Artin's quote shows, the quotient field of $\\rm\\:\\mathbb I\\:$ is the field of all algebraic numbers. Hence a few natural hypotheses on the notion of an algebraic integer imply the standard criterion in terms of minimal polynomials.\nBecause this notion of integer faithfully extends the notion of rational integers, we can employ algebraic integers to deduce results about rational integers. This often results in great simplifications because many diophantine equations become simpler - being \"linearized\" - when one factors them in algebraic extension fields. For example, see proofs about Pythagorean triples using Gaussian integers, or classical proofs of FLT for small exponents employing algebraic integers.", "meta": {"post_id": 70088, "input_score": 27, "output_score": 36, "post_title": "Why do we use this definition of \"algebraic integer\"?"}}
{"input": "I am new to differential geometry and I am trying to understand Gaussian curvature. The definitions found at Wikipedia and Wolfram sites are too mathematical. Is there any intuitive way to understand Gaussian curvature?", "output": "I know you're looking for an intuitive explanation, but I've always believed that intuition ought to come from concrete mathematical facts, if possible.  Otherwise, you have no way of knowing whether or not the intuition someone feeds you actually matches the formal mathematics.  (On that note, +1 for Joseph O'Rourke's answer.)\nThe Gaussian curvature, $K$, is given by $$K = \\kappa_1 \\kappa_2,$$ where $\\kappa_1$ and $\\kappa_2$ are the principal curvatures.  Just from this definition, we know a few things:\n\nFor $K$ to be a large positive number, then $\\kappa_1$ and $\\kappa_2$ should both be large and have the same sign (i.e. both positive or both negative).\nFor $K$ to be zero, either $\\kappa_1 = 0$ or $\\kappa_2 = 0$.\nFor $K$ to be a large negative number, then $\\kappa_1$ and $\\kappa_2$ should both be large but have opposite signs.\n\nNow recall that $\\kappa_1(p)$ and $\\kappa_2(p)$ are the maximum and minimum normal curvatures of all curves passing through $p.$  So:\n\n$K(p) > 0$ means that the curves through $p$ of extremal normal curvature \"curve the same way\" (such as the red curve and the green curve).  So, points in the purple region have $K > 0$.  In some sense, the surface is shaped like an elliptic paraboloid there (like a bowl).\n$K(p) = 0$ means that one of the curves through $p$ of extremal curvature has zero normal curvature (such as the yellow curve).  So, points along the yellow curve have $K = 0$.  In some sense, the surface is shaped like a parabolic cylinder there (like a bent piece of paper).\n$K(p) < 0$ means that the curves through $p$ of extremal curvature \"curve in opposite ways\" (such as the blue curve and green curve).  For example, points in the gray region have $K < 0$.  In some sense, the surface is shaped like a hyperbolic paraboloid there (a saddle).\n\n\nIn fact, using Dupin's Indicatrix (which is really just a 2nd-order Taylor expansion) we can make rigorous the notion of being \"locally like\" an elliptic paraboloid, a cylinder, or a hyperbolic paraboloid.", "meta": {"post_id": 70210, "input_score": 87, "output_score": 38, "post_title": "Is there any easy way to understand the definition of Gaussian Curvature?"}}
{"input": "I'm looking for subset $A$ of $\\mathbb R$ such that $A$ is a Borel set but $A$ is neither $F_\\sigma$ nor $G_\\delta$.", "output": "There are many examples. Here's one:\nObserve first that the rational numbers $\\mathbb{Q}$ are an $F_{\\sigma}$. This is because they are a countable union of points. The irrational numbers $\\mathbb{R} \\smallsetminus \\mathbb{Q} = \\bigcap_{q \\in \\mathbb{Q}} \\mathbb{R} \\smallsetminus \\{q\\}$ are thus a $G_{\\delta}$. Since both $\\mathbb{Q}$ and $\\mathbb{R} \\smallsetminus \\mathbb{Q}$ are dense and disjoint it follows from the Baire category theorem that $\\mathbb{Q}$ cannot be a $G_{\\delta}$. [Edit: See also this thread here containing several proofs that $\\mathbb{Q}$ can't be a $G_{\\delta}$ in $\\mathbb{R}$. These proofs explicitly avoid Baire].\nThe same reasoning shows that $F = \\mathbb{Q}_{\\geq 0}$ is an $F_{\\sigma}$ in $[0,\\infty)$, but isn't a $G_{\\delta}$, and that $G= \\mathbb{R}_{\\leq 0} \\smallsetminus \\mathbb{Q}_{\\leq 0}$ is a $G_{\\delta}$ in $(-\\infty,0]$, but isn't an $F_{\\sigma}$. Their union $F \\cup G$ is then an example of a Borel subset of $\\mathbb{R}$ which is neither an $F_{\\sigma}$ nor a $G_{\\delta}$ because if it were an $F_{\\sigma}$ then the same would hold for $G = (F \\cup G) \\cap (-\\infty,0)$, for example. I leave it as an exercise to show that $F \\cup G$ is both an $F_{\\sigma\\delta}$ and a $G_{\\delta\\sigma}$.\nThat's probably the easiest example. A few more (both more interesting but also more involved ones) can be found in this MO thread.\n\nFor a much more in-depth discussion of such ideas, I recommend looking into one of the following books:\n\nA.S. Kechris, Classical Descriptive Set Theory, Springer GTM\u00a0156.\nS.M. Srivastava, A course on Borel sets, Springer GTM\u00a0180.\nJ.C. Oxtoby, Measure and Category, Springer GTM\u00a02.\n\nSpecifically, look up the sections on the Borel hierarchy.", "meta": {"post_id": 73296, "input_score": 52, "output_score": 57, "post_title": "Example of a Borel set that is neither $F_\\sigma$ nor $G_\\delta$"}}
{"input": "Construct a function which is continuous in $[1,5]$ but not differentiable at $2, 3, 4$.\n\nThis question is just after the definition of differentiation and the theorem that if $f$ is finitely derivable at $c$, then $f$ is also continuous at $c$. Please help, my textbook does not have the answer.", "output": "$$\\ \\ \\ \\ \\mathsf{W}\\ \\ \\ \\ $$", "meta": {"post_id": 74347, "input_score": 103, "output_score": 995, "post_title": "Construct a function which is continuous in $[1,5]$ but not differentiable at $2, 3, 4$"}}
{"input": "I found this problem in a textbook of abstract algebra:\n\nLet $H$ be a subgroup of $G$. Prove that $$\\{x\\in G:xHx^{-1}\\subseteq H\\}$$ is a subgroup of $G$.\n\nIt's easy to prove that the set is closed under multiplication, but I'm stuck on proving that it is closed under inverses.\nIf $H$ is finite, say $H=\\{a_1,\\ldots,a_n\\}$, suppose $x$ is an element of the set. Then $xa_1x^{-1},\\ldots,xa_nx^{-1}$ are all distinct, hence they are exactly $a_1,\\ldots,a_n$, in some order. Therefore any element $b\\in H$ can be written as $xcx^{-1}$ for some $c\\in H$, and hence $x^{-1}bx=x^{-1}(xcx^{-1})x=c$ is also in $H$. So $x^{-1}$ is also an element of the set.\nHowever, the above method does not work if $H$ is infinite. The main idea is to prove that $x^{-1}ax\\in H$ for every $a\\in H$, given that $xax^{-1}\\in H$ for every $a\\in H$. I was trying to do some substitutions of $a$ to get the required result, but I can't seem to get the $x^{-1}$ to the left.\nAny help would be appreciated. It may be worth mentioning that I just started learning this group theory thing for a few days, so please adjust your explanation accordingly.\nThanks in advance.", "output": "The reason you are having trouble proving it is that it is not true as stated. \nFor a heavy-handed example, let $G$ be the free group on $x$ and $y$, and let $H$ be the subgroup generated by all elements of the form $x^nyx^{-n}$ with $n\\gt 0$. \nThen for any $a\\in H$ we have $xax^{-1}\\in H$. However, $x^{-1}yx\\notin H$, because any element of $H$ is a word that starts with a nonnegative power of $x$.\nTo fix the problem, you would need to require $xHx^{-1}=H$, rather than $xHx^{-1}\\subseteq H$. Then your argument would go through in the infinite case as well.\n\nOkay, here's an example you can get your hands on (courtesy my Math 257 notes with T.Y. Lam, Spring 97):\nLet $G$ be the group of all invertible $2\\times 2$ matrices with coefficients in $\\mathbb{Q}$, $G=\\mathrm{GL}_2(\\mathbb{Q})$.\nLet $H$ be the subgroup given by\n$$ H = \\left\\{\\left.\\left(\\begin{array}{cc}\r\n1 & m\\\\\r\n0 & 1\\end{array}\\right)\\in G\\ \\right|\\ m\\in\\mathbb{Z}\\right\\}.$$\nLet \n$$x = \\left(\\begin{array}{cc}\r\n2 & 0\\\\\r\n0 & 1\\end{array}\\right), \\qquad x^{-1} = \\left(\\begin{array}{cc}\r\n\\textstyle\\frac{1}{2} & 0 \\\\\r\n0 & 1\r\n\\end{array}\\right).$$\nThen for every $m\\in\\mathbb{Z}$ we have:\n$$\\begin{align*}\r\n\\left(\\begin{array}{cc}\r\n2 & 0\\\\ 0 & 1\\end{array}\\right)\\left(\\begin{array}{cc}1 & m\\\\0 & 1\\end{array}\\right) \\left(\\begin{array}{cc}\\textstyle\\frac{1}{2}&0\\\\0&1\\end{array}\\right)\r\n&= \\left(\\begin{array}{cc}\r\n2 & 2m\\\\0 & 1\\end{array}\\right)\\left(\\begin{array}{cc}\r\n\\textstyle\\frac{1}{2}&0\\\\0 & 1\\end{array}\\right)\\\\\r\n&= \\left(\\begin{array}{cc}\r\n1 & 2m\\\\\r\n0 & 1\\end{array}\\right)\\in H.\r\n\\end{align*}$$\nSo $xHx^{-1}\\subseteq H$. However, even though\n$$ \\left(\\begin{array}{cc}1&1\\\\0&1\\end{array}\\right)\\in H,$$\nwe have\n$$\\begin{align*}\r\nx^{-1}\\left(\\begin{array}{cc}1&1\\\\0&1\\end{array}\\right)x &= \\left(\\begin{array}{cc}\r\n\\textstyle\\frac{1}{2} & 0\\\\0 &1\\end{array}\\right)\\left(\\begin{array}{cc}1 & 1\\\\ 0 & 1\\end{array}\\right)\\left(\\begin{array}{cc}2 & 0\\\\0&1\\end{array}\\right)\\\\\r\n&= \\left(\\begin{array}{cc}\r\n\\textstyle\\frac{1}{2}&\\textstyle\\frac{1}{2}\\\\0&1\\end{array}\\right)\\left(\\begin{array}{cc}2&0\\\\0&1\r\n\\end{array}\\right)\\\\\r\n&= \\left(\\begin{array}{cc}\r\n1 & \\textstyle\\frac{1}{2}\\\\0&1\\end{array}\\right)\\notin H.\r\n\\end{align*}$$\nSo $x\\in \\{g\\in G\\mid ghg^{-1}\\in H\\text{ for all }h\\in H\\}$, but $x^{-1}\\notin\\{g\\in G\\mid ghg^{-1}\\in H\\text{ for all }h\\in H\\}$. So the set need not be closed under inverses.", "meta": {"post_id": 75613, "input_score": 30, "output_score": 38, "post_title": "The set of all $x$ such that $xHx^{-1}\\subseteq H$ is a subgroup, when $H\\leq G$"}}
{"input": "I am developing a sample program to generate a 2D Barcode, using Reed-Solomon error correction codes. By going through this article, I am developing the program. But I couldn't understand how he generated the generator polynomial.\nCan anybody explain how they generated the generator polynomial? Please guide me to complete this correction step.", "output": "Let me show a toy example of a case, where we can use $\\alpha=2$. In the more general finite field we cannot think of $\\alpha$ as having a numerical 'value'. I use the finite field $GF(11)$ that is isomorphic to the ring of residue classes of integers modulo 11. I assume that you are at least somewhat familiar with modular arithmetic. This way we can take a look at the use of the generator polynomial in encoding the message without having to worry about the construction of the finite field as well. Also the answer will be of a `reasonable' size :-)\n$$\nGF(11)=\\{0,1,2,3,4,5,6,7,8,9,A=-1\\},\n$$\nwhere $A$ stands for the residue class of $10\\equiv-1\\pmod{11}$. The Reed-Solomon codes use\nthe fact that the multiplicative group of non-zero elements of this (and any other) finite field\nis cyclic, i.e. consists of powers of a carefully selected element $\\alpha$. Trial and error shows that we can select $\\alpha=2$, because $\\alpha^0=1$, $\\alpha^1=2$, $\\alpha^2=4$,\n$\\alpha^3=8$, $\\alpha^4=16= 5$, $\\alpha^5=\\alpha\\cdot \\alpha^4=2\\cdot 5=10=-1$,\n$\\alpha^6=2\\cdot A=20= 9$, $\\alpha^7=2\\cdot9=18= 7$, $\\alpha^8=2\\cdot7=14=3$, and\n$\\alpha^9=2\\cdot3=6$. Note that i) I simply equate any two numbers that are congruent with each other modulo 11, because then the two numbers represent the same element of the field, ii) I won't get any new elements by continuing, because $\\alpha^{10}=2\\cdot6=12=1=\\alpha^0$, so the powers of $\\alpha$ repeat starting from the tenth power. A similar thing happens with all the finite fields.\nIn this toy example I describe the encoding procedure using an RS-code with alphabet $GF(11)$ that has $r=4$ check symbols (IOW the code will have minimum distance $r+1=5$ and\nthus be able to correct up to $t=2$ errors, because $2t+1=5$. This type of an RS-code can\ncarry a message consisting of up to six ($6=11-1-r$) symbols $m_0,m_1,m_2,m_3,m_4,m_5$\nthat are all elements of the field $GF(11)$. We could agree to use shorter messages, but\nthis time I go with the maximum. The encoding process expands this message into a longer sequence $c_0,c_1,c_2,\\ldots,c_9$ of ten symbols from the field $GF(11)$.  In order to make the algebra easier to describe we view such sequences as a polynomials. So let $x$ be an unknown, and write\n$$m(x)= m_0+m_1x+m_2x^2+m_3x^3+m_4x^4+m_5x^5$$\nand\n$$c(x)= c_0+c_1x+c_2x^2+c_3x^3+\\cdots+c_9x^9.$$\nFor the error-correction to work as described, we must make sure that the polynomial $c(x)$ represents a valid codeword, so it has to be a multiple of the generator polynomial\nof degree $r=4$\n$$\ng(x)=(x-\\alpha)(x-\\alpha^2)(x-\\alpha^3)(x-\\alpha^4)=(x-2)(x-4)(x-8)(x-5).\n$$\nAs an exercise you are invited to verify that after expanding this product and reducing all the coefficients modulo 11 you get\n$$\ng(x)=x^4+3x^3+5x^2+8x+1.\n$$\nThere are two common ways of turning the message polynomial $m(x)$ to a codeword $c(x)$\nthat is always divisible by $g(x)$. The simplest way (algebraically) is to declare\n$$\nc(x)=g(x)m(x).\n$$\nThis is what is known (see e.g. the Wikipedia page) as non-systematic encoding, so e.g. the said Wikipedia page and\nDilip's answer denote this polynomial \n$c_{nonsys}(x)$. For example, if the message sequence that you want to encode is\n$(m_0,m_1,m_2,m_3,m_4,m_5)=(3,0,0,0,0,1)$, the message polynomial is $m(x)=3+x^5$, and\n$$\nc_{nonsys}(x)=g(x)m(x)=g(x)(x^5+3)=3 + 2x + 4x^2 + 9x^3 + 3x^4 + x^5 + 8x^6 + 5x^7 + \n    3x^8 + x^9,\n$$\nso the encoded message is the sequence $(3,2,4,9,3,1,8,5,3,9)$.\nFor practical reasons engineers often prefer to use so called systematic encoding. Dilip's answer (linked to above) gives you the following recipe: Compute the polynomial\n$x^rm(x)=x^4(x^5+3)=x^9+3x^4$, and then compute the remainder, when you divide this\npolynomial with the generator polynomial $g(x)$. The answer is $r(x)=4x+4x^2+x^3$.\nThus the polynomial\n$$\nc_{sys}(x)=x^4 m(x)-r(x)=x^9+3x^4-x^3-4x^2-4x=7x+7x^2+Ax^3+3x^4+x^9\n$$\nis also divisible by $g(x)$. This time the encoded sequence is thus $(0,7,7,A,3,0,0,0,0,1)$. The reason why this is called systematic is that you see the\npayload message sequence $(3,0,0,0,0,1)$ at the end.\n=======================\nAdded: Constructing finite fields of characteristic two.\nHere we need more algebra. The field $GF(256)$ is of characteristic two. In other words, every element $\\beta \\in GF(256)$ satisfies the relation $\\beta+\\beta=0$. To give you the idea I first describe, how you get a smaller field $GF(8)$. This is just to save space.\nThe idea is that we want to list the elements of this field as powers of a special element\n$\\alpha$ the same way we used powers of two in the earlier example. A field will always have special elements $0,1$, so to get to eight elements we want the field to look like\n$$\nGF(8)=\\{0,1,\\alpha,\\alpha^2,\\alpha^3,\\alpha^4,\\alpha^5,\\alpha^6\\}.\n$$\nIn the above example of $GF(11)$ the powers of $\\alpha$ started repeating after the tenth power ($2^{10}\\equiv 1\\pmod{11}$). Here we want the powers to start repeating starting from the seventh ($7=8-1$, $10=11-1$), so we want $\\alpha^7=1$. Furthermore, we want to be able to add elements of the field together, like $\\alpha^3+\\alpha^5$ or $1+\\alpha^4$ should be one of the elements. The way to achieve this is to declare that $\\alpha$ is a root of certain carefully chosen polynomial equation. This time we choose the equation \n$$\\alpha^3+\\alpha+1=0.$$\nIOW, $\\alpha$ is a root of the polynomial $p(x)=x^3+x+1$. \nHow does that help? The idea is that then we can calculate with powers of $\\alpha$, and always use that equation $p(\\alpha)=0$ to reduce to lower powers of $\\alpha$. This is much the same way as when you multiply complex numbers together, you use the equation $i^2=-1$, e.g. $(2+i)(3+i)=6+2i+3i+i^2=6+5i+i^2=6+5i-1=5+5i$. Only this time the equation only begins to help, when we reach the third power. Here \n$$\n\\alpha^3=\\alpha^3+0=\\alpha^3+p(\\alpha)=\\alpha^3+\\alpha^3+\\alpha+1=1+\\alpha,\n$$\nbecause $\\alpha^3+\\alpha^3=0$. Let's see what happens, when we reduce the powers of $\\alpha$ using this relation. In the last column of the following table I always\nlist the fully reduced version of that power of $\\alpha$.\n$$\n\\eqalign{\n\\alpha^0&=&&=1,\\\\\n\\alpha^1&=&&=\\alpha,\\\\\n\\alpha^2&=&&=\\alpha^2,\\\\\n\\alpha^3&=&&=1+\\alpha,\\\\\n\\alpha^4&=&\\alpha\\cdot\\alpha^3=\\alpha(1+\\alpha)&=\\alpha+\\alpha^2,\\\\\n\\alpha^5&=&\\alpha\\cdot\\alpha^4=\\alpha(\\alpha+\\alpha^2)=\\alpha^2+\\alpha^3=\\alpha^2+(1+\\alpha)&=1+\\alpha+\\alpha^2,\\\\\n\\alpha^6&=&\\alpha\\cdot\\alpha^5=\\alpha(1+\\alpha+\\alpha^2)=\\alpha+\\alpha^2+\\alpha^3=\n\\alpha+\\alpha^2+(1+\\alpha)&=1+\\alpha^2,\\\\\n\\alpha^7&=&\\alpha\\cdot\\alpha^6=\\alpha(1+\\alpha^2)=\\alpha+\\alpha^3=\\alpha+(1+\\alpha)&=1.\n}$$\nHere the last row was in a way superfluous, but I wanted to show you that this choice of the polynomial $p(x)$ leads to the desired conclusion that the powers of $\\alpha$ start repeating after the seventh. Notice how a new line always depends on the preceding one, and how\nthe relation $\\alpha^3=\\alpha+1$ is applied as many times as necessary to get rid of all the cubic terms and higher.\nNow you should notice that the end results in the last column contain all the quadratic polynomials of the form $b_0+b_1\\alpha+b_2\\alpha^2$, where all the coefficients $b_i,i=0,1,2$ are bits, i.e. elements of the set $\\{0,1\\}$. That this worked out in this way is kind of a miracle, but it happened, because we were smart in choosing the polynomial $p(x)$. Notice that $p(x)$ is of degree three, and $8=2^3$. Further notice that we can choose to represent\nthe elements of $GF(8)$ in two ways: either as a power of $\\alpha$ or as a quadratic polynomial of $\\alpha$. Which do we use? Depends on what we want to do! If we want to add\ntwo elements of the field, we first switch to the quadratic polynomial form, so for example\n$$\n\\alpha^3+\\alpha^5=(1+\\alpha)+(1+\\alpha+\\alpha^2)=(1+1)+(\\alpha+\\alpha)+\\alpha^2=\\alpha^2.\n$$\nOn the other hand, if we want to multiply two elements of the field, we simply use the\nfact that the powers start repeating after the seventh, so $\\alpha^6\\cdot\\alpha^4=\\alpha^{10}=\n\\alpha^{7}\\cdot\\alpha^3=1\\cdot\\alpha^3=\\alpha^3$. Or, if the elements are given in the\nquadratic polynomial form, then we read the table from right to left e.g.\n$$\n(1+\\alpha)(1+\\alpha+\\alpha^2)=\\alpha^3\\cdot\\alpha^5=\\alpha^8=\\alpha\\cdot\\alpha^7=\\alpha.\n$$\nObserve that addition of two quadratic polynomials\n$$\n(a_0+a_1\\alpha+a_2\\alpha^2)+(b_0+b_1\\alpha+b_2\\alpha^2)=(c_0+c_1\\alpha+c_2\\alpha^2)\n$$\namounts to (because of the $\\beta+\\beta=0$ rule) bitwise XORing of the bit strings, or\n$c_0c_1c_2=(a_0a_1a_2)$^$(b_0b_1b_2)$, if I remember the correct C-notation. \nFor these reasons I keep two look up tables at hand, when I implement a finite field of characteristic two in a program. One to convert the powers $\\alpha^i$ to low degree polynomials, and another to go to the reverse direction. \nOk, so that was $GF(8)$, but you want $GF(256)$, where $256=2^8$. This time the field should look like\n$$\nGF(256)=\\{0,1,\\alpha,\\alpha^2,\\alpha^3,\\ldots,\\alpha^{254}\\}.\n$$\nNow the powers of $\\alpha$ start repeating starting from the $255^{th}$, so $\\alpha^{255}=1$.\nInstead of quadratic polynomials we now end up using the representation in the form\n$$\nb_0+b_1\\alpha+b_2\\alpha^2+\\cdots+b_7\\alpha^7\n$$\nin terms of $8$ bits $b_0,b_1,\\ldots,b_7$. In other words, a single byte will represent an element of this field in the 'additive' form. How do we build the conversion table? To do that we need a suitable polynomial $p(x)$. This time $p(x)$ must be of degree 8. The most common choice is\n$$\np(x)=x^8+x^4+x^3+x^2+1.\n$$\nThe page that you linked to seems to be using this. The replacement rule that we get out of this is $\\alpha^8=\\alpha^4+\\alpha^3+\\alpha^2+1$. You can use this relation to get rid of\nall the eighth powers (and higher!) of $\\alpha$. This time the table would have 255 rows, so I hope that you understand, why I won't reproduce it here. Your link seems to have that table. \nFor a list of other possible polynomials see this link. They give some further links there, too. The links from your other questions lead to some hopefully useful C-code.", "meta": {"post_id": 76045, "input_score": 17, "output_score": 43, "post_title": "Reed Solomon Polynomial Generator"}}
{"input": "How to show that $x^4+1$ is irreducible in $\\mathbb Z[x]$ but it is reducible modulo every prime $p$?\n\nFor example I know that $x^4+1=(x+1)^4\\bmod 2$. Also $\\bmod 3$ we have that $0,1,2$ are not solutions of $x^4+1=0$ then if it is reducible the factors are of degree $2$. This gives that $x^4+1=(x^2+ax+b)(x^2+cx+d)$ and solving this system of equations $\\bmod 3$  gives that $x^4+1=(x^2+x+2) (x^2+2x+2) \\pmod 3$. But is there a simpler method to factor $x^4+1$ modulo a prime $p$?", "output": "If $-1$ is a square in $\\Bbb F_p$ (which includes the case $p=2$), say $a^2=-1$, then we have $$X^4+1=X^4-a^2=(X^2+a)(X^2-a).$$\nIf $p$ is odd and $2$ is a square in $\\Bbb F_p$, say $2=b^2$, then we have\n$$X^4+1=(X^2+1)^2-(bX)^2=(X^2+bX+1)(X^2-bX+1) $$\nIf $p$ is odd and neither $-1$ nor $2$ is a square, then their product $-2$ is a square, say $-2=c^2$. (Without using anything even remotely as deep as quadratic reciprocity, this follows immediately from the fact that $\\Bbb F_p^\\times$ is a cyclic group of even order). Then we have\n$$ X^4+1=(X^2-1)^2-(cX)^2=(X^2-cX-1)(X^2+cX-1)$$", "meta": {"post_id": 77155, "input_score": 50, "output_score": 68, "post_title": "Irreducible polynomial which is reducible modulo every prime"}}
{"input": "Let $f(n)$ denote the order of the smallest finite group which cannot be generated with less than $n$ elements. Trivially $f(n) \\leq 2^n$ since ${\\mathbb F}_2^n$ can be seen as a vector space with dimension $n$. Is the exact value of $f(n)$ known?", "output": "$f(n) = 2^n$\nLet $X$ be the set of n generators of the finite group $G$ where $G$ cannot be generated by $n-1$ elements. Consider a sequence of subgroups of $G$, the first subgroup being $G$ itself generated by all of $X$, each of the next subgroups generated by one fewer generator than the last, finally down the trivial group. Each subgroup in the sequence must have distinct order which divides the order of the previous subgroup in the sequence. Thus the order of $G$ must be the product of $n$ terms all greater than one. Thus the smallest possible order of $G$ is $2^n$.", "meta": {"post_id": 78179, "input_score": 33, "output_score": 35, "post_title": "Smallest order for finite group that needs many elements to generate it"}}
{"input": "Possible Duplicate:\nOrder of elements in abelian groups \n\n\nLet $G$ be an abelian group and suppose that $G$ has elements of orders $m$ and $n$, respectively. Prove that $G$ has an element whose order is the least common multiple of $m$ and $n$.\n\nI've attempted this problem for quite some time, but didn't seem to get anywhere.\nFirst, let $a$ and $b$ be the elements whose orders are $m$ and $n$, respectively. I guessed that we can find the element of order $lcm(m,n)$ explicitly, instead of simply proving its existence. Furthermore, I also guessed that the element can be expressed in the form $a^kb^l$, because the statement must also hold when $G$ is generated by $a$ and $b$.\nThen I let $k$ be the smallest positive integer such that $a^k$ is a power of $b$, say $a^k=b^l$. Then I proved that $l$ is also the smallest positive integer such that $b^l$ is a power of $a$, and that $ml=nk$. I'm not sure whether it's correct though.\nThen I tried to find the order of ab. I can prove that the order is divisible by $\\frac{lcm(m,n)}{\\gcd(m,n)}$, but I can't prove whether it is equal to $lcm(m,n)$. Apparently, taking any $a^ib^j$ won't be any better. And now, I'm at wits end.\nPlease tell me whether I'm on the right path. If not, please give me some adequate hints so I can work on it.", "output": "First we consider the case where $(m,n)=1$.\nBecause $(m,n)=1$ the least common multiple of $m$ and $n$ is $mn$. \nConsider the element $ab$.\nBecause $G$ is Abelian, $$(ab)^{mn} = \\overbrace{ab\\ ab\\ ab \\ldots ab}^{mn} = \\overbrace{aaa\\ldots a}^{mn} \\overbrace{bbb\\ldots b}^{mn} = a^{mn}b^{mn} = ee =e$$\nso the order of $ab$ is at most $mn$.\nSay the order of $ab$ is $k$, then we just showed that $k \\leq mn$.\nWe see that $$e= (ab)^k = (ab)^{km} = a^{km}b^{km} = eb^{km} = b^{km} $$\nso $n$ divides $km$. But since $(m,n)=1$ it follows that $n$ divides $k$.\nSimilarly  $$e=(ab)^k = (ab)^{kn} = a^{kn}b^{kn} = a^{kn}e = a^{kn} $$ so $m$ divides $kn$, and so $m$ divides $k$.\nThus $mn$ divides $k$.\nAnd since $k \\leq mn$ it follows that $k = mn$, and so $ab$ has order $mn$, the lowest common multiple of $m$ and $n$.\nWe need to take from this the fact that the order of a product of elements that have relatively prime orders is the product of the orders of those elements.\nNow we consider the case where $m$ and $n$ are not relatively prime.\nCall $L$ the least common multiple of $m$ and $n$.\nWe write $L=p^{r_1}_1\\cdots p^{r_s}_s$ for distinct primes $p_i$ and strictly positive powers $r_i$.\nIf we could find an element of $G$ with order $p^{r_i}_i$ for every $i$, then by the first half of this proof, the product of these elements would have order $L$ because prime powers are all relatively prime to prime powers of different primes.\nLet $i$ with $1 \\leq i \\leq s$ be given.\nWe note that $p^{r_i}_i$ divides either $m$ or $n$.\nThus $a^{m/p^{r_i}_i}$ or $b^{n/p^{r_i}_i}$ (whichever one divides evenly) has order $p^{r_i}_i$.\nTherefore we have shown that an element of order $L$, the least common multiple of $m$ and $n$, is in $G$.\nThe statement is not necessarily true if $G$ is not Abelian.\nConsider $G=S_3$.\nThis group contains a cycle of order 3, and a transposition (of order 2), but no element of order 6.", "meta": {"post_id": 78544, "input_score": 18, "output_score": 35, "post_title": "If ord$(a)=m$, ord$(b)=n$ then does there exist  $c$ such that ord $(c)=lcm(m,n)$?"}}
{"input": "Can the determinant (assuming it's non-zero) be used to determine that the vectors given are linearly independent, span the subspace and are a basis of that subspace? (In other words assuming I have a set which I can make into a square matrix, can I use the determinant to determine these three properties?)\nHere are two examples:\n\nSpan\nDoes the following set of vectors span $\\mathbb R^4$: $[1,1,0,0],[1,2,-1,1],[0,0,1,1],[2,1,2,-1]$? Now the determinant here is $1$, so the set of vectors span $\\mathbb R^4$.\nLinear Independence Given the following augmented matrix:\n\n$$\\left[\\begin{array}{ccc|c}\r\n1 & 2  & 1 & 0 \\\\\r\n0 & -1 & 0 & 0 \\\\\r\n0 &  0 & 2 & 0 \r\n\\end{array}\\right],\r\n$$\nwhere again the determinant is non-zero ($-2$) so this set S is linearly independent.\nOf course I am in trouble if you can't make a square matrix - I figure for spans you can just rref it, and I suppose so for linear independence and basis?", "output": "Most introductory books on Linear Algebra have a Theorem which says something like\nLet $A$ be a square $n \\times n$ matrix. Then the following are equivalent:\n\n$A$ is invertible.\n$\\det(A) \\neq 0$.\nThe columns of $A$ are linearly independent.\nThe columns of $A$ span $R^n$.\nThe columns of $A$ are a basis in  $R^n$.\nThe rows of $A$ are linearly independent.\nThe rows of $A$ span $R^n$.\nThe rows of $A$ are a basis in  $R^n$.\nThe reduced row echelon form of $A$ has a leading 1 in each row.\nand many other conditions.....\n\nWhat does this mean, it simply means that if you want to check if any of these conditions is true or false, you can simply pick whichever other condition from the list and check it instead..\nYour question is: Can instead of third or fourth condition, check the second? That's exactly what the Theorem says: YES.", "meta": {"post_id": 79356, "input_score": 34, "output_score": 67, "post_title": "Using the Determinant to verify Linear Independence, Span and Basis"}}
{"input": "I am a big fan of the old-school games and I once noticed that there is a sort of parity associated to one and only one Tetris piece, the $\\color{purple}{\\text{T}}$ piece.  This parity is found with no other piece in the game.\nBackground: The Tetris playing field has width $10$.  Rotation is allowed, so there are then exactly $7$ unique pieces, each of which is composed of $4$ blocks.\nFor convenience, we can name each piece by a letter.  See this Wikipedia page for the Image ($\\color{cyan}{\\text{I}}$ is for the stick piece, $\\color{goldenrod}{\\text{O}}$ for the square, and $\\color{green}{\\text{S}},\\color{purple}{\\text{T}},\\color{red}{\\text{Z}},\\color{orange}{\\text{L}},\\color{blue}{\\text{J}}$ are the others)\nThere are $2$ sets of $2$ pieces which are mirrors of each other, namely $\\color{orange}{\\text{L}}, \\color{blue}{\\text{J}}$ and $\\color{green}{\\text{S}},\\color{red}{\\text{Z}}$ whereas the other three are symmetric $\\color{cyan}{\\text{I}},\\color{goldenrod}{\\text{O}}, \\color{purple}{\\text{T}}$\nLanguage: If a row is completely full, that row disappears.  We call it a perfect clear if no blocks remain in the playing field.  Since the blocks are size 4, and the playing field has width $10$, the number of blocks for a perfect clear must always be a multiple of $5$.\nMy Question:  I noticed while playing that the $\\color{purple}{\\text{T}}$ piece is particularly special.  It seems that it has some sort of parity which no other piece has.  Specifically:\n\nConjecture:  If we have played some number of pieces, and we have a perfect clear, then the number of $\\color{purple}{\\text{T}}$ pieces used must be even.  Moreover, the $\\color{purple}{\\text{T}}$ piece is the only piece with this property.\n\nI have verified the second part; all of the other pieces can give a perfect clear with either an odd or an even number used.  However, I am not sure how to prove the first part.  I think that assigning some kind of invariant to the pieces must be the right way to go, but I am not sure.\nThank you,", "output": "My colleague, Ido Segev, pointed out that there is a problem with most of the elegant proofs here - Tetris is not just a problem of tiling a rectangle.\nBelow is his proof that the conjecture is, in fact, false.", "meta": {"post_id": 80246, "input_score": 280, "output_score": 205, "post_title": "The Mathematics of Tetris"}}
{"input": "I have been trying to find the general formula for the $k$th order statistics of $n$ i.i.d exponential distribution random variables with mean $1$. And how to calculate the expectation and the variance of the $k$th order statistics. Can someone give me some general formula? It would be nice if there is any proof.", "output": "The minimum $X_{(1)}$ of $n$ independent exponential random variables with parameter $1$ is exponential with parameter $n$. Conditionally on  $X_{(1)}$, the second smallest value  $X_{(2)}$ is distributed like the sum of  $X_{(1)}$ and an independent exponential random variable with parameter $n-1$. And so on, until the $k$th smallest value  $X_{(k)}$ which is distributed like the sum of  $X_{(k-1)}$ and an independent exponential random variable with parameter $n-k+1$.\nOne sees that $X_{(k)}=Y_{n}+Y_{n-1}+\\cdots+Y_{n-k+1}$ where the random variables $(Y_i)_i$ are independent and exponential with parameter $i$. Each $Y_i$ is distributed like $\\frac1iY_1$, and $Y_1$ has expectation $1$ and variance $1$, hence\n$$\r\n\\mathrm E(X_{(k)})=\\sum\\limits_{i=n-k+1}^n\\frac1i,\\qquad\r\n\\mbox{Var}(X_{(k)})=\\sum\\limits_{i=n-k+1}^n\\frac1{i^2}.\r\n$$", "meta": {"post_id": 80475, "input_score": 30, "output_score": 53, "post_title": "Order statistics of i.i.d. exponentially distributed sample"}}
{"input": "Let $c$ denote the space of convergent sequences in $\\mathbb C$, $c_0\\subset c$ be the space of all sequences that converge to $0$. Given the uniform metric, both of them can be made into Banach spaces. It can be shown that the dual spaces of them are isometrically isomorphic, i.e. $c^*\\cong c_0^*$. Are $c$ and $c_0$ isometrically isomorphic? If not, how can one show the absence of such a isometric isomorphism? Thanks!", "output": "The closed unit ball of $c_0$ has no extreme points. The closed unit ball of $c$ has many extreme points, such as $(1,1,\\ldots)$. Since the property of being an extreme point is preserved by isometries, $c$ and $c_0$ are not isometrically isomorphic.", "meta": {"post_id": 80727, "input_score": 55, "output_score": 55, "post_title": "Are these two Banach spaces isometrically isomorphic?"}}
{"input": "This is a homework question and I am not really sure where to go with it. I have a lot of trouble with sequences and series, can I get a tip or push in the right direction?", "output": "You have:\n$$\r\nx_n:=\\left(1-\\frac1n\\right)^{-n} = \\left(\\frac{n-1}n\\right)^{-n} = \\left(\\frac{n}{n-1}\\right)^{n}\r\n$$\n$$\r\n = \\left(1+\\frac{1}{n-1}\\right)^{n} = \\left(1+\\frac{1}{n-1}\\right)^{n-1}\\cdot \\left(1+\\frac{1}{n-1}\\right) = a_n\\cdot b_n.\r\n$$\nSince $a_n\\to \\mathrm e$ and $b_n\\to 1$ you obtain what you need.", "meta": {"post_id": 82034, "input_score": 18, "output_score": 62, "post_title": "Prove that $(1 - \\frac{1}{n})^{-n}$ converges to $e$"}}
{"input": "I understand the whole concept of Rencontres numbers but I can't understand how to prove this equation\n$$D_{n,0}=\\left[\\frac{n!}{e}\\right]$$\nwhere $[\\cdot]$ denotes the rounding function (i.e., $[x]$ is the integer nearest to $x$). This equation that I wrote comes from solving the following recursion, but I don't understand how exactly the author calculated this recursion.\n$$\\begin {align*} \nD_{n+2,0} & =(n+1)(D_{n+1,0}+D_{n,0}) \\\\\nD_{0,0} & = 1 \\\\\nD_{1,0} & = 0 \n\\end {align*}\n$$", "output": "Derangements:\nA Derangement is a permutation, $P$, in which no element is mapped to itself; that is, $P(k)\\ne k$, for $1\\le k\\le n$. Let $\\mathcal{D}(n)$ be the number of derangements of $n$ items.\nHere are a few methods of computing $\\mathcal{D}(n)$.\nMethod 1 (build from smaller derangements):\nLet us count the number of derangements of $n$ items so that $P(P(n))=n$.  There are $n-1$ choices for $P(n)$, and for each of those choices, $\\mathcal{D}(n-2)$ ways to arrange the other $n-2$ items. Thus, there are $(n-1)\\mathcal{D}(n-2)$ derangements of $n$ items so that $P(P(n))=n$.\nLet us count the number of derangements of $n$ items so that $P(P(n))\\not=n$. There are $n-1$ choices for $P(n)$, and for each choice, there is a derangement of $n-1$ items identical to $P$ except that they map $P^{-1}(n)\\to P(n)$. Thus, there are $(n-1)\\mathcal{D}(n-1)$ derangements of $n$ items so that $P(P(n))\\not=n$.\nTherefore,\n$$\n\\mathcal{D}(n)=(n-1)(\\mathcal{D}(n-1)+\\mathcal{D}(n-2))\\tag{1}\n$$\nMethod 2 (count permutations):\nCount the number of permutations of $n$ items by counting how many fix exactly $k$ items.\nThere are $\\binom{n}{k}$ ways to choose the $k$ items to fix, then $\\mathcal{D}(n-k)$ ways to arrange the $n-k$ items that are not fixed. Since there are $n!$ permutations of $n$ items, we get\n$$\nn!=\\sum_{k=0}^n\\binom{n}{k}\\mathcal{D}(n-k)\\tag{2}\n$$\nand therefore, rearranging $(2)$ yields\n$$\n\\mathcal{D}(n)=n!-\\sum_{k=1}^n\\binom{n}{k}\\mathcal{D}(n-k)\\tag{3}\n$$\nMethod 3 (inclusion-exclusion):\nLet $S_i$ be the set of permutations of $n$ items which fix item $i$. Then the number of permutations in $k$ of the $S_i$ would be the number of permutations that fix $k$ items.  There are $\\binom{n}{k}$ ways to choose the $k$ items to fix, and $(n-k)!$ ways to arrange the other $n-k$ items. Thus, the number of permutations that fix at least $1$ item would be\n$$\n\\sum_{k=1}^n(-1)^{k-1}\\binom{n}{k}(n-k)!=\\sum_{k=1}^n(-1)^{k-1}\\frac{n!}{k!}\\tag{4}\n$$\nSince there are $n!$ permutations in total, the number of permutations that don't fix any items is\n$$\n\\begin{align}\n\\mathcal{D}(n)\n&=n!-\\sum_{k=1}^n(-1)^{k-1}\\frac{n!}{k!}\\\\\n&=\\sum_{k=0}^n(-1)^k\\frac{n!}{k!}\\tag{5}\\\\\n&\\approx \\frac{n!}{e}\n\\end{align}\n$$\nIn fact, the difference\n$$\n\\begin{align}\n\\left|\\frac{n!}{e}-\\mathcal{D}(n)\\right|\n&=\\left|\\sum_{k=n+1}^\\infty(-1)^k\\frac{n!}{k!}\\right|\\\\\n&=\\left|\\frac{1}{n+1}-\\frac{1}{(n+1)(n+2)}+\\frac{1}{(n+1)(n+2)(n+3)}-\\dots\\right|\\\\\n&<\\frac{1}{n+1}\\tag{6}\n\\end{align}\n$$\nThis method yields directly that $\\mathcal{D}(n)$ is the closest integer to $\\frac{n!}{e}$ for $n>0$.\nDerivation of the Closed Form from the Recursion:\nGiven $\\mathcal{D}(0)=1$ and $\\mathcal{D}(1)=0$, and the recursion $(1)$, let's derive $(5)$. Subtracting $n\\mathcal{D}(n-1)$ from both sides of $(1)$ yields\n$$\n\\mathcal{D}(n)-n\\mathcal{D}(n-1)=-(\\mathcal{D}(n-1)-(n-1)\\mathcal{D}(n-2))\\tag{7}\n$$\nUsing the initial conditions, $(7)$ implies\n$$\n\\mathcal{D}(n)-n\\mathcal{D}(n-1)=(-1)^n\\tag{8}\n$$\nDividing both sides of $(8)$ by $n!$ yields\n$$\n\\frac{\\mathcal{D}(n)}{n!}-\\frac{\\mathcal{D}(n-1)}{(n-1)!}=\\frac{(-1)^n}{n!}\\tag{9}\n$$\nEquation $(9)$ is very simple to solve for $\\frac{\\mathcal{D}(n)}{n!}$:\n$$\n\\frac{\\mathcal{D}(n)}{n!}=\\sum_{k=0}^n\\frac{(-1)^k}{k!}+C\\tag{10}\n$$\nPlugging $n=0$ into equation $(10)$ yields that $C=0$. Therefore,\n$$\n\\mathcal{D}(n)=n!\\sum_{k=0}^n\\frac{(-1)^k}{k!}\\tag{11}\n$$\n\nIncomplete Gamma Function:\n$$\n\\begin{align}\n\\mathcal{D}(n)\n&=n!\\sum_{k=0}^n\\frac{(-1)^k}{k!}\\tag{12a}\\\\\n&=\\sum_{k=0}^n(-1)^k\\frac{n!}{k!}\\tag{12b}\\\\\n&=\\sum_{k=0}^n(-1)^k\\binom{n}{k}(n-k)!\\tag{12c}\\\\\n&=\\sum_{k=0}^n(-1)^k\\binom{n}{k}\\int_0^\\infty x^{n-k}e^{-x}\\,\\mathrm{d}x\\tag{12d}\\\\\n&=\\int_0^\\infty\\sum_{k=0}^n\\binom{n}{k}(-1)^kx^{n-k}e^{-x}\\,\\mathrm{d}x\\tag{12e}\\\\\n&=\\int_0^\\infty(x-1)^ne^{-x}\\,\\mathrm{d}x\\tag{12f}\\\\\n&=\\frac1e\\int_{-1}^\\infty x^ne^{-x}\\,\\mathrm{d}x\\tag{12g}\\\\\n&=\\frac1e\\Gamma(n+1,-1)\\tag{12h}\n\\end{align}\n$$\nExplanation:\n$\\text{(12a):}$ $(11)$\n$\\text{(12b):}$ bring the factor of $n!$ inside the sum\n$\\text{(12c):}$ $\\frac{n!}{k!}=\\binom{n}{k}(n-k)!$\n$\\text{(12d):}$ $n!=\\int_0^\\infty x^ne^{-x}\\,\\mathrm{d}x$\n$\\text{(12e):}$ swap the finite sum and the integral\n$\\text{(12f):}$ apply the Binomial Theorem\n$\\text{(12g):}$ substitute $x\\mapsto x+1$\n$\\text{(12h):}$ $\\Gamma(n,s)=\\int_s^\\infty x^{n-1}e^{-x}\\,\\mathrm{d}x$ is the Incomplete Gamma Function\nNegative Integer Arguments:$\\newcommand{\\Ei}{\\operatorname{Ei}}\\newcommand{\\PV}{\\operatorname{PV}}$\n$$\n\\begin{align}\n\\mathcal{D}(-1)\n&=\\frac1e\\int_{-1}^\\infty\\frac{e^{-x}}x\\,\\mathrm{d}x\\tag{13a}\\\\\n&=-\\frac{\\Ei(1)+\\pi i}e\\tag{13b}\n\\end{align}\n$$\nExplanation:\n$\\text{(13a):}$ apply $\\text{(12g)}$\n$\\text{(13b):}$ $\\Ei(z)=-\\PV\\int_{-z}^\\infty\\frac{e^{-t}}{t}\\,\\mathrm{d}t$\n$\\phantom{\\text{(13b):}}$ the infinitesimal clockwise semicircle\n$\\phantom{\\text{(13b):}}$ around the singularity at $0$ gives $-\\pi i$\nFor $n\\ge2$,\n$$\n\\begin{align}\n\\mathcal{D}(-n)\n&=\\frac1e\\int_{-1}^\\infty x^{-n}e^{-x}\\,\\mathrm{d}x\\tag{14a}\\\\\n&=-\\frac1{n-1}\\frac1e\\int_{-1}^\\infty e^{-x}\\,\\mathrm{d}x^{1-n}\\tag{14b}\\\\\n&=-\\frac1{n-1}\\frac1e\\left((-1)^ne+\\int_{-1}^\\infty x^{1-n}e^{-x}\\mathrm{d}x\\right)\\tag{14c}\\\\\n&=\\frac{(-1)^{n-1}}{n-1}-\\frac1{n-1}\\frac1e\\int_{-1}^\\infty x^{1-n}e^{-x}\\mathrm{d}x\\tag{14d}\\\\\n&=\\frac{(-1)^{n-1}}{n-1}-\\frac1{n-1}\\mathcal{D}(1-n)\\tag{14e}\n\\end{align}\n$$\nExplanation:\n$\\text{(14a):}$ apply $\\text{(12g)}$\n$\\text{(14b):}$ prepare to integrate by parts\n$\\text{(14c):}$ integrate by parts\n$\\text{(14d):}$ distribute\n$\\text{(14e):}$ apply $\\text{(12g)}$\nMultiply $(14)$ by $(-1)^{n-1}(n-1)!$ and apply induction:\n$$\n\\begin{align}\n(-1)^{n-1}(n-1)!\\mathcal{D}(-n)\n&=(n-2)!+(-1)^{n-2}(n-2)!\\mathcal{D}(1-n)\\tag{15a}\\\\\n&=\\sum_{k=0}^{n-2}k!+\\mathcal{D}(-1)\\tag{15b}\n\\end{align}\n$$\nCombine $(13)$ and $(15)$ and divide by $(-1)^{n-1}(n-1)!$:\n$$\n\\mathcal{D}(-n)=\\frac{(-1)^{n-1}}{(n-1)!}\\left(\\sum_{k=0}^{n-2}k!-\\frac{\\Ei(1)+\\pi i}e\\right)\\tag{16}\n$$", "meta": {"post_id": 83380, "input_score": 33, "output_score": 63, "post_title": "I have a problem understanding the proof of Rencontres numbers (Derangements)"}}
{"input": "Let's say I have $\\int_{0}^{\\infty}\\sum_{n = 0}^{\\infty} f_{n}(x)\\, dx$ with $f_{n}(x)$ being continuous functions. When can we interchange the integral and summation? Is $f_{n}(x) \\geq 0$ for all $x$ and for all $n$ sufficient? How about when $\\sum f_{n}(x)$ converges absolutely? If so why?", "output": "While most of the time I would use the Fubini/Tonelli conditions, the dominated convergence theorem is actually strictly stronger in this mixed sum/integral case, because it can take into account the order structure of the integers. An example (that I first worked up back in [2009])(http://artofproblemsolving.com/community/c7h294262p1593291):\nConsider the calculation\n\\begin{align*}\\ln 2 &= \\int_0^1 \\frac1{1+x}\\,dx = \\int_0^1\\sum_{n=0}^{\\infty} (-1)^n x^n\\,dx\\\\\n?&= \\sum_{n=0}^{\\infty}\\int_0^1(-1)^n x^n\\,dx = 1-\\frac12+\\frac13-\\frac14+\\cdots\\end{align*}\nFubini's theorem isn't strong enough to justify the interchange. If we put absolute values on the terms, it blows up to $\\int_0^1 \\frac1{1-x}\\,dx = 1+\\frac12+\\frac13+\\frac14+\\cdots=\\infty$.\nOn the other hand, the dominated convergence theorem cares about the partial sums $\\sum_{n=0}^{N}(-1)^n x^n$. By the alternating series estimate,\n$$0\\le \\sum_{n=0}^{N}(-1)^n x^n\\le 1$$\nfor all $x\\in [0,1]$. $1$ is integrable on this interval, and the interchange\n$$\\int_0^1\\left(\\lim_{N\\to\\infty}\\sum_{n=0}^{N}(-1)^n x^n\\right)\\,dx = \\lim_{N\\to\\infty}\\int_0^1 \\sum_{n=0}^{N}(-1)^n x^n\\,dx$$\nis justified, proving the result $1-\\frac12+\\frac13-\\frac14+\\cdots=\\ln 2$.\nThis situation with the dominated convergence theorem being stronger than Fubini's theorem can come up when we've got a reasonable bound on partial sums but not absolute convergence as a whole.\nThe monotone convergence theorem, on the other hand, is exactly the same as Tonelli's theorem - when everything's positive, either both sides are the same and finite or both sides are infinite.", "meta": {"post_id": 83721, "input_score": 217, "output_score": 36, "post_title": "When can a sum and integral be interchanged?"}}
{"input": "I recently solved a problem, which says that,\n\nA positive integer can be multiplied with another integer resulting in\n  a positive integer that is composed only of one and zero as digits.\n\nHow can I prove that this is true(currently I assume that it is). Also, is it possible to establish an upper bound on the length(number of digits) of the number generated?", "output": "Here is an alternate solution, which is based on the pigeonhole principle:\nList all the numbers 1, 11, 111, ... , 111...1 where the last number 111...1 contains $n+1$ ones.\nLook now at their remainders when divided by $n$. By the pigeonhole principle, two of them have the same remainder. But then their difference is of the form $1111..100000..0$ and is divisible by $n$..", "meta": {"post_id": 83932, "input_score": 51, "output_score": 145, "post_title": "A natural number multiplied by some integer results in a number with only ones and zeros"}}
{"input": "I'm trying to understand the derivation of Wiener deconvolution given on its Wikipedia page.  In the last couple steps under the derivation section, they take the derivative with respect to $G(f)$ of an equation that has both $G(f)$ and $G^\\ast(f)$ in it.  They simply state that $G^\\ast (f)$ acts as a constant in the differentiation.  However, it seems to me that if you don't treat $G(f)$ as a constant, then you shouldn't be able to treat $G^\\ast (f)$ as a constant because they are directly related.  \nI searched around some looking for an explanation.  I found this page, which seems to agree that the complex conjugate can be treated as a constant.  I also found some stuff about the Cauchy-Riemann equations, which seem to be related.  However, I haven't had any classes on complex analysis and don't understand the intuition behind why this can be done.\nWhy can the complex conjugate of a variable be treated as a constant when differentiating with respect to that variable?", "output": "The nomenclature of $\\dfrac{\\partial}{\\partial z}$ and $\\dfrac{\\partial}{\\partial\\bar{z}}$ is confusing because it gives the impression that these are really partial derivatives with respect to two independent variables, $z$ and $\\bar{z}$. However, it is clear that $z$ and $\\bar{z}$ are not independent.\nDifferentiable Functions and Conformal Maps\nA differentiable function on $\\mathbb{R}$ locally looks like a linear function, that is, there is a real constant, called $f'(x)$, so that for small $h$,\n$$\nf(x+h)=f(x)+f'(x)h+o(h)\\tag{1}\n$$\nAnalogously, a differentiable function on $\\mathbb{C}$ satisfies $(1)$ for some complex number $f'(x)$.\nMultiplication on $\\mathbb{C}$ acts as a rotation and radial scale when viewed as an action on $\\mathbb{R}^2$. Thus, if $f$ is differentiable on $\\mathbb{C}$,\n$$\nf(z+h)-f(z)=f'(z)h+o(h)\\tag{2}\n$$\nThat is, when $h$ is small, $h\\mapsto f(z+h)-f(z)$ looks like a scaled rotation. For this reason, a differentiable function on $\\mathbb{C}$ is called conformal: small features are replicated (scaled and rotated) and angles are preserved.\nComplex Conjugation and Orientation Reversal\nComplex conjugation, $z\\mapsto\\bar{z}$, is an orientation reversing isometry. Thus, when composed with a conformal map, either before or after, the composition is an orientation-reversing conformal map. Furthermore, double composition yields an orientation-preserving conformal map; for example, if $f(z)$ is conformal, then so is $\\overline{f(\\bar{z})}$.\nAs a function on $\\mathbb{R}^2$, complex conjugation can be represented by the matrix $\\begin{bmatrix}1&0\\\\0&-1\\end{bmatrix}$.\nConformal and Conjugate Conformal\nThe partial derivatives of a general differentiable function on $\\mathbb{R}^2$ given by $x+iy\\mapsto u+iv$ are usually given in a $2\\times2$ Jacobian matrix:\n$$\n\\frac{\\partial(u,v)}{\\partial(x,y)}=\\begin{bmatrix}\\frac{\\partial u}{\\partial x}&\\frac{\\partial v}{\\partial x}\\\\\\frac{\\partial u}{\\partial y}&\\frac{\\partial v}{\\partial y}\\end{bmatrix}\\tag{3}\n$$\nThe Cauchy-Riemann equations specify that $\\dfrac{\\partial u}{\\partial x}=\\dfrac{\\partial v}{\\partial y}$ and $\\dfrac{\\partial u}{\\partial y}=-\\dfrac{\\partial v}{\\partial x}$, which agrees with the following basis for the orientation-preserving conformal Jacobians on $\\mathbb{R}^2$:\n$$\n\\left\\{\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix},\\begin{bmatrix}0&1\\\\-1&0\\end{bmatrix}\\right\\}\\tag{4}\n$$\nNote that the determinant of any linear combination of these matrices has positive determinant (thus orientation is preserved).\nThe following basis for the orientation-reversing conformal Jacobians on $\\mathbb{R}^2$ follows by composing conjugation with $(4)$:\n$$\n\\left\\{\\begin{bmatrix}1&0\\\\0&-1\\end{bmatrix},\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\right\\}\\tag{5}\n$$\nNote that the determinant of any linear combination of these matrices has negative determinant (thus orientation is reversed).\nUsing $(4)$ and $(5)$, we can break any Jacobian into conformal and conjugate conformal parts. Using the component-wise orthogonality that exists among the bases, we can write the conformal part as\n$$\n\\frac{1}{2}\\left(\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}\\right)\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}+\\frac{1}{2}\\left(\\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y}\\right)\\begin{bmatrix}0&1\\\\-1&0\\end{bmatrix}\\tag{6}\n$$\nand the conjugate conformal part as\n$$\n\\frac{1}{2}\\left(\\frac{\\partial u}{\\partial x}-\\frac{\\partial v}{\\partial y}\\right)\\begin{bmatrix}1&0\\\\0&-1\\end{bmatrix}+\\frac{1}{2}\\left(\\frac{\\partial v}{\\partial x}+\\frac{\\partial u}{\\partial y}\\right)\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\tag{7}\n$$\n$\\dfrac{\\partial}{\\partial z}$, $\\dfrac{\\partial}{\\partial\\bar{z}}$, and Quaternions\nThe definitions of $\\dfrac{\\partial}{\\partial z}$ and $\\dfrac{\\partial}{\\partial\\bar{z}}$ say\n$$\n\\begin{align}\n\\frac{\\partial}{\\partial z}(u+iv)\n&=\\frac{1}{2}\\left(\\frac{\\partial}{\\partial x}-i\\frac{\\partial}{\\partial y}\\right)(u+iv)\\\\\n&=\\frac{1}{2}\\left(\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}\\right)+\\frac{i}{2}\\left(\\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y}\\right)\\tag{8}\n\\end{align}\n$$\nand\n$$\n\\begin{align}\n\\frac{\\partial}{\\partial\\bar{z}}(u+iv)\n&=\\frac{1}{2}\\left(\\frac{\\partial}{\\partial x}+i\\frac{\\partial}{\\partial y}\\right)(u+iv)\\\\\n&=\\frac{1}{2}\\left(\\frac{\\partial u}{\\partial x}-\\frac{\\partial v}{\\partial y}\\right)+\\frac{i}{2}\\left(\\frac{\\partial v}{\\partial x}+\\frac{\\partial u}{\\partial y}\\right)\\tag{9}\n\\end{align}\n$$\nThe space of $2\\times2$ Jacobians has $4$ dimensions, so trying to represent these $4$ dimensions with the $2$ dimensions of $\\mathbb{C}$, using $\\dfrac{\\partial}{\\partial z}$ and $\\dfrac{\\partial}{\\partial\\bar{z}}$, obscures something.\nThere is a common matrix representation of the complex numbers as $2\\times2$ real matrices where\n$$\n\\begin{align}\n\\mathbf{1}&\\leftrightarrow\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}\\tag{10}\\\\\n\\mathbf{i}&\\leftrightarrow\\begin{bmatrix}0&1\\\\-1&0\\end{bmatrix}\\tag{11}\n\\end{align}\n$$\nHowever, there is also a matrix representation of the quaternions as $2\\times2$ complex matrices where, in addition to $(10)$ and $(11)$,\n$$\n\\begin{align}\n\\mathbf{j}&\\leftrightarrow\\begin{bmatrix}i&0\\\\0&-i\\end{bmatrix}\\tag{12}\\\\\n\\mathbf{k}&\\leftrightarrow\\begin{bmatrix}0&-i\\\\-i&0\\end{bmatrix}\\tag{13}\n\\end{align}\n$$\nEmbed $(8)$ and $(9)$ in the quaternions to get\n$$\n\\left(\\frac{\\partial}{\\partial z}(u+iv)\\right)\\mathbf{1}\n=\\frac{\\mathbf{1}}{2}\\left(\\frac{\\partial u}{\\partial x}+\\frac{\\partial v}{\\partial y}\\right)+\\frac{\\mathbf{i}}{2}\\left(\\frac{\\partial v}{\\partial x}-\\frac{\\partial u}{\\partial y}\\right)\\tag{14}\n$$\nand\n$$\n\\left(\\frac{\\partial}{\\partial\\bar{z}}(u+iv)\\right)\\mathbf{j}\n=\\frac{\\mathbf{j}}{2}\\left(\\frac{\\partial u}{\\partial x}-\\frac{\\partial v}{\\partial y}\\right)+\\frac{\\mathbf{k}}{2}\\left(\\frac{\\partial v}{\\partial x}+\\frac{\\partial u}{\\partial y}\\right)\\tag{15}\n$$\nFinally, substituting $(10)$-$(13)$ into $(14)$ and $(15)$, it becomes apparent, upon comparison with $(6)$ and $(7)$, that $\\left(\\dfrac{\\partial}{\\partial z}(u+iv)\\right)\\mathbf{1}$ represents the conformal part of the Jacobian and $\\left(\\dfrac{\\partial}{\\partial\\bar{z}}(u+iv)\\right)\\mathbf{j}$ represents the conjugate conformal part.\nConclusion\nFor a general $f:\\mathbb{C}\\mapsto\\mathbb{C}$, $\\dfrac{\\partial}{\\partial z}f$ can be mapped to the conformal part of the $2\\times2$ Jacobian, $\\dfrac{\\partial f}{\\partial z}=\\dfrac{\\partial(u,v)}{\\partial(x,y)}$, and $\\dfrac{\\partial}{\\partial\\bar{z}}f$ can be mapped to the conjugate conformal part. It is merely convenience of notation that we write $\\dfrac{\\partial}{\\partial z}$ and $\\dfrac{\\partial}{\\partial\\bar{z}}$ because $\\dfrac{\\partial}{\\partial z}f\\;\\mathrm{d}z+\\dfrac{\\partial}{\\partial\\bar{z}}f\\;\\mathrm{d}\\bar{z}=\\mathrm{d}f$. However, they are not true partial derivatives, but $2$ pieces of a $2\\times2$ Jacobian composed of $4$ partial derivatives.\nSo, to answer the question asked, $z\\mapsto\\bar{z}$ is conjugate conformal, so $\\frac{\\partial}{\\partial z}\\bar{z}=0$; therefore, $\\bar{z}$ acts like a constant under $\\frac{\\partial}{\\partial z}$.", "meta": {"post_id": 85648, "input_score": 27, "output_score": 52, "post_title": "Why can the complex conjugate of a variable be treated as a constant when differentiating with respect to that variable?"}}
{"input": "This question is about asymptotic notation in general. For simplicity I will use examples about big-O notation for function growth as $n\\to\\infty$ (seen in algorithmic complexity), but the issues that arise are the same for things like $\\Omega$ and $\\Theta$ growth as $n\\to\\infty$, or for little-o as $x\\to 0$ (often seen in analysis), or any other combination.\nThe interaction between big-O notation and equals signs can be confusing. We write things like\n$$\\tag{1} 3n^2+4 = O(n^2)$$\n$$\\tag{2} 5n^2+7n = O(n^2)$$\nBut we're not allowed to conclude from these two statements that $3n^2+4=5n^2+7n$. Thus it seems that transitivity of equality fails when big-O is involved. Also, we never write things such as $$\\tag{3} O(n^2)=3n^2+4,$$\nso apparently commutativity is also at risk.\nMany textbooks point this out and declare, with varying degrees of indignation, that notations such as $(1)$ and $(2)$ constitute an \"abuse of notation\" that students are just going to have to get used to. Very well, but then what are the rules that govern this abuse? Mathematicians seem to be able to communicate using it, so it can't be completely random.\nOne simple way out is to define that the notation $O(n^2)$ properly denotes the set of functions that grow at most quadratically, and that equations like $(1)$ are just conventional abbreviations for\n$$\\tag{4} (n\\mapsto 3n^2+4)\\in O(n^2)$$\nSome authors even insist that writing $(1)$ is plain wrong, and that $(4)$ is the only correct way to express the estimate.\nHowever, other authors blithely write things like\n$$\\tag{5} 5 + O(n) + O(n^2)\\log(O(n^3)) = O(n^2\\log n)$$\nwhich does not seem to be easily interpretable in terms of sets of functions.\nThe question: How can we assign meaning to such statements in a principled way such that $(1)$, $(2)$ and $(4)$ are true but $(3)$ is not?", "output": "My take on this notation agrees with David Speyer's comment. As Henning's answer notes, our answers are just different ways to look at the same thing. Often I resort to rules similar to his as the operative definition, but I find the my approach easier to motivate. \nWe proceed in two steps: (a.) understanding an expression involving asymptotic notation, and (b.) understanding the use of the equals sign in this notation.\n\nRule for Interpreting Expressions. Every (well-formed) expression involving standard functions and operations (e..g, ${ +, \\times, -, \\div, \\exp, \\log }$) and asymptotic notations $\\{ O, o, \\Omega, \\omega, \\Theta\\}$ corresponds to a set of functions. [For simplicity, we will restrict ourselves to only the $O$ notation.] This set is built recursively exactly as one would expect:\n$$\r\n\\begin{align*}\r\nE_1 \\circ E_2 &:= \\{ f(n) \\circ g(n) \\quad : \\quad f(n) \\in E_1, g(n) = E_2 \\}, \\quad \\circ \\in \\{ +, \\times, -, \\div \\}; \\\\\r\n\\Psi (E) &:= \\{ \\Psi(f(n)) \\quad : \\quad f(n) \\in E \\}, \\quad \\Psi \\in \\{ \\exp, \\log \\}. \\\\\r\n\\end{align*}\r\n$$\nThis interpretation of expressions is perfectly natural; it's analogous to the Minkowski sum of two sets, extended for more general operations. We can even enrich this with a slightly more involved rule for $O$:\n\n$$\r\nO(E) := \\bigcup_{f(n) \\in E} O(f(n)).\r\n$$\n\nThese rules are not complete; one needs to append the appropriate base cases when $E$, $E_1$ and $E_2$ are single functions rather than a set of functions. The most interesting base case is the expression $O(f(n))$ which is defined exactly as in the post. As a final point, we can agree to identity a single function $f(n)$ with the set $\\{ f(n) \\}$; this turns out to be very convenient in practice. \nFor example, an expression of the form  $5 + O(n) + O(n^2)\\log(O(n^3))$ stands for the set\n$$\r\n\\{  5 + f(n) + g(n) \\log h(n) \\ : \\ f(n) \\in O(n), \\quad g(n) \\in O(n^2), \\quad h(n) \\in O(n^3)  \\}. \r\n$$\nSimilarly we can interpret the expression like $O(2^{n^2 + O(n)})$ by applying the $O(\\cdot)$ rule twice. \n\nRules for Interpreting the Equals sign. When one asserts that $E_1 = E_2$ where $E_1$ and $E_2$ are sets represented by expressions like the above, one should always interpret it to mean that $E_1 \\subseteq E_2$. E.g., $$5 + O(n) + O(n^2)\\log(O(n^3)) \\ \\color{Red}{=} \\ O(n^2\\log n)$$ is really a shorthand for $$5 + O(n) + O(n^2)\\log(O(n^3)) \\ \\color{Red}{\\subseteq} \\ O(n^2\\log n) .$$ \nIn addition, there is the case $f(n) = E$; e.g., $n = O(n^2)$. In this case, we can identity the function $f(n)$ with the set $\\{ f(n) \\}$ and apply the above interpretation. This tells us that $f(n) = E$ is the same as saying $f(n) \\in E$ (because $f(n) \\in E \\iff \\lbrace f(n) \\rbrace \\subseteq E$). \nThis is arguably unnatural since this clashes starkly with the standard interpretation of the equals sign as (set) equality. In fact, this is an inherent difficulty since we would expect any reasonable definition of equality to be symmetric: \n$$\r\nE_1 = E_2 \\quad \\stackrel{\\color{Red}{(??)}}{\\implies} \\quad E_2 = E_1.\r\n$$\nHowever, as we all know, this is seldom true with the asymptotic notation. Thus the only way out seems to be to override the intuitive notion of equality in favor of a less natural one.  [At this stage, it is worth pointing out that Henning's rules also resorts to redefining the equals sign suitably.] \n\nA sample of properties. Many commonly used rules for manipulating asymptotic notation follow directly from the above interpretation. As a case study, I consider some of the properties already studied by Henning in his answer: \n\n\"Local scope\" or Change of meaning. For example, In $O(n) + O(n) = O(n)$, you can substitute distinct functions $f(n)$, $g(n)$ and $h(n)$. The definition of \"$+$\" in the set-of-functions interpretation is clearly based on this idea. \nThe asymptotic notation is not symmetric. E.g., $O(n) = O(n^2)$ whereas $n^2 \\stackrel{\\color{Red}{?}}{=} O(n)$ is false. This is for the same reason that set inclusion is not.\nHowever it is transitive; i.e., if $E_1 = E_2$ and $E_2 = E_3$, then $E_1 = E_3$. This simply follows from the transitivity of set inclusion.\n\n\nFinal words. One main complaint against the asymptotic notation seems to be that it is usually not symmetric as is implied by the equals sign. This is legitimate, but in this case, the real issue is with the abuse of equals sign, rather than the functions-as-sets idea. In fact, as far as I remember, I am yet to come across a single use of the asymptotic notation that is technically wrong even after one mentally replaces the equals sign with either $\\in$ or $\\subseteq$ as appropriate.", "meta": {"post_id": 86076, "input_score": 59, "output_score": 34, "post_title": "What are the rules for equals signs with big-O and little-o?"}}
{"input": "In the hopes of improving my knowledge on the question, could someone outline the inputs and outputs for the 3-SAT problem? It would also be helpful if you could express how this problem differs in structure to SAT, 2-SAT or 4-SAT problems.", "output": "There are a bunch of teddy bears A, B, C, D and so on that are red on one side and blue on the other! (You choose how to color them)\nAND there are a bunch of 3 armed aliens with really long arms. Each alien grabs 3 teddy bear hands!  (A teddy bear hand can be grabbed by more than one alien.)\n3-SAT is the problem of whether you can color the teddy bears such that every alien is holding at least one blue hand!", "meta": {"post_id": 86210, "input_score": 47, "output_score": 69, "post_title": "What is the $3$-SAT problem?"}}
{"input": "While reading one of Keith Conrad's great blurbs, Linear Independence of Characters, there is a footnote at the bottom of page 2 saying\n\nIn general, the primitive $n$th roots of unity in the $n$th cyclotomic field form a normal basis over $\\mathbf{Q}$ if and only if $n$ is squarefree.\n\nA little bit of research didn't turn up any results, except apparently the result is in the paper K. Johnsen, Lineare Abh\u00e4ngigkeiten von Einheitswurzeln. Elem. Math. 40 (1985), 57\u201359 which I can't find anywhere. (J.M. found it here.)\nCould someone be kind enough to give a proof of why this statement is true? \n\nI made this attempt at the if direction. I try induction on $n$. If $n$ is prime, then the result follows from field theory, since the powers of $\\alpha$ form a $F$-basis of $F(\\alpha)$ over $F$. So let $n=mp$ where $m$ is coprime to $p$. If $n$ is squarefree, so is $m$, so by induction, the $m$th primitive roots of units form a basis for $\\mathbb{Q}(\\zeta_m)$ over $\\mathbb{Q}$. \nThen I think the primitive $p$th roots of unity form a basis of $\\mathbb{Q}(\\zeta_m,\\zeta_p)$ over $\\mathbb{Q}(\\zeta_m)$, so taking pairwise products of the two bases gives a basis of primitive $n$th roots of $\\mathbb{Q}(\\zeta_m,\\zeta_p)$ over $\\mathbb{Q}$? Does $\\mathbb{Q}(\\zeta_m,\\zeta_p)=\\mathbb{Q}(\\zeta_n)$? I know $\\mathbb{Q}(\\zeta_m,\\zeta_p)\\subseteq\\mathbb{Q}(\\zeta_n)$, but am not sure of the other containment, or if this argument is oversimplifying things.", "output": "In the comments to the question, Jyrki showed that if $n$ has a square factor bigger than $1$ then there are nontrivial ${\\mathbf Q}$-linear relations among $\\{\\zeta_n^a : (a,n) = 1\\}$, so this set can only be a ${\\mathbf Q}$-basis of ${\\mathbf Q}(\\zeta_n)$ when $n$ is squarefree. Another way to see this is that for all $n \\geq 1$, the sum of the primitive $n$th roots of unity is the Moebius value $\\mu(n)$, and this is $0$ if $n$ has a square factor bigger than $1$.\nTo show that when $n$ is squarefree the set $\\{\\zeta_n^a : (a,n)=1\\}$ is a ${\\mathbf Q}$-basis of ${\\mathbf Q}(\\zeta_n)$ we will use induction on the number of prime factors of $n$.\nSuppose first that $n = p$ is a prime. The usual ${\\mathbf Q}$-basis of ${\\mathbf Q}(\\zeta_p)$ is $\\{1,\\zeta_p,\\zeta_p^2,\\cdots,\\zeta_{p}^{p-2}\\}$. Since $\\zeta_p^{p-1} = -1-\\zeta_p - \\cdots - \\zeta_p^{p-2}$, if we replace $1$ with $\\zeta_p^{p-1}$ we still have a ${\\mathbf Q}$-basis, so $\\{\\zeta_p,\\zeta_p^2,\\cdots,\\zeta_p^{p-1}\\}$ is a basis of ${\\mathbf Q}(\\zeta_p)/{\\mathbf Q}$.\nNow suppose we've proved the result when $n$ is any product of $r$ primes and consider a squarefree positive integer $n$ that is a product of $r+1$ primes. Write $n = mp$ where $p$ is one of the prime factors of $n$, so we know\n$\\{\\zeta_m^i : 1 \\leq i \\leq m, (i,m) = 1\\}$ is a ${\\mathbf Q}$-basis of ${\\mathbf Q}(\\zeta_m)$ and $\\{\\zeta_p^{j} : 1 \\leq j \\leq p, (j,p) = 1\\}$ is a ${\\mathbf Q}$-basis of ${\\mathbf Q}(\\zeta_p)$. From Galois theory, if $E$ and $F$ are (finite) Galois extensions of a common field $L$ and $E \\cap F = L$ then as a basis of $EF$ over $L$ one can use the set of products $\\{e_if_j\\}$ where $\\{e_i\\}$ is any $L$-basis of $E$ and $\\{f_j\\}$ is any $L$-basis of $F$. We can apply this to the fields $E = {\\mathbf Q}(\\zeta_m)$, $F = {\\mathbf Q}(\\zeta_p)$, and $L = {\\mathbf Q}$. (That the intersection of those two cyclotomic fields is ${\\mathbf Q}$ is a special case of a general formula ${\\mathbf Q}(\\zeta_r) \\cap {\\mathbf Q}(\\zeta_s) = {\\mathbf Q}(\\zeta_{(r,s)})$, which becomes ${\\mathbf Q}$ when $r$ and $s$ are relatively prime.) Therefore a ${\\mathbf Q}$-basis of $EF = {\\mathbf Q}(\\zeta_m,\\zeta_p) = {\\mathbf Q}(\\zeta_n)$ is the set of products $\\{\\zeta_m^i\\zeta_p^j\\}$ with $i$ and $j$ running over the integers listed earlier. Inside ${\\mathbf Q}(\\zeta_n)$ we can use $\\zeta_m := \\zeta_n^{n/m}$ and $\\zeta_p := \\zeta_n^{n/p}$, so a basis is $\\{\\zeta_n^{(n/m)i + (n/p)j}\\} = \\{\\zeta_n^{pi + mj}\\}$ where $i$ runs over integers from 1 to $m$ which are relatively prime to $m$ and $j$ runs over integers from 1 to $p$ which are relatively prime to $p$ (that is, $1 \\leq j \\leq p-1$).\nWhich $n$th roots of unity are in the set $\\{\\zeta_n^{pi + mj}\\}$ and how many are there? The integers $pi+mj$ are all relatively prime to $n = mp$ (just reduce them mod $m$ and mod $p$ to check they are relatively prime to $m$ and $p$ separately), so the set consists of primitive $n$th roots of unity. If $pi + mj \\equiv pi' + mj' \\bmod n$ (where $i'$ and $j'$ are just second choices of parameters) then by reducing mod $m$ and mod $p$ we get $i \\equiv i' \\bmod m$ and $j \\equiv j' \\bmod p$, so $i = i'$ and $j = j'$ on account of the ranges of these parameters. Therefore the number of roots of unity in this set is $\\varphi(m)\\varphi(p) = \\varphi(n)$, so our set is exactly the set of all primitive $n$th roots of unity. This completes the proof that the primitive $n$th roots of unity are a ${\\mathbf Q}$-basis of ${\\mathbf Q}(\\zeta_n)$ when $n$ is squarefree.\nAlthough the question has now been answered, let me indicate a place where it naturally fits into a broader picture within algebraic number theory.\nThe result is related to a theorem of Emmy Noether on normal integral bases.\nFor any (finite) Galois extension $K/{\\mathbf Q}$, a normal integral basis is a normal basis for the field extension which consists of a ${\\mathbf Z}$-basis of ${\\mathcal O}_K$. For example, ${\\mathbf Q}(\\sqrt{5})/{\\mathbf Q}$ has normal integral basis $\\{(1+\\sqrt{5})/2,(1-\\sqrt{5})/2\\}$. Another example is ${\\mathbf Q}(\\zeta_p)$ for any odd prime $p$: the ring of integers is ${\\mathbf Z}[\\zeta_p]$ and the usual ${\\mathbf Z}$-basis you may want to use is $\\{1,\\zeta_p,\\cdots,\\zeta_p^{p-2}\\}$, but that's not a normal basis because it has 1 in it. Instead you can use $\\{\\zeta_p,\\zeta_p^2,\\cdots,\\zeta_p^{p-1}\\}$; that is a normal basis and it's also a ${\\mathbf Z}$-basis of ${\\mathbf Z}[\\zeta_p]$, so ${\\mathbf Q}(\\zeta_p)/{\\mathbf Q}$ has a normal integral basis.\nHere's an example without a normal integral basis. If $d$ is squarefree and not $1 \\bmod 4$, the ring of integers of ${\\mathbf Q}(\\sqrt{d})$ is ${\\mathbf Z}[\\sqrt{d}]$. A normal basis over ${\\mathbf Q}$ has the form $\\{a+b\\sqrt{d},a-b\\sqrt{d}\\}$ for rational $a$ and $b$ with $b \\not= 0$. If this basis is in ${\\mathbf Z}[\\sqrt{d}]$ then it can't be a ${\\mathbf Z}$-basis of ${\\mathbf Z}[\\sqrt{d}]$ because ${\\mathbf Z}(a+b\\sqrt{d}) + {\\mathbf Z}(a-b\\sqrt{d})$ has index $2|ab| \\geq 2$ inside ${\\mathbf Z}[\\sqrt{d}]$.\nThe ring of integers of ${\\mathbf Q}(\\zeta_n)$ is ${\\mathbf Z}[\\zeta_n]$. Check that if $\\{\\zeta_n^a : (a,n) = 1\\}$ were a basis of ${\\mathbf Q}(\\zeta_n)/{\\mathbf Q}$ then it would be a ${\\mathbf Z}$-basis of ${\\mathbf Z}[\\zeta_n]$, hence the $n$-th cyclotomic field would have a normal integral basis.\nEmmy Noether proved that if a Galois extension $K/{\\mathbf Q}$ has a normal integral basis then $K$ is tamely ramified over ${\\mathbf Q}$. So being tamely ramified is a necessary (although generally not sufficient) condition for a Galois extension of ${\\mathbf Q}$ to have a normal integral basis. For example, if $d$ is squarefree and $d \\not\\equiv 1 \\bmod 4$ then 2 is not tamely ramified in ${\\mathbf Q}(\\sqrt{d})$, so Noether's theorem implies that this quadratic field has no normal integral basis over ${\\mathbf Q}$, which we already proved by a direct computation above. An example more relevant for us here is ${\\mathbf Q}(\\zeta_{p^2})$. The prime $p$ is not tamely ramified in this field, so any cyclotomic field ${\\mathbf Q}(\\zeta_n)$ with $n$ divisible by the square of a prime is not tamely ramified at that prime, hence it doesn't have a normal integral basis. Therefore a necessary condition for $\\{\\zeta_n^a : (a,n) = 1\\}$ to be a normal basis of the $n$-th cyclotomic field is that $n$ is squarefree.  Noether's theorem has provided a conceptual explanation for why $n$ must be squarefree.\nI suggest looking at Robert Long's book \"Algebraic Number Theory\" for more information on tamely ramified extensions of ${\\mathbf Q}$ and the relation to normal integral bases. I don't have the book in front of me right now and Google Books is not giving good views of it, but I'm pretty sure he has a chapter on this topic in it since Galois module structure was one of his areas of interest.", "meta": {"post_id": 87290, "input_score": 31, "output_score": 34, "post_title": "Basis of primitive nth Roots in a Cyclotomic Extension?"}}
{"input": "I am using quaternions to represent orientation as a rotational offset from a global coordinate frame.\nIs it correct in thinking that quaternion distance gives a metric that defines the closeness of two orientations? i.e. similar orientations give low distances, and dissimilar orientations give high distances.\nDoes zero distance mean that the orientations are exactly the same?\nThis seems obvious, but I want to ensure that there are no subtleties that get in the way of logic.", "output": "First, I assume that you're using unit quaternions, i.e. quaternions $a+b\\,\\textbf{i}+c\\,\\textbf{j}+d\\,\\textbf{k}$ that satisfy $a^2 + b^2 + c^2 + d^2 = 1$.  If not, you'll want to scale your quaternions before computing distance.\nDistance between quaternions will correspond roughly to distance between orientations as long as the quaternions are fairly close to each other.  However, if you're comparing quaternions globally, you should remember that $q$ and $-q$ always represent the same orientation, even though the distance between them is $2$.\nThere are better ways to compute the closeness of two orientations that avoid this problem.  For example, the angle $\\theta$ of rotation required to get from one orientation to another is given by the formula\n$$\r\n\\theta \\;=\\; \\cos^{-1}\\bigl(2\\langle q_1,q_2\\rangle^2 -1\\bigr)\r\n$$\nwhere $\\langle q_1,q_2\\rangle$ denotes the inner product of the corresponding quaternions:\n$$\r\n\\langle a_1 +b_1 \\textbf{i} + c_1 \\textbf{j} + d_1 \\textbf{k},\\; a_2 + b_2 \\textbf{i} + c_2 \\textbf{j} + d_2 \\textbf{k}\\rangle \\;=\\; a_1a_2 + b_1b_2 + c_1 c_2 + d_1d_2.\r\n$$\n(This formula follows from the double-angle formula for cosine, together with the fact that the angle between orientations is precisely twice the angle between unit quaternions.)\nIf you want a notion of distance that can be computed without trig functions, the quantity\n$$\r\nd(q_1,q_2) \\;=\\; 1 - \\langle q_1,q_2\\rangle^2\r\n$$\nis equal to $(1-\\cos\\theta)/2$, and gives a rough estimate of the distance.  In particular, it gives $0$ whenever the quaternions represent the same orientation, and it gives $1$ whenever the two orientations are $180^\\circ$ apart.", "meta": {"post_id": 90081, "input_score": 41, "output_score": 69, "post_title": "Quaternion distance"}}
{"input": "There appears to be an interesting pattern in the decimal expansion of $\\dfrac1{243}$:\n\n$$\\frac1{243}=0.\\overline{004115226337448559670781893}$$\n\nI was wondering if anyone could clarify how this comes about?", "output": "$\\frac{1}{243}=\\frac{1}{333}+\\frac{10}{8991}$\n$\\frac{1}{333}=.\\overline{003}$\n$\\frac{1}{8991}=.\\overline{000111222333444555666777889}=\\frac{111}{998001}=\\frac{111}{10^6-2\\cdot10^3+1}$", "meta": {"post_id": 90690, "input_score": 13, "output_score": 36, "post_title": "Interesting pattern in the decimal expansion of $\\frac1{243}$"}}
{"input": "I am quite confused about the meaning of shadow price from explanations on the internet.\nIt can be understood as the value of a change in revenue if the constraint is relaxed, or how much you would be willing to pay for an additional resource.\nFor example:\n$$\\begin{array}{ll} \\text{maximize} & 5 x_1 + 4 x_2 + 6 x_3\\\\ \\text{subject to} & 6 x_1 + 5 x_2 + 8 x_ 3 \\leq 16 \\quad\\quad\\quad\\quad\\,\\,{(c_1)}\\\\ & 10 x_1 + 20 x_2 + 10 x_3 \\leq 35 \\quad\\quad\\quad{(c_2)}\\\\ & 0 \\leq x_1, x_2, x_3 \\leq 1\\\\\\end{array}$$\nSolving this problem, we get the shadow price of $c_1 = 0.727273$, $c_2 = 0.018182$.\nComparing $c_1$ and $c_2$, if one constraint can be relaxed, we should relax $c_1$ instead of $c_2$?", "output": "Here's perhaps a better way to think of the shadow price.  (I don't like the word \"relax\" here; I think it's confusing.)\nFor maximization problems like this one the constraints can often be thought of as restrictions on the amount of resources available, and the objective can be thought of as profit.  Then the shadow price associated with a particular constraint tells you how much the optimal value of the objective would increase per unit increase in the amount of resources available.  In other words, the shadow price associated with a resource tells you how much more profit you would get by increasing the amount of that resource by one unit.  (So \"How much you would be willing to pay for an additional resource\" is a good way of thinking about the shadow price.)  \nIn the example you give, there are 16 units available of the first resource and 35 units available of the second resource.  The fact that the shadow price of $c_1$ is 0.727273 means that if you could increase the first resource from 16 units to 17 units, you would get an additional profit of about \\$0.73.  Similarly, if you could increase the second resource from 35 units to 36 units then you would get an additional profit of about \\$0.02.  \nSo if you could increase just one resource by one unit, and the cost of increasing the first resource is the same as that of increasing the second resource (this assumption is not part of the model), then, yes, you should definitely increase the first resource by one unit.", "meta": {"post_id": 91504, "input_score": 40, "output_score": 67, "post_title": "Shadow prices in linear programming"}}
{"input": "Please correct any mistakes in this proof and, if you're feeling inclined, please provide a better one where \"better\" is defined by whatever criteria you prefer. \n\nAssume $2^{1/2}$ is irrational. \n$2^{1/3} * 2^{x} = 2^{1/2} \\Rightarrow x = 1/6$. \n$2^{1/3} * {2^{1/2}}^{1/3} = 2^{1/2}$.\nif $2^{1/2}$ is irrational, then ${2^{1/2}}^{1/3}$ is irrational. \n$2^{1/3} = 2^{1/2} / {2^{1/2}}^{1/3}$. \n$2^{1/3}$ equals an irrational number divided by an irrational number. \n$2^{1/3}$ is an irrational number.", "output": "I can't resist:  Suppose $2^{\\frac{1}{3}}=\\frac{n}{m}$.  Then $$2m^3=n^3,$$ or in other words $$m^3+m^3=n^3.$$  But this contradicts Fermats Last Theorem.", "meta": {"post_id": 91538, "input_score": 37, "output_score": 88, "post_title": "Prove $2^{1/3}$ is irrational."}}
{"input": "$R$ is a domain with characteristic $p$ ($p$ is prime). There is a homomorphism $f : R \\to R$, $f(a)=a^p$. $f$ is called the Frobenius endomorphism. And I have known this. \nWhen $R$ which is mentioned above is also a field, it is said that $f$ is an isomorphism. I think I just need to ensure $f$ is bijective. But I don't know how to prove it's surjective.\n\nActually I have got your ideas. In some of your opinions, if it is finite, then it can be surjective. Then what if it is infinite?\nIn my question, $R$ is a field as well as a domain. Then I wonder whether the \"domain\" is helpful for the proof of surjection or not.", "output": "All fields are domains; being a domain is not some extra property that a field may or may not have. \nThe Frobenius homomorphism is often called the Frobenius endomorphism, since \"endomorphism\" is a more specific term meaning \"map from something to itself\". However, I'll use \"homomorphism\" here to avoid confusion.\nLet $K=\\mathbb{F}_p(T)$, the rational functions in the variable $T$ over the field $\\mathbb{F}_p=\\mathbb{Z}/p\\mathbb{Z}$. That is, \n$$K=\\mathbb{F}_p(T)=\\left\\{\\,\\frac{f}{g}\\,\\Bigg|\\,\\,\\,f,g\\in\\mathbb{F}_p[T], g\\neq0\\right\\}.$$\nThen the Frobenius homomorphism $\\phi:K\\to K$ is not surjective, because (for instance) the element $T$ is not in the image of $\\phi$. This is because if we had $\\phi(\\frac{f}{g})=\\frac{f^{\\;p}}{g^{\\;p}}=T$ for some $\\frac{f}{g}\\in K$, then we have $p(\\deg(f)-\\deg(g))=\\deg(T)=1$, which is impossible as $\\deg(f)-\\deg(g)$ is an integer and $1$ is not divisible by $p$.\nNow let $K=\\mathbb{F}_p$. Then the Frobenius homomorphism $\\phi:K\\to K$ is surjective, because it is in fact equal to the identity map, i.e. $\\phi(x)=x$ for all $x\\in K$. More generally, if $K$ is any finite field of characteristic $p$, then the Frobenius homomorphism is surjective, because (as I will show below) it is always injective, and an injective function from a finite set to itself must be surjective.\nHowever, it is not necessary that $K$ be finite in order for the Frobenius homomorphism to be surjective. For example, now let $K=\\mathbb{F}_p(T^{\\;1/p^\\infty})$. That is,\n$$K=\\mathbb{F}_p(T^{\\;1/p^\\infty})=\\mathbb{F}_p(T,\\sqrt[p]{T\\;},\\sqrt[p^2]{T\\;},\\ldots).$$ This is certainly an infinite field. The Frobenius homomorphism $\\phi:K\\to K$ is surjective. For example, the element $\\alpha\\in K$,\n$$\\alpha=\\frac{\\sqrt[p]{T\\;}+2\\cdot(\\sqrt[p^2]{T\\;})^3}{T^{\\;2}-5T^{\\;p}}$$\nis in the image of $\\phi$, because we can replace every occurrence of a $T^{\\;p^d}$ with a one-lower-power-of-$p$ exponent, i.e.\n$$\\phi(\\beta)=\\alpha,\\text{ where }\\quad\\beta=\\frac{\\sqrt[p^2]{T\\;}+2\\cdot(\\sqrt[p^3]{T\\;})^3}{(\\sqrt[p]{T\\;})^{\\;2}-5T}$$\nSimilarly with any other element of $K$. (Note that $2$ or $5$ could very well be equal to $p$, and hence equal to $0$.)\nIncidentally, the field $\\mathbb{F}_p(T^{\\;1/p^\\infty})$ is called the perfection of the field $\\mathbb{F}_p(T)$. It is a theorem that a field $K$ of characteristic $p$ is perfect if and only if the Frobenius homomorphism $\\phi:K \\to K$ is surjective, and by adding in $p^n$-th roots of every element of $\\mathbb{F}_p(T)$, we have made a field for which it must be surjective, hence the name.\n\nNow, some books (for example, McCarthy's Algebraic Extensions of Fields) use the term \"isomorphism\" to mean what we now call \"monomorphism\" (injective homomorphism), and when they want to express what we now call \"isomorphism\" (bijective homomorphism), they would say \"onto isomorphism\" (since onto is a synonym for surjective, and bijective = injective + surjective). \nIf for some reason you are using this older terminology, then the Frobenius homomorphism for a field is always an \"isomorphism\" (i.e. injective). This is because, if $K$ is any field of characteristic $p$, and $\\phi:K\\to K$ is the Frobenius homomorphism, then\n$$\\begin{align*}\\phi(\\alpha)=\\alpha^p=\\beta^p=\\phi(\\beta)&\\implies\\alpha^p-\\beta^p=0\\\\ &\\implies(\\alpha-\\beta)^p=0\\\\ & \\implies\\alpha-\\beta=0\\\\ & \\implies\\alpha=\\beta.\\end{align*}$$\nIn fact, any homomorphism from a field $K$ to a non-zero ring $R$ must be injective (what could its kernel be, as an ideal of the field $K$?) Also note that, by the First Isomorphism Theorem for Rings, we therefore have that any homomorphism $f:K\\to R$ from a field $K$ to a non-zero ring $R$ is an isomorphism (in the modern sense) onto the subring of $R$ that is the image of $f$, i.e. if we threw out the rest of $R$ and only looked at the subring $f(K)$, then $f:K\\to f(K)$ is an isomorphism (in the modern sense). This is what Jyrki Lahtonen conjectured that you meant above.\nIf we look at things that are not domains (and so in particular are not fields), then the Frobenius homomorphism need not be injective. For example, let $R=\\mathbb{F}_p[x]/(x^p)$. That is,\n$$R=\\mathbb{F}_p[x]/(x^p)=\\{a_0+\\cdots+a_{p-1}x^{p-1}+(x^p)\\mid a_i\\in\\mathbb{F}_p\\}.$$\nThen if $\\phi:R\\to R$ is the Frobenius homomorphism, we have for example $x+(x^p)\\neq0+(x^p)$, but $$\\phi(x+(x^p))=x^p+(x^p)=0+(x^p)=0^p+(x^p)=\\phi(0+(x^p)).$$\n\nOn a final note, I just want to emphasize that the concept of a Frobenius homomorphism is only applicable if the field is of characteristic $p$ (as all the examples above were). For example, $\\mathbb{Q}$ is a field of characteristic $0$, and for any prime $p$, the function $f:\\mathbb{Q}\\to\\mathbb{Q}$ defined by $f(a)=a^p$ is not a homomorphism (compare $f(2)$ with $2\\cdot f(1)$).", "meta": {"post_id": 91688, "input_score": 23, "output_score": 54, "post_title": "How to prove that the Frobenius endomorphism is surjective?"}}
{"input": "Let $f$ be a continuous, even function over some interval $I=[-a,a]$ such that the total arc length of $f$ over $I$ is at least $2$, $f(0)=0$, and $f$ is increasing on $(0,a)$.  [You might imagine something like $f(x)=x^2$.] View the graph of $z=f(x)$ as a surface in $\\mathbb{R}^3$ (so, parallel to the $y$-axis).\nModel a corn tortilla (or pita bread if you prefer) as a pliable disk of radius $1$, place it in the $xy$-plane with its center at the origin, and wrap the tortilla up against the surface $z=f(x)$. [It should now be clear why there was the arc length condition above.]\nAt this point, we have something like a taco shell.  Now take its convex hull and consider its volume $V$.\nQuestion:\nCan we determine how to maximize $V$ with respect to all such functions $f$?  Because, you know, I want my taco to be the biggest.\nElementary candidates for $f$ include a semicircle function, a parabola, an absolute value function, a cosine wave, etc.  But if there is a unique answer, it might not be an elementary curve at all.\n\nEdit: This probably shouldn't have been asked in a way that relies on a function $f$.  Maybe the biggest taco has regions where the shell is curved completely vertical, and we no longer have the graph of a continuous function.  If you prefer, consider curves parametrized by arc length $$\\left(X(t),0,Z(t)\\right):[0,1]\\to xz\\mbox{-plane}$$ with $X(0)=Z(0)=0$, and $X$ and $Z$ (not necessarily strictly) increasing.  (And of course then reflect that curve through the $yz$-plane and cross it with the $y$-axis to get the entire surface.)", "output": "Here's your optimal taco (with vegetarian filling):\n\nI'd call it a Jordan curve, but that name is unfortunately already taken, so how about \"tacoid\"?\nRather appropriately, we can calculate the volume of the taco using something similar to the method of shells. Let $s$ be the arc length parameter along the curve, and consider the part of the taco volume between the two surface elements that correspond to an arc length element $\\mathrm ds$. The distance between them along the $x$ axis is $2x$, their length along the $y$ axis is $\\sqrt{1-s^2}$, and their height along the $z$ axis is $\\mathrm dz=\\dot z\\mathrm ds$, where here and in the following a dot denotes differentiation with respect to the arc length.\nThus the taco volume is \n$$V=\\int_0^12\\sqrt{1-s^2}x\\dot z\\mathrm ds\\;.$$\nWe can eliminate $\\dot z$ from $\\dot x^2+\\dot z^2=1$, which gives $\\dot x=\\sqrt{1-\\dot z^2}$, and thus\n$$V=\\int_0^12\\sqrt{1-s^2}x\\sqrt{1-\\dot x^2}\\mathrm ds=:\\int_0^1\\mathcal L(s,x,\\dot x)\\mathrm ds\\;.$$\nWe can maximize this using the calculus of variations. The Euler\u2013Lagrange equation is\n$$\\frac{\\mathrm d}{\\mathrm ds}\\frac{\\partial\\mathcal L}{\\partial \\dot x}=\\frac{\\partial\\mathcal L}{\\partial x}\\;,$$\nwhich yields\n$$\\frac{\\mathrm d}{\\mathrm ds}\\left(-\\sqrt{1-s^2}x\\frac{\\dot x}{\\sqrt{1-\\dot x^2}}\\right)=\\sqrt{1-s^2}\\sqrt{1-\\dot x^2}\\;,$$\nwhich we can simplify to\n$$x\\ddot x+1-\\dot x^2=x\\dot x(1-\\dot x^2)\\frac{s}{1-s^2}\\;.$$\nNote that the left-hand side is what we'd usually get for a circle if we didn't have the factor $\\sqrt{1-s^2}$ from the shape of the taco; a rectangular taco wouldn't have this term and would thus take the form of a semi-cylinder, as Will noted.\nNone of the candidates solves this equation; I doubt that it can be solved analytically. I solved it numerically using RK4. This was a bit tricky because there are singularities both at $x=0$ (horizontal tangent) and at $s=1$ (vertical tangent).\nSince we have an initial condition at $s=0$ and a fixed arc length parameter value for the endpoint, $s=1$, we'd like to integrate the equation from $s=0$ towards $s=1$; then we could vary the remaining initial condition to get the optimal volume. However, the problem is that the singularity at $x=0$ forces the tangent to be horizontal there, so we can't freely specify a first derivative there.\nThe situation at $x=0$, where $s=0$, is basically the same as for the semi-circle we get without the $\\sqrt{1-s^2}$ term, so we can try to get some guidance from there. The solution in that case is\n$$\\begin{array}{ccccc}\nx=r\\sin\\frac sr&&\\dot x=\\cos\\frac sr&&\\ddot x=-\\frac1r\\sin\\frac sr\\\\\nz=r(1-\\cos\\frac sr)&&\\dot z=\\sin\\frac sr&&\\ddot z=\\frac1r\\cos\\frac sr\\\\\n\\end{array}\n$$\nwith a free parameter $r$, the radius of the semi-circle.\nWe can see from this solution that we also can't freely specify $\\ddot x$ at $x=0$, since its value $0$ is independent of the curvature, as is $\\dot z=0$. However, $\\ddot z=1/r$ is the curvature, which we can choose freely. Thus, we transform from $x$ to $z$ using\n$$\\dot x=\\sqrt{1-\\dot z^2}$$\nand\n$$\\ddot x=-\\frac{\\dot z\\ddot z}{\\sqrt{1-\\dot z^2}}$$\nto obtain\n$$\\ddot z=\\frac{\\dot x\\dot z}x-\\dot x^2\\dot z\\frac s{1-s^2}\\;.$$\nNow we can circumvent the singularity by calculating $\\ddot z$ from the differential equation for $x\\ne0$ and freely choosing a value for it at $x=0$, which will determine the taco's curvature. We can then vary this parameter to maximize the volume.\nIt turns out that at small initial curvatures, the graph is at first roughly circular (near $s=0$) but then turns and ends in a horizontal tangent. This makes sense if you look at the signs of $\\ddot x$ and $\\ddot z$: The additional term from the $\\sqrt{1-s^2}$ factor increases $\\ddot x$ and decreases $\\ddot z$, so if it dominates at $s=1$, it causes the tangent to be horizontal. (The tangent has to be either horizontal or vertical at $s=1$ to counter the pole in the $s$-dependent term.)\nHowever, as the initial curvature is increased, eventually the tangent becomes vertical before $s=1$ is reached. The mechanism for this is similar as in the circular case, and the $s$-dependent term only causes a quantitative correction in this case. It turns out that, as in the circular case, the volume is maximal precisely at the curvature that makes the tangent at $s=1$ come out vertical. To avoid problems from this singularity, I switched from the equation for $\\ddot z$ to the equation for $\\ddot x$ at $s=1/2$, so that at either singularity I was effectively dealing with a \"horizontal\" tangent and finite curvature.\nIt turns out that the correction due to the $s$-dependent term is actually quite small. In practical terms, it's probably not worth making the effort to optimize the form, since the cylindrical form is almost as good. The optimal volume is about $0.415015$ (with the curvature at $x=0$ about $2.0328$), whereas if you wrap the disk around the cylinder with radius $2/\\pi$ that solves the rectangular version, the volume is $1/2-J_1(\\pi)/\\pi\\approx0.409404$ (according to Wolfram|Alpha).\nHere's a comparison of the tacoid and the semi-circle:\n\nRed is the tacoid, green is the semi-circle. The tacoid rises more rapidly, since it doesn't want to waste arc length on first getting further out, since it can't profit from that as much later on when the $\\sqrt{1-s^2}$ factor kicks in. The tacoid would be a bit worse than the semi-circle if the taco were rectangular (only $0.618983$ instead of the optimal volume $2/\\pi\\approx0.636620$ of the semi-cylinder). To illustrate how this changes due to the $\\sqrt{1-s^2}$ factor, I weighted the $x$ values by that factor. Blue shows the weighted tacoid, and yellow shows the weighted semi-circle. The taco's gain near the top is slightly bigger than the loss near the bottom.\nWe can also compare with a circular graph optimized for a disk-shaped taco. The optimal value of the radius in that case is about $0.899736\\cdot2/\\pi\\approx0.572790$, and the corresponding cylindrical volume is about $0.413570$, which is less than half a percent below the optimum, so in calculating cylindrical approximations your students were coming quite close to the optimal result.\nWe can draw the same graphs as above to compare the tacoid and the reoptimized cylinder,\n\nbut perhaps more interesting is a comparison of the enclosed volume plotted against the arc length,\n\nwhich shows that these two solutions make use of their arc length in a surprisingly similar manner, with the tacoid (red) only very slightly more greedy than the cylinder (green), and even that small difference almost evening out at the end.\nSince the reoptimized cylinder has the added advantage of bending back inwards slightly to improve your grip on the contents, this raises the question whether it might be preferable in practical terms to forgo the half-percent optimization and go for a simple cylindrical form with optimal radius.", "meta": {"post_id": 91937, "input_score": 100, "output_score": 123, "post_title": "What's the largest possible volume of a taco, and how do I make one that big?"}}
{"input": "I am trying to write a program that will perform OCR on a mobile phone, and I recently encountered this article : \n\n\n\nCan someone explain this to me ?", "output": "J.M. has given a very good answer explaining singular values and how they're used in low rank approximations of images. However, a few pictures always go a long way in appreciating and understanding these concepts. Here is an example from one of my presentations from I don't know when, but is exactly what you need. \nConsider the following grayscale image of a hummingbird (left). The resulting image is a $648\\times 600$ image of MATLAB doubles, which takes $648\\times 600\\times 8=3110400$ bytes. \n\n\nNow taking an SVD of the above image gives us $600$ singular values that, when plotted, look like the curve on the right. Note that the $y$-axis is in decibels (i.e., $10\\log_{10}(s.v.)$). \nYou can clearly see that after about the first $20-25$ singular values, it falls off and the bulk of it is so low, that any information it contains is negligible (and most likely noise). So the question is, why store all this information if it is useless?\nLet's look at what information is actually contained in the different singular values. The figure on the left below shows the image recreated from the first 10 singular values ($l=10$ in J.M.'s answer). We see that the essence of the picture is basically captured in just 10 singular values out of a total of 600. Increasing this to the first 50 singular values shows that the picture is almost exactly reproduced (to the human eye). \nSo if you were to save just the first 50 singular values and the associated left/right singular vectors, you'd need to store only $(648\\times 50 + 50 + 50\\times 600)\\times 8=499600$ bytes, which is only about 16% of the original! (I'm sure you could've gotten a good representation with about 30, but I chose 50 arbitrarily for some reason back then, and we'll go with that.) \n\n\nSo what exactly do the smaller singular values contain? Looking at the next 100 singular values (figure on the left), we actually see some fine structure, especially the fine details around the feathers, etc., which are generally indistinguishable to the naked eye. It's probably very hard to see from the figure below, but you certainly can in this larger image. \n\n\nThe smallest 300 singular values (figure on the right) are complete junk and convey no information. These are most likely due to sensor noise from the camera's CMOS.", "meta": {"post_id": 92171, "input_score": 61, "output_score": 82, "post_title": "What do eigenvalues have to do with pictures?"}}
{"input": "An undergraduate was telling me about a puzzle he'd found: the idea was to make $2011$ out of the numbers $1, 2, 3, 4, \\ldots, n$ with the following rules/constraints: the numbers must stay in order, and you can only use $+$, $-$, $\\times$, $/$, ^ and $!$. In words, \"plus minus times divide, exponentiation and factorial\". The game was to construct $2011$ with $n$ as small as possible.\nTo my amazement, he had managed to do $n=5$: indeed\n$$((1+2)!)!+(3!)^4-5=2011.$$\nHe now wanted to solve the puzzle completely by proving that $n=1,2,3,4$ are impossible.\nSo there's where it gets interesting. My first thought was \"computer search -- done\". But it's not as easy as that, because factorial is only a unary operator. For example one has to rule out any possible amazing cancellations between very large numbers, e.g. one has to check\n$$(1+2)!!!!!!!!!!!!!-3^{(4!!!!!!!!!!!)} \\neq  2011.$$\nThis one in particular is not hard to rule out, because, for example, the left hand side is a multiple of $3$ and the right hand side isn't. In general, this approach can perhaps be used to bound the number of factorials that can occur in any presentation of $2011$ using $1, 2, 3, 4$ only -- but making this rigorous seemed a bit delicate and I wondered if I was missing something. Anyone any ideas?\nEDIT: Ron Maimon's heroic attempt to deal with the problem by brute force has led him to the interesting case of $2011=(1+2)!!!...!!!/(3!!!...!!!/4!!!!...!!!)$, which seems to be tougher to rule out than the others: the game here is to prove that $2011$ cannot be written as $3!!!...!!!*4!!!!...!!!/3!!!!...!!!$ for any choices of numbers of factorials.\nEDIT: Size considerations seem to deal with the above case. Ultimately it seems that the question I asked can be answered using a rather lengthy, but finite(!), procedure. Thanks Ron for your efforts.", "output": "If square roots and percentages (and unary minus) are permitted, then there are a number of solutions, the simplest probably being\n$$2011 = (-1 + (2 + 3!)!\\%) \\div \\sqrt{4\\%}$$\nMany years ago, I wrote a simple Windows program called $200$ Up (link to Dropbox folder) for solving problems like these:\n$\\hspace{1.75in}$ \nThe domain of calculation is $\\mathbb{Z}/d$ for some specified denominator $d\\in\\mathbb{N}$ (and with limits on the magnitude of the numerator).\nNote that, in the image above, the solution for 2018 is\n$$\n\\large 2018 \\;\\;=\\;\\; \\big( \\sqrt[\\sqrt{\\cdot \\dot{1}}]{\\Large 2} \\; + \\; 3!!\\%\\% \\big) \\; \\div \\; \\cdot 4\\%.\n$$", "meta": {"post_id": 92230, "input_score": 226, "output_score": 50, "post_title": "Proving you *can't* make $2011$ out of $1,2,3,4$: nice twist on the usual"}}
{"input": "For the small values of n I have been able to check, it seems that for $n>3$, there exist whole numbers $x,y$ s.t. $n! = x^2 - y^2$. For example ..\n$4! = 5^2 - 1^2$\n$5! = 11^2 - 1^2$\n$6! = 27^2 - 3^2$\n$7! = 71^2 - 1^2$\n$8! = 201^2 - 9^2$\n$9! = 603^2 - 27^2$\n$10! = 1905^2 - 15^2$\n$11! = 6318^2 - 18^2$\n$12! = 21888^2 - 288^2$\nIn most of the cases above, the $x$ value is just the next integer larger than $\\sqrt{n!}$, though at $n=12$ and $n=17$ it's the one following that. With the tools at hand I've only been able to check this as far as $n=17$. \nI expect there's probably already a name for this, but not knowing that name, googling was coming up dry.", "output": "If $n >3$, then $n!$ is divisible by $4$.\nSo $n!=4k=(2)(2k)$ for some integer $k$.\nNote now that \n$$4k=(k+1)^2-(k-1)^2.$$\nIf $n$ is large, there are many representations of $n!$ as a difference of two squares.  For let $2a$ and $2b$ be any two even numbers whose product is $n!$. Then\n$$n!=4ab=(a+b)^2-(a-b)^2.$$\nComment: Let $a$ be an odd integer. Then $a+1$ and $a-1$ are even, and therefore $(a+1)/2$ and $(a-1)/2$ are integers. We have\n$$a=\\left(\\frac{a+1}{2}\\right)^2-\\left(\\frac{a-1}{2}\\right)^2,$$\nso $a$ is a difference of two squares.\nIf $a$ is divisible by $4$, the argument we gave above shows that $a$ is a difference of two squares.\nIf $a$ is even but not divisible by $4$, then $a$ is not a difference of two squares. For a difference of two even squares is divisible by $4$, and a difference of two odd squares is divisible by $8$.", "meta": {"post_id": 92297, "input_score": 22, "output_score": 37, "post_title": "are all $n!$ ($n>3$) the difference of two squares?"}}
{"input": "Given some Abelian group $(G, +)$, does there always exist a binary operation $*$ such that $(G, +, *)$ is a ring? That is, $*$ is associative and distributive:\n\\begin{align*}\n&a * (b * c) = (a*b) * c \\\\\n&a * (b + c) = a * b + a * c \\\\\n&(a + b) * c = a * c + b * c \\\\\n\\end{align*}\nWe also might have multiplicative identity $1 \\in G$, with $a * 1 = 1 * a = a$ for any $a \\in G$. Multiplication may or may not be commutative.\nDepending on the definition, the answer could be no in the case of the group with one element: then $1 = 0$. But the trivial ring is not a very interesting case. For cyclic groups the statement is certainly true, since $(\\mathbb{Z}_n, +, \\cdot)$ and $(\\mathbb{Z}, +, \\cdot)$ are both rings. What about in general? Is there some procedure to give arbitrary abelian groups ring structure?", "output": "If your group has the property that every element has finite order, but there is no upper bound on the orders of the elements, then it is not the additive abelian group of a ring with identity.  The reason is that if there were such a ring structure with an identity $1$, then $1$ would have finite additive order $k$, and then for all $a$ in your group, $k\\cdot a=(k\\cdot1)a=0a=0$, which forces $a$ to have order at most $k$.  \nFor each prime $p$, the Pr\u00fcfer $p$-group $\\mathbb Z(p^\\infty)$ is an example of such a group.  The quotient group $\\mathbb Q/\\mathbb Z$ is another.  Direct sums (but not direct products) of infinitely many finite cyclic groups of unbounded order would also be examples.", "meta": {"post_id": 93409, "input_score": 83, "output_score": 96, "post_title": "Does every Abelian group admit a ring structure?"}}
{"input": "It is obvious that $\\mathbb{Q}_r$ is topologically isomorphic to $\\mathbb Q_s$ while $r$ and $s$ denote different primes. But I really don't know whether it is true in the aspect of algebra. As I failed to prove it, I think that it is false, but I can't give a counterexample.\nLast I'm quite sorry that I'm new to MathJax and I don't know how to use it properly.Thanks for reading and I would appreciate it if you could solve my problem.", "output": "Never. Looking at the number of roots of unity in your field suffices to distinguish all ${\\mathbb{Q}}_p$ for odd values of $p$, because the number of roots of $1$ there is precisely $p-1$. It's different for the $2$-adic numbers, since they have two roots of unity, same as the $3$-adics. But the $2$-adics have a square root of $-7$ and the $3$-adics don't, whereas the $3$-adics have a square root of $10$ and the $2$-adics don't.", "meta": {"post_id": 93633, "input_score": 14, "output_score": 35, "post_title": "Is $\\mathbb Q_r$ algebraically isomorphic to $\\mathbb Q_s$ while r and s denote different primes?"}}
{"input": "Possible Duplicate:\nMatrix is conjugate to its own transpose \n\nHow can I prove that a matrix is similar to its transpose?\nMy approach is: if $A$ is the matrix then $f$ is the associated application from $K^n\\rightarrow K^n$. Define $g:K^n\\rightarrow (K^n)^*$ by $g(e_i)=e_i^*$, and define $f^T$ to be the transpose application of $f$. I proved that $f^T=gfg^{-1}$. What I don't understand is, what is the matrix associated to $g$, so I can write $A^T=PAP^{-1}$.", "output": "Consider...\n$$B^{-1} = B = \\begin{bmatrix} 0 & 0 & \\cdots & 0 & 1 \\\\ 0 & 0 & \\cdots & 1 & 0 \\\\ \\vdots & \\ \\vdots & & \\vdots & \\vdots \\\\ 0 & 1 & \\cdots & 0 & 0 \\\\ 1 & 0 & \\cdots & 0 & 0 \\end{bmatrix} \\qquad \\mathrm{and} \\qquad J = \\begin{bmatrix} \\lambda & 1 & & \\\\ & \\ddots & \\ddots &\n& \\\\ & & \\lambda & 1 \\\\ &  &  & \\lambda \\end{bmatrix} $$\nThen $B^{-1}J^TB = J$. Thus a Jordan block $J$ and its transpose $J^T$ are similar.\nSo using $B_1,\\dots B_\\ell$ for each Jordan block $J_1,\\dots,J_\\ell$ and letting\n$$B = \\begin{bmatrix} B_1 & & & \\\\ & B_2 & & \\\\ & & \\ddots & \\\\ & & & B_\\ell \\end{bmatrix} \\qquad \\mathrm{and} \\qquad J = \\begin{bmatrix} J_1 & & & \\\\ & J_2 & & \\\\ & & \\ddots & \\\\ & & & J_\\ell \\end{bmatrix}$$\nThen $B^{-1}J^TB=J$. Therefore, a Jordan form and its transpose are similar.\nFinally, put $A$ into its Jordan form: $P^{-1}AP=J$ then $J^T = (P^{-1}AP)^T=P^TA^T(P^T)^{-1}$ so thus $A$ is similar to $J$. $J$ is similar to $J^T$ and $J^T$ is similar to $A^T$. Hence by transitivity $A$ and $A^T$ are similar.\nEdit: What if our field is not algebraically closed? [From the comments.]\nOf course, we only have a Jordan form when our characteristic polynomial splits over our field. Thus if we are working over a field such as $\\mathbb{R}$ or $\\mathbb{Q}$, my quick argument does not apply. Here is a fix:\nSay we are working over a field $\\mathbb{F}$. First, tack on all of the eigenvalues and call that field $\\mathbb{K}$ (i.e., $\\mathbb{K}$ is the splitting field for our characteristic polynomial).\nBy the above answer, using Jordan form, $A$ is similar to $A^T$ (working over $\\mathbb{K}$). Therefore, $A$ and $A^T$ share the same rational canonical form. Since a matrix is similar to its rational canonical form over the field generated by its entries, $A$ and $A^T$ are similar to this common form working over $\\mathbb{F}$ (or possibly even a smaller field). Therefore, $A$ and $A^T$ are similar over $\\mathbb{F}$.", "meta": {"post_id": 94599, "input_score": 45, "output_score": 57, "post_title": "A matrix is similar to its transpose"}}
{"input": "I want to ask about various mathematical fora and discussion boards available online. I think it might be useful to have such a list here at Math.SE.\nIf I may suggest, it could be useful to keep one long list with basic information as a community wiki, and if you want add some additional information on some of them, a separate answer might be the best way. (E.g. information like: \"this forum is particularly suitable for questions about contest math\" or \"this forum is frequented by specialists in computer algebra, most discussions are about CAS software such as GAP\", etc.)\nIn particular, if you deem some of other fora more suitable for some kind of questions than math.SE, this would probably be an interesting information for users of this site.\nA related question: Which discussion board is good for homework questions?", "output": "English language\n\nOf course, there is mathoverflow, many users of this site are active there too. google: mathoverflow site:math.stackexchange.com\n\nsci.math, Usenet group, google: \"sci.math\" site:stackexchange.com\n\nAoPS - Art of Problem Solving, phpBB, supports TeX using LatexRender, google: artofproblemsolving site:stackexchange.com\n\nMHF - mathhelpforum, supports TeX, google: mathhelpforum site:stackexchange.com\n\nmymathforum, phpBB, suports TeX, google: mymathforum site:stackexchange.com\n\nPhysics Forums - Sub-forum Mathematics, google: physicsforums site:math.stackexchange.com\n\nS.O.S. Mathematics CyberBoard, phpBB, supports TeX using LatexRender,\ngoogle: sosmath site:stackexchange.com\n\nTopology Q+A Board, google: \"yorku.ca\" site:stackexchange.com\n\nFree Math Help, google: \"freemathhelp.com\" site:stackexchange.com\n\nMath Help Boards, google: \"mathhelpboards.com\" site:stackexchange.com\n\nTetration forum(about)\nand Forum itself (moderated, all levels,phpBB,TeX), google: tetrationforum site:stackexchange.com\n\nMathematics subforum of xkcd; phpBB; supports LaTeX using jsMath - but they prefer avoiding it where possible, so that the site is not slowed down, see here; google: \"fora.xkcd.com\" site:stackexchange.com This forum no longer exists.\n\nAbstract and Linear Algebra. , Google: \"math.miami.edu/forum\" site:stackexchange.com\n\nPurplemath, phpBB, supports LaTeX;\nGoogle: \"purplemath.com\" site:stackexchange.com\n\nWikipedia Reference desk Mathematics\n\nIntegrals and Series, users communicate in TeX while the interface does NOT render.\n\n\nGerman language\n\nde.sci.mathematik, Usenet group\n\nMatroids Matheplanet, supports LaTeX\n\n\nCzech language\n\nMatematick\u00e9 F\u00f3rum at matematika.cz (formerly matweb.cz); supports TeX\n\nItalian language\n\nMatematicamente.it/forum, phpBB, supports LaTeX using MathJax; Google: matematicamente.it/forum site:stackexchange.com.\nThe forum also has an English language section (called English Corner).\n\nRussian language\n\ndxdy, phpBB, supports LaTeX.", "meta": {"post_id": 95787, "input_score": 46, "output_score": 49, "post_title": "Useful mathematical fora"}}
{"input": "Can someone explain why taking an average of an average usually results in a wrong answer?  Is there ever a case where the average of the average can be used correctly?  \nAs an example, let's say that an assessment is given to three schools and I want to find out the average score for all three schools combined and the average score per school.  When I attempt to add the three individual scores and divide by three I get a number that is very close (+/- 1 percent) to the actual overall average.", "output": "Thomas Andrews already answered the question, but I'd like to present a more analytical solution to the problem.\nThe average of averages is only equal to the average of all values in two cases:\n\nif the number of elements of all groups is the same; or\nthe trivial case when all the group averages are zero\n\nHere's why this is so.\nConsider two sets $X = \\{x_1, x_2, ..., x_n\\}$ and $Y = \\{y_1, y_2, ..., y_m\\}$ and their averages:\n$$ \\bar{x} = \\frac{\\sum_{i=1}^{n}{x_i}}{n} \\,,\\, \n   \\bar{y} = \\frac{\\sum_{i=1}^{m}{y_i}}{m}\n$$\nThe average of the averages is:\n$$ average(\\bar{x}, \\bar{y}) \n  = \\frac{\\frac{\\sum_{i=1}^{n}{x_i}}{n} + \\frac{\\sum_{i=1}^{m}{y_i}}{m}}{2}\n  = \\frac{\\sum_{i=1}^{n}{x_i}}{2n} + \\frac{\\sum_{i=1}^{m}{y_i}}{2m}\n$$\nNow consider the whole group \n$Z = \\{x_1, x_2, ..., x_n, y_1, y_2, ..., y_m\\}$ and its average:\n$$ \\bar{z} = \\frac{\\sum_{i=1}^{n}{x_i} + \\sum_{i=1}^{m}{y_i}}{n + m}$$\nFor the general case, we can see that these averages are different:\n$$  \\frac{\\sum_{i=1}^{n}{x_i}}{2n} + \\frac{\\sum_{i=1}^{m}{y_i}}{2m} \n\\ne \\frac{\\sum_{i=1}^{n}{x_i} + \\sum_{i=1}^{m}{y_i}}{n + m} \n$$\nThis answers the first OP question, as to why the average of averages usually gives the wrong answer.\nHowever, if we make $n = m$, we have:\n$$  \\frac{\\sum_{i=1}^{n}{x_i}}{2n} + \\frac{\\sum_{i=1}^{m}{y_i}}{2n} \n  = \\frac{\\sum_{i=1}^{n}{x_i} + \\sum_{i=1}^{n}{y_i}}{2n} \n$$\nThis is why the average of averages is equal to the average of the whole group when the groups have the same size.\nThe second case is trivial: $\\bar{x} = \\bar{y} = average(\\bar{x}, \\bar{y}) = 0$.\nNote that the above reasoning can be extended for any number of groups.", "meta": {"post_id": 95909, "input_score": 131, "output_score": 39, "post_title": "Why is an average of an average usually incorrect?"}}
{"input": "If $(V, \\langle \\cdot, \\cdot \\rangle)$ is a finite-dimensional inner product space and $f,g : \\mathbb{R} \\longrightarrow V$ are differentiable functions, a straightforward calculation with components shows that \n$$\r\n\\frac{d}{dt} \\langle f, g \\rangle = \\langle f(t), g^{\\prime}(t) \\rangle + \\langle f^{\\prime}(t), g(t) \\rangle\r\n$$\nThis approach is not very satisfying. However, attempting to apply the definition of the derivative directly doesn't seem to work for me. Is there a slick, perhaps intrinsic way, to prove this that doesn't involve working in coordinates?", "output": "This answer may be needlessly complicated if you don't want such generality, taking the approach of first finding the Fr\u00e9chet derivative of a bilinear operator.\nIf $V$, $W$, and $Z$ are normed spaces, and if $T:V\\times W\\to Z$ is a continuous (real) bilinear operator, meaning that there exists $C\\geq 0$ such that $\\|T(v,w)\\|\\leq C\\|v\\|\\|w\\|$ for all $v\\in V$ and $w\\in W$, then the derivative of $T$ at $(v_0,w_0)$ is $DT|_{(v_0,w_0)}(v,w)=T(v,w_0)+T(v_0,w)$.  (I am assuming that $V\\times W$ is given a norm equivalent with $\\|(v,w)\\|=\\sqrt{\\|v\\|^2+\\|w\\|^2}$.)  This follows from the straightforward computation \n$$\\frac{\\|T(v_0+v,w_0+w)-T(v_0,w_0)-(T(v,w_0)+T(v_0,w))\\|}{\\|(v,w)\\|}=\\frac{\\|T(v,w)\\|}{\\|(v,w)\\|}\\leq C\\frac{\\|v\\|\\|w\\|}{\\|(v,w)\\|}\\to 0$$\nas $(v,w)\\to 0$.\nWith $V=W$, $Z=\\mathbb R$ or $Z=\\mathbb C$, and $T:V\\times V\\to Z$ the inner product, this gives $DT_{(v_0,w_0)}(v,w)=\\langle v,w_0\\rangle+\\langle v_0,w\\rangle$.  Now if $f,g:\\mathbb R\\to V$ are differentiable, then $F:\\mathbb R\\to V\\times V$ defined by $F(t)=(f(t),g(t))$ is differentiable with $DF|_t(h)=h(f'(t),g'(t))$.  By the chain rule, \n$$D(T\\circ F)|_{t}(h)\r\n=DT|_{F(t)}\\circ DF|_t(h)=h(\\langle f'(t),g(t)\\rangle+\\langle f(t),g'(t)\\rangle),$$\nwhich means $\\frac{d}{dt} \\langle f, g \\rangle = \\langle f'(t),g(t)\\rangle+\\langle f(t),g'(t)\\rangle$.", "meta": {"post_id": 96265, "input_score": 65, "output_score": 39, "post_title": "Differentiating an Inner Product"}}
{"input": "If I blow up a complex manifold along a submanifold, can you give me a picture to have in mind for the blown-up manifold? Can you also tell me why this is the right picture?", "output": "The following is more or less the description you can find in Griffiths and Harris's Principles of Algebraic Geometry on page 182.\nFor the case of a point in a complex manifold, the idea is to take a local neighborhood homeomorphic to a disc $\\Delta$ in $\\mathbb{C}^n$ centered at 0, and take the projection $\\pi: \\tilde{\\Delta} \\longrightarrow \\Delta$ where $\\tilde{\\Delta} = \\{(z,l) | z_il_j = z_j l_i \\,\\forall i,j \\}\\subset \\mathbb{C}^n\\times \\mathbb{P}^{n-1}$ where $z \\in \\Delta$ and $l\\in \\mathbb{P}^{n-1}$.  (If you have trouble seeing this as a manifold, perhaps recall that there is an embedding of $\\mathbb{P}^n\\times \\mathbb{P}^m$ into $\\mathbb{P}^{(n+1)(m+1)-1}$ and work out the defining equations in that space).\nAway from $z = 0$ the projection $(z,l) \\mapsto z$ is going to be one-to-one. In fact it is a homeomorphism.\nHowever at $z=0$ we see that $\\pi^{-1}(0) = \\{ (0,l)\\} \\ \\cong \\mathbb{P}^{n-1}$  since of course $0=0$.  Now the trick is to understand how lines through $z=0$ in $\\Delta$ lift to $\\tilde{\\Delta}$ at $z=0$.  To do this, take the limit of the preimage of a point travelling along a line in $\\Delta$ towards $0$.  You will see that it goes to $(0,l)$ where $l$ is the equivalence class of the line.\nExplicitly, the line has equation $t(a_1,...,a_n)$ for $a_i\\in \\mathbb{C}$ not all zero and $t\\in \\mathbb{C}$.  If $t\\neq 0$ then $\\pi^{-1}(t(a_1,...,a_n)) = (t(a_1,...,a_n),[a_1:...:a_n])$. The limit as $t\\rightarrow 0$ is clearly (0,[a_1:...:a_n]) in $\\tilde\\Delta$ and 0 in $\\Delta$.\nIf we have a curve $C$ through $0$ in the manifold, we define the total transform of $C$ to be the homeomorphic preimage of $\\pi^{-1}(C-\\{0\\})$ plus the points in the fibre over $0$ that correspond to the different angles at which $C$ approaches $0$.  In the zariski topology this is the closure of $\\pi^{-1}(C-\\{0\\})$ (since these points are the limits of points in the preimage, as i described above).\nTo make the blow up of the manifold, one attaches $\\tilde\\Delta$ to $\\Delta$ away from $z=0$ by the homeomorphism. Away from $0$, the other charts remain the same.\nHere is some pictures from an undergrad paper, I think this helps get an intuition for how blowing up separates the slopes at 0. Here we have a node $y^2  -x^2(1+x) = 0$ and a cusp $x^2 \u2212 y^3 = 0$ in $\\mathbb{C^2}$ (be careful since this is only the real picture).  In the first case, the blow up separates the curve going through $0$ by taking the preimage of $0$ to two points corresponding to the slopes of the curve through $0$. In the second case, the curve approaches 0 from one direction.\nNote that the resulting total transforms are not singular.\n\n\nHere is another picture of the same thing with a local picture of the blow up of the disc, which you can find in this great paper.\n\nFor the case of a higher dimensional submanifold, the intuition remains the same. As you can see from the wikipedia article, it is defined locally by equations that are the same as the blow up of a dimension 0 submanifold. You are taking a projection $\\tilde M \\longrightarrow M$ that is a homeomorphism everywhere except at the submanifold, and when you lift a curve that intersects the submanifold, we define the points in the preimage of the submanifold to be the ones corresponding to the slope at which the curve intersects it. \n1 http://math.berkeley.edu/~aboocher/emma.pdf (picture)", "meta": {"post_id": 97284, "input_score": 37, "output_score": 58, "post_title": "Intuition for Blow-up."}}
{"input": "I know that if $X$ were distributed as a standard normal, then $X^2$ would be distributed as chi-squared, and hence have expectation $1$, but I'm not sure about for a general normal.\nThanks", "output": "Use the identity\n$$\r\nE(X^2)=\\text{Var}(X)+[E(X)]^2\r\n$$\nand you're done.\nSince you know that $X\\sim N(\\mu,\\sigma)$, you know the mean and variance of $X$ already, so you know all terms on RHS.", "meta": {"post_id": 99025, "input_score": 24, "output_score": 48, "post_title": "What is the expectation of $ X^2$ where $ X$ is distributed normally?"}}
{"input": "Some days ago, I was thinking on a problem, which states that $$AB-BA=I$$ does not have a solution in $M_{n\\times n}(\\mathbb R)$ and $M_{n\\times n}(\\mathbb C)$. (Here $M_{n\\times n}(\\mathbb F)$ denotes the set of all $n\\times n$ matrices with entries from the field $\\mathbb F$ and $I$ is the identity matrix.)\nAlthough I couldn't solve the problem, I came up with this problem:\n\nDoes there exist a field $\\mathbb F$ for which that equation $AB-BA=I$ has a solution in $M_{n\\times n}(\\mathbb F)$?\n\nI'd really appreciate your help.", "output": "Let $k$ be a field. The first Weyl algebra $A_1(k)$ is the free associative $k$-algebra generated by two letters $x$ and $y$ subject to the relation $$xy-yx=1,$$ which is usually called the Heisenberg or Weyl commutation relation. This is an extremely important example of a non-commutative ring which appears in many places, from the algebraic theory of differential operators to quantum physics (the equation above is Heisenberg's indeterminacy principle, in a sense) to the pinnacles of Lie theory to combinatorics to pretty much anything else.\nFor us right now, this algebra shows up because \n\nan $A_1(k)$-modules are essentially the same thing as solutions to the equation $PQ-QP=I$ with $P$ and $Q$ endomorphisms of a vector space. \n\nIndeed:\n\nif $M$ is a left $A_1(k)$-module then $M$ is in particular a $k$-vector space and there is an homomorphism of $k$-algebras $\\phi_M:A_1(k)\\to\\hom_k(M,M)$ to the endomorphism algebra of $M$ viewed as a vector space. Since $x$ and $y$ generate the algebra $A_1(k)$, $\\phi_M$ is completely determined by the two endomorphisms $P=\\phi_M(x)$ and $Q=\\phi_M(y)$; moreover, since $\\phi_M$ is an algebra homomorphism, we have $PQ-QP=\\phi_1(xy-yx)=\\phi_1(1_{A_1(k)})=\\mathrm{id}_M$. We thus see that $P$ and $Q$ are endomorphisms of the vector space $M$ which satisfy our desired relation.\nConversely, if $M$ is a vector space and $P$, $Q:M\\to M$ are two linear endomorphisms, then one can show more or less automatically that there is a unique algebra morphism $\\phi_M:A_1(k)\\to\\hom_k(M,M)$ such that $\\phi_M(x)=P$ and $\\phi_M(y)=Q$. This homomorphism turns $M$ into a left $A_1(k)$-module.\nThese two constructions, one going from an $A_1(k)$-module to a pair $(P,Q)$ of endomorphisms of a vector space $M$ such that $PQ-QP=\\mathrm{id}_M$, and the other going the other way, are mutually inverse.\n\nA conclusion we get from this is that your question \n\nfor what fields $k$ do there exist $n\\geq1$ and matrices $A$, $B\\in M_n(k)$ \n  such that $AB-BA=I$?\n\nis essentially equivalent to\n\nfor what fields $k$ does $A_1(k)$ have finite dimensional modules?\n\nNow, it is very easy to see that $A_1(k)$ is an infinite dimensional algebra, and that in fact the set $\\{x^iy^j:i,j\\geq0\\}$ of monomials is a $k$-basis.\nTwo of the key properties of $A_1(k)$ are the following:\n\nTheorem. If $k$ is a field of characteristic zero, then $A_1(k)$ is a simple algebra\u2014that is, $A_1(k)$ does not have any non-zero proper bilateral ideals. Its center is trivial: it is simply the $1$-dimensional subspace spanned by the unit element.\n\nAn immediate corollary of this is the following\n\nProposition. If $k$ is a field of characteristic zero, the $A_1(k)$ does not have any non-zero finite dimensional modules. Equivalently, there do not exist $n\\geq1$ and a pair of matrices $P$, $Q\\in M_n(k)$ such that $PQ-QP=I$.\n\nProof. Suppose $M$ is a finite dimensional $A_1(k)$-module. Then we have an algebra homomorphism $\\phi:A_1(k)\\to\\hom_k(M,M)$ such that $\\phi(a)(m)=am$ for all $a\\in A_1(k)$ and all $m\\in M$. Since $A_1(k)$ is infinite dimensional and $\\hom_k(M,M)$ is finite dimensional (because $M$ is finite dimensional!) the kernel $I=\\ker\\phi$ cannot be zero \u2014in fact, it must hace finite codimension. Now $I$ is a bilateral ideal, so the theorem implies that it must be equal to $A_1(k)$. But then $M$ must be zero dimensional, for $1\\in A_1(k)$ acts on it at the same time as the identity and as zero. $\\Box$\nThis proposition can also be proved by taking traces, as everyone else has observed on this page, but the fact that $A_1(k)$ is simple is an immensely more powerful piece of knowledge (there are examples of algebras which do not have finite dimensional modules and which are not simple, by the way :) )\nNow let us suppose that $k$ is of characteristic $p>0$. What changes in term of the algebra? The most significant change is \n\nObservation. The algebra $A_1(k)$ is not simple. Its center $Z$ is generated by the elements $x^p$ and $y^p$, which are algebraically independent, so that $Z$ is in fact isomorphic to a polynomial ring in two variables. We can write $Z=k[x^p,y^p]$.\n\nIn fact, once we notice that $x^p$ and $y^p$ are central elements \u2014and this is proved by a straightforward computation\u2014 it is easy to write down non-trivial bilateral ideals. For example, $(x^p)$ works; the key point in showing this is the fact that since $x^p$ is central, the left ideal which it generates coincides with the bilateral ideal, and it is very easy to see that the left ideal is proper and non-zero.\nMoreover, a little playing with this will give us the following. Not only does $A_1(k)$ have bilateral ideals: it has bilateral ideals of finite codimension. For example, the ideal $(x^p,y^p)$ is easily seen to have codimension $p^2$; more generally, we can pick two scalars $a$, $b\\in k$ and consider the ideal $I_{a,b}=(x^p-a,y^p-b)$, which has the same codimension $p^2$. Now this got rid of the obstruction to finding finite-dimensional modules that we had in the characteristic zero case, so we can hope for finite dimensional modules now!\nMore: this actually gives us a method to produce pairs of matrices satisfying the Heisenberg relation. We just can pick a proper bilateral ideal $I\\subseteq A_1(k)$ of finite codimension, consider the finite dimensional $k$-algebra $B=A_1(k)/I$ and look for finitely generated $B$-modules: every such module is provides us with a finite dimensional $A_1(k)$-module and the observations above produce from it pairs of matrices which are related in the way we want.\nSo let us do this explicitly in the simplest case: let us suppose that $k$ is algebraically closed, let $a$, $b\\in k$ and let $I=I_{a,b}=(x^p-a,y^p-b)$. The algebra $B=A_1(k)/I$ has dimension $p^2$, with $\\{x^iy^j:0\\leq i,j<p\\}$ as a basis. The exact same proof that the Weyl algebra is simple when the ground field is of characteristic zero proves that $B$ is simple, and in the same way the same proof that proves that the center of the Weyl algebra is trivial in characteristic zero shows that the center of $B$ is $k$; going from $A_1(k)$ to $B$ we have modded out the obstruction to carrying out these proofs. In other words, the algebra $B$ is what's called a (finite dimensional) central simple algebra. Wedderburn's theorem now implies that in fact $B\\cong M_p(k)$, as this is the only semisimple algebra of dimension $p^2$ with trivial center. A consequence of this is that there is a unique (up to isomorphism) simple $B$-module $S$, of dimension $p$, and that all other finite dimensional $B$-modules are direct sums of copies of $S$. \nNow, since $k$ is algebraically closed (much less would suffice) there is an $\\alpha\\in k$ such that $\\alpha^p=a$. Let $V=k^p$ and consider the $p\\times p$-matrices $$Q=\\begin{pmatrix}0&&&&b\\\\1&0\\\\&1&0\\\\&&1&0\\\\&&&\\ddots&\\ddots\\end{pmatrix}$$ which is all zeroes expect for $1$s in the first subdiagonal and a $b$ on the top right corner, and $$P=\\begin{pmatrix}-\\alpha&1\\\\&-\\alpha&2\\\\&&-\\alpha&3\\\\&&&\\ddots&\\ddots\\\\&&&&-\\alpha&p-1\\\\&&&&&-\\alpha\\end{pmatrix}.$$ One can show that $P^p=aI$, $Q^p=bI$ and that $PQ-QP=I$, so they provide us us a morphism of algebras $B\\to\\hom_k(k^ p,k^ p)$, that is, they turn $k^p$ into a $B$-module. It must be isomorphic to $S$, because the two have the same dimension and there is only one module of that dimension; this determines all finite dimensional modules, which are direct sums of copies of $S$, as we said above..\nThis generalizes the example Henning gave, and in fact one can show that this procedure gives all $p$-dimensional $A_1(k)$-modules can be constructed from quotients by ideals of the form $I_{a,b}$. Doing direct sums for various choices of $a$ and $b$, this gives us lots of finite dimensional $A_1(k)$-modules and, then, of pairs of matrices satisfying the Heisenberg relation. I think we obtain in this way all the semisimple finite dimensional $A_1(k)$-modules but I would need to think a bit before claiming it for certain.\nOf course, this only deals with the simplest case. The algebra $A_1(k)$ has non-semisimple finite-dimensional quotients, which are rather complicated (and I think there are pretty of wild algebras among them...) so one can get many, many more examples of modules and of pairs of matrices.", "meta": {"post_id": 99175, "input_score": 54, "output_score": 53, "post_title": "Solutions to the matrix equation $\\mathbf{AB-BA=I}$ over general fields"}}
{"input": "I have a problem where I have TWO NON-rotated rectangles (given as two point tuples {x1 x2 y1 y2}) and I like to calculate their intersect area. I have seen  more general answers to this question, e.g. more rectangles or even rotated ones, and I was wondering whether there is a much simpler solution as I only have two non-rotated rectangles. \nWhat I imagine should be achievable is an algorithm that only uses addition, subtraction and multiplication, possibly abs() as well. What certainly should not be used are min/max, equal, greater/smaller and so on, which would make the question obsolete.\nThank you!\nEDIT 2: okay, it's become too easy using min/max or abs(). Can somebody show or disprove the case only using add/sub/mul?\nEDIT: let's relax it a little bit, only conditional expressions (e.g. if, case) are prohibited!\nPS: I have been thinking about it for a half hour, without success, maybe I am now too old for this :)", "output": "Uses only max and min (drag the squares to see the calculation. Forget about most of the code, the calculation is those two lines with the min and max):\nhttp://jsfiddle.net/Lqh3mjr5/\nYou can also reduce min to max here (or the opposite), i.e. $min\\{a,b\\} = -max\\{-a,-b\\}$.\n\nFirst compute the bounding rectangles rect1 and rect2 with the following properties:\nrect = {\n  left: x1,\n  right: x1 + x2,\n  top: y1,\n  bottom: y1 + y2,\n}\n\nThe overlap area can be computed as follows:\nx_overlap = Math.max(0, Math.min(rect1.right, rect2.right) - Math.max(rect1.left, rect2.left));\ny_overlap = Math.max(0, Math.min(rect1.bottom, rect2.bottom) - Math.max(rect1.top, rect2.top));\noverlapArea = x_overlap * y_overlap;", "meta": {"post_id": 99565, "input_score": 45, "output_score": 78, "post_title": "Simplest way to calculate the intersect area of two rectangles"}}
{"input": "Let $X$ be a compact K\u00e4hler manifold of complex dimension $\\dim_{\\mathbb C} = n$. Let $[\\omega]$ be the cohomology class of a K\u00e4hler metric on $X$. Then powers of the class $[\\omega]$ defines a linear morphism between cohomology groups\n$$ L^k :  H^{n-k}(X,\\mathbb C) \\longrightarrow H^{n+k}(X,\\mathbb C) $$\nwhich is simply given by cup product against the class $[\\omega]^k$. The hard Lefschetz theorem says that this is in fact an isomorphism of vector spaces.\nQuestion: Why do we call this the \"hard\" Lefschetz theorem?\nModern proofs of this theorem are not that involved; one picks a K\u00e4hler metric $\\omega$ and proves the K\u00e4hler identities on $X$, and the rest then follows from the existence of primitive decompositions. Thus it seems a bit of hype to call the theorem \"hard\".\nOne might think this is to distinguish this from another theorem of Lefschetz, often called the \"weak\" Lefschetz theorem, which gives a similar result in the case where $[\\omega]$ is the Chern class of an ample line bundle. But then we'd surely call this the \"strong\" Lefschetz theorem, right?", "output": "This question can only have a subjective answer (which is actually fun, from time to time!), so here  are a few personal remarks.  \n1) You are a dynamic PhD student working in 2012 under the supervision of Demailly, a world leader in complex algebraic geometry.\nYou have at your disposal a technology that didn't exist on Lefschetz' time: singular and De Rham cohomology, higher homotopy groups, K\u00e4hler manifolds, Hodge theory,...\nEven the abstract notion of a finite-dimensional vector space had not been axiomatized.\nSo  when you claim that the theorem is not that hard, you should not lose sight of  the historic context in which Lefschetz \"proved\" his theorem in 1924.  \n2) I wrote \"proved\" in quotes, since as Sabbah diplomaticallty puts it, Lefschetz' proof was \"insufficient\".\nSo the theorem was not easy, even for Lefschetz. \n3) The theorem has fascinated many Fields medalists and other giants who gave proofs of some version of the theorem: Andreotti, Frankel, Thom, Bott, Kodaira, Spencer, Artin, Grothendieck, Deligne.\nThis is certainly an indication of the depth of the theorem...  \n4) Like you I am enthusiastic about complex algebraic manifolds and am grateful for  the transcendental methods , like K\u00e4hler theory, which allow us to study them.\nHowever algebraic geometers also  want to consider algebraic varieties in characteristic $p$, and  there these transcendental tools  unfortunately completely break down.\nHard Lefschetz for smooth varieties over finite fields was proved by Deligne only in 1980, after much preliminary work by himself and Grothendieck (cf. SGA7).\n I would surmise that  the terminology \"Lefschetz vache\" introduced by Grothendieck is to be understood in that context.  \n5) Finally even in the complex case, I find the proof of hard Lefschetz starting from scratch not so easy.\nI'll let you and the  other users judge by linking to a free online course of Sabbah on Hodge theory and hard Lefschetz (in the Introduction  of which he writes the diplomatic remark mentioned above!)\nEdit\nSince this is a good-humoured, non-technical  answer,  I'll  take the liberty of quoting the following picturesque metaphor by Lefschetz:    \nIt was my lot to plant the harpoon of algebraic topology into the body of the whale of algebraic geometry", "meta": {"post_id": 99840, "input_score": 36, "output_score": 48, "post_title": "Why is the hard Lefschetz theorem \"hard\"?"}}
{"input": "I understand that the axiom of choice, given the axioms of ZF set theory, is equivalent to the statement that \"the Cartesian product of any family of nonempty sets is nonempty.\" I've been unable to find this proof. Could someone sketch it for me? Or provide me with a source at least?", "output": "Suppose $X=\\{X_i\\mid i\\in I\\}$ is a family of nonempty sets.\nIf there exists a choice function, then $\\langle f(i)\\mid i\\in I\\rangle$ is an element of the product $\\prod_{i\\in I}X_i$.\nIf $\\prod_{i\\in I}X_i$ is nonempty then there is $f=\\langle x_i\\mid i\\in I\\rangle$ in this product, which is a sequence of $x_i$ such that $x_i\\in X_i$. The function $f(i)=x_i$ is a choice function.\n\nIndeed as Nate comments, it is most common to define the product $\\prod_{i\\in I}X_i$ as the set of functions $f:I\\to\\bigcup\\{X_i\\mid i\\in I\\}$ such that $f(i)\\in X_i$ for all $i\\in I$.\nOne can easily observe that under this definition the product is exactly the set of choice functions, therefore the product is nonempty if and only if there exists a choice function.", "meta": {"post_id": 101004, "input_score": 21, "output_score": 34, "post_title": "The Axiom of Choice and the Cartesian Product."}}
{"input": "Is there a systematic way of finding the conjugacy class and centralizer of an element? Could the task be simplified if we are working with \"special groups\" such as $S_n$ or $A_n$? Are there any intuitive approaches? Thanks.", "output": "Well, I am happy you call the groups, $S_n$ and $A_n$ special. They are!\nSo, I'll describe a method for calculating the conjugacy class and hence the centralizer in $S_n$.\nStep 1:\nThis comes from realising that any two elements of the same cycle type are conjugate to each other in $S_n$. How do we prove this startling result?\nProof.\nLemma\nFor any $\\tau, \\sigma \\in S_n$, since, $\\sigma$ is a product of disjoint cycles, let's say for instance, that, cycle decomposition of $\\sigma $ is given by  $$\\sigma=(a_1a_2a_3\\cdots a_k)(b_1b_2b_3\\cdots b_l)\\cdots$$\nWe claim that $\\tau\\sigma\\tau^{-1}=(\\tau(a_1)\\tau(a_2)\\cdots\\tau(a_k))(\\tau(b_1)\\tau(b_2)\\cdots\\tau(b_l))\\cdots$\nProof of Lemma:\nThe proof is subtle. Suppose $i \\overset{\\sigma}{\\mapsto}j$, we'll prove that $\\tau(i) \\overset{\\tau\\sigma\\tau^{-1}}{\\mapsto}\\tau(j)$.\nNow $$\\begin{align*}\\tau\\sigma\\tau^{-1}(\\tau(i))&=\\tau(\\sigma(i))\\\\&=\\tau(j)\\end{align*}$$\nThis proves the claim. $\\diamond$\nSo, the cycle type of an element and its conjugate are the same. So, this proves that all elements of the same cycle type are conjugate to each other.$~~~~~~~~~~\\blacksquare$\nStep-2\nThis step is a little intuitive. We claim that the number of conjugacy classes in $S_n$ equals the partition of $n$.\nProof.\nFirstly, let's prove that the number of cycle types in $S_n$ equal the number of partitions of $n$. The proof of this is rather intuitive. With any permutation, associate the partition whose parts equal the number of elements in a cycle. This gives you the required bijection between the cycle types of $S_n$ and partition of $n$.\nTo get a feel for it, look at the following in $S_4$:\n$$\\begin{align*}(1234)&\\cong 4\\\\(12)(34) &\\cong 2+2\\\\ (34) &\\cong 1+1+2(\\text{since (1) and (2) are omitted in this notation})\\end{align*}$$\nSo, now, since two elements of the same cycle type are conjugate in $S_n$$^\\dagger$, we have that they belong to the same conjugacy class. So, counting the number of cycle types gives you the number of conjugacy classes. So, we have that the number of conjugacy classes is equal to the number of partitions of $n$. $\\blacksquare$\nSo, we have so far described the conjugacy classes in $S_n$. But, unfortunately, human brain cannot defeat the symmetry in nature, thanks to people like Ramanujan, little atleast is known about $p(n)$, the number of partitions of $n$. Among the little, a closed form formula is not one!\nNow, use the orbit-stabilizer lemma, with the understanding that stabilizer of a point under conjugacy is nothing but its centralizer.\nFor a specific example such as the one you asked Gerry through comments, you can actually work through with this description.\nFor instance, $(1234567)$ in $S_n$\nA little more machinery is involved! You need to find the number of $r$-cycles in $S_n$ for appropriate $r$. This is merely a combinatorial argument: You should find that this equals,\n$$\\dfrac{\\binom{n}{r}\\cdot r!}{r}$$\nCentralizer of $(1234567)$:\nNextly, we describe the form of the element that commutes with this element. It should be precisely, $$(1234567)^i \\sigma~;~~0\\leq i < 6 ~~~\\text{$\\sigma$ is disjoint from $(1234567)$}$$\nThe facts you'll need are:\n\nThe order of an $r$-cycle is $r$\nDisjoint cycles commute\nCyclic groups are also abelian.\n\nHope this helps.\nReferences:\nHerstein's Exercises and Dummit and Foote's description.\n\n$\\dagger$ This requires a proof! The reader is advised to prove this and not take it on faith because, having written this detailed answer, I'd have added this as well, if I knew the argument :)", "meta": {"post_id": 102170, "input_score": 33, "output_score": 40, "post_title": "Is there a systematic way of finding the conjugacy class and/or centralizer of an element?"}}
{"input": "Here's a little brain teaser, for your coffee break:\n$$\n62-63 = 1\n$$\nMove only one digit to make it right!\nHave fun!", "output": "$6263 \\neq 1$.\nI moved one character not a digit :)", "meta": {"post_id": 102946, "input_score": 20, "output_score": 43, "post_title": "Coffee Break Riddle"}}
{"input": "I have several related questions:\nDo there exist colimits in the category of schemes? If not, do there exist just direct limits? Do there exist limits? If not, do there exist just inverse limits? With more generality and summarizing, with which generality there exist limits and colimits in Schemes?\nThen, if I have a colimit of rings, its Spec is a limit in the category of affine schemes. Is it so in the category of all schemes? If not, with which generality, that is, what kinds of colimits does Spec transform to limits?\nAnd does Spec transform limits into colimits? If not, whith which generality, that is, what kinds of limits does Spec transform to colimits?", "output": "Of course there is the important gluing lemma for schemes which says that certain pushouts along open immersions exist. But the category of schemes has not all colimits, see MO/9961.\n\nThe category of schemes has not all limits, see MO/9134 and MO/65506.\n\nIn contrast to 1) and 2), the category of locally ringed spaces (as well as the category of ringed spaces) has all limits and all colimits. See the paper by Gilliam on localization of ringed spaces, as well as Prop. I.1.6. in Groupes algebriques by Demazure-Gabriel.\n\n$\\mathrm{Spec} : \\mathsf{CRing}^{\\mathrm{op}} \\to \\mathsf{Sch}$ is right adjoint to the functor of global sections, therefore it preserves all limits. In particular, limits of affine schemes always exist, and their coordinate ring is just the colimit of the coordinate rings. For example, the limit of $\\dotsc \\to \\mathbb{A}^2_k \\to \\mathbb{A}^1_k \\to \\mathbb{A}^0_k$  is $\\mathbb{A}^{\\infty}_k = \\mathrm{Spec}(k[x_1,x_2,\\dotsc])$.\n\nMore generally, for an arbitrary base scheme $S$, the functor $\\mathrm{Spec} : \\mathsf{qcAlg}(S)^{\\mathrm{op}} \\to \\mathsf{Sch}/S$, which sends a quasi-coherent algebra on $S$ to its relative spectrum, is left adjoint to the functor which sends $p : T \\to S$ to $p_* \\mathcal{O}_T$, and induces an anti-equivalence of categories between quasi-coherent algebras on $S$ and the category of affine $S$-schemes. Hence, limits of affine $S$-schemes exist in the category of all $S$-schemes, are affine, and correspond to the colimit of the corresponding quasi-coherent algebras. This is quite useful and used often in conjunction with the Noetherian Approximation Theorem: If $S$ is noetherian, every quasi-compact quasi-separated $S$-scheme is a directed limit of noetherian $S$-schemes with affine transition morphisms. These directed limits are preserved by the forgetful functor to topological spaces. Details can be found in EGA.\n\n$\\mathrm{Spec}$ does not preserve colimits, i.e. the spectrum of a limit of rings is not the colimit of the spectra of the rings. For example, $\\mathrm{Spec}\\left(\\prod_{i \\in I} \\mathbb{F}_2\\right)$ can be identified with the space of ultrafilters on $I$, and only the small part of principal ultrafilters on $I$ constitutes $\\coprod_{i \\in I} \\mathrm{Spec}(\\mathbb{F}_2)$.\n\nSome colimits are preserved: The initial object, and more generally finite coproducts. Also, if $A \\to C$ is a surjective ring homomorphism, and $B \\to C$ is an arbitrary ring homomorphism, then $\\mathrm{Spec}(A \\times_C B)=\\mathrm{Spec}(A) \\cup_{\\mathrm{Spec}(C)} \\mathrm{Spec}(B)$, and the forgetful functor to ringed spaces preserves this pushout. More generally, when $Z \\to X$ and $Z' \\to X$ are closed immersions, then the pushout $Z \\cup_X Z'$ exists, and it is preserved by the forgetful functor to ringed spaces. See Schwedes paper on glueing schemes.", "meta": {"post_id": 102973, "input_score": 33, "output_score": 45, "post_title": "On limits, schemes and Spec functor"}}
{"input": "Prove that for any $x_0 \\in X$ and any $r>0$, the open ball $B_r(x_o)$ is open.\n\nMy attempt: Let $y\\in B_r(x_0)$. By definition, $d(y,x_0)<r$. I want to show there exists an $r_1\\in\\mathbb{R^+}$ s.t. $B_{r_{1}}(y)\\subseteq B_r(x_0)$. Let $a\\in B_{r_{1}}(y)$. Then, $d(a,y)<r_1$. For $a\\in B_{r}(x_0)$, $d(a,x_0)<r$. I want to show $d(a,y)<r_1$ implies $d(a,x_0)<r$. By triangle inequality, $d(a,y)\\leq d(a,x_0) + d(y,x_0) \\rightarrow$  $d(a,y)<r_1\\leq d(a,x_0)+d(y,x_0)<2r...$\nI'm a little stuck after this point.", "output": "You need to specify $r_1$.\nFor $y\\in  B_r(x_0)$, let $r_1=r-d(y,x_0)$. Then if $x\\in B_{r_1}(y)$\n$$\nd(x,x_0)\\le d(x,y)+d(y,x_0)< r_1+ d(y,x_0)=r.\n$$\nSo $ B_{r_1}(y)\\subseteq B_r(x_0)$. This shows that $B_r(x_0)$ is open.\n$\\quad\\quad\\quad\\quad\\quad\\quad\\quad$", "meta": {"post_id": 104083, "input_score": 32, "output_score": 88, "post_title": "An open ball is an open set"}}
{"input": "We know that the trace of a matrix is a linear map for all square matrices and that $\\operatorname{trace}(AB)=\\operatorname{trace}(BA)$ when the multiplication makes sense. \nOn the Wikipedia page for trace, under properties, it says that these properties characterize the trace completely in the following sense: If $f$ is a linear function on the space of square matrices satisfying $f(xy)=f(yx)$, then $f$ and $\\operatorname{tr}$ are proportional. A note on the bottom of the page gives the justification, but I do not understand the logic of it. Thanks", "output": "Let $f$ be such a map and $E_{ij}$ be the matrix with a one in $(i,j)$-entry and zero elsewhere. We have\n$$E_{ij}E_{kl}=\\left\\{\\begin{array}{cc} 0, &\\mbox{ if } \\; j \\neq k\n\\\\E_{il}, & \\mbox{ if } \\; j=k\\end{array}\\right.$$\nTherefore, if $i\\not=j, \\; E_{ij}=E_{i1}E_{1j},$ then by hypothesis,\n$$f(E_{ij})=f(E_{i1}E_{1j})=f(E_{1j}E_{i1})=f(o)=0,$$\nand\n$$f(E_{ii})=f(E_{i1}E_{1i})=f(E_{1i}E_{i1})=f(E_{11})$$\nfor each $1 \\leq i \\leq n.$\nNow, let $C=\\sum_{1\\leq i,j \\leq n}a_{i,j}E_{ij}$ be a vector in the above basis, then\n$$f(C)=\\sum_{1\\leq i,j \\leq n}a_{ij}f(E_{ij})=\\sum_{i=1}^n a_{ii}f(E_{11})=f(E_{11})\\sum_{i=1}^n a_{ii}=f(E_{11})tr(C).$$", "meta": {"post_id": 104854, "input_score": 22, "output_score": 36, "post_title": "Characterization of the trace function"}}
{"input": "What is the difference between a direct product and a semi-direct product in group theory? \nBased on what I can find, difference seems only to be the nature of the groups involved, where a direct product can involve any two groups and the semi-direct product only allows a normal subgroup $N$ of some group $G$ and another subgroup of $G$ that intersects trivially with $N$. \nIs this all? What are the significance? Thank you.", "output": "Let's look at three related concepts, in increasing order of complexity:\n\nDirect products. We say that $G$ is (isomorphic to) a direct product of $M$ and $N$ if and only if there exist subgroups $H$ and $K$ of $G$ such that:\n\n$H\\cong M$ and $K\\cong N$;\n$H\\triangleleft G$ and $K\\triangleleft G$;\n$H\\cap K=\\{e\\}$;\n$G=HK$.\n\nSemidirect products. We say that $G$ is (isomorphic to) a semidirect product of $M$ by $N$ if and only if there exist subgroups $H$ and $K$ of $G$ such that:\n\n$H\\cong M$ and $K\\cong N$;\n$H\\triangleleft G$;\n$H\\cap K=\\{e\\}$;\n$G=HK$. \n\nExtensions. We say that $G$ is (isomorphic to) an extension of $M$ by $N$ if and only if there exists a subgroup $H$ of $G$ such that\n\n$H\\cong M$;\n$H\\triangleleft G$;\n$G/H\\cong N$.\n\n\n1 and 2 look very similar. In fact, 1 is a special case of 2 (when $K$ is normal); and 2 is a special case of 3: if $G=HK$, $H\\triangleleft G$, and $H\\cap K=\\{e\\}$, then $K$ maps isomorphically onto $G/H$ via the natural projection (the intersection with the kernel is trivial, so the projection restricted to $K$ is one-to-one; and every element of $G$ can be written as $x=hk$ with $h\\in H$ and $k\\in K$, so $Hx = Hk$, hence the map is onto when restricted to $K$). \nBut each one is a more general construction than the previous one, yielding more general types of groups.\nFor instance, in Direct Products, the conditions immediately imply that elements of $H$ commute with elements of $K$:\nLemma. Let $G$ be a group, and let $H$ and $K$ be normal subgroups of $G$. If $H\\cap K=\\{e\\}$, then $hk=kh$ for all $h\\in H$ and $k\\in K$.\nProof. Consider $hkh^{-1}k^{-1}$. Since $K$ is normal in $G$, then \n$$hkh^{-1}k^{-1} = (hkh^{-1})k^{-1} \\in (hKh^{-1})K = KK = K;$$\nand since $H$ is normal in $G$, then\n$$hkh^{-1}k^{-1} = h(kh^{-1}k^{-1}) \\in H(kHk^{-1}) = HH = H.$$\nTherefore, $hkh^{-1}k^{-1}\\in H\\cap K = \\{e\\}$, so $hkh^{-1}k^{-1}=e$. Multiplying on the right by $kh$ gives $hk=kh$, as desired. $\\Box$\nSo, for example, if all you know are direct abelian groups, then direct products will only give you abelian groups. If both $M$ and $N$ have exponent $k$, then direct products will give you a group of exponent $k$. (In fact, any identity satisfied by both $M$ and $N$ will be satisfied by $M\\times N$; but this is perhaps a little advanced for you right now, so don't worry too much about it).\nBy contrast, semidirect products are more complicated, because that second subgroup doesn't have to be normal. The argument above is invalid, and we don't always get that elements of $H$ and elements of $K$ commute (if they do, then you have a direct product). The smallest example is $S_3$, the nonabelian group of order $6$ viewed as the permutations of $\\{1,2,3\\}$, with $M=C_3$, the cyclic group of order $3$, $N=C_2$, the cyclic group of order $2$, and $H=\\{I, (1,2,3), (1,3,2)\\}$, $K = \\{I, (1,2)\\}$ (other choices of $K$ are possible). \nIn a semidirect product, the fact that $H$ is normal means that for every $k\\in K$ you have $kHk^{-1}=H$; that is, each $k$ induces an automorphism of $H$. So we can define a homomorphism $K\\to \\mathrm{Aut}(H)$, by letting $k$ map to the homomorphism $h\\mapsto khk^{-1}$. If this map is trivial, you get the direct product of $H$ and $K$. If the map is not trivial, then you get more interesting groups. Different homomorphisms may lead to nonisomorphic groups, so that now we have to be careful: while there is one and only one way to construct a \"direct product\" of two groups $M$ and $N$, there may, in general, be many (non-equivalent) ways of constructing semidirect products of $M$ by $N$.\nNote that it is now possible to have a semidirect product of abelian groups that is not abelian (as in the $S_3$ example). And it is no longer true that if both $M$ and $N$ are of exponent $k$, then a semidirect product will also have exponent $k$. For example, take $M=C_2\\times C_2 = \\{1,x\\}\\times\\{1,x\\}$, which is of exponent $2$, take $N=\\{1,n\\} = C_2$, also of exponent $2$, and let the nontrivial element of $N$ act on $M$ by the rule $n^{-1}(a,b)n = (b,a)$. Then $(x,1)n$ has order $4$:\n$$\\begin{align*}\r\n\\bigl((x,1)n\\bigr)^2 = (x,1)n(x,1)n &= (x,1)(n^{-1}(x,1)n) = (x,1)(1,x) = (x,x)\\\\\r\n\\bigl((x,1)n\\bigr)^3 = (x,x)(x,1)n &= (1,x)n\\\\\r\n\\bigl((x,1)n\\bigr)^4 = (1,x)n(x,1)n &= (1,x)(n^{-1}(x,1)n) = (1,x)(1,x) = (1,1).\r\n\\end{align*}$$\nExtensions are even more complex: in essence, every finite group can be viewed as a sequence of extensions of simple groups (hence, in part, the interest in classifying all finite simple groups). Not every extension is a semidirect product (or a direct product). For example, $\\mathbb{Z}_4$, the cyclic group of order $4$, is an extension of $\\mathbb{Z}_2$ by $\\mathbb{Z}_2$: the subgroup $H=\\{\\overline{0},\\overline{2}\\}$ is cyclic of order $2$ and normal, and the quotient $\\mathbb{Z}_4/H$ is of order $2$, hence cyclic of order $2$. If it were a semidirect product of $\\mathbb{Z}_2$ by $\\mathbb{Z}_2$, then being abelian it would necessarily be a direct product, and so would have exponent $2$; so it cannot be written as a semidirect product.\nAs I mentioned, every group can be expressed as a sequence of extensions using simple groups. By the Jordan-H\u00f6lder Theorem, although the sequence is not unique, the precise simple groups that occur is (counting multiplicity). \nThe definitions look quite similar: we just drop the condition of normality for one factor when going from direct product to semidirect product; we just exchange \"there is a subgroup isomorphic to $N$ that maps isomorphically onto the quotient\" with \"the quotient is isomorphic to $N$\" in going from semidirect product to extension. But the consequences of these \"little changes\" is large. Much like the difference between \"finite abelian group\" and \"finite group\" looks very small (just a single line dropped), but the implications in our ability to classify/understand the objects in question are enormous.", "meta": {"post_id": 106028, "input_score": 67, "output_score": 129, "post_title": "Semi-direct v.s. Direct products"}}
{"input": "Definition of a countable set, from Stanford, as I didn't want to quote Wikipedia:\n\nDefinition. A set S is countable if |S| = |N|.\nThus a set S is countable if there is a one-to-one mapping of Num onto S, that is, if S is the range of an infinite one-to-one sequence.\n\nSo it seems that if we can define a set of numbers that does not map one-to-one to the natural numbers, then it is not a countable set. The natural numbers quite obviously map one-to-one to the natural numbers, so can they possibly be uncountable?\nLet's say we have a list containing all of the natural numbers. Excerpt:\n\n...\n000099\n000100\n000101\n000102\n...\n\nWe can define a number that is different from each element in this list as follows: for the ith number in the list, the ith digit is one of the 8 (or 9) non-zero alternatives that make our new number different from the number on the list. For example:\n\n...\n00009 9\n0001 0 0\n000 1 01\n00 0 102\n...\n12 3456\n\nAs we keep going, we will end up with a sequence of non-zero digits, which forms a valid Natural number, that is not on our list of all natural numbers, so our mapping of the natural numbers to the natural numbers breaks.\nDoes this make sense, or is there something I'm missing?", "output": "Your error is in the assertion:\n\n... we will end up with a sequence of non-zero digits, which forms a valid Natural number...\n\nNatural numbers have only finitely many nonzero digits in their decimal expansion. This can be proven by induction: it is true of $1$; and the number of nonzero digits in the decimal expansion of $n+1$ is at most one more than that of $n$ (you need at most one more digit before you start padding with $0$s on the left).\nAn infinite list which contains infinitely many nonzero digits does not yield a natural number. In fact, one can prove that your procedure will produce a sequence of digits with infinitely many nonzero digits, hence not correspond to a natural number.\nLet's say that your procedure selects $0$ whenever possible (that is, whenever the $i$th digit, from right to left, of the $i$th natural number, is nonzero; you really want this, because if you insist, as in your post, that you always select a nonzero digit, then you guarantee what you get is not a natural number). Can the list be arranged in such a way that the $i$th digit of the $i$th number is nonzero for all $i\\gt N$ for some $N$? No: for any given $N$, there are $10^{N-1}$ natural numbers that require fewer than $N$ digits to write; since there are only $N$ positions prior to $N$, at least one of these $10^{N-1}$ numbers must be listed in a position below $N$; but if it is listed in position $j\\gt N$, then its $j$th digit will be $0$, so the constructed sequence will necessarily have a nonzero entry at $j\\gt N$. Since this holds for all $N$, the list obtained is never eventually $0$, and so does not correspond to a natural number.\n\nNow, if you want to extend your notion of numbers to include expressions with infinitely many nonzero digits, then your argument is correct: the set of all such \"numbers\" is not countable. However, those are not the natural numbers.", "meta": {"post_id": 106546, "input_score": 7, "output_score": 37, "post_title": "Can the natural numbers be uncountable?"}}
{"input": "Edit 8.8.2013: See this question also.\nThe Fourier cosine transform of an exponential sawtooth wave times $e^{-x/2}$:\n$$\\operatorname{FourierCosineTransform}(\\operatorname{SawtoothWave}(e^x)\\cdot e^{-\\frac{x}{2}})$$\ncan be plotted with the following Mathematica 8 program:\nscale = 1000000;\nxres = .00001;\nx = Exp[Range[0, Log[scale], xres]];\na = FourierDCT[SawtoothWave[x]*x^(-1/2)];\nc = 62.357\nd = N[Im[ZetaZero[1]]]\ndatapointsdisplayed = 300;\nymin = -10;\nymax = 10;\np = 0.013;\ng1 = ListLinePlot[a[[1 ;; datapointsdisplayed]], \n   PlotRange -> {ymin, ymax}, \n   DataRange -> {0, N[Im[ZetaZero[1]]]/c*datapointsdisplayed}];\ng2 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[1]]], 0}]}];\ng3 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[2]]], 0}]}];\ng4 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[3]]], 0}]}];\ng5 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[4]]], 0}]}];\ng6 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[5]]], 0}]}];\ng7 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[6]]], 0}]}];\ng8 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[7]]], 0}]}];\ng9 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[8]]], 0}]}];\ng10 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[9]]], 0}]}];\nShow[g1, g2, g3, g4, g5, g6, g7, g8, g9, g10, ImageSize -> Large]\nN[Im[ZetaZero[Range[15]]]]\n\nwhich outputs:\n\nFigure 1.\nWhere the black dots are equal to the imaginary parts of the Riemann zeta zeros.\n\nDoes the blue curve cross the x-axis at values equal to the imaginary parts of the Riemann zeta zeros?\n\n\nEdit 21.2.2012:\nTaking the Fourier Sine Transform of the result in Figure 1:\n(*Mathematica 8*)\nClear[x]\nscale = 1000000;\nxres = .00001;\nx = Exp[Range[0, Log[scale], xres]];\na = FourierDST[FourierDCT[SawtoothWave[x]*x^(-1/2)]];\n(*b=Length[a]*)\nc = 1410000\ndatapointsdisplayed = scale;\nymin = -0.5;\nymax = 1.5;\np = 0.011;\ng1 = ListLinePlot[a[[1 ;; datapointsdisplayed]], \n   PlotRange -> {ymin, ymax}, \n   DataRange -> {0, N[Im[ZetaZero[1]]]/c*datapointsdisplayed}];\ng2 = Graphics[{PointSize[p], Point[{N[Log[2]], 0}]}];\ng3 = Graphics[{PointSize[p], Point[{N[Log[3]], 0}]}];\ng4 = Graphics[{PointSize[p], Point[{N[Log[4]], 0}]}];\ng5 = Graphics[{PointSize[p], Point[{N[Log[5]], 0}]}];\ng6 = Graphics[{PointSize[p], Point[{N[Log[6]], 0}]}];\ng7 = Graphics[{PointSize[p], Point[{N[Log[7]], 0}]}];\ng8 = Graphics[{PointSize[p], Point[{N[Log[8]], 0}]}];\ng9 = Graphics[{PointSize[p], Point[{N[Log[9]], 0}]}];\ng10 = Graphics[{PointSize[p], Point[{N[Log[10]], 0}]}];\ng11 = Graphics[{PointSize[p], Point[{N[Log[11]], 0}]}];\nShow[g1, g2, g3, g4, g5, g6, g7, g8, g9, g10, g11, ImageSize -> Large]\nN[Log[Range[11]]]\n\nwe get as suggested by draks , a spectrum with logarithms as frequencies:\n\nFigure 2.\nwhere the black dots are at x-values of $\\log(n)$ , $n=(1),2,3...$\nTrying to mimic this picture with discrete deltas:\n(*Mathematica 8*)\nClear[x, xx]\nscale = 1000000;\nxres = .00001;\nx = Exp[Range[0, Log[scale], xres]];\nxx = Flatten[{0, Differences[Floor[Exp[Range[0, Log[scale], xres]]]]}];\nListLinePlot[xx*x^(-1/2), PlotRange -> {-0.1, 0.8}, \n ImageSize -> Large]\n\nwe have:\n\nFigure 3.\n\nEdit 22.2.2012: Adjusting the resolution and scale in the Inverse Fourier Sine Transform\n(*Mathematica 8*)\nClear[x, xx]\nscale = 1000;\nxres = .000001;\nx = Exp[Range[0, Log[scale], xres]];\nxx = Flatten[{0, Differences[Floor[Exp[Range[0, Log[scale], xres]]]]}];\na = FourierDST[xx*x^(-1/2), 3];\n(*b=Length[a]*)\nc = 31.2\nvdatapointsdisplayed = 150;\nymin = -1/400;\nymax = 1/400;\np = 0.013;\ng1 = ListLinePlot[a[[1 ;; datapointsdisplayed]], \n   PlotRange -> {ymin, ymax}, \n   DataRange -> {0, N[Im[ZetaZero[1]]]/c*datapointsdisplayed}];\ng2 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[1]]], 0}]}];\ng3 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[2]]], 0}]}];\ng4 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[3]]], 0}]}];\ng5 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[4]]], 0}]}];\ng6 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[5]]], 0}]}];\ng7 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[6]]], 0}]}];\ng8 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[7]]], 0}]}];\ng9 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[8]]], 0}]}];\ng10 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[9]]], 0}]}];\ng11 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[10]]], 0}]}];\nShow[g1, g2, g3, g4, g5, g6, g7, g8, g9, g10, g11, ImageSize -> Large]\nN[Im[ZetaZero[Range[15]]]]\n\nwe get:\n\nFigure 4.\nwhere the black dots are at x-values equal to imaginary parts of the Riemann zeta zeros.\nTrying to mimic this time the plot in Figure 4 we can try a logarithmic Fourier series with square roots as dividing multiples, based on the spectrum in Figure 2.\n$$ \\frac{\\sin(\\log(1) x)}{\\sqrt 1} + \\frac{\\sin(\\log(2) x)}{\\sqrt 2} + \\frac{\\sin(\\log(3) x)}{\\sqrt 3} + ... + \\frac{\\sin(\\log(n) x)}{\\sqrt n}$$\nWhich as a Mathematica program is:\nClear[c, p, u]\nc = 4.885;\np = 0.013;\nu = N[22 Pi]\nMonitor[g1 = \n   ListLinePlot[\n    Table[Total[Table[Sin[Log[i]*x]/i^(1/2), {i, 1, 80}]], {x, 0, u, \n      0.01}], DataRange -> {0, N[Im[ZetaZero[1]]]*c}];, x]\ng2 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[1]]], 0}]}];\ng3 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[2]]], 0}]}];\ng4 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[3]]], 0}]}];\ng5 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[4]]], 0}]}];\ng6 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[5]]], 0}]}];\ng7 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[6]]], 0}]}];\ng8 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[7]]], 0}]}];\ng9 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[8]]], 0}]}];\ng10 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[9]]], 0}]}];\ng11 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[10]]], 0}]}];\ng12 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[11]]], 0}]}];\ng13 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[12]]], 0}]}];\ng14 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[13]]], 0}]}];\ng15 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[14]]], 0}]}];\ng16 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[15]]], 0}]}];\ng17 = Graphics[{PointSize[p], Point[{N[Im[ZetaZero[16]]], 0}]}];\nShow[g1, g2, g3, g4, g5, g6, g7, g8, g9, g10, g11, g12, g13, g14, \\\ng15, g16, g17, ImageSize -> Large]\n\nThis gives the plot:\n\nFigure 5.\nWhere again the black dots are at x-values equal to imaginary parts of Riemann zeta zeros.\n\nEdit 19 03 2015:\nSawtoothwaves with envelopes. \n\n\nEdit 17 01 2013:\n$$-\\text{FourierDCT}\\left[\\log (x) \\text{FourierDST}\\left[\\frac{1}{\\sqrt{x}} (\\text{SawtoothWave}[x]-1)\\right]\\right];$$\nscale = 1000000;\nxres = .00001;\nx = Exp[Range[0, Log[scale], xres]];\na = -FourierDCT[Log[x]*FourierDST[(SawtoothWave[x] - 1)*(x)^(-1/2)]];\nc = 62.357\nd = N[Im[ZetaZero[1]]]\ndatapointsdisplayed = 500000;\nymin = -0.5;\nymax = 2;\np = 0.013;\ng1 = ListLinePlot[a[[1 ;; datapointsdisplayed]], \n   PlotRange -> {ymin, ymax}, \n   DataRange -> {0, N[Im[ZetaZero[1]]]/c*datapointsdisplayed}];\nShow[g1, ImageSize -> Large]\n\n\nEdit 7.7.2014:\nRiemann zeta function from Fast Fourier Transform of exponential sawtooth wawe in Mathematica 8.0:\nscale = 1000000;\nxres = .00001;\nx = Exp[Range[0, Log[scale], xres]];\nRealPart = -Log[x]*FourierDST[(SawtoothWave[x] - 1)*x^(-1/2)];\nImaginaryPart = -Log[x]*FourierDCT[(SawtoothWave[x] + 0)*x^(-1/2)];\ndatapointsdisplayed = 300;\nymin = -0.012;\nymax = 0.018;\ng1 = ListLinePlot[{RealPart[[1 ;; datapointsdisplayed]], \n      ImaginaryPart[[1 ;; datapointsdisplayed]]}/xres/300, \n   DataRange -> {0, 68.00226987379779}, Filling -> Axis];\nShow[Flatten[{g1, \n   Table[Graphics[{PointSize[0.013], \n      Point[{N[Im[ZetaZero[n]]], 0}]}], {n, 1, 16}]}], \n ImageSize -> Large]", "output": "The answer is almost, but not quite. Why they are so close will be made clear momentarily.\nWe begin by partially evaluating (half) the usual Fourier transform up to $\\log N$:\n$$F_N(\\omega)=\\int_0^{\\log N} \\big(e^x-\\lfloor e^x\\rfloor\\big) e^{-x/2} e^{ix \\omega}dx \\tag{A}$$\n$$=\\int_1^N\\big(u-\\lfloor u\\rfloor\\big)u^{-1/2}u^{i\\omega}\\frac{du}{u} \\tag{B}$$\n$$=\\sum_{n=1}^{N-1}\\int_0^1 t(n+t)^{s-2}dt \\tag{X}$$\n$$=\\frac{1}{s-1}\\sum_{n=1}^{N-1} \\left(\\frac{n^s-(n+1)^s}{s}+(n+1)^{s-1}\\right)\\tag{Y}$$\n$$=\\frac{1}{s-1}\\left(\\frac{1-N^s}{s}+H_{N,1-s}-1\\right).\\tag{Z}$$\nAbove we write $H_{n,r}$ for the generalized harmonic number and $s=\\frac{1}{2}+i\\omega$; note $1-s=\\bar{s}$. The ever-useful Euler-Maclaurin formula provides the asymptotic form\n$$H_{N,v}=\\frac{N^{1-v}}{1-v}+\\zeta(v)+\\mathcal{O}\\left(N^{-1/2}\\right) \\tag{C}$$\nSee Numerical Evaluation of the Riemann Zeta function (the very first equation). Also see the work given in answers to this Math.SE question.\nPlugging $(6)$ into $(5)$ into $F_N(\\omega)+F_N(-\\omega)$, we obtain\n$$\\frac{1}{s-1}\\left(\\frac{1}{s}+\\zeta(1-s)-1\\right)+\\frac{1}{-s}\\left(\\frac{1}{1-s}+\\zeta(s)-1\\right)+\\mathcal{O}\\left(N^{-1/2}\\right). \\tag{D}$$\nUsing the formulas $\\overline{\\alpha\\beta}=\\overline{\\alpha}\\overline{\\beta\\,}$, $1-s=\\overline{s}$, $z+\\overline{z}=2\\mathrm{Re}(z)$, $w\\overline{w}=|w|^2$, and the formula for the cosine transform as the limit of half-partial Fourier transforms, $\\displaystyle C(\\omega)=\\lim_{N\\to\\infty}\\frac{F_N(\\omega)+F_N(-\\omega)}{\\sqrt{2\\pi}}$, we get\n$$C(\\omega)=-\\sqrt{\\frac{2}{\\pi}}\\left[\\frac{1}{|s|^2}+\\operatorname{Re}\\left(\\frac{\\zeta(s)-1}{s}\\right)\\right], \\tag{L}$$\nand a similar computation shows the Fourier sine transform is (as you note in the comments)\n$$S(\\omega)=\\sqrt{\\frac{2}{\\pi}}\\mathrm{Im}\\left(\\frac{\\zeta(s)-1}{s}\\right). \\tag{R}$$\nClearly plugging in nontrivial roots $s$ of $\\zeta(s)$ into $(L)$ and $(R)$ will yield fairly small values, just about inversely proportional to the modulus of $s$. This explains the numerical coincidence.", "meta": {"post_id": 107483, "input_score": 40, "output_score": 34, "post_title": "Are these zeros equal to the imaginary parts of the Riemann zeta zeros?"}}
{"input": "I am new in this forum. \nMy question:\nSuppose a real valued function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is continuous everywhere. Is it possible to construct $f$ that is differentiable at only one point? If possible, please give an example.\nNote: \nI am aware that there is a function which is differentiable at a single point but discontinuous elsewhere. I also know about Weierstrass function that continuous everywhere but nowhere differentiable. But is there a function which is continuous but only differentiable in one point?\nIn fact, I found this discussion but unfortunately it still does not give a definitive answer. Moreover they consider only in an interval, whereas my problem is for the entire domain. Thank you very much", "output": "It is certainly possible. Fix a nowhere-differentiable function $f$ such that $0\\leq f(x)\\leq 1$ for all $x$. Now consider $x^2f(x)$. This is differentiable at $0$ but nowhere else. You can verify it is differentiable at $0$ using the limit definition of derivative.\n$$\\lim_{h\\to 0} \\frac{h^2f(h)-0^2f(0)}{h}=\\lim_{h\\to 0}hf(h)$$\nand $0\\leq f(h)\\leq 1$ implies $0\\leq hf(h)\\leq h$. So the limit goes to $0$ by the squeeze theorem.\nTo see it is not differentiable elsewhere is a slightly harder exercise. Suppose $x^2f(x)$ is differentiable at $x\\neq 0$. Then \n$$\\lim_{h\\to 0} \\frac{(x+h)^2f(x+h)-x^2f(x)}{h}=L.$$\nAdding and subtracting a mixed term $x^2f(x+h)$ in the middle, this becomes\n$$\r\n\\lim_{h\\to 0} \\frac{(x+h)^2f(x+h)-x^2f(x+h)}{h}+\\frac{x^2f(x+h)-x^2f(x)}{h}=L\r\n$$\nThe left-hand term limits to $2x f(x).$ The right-hand term limits to $x^2f'(x)$. This implies that $f'(x)$ exists, since it is equal to $x^{-2}(2xf(x)-L)$. (This fails for $x=0$.)", "meta": {"post_id": 108388, "input_score": 40, "output_score": 39, "post_title": "Function which is continuous everywhere in its domain, but differentiable only at one point"}}
{"input": "Is a set of convex functions closed under composition? I don't necessarily need a proof, but a reference would be greatly appreciated.", "output": "There is no need for the first function in the composition to be nondecreasing. And here is a proof for the nondifferentiable case as well. The only assumptions are that the composition is well defined at the points involved in the proof for every $\\alpha \\in [0, 1]$ and that $f_n, f_{n - 1}, \\dots, f_1$ are convex nondecreasing functions of one variable and that $f_0 : \\mathbb R^n \\to \\mathbb R$ is a convex function.\nFirst let $g : \\mathbb R^m \\to \\mathbb R$ a convex function and $f : \\mathbb R \\to \\mathbb R$ a convex nondecreasing function, then, by convexity of $g$:\n$$\ng( \\alpha x + ( 1 - \\alpha ) y ) \\leq \\alpha g( x ) + ( 1 - \\alpha )g( y ).\n$$\nSo, using the fact that f is nondecreasing:\n$$\nf( g( \\alpha x + ( 1 - \\alpha ) y ) ) \\leq f( \\alpha g( x )+ ( 1 - \\alpha )g( y ) ).\n$$\nTherefore, again by convexity:\n$$\nf( g( \\alpha x + ( 1 - \\alpha ) y ) ) \\leq \\alpha f( g( x ) ) + ( 1 - \\alpha )f( g( y ) ).\n$$\nThis reasoning can be used inductively in order to prove the result that\n$$\nf_n \\circ f_{n - 1}\\circ\\cdots\\circ f_0\n$$\nis convex under the stated hypothesis. And the composition will be nondecreasing if $f_0$ is nondecreasing.", "meta": {"post_id": 108393, "input_score": 50, "output_score": 45, "post_title": "Is the composition of $n$ convex functions itself a convex function?"}}
{"input": "I found this symbol on Wolfram|Alpha. Does it mean \"or\"?\n$$ \\large \\cos^{-1}(-1)=\\mathrm{cd}^{-1}(-1\\mid 0)$$", "output": "Since I don't believe any of the previous two answers explained the notation properly (which I guess is forgivable since knowledge of elliptic stuff is not as common as it once was), here's my take:\nThe funny thing about the theory of elliptic integrals and elliptic functions is that people use related, but rather completely different conventions and notation. One guy has his favorite set, one has another way of doing things; heck, I have a particular preference myself. I'll explain three of them.\nThe incomplete elliptic integral of the first kind can be defined in at least three ways:\n$$\\begin{align*}\nF(\\phi,k)&=\\int_0^\\phi \\frac{\\mathrm du}{\\sqrt{1-k^2\\sin^2 u}}\\\\\nF(\\phi\\mid m)&=\\int_0^\\phi \\frac{\\mathrm du}{\\sqrt{1-m\\,\\sin^2 u}}\\\\\nF(\\phi\\backslash \\alpha)&=\\int_0^\\phi \\frac{\\mathrm du}{\\sqrt{1-\\sin^2\\alpha\\,\\sin^2 u}}\n\\end{align*}$$\nYes, kids, this is a Spot The Difference game! If you compare these three definitions, we have the relationship\n$$m=k^2=\\sin^2\\alpha$$\nNow, $m$ is what's called a parameter; $k$ is what's called a modulus; and, $\\alpha$ is termed the modular angle. Whichever of these arguments one is concerned with in elliptic integrals is easily indicated by the choice of delimiter: comma for modulus, pipe/bar for parameter, and backslash for modular angle.\nSince Jacobian elliptic functions can be constructed in terms of the inverse of the incomplete elliptic integral of the first kind (what is called the Jacobi amplitude, $\\mathrm{am}(u,k)$/$\\mathrm{am}(u\\mid m)$/$\\mathrm{am}(u\\backslash \\alpha)$), and since the inverse Jacobian elliptic functions can be expressed as compositions of the incomplete elliptic integral of the first kind with inverse trigonometric functions (in particular, we have $\\mathrm{cd}^{-1}(w\\mid m)=F\\left(\\dfrac{\\pi}{2}\\mid m\\right)-F(\\arcsin\\,w\\mid m)$), they too inherit the delimiter convention used for elliptic integrals. (Yes, it's a rather confusing system, but there you are.)\nSo that's it: Wolfram Alpha likes using the parameter convention for its elliptic integrals and elliptic functions, which is why you see the nice pipe cleanly separating the arguments of the inverse Jacobian elliptic function in Alpha's output.", "meta": {"post_id": 108575, "input_score": 5, "output_score": 36, "post_title": "What does | mean?"}}
{"input": "Show that a totally bounded complete metric space $X$ is compact.\n\nI can use the fact that sequentially compact $\\Leftrightarrow$ compact. \nAttempt: Complete $\\implies$ every Cauchy sequence converges. Totally bounded $\\implies$ $\\forall\\epsilon>0$, $X$ can be covered by a finite number of balls of radius $\\epsilon$. I'm trying to show that all sequences in $X$ have a subsequence that converges to an element in $X$. I don't see how to go from convergent Cauchy sequences and totally bounded to subsequence convergent $in$ $X$.", "output": "You need to show that if $X$ is totally bounded, every sequence in $X$ has a Cauchy subsequence. Let $\\sigma=\\langle x_n:n\\in\\mathbb{N}\\rangle$ be a sequence in $X$. For each $n\\in\\mathbb{N}$ let $D_n$ be a finite subset of $X$ such that the open balls of radius $2^{-n}$ centred at the points of $D_n$ cover $X$. $D_0$ is finite, so there is a point $y_0\\in D_0$ such that infinitely many terms of $\\sigma$ are in $B(y_0,1)$. Let $$A_0=\\{n\\in\\mathbb{N}:x_n\\in B(y_0,1)\\}\\;,$$ so that $A_0$ is infinite. Now $D_1$ is finite, so there is a $y_1\\in D_1$ such that $$A_1=\\{n\\in A_0:x_n\\in B(y_1,2^{-1})\\}$$ is infinite. Repeat: if $A_k$ is an infinite subset of $\\mathbb{N}$, there must be a $y_{k+1}\\in D_{k+1}$ such that $$A_{k+1}=\\{n\\in A_k:x_n\\in B(y_{k+1},2^{-(k+1)})\\}$$ is infinite, and the process can continue.\nNow choose a strictly increasing sequence $\\langle n_k:k\\in\\mathbb{N}\\rangle$ of natural numbers in such a way that $n_k\\in A_k$ for every $k\\in\\mathbb{N}$. Can you show that $\\langle x_{n_k}:k\\in\\mathbb{N}\\rangle$ is Cauchy?", "meta": {"post_id": 109550, "input_score": 42, "output_score": 49, "post_title": "Totally bounded, complete $\\implies$ compact"}}
{"input": "An equation that seems to come up everywhere is the transcendental $\\tan(x) = x$. Normally when it comes up you content yourself with a numerical solution usually using Newton's method. However, browsing today I found an asymptotic formula for the positive roots $x$:\n$x = q - q^{-1} - \\frac23 q^{-3} + \\cdots$\nwith $q = (n + 1/2) \\pi$ for positive integers $n$. For instance here: http://mathworld.wolfram.com/TancFunction.html, and here: http://mathforum.org/kb/message.jspa?messageID=7014308 found from a comment here: Solution of tanx = x?.\nThe Mathworld article says that you can derive this formula using series reversion, however I'm having difficulty figuring out exactly how to do it.\nAny help with a derivation would be much appreciated.", "output": "You may be interested in N. G. de Bruijn's book Asymptotic Methods in Analysis, which treats the equation $\\cot x = x$.  What follows is essentially a minor modification of that section in the book.\nThe central tool we will use is the Lagrange inversion formula.  The formula given in de Bruijn differs slightly from the one given on the wiki page so I'll reproduce it here.\n\nLagrange Inversion Formula.\nLet the function $f(z)$ be analytic in some neighborhood of the point $z=0$ of the complex plane.  Assuming that $f(0) \\neq 0$, we consider the equation $$w = z/f(z),$$ where $z$ is the unknown.  Then there exist positive numbers $a$ and $b$ such that for $|w| < a$ the equation has just one solution in the domain $|z| < b$, and this solution is an analytic function of $w$: $$z = \\sum_{k=1}^{\\infty} c_k w^k \\hspace{1cm} (|w| < a),$$ where the coefficients $c_k$ are given by $$c_k = \\frac{1}{k!} \\left\\{\\left(\\frac{d}{dz}\\right)^{k-1} (f(z))^k\\right\\}_{z=0}.$$\n\nEssentially what this says is that we can solve the equation $w = z/f(z)$ for $z$ as a power series in $w$ when $|w|$ and $|z|$ are small enough.\nOkay, on to the problem.  We wish to solve the equation $$\\tan x = x.$$  As with many asymptotics problems, we need a foothold to get ourselves going.  Take a look at the graphs of $\\tan x$ and $x$:\n\nWe see that in each interval $\\left(\\pi n - \\frac{\\pi}{2}, \\pi n + \\frac{\\pi}{2}\\right)$ there is exactly one solution $x_n$ (i.e. $\\tan x_n = x_n$), and, when $n$ is large, $x_n$ is approximately $\\pi n + \\frac{\\pi}{2}$.  But how do we show this second part?\nSince $\\tan$ is $\\pi$-periodic we have\n$$\\tan\\left(\\pi n + \\frac{\\pi}{2} - x_n\\right) = \\tan\\left(\\frac{\\pi}{2} - x_n\\right)$$\n$$\\hspace{2.4 cm} = \\frac{1}{\\tan x_n}$$\n$$\\hspace{2.6 cm} = \\frac{1}{x_n} \\to 0$$\nas $n \\to \\infty$, where the second-to-last equality follows from the identites $$\\sin\\left(\\frac{\\pi}{2} - \\theta\\right) = \\cos \\theta,$$ $$\\cos\\left(\\frac{\\pi}{2} - \\theta\\right) = \\sin \\theta.$$\nSince $-\\frac{\\pi}{2} < \\pi n + \\frac{\\pi}{2} - x_n < \\frac{\\pi}{2}$ and since $\\tan$ is continuous in this interval we have $\\pi n + \\frac{\\pi}{2} - x_n \\to 0$ as $n \\to \\infty$.  Thus we have shown that $x_n$ is approximately $\\pi n + \\frac{\\pi}{2}$ for large $n$.\nNow we begin the process of putting the equation $\\tan x = x$ into the form required by the Lagrange inversion formula.  Set $$z = \\pi n + \\frac{\\pi}{2} - x$$ and $$w = \\left(\\pi n + \\frac{\\pi}{2}\\right)^{-1}.$$  Note that we do this because when $|w|$ is small (i.e. when $n$ is large) we may take $|z|$ small enough such that there will be only one $x$ (in the sense that $x = \\pi n + \\frac{\\pi}{2} - z$) which satisfies $\\tan x = x$.  Plugging $x = w^{-1} - z$ into the equation $\\tan x = x$ yields, after some simplifications along the lines of those already discussed, $$\\cot z = w^{-1} - z,$$ which rearranges to $$w = \\frac{\\sin z}{\\cos z + z\\sin z} = z/f(z),$$ where $$f(z) = \\frac{z(\\cos z + z\\sin z)}{\\sin z}.$$  Here note that $f(0) = 1$ and that $f$ is analytic at $z = 0$.  We have just satisfied the requirements of the inversion formula, so we may conclude that we can solve $w = z/f(z)$ for $z$ as a power series in $w$ in the form given earlier in the post.\nWe have $c_1 = 1$ and, since $f$ is even, it can be shown that $c_{2k} = 0$ for all $k$.  Calculating the first few coefficients in Mathematica gives $$z = w + \\frac{2}{3}w^3 + \\frac{13}{15}w^5 + \\frac{146}{105}w^7 + \\frac{781}{315}w^9 + \\frac{16328}{3465}w^{11} + \\cdots.$$  Substituting this into $x = w^{-1} - z$ and using $w = \\left(\\pi n + \\frac{\\pi}{2}\\right)^{-1}$ gives the desired series for $x_n$ when $n$ is large enough: $$x_n = \\pi n + \\frac{\\pi}{2} - \\left(\\pi n + \\frac{\\pi}{2}\\right)^{-1} - \\frac{2}{3}\\left(\\pi n + \\frac{\\pi}{2}\\right)^{-3} - \\frac{13}{15}\\left(\\pi n + \\frac{\\pi}{2}\\right)^{-5} - \\frac{146}{105}\\left(\\pi n + \\frac{\\pi}{2}\\right)^{-7} - \\frac{781}{315}\\left(\\pi n + \\frac{\\pi}{2}\\right)^{-9} - \\frac{16328}{3465}\\left(\\pi n + \\frac{\\pi}{2}\\right)^{-11} + \\cdots$$", "meta": {"post_id": 110256, "input_score": 24, "output_score": 39, "post_title": "Derivation of asymptotic solution of $\\tan(x) = x$."}}
{"input": "In the process of touching up some notes on infinite series, I came across the following \"result\":\n\nTheorem: For an ordered field $(F,<)$, the following are equivalent:\n(i) Every Cauchy sequence in $F$ is convergent.\n(ii) Absolutely convergent series converge: $\\sum_n |a_n|$ converges in $F$ $\\implies$ $\\sum_n a_n$ converges in $F$.\n\nBut at present only the proof of (i) $\\implies$ (ii) is included, and unfortunately I can no longer remember what I had in mind for the converse direction.  After thinking it over for a bit, I wonder if I was confusing it with this result:\n\nProposition: In a normed abelian group $(A,+,|\\cdot|)$, the following are equivalent:\n(i) Every Cauchy sequence is convergent.\n(ii) Absolutely convergent series converge: $\\sum_n |a_n|$ converges in $\\mathbb{R}$ $\\implies$ $\\sum_n a_n$ converges in $A$.\n\nFor instance, one can use a telescoping sum argument, as is done in the case of normed linear spaces over $\\mathbb{R}$ in (VIII) of this note.\nBut the desired result is not a special case of this, because by definition the norm on a normed abelian group takes values in $\\mathbb{R}^{\\geq 0}$, whereas the absolute value on an ordered field $F$ takes values in $F^{\\geq 0}$.\nI can show (ii) $\\implies$ (i) of the Theorem for ordered subfields of $\\mathbb{R}$.  Namely, every real number $\\alpha$ admits a signed binary expansion $\\alpha = \\sum_{n = N_0}^{\\infty} \\frac{\\epsilon_n}{2^n}$, with $N_0 \\in \\mathbb{Z}$ and $\\epsilon_n \\in \\{ \\pm 1\\}$, and the associated \"absolute series\" is $\\sum_{n=N_0}^{\\infty} \\frac{1}{2^n} = 2^{1-N_0}$.\nBecause an ordered field is isomorphic to an ordered subfield of $\\mathbb{R}$ iff it is Archimedean, this actually proves (ii) $\\implies$ (i) for Archimedean ordered fields.  But on the one hand I would prefer a proof of this that does not use the (nontrivial) result of the previous sentence, and on the other hand...what about non-Archimedean ordered fields?\nAdded: The article based on this question and answer has at last appeared:\n\nClark, Pete L.; Diepeveen, Niels J.;\nAbsolute Convergence in Ordered Fields.\nAmer. Math. Monthly 121 (2014), no. 10, 909\u2013916.\n\nIf you are a member of the MAA, you will be frustrated if you try to access it directly: the issue is currently advertised on their website but the articles are not actually available to members.  The article is available on JSTOR and through MathSciNet.  Anyway, here is an isomorphic copy.  Thanks again to Niels Diepeveen!", "output": "Proving this for arbitrary ordered fields is a little\ntrickier than for Archimedean fields, partly because\nthere are no concrete sequences -- other than eventually\nconstant ones -- that are guaranteed to converge or\neven to be Cauchy sequences and partly because we lack\nthe embedding in a completely ordered field.\nThese problems can be overcome by constructing all the\nnecessary sequences and series from the one Cauchy\nsequence we assume to exist. To begin with, note two\nimportant facts. First, for a Cauchy sequence to\nconverge, it is sufficient that some subsequence\nconverges. Second, any sequence has a strictly increasing\nsubsequence, a strictly decreasing subsequence or a\nconstant subsequence. For this problem, the latter\ncase is trivial and the first two can be reduced to each\nother by negation, so we need to prove only one of them.\nLet $K$ be an ordered field in which every absolutely\nconvergent series is convergent. If $\\{a_n\\}$ is a strictly\nincreasing Cauchy sequence in $K$, then $\\{a_n\\}$ converges.\nProof:\nLet $b_n = a_{n+1} - a_n$. Then ${b_n}$ is positive and\nconverges to $0$, so it has a strictly decreasing\nsubsequence $\\{b_{n_k}\\}$.\nLet $c_k = b_{n_k} - b_{n_{k+1}}$. We now have a convergent\nseries with positive terms $\\sum_{k=1}^\\infty c_k = b_{n_1}$.\nAs $\\{a_n\\}$ is a Cauchy sequence, it has a subsequence $\\{a_{m_k}\\}$\nsuch that $a_{m_{k+1}} - a_{m_k} < c_k$ for all $k$.\nNow consider the series $\\sum_{i=1}^\\infty d_i$ where\n$d_{2k-1} = a_{m_{k+1}} - a_{m_k}$ and\n$d_{2k} = a_{m_{k+1}} - a_{m_k} - c_k$.\nNote that $-c_k < d_{2k} < 0 < d_{2k-1} < c_k$, so we can pair off\nterms to get\n$$\r\n\\sum_{i=1}^\\infty |d_i| = \\sum_{k=1}^\\infty (d_{2k-1} - d_{2k})\r\n  = \\sum_{k=1}^\\infty c_k = b_{n_1}\r\n$$\nBy the hypothesis on $K$ we may conclude that $\\sum_{i=1}^\\infty d_i$\nconverges and\n$$\r\n\\sum_{i=1}^\\infty d_i + \\sum_{k=1}^\\infty c_k\r\n  = \\sum_{k=1}^\\infty (d_{2k-1} + d_{2k} + c_k)\r\n  = 2 \\sum_{k=1}^\\infty (a_{m_{k+1}} - a_{m_k}).\r\n$$\nBecause a Cauchy sequence with a convergent subsequence converges,\nwe have\n$$\r\n\\lim_{n \\to \\infty} a_n = a_{m_1} + \\sum_{k=1}^\\infty (a_{m_{k+1}} - a_{m_k})\r\n  = a_{m_1} + \\frac{1}{2}\\left(b_{n_1} + \\sum_{i=1}^\\infty d_i \\right)\r\n$$\n\nTo the question \"how did I come up with this?\": there are\nnot many things that could possibly work. The problem is\nset in an environment where none of the power tools of\nanalysis work. Basic arithmetic works, inequalities work,\nsome elementary properties of sequences and series work, but\nif you want to take a limit of something it'd better be\nconvergent by hypothesis or by construction.\nOne more or less obvious attack is by contraposition: assume\nthat there is a divergent Cauchy sequence and try to construct\na divergent, absolutely convergent series. Such a series\nmust be decomposable into a positive part $a$ and a negative\npart $b$, where $a+b$ diverges and $a-b$ converges. This\nis possible in several ways by taking $a$ and $b$ to be linear\ncombinations of known convergent and divergent series.\nA complication is that the terms of the convergent series\nmust dominate those of the divergent series, as they must\ncontrol the signs. I wasted a lot of time trying to get the\nconvergent series to do this, which is very hard, perhaps\nimpossible. Then I turned to the proof for vector spaces\nfor inspiration, and saw that it was in fact very easy to\nadjust the divergent series instead, as the partial sums\nare a Cauchy sequence. I also adopted the\noverall structure of that proof, which is why the final\nversion is not by contraposition.", "meta": {"post_id": 111164, "input_score": 57, "output_score": 45, "post_title": "In which ordered fields does absolute convergence imply convergence?"}}
{"input": "I was wondering if some one could please shed some light on why or how a dual space itself becomes a vector space over the field. Finite-Dimensional Vector Spaces by Paul Halmos states:\n\n.\u00a0.\u00a0. to every vector space V we make correspond the dual space $V^*$ consisting of all the linear functionals on $V$.\u00a0.\u00a0.\u00a0.\u00a0\n \u2014 p. 21, notation edited \n\nThe book goes on to present the defining property of a linear functional and the definition of the linear operations for linear functionals.\nAlso, for the sake of completion, a linear functional is defined by the text as\n\na scalar-valued function $y$ defined for every vector $x$, with the property that (identically in the vectors $x_{1}$ and $x_{2}$ and the scalar $\\alpha _{1}$ and $\\alpha _{2}$)\n$$y( \\alpha _{1}x_{1}+\\alpha _{2}x_{2}) =\\alpha _{1}\\,y\\left( x_{1}\\right) +\\alpha _{2}\\,y\\left( x_{2}\\right)$$\n \u2014 p. 20 \n\nBased on these definitions, isn't $V^*$ composed of scalar-valued functions with the above property? I fail to see any vectors present in $V^*$. Yet the book later assumes that we must know that and starts defining a dual space $V^{**}$ of a dual space $V^*$ of a vector space $V$.\nAny help would be much appreciated.", "output": "Let's go back further:\nLet $\\mathbf{V}$ and $\\mathbf{W}$ be any two vector spaces over the same field $\\mathbf{F}$. Let $\\mathcal{L}(\\mathbf{V},\\mathbf{W})$ be the set of linear transformations $T\\colon \\mathbf{V}\\to\\mathbf{W}$.\nWe will make $\\mathcal{L}(\\mathbf{V},\\mathbf{W})$ into a vector space over $\\mathbf{F}$. In order to do this, we need to define an \"addition of linear transformations\" and a \"scalar multiplication of elements of $\\mathbf{F}$ by linear transformations\" (that is, our \"vectors\" will be linear transformations from $\\mathbf{V}$ to $\\mathbf{W}$; remember that a vector space is just a set with a \"vector addition\" and a \"scalar multiplication\" that satisfy certain properties, and we call the elements of the set \"vectors\"; they don't have to be \"tuples\" in the usual sense).\nSo, given two linear transformations $T,U\\colon \\mathbf{V}\\to\\mathbf{W}$, we need to define a new linear transformation that is called the \"sum of $T$ and $U$\". I'm going to write this as $T\\oplus U$, to distinguish the \"sum of linear transformations\" from the sum of vectors. Since we want $T\\oplus U$ to be a linear transformation (which is a special kind of function) from $\\mathbf{V}$ to $\\mathbf{W}$, in order to specify it we need to say what the value of $T\\oplus U$ is at every $\\mathbf{v}\\in \\mathbf{V}$. My definition is:\n$$(T\\oplus U)(\\mathbf{v}) = T(\\mathbf{v}) + U(\\mathbf{v}),$$\nwhere the sum on the right is taking place in $\\mathbf{W}$. This makes sense, because $T$ and $U$ are already functions from $\\mathbf{V}$ to $\\mathbf{W}$, so $T(\\mathbf{v})$ and $U(\\mathbf{v})$ are vectors in $\\mathbf{W}$, which we can add.\nIs $T\\oplus U$ a linear transformation from $\\mathbf{V}$ to $\\mathbf{W}$? First, it is a function from $\\mathbf{V}$ to $\\mathbf{W}$. Now, to check that it is a linear transformation, we need to check that for all $\\mathbf{v}_1,\\mathbf{v}_2\\in\\mathbf{V}$ and all $\\alpha\\in \\mathbf{F}$, we have\n$$(T\\oplus U)(\\mathbf{v}_1+\\mathbf{v}_2) = (T\\oplus U)(\\mathbf{v}_1)+(T\\oplus U)(\\mathbf{v}_2)\\quad\\text{and}\\quad (T\\oplus U)(\\alpha\\mathbf{v}_1) = \\alpha((T\\oplus U)(\\mathbf{v}_1)).$$\nIndeed, since $T$ and $U$ are themselves linear transformations, we have:\n$$\\begin{align*}\n(T\\oplus U)(\\mathbf{v}_1+\\mathbf{v}_2) &= T(\\mathbf{v}_1+\\mathbf{v}_2) + U(\\mathbf{v}_1+\\mathbf{v}_2) &\\text{(by definition of }T\\oplus U\\text{)}\\\\\n&= T(\\mathbf{v}_1)+T(\\mathbf{v}_2) + U(\\mathbf{v}_1)+U(\\mathbf{v}_2) &\\text{(by linearity of }T\\text{ and }U\\text{)}\\\\\n&= T(\\mathbf{v}_1)+U(\\mathbf{v}_1) + T(\\mathbf{v}_2)+U(\\mathbf{v}_2)\\\\\n&= (T\\oplus U)(\\mathbf{v}_1) + (T\\oplus U)(\\mathbf{v}_2) &\\text{(by definition of }T\\oplus U\\text{)}\\\\\n(T\\oplus U)(\\alpha\\mathbf{v}_1) &= T(\\alpha\\mathbf{v}_1) + U(\\alpha\\mathbf{v}_1) &\\text{(by definition of }T\\oplus U\\text{)}\\\\\n&= \\alpha T(\\mathbf{v}_1) + \\alpha U(\\mathbf{v}_1) &\\text{(by linearity of }T\\text{ and }U\\text{)}\\\\\n&= \\alpha(T(\\mathbf{v}_1) + U(\\mathbf{v}_1))\\\\\n&= \\alpha((T\\oplus U)(\\mathbf{v}_1)) &\\text{(by definition of }T\\oplus U\\text{)}\n\\end{align*}$$\nso $T\\oplus U$ is indeed an element of $\\mathcal{L}(\\mathbf{V},\\mathbf{W})$.\nI'll let you verify that $(S\\oplus T)\\oplus U = S\\oplus (T\\oplus U)$ for all $S,T,U\\in\\mathcal{L}(\\mathbf{V},\\mathbf{W})$ (since this is an equality of functions, you need to check that they have the same value at every $\\mathbf{v}\\in \\mathbf{V}$). That $T\\oplus U=U\\oplus T$ for all $T,U\\in\\mathcal{L}(\\mathbf{V},\\mathbf{W})$; that if $\\mathbf{0}$ is the linear transformation that sends every $\\mathbf{v}\\in\\mathbf{V}$ to $\\mathbf{0}\\in\\mathbf{W}$, then $T\\oplus\\mathbf{0}=T$ for all $T$; and that given $T\\in\\mathcal{L}(\\mathbf{V},\\mathbf{W})$, and we define $-T$ to be the function $(-T)(\\mathbf{v}) = -(T(\\mathbf{v}))$, then $T\\oplus (-T) = \\mathbf{0}$.\nNow we define a scalar multiplication, which I will denote by $\\odot$ (again, to avoid confusion with the scalar multiplication from $\\mathbf{V}$ and $\\mathbf{W}$. Given $T\\colon \\mathbf{V}\\to\\mathbf{W}$ and $\\alpha\\in\\mathbf{F}$, define $(\\alpha\\odot T)$ to be the function \n$$(\\alpha\\odot T)(\\mathbf{v}) = \\alpha T(\\mathbf{v}).$$\nI will let you verify that this definition works, in that $\\alpha\\odot T$ is a linear transformation when $T$ is a linear transformation; and that it satisfies the necessary properties:\n\n$\\alpha\\odot(\\beta\\odot T) = (\\alpha\\beta)\\odot T$;\n$1\\odot T = T$;\n$(\\alpha + \\beta)\\odot T = (\\alpha\\odot T)\\oplus (\\beta\\odot T)$;\n$\\alpha\\odot(T\\oplus U) = (\\alpha\\odot T)\\oplus (\\alpha\\odot U)$.\n\nSo $(\\mathcal{L}(\\mathbf{V},\\mathbf{W}),\\oplus,\\odot)$ is a vector space over $\\mathbf{F}$ whenever $\\mathbf{V}$ and $\\mathbf{W}$ are vector spaces over $\\mathbf{F}$. \n\nSo now, dual spaces: Note that $\\mathbf{F}$ is always a vector space over itself, by defining vector addition to be the same as the addition of $\\mathbf{F}$, and scalar multiplication to be the same as multiplication in $\\mathbf{F}$.\nSo if $\\mathbf{V}$ is any vector space over $\\mathbf{F}$, then we can consider $\\mathcal{L}(\\mathbf{V},\\mathbf{F})$: this makes sense, because both $\\mathbf{V}$ and $\\mathbf{F}$ are vector spaces over $\\mathbf{F}$; and this is itself a vector space over $\\mathbf{F}$ with vector addition $\\oplus$ and scalar multiplication $\\odot$ as defined above.\nThis vector space, $\\mathcal{L}(\\mathbf{V},\\mathbf{F})$, is called the dual space of $\\mathbf{V}$. We write $\\mathbf{V}^*$ instead of $\\mathcal{L}(\\mathbf{V},\\mathbf{F})$, and the elements of $\\mathbf{V}^*$ are called \"functionals\".\nBy abuse of notation, we usually write $+$ instead of $\\oplus$ (just like we use the same symbol for the addition of $\\mathbf{V}$ and the addition of $\\mathbf{W}$), and $\\cdot$ or just juxtaposition instead of  $\\odot$. \nThe equation you have,\n$$y(\\alpha_1 x_1 + \\alpha_2x_2) = \\alpha_1y(x_1) + \\alpha_2y(x_2)$$\nis just telling you that the function $y$ is a linear transformation from $\\mathbf{V}$ to $\\mathbf{F}$. \nIt is traditional to use boldface lower case letters like $\\mathbf{f}$, $\\mathbf{g}$, $\\mathbf{h}$ to represent functionals. This to remind us that even though they are vectors in the vector space $\\mathbf{V}^*$, they are \"really\" functions (when they are at home).\n\nIn fact, you could go back even further. If $\\mathbf{W}$ is a vector space over $\\mathbf{F}$, and $X$ is any set, then we can look at\n$$\\mathcal{F}(X,\\mathbf{W}) = \\{f\\colon X\\to\\mathbf{W}\\mid f\\text{ is a function}\\}.$$\nThen $\\mathcal{F}(X,\\mathbf{W})$ is a vector space, with addition $(f\\oplus g)(x) = f(x)+g(x)$ and scalar multiplication $(\\alpha\\odot f)(x) = \\alpha f(x)$.  The case of $\\mathcal{L}(\\mathbf{V},\\mathbf{W})$ corresponds to looking at a subspace of $\\mathcal{F}(\\mathbf{V},\\mathbf{W})$ consisting of linear transformations.\nThis is a standard construction in abstract algebra. Whenever $A$ is an algebra (in the sense of General Algebra; a group, semigroup, ring, vector space, lattice, etc), and $X$ is a set, the collection of all function $f\\colon X\\to A$ becomes an algebra of the same type under \"pointwise operations\". In fact, this is nothing more than a \"direct power\" (a direct product in which every factor is the same) indexed by $X$.", "meta": {"post_id": 111371, "input_score": 15, "output_score": 45, "post_title": "Why is a dual space a vector space?"}}
{"input": "There is a well-known result in elementary analysis due to Darboux which says if $f$ is a differentiable function then $f'$ satisfies the intermediate value property.  To my knowledge, not many \"highly\" discontinuous Darboux functions are known--the only one I am aware of being the Conway base 13 function--and few (none?) of these are derivatives of differentiable functions.  In fact they generally cannot be since an application of Baire's theorem gives that the set of continuity points of the derivative is dense $G_\\delta$.\nIs it known how sharp that last result is?  Are there known Darboux functions which are derivatives and are discontinuous on \"large\" sets in some appropriate sense?", "output": "What follows is taken (mostly) from more extensive discussions in the following sci.math posts:\nhttp://groups.google.com/group/sci.math/msg/814be41b1ea8c024 [23 January 2000]\nhttp://groups.google.com/group/sci.math/msg/3ea26975d010711f [6 November 2006]\nhttp://groups.google.com/group/sci.math/msg/05dbc0ee4c69898e [20 December 2006]\nNote: The term interval is restricted to nondegenerate intervals (i.e. intervals containing more than one point).\nThe continuity set of a derivative on an open interval $J$ is dense in $J.$ In fact, the continuity set has cardinality $c$ in every subinterval of $J.$ On the other hand, the discontinuity set $D$ of a derivative can have the following properties:\n\n$D$ can be dense in $\\mathbb R$.\n$D$ can have cardinality $c$ in every interval.\n$D$ can have positive measure. (Hence, the function can fail to be Riemann integrable.)\n$D$ can have positive measure in every interval.\n$D$ can have full measure in every interval (i.e. measure zero complement).\n$D$ can have a Hausdorff dimension zero complement.\n$D$ can have an $h$-Hausdorff measure zero complement for any specified Hausdorff measure function $h.$\n\nMore precisely, a subset $D$ of $\\mathbb R$ can be the discontinuity set for some derivative if and only if $D$ is an $F_{\\sigma}$ first category (i.e. an $F_{\\sigma}$ meager) subset of $\\mathbb R.$\nThis characterization of the discontinuity set of a derivative can be found in the following references: Benedetto [1] (Chapter 1.3.2, Proposition, 1.10, p. 30); Bruckner [2] (Chapter 3, Section 2, Theorem 2.1, p. 34); Bruckner/Leonard [3] (Theorem at bottom of p. 27); Goffman [5] (Chapter 9, Exercise 2.3, p. 120 states the result); Klippert/Williams [7].\nRegarding this characterization of the discontinuity set of a derivative, Bruckner and Leonard [3] (bottom of p. 27) wrote the following in 1966: Although we imagine that this theorem is known, we have been unable to find a reference. I have found the result stated in Goffman's 1953 text [5], but nowhere else prior to 1966 (including Goffman's Ph.D. Dissertation).\nInterestingly, in a certain sense most derivatives have the property that $D$ is large in all of the ways listed above (#1 through #7).\nIn 1977 Cliff Weil [8] published a proof that, in the space of derivatives with the sup norm, all but a first category set of such functions are discontinuous almost everywhere (in the sense of Lebesgue measure). When Weil's result is paired with the fact that derivatives (being Baire $1$ functions) are continuous almost everywhere in the sense of Baire category, we get the following:\n(A) Every derivative is continuous at the Baire-typical point.\n(B) The Baire-typical derivative is not continuous at the Lebesgue-typical point.\nNote that Weil's result is stronger than simply saying that the Baire-typical derivative fails to be Riemann integrable (i.e. $D$ has positive Lebesgue measure), or even stronger than saying that the Baire-typical derivative fails to be Riemann integrable on every interval. Note also that, for each of these Baire-typical derivatives, $\\{D, \\; {\\mathbb R} - D\\}$ gives a partition of $\\mathbb R$ into a first category set and a Lebesgue measure zero set.\nIn 1984 Bruckner/Petruska [4] (Theorem 2.4) strengthened Weil's result by proving the following: Given any finite Borel measure $\\mu,$ the Baire-typical derivative is such that the set $D$ is the complement of a set that has $\\mu$-measure zero.\nIn 1993 Kirchheim [5] strengthened Weil's result by proving the following: Given any Hausdorff measure function $h,$ the Baire-typical derivative is such that the set $D$ is the complement of a set that has Hausdorff $h$-measure zero.\n[1] John J. Benedetto, Real Variable and Integration With Historical Notes, Mathematische Leitf\u00e4den. Stuttgart: B. G. Teubne, 1976, 278 pages. [MR 58 #28328; Zbl 336.26001]\n[2] Andrew M. Bruckner, Differentiation of Real Functions, 2nd edition, CRM Monograph Series #5, American Mathematical Society, 1994, xii + 195 pages. [The first edition was published in 1978 as Springer-Verlag's Lecture Notes in Mathematics #659. The second edition is essentially unchanged from the first edition with the exception of a new chapter on recent developments (23 pages) and 94 additional bibliographic items.] [MR 94m:26001; Zbl 796.26001]\n[3] Andrew M. Bruckner and John L. Leonard, Derivatives, American Mathematical Monthly 73 #4 (April 1966) [Part II: Papers in Analysis, Herbert Ellsworth Slaught Memorial Papers #11], 24-56. [MR 33 #5797; Zbl 138.27805]\n[4] Andrew M. Bruckner and Gy\u00f6rgy Petruska, Some typical results on bounded Baire $1$ functions, Acta Mathematica Hungarica 43 (1984), 325-333. [MR 85h:26004; Zbl 542.26004]\n[5] Casper Goffman, Real Functions, Prindle, Weber & Schmidt, 1953/1967, x + 261 pages. [MR 14,855e; Zbl 53.22502]\n[6] Bernd Kirchheim, Some further typical results on bounded Baire one functions, Acta Mathematica Hungarica 62 (1993), 119-129. [94k:26008; Zbl 786.26002]\n[7] John Clayton Klippert and Geoffrey Williams, On the existence of a derivative continuous on a $G_{\\delta}$, International Journal of Mathematical Education in Science and Technology 35 (2004), 91-99.\n[8] Clifford Weil, The space of bounded derivatives, Real Analysis Exchange 3 (1977-78), 38-41. [Zbl 377.26005]", "meta": {"post_id": 112067, "input_score": 299, "output_score": 285, "post_title": "How discontinuous can a derivative be?"}}
{"input": "For me those are references to the same thing.\nOn Wikipedia there are references to both but I still don't see the difference. http://en.wikipedia.org/wiki/Cartesian_coordinate_system#Cartesian_space\nIs there any difference? If yes, is there a simple way to describe it?", "output": "Point in Euclidean plane can be written in many ways: either using Cartesian coordinate system, or polar coordinate system.  That is same point $p$ can be written in two ways...   If we are saying Euclidean plane, It simply means that we are giving some axioms and using theorem based on that axioms.   But if we are saying Cartesian plane, it means that with euclidean axiom we are giving some method of representing of points.\nThis means:  Euclidean Plane means we have only some set of axiom\nCartesian plane means   Euclidean plane+ One fixed method of representing points.", "meta": {"post_id": 112076, "input_score": 48, "output_score": 44, "post_title": "What is the difference between Euclidean and Cartesian spaces?"}}
{"input": "Cauchy's integral formula says\n$$\r\nf^{(n)}(z)=\\frac{n!}{2\\pi i}\\int_C\\frac{f(\\zeta)d\\zeta}{(\\zeta-z)^{n+1}}.\r\n$$\nIf we let $C$ be the circle of radius $r$, such that $|f(\\zeta)|\\leq M$ on $C$, then taking $z=a$, one obtains Cauchy's estimate that \n$$\r\n|f^{(n)}(a)|\\leq Mn!r^{-n}.\r\n$$\nHow is this derived? I see instead\n$$\r\n|f^{(n)}(a)|\\leq\\frac{n!}{2\\pi}\\int_C \\frac{|f(\\zeta)||d\\zeta|}{|\\zeta-a|^{n+1}}\\leq Mn!\\int_C\\frac{|d\\zeta|}{|\\zeta-a|^{n+1}}\r\n$$\nbut I don't see how this eventually gets to Cauchy's estimate.", "output": "By Cauchy's integral formula you have given, we have \n$$f^{(n)}(a)=\\frac{n!}{2\\pi i}\\int_C\\frac{f(\\zeta)d\\zeta}{(\\zeta-a)^{n+1}}$$\nwhere $C$ is a circle of radius $r$ centered at $a$. Therefore, $C$ can be parametrized as $\\zeta=a+re^{i\\theta}$, $0\\leq \\theta\\leq 2\\pi$, which implies \n$$|f^{(n)}(a)|=\\left|\\frac{n!}{2\\pi i}\\int_0^{2\\pi}\\frac{f(a+re^{i\\theta})rie^{i\\theta}d\\theta}{(re^{i\\theta})^{n+1}}\\right|\\leq\\frac{n!}{2\\pi }\\int_0^{2\\pi}\\left|\\frac{f(a+re^{i\\theta})rie^{i\\theta}}{(re^{i\\theta})^{n+1}}\\right|d\\theta$$\n$$=\\frac{n!}{2\\pi }\\int_0^{2\\pi}\\frac{|f(a+re^{i\\theta})|}{r^n}d\\theta\\leq \\frac{n!}{2\\pi }\\int_0^{2\\pi}\\frac{M}{r^n}d\\theta=\\frac{Mn!}{r^n}$$\nwhere the last equality follows from $|e^{i\\theta}|=1$ and $|i|=1$, and the last inequality follows from the assumption that $|f|\\leq M$ on $C$.", "meta": {"post_id": 114349, "input_score": 22, "output_score": 37, "post_title": "How is Cauchy's estimate derived?"}}
{"input": "I'm trying to show the inclusion :\n$\\ell^p\\subseteq\\ell^q$ for real-value sequences, and show that the norms satisfy: $\\|\\cdot\\|_q<\\|\\cdot\\|_p$.\nI think I can show the first part without much trouble:\nTake $a_n$ in $\\ell^p$, then the partial sums are a Cauchy sequence, i.e., for any $\\epsilon>0$ , there is a natural $N$ with $|S_{n,p}-S_{k,p}|<\\epsilon$ for $n,k>N$, and $S_{n,p}$ the partial sums of $|a_n|^p$ and the individual terms go to $0$. So, we choose an index $J$ with $a_j<1$ for $j>J$. We then use that $f(x)=a^x$ decreases in $[0,1]$. This means that $|a_j|^p<|a_j|^q$.\nSo the tail of $S_{n,q}$, the partial sums of $|a_n|^q$ decrease fast-enough to converge, by comparison with the tail of $S_{n,p}$.\nBut I'm having trouble showing $\\|\\cdot\\|_q<\\|\\cdot\\|_p$ . Also, is there a specific canonical embedding between the two spaces?", "output": "Let $x\\in \\ell^p$ and $0<p<q<+\\infty$. If $x=0$, then everything is obvious. Otherwise consider $e=\\frac{x}{\\Vert x\\Vert_p}$. For all $k\\in\\mathbb{N}$ we have $|e_k|\\leq 1$ and $\\Vert e\\Vert_p=1$. Now since $p<q$ we get\n$$\n\\Vert e\\Vert_q=\n\\left(\\sum\\limits_{k=1}^\\infty |e_k|^q\\right)^{1/q}\\leq \n\\left(\\sum\\limits_{k=1}^\\infty |e_k|^p\\right)^{1/q}=\n\\Vert e\\Vert_p^{p/q}=1\n$$\nThen we can write\n$$\n\\Vert x\\Vert_q=\\Vert \\Vert x\\Vert_p e\\Vert_q=\\Vert x\\Vert_p\\Vert e\\Vert_q\\leq\\Vert x\\Vert_p\n$$\nIn fact this inequality means that $\\ell^p\\subseteq \\ell^q$. Also we can exclude the equality sign in this inclusion, because the sequence $x(k)=k^{-\\frac{1}{p}}$ belongs to $\\ell^q$ but not to $\\ell^p$. If we assume that $p\\geq 1$, we can speak of normed spaces $\\ell^p$ and $\\ell^q$. Then the last inequality means that the natural inclusion $i:\\ell^p\\to \\ell^q:x\\mapsto x$ is a continuous linear operator. \nIt is worth to note that the inequality $\\Vert\\cdot\\Vert_p\\leq C\\Vert\\cdot\\Vert_q$ is impossible for any constant $C\\geq 0$. Indeed consider sequences\n$$\nx_n(k)=\n\\begin{cases}\n1,\\qquad 1\\leq k\\leq n\\\\\n0,\\qquad k>n\n\\end{cases}\n$$\nThen\n$$\nC\\geq\\lim\\limits_{n\\to\\infty}\\frac{\\Vert x_n\\Vert_p}{\\Vert x_n\\Vert_q}=\\lim\\limits_{n\\to\\infty}n^{\\frac{1}{p}-\\frac{1}{q}}=+\\infty.\n$$\nTherefore such a constant $C>0$ doesn't exist.", "meta": {"post_id": 114650, "input_score": 19, "output_score": 41, "post_title": "$\\ell^p\\subseteq\\ell^q$ for $0<p<q<\\infty$ and $\\|\\cdot\\|_q<\\|\\cdot\\|_p$"}}
{"input": "I read an article not too long ago that posed the following problem:\nWhat is the volume of the solid of revolution created by spinning a unit cube about an axis joining two opposing vertices?\nSo the shape generated will be two cones and a parabola-like curve in the \"middle\". I hope that makes sense. At first, I tried to find a cross section of the resulting structure and then integrating it with disks, but I think I am over-complicating it. How would I go about solving this problem if I define my unit cube to be on the first octant with $i, j,$ and $k$ (so the axis would be $r(t)=t\\langle1,1,1\\rangle,0 < t <1$)? Thank you.", "output": "Suppose the cube has vertices $\\{0,1\\}^3$ and the axis of revolution is from $(0,0,0)$ to $(1,1,1)$. The axis has length $\\sqrt{3}$.\nUsing dot products, we get that three of the vertices are on a plane perpendicular to the axis at a distance of $\\dfrac{2}{\\sqrt{3}}$ from $(0,0,0)$ and the other three vertices are on a plane perpendicular to the axis at a distance of $\\dfrac{1}{\\sqrt{3}}$ from $(0,0,0)$.\nUsing cross products we get that each of these six vertices are at a distance of $\\dfrac{\\sqrt{2}}{\\sqrt{3}}$ from the axis.\nThe vertices in each of these planes form an equilateral triangle centered on the axis and are rotated at an angle of $\\dfrac{\\pi}{3}$ from each other.\nThe lines from the ends of the axis to the vertices closest to them sweep out a cone that is $\\dfrac{1}{\\sqrt{3}}$ high and has a base radius of $\\dfrac{\\sqrt{2}}{\\sqrt{3}}$. The total volume of these two cones is\n$$\n2\\cdot\\frac13\\pi\\frac{1}{\\sqrt{3}}\\left(\\frac{\\sqrt{2}}{\\sqrt{3}}\\right)^2=\\frac{4\\pi\\sqrt{3}}{27}\\tag{1}\n$$\nThe middle section is a bit trickier. Take a cylinder of height $1$ and radius $1$ with a line connecting corresponding points in the top and bottom. Give the top a twist of $\\alpha$ with respect to the bottom (keeping the top and bottom at the same distance from each other). Projecting this line onto a plane containing the axis of the cylinder and rotating the cylinder yields the line\n$$\ny=(1-x)\\cos(\\theta)+x\\cos(\\alpha-\\theta)\\tag{2}\n$$\nwhere x is the distance along the axis from the bottom ($x=0$) to the top ($x=1$) and y is the distance from the axis. The envelope of this family of lines is the hyperbola\n$$\ny^2=\\sin^2(\\alpha/2)(2x-1)^2+\\cos^2(\\alpha/2)\\tag{3}\n$$\nThe volume of the hyperboloid of revolution is pretty simple to compute\n$$\n\\begin{align}\n\\int_0^1\\pi y^2\\mathrm{d}x\n&=\\int_0^1\\pi\\left(\\sin^2(\\alpha/2)(2x-1)^2+\\cos^2(\\alpha/2)\\right)\\mathrm{d}x\\\\\n&=\\pi\\left(\\sin^2(\\alpha/2)\\frac12\\int_{-1}^1t^2\\mathrm{d}t+\\cos^2(\\alpha/2)\\right)\\\\\n&=\\pi\\left(\\frac13\\sin^2(\\alpha/2)+\\cos^2(\\alpha/2)\\right)\\\\\n&=\\pi\\frac{2+\\cos(\\alpha)}{3}\\tag{4}\n\\end{align}\n$$\nScaling $(4)$ for a general height and radius yields\n$$\nV=\\pi r^2h\\frac{2+\\cos(\\alpha)}{3}\\tag{5}\n$$\nIn our case, $\\alpha=\\dfrac{\\pi}{3}$ yielding $\\dfrac{2+\\cos(\\alpha)}{3}=\\dfrac56$. Therefore, the volume of the middle section is\n$$\n\\pi\\left(\\frac{\\sqrt{2}}{\\sqrt{3}}\\right)^2\\frac{1}{\\sqrt{3}}\\cdot\\frac56=\\frac{5\\pi\\sqrt{3}}{27}\\tag{6}\n$$\nAdding the volumes in $(1)$ and $(6)$ we get the total volume to be $\\dfrac{\\pi}{\\sqrt{3}}$.\n$\\hskip{4.5cm}$\nThe code for the animation above can be found here.", "meta": {"post_id": 115743, "input_score": 27, "output_score": 54, "post_title": "Question about a rotating cube?"}}
{"input": "In english based math language it seems that\nnon-increasing $\\Longleftrightarrow$ less or equal  (non-strict decreasing)\ndecreasing     $\\Longleftrightarrow$ strict less    (    strict decreasing)\nIs that correct ?\nIf so, how does it make sense ?\nprecision\nI should note that even very good math teacher are making mistakes about this.\nActually I asked this question after watching Boyd's video on convex optimization where even him is confused about this.... So I imagine many many people are, and there must be classes and tests about this absurd and buggy concept, which yields absolutely nothing interesting.\nSo I just wonder if I really am missing something, or if, yes, some people decided to create an abstraction that is leaky (not not increasing $\\neq$ increasing ?) verbose (4 words, with special negation logic, instead of using the word 'strict'  and keeping the usual well defined predicate logic rules)\nabsurdity of the concept\nThis notation is absurd for the following reason :\n\nwhen dealing with element instead of functions we dont apply the same logic :\nwe dont phrase $x < y$ as  \"$x$ is less than $y$\" nor  \"$x\\leq y$\" as \"$x$ is not-more than $y$\". (If we did though, at least it would not be so harmful as not not-more would mean more)\n\nyou have to define functions using a not notation, $f$ is non-increasing function $\\Longrightarrow$ if $x$ is not-less than $y$, say 0.3 feet and 2.5 inches, then $f(x)$ is not-more than $f(y)$\n\n\nThis also violates a very basic tenet in programming style 101, which is here for a reason : never define or use something with a negation, it is confusing.\n\nTo apply composition rules between functions, you better be buckled up with all the not. must be a fluff of cases\n\nMore profoundly, this violates a fundamental principle of logic which is that given some ambiguity, you should assume the most general case apply.\nIt is way worse than measuring things with non integral units.\nThis is violating logical rules, and leaving a very basic concept obfuscated.", "output": "Personally I find this among the most awful terminology in existence. It starts with the ambiguity present in \"increasing\" and \"decreasing\" themselves: common sense would have that this means getting ever larger/smaller; yet (if I take Wikipedia as reference) both the terms monotonically increasing function and monotonically increasing sequence allow for (local) constancy. (It seems unlikely that the purpose of \"monotonically\" is to weaken the notion following it; rather it seems to indicate that a formally defined rather than colloquial notion is meant.) So if there is doubt about what a bare \"increasing\" meant, the proper remedy would be to always accompany it with a disambiguating \"weakly\" or \"strictly\"; this would settle the matter.\nFor some reason however many people seem to find that \"nondecreasing\" is preferable to \"weakly increasing\". I work a lot with integers partitions, which most authors introduce as nonincreasing sequences of integers (with finite sum). Clearly what is meant here is not the absence of \"monotonic increase\" between successive integers, since that would imply strict decrease. One might conclude that when using negative terminology, people implicitly revert to the colloquial rather than formal meaning of the base notion. For comparison, even here in France, where \"n\u00e9gatif\" is taken to include $0$ (as does \"positif\"), few people would be willing to interpret \"entier non-n\u00e9gatif\" as designating integers${}>0$.\nHowever, even apart from the fact that negation does nothing to remove ambiguity from a notion, there are other drawbacks specific to this case:\n\nNonincreasing is not the negation of (strictly) increasing for sequences of length${}>2$, and should therefore be carefully distinguished from \"not increasing\". The sequence $0,1,-1,2,-2,3,-3,\\ldots$ is all of \"not increasing\",\n\"not decreasing\" and \"not constant\"; however, it is neither of \"nonincreasing\" nor \"nondecreasing\", but it is \"nonconstant\". A nice mess.\nIn the presence of partial ordering, having \"nonincreasing\" mean \"weakly decreasing\" is even less justified; here weak decrease is stronger than the absence of strict increase even for sequences of length $2$. I think what is needed in such context is almost never \"nonincreasing\", even between successive elements. For instance a \"plane partition\" could be defined as a weakly decreasing sequence of partitions (for the containment-of-diagrams partial ordering); saying \"nonincreasing\" here would be utterly confusing.\n\nIf one must absolutely use negative terminology, then it would have been much better to use \"nowhere increasing\" rather than \"nonincreasing\" (and even then only for total orderings).\nIn conclusion: if you want to be precise, it is better to say what you mean rather than to say what you don't mean (or even to not say what you are nonmeaning).", "meta": {"post_id": 115912, "input_score": 16, "output_score": 37, "post_title": "why do we use 'non-increasing' instead of decreasing?"}}
{"input": "I have a friend who turned $32$ recently.  She has an obsessive compulsive disdain for odd numbers, so I pointed out that being $32$ was pretty good since not only is it even, it also has no odd factors.  That made me realize that $64$ would be an even better age for her, because it's even, has no odd factors, and has no odd digits.  I then wondered how many other powers of $2$ have this property.  The only higher power of $2$ with all even digits that I could find was $2048.$ \nSo is there a larger power of $2$ with all even digits?  If not, how would you go about proving it?\nI tried examining the last $N$ digits of powers of $2$ to look for a cycle in which there was always at least one odd digit in the last $N$ digits of the consecutive powers.  Unfortunately, there were always a very small percentage of powers of $2$ whose last $N$ digits were even.\nEdit: Here's a little more info on some things I found while investigating the $N$ digit cycles.\n$N$: $2,3,4,5,6,7,8,9$\nCycle length: $20,100,500,2500,12500,62520,312500,1562500,\\dotsc, 4\\cdot 5^{N-1}$\nNumber of suffixes with all even digits in cycle: $10, 25, 60, 150, 370, 925, 2310,5780,\\sim4\\cdot2.5^{N-1}$ \nIt seems there are some interesting regularities there.  Unfortunately, one of the regularities is those occurrences of all even numbers!  In fact, I was able to find a power of $2$ in which the last $33$ digits were even $(2^{3789535319} = \\dots 468088628828226888000862880268288)$. \nYes it's true that it took a power of $2$ with over a billion digits to even get the last $33$ to be even, so it would seem any further powers of $2$ with all even digits are extremely unlikely.  But I'm still curious as to how you might prove it.\nEdit 2: Here's another interesting property I noticed.  The next digit to the left of the last $N$ digits will take on every value of its parity as the $N$ digits cycle each time.  Let me illustrate.\nThe last $2$ digits cycle every $20$ powers.  Now examine the following:\n$2^7 =    128$\n$2^{27} = \\dots 728$\n$2^{47} = \\dots 328$\n$2^{67} = \\dots 928$\n$2^{87} = \\dots 528$\n$2^{107} = \\dots 128$ \nNotice that the hundreds place starts out odd and then proceeds to take on every odd digit as the final 2 digits cycle.\nAs another example, let's look at the fourth digit (knowing that the last 3 digits cycle every 100 powers.)\n$2^{18} = 262144$, \n$2^{118} = \\dots 6144$, \n$2^{218} = \\dots 0144$, \n$2^{318} = \\dots 4144$, \n$2^{418} = \\dots 8144$, \n$2^{518} = \\dots 2144$ \nThis explains the power of 5 in the cycle length as each digit must take on all five digits of its parity.\nEDIT 3:  It looks like the $(N+1)$st digit takes on all the values $0-9$ as the last $N$ digits complete half a cycle.  For instance, the last $2$ digits cycle every $20$ powers, so look at the third digit every $10$ powers:\n$2^{8} = 256$, \n$2^{18} = \\dots 144$, \n$2^{28} = \\dots 456$, \n$2^{38} = \\dots 944$, \n$2^{48} = \\dots 656$, \n$2^{58} = \\dots 744$, \n$2^{68} = \\dots 856$, \n$2^{78} = \\dots 544$, \n$2^{88} = \\dots 056$, \n$2^{98} = \\dots 344$ \nNot only does the third digit take on every value 0-9, but it also alternates between odd and even every time (as the Edit 2 note would require.)  Also, the N digits cycle between two values, and each of the N digits besides the last one alternates between odd and even.  I'll make this more clear with one more example which looks at the fifth digit:\n$2^{20} = \\dots 48576$, \n$2^{270} = \\dots 11424$, \n$2^{520} = \\dots 28576$, \n$2^{770} = \\dots 31424$, \n$2^{1020} = \\dots 08576$, \n$2^{1270} = \\dots 51424$, \n$2^{1520} = \\dots 88576$, \n$2^{1770} = \\dots 71424$, \n$2^{2020} = \\dots 68576$, \n$2^{2270} = \\dots 91424$\nEDIT 4: Here's my next non-rigorous observation.  It appears that as the final N digits cycle 5 times, the $(N+2)$th digit is either odd twice and even three times, or it's odd three times and even twice.  This gives a method for extending an all even suffix.  \nIf you have an all even N digit suffix of $2^a$, and the (N+1)th digit is odd, then one of the following will have the (N+1)th digit even:\n$2^{(a+1*4*5^{N-2})}$, \n$2^{(a+2*4*5^{N-2})}$, \n$2^{(a+3*4*5^{N-2})}$\nEdit 5: It's looking like there's no way to prove this conjecture solely by examining the last N digits since we can always find an arbitrarily long, all even, N digit sequence.  However, all of the digits are distributed so uniformly through each power of 2 that I would wager that not only does every power of 2 over 2048 have an odd digit, but also, every power of 2 larger than $2^{168}$ has every digit represented in it somewhere.\nBut for now, let's just focus on the parity of each digit.  Consider the value of the $k^{th}$ digit of $2^n$ (with $a_0$ representing the 1's place.)  \n$$\na_k = \\left\\lfloor\\frac{2^n}{10^k}\\right\\rfloor \\text{ mod 10}\\Rightarrow a_k = \\left\\lfloor\\frac{2^{n-k}}{5^k}\\right\\rfloor \\text{ mod 10}\n$$\nWe can write \n$$2^{n-k} = d\\cdot5^k + r$$\nwhere $d$ is the divisor and $r$ is the remainder of $2^{n-k}/5^k$.  So\n$$\na_k \\equiv \\frac{2^{n-k}-r}{5^k} \\equiv d \\pmod{10}\n$$\n$$\\Rightarrow a_k \\equiv d \\pmod{2}$$\nAnd\n$$d\\cdot5^k = 2^{n-k} - r \\Rightarrow d \\equiv r \\pmod{2}$$\nRemember that $r$ is the remainder of $2^{n-k} \\text{ div } {5^k}$ so \n$$\\text{The parity of $a_k$ is the same as the parity of $2^{n-k}$ mod $5^k$.}$$\nNow we just want to show that for any $2^n > 2048$ we can always find a $k$ such that $2^{n-k} \\text{ mod }5^k$ is odd.\nI'm not sure if this actually helps or if I've just sort of paraphrased the problem.\nEDIT 6: Thinking about $2^{n-k}$ mod $5^k$, I realized there's a way to predict some odd digits.  \n$$2^a \\pmod{5^k} \\text{ is even for } 1\\le a< log_2 5^k$$\nThe period of $2^a \\pmod{5^k}$ is $4\\cdot5^{k-1}$ since 2 is a primitive root mod $5^k$.  Also \n$$2^{2\\cdot5^{k-1}} \\equiv -1 \\pmod{5^k}$$\nSo multiplying any $2^a$ by $2^{2\\cdot5^{k-1}}$ flips its parity mod $5^k$.  Therefore  $2^a \\pmod{5^k}\\text{ }$ is odd for\n$$1 + 2\\cdot5^{k-1} \\le a< 2\\cdot5^{k-1} + log_2 5^k$$\nOr taking the period into account, $2^a \\pmod{5^k} \\text{ }$ is odd for any integer $b\\ge0$ such that\n$$1 + 2\\cdot5^{k-1} (1 + 2b) \\le a< 2\\cdot5^{k-1} (1 + 2b) + log_2 5^k$$\nNow for the $k^{th}$ digit of $2^n$ ($ k=0 \\text{ } $ being the 1's digit), we're interested in the parity of $2^{n-k}$ mod $5^k$.  Setting $ a =n-k \\text{  } $ we see that the $k^{th}$ digit of $2^n$ is odd for integer $b\\ge0$ such that\n$$1 + 2\\cdot5^{k-1} (1 + 2b) \\le n - k < 2\\cdot5^{k-1} (1 + 2b) + log_2 5^k$$\nTo illustrate, here are some guaranteed odd digits for different $2^n$:  \n(k=1 digit): $ 2\\cdot5^0 + 2 = 4 \\le n \\le 5 $\n(k=2 digit): $ 2\\cdot5^1 + 3 = 13 \\le n \\le 16 $\n(k=3 digit): $ 2\\cdot5^2 + 4 = 54 \\le n \\le 59 $\n(k=4 digit): $ 2\\cdot5^3 + 5 = 255 \\le n \\le 263 $ \nAlso note that these would repeat every $4\\cdot5^{k-1}$ powers.\nThese guaranteed odd digits are not dense enough to cover all of the powers, but might this approach be extended somehow to find more odd digits?\nEdit 7: The two papers that Zander mentions below make me think that this is probably a pretty hard problem.", "output": "This seems to be similar to (I'd venture to say as hard as) a problem of Erd\u0151s open since 1979, that the base-3 representation of $2^n$ contains a 2 for all $n>8$.\nHere is a paper by Lagarias that addresses the ternary problem, and for the most part I think would generalize to the question at hand (we're also looking for the intersection of iterates of $x\\rightarrow 2x$ with a Cantor set). Unfortunately it does not resolve the problem.\nBut Conjecture 2' (from Furstenberg 1970) in the linked paper suggests a stronger result, that every $2^n$ for $n$ large enough will have a 1 in the decimal representation. Though it doesn't quantify \"large enough\" (so even if proved wouldn't promise that 2048 is the largest all-even decimal), it looks like it might be true for all $n>91$ (I checked up to $n=10^6$).", "meta": {"post_id": 116026, "input_score": 173, "output_score": 49, "post_title": "Is $2048$ the highest power of $2$ with all even digits (base ten)?"}}
{"input": "Suppose $f$ is some real function with the above property, i.e.\nif $\\sum\\limits_{n = 0}^\\infty {x_n}$ converges, then $\\sum\\limits_{n = 0}^\\infty {f(x_n)}$ also converges. \nMy question is: can anything interesting be said regarding the behavior of such a function close to $0$, other than the fact that $f(0)=0$?", "output": "I'm quite late on this one, but I think the result is nice enough to be included here.\nDefinition A function $f : \\mathbb R \\to \\mathbb R$ is said to be convergence-preserving (hereafter CP) if $\\sum f(a_n)$ converges for every convergent series $\\sum a_n$.\nTheorem (Wildenberg): The CP functions are exactly the ones which are linear on some neighbourhood of $0$.\nProof (Smith): Clearly, whether $f$ is CP only depends on the restriction of $f$ on an arbitrary small neighbourhood of $0$. Since the linear functions are CP, the condition is clearly sufficient. Let's prove that it is also necessary.\nWe will prove two preliminary results.\nLemma 1: $f$ CP $\\Rightarrow$ $f$ continuous at $0$. \nProof: Let's suppose that $f$ isn't continuous at 0. This implies that there exists a sequence $\\epsilon_n \\to 0$ and a positive real $\\eta > 0$ such that $\\forall n, |f(\\epsilon_n)| \\geq \\eta$. But it is easy to extract a subsequence $\\epsilon_{\\phi(n)}$ such that $\\sum \\epsilon_{\\phi(n)}$ converges (take $\\phi$ such that $\\epsilon_{\\phi(n)} \\leq 2^{-n}$, for instance). For such a subsequence, we still have that $|f(\\epsilon_{\\phi(n)})| \\geq \\eta$. This prevents $\\sum f(\\epsilon_{\\phi(n)})$ to converge and, thus, $f$ to be CP, a contradiction.\nLemma 2: The function $(x, y) \\mapsto f(x+y) + f(-x) + f(-y)$ vanishes on some neighbourhood of $0$.\nProof: If it didn't, one would be able to find sequences $x_n \\to 0$ and $y_n \\to 0$ s.t. $\\forall n, f(x_n + y_n) + f(-x_n) + f(-y_n) \\neq 0$. Up to some extraction, we can assume that $\\delta_n = f(x_n + y_n) + f(-x_n) + f(-y_n)$ always has the same sign (let's say $\\delta_n > 0$, for the sake of simplicity.)\nConsider now the series $$\\begin{array}{l@{}l}\n(x_0 + y_0) &+ (-x_0) + (-y_0) +  \\cdots + (x_0 + y_0) + (-x_0) + (-y_0)\\\\ \n& +(x_1 + y_1) + (-x_1) + (-y_1) +  \\cdots + (x_1 + y_1) + (-x_1) + (-y_1)\\\\\n&+\\cdots\\\\\n&+(x_n + y_n) + (-x_n) + (-y_n) +  \\cdots + (x_n + y_n) + (-x_n) + (-y_n)+\\cdots,\n\\end{array}$$\nwhere every triplet of termes $(x_i+y_i) + (-x_i) + (-y_i)$ is repeated $M_i > 0$ times, for some integer $M_i > 0$.\nBecause $x_n \\to 0$ and $y_n \\to 0$ and the three terms $x_i + y_i, -x_i, -y_i$ add to 0, it is easy to see that this series is convergent, regardless of the choice of the $M_i$'s.\nOn the other hand, if we choose $M_i \\geq \\delta_i^{-1}$, the image of our series by $f$ is $$\\begin{array}{l@{}l}\nf(x_0 + y_0) &+ f(-x_0) + f(-y_0) +  \\cdots + f(x_0 + y_0) + f(-x_0) + f(-y_0)\\\\ \n& +f(x_1 + y_1) + f(-x_1) + f(-y_1) +  \\cdots + f(x_1 + y_1) + f(-x_1) + f(-y_1)\\\\\n&+\\cdots\\\\\n&+f(x_n + y_n) + f(-x_n) + f(-y_n) +  \\cdots + f(x_n + y_n) + f(-x_n) + f(-y_n)+\\cdots,\n\\end{array}$$\nwhich diverges, for every line adds to $M_i \\delta_i > 1$. Again, this in direct contradiction with the CPness of $f$.\nIf we apply the result of lemma 2 with $y = 0$, we get that $f(-x) = -f(x)$. So we can rewrite lemma 2 in the following way:\n$\\exists \\eta > 0 : \\forall x, y \\in (-\\eta, \\eta), f(x+y) = f(x) + f(y)$.\nThis property and the continuity at 0 imply first the continuity on the whole of $(-\\eta, \\eta)$ and it is then not hard to adapt the classical proof to show that $f$ is linear on $(-\\eta, \\eta)$. Q.E.D.", "meta": {"post_id": 116964, "input_score": 31, "output_score": 34, "post_title": "The set of functions which map convergent series to convergent series"}}
{"input": "The context:\nI'm going to start working on a project that involves running predefined algorithms (and defining my own) for very big graphs (thousands of nodes). Visualization would also be welcome if possible.\nThis is a research project and the goal is to produce a prototype that generates/handles biological reaction networks.\nAlso, I would like to stick to these kind of platforms with \"everything but the kitchen-sink\" for scientific computing. The reason is, we will need other features like differentiation (symbolic and numeric) during the project.\nThe question:\nSo my question is, how complete is sage for graph theory algorithms, when compared to Mathematica (Combinatorica) and Matlab.", "output": "Try asking on http://ask.sagemath.org/questions/\nIn fact, there was already a general question asked there about Sage versus other software, and the top answer said, \"If you are doing graph theory or serious number theory, you shouldn't even be asking the question of which package to use.\"  That is, if you're doing graph theory, or serious number theory, Sage is the winner by far.  This was comparing Sage to all other computer algebra systems.  So, I believe the answer is Sage is the best graph theory program that exists.  And, it is getting better all the time.\nhttp://ask.sagemath.org/question/1010/reliability-of-sage-vs-commercial-software\nSage combines together many open source graph theory tools that exist (nauty generator, networkx which contains tons of stuff by itself, cliquer, and more) and also all the things that have been programmed by Sage developers.  And, if there is anything you want to be able to do that isn't already programmed, you can program it in Python (or if you need it to be really fast, in cython).\nAs far as visualization, yes, Sage graphs are extremely good graphics.  If you save them to a PDF, you can zoom in as much as you want (I'm not exaggerating) and they will still be crisp, clean graphics.  And, there is a graph editor that allows you to draw graphs and move the vertices around and add vertices and edges and things like that.  Not to mention, there are many built in graphs.\nOh, and by the way, if Mathematica (or one of many other programs) is installed on the same computer as Sage, you can use the functions from those other programs and get the results in Sage.\nHere's a video tutorial on graph theory (second video from top).  It gives a lot of detail, and includes all the info on graphics such as saving pdfs and using the graph editor to make nice looking graphs.\nhttp://www.sagemath.org/help-video.html\nHere's a real quick tutorial on graph theory in Sage:\nhttp://steinertriples.fr/ncohen/tut/Graphs/\nHere are the functions available:\n    g.add_cycle\n    g.add_edge\n    g.add_edges\n    g.add_path\n    g.add_vertex\n    g.add_vertices\n    g.adjacency_matrix\n    g.all_paths\n    g.allow_loops\n    g.allow_multiple_edges\n    g.allows_loops\n    g.allows_multiple_edges\n    g.am\n    g.antisymmetric\n    g.automorphism_group\n    g.average_degree\n    g.average_distance\n    g.bipartite_color\n    g.bipartite_sets\n    g.blocks_and_cut_vertices\n    g.bounded_outdegree_orientation\n    g.breadth_first_search\n    g.canonical_label\n    g.cartesian_product\n    g.categorical_product\n    g.category\n    g.center\n    g.centrality_betweenness\n    g.centrality_closeness\n    g.centrality_degree\n    g.characteristic_polynomial\n    g.charpoly\n    g.check_embedding_validity\n    g.check_pos_validity\n    g.chromatic_number\n    g.chromatic_polynomial\n    g.clear\n    g.clique_complex\n    g.clique_maximum\n    g.clique_number\n    g.cliques\n    g.cliques_containing_vertex\n    g.cliques_get_clique_bipartite\n    g.cliques_get_max_clique_graph\n    g.cliques_maximal\n    g.cliques_maximum\n    g.cliques_number_of\n    g.cliques_vertex_clique_number\n    g.cluster_transitivity\n    g.cluster_triangles\n    g.clustering_average\n    g.clustering_coeff\n    g.coarsest_equitable_refinement\n    g.coloring\n    g.complement\n    g.connected_component_containing_vertex\n    g.connected_components\n    g.connected_components_number\n    g.connected_components_subgraphs\n    g.convexity_properties\n    g.copy\n    g.cores\n    g.cycle_basis\n    g.db\n    g.degree\n    g.degree_constrained_subgraph\n    g.degree_histogram\n    g.degree_iterator\n    g.degree_sequence\n    g.degree_to_cell\n    g.delete_edge\n    g.delete_edges\n    g.delete_multiedge\n    g.delete_vertex\n    g.delete_vertices\n    g.density\n    g.depth_first_search\n    g.diameter\n    g.disjoint_routed_paths\n    g.disjoint_union\n    g.disjunctive_product\n    g.distance\n    g.distance_all_pairs\n    g.distance_graph\n    g.dominating_set\n    g.dump\n    g.dumps\n    g.eccentricity\n    g.edge_boundary\n    g.edge_connectivity\n    g.edge_cut\n    g.edge_disjoint_paths\n    g.edge_disjoint_spanning_trees\n    g.edge_iterator\n    g.edge_label\n    g.edge_labels\n    g.edges\n    g.edges_incident\n    g.eigenspaces\n    g.eigenvectors\n    g.eulerian_circuit\n    g.eulerian_orientation\n    g.flow\n    g.fractional_chromatic_index\n    g.genus\n    g.get_boundary\n    g.get_embedding\n    g.get_pos\n    g.get_vertex\n    g.get_vertices\n    g.girth\n    g.gomory_hu_tree\n    g.graph6_string\n    g.graphics_array_defaults\n    g.graphplot\n    g.graphviz_string\n    g.graphviz_to_file_named\n    g.hamiltonian_cycle\n    g.has_edge\n    g.has_loops\n    g.has_multiple_edges\n    g.has_vertex\n    g.incidence_matrix\n    g.independent_set\n    g.independent_set_of_representatives\n    g.interior_paths\n    g.is_bipartite\n    g.is_chordal\n    g.is_circular_planar\n    g.is_clique\n    g.is_connected\n    g.is_directed\n    g.is_drawn_free_of_edge_crossings\n    g.is_equitable\n    g.is_eulerian\n    g.is_even_hole_free\n    g.is_forest\n    g.is_gallai_tree\n    g.is_hamiltonian\n    g.is_independent_set\n    g.is_interval\n    g.is_isomorphic\n    g.is_line_graph\n    g.is_odd_hole_free\n    g.is_overfull\n    g.is_perfect\n    g.is_planar\n    g.is_prime\n    g.is_regular\n    g.is_split\n    g.is_subgraph\n    g.is_transitively_reduced\n    g.is_tree\n    g.is_triangle_free\n    g.is_vertex_transitive\n    g.kirchhoff_matrix\n    g.laplacian_matrix\n    g.latex_options\n    g.layout\n    g.layout_circular\n    g.layout_default\n    g.layout_extend_randomly\n    g.layout_graphviz\n    g.layout_planar\n    g.layout_ranked\n    g.layout_spring\n    g.layout_tree\n    g.lex_BFS\n    g.lexicographic_product\n    g.line_graph\n    g.longest_path\n    g.loop_edges\n    g.loop_vertices\n    g.loops\n    g.matching\n    g.matching_polynomial\n    g.max_cut\n    g.maximum_average_degree\n    g.merge_vertices\n    g.min_spanning_tree\n    g.minimum_outdegree_orientation\n    g.minor\n    g.modular_decomposition\n    g.multicommodity_flow\n    g.multiple_edges\n    g.multiway_cut\n    g.name\n    g.neighbor_iterator\n    g.neighbors\n    g.networkx_graph\n    g.num_edges\n    g.num_verts\n    g.number_of_loops\n    g.order\n    g.periphery\n    g.plot\n    g.plot3d\n    g.radius\n    g.random_edge\n    g.random_subgraph\n    g.random_vertex\n    g.relabel\n    g.remove_loops\n    g.remove_multiple_edges\n    g.rename\n    g.reset_name\n    g.save\n    g.set_boundary\n    g.set_edge_label\n    g.set_embedding\n    g.set_latex_options\n    g.set_planar_positions\n    g.set_pos\n    g.set_vertex\n    g.set_vertices\n    g.shortest_path\n    g.shortest_path_all_pairs\n    g.shortest_path_length\n    g.shortest_path_lengths\n    g.shortest_paths\n    g.show\n    g.show3d\n    g.size\n    g.spanning_trees_count\n    g.sparse6_string\n    g.spectrum\n    g.steiner_tree\n    g.strong_orientation\n    g.strong_product\n    g.subdivide_edge\n    g.subdivide_edges\n    g.subgraph\n    g.subgraph_search\n    g.subgraph_search_count\n    g.subgraph_search_iterator\n    g.szeged_index\n    g.tensor_product\n    g.to_directed\n    g.to_simple\n    g.to_undirected\n    g.topological_minor\n    g.trace_faces\n    g.transitive_closure\n    g.transitive_reduction\n    g.traveling_salesman_problem\n    g.two_factor_petersen\n    g.union\n    g.version\n    g.vertex_boundary\n    g.vertex_connectivity\n    g.vertex_cover\n    g.vertex_cut\n    g.vertex_disjoint_paths\n    g.vertex_iterator\n    g.vertices\n    g.weighted\n    g.weighted_adjacency_matrix\n    g.wiener_index\n    g.write_to_eps\n\nAnd here are some graphs you can easily generate:\ngraphs.BalancedTree\ngraphs.BarbellGraph\ngraphs.BidiakisCube\ngraphs.BrinkmannGraph\ngraphs.BubbleSortGraph\ngraphs.BuckyBall\ngraphs.BullGraph\ngraphs.ButterflyGraph\ngraphs.ChvatalGraph\ngraphs.CirculantGraph\ngraphs.CircularLadderGraph\ngraphs.ClawGraph\ngraphs.CompleteBipartiteGraph\ngraphs.CompleteGraph\ngraphs.CompleteMultipartiteGraph\ngraphs.CubeGraph\ngraphs.CycleGraph\ngraphs.DegreeSequence\ngraphs.DegreeSequenceBipartite\ngraphs.DegreeSequenceConfigurationModel\ngraphs.DegreeSequenceExpected\ngraphs.DegreeSequenceTree\ngraphs.DesarguesGraph\ngraphs.DiamondGraph\ngraphs.DodecahedralGraph\ngraphs.DorogovtsevGoltsevMendesGraph\ngraphs.DurerGraph\ngraphs.DyckGraph\ngraphs.EmptyGraph\ngraphs.ErreraGraph\ngraphs.FibonacciTree\ngraphs.FlowerSnark\ngraphs.FranklinGraph\ngraphs.FriendshipGraph\ngraphs.FruchtGraph\ngraphs.FuzzyBallGraph\ngraphs.GeneralizedPetersenGraph\ngraphs.GoldnerHararyGraph\ngraphs.Grid2dGraph\ngraphs.GridGraph\ngraphs.GrotzschGraph\ngraphs.HanoiTowerGraph\ngraphs.HeawoodGraph\ngraphs.HerschelGraph\ngraphs.HexahedralGraph\ngraphs.HigmanSimsGraph\ngraphs.HoffmanSingletonGraph\ngraphs.HouseGraph\ngraphs.HouseXGraph\ngraphs.HyperStarGraph\ngraphs.IcosahedralGraph\ngraphs.IntervalGraph\ngraphs.KneserGraph\ngraphs.KrackhardtKiteGraph\ngraphs.LCFGraph\ngraphs.LadderGraph\ngraphs.LollipopGraph\ngraphs.MoebiusKantorGraph\ngraphs.MoserSpindle\ngraphs.MycielskiGraph\ngraphs.MycielskiStep\ngraphs.NKStarGraph\ngraphs.NStarGraph\ngraphs.OctahedralGraph\ngraphs.OddGraph\ngraphs.PappusGraph\ngraphs.PathGraph\ngraphs.PetersenGraph\ngraphs.RandomBarabasiAlbert\ngraphs.RandomBipartite\ngraphs.RandomGNM\ngraphs.RandomGNP\ngraphs.RandomHolmeKim\ngraphs.RandomInterval\ngraphs.RandomLobster\ngraphs.RandomNewmanWattsStrogatz\ngraphs.RandomRegular\ngraphs.RandomShell\ngraphs.RandomTree\ngraphs.RandomTreePowerlaw\ngraphs.ShrikhandeGraph\ngraphs.StarGraph\ngraphs.TetrahedralGraph\ngraphs.ThomsenGraph\ngraphs.ToroidalGrid2dGraph\ngraphs.WheelGraph\ngraphs.WorldMap\ngraphs.cospectral_graphs\ngraphs.line_graph_forbidden_subgraphs\ngraphs.nauty_geng\ngraphs.trees", "meta": {"post_id": 119386, "input_score": 53, "output_score": 48, "post_title": "Is Sage on the same level as Mathematica or Matlab for graph theory and graph visualization?"}}
{"input": "Let $n_1,...,n_k \\in \\mathbb{N}$ and let $a_1,...,a_k \\in \\mathbb{Z}$. \nHow to prove the following version of the Chinese remainder theorem (see here):\nThere exists a $x \\in \\mathbb{Z}$ satisfying system of equations:\n$$x=a_1 \\pmod {n_1}$$\n$$x=a_2  \\pmod {n_2}$$\n$$\\ldots$$\n$$x=a_k \\pmod{n_k}$$\nif and only if $a_i=a_j \\pmod{\\gcd(n_i,n_j)}$ for all $i,j=1,...,k$?\nIf numbers $n_i$, for $i=1,...,k$, are pairwise coprime, it is a classical version of Chinese remainder theorem.\nThanks.", "output": "If we factor $n_k$ into primes, $n_k = p_{1}^{b_{1}}\\cdots p_r^{b_{r}}$, then the Chinese Remainder Theorem tells us that $x\\equiv a_k\\pmod{n_k}$ is equivalent to the system of congruences\n$$\\begin{align*}\r\nx&\\equiv a_k\\pmod{p_1^{b_{1}}}\\\\\r\nx&\\equiv a_k\\pmod{p_2^{b_{2}}}\\\\\r\n&\\vdots\\\\\r\nx&\\equiv a_k\\pmod{p_r^{b_{r}}}\r\n\\end{align*}$$\nThus, we can replace the given system of congruences with one in which every modulus is a prime power, $n_i = p_i^{b_i}$. \nNote that the assumption that $a_i\\equiv a_j\\pmod{\\gcd(n_i,n_j)}$ \"goes through\" this replacement (if they were congruend modulo $\\gcd(n_i,n_j)$, then they are congruent modulo the gcds of the prime powers as well).\nSo, we may assume without loss of generality that every modulus is a prime power.\nI claim that we can deal with each prime separately, again by the Chinese Remainder Theorem. If we can solve all congruences involving the prime $p_1$ to obtain a solution $x_1$ (which will be determined modulo the highest power of $p_1$ that occurs); and all congruences involving the prime $p_2$ to obtain a solution $x_2$ (which will be determined modulo the highest power of $p_2$ that occurs); and so on until we obtain a solution $x_n$ for all congruences involving the prime $p_n$ (determined modulo the highest power of $p_n$ that occurs), then we can obtain a simultaneous solution by solving the usual Chinese Remainder Theorem system\n$$\\begin{align*}\r\nx &\\equiv x_1 \\pmod{p_1^{m_1}}\\\\\r\n&\\vdots\\\\\r\nx &\\equiv x_n\\pmod{p_n^{m_n}}\r\n\\end{align*}$$\n(where $m_i$ is the highest power of $p_i$ that occurs as a modulus).\nSo we are reduced to solving figuring out whether we can solve the system\n$$\\begin{align*}\r\nx &\\equiv a_1\\pmod{p^{b_1}}\\\\\r\nx &\\equiv a_2\\pmod{p^{b_2}}\\\\\r\n&\\vdots\\\\\r\nx & \\equiv a_n\\pmod{p^{b_n}}\r\n\\end{align*}$$\nwith, without loss of generality, $b_1\\leq b_2\\leq\\cdots\\leq b_n$.\nWhen can this be solved? Clearly, this can be solved if and only if $a_i\\equiv a_j\\pmod{p^{b_{\\min(i,j)}}}$: any solution must satisfy this condition, and if this condition is satisfied, then $a_n$ is a solution.\nFor example: say the original moduli had been $n_1 = 2^3\\times 3\\times 7^2$, $n_2= 2^2\\times 5\\times 7$, $n_3=3^2\\times 5^3$. First we replace the system with the system of congruences\n$$\\begin{align*}\r\nx&\\equiv a_1 \\pmod{2^3}\\\\\r\nx&\\equiv a_2\\pmod{2^2}\\\\\r\nx&\\equiv a_1\\pmod{3}\\\\\r\nx&\\equiv a_3\\pmod{3^2}\\\\\r\nx&\\equiv a_2\\pmod{5}\\\\\r\nx&\\equiv a_3\\pmod{5^3}\\\\\r\nx&\\equiv a_1\\pmod{7^2}\\\\\r\nx&\\equiv a_2\\pmod{7}.\r\n\\end{align*}$$\nThen we separately solve the systems:\n$$\\begin{align*}\r\nx_1&\\equiv a_1 \\pmod{2^3} &x_2&\\equiv a_1\\pmod{3}\\\\\r\nx_1&\\equiv a_2\\pmod{2^2}&x_2&\\equiv a_3\\pmod{3^2}\\\\\r\n\\strut\\\\\r\nx_3&\\equiv a_2\\pmod{5}&x_4&\\equiv a_1\\pmod{7^2}\\\\\r\nx_3&\\equiv a_3\\pmod{5^3}&x_4&\\equiv a_2\\pmod{7}.\r\n\\end{align*}$$\nAssuming we can solve these, $x_1$ is determined modulo $2^3$, $x_2$ modulo $3^2$, $x_3$ modulo $5^3$, and $x_4$ modulo $7^2$, so we then solve the system\n$$\\begin{align*}\r\nx &\\equiv x_1\\pmod{2^3}\\\\\r\nx &\\equiv x_2\\pmod{3^2}\\\\\r\nx&\\equiv x_3 \\pmod{5^3}\\\\\r\nx&\\equiv x_4\\pmod{7^2}\r\n\\end{align*}$$\nand obtain a solution to the original system.\nHence, if the condition $a_i\\equiv a_j\\pmod{\\gcd(n_i,n_j)}$ holds in the original system, then we obtain a solution for each prime, and from the solution for each prime we obtain a solution to the original system by applying the usual Chinese Remainder Theorem twice.", "meta": {"post_id": 120070, "input_score": 35, "output_score": 60, "post_title": "Chinese Remainder theorem with non-pairwise coprime moduli"}}
{"input": "Are there two orthogonal complete Latin squares of any order greater than 1? If so, what is the smallest order for which they exist?\n(A Latin square of order $n$ is an $n\\times n$ array of symbols $\\{ s_1\\ldots s_n \\}$ such that each of the symbols appears exactly once in each row and in each column.  Two Latin squares $L_{ij}$ and $G_{ij}$ are orthogonal if each of the $n^2$ pairs $(L_{ij}, G_{ij})$ is distinct; such a pair together form a Graeco-Latin square.  A Latin square  $L_{ij}$ is complete if each of the $n\\cdot(n-1)$ pairs $(L_{ij}, L_{i+1,j})$ is distinct and if each of the $n\\cdot(n-1)$ pairs $(L_{ij}, L_{i,j+1})$ is distinct.)", "output": "The problem is open no longer. It turns out that there are many pairs\nof orthogonal complete Latin squares of order 12.\nHere is one example. I also know how to build some bigger ones.\n\\begin{pmatrix}\n  1 & 2 & 5 & 7 & 4 & 8 & 9 &11 & 6 &12 &10 & 3\\\\\n  7 &12 & 9 & 3 &10 & 2 & 1 & 5 & 8 & 4 & 6 &11\\\\\n  2 & 3 & 6 & 8 & 5 & 9 &10 &12 & 1 & 7 &11 & 4\\\\\n  8 & 7 &10 & 4 &11 & 3 & 2 & 6 & 9 & 5 & 1 &12\\\\\n 10 & 9 &12 & 6 & 7 & 5 & 4 & 2 &11 & 1 & 3 & 8\\\\\n  3 & 4 & 1 & 9 & 6 &10 &11 & 7 & 2 & 8 &12 & 5\\\\\n  9 & 8 &11 & 5 &12 & 4 & 3 & 1 &10 & 6 & 2 & 7\\\\\n 12 &11 & 8 & 2 & 9 & 1 & 6 & 4 & 7 & 3 & 5 &10\\\\\n 11 &10 & 7 & 1 & 8 & 6 & 5 & 3 &12 & 2 & 4 & 9\\\\\n  6 & 1 & 4 &12 & 3 & 7 & 8 &10 & 5 &11 & 9 & 2\\\\\n  4 & 5 & 2 &10 & 1 &11 &12 & 8 & 3 & 9 & 7 & 6\\\\\n  5 & 6 & 3 &11 & 2 &12 & 7 & 9 & 4 &10 & 8 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n  1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 &10 &11 &12\\\\\n  6 & 4 &11 & 2 & 8 & 1 & 9 & 5 & 7 &12 & 3 &10\\\\\n  4 &10 & 7 & 1 &11 & 9 & 3 &12 & 6 & 2 & 5 & 8\\\\\n  9 & 1 & 5 &10 &12 & 4 & 6 &11 & 3 & 8 & 7 & 2\\\\\n  2 &12 & 9 & 6 & 3 & 7 &11 &10 & 1 & 4 & 8 & 5\\\\\n  3 & 9 &12 & 8 & 2 &10 & 4 & 7 & 5 &11 & 6 & 1\\\\\n 10 & 8 & 6 & 9 & 7 & 3 & 5 & 2 & 4 & 1 &12 &11\\\\\n  5 & 3 & 2 &11 & 1 & 8 &10 & 6 &12 & 7 & 4 & 9\\\\\n 11 & 7 &10 & 5 & 4 &12 & 2 & 9 & 8 & 3 & 1 & 6\\\\\n  8 &11 & 4 & 3 & 6 & 5 &12 & 1 &10 & 9 & 2 & 7\\\\\n  7 & 6 & 8 &12 &10 & 2 & 1 & 3 &11 & 5 & 9 & 4\\\\\n 12 & 5 & 1 & 7 & 9 &11 & 8 & 4 & 2 & 6 &10 & 3\n\\end{pmatrix}", "meta": {"post_id": 120212, "input_score": 21, "output_score": 51, "post_title": "Are there complete Graeco-Latin squares?"}}
{"input": "I need to prove that: If a nonzero linear functional $f$ on a Banach Space $X$ is discontinuous then the nullspace $N_f$ is dense in $X$. \nTo prove that $N_f$ is dense, it suffices to show that $\\overline N_f = X$ which is equivalent to $(X \\setminus N_f)^o=\\emptyset$. (the interior of complement of $N_f$ is null set.)\nSince $f$ is a linear functional and is discontinuous, it has to be unbounded. I don't know exactly how to utilize these observations. \nAlso on a related topic, I'm a little confused about how to exploit the a Linear Functional $f:X \\to R$ or a Linear Operator $T:X \\to Y$ being unbounded. Can I say that if a linear operator is unbounded then exists a sequence $<x_n>$ in $X$ s.t. \n$||Tx_n|| > n^2||x_n||$ for each $n$ or $||Tx_n|| > n||x_n||$ ?", "output": "If $f$ is discontinuous, then you can find a sequence of non-zero vectors $(x_n)$ with $|f(x_n)|\\ge n \\Vert x_n\\Vert$ for each $n$.\nNormalizing the $x_n$, and still calling them $x_n$, we obtain a sequence of norm one vectors \n$(x_n)$ such that $$\\tag{1}|f(x_n)|\\ge n,\\quad\\text{for each } n=1,2,\\ldots.$$\nNow suppose $x\\notin  {\\text{Ker}(f)} $. Consider the sequence\n$$\r\nz_n = x-\\textstyle{f(x)\\over f(x_n) } x_n.\r\n$$\nOne easily verifies that  $z_n\\in {\\text{Ker}(f)}$ for each $n$. Moreover, from $(1)$, we have\n$$\\Vert z_n - x\\Vert=\\Bigl\\Vert\\textstyle{f(x)\\over f(x_n) } x_n \\Bigr\\Vert\r\n=\\Bigl|\\textstyle{f(x)\\over f(x_n) }\\Bigr|\\quad\\buildrel{n\\rightarrow\\infty}\\over\\longrightarrow\\quad0 .$$\nFrom this it follows that $x\\in\\overline{\\text{Ker}(f)}$.  As $x$ was an arbitrary element not in  $ {\\text{Ker}(f)}$, it follows that  $\\overline{\\text{Ker}(f)}=X$.\n\nWith regards to your last question, if $f$ is discontinuous and if $\\alpha_n$ is any sequence of scalars, you can find a sequence of non-zero vectors $(x_n)$ with  $|f(x_n)|\\ge \\alpha_n \\Vert x_n\\Vert$.", "meta": {"post_id": 123282, "input_score": 20, "output_score": 35, "post_title": "Linear functional on a Banach space is discontinuous then its nullspace is dense."}}
{"input": "What is the \"standard basis\" for fields of complex numbers?\nFor example, what is the standard basis for $\\Bbb C^2$ (two-tuples of the form: $(a + bi, c + di)$)?  I know the standard for $\\Bbb R^2$ is $((1, 0), (0, 1))$.  Is the standard basis exactly the same for complex numbers?\nP.S. - I realize this question is very simplistic, but I couldn't find an authoritative answer online.", "output": "Just to be clear, by definition, a vector space always comes along with a field of scalars $F$.  It's common just to talk about a \"vector space\" and a \"basis\"; but if there is possible doubt about the field of scalars, it's better to talk about a \"vector space over $F$\" and a \"basis over $F$\" (or an \"$F$-vector space\" and an \"$F$-basis\").\nYour example, $\\mathbb{C}^2$, is a 2-dimensional vector space over $\\mathbb{C}$, and the simplest choice of a $\\mathbb{C}$-basis is $\\{ (1,0), (0,1) \\}$.\nHowever, $\\mathbb{C}^2$ is also a vector space over $\\mathbb{R}$.  When we view $\\mathbb{C}^2$ as an $\\mathbb{R}$-vector space, it has dimension 4, and the simplest choice of an $\\mathbb{R}$-basis is $\\{(1,0), (i,0), (0,1), (0,i)\\}$.\nHere's another intersting example, though I'm pretty sure it's not what you were asking about:\nWe can view $\\mathbb{C}^2$ as a vector space over $\\mathbb{Q}$.  (You can work through the definition of a vector space to prove this is true.)  As a $\\mathbb{Q}$-vector space, $\\mathbb{C}^2$ is infinite-dimensional, and you can't write down any nice basis.  (The existence of the $\\mathbb{Q}$-basis depends on the axiom of choice.)", "meta": {"post_id": 123448, "input_score": 79, "output_score": 104, "post_title": "What is the \"standard basis\" for fields of complex numbers?"}}
{"input": "Let $(X,\\mathcal{M},\\mu)$ be a measure space and suppose $\\{f_n\\}$ are non-negative measurable functions decreasing pointwise to $f$. Suppose also that $\\int f_1 \\lt \\infty$. Then $$\\int_X f~d\\mu = \\lim_{n\\to\\infty}\\int_X f_n~d\\mu.$$  \n\nAtempt: \nSince $\\{f_n\\}$ are decreasing, and converges pointwise to $f$, then $\\{-f_n\\}$ is increasing pointwise to $f$. So by the monotone convergence theorem \n$$ \\int_X -f~d\\mu = \\lim_{n\\to\\infty}\\int_X -f_n ~d\\mu$$ and so $$\\int_X f~d\\mu = \\lim_{n\\to\\infty}\\int_X f_n~d\\mu.$$", "output": "The problem is that $-f_n$ increases to $-f$ which is not non-negative, so we can't apply directly to $-f_n$ the monotone convergence theorem. But if we take $g_n:=f_1-f_n$, then $\\{g_n\\}$ is an increasing sequence of non-negative measurable functions, which converges pointwise to $f_1-f$. Monotone convergence theorem yields:\n$$\\lim_{n\\to +\\infty}\\int_X (f_1-f_n)d\\mu=\\int_X\\lim_{n\\to +\\infty} (f_1-f_n)d\\mu=\\int_X f_1d\\mu-\\int_X fd\\mu$$\nso $\\lim_{n\\to +\\infty}\\int_X f_nd\\mu=\\int_X fd\\mu$.\nNote that the fact that there is an integrable function in the sequence is primordial, indeed, if you take $X$ the real line, $\\mathcal M$ its Borel $\\sigma$-algebra and $\\mu$ the Lebesgue measure, and $f_n(x)=\\begin{cases} 1&\\mbox{ if }x\\geq n\\\\\n0&\\mbox{ otherwise}\n\\end{cases}$\nthe sequence $f_n $ decreases to $0$ but $\\int_{\\mathbb R}f_nd\\mu=+\\infty$ for all $n$.", "meta": {"post_id": 124033, "input_score": 58, "output_score": 84, "post_title": "Monotone Convergence Theorem for non-negative decreasing sequence of measurable functions"}}
{"input": "When analyzing real integrals with contour integrals, how does one choose a proper contour integral? \nMany cases can be solved by integrating around the top half of a circle with radius of infinity and then integrating along the entire real line.\nI understand how when integrating one would avoid the branch cuts, but how would one know to use a rectangle or a quarter of a circle as a contour?", "output": "In response to the comment, I will do my best to attempt to explain how I choose contours for integration.\nI first look at the bounds of integration.  If it is over $[0, \\infty)$ and even, make it over $(-\\infty,\\infty)$.\nNext, I look at how the function behaves around infinity in the top half of the plane.  If it decreases fast enough (e.g. $\\frac{\\exp(ix)}{x^2+1}$), we can integrate with a semicircle contour.  If not, find a value $a$ such that when you integrate a rectangle with vertices at $-R, R, R+ia, -R+ia$ (as $R\\to\\infty$), the vertical sides disappear and the horizontal integrals are equal when multiplied by a constant.  \nIf the function cannot be made even, there is still some hope left to contour integrate.  If the function has a branch cut (e.g. $\\frac{\\sqrt x}{x^2+1}$), try a keyhole contour if the function decays fast enough around $\\infty$.  Otherwise, try a rectangle.\nIf the integrand can be simplified as $f(x+ia) = g(x)$, where the integral of $g$ is known or is in the form $A f(x)$, it may be able to be exploited using a rectangular contour. For example, a rectangular contour of infinite width and height $a$ along the real line along with the knowledge that $\\int_{-\\infty}^\\infty e^{-x^2}\\, dx =\\sqrt{\\pi}$ can easily demonstrate that $\\int_{-\\infty}^\\infty e^{-(x+a)^2}\\, dx =\\sqrt{\\pi}$. \nWedge contours can be used in situations like the rectangular contours can, but instead of having $f(x+ia)$ being well-behaved, we have $f(e^{i\\theta}x)$ being well behaved. For example, taking again $f(x) = e^{-x^2}$, we see that $f(xe^{i\\pi/4}) = e^{-ix^2}$. Thus, taking a wedge contour with $\\theta=\\pi/4$ we can deduce from the integral $\\int_{0}^\\infty e^{-x^2}\\, dx =\\sqrt{\\pi}/2$ that $\\int_{0}^\\infty \\sin(x^2)\\, dx =\\int_{0}^\\infty \\cos(x^2)\\, dx =\\sqrt{\\pi/2}/2$.\nOther contours exist and can be used (e.g. the trapezoid contour for integrating the gaussian integral), though I've found the above contours work for most standard integrals.\nIf the contour travels through a pole, indent it with a semicircle - with a simple pole, $z_0$, the contributed value from that integral equals $i\\theta\\operatorname*{Res}f_{z=z_0}$ where $\\theta$ equals the angle traversed around the pole.\nIt is often convenient to change $\\sin$ or $\\cos$ in the numerator to $e^{ix}$ (which is better behaived for integration around the top half of the plane) and take the real or imaginary part after integration - this can even be done if the other part diverges. \nWhen dealing with exponents, use trigonometric identities to reduce the function into an exponential that decays. For example, it is easier to deal with $\\Re \\left[\\frac{1-e^{2ix}}{2}\\right] = \\sin^2(x)$ instead of just $\\sin^2(x)$.", "meta": {"post_id": 124453, "input_score": 47, "output_score": 46, "post_title": "How to choose a proper contour for a contour integral?"}}
{"input": "Can someone give an example of a ring $R$, a left $R$-module $M$ and a submodule $N$ of $M$ such that $M$ is finitely generated, but $N$ is not finitely generated?\n\nI tried a couple of examples of modules I know and got nothing...", "output": "Consider the simplest possible nontrivial (left) $R$-module: $R$ itself. It's certainly finitely generated, by $\\{ 1 \\}$. The submodules are exactly the (left) ideals of $R$. So you want a ring which has (left) ideals which are not finitely generated. For example, you could use a non-Noetherian commutative ring, such as $\\mathbb{Z}[X_1, X_2, X_3, \\ldots ]$.", "meta": {"post_id": 125015, "input_score": 43, "output_score": 64, "post_title": "Finitely generated module with a submodule that is not finitely generated"}}
{"input": "Mesurable Functions by definition(from Stein and Shakarchi):\nA function $f$ defined on a measurable subset $E$ of $\\mathbb{R}^d$ is measurable, if for all $a\\in \\mathbb{R}$, the set \n$$f^{-1}([-\\infty,a))=\\{x\\in E: f(x)<a\\}$$\nis measurable. \nNow a set $E$ is called measurable if $m_*(E)=0.$ \nIntuitively, the definition doesn't make much sense to me and would appreciate it if someone can explain it to me. A bonus would be if you can give me some simple examples of measurable and non measurable functions? Thanks.", "output": "First, I don't know your definition of measurable sets.\nWhy do people define measurable function this way?\nA non-mathematical reason\nLaziness. Well, this is just an opinion, but I think that when you define measurable function like this, then you don't need to go into the trouble of explaining (or even understanding yourself) the definition.\nA mathematical reason\nPoint 1. We talk about the probability of subsets of $\\Omega$, not elements.\nLet's take probability theory as our model of reference.  If you have a finite set, $\\Omega$, you can define a probability $\\mu$ in $\\Omega$ simply defining the probability of each element of $\\Omega$. But when you have an uncountable set, this approach is not viable anymore. I will not go into details... I expect you to agree that for the \"uniform probability in $[0,1]$, the probability of a set $\\{x\\}$ is $0$ for every $x \\in [0,1]$.\nThe same reasoning applies if you are considering not probabilities, but the length of a set. It is true that if the interval $I$ is the disjoint sum of two other intervals $J$ and $K$, then the length of $I$ will be the sum of the lengths of $J$ and $K$. But $[0,1]$ is the disjoint union of the sets of the form $\\{x\\}$, whose length is $0$. Nevertheless, the length of $[0,1]$ is not $0$. For that reason, we do not talk about the size or the probability of points in $\\Omega$. We talk about the probabilities or size of subsets of $\\Omega$.\nPoint 2. We know the size of certain sets (think of the intervals).\nUsually, we know the measure of certain subsets. For example, in the case of the unit interval $[0,1]$, one usually takes the size of an interval $[a,b]$ to be the value $b - a$.\nPoint 3. The sets for which we do have a probability defined is the family $\\mathcal{B}$. Those are the \"measurable\" sets.\nBased on the size of this simple sets, we can manage to EXTEND our measure to other sets. The next simpler case is when the set is the finite disjoint union of intervals. It happens that, given the constraints we want the measure to satisfy, not always it is possible to EXTEND the measure to the whole family of subsets of $\\Omega$. So, we are happy to limit the domain of our measure $\\mu$ to some class $\\mathcal{B}$ of subsets of $\\Omega$. We shall use the notation $(\\Omega, \\mathcal{B})$ to indicate that we are talking about the family $\\mathcal{B}$ of subsets of $\\Omega$. So, the measure is a function\n$$\n  \\mu: \\mathcal{B} \\to [0,1].\n$$\nPoint 4. A measurable function $f: \\Omega \\to X$ transports the probability in $(\\Omega, \\mathcal{B})$ to a probability $(X, \\mathcal{F})$.\nNow, suppose that you have a probability $\\mu: \\mathcal{B} \\to [0,1]$ defined for a family of subsets of $\\Omega$. And also, suppose that you have a function $f: \\Omega \\to \\mathbb{R}$. Then, you may wish to TRANSPORT your probability from $\\Omega$ to $\\mathbb{R}$. For example, suppose that $\\Omega = \\{1,\\dotsc,6\\}$ is a dice, and you are gambling. If the value of the dice is odd then you get BRL 10, if it is even, then you lose BRL 10. This is the definition of $f: \\Omega \\to \\{-10,10\\}$. Now, instead of talking about a probability in $\\Omega$, we can talk about the probability of, in one bet, getting or losing 10 Brazilian Reals. We transported the probability in $\\Omega$ to a probability in $\\mathbb{R}$. This is a measurable function! The probability of getting BRL 10 is the probability of the event $f^{-1}(10)$, and the probability of losing BRL 10 is the probability of $f^{-1}(-10)$. The probability of losing money is the probability of the set $f^{-1}((-\\infty,0))$.\nIf you think that $f^{-1}$ is a function that takes subsets of $\\mathbb{R}$ to subsets of $\\Omega$, then you can TRY to compose $\\mu$ with $f^{-1}$ to get $\\mu \\circ f^{-1}$. In order for this to work, if you want to know the probability of a set $A \\subset \\mathbb{R}$, you will need that $f^{-1}(A) \\in \\mathcal{B}$.\nPoint 5. We want $f^{-1}(I)$ to be measurable.\nFinally, since we are talking about a function $f: \\Omega \\to \\mathbb{R}$, it might happen that we want the probabilities to be defined at least for the intervals. That is, given an interval $I \\subset \\mathbb{R}$, we want $f^{-1}(I)$ to have a probability associated with it.\nPoint 6. We got to a definition of \"measurable function\" which is easier to state without appealing to measure theory.\nBut $f^{-1}(I)$ will be measurable for every interval $I$ exactly when $f^{-1}([-\\infty,a))$ is measurable for every $a$.\nPoint 7. We can integrate measurable functions (and get the \"expected value\").\nWith a function $f$ like this, we can calculate the mean, that is, the integral of the function.\nNow, I realise that you are not talking about probabilities, you are talking about analysis. But then, you just have to change the terms \"probability\" by measure. And for the same reason, technicalities aside, you can calculate the integral of measurable functions. It is just a bit harder to understand because now $\\Omega = \\mathbb{R}$.", "meta": {"post_id": 125122, "input_score": 22, "output_score": 56, "post_title": "Intuitively, how should I think of Measurable Functions?"}}
{"input": "Let $A$ and $B$ be two sets of real numbers. Define the distance from $A$ to $B$ by $$\\rho (A,B) = \\inf \\{ |a-b| : a \\in A, b \\in B\\} \\;.$$ Give an example to show that the distance between two closed sets can be $0$ even if the two sets are disjoint.", "output": "Let $A = \\mathbb N$ and let $B = \\left\\{n+\\frac{1}{2n} :n\\in \\mathbb N\\right\\}$.  Then A and B are closed and disjoint, but $$\\inf \\{|a\u2212b|:a \\in A,b \\in B\\} = \\inf \\left | \\frac{1}{2n}\\right| = 0$$", "meta": {"post_id": 125709, "input_score": 26, "output_score": 40, "post_title": "Example to show the distance between two closed sets can be 0 even if the two sets are disjoint"}}
{"input": "In my introductory Analysis course, we learned two definitions of continuity.\n$(1)$ A function $f:E \\to \\mathbb{C}$ is continuous at $a$ if every sequence $(z_n) \\in E$ such that $z_n \\to a$ satisfies $f(z_n) \\to f(a)$.\n$(2)$ A function $f:E \\to \\mathbb{C}$ is continuous at $a$ if $\\forall \\varepsilon>0, \\exists \\delta >0:\\forall z \\in E, |z-a|<\\delta \\implies |f(z)-f(a)|<\\varepsilon$.\nThe implication $(2)\\implies(1)$ is trivial (though I will happily post a proof if there is sufficient interest).  The proof of the implication $(1)\\implies(2)$ is worth remarking on, though.\nProof that $(1)\\implies(2)$:\nSuppose on the contrary that $\\exists \\varepsilon>0:\\forall \\delta>0, \\exists z \\in E:\\left (|z-a|<\\delta \\; \\mathrm{and} \\; |f(z)-f(a)|\\ge \\varepsilon\\right )$.  Let $A_n$ be the set $\\{z\\in E:|z-a|<\\frac{1}{n} \\; \\mathrm{ and }\\; |f(z)-f(a)|\\ge\\varepsilon\\}$.  Now use the Axiom of Choice to construct a sequence $(z_n)$ with $z_n \\in A_n \\; \\forall n \\in \\mathbb{N}$.  But now $a-\\frac{1}{n}<z_n<a+\\frac{1}{n}\\; \\forall n \\in \\mathbb{N}$ so $z_n \\to a$.  So $f(z_n) \\to f(a)$.  But $|f(z_n)-f(a)|\\ge\\varepsilon\\; \\forall n \\in \\mathbb{N}$, which is a contradiction.\nYou will have noticed that the above proof uses the Axiom of Choice (the lecturer didn't explicitly spell out the dependence, but it's definitely there).  My question is: is it possible to prove that $(1) \\implies (2)$ without using the Axiom of Choice.  I strongly suspect that it isn't.  In that case, can anyone prove that we have to use the Axiom of Choice?  I can think of three ways to do this:\n(A) Show that $\\left( (1) \\implies (2)\\right)\\implies \\mathrm{AC}$.  I suspect that this statement is untrue. This is definitely untrue, as Arthur points out, because I only used the axiom of countable choice, which is strictly weaker than AC.\n(B) Show that $(1)\\implies (2)$ is equivalent to some other statement known to require the Axiom of Choice (the obvious example being the well-ordering of the real numbers).\n(C) Construct or show the existence of a model of ZF in which there exist sequences which satisfy $(1)$ but not $(2)$.\nOf course, if anyone can think of another way, I would be very interested to hear about it.\nOne final note - I am aware that very many theorems in Analysis use the Axiom of Choice in one way or another, and that this is just one example of such a theorem.  If there exists a model of ZF like the one described in (C), is the study of Analysis in that model interesting?", "output": "The implication $(1)\\to (2)$ can be proved in ZF alone, though this requires major revision of the argument.\nAssume that $f:\\Bbb R\\to\\Bbb R$ is sequentially continuous (i.e., satisfies (1)). Let $x\\in\\Bbb R$ be arbitrary.\n\nClaim: $f\\upharpoonright(\\Bbb Q\\cup\\{x\\})$ is continuous at $x$.\nProof: Enumerate $\\Bbb Q=\\{q_n:n\\in\\omega\\}$. If $f\\upharpoonright(\\Bbb Q\\cup\\{x\\})$ is not continuous at $x$, there is an $\\epsilon>0$ such that for each $k\\in\\omega$, $$A_k\\triangleq\\{q\\in\\Bbb Q:|q-x|<2^{-k}\\text{ and }|f(q)-f(x)|\\ge\\epsilon\\}\\ne\\varnothing\\;.$$ Let $$n(0)=\\min\\{k\\in\\omega:q_k\\in A_0\\}\\;.$$ Given $n(m)$, let $$n(m+1)=\\min\\{k\\in\\omega:k>n(m)\\text{ and }q_k\\in A_{m+1}\\}\\;.$$ Then $\\langle q_{n(k)}:k\\in\\omega\\rangle\\to x$, but $|f(q_{n(k)})-f(x)|\\ge\\epsilon$ for all $k\\in\\omega$, which is a contradiction. Note that no choice was used in this construction. $\\dashv$\n\nNow let $\\epsilon>0$. It follows from the Claim that there is a $\\delta>0$ such that $|f(x)-f(q)|\\le\\epsilon$ whenever $q\\in\\Bbb Q$ and $|x-q|\\le\\delta$. Now suppose that $y\\in\\Bbb R$ with $|x-y|\\le\\delta$. Let $I$ be the closed interval whose endpoints are $x$ and $y$. For each $k\\in\\omega$ let $$n(k)=\\min\\{m\\in\\omega:q_m\\in I\\text{ and }|q_m-y|<2^{-k}\\}\\;.$$ Then $\\langle q_{n(k)}:k\\in\\omega\\rangle\\to y$, so $\\langle f(q_{n(k)}):k\\in\\omega\\rangle\\to f(y)$. However, each $q_{n(k)}\\in I$, so $|q_{n(k)}-x|\\le\\delta$ for each $k\\in\\omega$, and therefore $|f(q_{n(k)})-f(x)|\\le\\epsilon$ for each $k\\in\\omega$. Clearly this implies that $|f(y)-f(x)|\\le\\epsilon$ as well.\nThus, we\u2019ve shown that for each $\\epsilon>0$ there is a $\\delta>0$ such that $|f(y)-f(x)|\\le\\epsilon$ whenever $|y-x|<\\delta$, which is sufficient.\nThis argument is expanded from the ZF proof of Theorem 3.15 in Horst Herrlich, Axiom of Choice, Lecture Notes in Mathematics 1876. Note that it is not true in ZF that a function $f:\\Bbb R\\to\\Bbb R$ is continuous at a point $x$ iff it is sequentially continuous at $x$: in Theorem 4.54 he proves that this assertion is equivalent to $\\mathbf{CC}(\\Bbb R)$, the assertion that every countable family of non-empty subsets of $\\Bbb R$ has a choice function.\nAdded: I failed to notice that the functions in the original question are complex-valued, not real-valued. However, the argument can easily be adapted to $\\Bbb R^2$, replacing $\\Bbb Q$ by $\\Bbb Q^2$ and using the $\\max$ norm.\nAdded2: The only slightly tricky bit is figuring out what to use for $I$. Suppose that $x=\\langle x_1,x_2\\rangle,y=\\langle y_1,y_2\\rangle\\in\\Bbb R^2$ with $0<\\|x-y\\|\\le\\delta$. If $x_1\\ne y_1$ and $x_2\\ne y_2$, let $$I=\\big[\\min\\{x_1,y_1\\},\\max\\{x_1,y_1\\}\\big]\\times\\big[\\min\\{x_2,y_2\\},\\max\\{x_2,y_2\\}\\big]\\;.$$ If $x_1\\ne y_1$ and $x_2=y_2$, let $$I=\\big[\\min\\{x_1,y_1\\},\\max\\{x_1,y_1\\}\\big]\\times\\big[x_2,x_2+\\delta\\big]\\;,$$ and if $x_1=y_1$ and $x_2\\ne y_2$ let $$I=\\big[x_1,x_1+\\delta\\big]\\times\\big[\\min\\{x_2,y_2\\},\\max\\{x_2,y_2\\}\\big]\\;.$$\nLet $\\Bbb Q^2=\\{q_n:n\\in\\omega\\}$ be an enumeration of $\\Bbb Q^2$, and for $k\\in\\omega$ let $$n(k)=\\min\\{m\\in\\omega:q_m\\in I\\text{ and }\\|q_m-y\\|<2^{-k}\\}\\;.$$ Everything else is the same as before, except that $|\\cdot|$ must be replaced throughout by $\\|\\cdot\\|$, where $$\\|\\langle x,y\\rangle\\|=\\max\\{|x|,|y|\\}\\;.$$\nWith very minor modification this works for $\\Bbb R^n$ for any $n\\in\\Bbb Z^+$.", "meta": {"post_id": 126010, "input_score": 71, "output_score": 35, "post_title": "Continuity and the Axiom of Choice"}}
{"input": "Is it possible to calculate and find the solution of $ \\; \\large{105^{1/5}} \\; $ without using a calculator? Could someone show me how to do that, please?\nWell, when I use a Casio scientific calculator, I get this answer: $105^{1/5}\\approx \" 2.536517482 \"$. With WolframAlpha, I can an even more accurate result.", "output": "I go back to the days BC (before calculators).  We did have electricity, but you had to rub a cat's fur to get it.\nWe also had slide rules, from which a $2$  to  $3$ place answer could be found quickly, with no battery to go dead in the middle of an exam. Engineering students wore theirs in a belt holster.  Unfortunately, slide rules were expensive, roughly the equivalent of two meals at a very good restaurant.   For higher precision work, everyone had a book of tables.\nMy largish book of tables has the entry $021189$ beside $105$. This means that $\\log(105)=2.021189$ (these are logarithms to the base $10$, and of course the user supplies the $2$). Divide by $5$, which is trivial to do in one's head (multiply by $2$, shift the decimal point). We get $0.4042378$. \nNow use the tables backwards. The log entry for $2536$ is $404149$, and the entry for $2537$ is $414320$. Note that our target $0.4042378$ is about halfway between these. We conclude that $(105)^{1/5}$ is about $2.5365$. \nThe table also has entries for \"proportional parts,\" to make interpolation faster. As for using the table backwards, that is not hard. Each  page of the $27$ page logarithms section has in a header the range of numbers, and the range of logarithms. The page I used for reverse lookup is headed \"Logs $.398\\dots$ to $.409\\dots$.\"\nThere are other parts of the book of tables that deal with logarithms, $81$ pages of logs of trigonometric functions (necessary for navigation, also for astronomy, where one really wants good accuracy).  And of course there are natural logarithms, only $17$ pages of these. And exponential and hyperbolic functions, plus a few odds and ends.", "meta": {"post_id": 127310, "input_score": 94, "output_score": 72, "post_title": "Root Calculation by Hand"}}
{"input": "Find the number of arrangements of $k \\mbox{  }1'$s, $k \\mbox{  }2'$s, $\\cdots, k \\mbox{  }n'$s - total $kn$ cards - so that\nno same numbers appear consecutively. For $k=2$ we can compute it by using the \nPIE, and it is $$\\frac{1}{2^n} \\sum_{i=0}^n (-1)^i \\binom{n}{i} (2n-i)! 2^i$$", "output": "I believe the answer is given by $$\\int_0^\\infty e^{-x} q_k(x)^n \\, dx$$ where $q_k(x) = \\sum_{i=1}^k \\frac{(-1)^{i-k}}{i!} {k-1 \\choose i-1}x^i$ for $k\\geq 1$, and $q_0(x) = 1$.  In general if we allow $k_i$ of the $i$th number the answer should be  $$\\int_0^\\infty e^{-x} \\prod_i q_{k_i}(x) \\, dx$$\nYou can check that this agrees with the sequences oeis.org/A114938 and oeis.org/A193638 above.  I do not (quite) have a proof of this, although I'm very close. The method is my own, and has not been published anywhere as far as I know. I'd be happy to give you more information in private, but I'm not sure I want to expose it publicly until it's proven.  Please let me know if you think this is noteworthy and any potential applications.\nEdit:  Following some information given to me by Byron, I found that this formula is already known and that in fact $q_n(x) = (-1)^{n}L_n^{(-1)}(x)$ where $L_n^{(\\alpha)} (x) $ denotes the generalized Laguerre polynomial.  See Section 6 here for a labelled version. I should have mentioned this sooner; thanks Byron!", "meta": {"post_id": 129451, "input_score": 21, "output_score": 38, "post_title": "Find the number of arrangements of $k \\mbox{  }1'$s, $k \\mbox{  }2'$s, $\\cdots, k \\mbox{  }n'$s - total $kn$ cards."}}
{"input": "Are there some good overviews of basic formulas about addition, multiplication and exponentiation of cardinals (preferably available online)?", "output": "$\\newcommand{\\alnul}{\\aleph_0}\\newcommand{\\mfr}[1]{\\mathfrak{#1}}\\newcommand{\\Ra}{\\Rightarrow}\\newcommand{\\card}[1]{\\left|#1\\right|}\\newcommand{\\powerset}[1]{\\mathcal P(#1)}\\newcommand{\\Lra}{\\Leftrightarrow}\\newcommand{\\Zobr}[3]{#1\\colon#2\\to#3}$I have no doubt that you there are many useful online resources for these, but many such identities are available here at MSE, together with their proofs.\nI'll give a list of some basic results on cardinal arithmetics and I'll add links to results, which have proofs here at MSE. I am making this CW, so feel free to add more identities and pointers to further useful questions and answers.\nIn the identities below, $a$, $b$, $c$ denote arbitrary cardinals, $X$ is an arbitrary set, $\\alnul$ is the cardinality of $\\mathbb N$ and $\\mfr c=2^{\\alnul}$. Cardinality of a set $X$ is denoted by $\\card X$ and $\\powerset X$ is the notation of the power set of $X$.\nEquality of cardinal numbers is defined as follows:\n$$|A|=|B| \\Lra \\text{ there exists a bijection }\\Zobr fAB.$$\nInequality of cardinal numbers is defined as follows:\n$$|A|\\le|B| \\Lra \\text{ there exists an injective function }\\Zobr fAB.$$\nThe definitions of the operations on cardinal numbers (addition, multiplication, exponentiation) can be found e.g. in this answer.\nValidity of Axiom of Choice is assumed. If you want to learn about cardinals without AC, you can have a look e.g. at this question: Defining cardinality in the absence of choice\n\n\n$a\\le b$ $\\land$ $b\\le c$ $\\Ra$ $a\\le c$\n\n\nThis follows from the fact that composition of two injective maps is injective\n\n\n$a\\le b \\land b\\le a \\Ra a=b$\n\n\nThis result is known as Schr\u00f6der\u2013Bernstein theorem. (Also called Cantor\u2013Bernstein theorem or Cantor\u2013Schr\u00f6der\u2013Bernstein theorem.)\n\nIf $A$ and $B$ are sets such that $|A|\\le |B|$ and $|B|\\le|A|$, then $|A|=|B|$.\n\nThe following results says that for any two sets the cardinalities are comparable.\n\n\nFor any two sets $A$, $B$ we have $|A|\\le|B|$ $\\lor$ $|B|\\le|A|$. I.e. any two sets/any two cardinal numbers are comparable.\n\n\nNote: Proof of this result uses the Axiom of Choice, but it is not required. For the role of AC in this result see here: Is the class of cardinals totally ordered? and For any two sets $A,B$ , $|A|\\leq|B|$ or $|B|\\leq|A|$ and Proving $(A\\le B)\\vee (B\\le A)$ for sets $A$ and $B$.\n\n\n$|A|\\le|B| \\Lra (\\text{there exists a surjective function }\\Zobr fBA \\text{ or $A$ is empty}).$\n\n\nThere exists an injection from $X$ to $Y$ if and only if there exists a surjection from $Y$ to $X$.\nNote: Proof of this result uses the Axiom of Choice.\n\n\n$\\card{\\powerset X}=2^{\\card X}$\n\n\nSee e.g. How to show equinumerosity of  the powerset of $A$ and the set of functions from $A$ to $\\{0,1\\}$ without cardinal arithmetic? or Finding a correspondence between $\\{0,1\\}^A$ and $\\mathcal P(A)$ or this answer. The question\nWhat is the set of all functions from $\\{0, 1\\}$ to $\\mathbb{N}$ equinumerous to? deals with a special case, but it can be easily generalized.\n\n\n$a+b=b+a$\n\n\nThis follows simply from commutativity of union: $A\\cup B=B\\cup A$.\n\n\n$a+(b+c)=(a+b)+c$\n\n\nThis follows from associativity of union: $A\\cup(B\\cup C)=(A\\cup B)\\cup C$.\n\n\n$b\\le c \\Ra a+b\\le a+c$\n\n\nThis is (after adding some details) basically the same thing as the implication $B\\subseteq C$ $\\Ra$ $A\\cup B\\subseteq A\\cup C$. See Does $a \\le b$ imply $a+c\\le b+c$ for cardinal numbers?\n\n\n$ab=ba$\n\n\nA bijection $A\\times B\\to B\\times A$ can be given by $(x,y)\\mapsto (y,x)$.\n\n\n$a(bc)=(ab)c$\n\n\nA bijection between $A\\times(B\\times C)$ and $(A\\times B)\\times C$ can be given by $(x,(y,z))\\mapsto ((x,y),z)$. See here: A bijection between $X \\times (Y \\times Z)$ and $ (X \\times Y) \\times Z$\n\n\n$a(b+c)=ab+ac$\n\n\nThis follows from the fact that $A\\times(B\\cup C)=A\\times B\\cup A\\times C$.\n\n\n$b\\le c \\Ra  ab\\le ac$\n\n\nSee e.g. Proof of cardinality inequality: $m_1\\le m_2$, $k_1\\le k_2$ implies $k_1m_1\\le k_2m_2$ or Will $\\kappa_1, \\kappa_2, m$ cardinals. Given $\\kappa_1 \\leq \\kappa_2$. prove: $\\kappa_1 \\cdot m \\leq \\kappa_2 \\cdot m$\n\n\n$a^2=a\\cdot a$\n\n\nSee e.g. this answer\n\n\n$a\\le b \\Ra a^c\\le b^c$\n\n\nSee e.g. this answer.\n\n\n$a\\le b \\land c\\ne 0 \\Ra c^a\\le c^b$\n\n\nSee e.g. this question.\nNote that this is not true for $c=0$, since $0^0=1$. (The set $\\emptyset^\\emptyset=\\{\\emptyset\\}$ has one element.) The set $\\emptyset^\\emptyset$ and its cardinality is also discussed here.\n\n\n$a^{b+c}=a^b\\cdot a^c$\n\n\nSee e.g. Let $A,B,C$ be sets, and $B \\cap C=\\emptyset$. Show $|A^{B \\cup C}|=|A^B \\times A^C|$ and Notation on proving injectivity of a function $f:A^{B\\;\\cup\\; C}\\to A^B\\times A^C$.\n\n\n$(a^b)^c=a^{bc}$\n\n\nSee e.g. How to show $(a^b)^c=a^{bc}$ for arbitrary cardinal numbers?\n\n\n$(ab)^c=a^c\\cdot b^c$\n\n\nSee e.g. Equinumerousity of operations on cardinal numbers and How to prove $|{^A}{(K \\times L)}| =_c |{^A}{K} \\times {^A}{L}|$?\n\n\n$a^b\\le 2^{ab}$\n\n\nSee e.g. this answer\n\n\n$a<2^a$\n\n\nThis is Cantor's theorem. The question Is the class of subsets of integers countably infinite? deals with the special case $a=\\alnul$, but there are answers which discuss the more general result or can be easily generalized. See also Understanding the proof for $ 2^{\\aleph_0} > \\aleph_0$. This question asks about the general result: Cardinality of a set A is strictly less than the cardinality of the power set of A\n\n\n$\\alnul+\\alnul=\\alnul$\n\n\nSee e.g. Let $X$ and $Y$ be countable sets. Then $X\\cup Y$ is countable\n\n\n$a\\ge\\alnul \\Ra \\alnul+a=a$\n\n$\\alnul\\cdot\\alnul=\\alnul$\n\n\n\nSee e.g. Bijecting a countably infinite set $S$ and its cartesian product $S \\times S$, How does one get the formula for this bijection from $\\mathbb{N}\\times\\mathbb{N}$ onto $\\mathbb{N}$?,\nThe cartesian product $\\mathbb{N} \\times \\mathbb{N}$ is countable and Proving the Cantor Pairing Function Bijective\nThe following result is, to some extent, related to the following:\n\n\nUnion of countably many countable sets is countable.\n\n\nIt is worth mentioning that proof of this uses Axiom of Choice. See Prove that the union of countably many countable sets is countable.\n\n\n$\\alnul^{\\alnul}=2^{\\alnul}=\\mfr c$\n\n\nSee e.g. Is $\\aleph_0^{\\aleph_0}$ smaller than or equal to $2^{\\aleph_0}$? or What is $\\aleph_0$ powered to $\\aleph_0$? or What's the cardinality of all sequences with coefficients in an infinite set? (One of the answers to this question also discusses powers of the form $\\aleph_\\alpha^{\\aleph_\\beta}$ in general.\n\n\nIf $a$ is infinite cardinal, then $a^2=a$.\n\n\nSee e.g. About a paper of Zermelo\nNote: Proof of this result uses the Axiom of Choice. See also: For every infinite $S$, $|S|=|S\\times S|$ implies the Axiom of choice\n\n\nIf $b$, $c$ are infinite cardinals, then $b+c=bc=\\max\\{b,c\\}$.\n\n\nSee e.g. How to prove that from \"Every infinite cardinal satisfies $a^2=a$\" we can prove that $b+c=bc$ for any two infinite cardinals $b,c$?\nNote: This is a consequence of the preceding result, so it relies on the Axiom of Choice too.", "meta": {"post_id": 131212, "input_score": 78, "output_score": 104, "post_title": "Overview of basic results on cardinal arithmetic"}}
{"input": "I'm terribly confused on the concept of \"rank of a linear transformation\". My book keeps using it, but it doesn't clarify what it means (or at least I haven't been able to find it). Is it the same as the rank of the matrix? For example, if A is a mxn matrix, what would be the rank(A)?", "output": "First: linear transformation vs. matrix.\nMatrices.\nA matrix is a rectangular array, in the context of linear algebra the entries are always elements of the ground field (in your case, probably either the real numbers or the complex numbers). An $n\\times m$ matrix has $n$ rows and $m$ columns.\nIf $A$ is an $n\\times m$ matrix, the rank of $A$ is the dimension of the row-space (the subspace of $\\mathbb{R}^m$, viewed as made up of row vectors, spanned by the rows), which equals the dimension of the column-space (the subspace of $\\mathbb{R}^n$, viewed as being made up of column vectors, spanned by the columns).\nThe rank also equals the number of nonzero rows in the row echelon (or reduced row echelon) form of $A$, which is the same as the number of rows with leading $1$s in the reduced row echelon form, which is the same as the number of columns with leading $1$s in the reduced row echelon form.\nThe nullity of the matrix is the dimension of the subspace of $\\mathbb{R}^m$ (viewed as column vectors) of all vectors $\\mathbf{x}$ such that $A\\mathbf{x}=\\mathbf{0}$. This is equal to the number of parameters/degrees of freedom in the general solution to $A\\mathbf{x}=\\mathbf{0}$, and is equal to the number of columns that do not have leading $1$s in the reduced row-echelon form of $A$. \nIn particular, since every column in the reduced row echelon form of $A$ either has a leading $1$ or does not have a leading one, we conclude that\n$$\\mathrm{rank}(A) + \\mathrm{nullity}(A) = \\text{number of columns of }A = m.$$\nThis is sometimes referred to as the Rank-Nullity Theorem in its matrix version.\nLinear transformations.\nGiven two vector spaces $V$ and $W$ (over the same field), a linear transformation is a map $T\\colon V\\to W$ such that $T(\\alpha\\mathbf{v}+\\mathbf{v}') = \\alpha T(\\mathbf{v})+T(\\mathbf{v}')$ for all $\\mathbf{v},\\mathbf{v}'\\in V$ and all scalars $\\alpha$. \nThe image of $T$ is the subspace of $W$ given by $\\{T(\\mathbf{v})\\mid \\mathbf{v}\\in V\\}$. The kernel of $T$ is the subspace of $V$ $\\{\\mathbf{v}\\in V\\mid T(\\mathbf{v}) = \\mathbf{0}\\}$. The rank of $T$ is the dimension of the image of $T$, $\\mathrm{rank}(T) = \\dim(\\mathrm{Im}(T))$. The nullity of $T$ is the dimension of the kernel of $T$, $\\mathrm{nullity}(T) = \\dim(\\mathrm{ker}(T))$. The Rank-Nullity Theorem in its version for linear transformations states that\n$$\\mathrm{rank}(T) + \\mathrm{nullity}(T) = \\dim(V).$$\nConnection between the two.\nAn $n\\times m$ matrix $A$ can be used to define a linear transformation $L_A\\colon\\mathbb{R}^m\\to\\mathbb{R}^n$ given by $L_A(\\mathbf{v}) = A\\mathbf{v}$. If we do this, the kernel of $L_A$ equals the nullspace of $A$, and the image of $L_A$ equals the column-space of $A$. In particular, $\\mathrm{rank}(A) = \\mathrm{rank}(L_A)$, $\\mathrm{nullity}(A)=\\mathrm{nullity}(L_A)$.\nGoing the other way, given a linear transformation $T\\colon V\\to W$, if we pick a basis $\\alpha$ for $V$ and a basis $\\beta$ for $W$, then we can define a matrix called the coordinate matrix of $T$ with respect to $\\alpha$ and $\\beta$, $[T]_{\\alpha}^{\\beta}$, which is a $\\dim(W)\\times\\dim(V)$ matrix whose columns are the coordinate vectors, relative to $\\beta$, of the images under $T$ of the vectors in $\\alpha$. The matrix has the property that for every $\\mathbf{v}\\in V$,\n$$[T]_{\\alpha}^{\\beta}[\\mathbf{v}]_{\\alpha} = [T(\\mathbf{v})]_{\\beta}$$\nwhere $[\\mathbf{v}]_{\\alpha}$ is the coordinate vector of $\\mathbf{v}$ with respect to the basis $\\alpha$, and $[T(\\mathbf{v})]_{\\beta}$ is the coordinate vector of $T(\\mathbf{v})$ with respect to $\\beta$. If we do this, then the rank of $T$ equals the rank of $[T]_{\\alpha}^{\\beta}$, and the nullity of $T$ equals the nullity of $[T]_{\\alpha}^{\\beta}$. \nSo the notions of rank and nullity for matrices and for linear transformations correspond to one another under the correspondence between matrices and linear transformations.", "meta": {"post_id": 131950, "input_score": 17, "output_score": 51, "post_title": "The rank of a linear transformation/matrix"}}
{"input": "Let $A_R$ be the finitely generated abelian group, determined by the relation-matrix \n$$R := \\begin{bmatrix}\n-6 &  111  & -36 & 6\\\\\n5  &  -672 & 210 & 74\\\\\n0 & -255 & 81 & 24\\\\\n-7  &   255       &-81 & -10\n\\end{bmatrix}$$ \nReduce this matrix using Smith Normal Form and determine the isomorphism type of $A_R$. \n\nI know that the Smith Normal Form of this matrix is:\n$$\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & 21 & 0 \\\\\n0 & 0 & 0 & 0 \n\\end{bmatrix}\n$$\nHowever, this was computed using Maple and I need to understand the method of computing this manually which I am struggling to grasp. Can anyone help?", "output": "The computation requires two processes:\n\nrow operations of the type used in Gaussian elimination (with some restrictions because we require that the integer equivalence class be preserved) and the corresponding column operations,\nthe Euclidean algorithm for finding the greatest common divisor of two integers.\n\nSpecifically, you are allowed to\n\ninterchange two rows or two columns,\nmultiply a row or column by $\\pm1$ (which are the invertible elements in $\\mathbf{Z}$),\nadd an integer multiple of row to another row (or an integer multiple of a column to another column).\n\nThe first goal is to reach diagonal form.  Let's first work on column 1: using operation 3 for rows repeatedly, you can, by following the Euclidean algorithm, form a row whose first element is the GCD of the elements in column 1.  You can then obtain a matrix with the GCD in the $(1,1)$ position and zeroes in the rest of column 1.\nNow work on row 1: do the same thing, but using column operations; eventually you will have the GCD of row 1 in the $(1,1)$ position, and zeroes elsewhere in row 1.  You will most likely have messed up column 1, but that's OK.  Go back and redo column 1, then redo row 1, and repeat until all elements in row and column 1 are 0 except for the $(1,1)$ element.  This process is guaranteed to terminate because the GCD gets smaller each time.\nNow you can move on to row/column 2, and repeat the process.  Continue for row/column 3, and so on, until you have reached diagonal form.\nYou may not be done at this point, because the diagonal elements may not satisfy the divisibility requirement of the Smith normal form.  You can, however, enforce this by some additional moves as in the following example:\n$$\n\\begin{bmatrix}8 & 0\\\\0 & 12\\end{bmatrix}\\longrightarrow\\begin{bmatrix}8 & 8\\\\0 & 12\\end{bmatrix}\\longrightarrow\\begin{bmatrix}8 & 8\\\\-8 & 4\\end{bmatrix}\\longrightarrow\\begin{bmatrix}24 & 8\\\\0 & 4\\end{bmatrix}\\longrightarrow\\begin{bmatrix}24 & 0\\\\0 & 4\\end{bmatrix}\\longrightarrow\\begin{bmatrix}4 & 0\\\\0 & 24\\end{bmatrix}.\n$$\nThe idea is again to use the Eulidean algorithm.  After adding column 1 to column 2, you may have to do several row operations to obtain the GCD of the two original diagonal elements.  In the example above, only one row operation was needed for this.\nAddendum: Details for the particular example in the OP's question.\nIf you add row 2 to row 1, and then multiply row 1 by $-1$, you get a pivot of 1.  You can then subtract suitable multiples of row 1 from rows 2 and 4, and end up with\n$$\n\\begin{bmatrix}\n1 & 561 & -174 & -80 \\\\\n 0 & -3477 & 1080 & 474 \\\\\n 0 & -255 & 81 & 24 \\\\\n 0 & -4182 & 1299 & 570\n\\end{bmatrix},\n$$\nwhich, by subtracting multiples of column 1 from columns 2, 3, and 4, leads to\n$$\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n 0 & -3477 & 1080 & 474 \\\\\n 0 & -255 & 81 & 24 \\\\\n 0 & -4182 & 1299 & 570\n\\end{bmatrix}.\n$$\nWe next need to do a series of row operations involving rows 2, 3, and 4 that results in the GCD of 3477, 255, and 4182.  If we always choose the pivot of smallest nonzero magnitude, the steps would be\n\nSubtract 14 times row 3 from row 2, and 17 times row 3 from row 4.\nAdd 2 times row 2 to row 3, and subtract row 2 from row 4.\nSubtract row 4 from row 2, and add row 4 to row 3.\nAdd 3 times row 3 to row 2, and 6 times row 3 to row 4.\nAdd row 2 to row 3, and subtract row 2 from row 4.\nAdd 2 times row 3 to row 2.\nSwap rows 2 and 3.\nNegate row 2.\n\nYou should now have\n$$\n\\begin{bmatrix}\n 1 & 0 & 0 & 0 \\\\\n 0 & 3 & 234 & -1410 \\\\\n 0 & 0 & -651 & 3906 \\\\\n 0 & 0 & -147 & 882\n\\end{bmatrix}.\n$$\nYou can now subtract suitable multiples of column 2 from columns 3 and 4.  Then you just have to deal with the $2\\times2$ block in the lower right.  You should be able to get it from here.", "meta": {"post_id": 133076, "input_score": 34, "output_score": 52, "post_title": "Computing the Smith Normal Form"}}
{"input": "I'm self-studying some complex analysis, and apparently holomorphic bijections between two annuli exist precisely when the ratios of the radii are the same. More exactly, if $A_{\\sigma,\\rho}=\\{z\\in\\mathbb{C}:\\sigma<|z|<\\rho\\}$, then there is a holomorphic bijection between $A_{\\sigma,\\rho}$ and $A_{\\sigma',\\rho'}$ iff $\\rho/\\sigma=\\rho'/\\sigma'$.\nIs there a reference where this fact is proven? Or can a proof be included here if it's not overly involved? Thanks.", "output": "This result (sometimes called Schottky's theorem) can be proved without any heavy machinery like Riemann mapping or even Schwarz reflection. I give two versions of the proof, with comments at the end.\nClaim. If there exists a bijective holomorphic map $f: A_{r,R}\\to A_{s,S}$, then $S/s\\ge R/r$.\nNote that we actually get equality by considering the inverse.\nProof. Normalize to $r=s=1$. By composing $f$ with inversion, we can make sure that $|f(z)|\\to 1$ as $|z|\\to1$. For $1<t<R$ let $A(t)$ denote the area within the Jordan curve $f(\\{z:|z|=t\\})$. There is a standard way to relate the area to the coefficient of the Laurent series $f(z)=\\sum_{n\\in\\mathbb Z} c_n z^n$: namely, use Green's formula for area in complex notation. \n$$A(t)=\\frac{1}{2i} \\int_{0}^{2\\pi}  f(te^{i\\theta}) \\bar f(te^{i\\theta})\\, d\\theta = \\pi \\sum_{n\\in\\mathbb Z}  n|c_n|^2t^{2n}$$ \nSince $A(1+)=\\pi$, we have \n$$(1)\\qquad\\qquad \\sum_{n\\in\\mathbb Z}  n|c_n|^2=1.$$\nFrom here the proof can proceed in at least two ways.\nVersion I: stick to area. Using (1), write\n$$A(t)-\\pi t^2=\\pi t^2 \\sum_{n\\in\\mathbb Z}  n|c_n|^2(t^{2n-2}-1)\\ge0$$\nwhere the inequality holds term-wise. Hence $A(R-)\\ge \\pi R^2$, which implies $S\\ge R$. QED\nVersion II: use $L^2$ norm. For $1<t<R$ define\n$$U(t)=\\frac{1}{2\\pi t} \\int_{|z|=t} |f(z)|^2|dz|$$ \nAgain, this can be put in terms of coefficients either by direct computation with $|f|^2=f\\bar f$, or by Parseval's identity: \n $U(t)= \\sum_{n\\in\\mathbb Z} |c_n|^2 t^{2n}$.\nSince $U(1+)=1$ and \n$$U'(t)-2t= 2t\\sum_{n\\in\\mathbb Z}  n|c_n|^2 (t^{2n-2}-1)\\ge 0 $$\nit follows that $U(R-)\\ge R^2$. But $|f|\\le S$, which implies $S\\ge R$. QED\nComments: the idea of relating coefficients to area as in (1) goes back to Gronwall. In the context of this problem it was used by P\u00f3lya and Szeg\u0151 in Problems and Theorems in Analysis II, part IV, problem 83 on page 15. Their solution is essentially Version I. I learned it from a short note by Pietro Poggi-Corradini. \nThe idea of using $L^2$ norm instead of area may look counterproductive, since Version II is longer. But it is a more robust argument, which works for harmonic maps as well. Robert Burckel and Pietro pointed out Version II to us as a simplification of $L^2$ argument specialized to holomorphic maps.", "meta": {"post_id": 133578, "input_score": 33, "output_score": 34, "post_title": "When can we find holomorphic bijections between annuli?"}}
{"input": "Is axiom of choice required to show the existence of non-measurable sets? Is there a Lebesgue non-measurable set that can be constructed without axiom of choice?\nRelated question on MO says it is consistent:\nhttps://mathoverflow.net/questions/73902/axiom-of-choice-and-non-measurable-set", "output": "The answer is, you cannot.\nIt is consistent with ZF that the real numbers are a countable union of countable sets, this implies that every set of reals is Borel and therefore measurable. Of course, in such model it is nearly impossible to develop the analysis we know.\nHowever it is consistent relative to an inaccessible cardinal that there is a model of ZF+DC where all the sets of real numbers are Lebesgue measurable, and DC allows us to do most of classical analysis too.\nNon-measurable sets can be generated by free ultrafilters over $\\mathbb N$ too, which as remarked is a strictly weaker assumption that the axiom of choice. If there are $\\aleph_1$ many real numbers and DC holds then there is an non-measurable set as well, which implies that ZF+DC($\\aleph_1$) also implies the existence of non-measurable sets of real numbers - however  this is not enough to imply the existence of free ultrafilters over the natural numbers!\nSeveral other ways to generate non-measurable sets of real numbers:\n\nThe axiom of choice for families of pairs;\nHahn-Banach theorem;\nThe existence of a Hamel basis for $\\mathbb R$ over $\\mathbb Q$.\n\nThere are several other ways as well, but none are quite close to the full power of the axiom of choice.\nOne important remark is that we can ensure that the axiom of choice holds for the real numbers as usual, but breaks in many many severe ways much much further in the universe (that is counterexamples will be sets generated much later than the real numbers in the von Neumann hierarchy). This means that the axiom of choice is severely negated - but the real numbers still behave as we know them.\nThe above constructions and to further read about ways to construct non-measurable sets cf. Horst Herrlich, Axiom of Choice, Lecture Notes in Mathematics v. 1876, Springer-Verlag (2006).", "meta": {"post_id": 133999, "input_score": 32, "output_score": 34, "post_title": "Can one construct a non-measurable set without Axiom of choice?"}}
{"input": "I am trying to get a better grasp of representation theory. I was asking myself \"what is the essential difference between representations of some group $G$ and a $KG$ module? How are they related, and what is the distinction?\"\nWhat's confusing me is: I can understand matrix representations of a group in a simple way, since they are isomorphic to some permutation group, but what about a module? How do I get things cleared out? I need some insight.", "output": "There's essentially no real difference between modules and representations. Think of them as two sides of the same coin.\nGiven a $\\mathbb{K}G$-module $V$, you have a linear action of $G$ on a $\\mathbb{K}$-vector space $V$. This in turn gives you a homomorphism from $G$ to $\\mathrm{GL}(V)$ (invertible $\\mathbb{K}$-linear endomorphisms). Such a homomorphism is a representation. And then this can be turned around. Given a representation, you get an associated module.\nSpecifically, let $V$ be a $\\mathbb{K}G$-module and let $g,h \\in G$, $v,w\\in V$, and $c\\in\\mathbb{K}$. Give a name to the map: $v \\mapsto g\\cdot v$ say: $\\varphi(g):V \\to V$ (so $\\varphi(g)(v)=g \\cdot v$). Then $\\varphi(g)(v+cw)$ $=g\\cdot(v+cw)$ $=g\\cdot v+cg\\cdot w$ $=\\varphi(g)(v)+c\\varphi(g)(w)$. Thus $\\varphi(g)$ is $\\mathbb{K}$-linear. Then because $\\varphi(1)$ is the identity map ($1 \\cdot v=v$) and $\\varphi(g^{-1})(\\varphi(g)(v))=g^{-1}\\cdot g\\cdot v=(g^{-1}g\\cdot v=1\\cdot v=v$ etc. we get $\\varphi(g)$ is an invertible linear map. Therefore: $\\varphi:G \\to \\mathrm{GL}(V)$. Moreover, $\\varphi(gh)=\\varphi(g)\\varphi(h)$ (easy to check) so $\\varphi$ is a homomorphism (which we call a representation). Without going into the details, this all reverses.\nSo $\\mathbb{K}G$-modules = representations of $G$ on $\\mathbb{K}$-vector spaces.\nIf you've studied group actions, you've already seen this type of correspondence. Let $G$ act on $X$. Then the map $x \\mapsto g \\cdot x$ turns out to be a bijection on $X$. Thus if we define $\\varphi(g)(x)=g\\cdot x$ for all $x\\in X$, then $\\varphi(g) \\in S(X)$ (permutations on $X$). Moreover, $\\varphi(gh)=\\varphi(g)\\varphi(h)$ so $\\varphi : G\\to S(X)$ is a group homomorphism. We call such things permutation representations. And again this can be reversed. Given a permutation representation: $\\varphi:G \\to S(X)$, one can define  a group action $g \\cdot x \\equiv \\varphi(g)(x)$. \nSo $G$-action on $X$ = permutation representation of $G$ on $X$.\nIf you look into other branches of algebra, you'll see this kind of thing over and over again: Lie algebra modules = Lie algebra representations etc.\nIt's just different points of view. You can either think of \"Algebra Thing\" acting on \"Thing\" or a homomorphism from \"Algebra Thing\" to Maps from \"Things to Things\".", "meta": {"post_id": 134062, "input_score": 27, "output_score": 43, "post_title": "Understanding the difference between group representations and modules"}}
{"input": "For a (complex valued) sequence $(a_n)_{n\\in\\mathbb{N}}$ there is the associated generating function\n$$\nf(z) = \\sum_{n=0}^\\infty a_nz^n$$\nand the $z$-Transform\n$$\nZ(a)(z) = \\sum_{n=0}^\\infty a_nz^{-n}$$\nwhich only differ by the sign of the exponent of $z$, that is, both are essentially the same and carry the same information about the sequence, though encoded slightly differently. The basic idea is the same: associate a holomorphic function with the sequence and use complex calculus (or formal power series).\nHowever, the engineering books I know which treat the $Z$-transform do not even mention the word \"generating function\" (well one does but means the generator of a multiscale analysis...) and the mathematics books on generating function do not mention the $Z$-transform (see for example \"generatingfunctionology\").\nI am wondering: Why is that? Has one formulation some advantage over the other? Or is it just for historical reasons?", "output": "I see three questions here:\n\nShouldn't exist more awareness about the fact that Z-transform (ZT) and generating functions (GF) are almost the same thing?\n\nI think so. I've always found this strange and unfortunate, and I'd like to see in every textbook about ZT or GF a footnote (\"The 'generating functions' employed in combinatorial mathematics are basically the same thing as the Z-transform\" and viceversa). \n\nAre they (apart from the change of sign) really the same thing?\n\nFormally, they are obviously the same thing, but the context is different:\nIn the Z-transform $x[n] \\leftrightarrow X(z) $, the input is usually double-sided (the sum runs over all integers), the \"right sided\" transform is less used. Further, in signal processing, $x[n]$ is almost always one of these: 1) a signal, 2) the impulse response of a LTI filter (causal or not), 3) a (auto/cross) correlation function. \nHence, $x[n]$ is typically either bounded and decreasing for $n\\to \\pm \\infty$ (for the case of filters and correlations) or (for the case of stochastic signals) stationary zero-mean sequences. \nThe generating function, instead, is usually applied to right-sided sequences (i.e. any $f:\\mathbb{N} \\to \\mathbb{R}$). Apart from that, they are arbitrary; they often grow without bounds.\nBecause the ZT is applied to double-sided input, then the mapping $x[n] \\leftrightarrow X(z) $ is not one-to-one: to have a unique inverse, we need to specify a ROC (region of convergence) of $X(z)$, in the complex plane.\nFor GF the problem of unicity does not arise, the ROC is implied. (However, as pointed out in a comment, the radius of convergence can be relevant to characterize some sequence properties).\nThe Z-transform $X(z)$  is not usually regarded as a formal series, but as a \"true\" complex function. And because of the AR/MA/ARMA models that are usually considered in classical signal processing, we almost always deal with rational functions, which can be characterized in terms of zeros and poles. \nThe ZT transform is naturally thought as a generalization of the Fourier transform, as typically $x[n]$ is square summable (with perhaps the addition of sinusoids - or countable Dirac deltas in the transform). This correspondence is given by the natural mapping $z \\leftrightarrow e^{jw}$, i.e. the DTF is the ZT along the unit circle in the complex plane (same as the continuous Fourier transform is the Laplace transform along the $y$ axis). And the classic concepts (e.g. energy per frequency band) are normally pertinent and useful. In the GF scenario, we don't often think of Fourier transforms.\n\nWhy the different sign?\n\nThe different convention can be understood from the previous difference. Regarding the ZT as a generalization of the DFT, the negative sign is more natural (the input is expressed as a \"synthesis\" of sinusoids). BTW: this gives a ROC that for causal signals -or right handed transform- extends \"to the exterior\" of the largest pole; which in turns implies the common rule: a stable causal filter must have its poles inside the unit circle.\nFor the GF, being just a formal series, it feels more natural to use positive exponents.", "meta": {"post_id": 137178, "input_score": 41, "output_score": 39, "post_title": "Why do engineers use the Z-transform and mathematicians use generating functions?"}}
{"input": "I've been working with DG-algebras for the last year, and was able to obtain using them some nice commutative homological algebra results.\nHowever, I keep hearing about a (more general???) concept of $E_\\infty$-rings, as a better notion for doing non-linear homological algebra.\nI was wondering if someone could give a down to earth explanation - what are $E_\\infty$-rings? What are they good for? \nAre they equivalent (in some sense) to super-commutative DG-algebras? Why do people prefer them over DG-algebras?\nAnd what are recommended references for studying them, from the point of view of commutative algebra?\nThank you!", "output": "I don't know much about dg-algebras, but I can at least tell you what an $E_\\infty$-ring is.    Hopefully somebody else will come along and fill in the gaps, as well as correct the mistakes I'm sure I'll make along the way.  The first thing to know is that \"$E_\\infty$-ring\" is actually short for \"$E_\\infty$-ring spectrum\".  I'll assume you know what a spectrum is, but of course just say so if you don't.\nA ring spectrum is a (naive) spectrum $X$ with a unit map $S^0 \\rightarrow X$ and a multiplication map $X \\wedge X \\rightarrow X$ such that in the homotopy category, $X$ has the structure of a ring-object.  (Recall that all spectra are abelian-group-objects up to homotopy; that's sort of the point.)  We could throw in the word \"commutative\", too.  But either way, this doesn't suit all of our needs by a long shot; often we don't want to pass to the homotopy category, but neither is it reasonable to restrict to ring-objects in the original category either.  So our compromise is that we still say that $X$ should only be a ring-object in the homotopy category, but we also want to remember the homotopies that make the axiom-diagrams commute, and moreover we want them all to be coherent in an appropriate way.\nTo figure out what this means, for the moment let's just start with $A_\\infty$ ($A$ stands for \"associative\", whereas $E$ stands for \"everything\", i.e. associative and commutative), and let's just talk about spaces.  We begin with a space $X$ equipped with a unit map $\\eta:\\mbox{pt} \\rightarrow X$ and a multiplication map $\\mu:X \\times X \\rightarrow X$ which satisfy the usual axioms.  (The unit map always has the unit for the monoidal structure as its source.)\nThis is supposed to be homotopy-associative, which first and foremost implies that $\\mu\\circ (1 \\times \\mu) \\simeq \\mu \\circ (\\mu \\times 1):X^{3} \\rightarrow X$.  This is witnessed by a homotopy $m_3:I \\times X^3 \\rightarrow X$; that is, we've got an interval parametrizing a whole family of triple-multiplications, such that on one end we've got $(ab)c$ and on the other end we've got $a(bc)$.\nThe next step, then, is to see what happens at 4-fold multiplications $X^4 \\rightarrow X$.  It turns out that there are five ways to parenthesize $abcd$, namely: $((ab)c)d$, $(ab)(cd)$, $a(b(cd))$, $a((bc)d)$, $(a(bc))d$.  If you look at what I've done, you'll see that these sort of sit naturally at the vertices of a pentagon $P$, and each edge corresponds to one re-association, i.e. an application of $m_3$.  For these all to be coherent, we demand that we can extend these maps $\\partial P \\times X^4 \\rightarrow X$ to a map $m_4:P \\times X^4 \\rightarrow X$ interpolating between all possible ways of associating four factors.\nFrom here, you can see what the general definition should be for \"higher coherence of homotopy-associativity\": there's a family of spaces $\\{\\mathcal{A}_n\\}_{n\\geq 0}$ and certain structure maps between them (which are messy and which I won't write down), and we're asking for a family of maps $m_n:\\mathcal{A}_n \\times X^n \\rightarrow X$ -- or equivalently a single map $m:\\coprod_{n\\geq 0} \\mathcal{A}_n \\times X^n \\rightarrow X$ -- which respect the structure maps (in a way that I also won't write down).  This family $\\{\\mathcal{A}_n\\}_{n\\geq 0}$ and its structure maps are known as the $A_\\infty$-operad, and a space $X$ along with such a family of maps is known as an algebra over this operad.  So $\\mathcal{A}_3=I$ and $\\mathcal{A}_4=P$; these spaces $\\mathcal{A}_n$ are known as (Stasheff) associahedra.  They are all contractible; the point is that even if there isn't a single unique $n$-fold multiplication, the next best thing is that they're interpolated in a homotopically trivial way.  (Level 0 is supposed to pick out the unit map, so we take $\\mathcal{A}_0=\\mbox{pt}$ and we need to agree that $X^0=\\mbox{pt}$; level 1 is supposed to pick out the identity map, so $\\mathcal{A}_1=\\mbox{pt}$ too, and this requirement is wrapped up in the structure maps.)\nThis is all part of a much bigger story, of course.  $\\{\\mathcal{A}_n\\}_{n\\geq 0}$ is an example of a non-symmetric operad; this is just a family of spaces with structure maps of the same signature.  Non-symmetric operads form a model category.  Its terminal object is the \"associative operad\", which parametrizes strictly-associative multiplication (so its spaces are all just $\\mbox{pt}$), and any cofibrant replacement may be considered as \"an\" $A_\\infty$-operad.\nThis leads us to $E_\\infty$-spaces; the $E_\\infty$-operad is a symmetric operad.  The difference is that the $n^{th}$ space $\\mathcal{E}_n$ comes with an action of the $n^{th}$ symmetric group $\\Sigma_n$, and the $n^{th}$ structure map for an algebra $X$ over this operad now takes the form $\\mathcal{E}_n \\times_{\\Sigma_n} X^n \\rightarrow X$ (where $\\Sigma_n$ acts by permutation on $X^n$).  The $E_\\infty$-operad has all its spaces contractible and all the symmetric actions free; in other words, $\\mathcal{E}_n = E\\Sigma_n$.  The one I like to think about is $\\mathcal{E}_2=E\\Sigma_2 = S^\\infty$, and the associated structure map $S^\\infty \\times_{\\Sigma_2} X^2 \\rightarrow X$.  A path in $S^\\infty$ from the north pole to the south pole (the space of which is contractible!) gives us a homotopy from some multiplication $\\mu:X^2 \\rightarrow X$ to $\\mu\\circ \\tau:X^2 \\rightarrow X$, where $\\tau:X^2\\rightarrow X^2$ is the twist map.  In other words, for $\\mu$ to satisfy the level-2 condition of describing an $E_\\infty$-multiplication on $X$, not only must there be a homotopy from $\\mu$ to $\\mu\\circ\\tau$, but the space of such homotopies that we keep track of must be contractible.\nThe language of operads that I've described easily carries directly over to spectra: an $E_\\infty$-ring spectrum is just a spectrum which is an algebra over the $E_\\infty$-operad (either the same one, using the fact that you can smash spaces and spectra, or else a version internal to the category of spectra).  $E_\\infty$-ring spectra are important in homotopy theory for example because they give rise to power operations (e.g. the Steenrod squares, i.e. the natural transformations of functors $H^m(-;\\mathbb{F}_2)\\rightarrow H^n(-:\\mathbb{F}_2)$), but there are external, algebraic reasons why they're interesting, too.  Most notably, $E_\\infty$-ring spectra are exactly those ring spectra whose categories of module-spectra admit a good notion of tensor product and internal hom and which are simplicially bitensored (meaning that there are good notions of tensoring a module with a simplicial set and the mapping-object of a simplicial set into a module, in a way compatible with the simplicial structure on the category of modules in the first place).  This allows you to do homological algebra, as you suggest, as well as what is known as \"derived\" algebraic geometry.\nLastly, I'd like to point out that the examples of spaces and spectra are even more intimately related than you might think.  The $E_n$-operad is also known as the \"little $n$-disks operad\"; its $n^{th}$ space is the space of ordered configurations of $n$ disjoint little $n$-disks (or, homotopy-equivalently, points) sitting inside the unit $n$-disk $D^n$.  An $E_n$-algebra structure on a space $X$ is precisely the same as an $n$-fold delooping $X_n$ of $X$ (i.e. a homeomorphism $X \\cong \\Omega^n X_n$).  The $E_\\infty$-operad is the direct limit of the $E_n$-operads (indeed, a model for $E\\Sigma_n$ is ordered configurations of $n$ points in $D^\\infty$), and hence an $E_\\infty$-algebra structure on $X$, assuming $X$ is \"group-like\" (i.e. $\\pi_0X$ is a group instead of just a monoid) is precisely the same as a connective-$\\Omega$-spectrum (i.e. an $\\Omega$-spectrum indexed on the non-negative integers with $X$ as its $0^{th}$ space).  In fact, assuming you've chosen a good $E_\\infty$-operad, the categories of group-like $E_\\infty$-spaces and connective spectra are Quillen equivalent.  Under this correspondence, an honest ring $R$ (viewed as a discrete space) corresponds to the Eilenberg-MacLane spectrum $HR$, which represents the cohomology theory $H^*(-;R)$, and up to equivalence, these are precisely the $E_\\infty$-ring spectra which have trivial homotopy groups away from dimension 0.  Thus, the category of $E_\\infty$-ring spectra is a vast enlargement of the category of ordinary rings.  In particular, algebraic geometers get excited because $\\mathbb{Z}$ is no longer initial -- the sphere spectrum is -- and hence \"derived\" algebraic geometry allows one to work over a deeper base.  There's some philosophy along the lines of \"the sphere spectrum is the K-theory spectrum of the field with one element\", but this is way out of my depth so I'll stop here.\n=============\nEDIT: For what it's worth, since posting the answer above I have learned a little bit about the relationship between dg-algebras and their cousins.  Here is what I can say.\nGiven an ordinary commutative ring $R$, there are (at least) three notions of \"derived commutative $R$-algebra\" that one might consider: simplicial $R$-algebras (which I'll denote by $\\mathcal{SCR}_{R/}$), dg $R$-algebras (which I'll denote by $\\mathcal{DGA}_{R/}$), and $E_\\infty$-ring spectra that are $HR$-algebras (where $H$ is the \"Eilenberg--MacLane spectrum\" functor) (which I'll denote by $\\mbox{Alg}_{E_\\infty}(\\mbox{Sp})_{HR/}$).  In general, there are functors\n$$ \\mathcal{SCR}_{R/} \\xrightarrow{f} \\mathcal{DGA}_{R/} \\xrightarrow{g} \\mbox{Alg}_{E_\\infty}(\\mbox{Sp})_{HR/} . $$\nNow, when $R$ is a $\\mathbb{Q}$-algebra, then $f$ induces an equivalence onto the \"connective\" objects (i.e. those that have no homology below degree 0) and $g$ is an equivalence.  However, in general these are not equivalences, and not only for the immediate reason that the objects of $\\mathcal{SCR}_{R/}$ are by construction connective while those of the latter two categories are not.  Indeed, $\\mathcal{SCR}_{R/}$ models the theory of commutative topological $R$-algebras (and weak homotopy equivalences) -- the word \"simplicial\" here is just a technical device to make things cleaner and more combinatorial.  On the other hand, the subcategory $\\mbox{Alg}_{E_\\infty}(\\mbox{Sp})_{HR/}^{\\scriptsize \\mbox{conn}} \\subset \\mbox{Alg}_{E_\\infty}(\\mbox{Sp})_{HR/}$ of connective $E_\\infty$-$HR$-algebras can be thought of as modeling commutative-up-to-coherent-homotopy topological $R$-algebras (i.e., $E_\\infty$, in the sense described earlier in this answer).  In fact, there is an adjunction\n$$ \\mathcal{SCR}_{R/} \\rightleftarrows \\mbox{Alg}_{E_\\infty}(\\mbox{Sp})_{HR/}^{\\scriptsize \\mbox{conn}} , $$\nwhere the left adjoint forgets that the multiplication was strict and the right adjoint takes the \"largest strictly-commutative subobject\".  Moreover, this adjunction is even comonadic: in other words, an object of $\\mathcal{SCR}_{R/}$ is no more or less than an object of $\\mbox{Alg}_{E_\\infty}(\\mbox{Sp})_{HR/}^{\\scriptsize \\mbox{conn}}$ whose natural inclusion from its \"largest strictly-commutative subobject\" is an equivalence.\nOn the other hand, we can also take a few steps back: the notions of dg-$R$-module and $HR$-module spectrum are equivalent (as are the notions of simplicial $R$-module and connective $HR$-module spectrum), and in fact dg-$R$-modules with a strictly associative multiplication do indeed model $A_\\infty$-$HR$-algebra spectra.  It's only when we pass to the commutative setting that we can no longer model the \"up to coherent homotopy\" version with the \"on the nose\" version.  Of course, one can still model $E_\\infty$-$HR$-algebra spectra via (the appropriate notion of) $E_\\infty$-dg-$R$-algebras...\nFor a bit more (including the comonadicity statement and a nice explanation via the two notions of \"the affine line\" in $E_\\infty$-ring spectra), I would recommend reading section 2.6 of Lurie's thesis (available as a link towards the bottom), pp. 45-50.", "meta": {"post_id": 137764, "input_score": 25, "output_score": 45, "post_title": "What are $E_\\infty$-rings?"}}
{"input": "I think $\\frac{d}{dx} \\int f(x) dx = f(x)$ right? So $\\frac{d}{dx} \\int^b_a f(x) dx = [f(x)]^b_a = f(a)-f(b)$? But why when: \n$$f(x) = \\int^{x^3}_{x^2} \\sqrt{7+2e^{3t-3}}$$\nthen \n$$f'(x) = \\color{red}{(x^3)'}\\sqrt{7+2e^{3x-3}} - \\color{red}{(x^2)'}\\sqrt{7+2e^{3x-3}}$$\nWhere did the $(x^3)'$ and $(x^2)'$ come from?", "output": "$\\int_a^bf(x)\\,dx$ is a number, so ${d\\over dx}\\int_a^bf(x)\\,dx=0$. \nNow suppose $\\int g(x)\\,dx=F(x)$. Then $\\int_{x^2}^{x^3}g(t)\\,dt=F(x^3)-F(x^2)$, so ${d\\over dx}\\int_{x^2}^{x^3}g(t)\\,dt=(x^3)'F'(x^3)-(x^2)'F'(x^2)=3x^2g(x^3)-2xg(x^2)$.", "meta": {"post_id": 139183, "input_score": 19, "output_score": 36, "post_title": "Differentiating Definite Integral"}}
{"input": "I've recently read about a number of different notions of \"degree.\"  Reading over Javier \u00c1lvarez' excellent answer for the thousandth time finally prompted me to ask this question:\n\nHow exactly do the following three notions of \"degree\" coincide?\n\n(1) Algebraic Topology. Let $f\\colon X \\to Y$ be a continuous map between compact connected oriented $n$-manifolds.\nWikipedia tells me that $H_n(X) \\cong H_n(Y) \\cong \\mathbb{Z}$, and that a choice of orientations for $X$ and $Y$ amount to choices of generators $[X], [Y]$ for $H_n(X), H_n(Y)$, respectively.  We then define $\\deg f$ via $$f_*([X]) = (\\deg f)[Y].$$\n(2) Differential Topology. Let $f\\colon X \\to Y$ be a smooth map between oriented $n$-manifolds, where $X$ is compact and $Y$ is connected.\nLet $y \\in Y$ be a regular value of $f$ (which exists by Sard's Theorem), let $D_xf\\colon T_xX \\to T_yY$ denote the derivative (a.k.a. pushforward), and define $$(\\deg f)_y = \\sum_{x \\in f^{-1}(y)}\\text{sgn}(\\det D_xf).$$\nIt can be shown that $(\\deg f)_y$ is independent of the choice of $y \\in Y$, so we can talk meaningfully about a single quantity $\\deg f = (\\deg f)_y$.\n(3) Riemann Surfaces. Let $f\\colon X \\to Y$ be a holomorphic map between compact connected Riemann surfaces.\nFor $x \\in X$, we let $\\text{mult}_x(f)$ denote the multiplicity of $f$ at $x \\in X$.  For $y \\in Y$, we define $$(\\deg f)_y = \\sum_{x \\in f^{-1}(y)} \\text{mult}_x(f).$$\nAs in (2), it can be shown that $(\\deg f)_y$ is independent of the choice of $y \\in Y$.  (Does this generalize to arbitrary complex manifolds?)\n\nThoughts: As was mentioned in my topology class last semester (and also on Wikipedia), there is this concept of \"local homology\" which lets us compute (1) as a sum of \"local degrees.\"\nI imagine that in the case of (2), each of these local degrees is, in fact, equal to $\\text{sgn}(\\det D_xf)$ because $f$ is a local diffeomorphism at each regular point $x$.  I also imagine that in the case of (3), each of these local degrees is, in fact, equal to $\\text{mult}_x(f)$ because the degree of $\\mathbb{S}^n \\to \\mathbb{S}^n$, $z \\mapsto z^k$ is $k$.  (Does this also mean that $f$ is not regular at any point where $\\text{mult}(f) \\geq 2$?  This would make sense, but what is the proof?)\nThis all seems correct in my head, but I would really like more details if possible.", "output": "First things first, thank you very much for your appraisal of my other answer on consequences of degree concepts, mostly for differentiable manifolds. I have in fact expanded it posting another answer listing the applications of degree theory in complex algebraic geometry using the definitions explained below. Indeed I have also spent a great deal of time trying to understand all connections among \"degrees\" as much as possible, so let me try to complete a little bit your list and my digression with the, in my view, most important, geometric and unifying notion of degree: that coming from complex algebraic geometry.\n(4) ALGEBRAIC GEOMETRY OF COMPLEX PROJECTIVE VARIETIES, that is to say, complex submanifolds $X$ of, or embeddings into, the complex projective space $\\mathbb{CP}^n$. Any non-singular projective $n$-variety is isomorphic, in the algebraic category, to a subvariety of $\\mathbb{CP}^{2n+1}$ [Shafarevich vol. I, 5.4 Th.9]. By Chow's theorem [Griffiths-Harris, p.167][Mumford Cor.4.6] any complex manifold seen as $k$-submanifold of $\\mathbb{CP}^n$ is algebraic, i.e. any real $2k$-submanifold of $\\mathbb{RP}^{2n}$ which admits a complex structure is actually given as the zero locus of a system of homogeneous polynomials (all varieties coming from manifolds are smooth, but there are singular varieties which are not manifolds though!). In particular any real closed orientable surface admits a complex structure and so is a complex projective algebraic curve, thus including case (3) of compact Riemann surfaces [Miranda, Th.IV.1.9]; higher dimensional manifolds may not always admit complex structures, a necessary and sufficient condition is the Newlander-Nirenberg theorem [Kobayashi-Nomizu vol. II, Appx.8][Voisin vol. I, sec.2.2.3]: vanishing of the Nijenhuis tensor for an almost-complex structure. By Lefschetz's principle this is essentially enough for dealing with the general case of abstract varieties (integral separated schemes of finite type over an algebraically closed field $k$) embedded as subschemes of $\\mathbb{P}^n_k$.\nAll the following definitions of degree are proved to be equivalent to each other so one can pick any of them as starting definition and get the rest as interesting theorems. (We always talk about nonsingular irreducible complex projective curves, surfaces, hypersurfaces, varieties... etc., and thus compact Riemann surfaces, except explicit mention of the contrary). I shall only explain in detail the original classical geometric notions of degree and mention the rest.\n\u2022 A. If $X$ is a hypersurface of $\\mathbb{CP}^n$, i.e. $\\dim X=n-1$, by [Hartshorne, Exercise I.2.8] it is given by the zero locus, $X=Z(f)$, of an irreducible homogeneous polynomial $f\\in S_d$ of algebraic degree $d$, i.e. any monomial summand $a_{k_0\\dots k_n}x_0^{k_0}\\cdots x_n^{k_n}$ in $f$ has degree $d=\\sum_i k_i$, where all such monomials generate the abelian group $S_d$, all of which make the ring of polynomials a graded ring $\\mathbb{C}[x_0,\\dots,x_n]=\\bigoplus_{d=0}^\\infty S_d$. So any such $f$ has a canonical associated (algebraic) degree, so\n\nDegree of a hypersurface as the algebraic degree of its defining homogeneous polynomial:  $$\\deg X_{n-1}=\\deg Z(f):=\\deg(f)=d,\\;\\;\\; f\\in S_d$$\n\n\u2022 B. If $k:=\\dim X_k< n-1$, let $L_r\\cong\\mathbb{CP}^r$ be a generic (i.e. in general position) linear variety (linear projective vector subspace) of $\\mathbb{CP}^n$ of dimension $r\\leq n-k-1$. The projecting cone, $C(X_k, L_r)$, of $X_k$ from \"vertex\" $L_r$ is defined to be the joint locus of the subspaces $L_{r+1}$ that join the given $L_r$ with each point of $X_k$ (this generalizes the intuitive \"cone\" of lines obtained by projecting from a point). By [Beltrametti et al., sec.3.4.5] the projecting cone is also a, possibly reducible, algebraic variety of dimension $r+k+1$ (which justifies the upper bound of r at the beginning). For a generic $L_{n-k-2}$ the projecting cone of $X_k$ is thus a, possibly reducible, hypersurface as in A. above (if it is irreducible it is the case A. if it is reducible then its defining zero locus polynomial is reducible but has nevertheless well defined degree). Call\n\nDegree of a subvariety as the degree of the generic projecting cones which are hypersurfaces: $$\\deg X_k:=\\max\\limits_{L\\in\\mathbb{Gr}(n-k-2,\\mathbb{CP}^n)}\\{\\deg C(X_k, L)\\}, $$ where $C(X_k, L_{n-k-2})=Z(g)\\,\\vert\\, g\\in S_p$.\n\nwhere $L$ is an element of the Grasssmannian of the required dimension. The degree of a variety $X_r\\subset\\mathbb{CP}^n$ is thus defined to be the degree of the generic hypersurface-projecting-cone; this is proved to be well-defined as this max deg is constant for a dense Zariski-open subset of the Grassmannian, cf. [Harris, Exercise 18.2]. For example if $L_0$ is a generic point in $\\mathbb{CP}^3$ and $X_1$ a spatial algebraic curve, for each point of $X_1$ there is only one line joining it with $L_0$. Moving along all such points of the curve we obtain a cone swept by the joining lines with the fixed $L_0$, cone which is an algebraic surface, thus the zero locus of an homogeneous polynomial in projective space. So we are calling the degree of the spatial curve the degree of its projecting cone generic surface polynomial. Note that a curve in projective space is generically given by the intersection of two surfaces of possibly different degrees, $X_1=Z(h_1,h_2)\\subset\\mathbb{CP}^3$, so it has no canonical unique polynomial degree as is the case for plane curves. It is also important to remember that any nonsingular algebraic curve (thus Riemann surface) is isomorphic to a smooth spatial curve in $\\mathbb{CP}^3$ [Hartshorne, Cor.IV.3.6][Shafarevich vol. I, sec.5.4 Cor.2] and birational to a plane curve with at most node singularities [Hartshorne, Cor.IV.3.11].\nNote also that the first definition A. above is necessary, since the projecting cone of a hypersurface cannot be defined due to the constraint $r\\leq n-k-1$. So what we have done is defining, for any lower dimensional variety, associated hypersurfaces which have generically well-defined polynomial degree.\n\u2022 C. The \"vertex\" $L_r$ of a generic projecting cone $C(X_k, L_r)$ of a variety $X_k$ is given by $n-r$ linearly independent linear equations: $L_r=Z(h_1,\\dots,h_{n-r})$ where $h_i$ are linear forms which define hyperplanes $H_i=Z(h_i)\\cong\\mathbb{CP}^{n-1}$ within $\\mathbb{CP}^n$, so that $L_r=\\bigcap_{i=1}^{n-r} H_i$. Projecting cones take their name from the fact that they define a generalized projection of a variety to a linear subspace (e.g. projecting from a point a spatial curve into a plane): the projection [Shafarevich vol. I, sec.4.4 Ex.1], with center or vertex $L_r$, is the rational map $\\pi_{L_r}(x):=[h_1(x):\\dots :h_{n-r}(x)]$ which is a regular morphism on the Zariski-open set $\\mathbb{CP}^n\\setminus L_r$. Therefore its restriction to any variety disjoint from the vertex, $\\pi_{L_r}\\vert_{X_k}:X_k\\rightarrow\\mathbb{CP}^{n-r-1}$ is a regular map of it to a projective subspace. Take any linear variety disjoint from $L_r$ as representative, i.e. $\\mathbb{CP}^{n-r-1}\\cong L'_{n-r-1}\\subset\\mathbb{CP}^n$ such that $L_r\\cap L'_{n-r-1}=\\varnothing$, which is always possible by [Beltrametti et al., Th.3.3.8] (since $\\dim L_r\\cap L'_{n-r-1}\\geq r+(n-r-1)-n=-1$ so they do not intersect necessarily). Now for every point $x\\in\\mathbb{CP}^n\\setminus L_r$, in particular $X_k$, there is a unique $L''_{r+1}$ passing through the vertex $L_r$ and $x$ by elementary dimension counting. The locus of all these generators $L''_{r+1}$ is just the generic projecting cone $C(X_k,L_r)$ for generic center $L_r$!. Each generator intersects $L'_{n-r-1}$ in a unique point (solution of a system of $n-(r+1)+n-(n-r-1)=n$ linear equations) which corresponds to $\\pi_{L_r}(x)$ through the isomorphism with $\\mathbb{CP}^{n-r-1}$. Therefore, given a generic linear subspace $L_r$ we can regularly (rationally if $L_r\\cap X_k\\neq\\varnothing$) project any variety $X_k\\subseteq\\mathbb{CP}^n$ to a lower dimensional generic linear subspace $L'_{n-r-1}\\cong\\mathbb{CP}^{n-r-1}$ by intersecting the projecting cone with it, $C(X_k,L_r)\\cap L'_{n-r-1}$, and calling $\\pi_{L_r}(X_k)\\subset \\mathbb{CP}^{n-r-1}$ the projection of $X_k$ from $L_r$ to $L'_{n-r-1}$. The case $r=0$ is the classical projection from a point into a hyperplane (like our spatial curve projected to a plane curve). Therefore for any $X_k$, projecting from a generic center $L_{n-k-2}$, we obtain a, possibly reducible, variety $\\bar{X}_k$ in $\\mathbb{CP}^{k+1}$ as projection; since this comes from the intersection of the hypersurface $C(X_k,L_{n-k-2})$ with a linear variety $L'_{k+1}$, by the projective dimension theorem [Hartshorne, Th.I.7.2] every of its irreducible components has dimension $\\geq (n-1)+(k+1)-n=k$. In fact, if $\\dim X_k\\geq 2$ by repeated application of Bertini's theorem [Hartshorne, Th.II.8.18], any such intersection is generically not only smooth but connected and thus irreducible, thus any generic such projection is a hypersurface $\\bar{X}_k\\subset\\mathbb{CP}^{k+1}$ (generically reducible for $X_1$ a curve) and so it has a defining zero locus irreducible homogeneous polynomial $q\\in\\mathbb{C}[x_0,...,x_{k+1}]$ with well-defined algebraic degree (if $X_1$ is a curve then each of its irreducible components after intersecting will be points solution of a reducible polynomial). This is equivalent to B. since the intersection of a generic projecting cone hypersurface with a generic linear variety has the same polynomial degree as the cone (solve as many variables as possible from the linear system defining the linear variety and substitute in the homogeneous polynomial of the cone; each of its equal-degree monomials produce new monomials in less variables but of the same degree as the original, so one gets a new homogeneous polynomial in less variables, i.e. a hypersurface in a lower-dimensional projective space). This shows a geometric construction for the theorem of the birational equivalence of any projective algebraic set of dimension $k$ with a hypersurface in $\\mathbb{CP}^{k+1}$, cf. [Beltrametti et al., sec.2.6.11] and [Hartshorne, Prop.I.4.9].\n\nDegree of a $k$-subvariety of $\\mathbb{CP}^{n}$ as the polynomial degree of the, possibly reducible, hypersurface obtained by generically projecting to $\\mathbb{CP}^{k+1}$, i.e. intersecting the projecting cone with a suitable generic linear subspace: $$\\deg X_k:=\\deg \\pi_{L}(X_k)=\\deg C(X_k,L_{n-k-2})\\cap L'_{k+1},$$ for generic $L\\in\\mathbb{Gr}(n-k-2,\\mathbb{CP}^n)$ and $L'\\in\\mathbb{Gr}(n-k-2,\\mathbb{CP}^n)$.\n\n\u2022 D. Now take the projected variety hypersurface $\\pi_{L_{n-k-2}}(X_k)= \\bar{X}_k \\subsetneq \\mathbb{CP}^{k+1}$, possibly reducible, and project it again with vertex a generic point $\\bar{L}_0\\in\\mathbb{CP}^{k+1}$ disjoint from $\\bar{X}_k$, onto a generic hyperplane $\\bar{L}_k\\in\\mathbb{Gr}(n-1,\\mathbb{CP}^{k+1})$. It is a standard exercise to prove that any projection from vertex $L_r$ can be decomposed into a sequence of projections from $r+1$ points $L_{0(0)}\\dots L_{0(r)}$ spanning $L_r$, so everything done in C. above can be interpreted as projecting our $k$-variety down from successive $n-k-1$ points to a projective $(k+1)$-space where it becomes a hypersurface, so that one can talk about a generic degree. Therefore, now we are just stopping our chain of projections when we get the surjection $\\pi_{L_{n-k-1}}|_{X_k}:X_k\\twoheadrightarrow\\mathbb{CP}^k$ which comes from  projecting from generic center $L_{n-k-1}$ onto generic linear variety of the same dimension $L'_k$ (each projection from a point reduces by 1 the dimension of the projective space into which we are projecting, so we need $n-k$ independent generic points). By construction the projection onto $L'_k\\cong\\mathbb{CP}^k$ is generically a finite map, since each projection of a hypersurface from a generic point is a line which intersects it in a finite number of points (by the projective dimension theorem), the fiber of the projected point, and this is an equivalent condition for finiteness of a morphism for projective varieties [Harris, Lemma 14.8]. Now, we defined above the degree of $X_k$ to be the degree of its hypersurface projected model into $\\mathbb{CP}^{k+1}$, so another projection from a generic point $\\bar{L}_{0(n-k)}\\in\\mathbb{CP}^{k+1}$ onto generic $\\mathbb{CP}^k\\cong\\bar{L}_k \\subset \\mathbb{CP}^{k+1}$ comes from a line joining the point with each point of $\\bar{X}_k$; as the vertex is generic, this line intersects $\\bar{X}_k$ in a finite number of points which is no other than the degree of its zero locus defining homogeneous polynomial $\\bar{X}_k=Z(q)$! (parametrize the straight line by $[x_0(t):...:x_{k+1}(t)]$ so that the intersection points are the finite number of roots of $g(t)=0$, which are $\\deg(g)$ in number by the fundamental theorem of algebra). It is not hard to convince oneself that the generic projection $\\pi_{L_{n-k-1}}(X_k)$ has the same number of points in its generic fiber as that last component projection which brings it down to $\\mathbb{CP}^k$, since up to $\\mathbb{CP}^{k+1}$ the hyperplanes are higher dimensional than $X_k$. (It is surjective because a line and a hypersurface always intersect in projective space). The number of points in a general fiber is called the degree of the map. It is the same Brower-Kronecker degree of a continuous mapping in Differential Topology, but in the complex case it coincides with the number of pre-images, for complex structure fixes orientation and regular maps=holomorphic maps preserve it because any complex linear transformation (e.g. the Jacobians of the map) are never negative, cf. [Dubrovin et al., Th.13.4.2]. Thus:\n\nDegree of a variety $X_k\\subset\\mathbb{CP}^n$ as the number of pre-images of a generic fiber (i.e. degree of a regular or rational map) of the generic finite surjective projection map $\\pi_{\\Lambda}:X_k\\twoheadrightarrow\\mathbb{CP}^k$: $$\\deg X_k:=\\deg\\pi_{\\Lambda}=\\#\\,\\pi_{\\Lambda}^{-1}(x),$$ for generic $x\\in\\mathbb{CP}^k,\\; \\Lambda\\in\\mathbb{Gr}(n-k-1,\\mathbb{CP}^n).$\n\n\u2022 E.  Following [Beltrametti et al., Prop.3.4.8] let us go back to B. or C. above, our projection of $X_k$ into a hypersurface of $\\mathbb{CP}^{k+1}$ via generic center $L_{n-k-2}\\subset\\mathbb{CP}^{n}$. Take a generic line $l_1\\subset\\mathbb{CP}^{n}$ not contained in $L_{n-k-2}$, so that the linear space $\\operatorname{Join}(L_{n-k-2}, l_1)=\\langle L_{n-k-2}, l_1 \\rangle$ is a generic $L'_{n-k}$ because this is just a projecting cone, thus having dimension $(n-k-2)+(1)+1$ (cf. beginning of B. above). It is clear that any such generic linear $(n-k)$-space can be obtained in this way by generically decomposing it into a line and a linear $n-k-2$-subspace contained in it. Now, the intersection of a $k$-variety with a generic hyperplane has irreducible components of dimension $k-1$ [Shafarevich vol. I, sec.6.2], thus $X_k\\cap L'_{n-k}$ consists generically of a finite number of points. This number of points is constant in a dense Zariski-open subset of the Grassmannian $\\mathbb{Gr}(n-k,\\mathbb{CP}^{n})$, cf. [Harris, Ex.18.2]. This can be readily proved by noticing that it is the number of points in the generic fiber of $\\pi_{\\Lambda}$ with center a generic hyperplane $\\Lambda\\subset L'_{n-k}$ seen in D. above. To see this, note that our generic $L'_{n-k}$ can be thought as a projecting cone with center $\\mathbb{CP}^{n-k-1}\\cong\\Lambda\\subset L'_{n-k}$, and the fiber of $\\pi_{\\Lambda}$, which is finite by D., is by construction the intersection of the projecting cone with the variety. Therefore $\\#\\, (X_k\\cap L'_{n-k})=\\deg \\pi_{\\Lambda}$, showing equivalence with all the previous notions. It is worth mentioning that many (most) classical treatments define degree as this finite number of points of a generic intersection with a linear space of dimension the codimension of the variety. An independent proof is [Mumford, Th.5.1] where it is shown that generic linear $(n-k)$-subspaces meeting transversaly our variety $X_k$, do so in a common number of points: the degree. (This relates to definitions in differential topology of intersections of submanifolds meeting properly, i.e. $T_pX_k\\cap T_pL'_{n-k}={0}$ and $T_pX_k\\oplus T_pL'_{n-k}=T_p\\mathbb{CP}^n$).\n\nDegree of a subvariety as the number of points of intersection (generically transversal) with a generic codimensional linear variety (i.e., $\\dim L=n-\\dim X$): $$ \\deg X_k := \\# (X_k\\cap L_{n-k})= \\# (X_k\\cap C(X_k, \\Lambda_{n-k-1})),$$ for generic $L\\in\\mathbb{Gr}(n-k,\\mathbb{CP}^n)$, $\\Lambda\\in\\mathbb{Gr}(n-k-1,\\mathbb{CP}^n)$.\n\n\u2022 F. The projection map of def. D. is a dominant rational map and as such defines a pullback  inclusion $\\pi_{\\Lambda}^\\ast:K(\\mathbb{CP}^k)\\hookrightarrow K(X_k)$ by $f\\mapsto f\\circ\\pi_{\\Lambda}$ for any rational (meromorphic) function $f\\in \\mathbb{C}(x_0,\u2026,x_k)$, which is a homomorphism of $\\mathbb{C}$-algebras. In fact by [Hartshorne, Th.I.4.4] this establishes a contravariant equivalence of categories between the category of complex projective varieties and dominant rational maps and the category of finitely generated field extensions of $\\mathbb{C}$, thus birational equivalent varieties have isomorphic function fields. Now by [Harris, Prop.7.16], the transcendence degree of the finite field extension is the number of points in the generic fiber of D., showing equivalence to all previous definitions.\n\nDegree as the transcendence degree of the finite field extension of the function field of projective space with respect to the function field of the variety, generically projected to it. $$\\deg X_k:=[K(\\mathbb{CP}^k):K(X_k)],$$ for generic $\\pi_{\\Lambda}^\\ast :K(\\mathbb{CP}^k) \\hookrightarrow K(X_k),\\; \\Lambda\\in\\mathbb{Gr}(n-k-1,\\mathbb{CP}^n).$\n\n\u2022 G.\n\nDegree as $\\dim X!$ times the leading coefficient of the Hilbert polynomial of the variety: $$P_{X}(t)=\\frac{\\deg X}{\\dim X!}t^{\\dim X}+\\dots,$$ where $P_{X}(t):=\\dim_{\\mathbb{C}}(\\mathbb{C}[X]\\cap S_t),\\, t\\gg 0.$ is the dimension of the $t$-graded homogeneous part of the coordinate ring of the variety, which is proved to be a polynomial in $t$ for large grading [Shafarevich vol. II, sec.4.2].\n\n\u2022 H.\n\nDegree as coefficient in homology $H_{2k}(\\mathbb{CP}^n,\\mathbb{Z})$ $$[X_k]=(\\deg X_k)\\cdot [L_k],$$ or integration coefficient in de Rham cohomology $H^{2k}_{dR}(\\mathbb{CP}^n,\\mathbb{C})$ $$\\langle [X_k],\\omega\\rangle=\\deg X_k\\cdot\\langle[L_k],\\omega \\rangle\\Leftrightarrow \\int_{X_k}\\omega=\\deg X_k\\cdot\\int_{L_k}\\omega.$$\n\n\u2022 I.\n\nDegree as coefficient in the linear equivalence class of the generic projection to a divisor in $\\mathbb{CP}^{\\dim X+1}$, given by the possibly reducible hypersurface $\\pi_{L_{n-k-2}}(X_k)=:\\bar{X}_k$: $$[\\bar{X}_k]\\sim^{lin.}(\\deg X_k)\\cdot [L_k],$$ where linear equivalence is considering every divisor mod a rational divisor, i.e. $[D]\\in \\operatorname{Cl}\\,(\\mathbb{CP}^{k+1}) :=\\operatorname{Div}(\\mathbb{CP}^{k+1})/\\sim^{lin.}$ with $D_1\\sim^{lin.}D_2 :\\Leftrightarrow \\exists f\\in\\mathbb{C}(X_k)$ such that $D_1-D_2=\\operatorname{div}(f)$ and Div is the free abelian group generated by the hypersurfaces. Equivalently, degree as the coefficient in the rational equivalence class of the Chow group of order $\\dim X$, i.e. $[X_k]\\in A_k(\\mathbb{CP}^n)$: $$[X_k]\\sim^{rat.}(\\deg X_k)\\cdot [L_k],$$\n\n\u2022 J.\n\nDegree as intersection number of self-intersecting $\\dim X$ times the hyperplane twisting sheaf $\\mathcal{O}_{\\mathbb{P}^n_k}(1)\\vert_{X}$, where the intersection of invertible sheaves (line bundles) is defined by$$(\\mathcal{L_1}\\cdot ... \\cdot\\mathcal{L_m})_{X}:=\\chi_{X}-\\sum_i\\chi_{X}(\\mathcal{L}_i^{-1})+\\sum_{i<j}\\chi_{X}(\\mathcal{L}_i^{-1}\\otimes\\mathcal{L}_j^{-1})-\\cdots+(-1)^n\\chi_{X}(\\mathcal{L}_1^{-1}\\otimes\\cdots\\otimes\\mathcal{L}_m^{-1})$$ for $m\\geq k$ line bundles on $X_k$, and $\\chi_X$ the Euler characteristics of the bundle. That is to say: $$\\deg X:= (\\mathcal{O}_{\\mathbb{P}^n_k}(1)\\vert_{X})^{\\dim X}.$$", "meta": {"post_id": 140123, "input_score": 64, "output_score": 63, "post_title": "How do different definitions of \"degree\" coincide?"}}
{"input": "A little bird told me that if $2^n+1$ is prime, then $n$ is a power of $2$. I tend not to trust talking birds, so I'm trying to verify that statement independently.\nSuppose $n$ is not a power of $2$. Then $n = a \\cdot 2^m$ for some $a$ not a power of $2$ and some integer $m$. This gives $2^n+1 = 2^{a \\cdot 2^m}+1$. Now I suspect there's a way to factor that, but I don't see how. Can someone give me a hint?", "output": "Hint. For any odd natural number $a$, the polynomial $x+1$ divides $x^a+1$ evenly.\n\n In particular, we have $$ \\frac{x^a+1}{x+1}=\\frac{(-x)^a-1}{(-x)-1}=1-x+x^2-\\cdots+ (-x)^{a-1}$$ by the geometric sum formula. In this case, specialize to $x=2^{2^{\\large m}}$ and we have a nontrivial divisor.\n\n (Also, $x^a+1\\equiv(-1)^a+1\\equiv-1+1\\equiv0\\mod x+1$ inside $\\Bbb Z[ x]$ is pretty slick.)", "meta": {"post_id": 140804, "input_score": 35, "output_score": 36, "post_title": "If $2^n+1$ is prime, why must $n$ be a power of $2$?"}}
{"input": "exact duplicate of Lebesgue measurable but not Borel measurable\nBUT! can you please translate Miguel's answer and expand it with a formal proof? I'm totally stuck...\n\nIn short: Is there a Lebesgue measurable set that is not Borel measurable?\nThey are an order of magnitude apart so there should be plenty examples, but all I can find is \"add a Lebesgue-zero measure set to a Borel measurable set such that it becomes non-Borel-measurable\". But what kind of zero measure set fulfills such a property?", "output": "Let $\\phi(x)$ be the Cantor function, which is non-decreasing continuous function on the unit interval $\\mathcal{U}_{(0,1)}$. Define $\\psi(x) = x + \\phi(x)$, which is an increasing continuous function $\\psi: [0,1] \\to [0,2]$, and hence for every $y \\in [0,2]$, there exists a unique $x \\in [0,1]$, such that $y = \\psi(x)$. Thus $\\psi$ and $\\psi^{-1}$ maps Borel sets into Borel sets.\nNow choose a non Borel subset $S \\subseteq \\psi(C)$. Its preimage $\\psi^{-1}(S)$ must be Lebesgue measurable, as a subset of Cantor set, but it is not Borel measurable, as a topological mapping of a non-Borel subset.", "meta": {"post_id": 141017, "input_score": 53, "output_score": 63, "post_title": "Lebesgue measurable set that is not a Borel measurable set"}}
{"input": "Why is it natural or useful to organize objects (of some appropriate category) into exact sequences? Exact sequences are ubiquitous - and I've encountered them enough to know that they can provide a very useful and efficient framework to work within. However, I have no idea what this framework truly is, or why it is effective.\nSo, my questions are:\n\nWhat makes exact sequences natural objects to deal with?\nWhat do they encode, generally speaking? Or if you are unable to think of a satisfactory answer in general, what are some specific examples of exact sequences encoding some desirable property?\n\nPlease set me straight! It seems like all of the references that I've come across only encyclopedically develop the idea of an exact sequence, sparing the reader of any qualification or exposition.", "output": "This was too long to put as a comment, I apologize if it doesn't help.\nI don't know how totally accurate this is, but I like to think of (short) exact sequences as being algebraified versions of fiber bundles. Thus, putting $X$ in a short exact sequence $0\\to Y\\to X\\to Z\\to0$ indicates to me that $X$ is put together in some way from $Y$ and $Z$, and in such a way that, in a perfect world where everything is nice, is just the product of $Y$ and $Z$. Therefore, $X$ is some kind of \"twisted product\" of $Y$ and $Z$.\nThus, any time we are able to put $X$ into an exact sequene we should (in spirit) be able to tell properties of $X$ from properties of $Y$ and $Z$.\nFor example, knowing that $B$ is an abelian groups such that \n$$0\\to A\\to B\\to C\\to 0$$\nis a SES for $B,C$ also abelian groups tells me that $\\text{rank}(B)=\\text{rank}(A)+\\text{rank}(C)$ (or more generally this works nicely for modules over PIDs). \nThe reason that SESs are such a convenient framework to deal with the notion of \"put-togetheredness\" is that we live in a fundamentally arrow obsessed world. Things phrased entirely in terms of arrows make us happy, because they are often easy to deal with.", "meta": {"post_id": 141215, "input_score": 61, "output_score": 45, "post_title": "What are exact sequences, metaphysically speaking?"}}
{"input": "The ring axioms require that addition is commutative, addition and multiplication are associative, multiplication distributes over addition. \nA field can be thought of as two groups with extra distributivity law. \nA ring is more complex: with abelian group and a semigroup with extra distributivity law.\nIs a ring a more basic structure than a field, or vice versa? What's the relation between them? What's the background why people study them?", "output": "A ring is an ordered triple, $(R,+,\\times)$, where $R$ is a set, $+\\colon R\\times R\\to R$ and $\\times\\colon R\\times R\\to R$ are binary operations (usually written in in-fix notation) such that:\n\n$+$ is associative.\nThere exists $0\\in R$ such that $0+a=a+0=a$ for all $a\\in R$.\nFor every $a\\in R$ there exists $b\\in R$ such that $a+b=b+a=0$.\n$+$ is commutative.\n$\\times$ is associative.\n$\\times$ distributes over $+$ on the left: for all $a,b,c\\in R$, $a\\times(b+c) = (a\\times b)+(a\\times c)$.\n$\\times$ distributes over $+$ on the right: for all $a,b,c\\in R$, $(b+c)\\times a = (b\\times a)+(c\\times a)$.\n\n1-4 tell us that $(R,+)$ is an abelian group. 5 tells us that $(R,\\times)$ is a semigroup. 6 and 7 are the two distributive laws that you mention.\nWe also have the following items:\na. There exists $1\\in R$ such that $1\\times a = a\\times 1 = a$ for all $a\\in R$.\nb. $1\\neq 0$.\nc. For every $a\\in R$, $a\\neq 0$, there exists $b\\in R$ such that $a\\times b = b\\times a = 1$.\nd. $\\times$ is commutative.\nA ring that satisfies (1)-(7)+(a) is said to be a \"ring with unity.\" Clearly, every ring with unity is also a ring; it takes \"more\" to be a ring with unity than to be a ring.\nA ring that satisfies (1)-(7)+(a,b,c) is said to be a division ring. Again, eveyr division ring is a ring, and it takes \"more\" to be a division ring than to be a ring. (5)+(a)+(b)+(c) tell us that $(R-\\{0\\},\\times)$ is a group (note that we need to remove $0$ because (c) specifies nonzero, and we need (b) to ensure we are left with something).\nA ring that satisfies (1)-(7)+(a,b,c,d) is a field.  Again, every field is a ring.\nWe do indeed have that $(R,+)$ is an abelian group, that $(R-\\{0\\},\\times)$ is an abelian group, and that these structures \"mesh together\" via (6) and (7). In a ring, we have that $(R,+)$ is an abelian group, that $(R,\\times)$ is a semigroup (or better yet, a semigroup with $0$), and that the two structures \"mesh well\".\nWe have that every field is a division ring, but there are division rings that are not fields (e.g., the quaternions); every division ring is a ring with unity, but there are rings with unity that are not division rings (e.g., the integers if you want commutativity, the $n\\times n$ matrices with coefficients in, say, $\\mathbb{R}$, $n\\gt 1$, if you want noncommutativity); every ring with unity is a ring, but there are rings that are not rings with unity (strictly upper triangular $3\\times 3$ matrices with coefficients in $\\mathbb{R}$, for instance). So\n$$\\text{Fields}\\subsetneq \\text{Division rings}\\subsetneq \\text{Rings with unity} \\subsetneq \\text{Rings}$$\nand\n$$\\text{Fields}\\subsetneq \\text{Commutative rings with unity}\\subsetneq \\text{Commutative rings}\\subsetneq \\text{Rings}.$$", "meta": {"post_id": 141249, "input_score": 62, "output_score": 91, "post_title": "What is difference between a ring and a field?"}}
{"input": "Prove that \n\nif $f$ is of bounded variation in $[a,b]$, it is the difference of two positive, monotonic increasing functions; and \nthe difference of two bounded monotonic increasing functions is a function of bounded variation.", "output": "Let $f$ a function of bounded variation. Let $F(x):=\\sup \\sum_{j=1}^{n-1}|f(x_{j+1})-f(x_j)|=:\\operatorname{Var}[a,x]$, where the supremum is taken over the $x_1,\\ldots,x_n$ which satisfy $a=x_1<x_2<\\ldots<x_n=x$. Since $f$ is of bounded variation, $F$ is bounded, and by definition increasing. Let $G:=F-f$. We have to show that $G$ is bounded and increasing. Boundedness follows from this property for $f$ and $F$, now fix $a\\leq x_1<x_2\\leq b$. We have \n$$G(x_2)-G(x_1)=F(x_2)-f(x_2)-F(x_1)+f(x_1)\\geq 0$$\nbecause $\\operatorname{Var}[a,x_1]+f(x_2)-f(x_1)\\leq \\operatorname{Var}[a,x_1]+|f(x_2)-f(x_1)|\\leq \\operatorname{Var}[a,x_2]$.\nIf $f$ and $g$ are of bounded variation so is $f-g$. If $f$ is increasing then we have, if $a=x_0<x_1<\\ldots<x_n=b$ that $\\sum_{j=1}^{n-1}|f(x_{j+1})-f(x_j)|=|f(b)-f(a)|$, so $f$ is of bounded variation. So the difference of two bounded monotonic increasing functions is of bounded variation.", "meta": {"post_id": 141338, "input_score": 38, "output_score": 58, "post_title": "Bounded variation, difference of two increasing functions"}}
{"input": "I thought many results in calculus need axiom choice.\nFor example, I thought one needs AC to prove that a bounded sequence in the real line has a convergent subsequence.\nRecently I was taught that one only needs mathematical induction to prove it.\nSo here are my questions.\nCan most results in calculus be proved without AC?\nIf yes, what are the exceptions, to name a few?\nObviously a theorem which uses Zorn's lemma most likely does need AC.\nSo please exclude obvious ones.\nEdit\nBy \"without AC\", I mean without any form of AC, i.e. countable or not, dependent or not. In other words, within ZF.\nEdit\nOne of the motivations of my question is as follows.\nPeople often unconciously use AC to prove theorems.\nAnd it often turns out that their uses of AC are unnecessary.\nFor example, an infinite subset of a compact metric space has a limit point.\nIn his book \"Principles of mathematical analysis\", Rudin proves this by choosing a suitable neighborhood of every point of the space.\nHe uses AC here, though he doesn't say so.\nHowever, since a compact metric space is separable, you can avoid AC to prove this theorem.\nEdit\nI'll make the above statement clearer. You can even avoid countable AC to prove the above theorem.\nIn other words, you can prove it within ZF.\nEdit\nI'll make my questions clearer and more specific.\nBy calculus, I mean classical analysis in Euclidean spaces.\nTake, for example, Rudin's \"Principles of mathematical analysis\".\nCan all the results in this book be proved within ZF?\nIf not, what are the exceptions?", "output": "To do classical analysis (i.e. things in $\\mathbb R^n$) you don't need the whole axiom of choice. Mostly you would need the axiom of countable choice, and rarely the stronger principle of dependent choice.\nFor example, the proof that continuity by $\\varepsilon$-$\\delta$ is equivalent to continuity by sequences (at a given point $x$) requires the axiom of countable choice.\nWe often use definitions by induction to define a certain sequence, the induction itself tells us that if we have defined $a_n$ we can define $a_{n+1}$ - so for a given finite number we can define a sequence longer that this number. We are usually interested, though, in the case where there is an infinite sequence, this is exactly where the axiom of choice comes into play. It tells us that there is such sequence and that we can use it.\nHowever not everything requires the axiom of choice. For example the Heine-Borel theorem that states that closed and bounded intervals are compact (every open cover has a finite sub-cover) requires no choice whatsoever. Similarly a continuous function everywhere is sequentially continuous everywhere and vice versa (while requiring continuity at a single point at a time needs some choice).\nDo note that general theorems about general functions/sequences/sets usually require some choice, on the other particular and very specific cases may be proved without it. \nSome words to address the second edit: It is true that most of the people simply work in ZFC, and why shouldn't they? It's a very comfortable system and it saves you the need to verify certain things (e.g. if you want to use countable choice you have to keep checking that the families you choose from are countable). On the other hand it is also very true that most results people use require far less than the full axiom of choice, at least in elementary analysis.\nRegardless to the above, once you get to functional analysis then the axiom of choice become more apparent (e.g. assuming the Ultrafilter lemma + Krein-Milman theorem implies AC in full). It is natural, too, that the larger your sets become the more choice you will need, and once you start talking about classes of spaces (e.g. all Hilbert spaces, or all locally-convex spaces) instead of a specific space - the more choice you will need to cover the general case.\nSimilarly in the \"small\" cases you can get away without choice (or with very very little) if you have a very specific case at hand. If, on the other hand, you want to prove a general theorem you might need to use some choice principle directly.\nDoing analysis without any choice whatsoever is very hard and very limiting. I suppose it can be challenging and fun to people who find pleasure in that, much as I find pleasure in studying the possible structure of the universe without the axiom of choice. My suggestion is to examine your internal drive for this question: if you merely wish to learn on the axiom of choice and what is possible without it, I suggest picking up some books about mathematics and the axiom of choice:\n\nHerrlich, H. The Axiom of Choice (Springer, 2006)\nMoore, G. H. Zermelo's Axiom of Choice (Dover Publications, 2012)\nFremlin, D. H. Measure Theory, vol. 5 (Torres Fremlin, 2000)\nSchechter, E. Handbook of Analysis and Its Foundations (Academic Press, 1997).\n\nIf on the other hand you simply wish to reject the axiom of choice, and you have decided to work without it, I am certain there are references suited for that, alas I am unfamiliar with any of them.\n\nFurther reading:\n\nContinuity and the Axiom of Choice\nFoundation for analysis without axiom of choice?\nTerry Tao's blog: Soft analysis, hard analysis, and the finite convergence principle.", "meta": {"post_id": 141666, "input_score": 20, "output_score": 39, "post_title": "Axiom of choice and calculus"}}
{"input": "Let $V_{n}(F)$ be a vector space over the field $F=\\mathbb Z_{p}$ with $\\dim V_{n} = n$, i.e., the cardinality of $V_{n}(\\mathbb Z_{p}) = p^{n}$. What is a general criterion to find the number of bases in such a vector space? For example, find the number of bases in $ V_{2}(\\mathbb Z_{3})$. Further, how can we find the number of subspaces of dimension, say, $r$? \nI need a justification with proof. I have a formula, but I am unable to understand the basic idea behind that formula.", "output": "Definition 1\nFor any natural numbers $n$ and $k$, define the Gaussian binomial coefficient, $\\binom n k_q$ by the number of $k$-dimensional subspaces of an $n$-dimensional vector space.\nTheorem 2\n$$\\binom n k_q=\\dfrac{(q^n-1)(q^n-q)\\cdots(q^n-q^{k-1})}{(q^k-1)(q^k-q)\\cdots(q^k-q^{k-1})}$$\nProof.\nTo specify a $k$-dimensional subspace, we need to specify $k$ linearly independent vectors. The first vector can be chosen from among the non-zero vectors in $q^n-1$ ways. Note that $0 \\in S \\implies S$ is linearly dependent. The second vector must be chosen outside the span of this vector. Since, the first vector generates a subspace of dimension $1$, we have that there are $q^n-q$ choices. Proceeding this way, we get that, there are $(q^n-1)(q^n-q) \\cdots(q^n-q^{k-1})$ ways of specifying a linearly independent set of cardinality $k$.\nNow note that, there are many linearly independent $k$ sets, that generate the same subspace. So, we need to divide this number by the number of $k$ sets that generate the same subspace. But, this is what we have already counted in a different fashion: We are asking for the number of basis for a $k$ dimensional subspace. That will be the number of linearly independent $k$ sets in a $k$-dimensional space. So, set $n=k$ in the previous count.\nThis gives us the claim. $\\blacksquare$\nRelated Reading\n\nThis blogpost by Prof. Peter Cameron is a nice exposition on Gaussian Coefficients.\n\nProf. Amritanshu prasad wrote an expository note on counting subspaces that appeared in Resonance in two parts.", "meta": {"post_id": 142589, "input_score": 47, "output_score": 68, "post_title": "How to count number of bases and subspaces of a given dimension in a vector space over a finite field?"}}
{"input": "The Banach-Alaoglu theorem is well-known. It states that the closed unit ball in the dual space of a normed space is $\\text{wk}^*$-compact. The proof relies heavily on  Tychonoff's theorem.\nAs I have recently figured out thanks to the nice guys on the chat belonging to this website is that Tychonoff's theorem is equivalent to the Axiom of Choice.\nCan we prove Tychonoff (or something else equivalent) from Alaoglu?", "output": "I decided to outline the proof of the equivalence of some of the results mentioned in the other answers since it is quite easy.\n\nThe following statements are equivalent in ZF:\n\nBanach\u2013Alaoglu: If $X$ is a normed space then the unit ball in $X^\\ast$ is weak$^\\ast$-compact.\nThe ultrafilter lemma: Every filter is contained in an ultrafilter.\nTikhonov for Hausdorff spaces: An arbitrary product of compact Hausdorff spaces is compact.\nTikhonov for the unit interval: An arbitrary product of copies of $[0,1]$ is compact.\n\n\nRemark. It is a theorem of Halpern\u2013Levy, The Boolean prime ideal theorem does not imply the axiom of choice, Axiomatic Set Theory Part\u00a01, Proc. Symp. Pure Math. Vol.\u00a013 (1971), 83\u2013134, that these equivalent statements are strictly weaker than the full axiom of choice. See also Jech's book The Axiom of Choice, chapter\u00a07.\nThe implications 2.\u00a0$\\Rightarrow$\u00a03. and 4.\u00a0$\\Rightarrow$\u00a01. are standard (the first one is obtained by inspection of the usual proof of Tikhonov's theorem while the other is one of the usual proofs of the Banach-Alaoglu theorem, as explained by M.\u00a0Turgeon here) and 3.\u00a0$\\Rightarrow$\u00a04. is trivial.\nIt remains to prove that the Banach\u2013Alaoglu theorem implies the ultrafilter lemma.\n\nRecall that an ultrafilter on a set $S$ is the same thing as a $\\{0,1\\}$-valued finitely additive probability measure defined on the entire power set $P(S)$: For an ultrafilter $\\mathfrak{U}$ and $A \\subset S$ set\n$$\n\\mu_{\\mathfrak{U}}(A) =\n\\begin{cases}\n1, & \\text{if } A \\in \\mathfrak{U}, \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$ \nto get a finitely additive measure $\\mu_\\mathfrak{U}$. Conversely, given a finitely additive $\\{0,1\\}$-valued probability measure $\\mu$, define $\\mathfrak{U}_\\mu = \\{A \\subset S\\,:\\,\\mu(A) = 1\\}$ then the the mutually exclusive possibilities $A \\in \\mathfrak{U}_\\mu$ or $S \\smallsetminus A \\in\\mathfrak{U}_\\mu$ imply that $\\mathfrak{U}_\\mu$ is an ultrafilter.\nObserve that $\\operatorname{ba}(S) = \\ell^{\\infty}(S)^\\ast$ is the same as the Banach space of bounded (and signed) finitely additive measures on $P(S)$ with the total variation norm. The identification is straightforward: since $P(S) = \\{0,1\\}^S \\subset \\ell^{\\infty}(S)$, every bounded linear functional on $\\ell^{\\infty}(S)$ gives a finitely additive measure. For the reverse direction, use the fact that the $\\mathbb{Q}$-linear span of $P(S)$ is norm-dense in $\\ell^{\\infty}(S)$. The identification of the norm follows by direct inspection of the definitions.\nLet $B$ be the unit ball of $\\operatorname{ba}(S)$, equipped with the weak$^\\ast$-topology, so that it is compact by Banach-Alaoglu. The \u201csubset of ultrafilters\u201d\n$$\nU = \\{\\mu \\in B\\,:\\,\\mu(A)\\in\\{0,1\\} \\text{ for all } A \\subsetneqq S\\text{ and }\\mu(S) = 1\\} \\subset B\n$$\nis weak$^\\ast$-closed and we have a map $\\delta: S \\to U$ sending $s \\in S$ to the corresponding Dirac measure (= principal ultrafilter).\nGiven a filter $\\mathfrak{F}$ on $S$, the collection $\\mathcal{F} = \\left\\{\\overline{\\delta(F)}\\right\\}_{F \\in \\mathfrak{F}}$ of closed subsets of $U$ has the finite intersection property, so by compactness of $U$ the intersection $\\bigcap \\mathcal{F}$ is non-empty. Let $\\mu$ be an element of that intersection. Notice that $\\mu$ is $\\{0,1\\}$-valued and $\\mu(F) = 1$ for all $F \\in \\mathfrak{F}$, so we're done, because the ultrafilter $\\mathfrak{U}_\\mu$ contains $\\mathfrak{F}$.\n\nRemark: Note that the idea is to implicitly work with the Stone\u2013\u010cech compactification $\\beta S$ by identifying it with the weak$^\\ast$-closure $U$ of $\\delta(S) \\subset \\operatorname{ba}(S)$ via the Gel'fand isomorphism $C(\\beta S) = \\ell^\\infty(S)$, but putting it this way again requires relying on an equivalent of the ultrafilter lemma.\n\nI second Asaf's recommendation to read the short article by Bell and Fremlin, A Geometric Form of the Axiom of Choice, Fund. Math. vol.\u00a077 (1972), 167\u2013170, showing various implications between functional analytic principles and choice. Especially the fact that the axiom of choice follows from the existence of an extreme point in the unit ball of a dual Banach space is a beautiful observation in the geometry of Banach spaces. \nIt would be tempting to put the upshot of Bell and Fremlin as \u201cHahn\u2013Banach and Kre\u012dn\u2013Mil'man imply the axiom of choice\u201d (as one can sometimes read), however, the situation is slightly more subtle than that. More details in Bell\u2013Fremlin and a bit more at the end of this answer.\n\nLet me add a few points to Asaf's answer:\nThe Tikhonov theorem implies the Hahn\u2013Banach theorem by an argument of \u0141o\u015b\u2013Ryll-Nardzewski, On the application of Tychonoff's theorem in mathematical proofs, Studia Math.\u00a038 (1951), 233\u2013237.\nThe idea is this: Let $U$ be a subspace of the vector space $V$ and let $f: U \\to \\mathbb{R}$ be a functional dominated by a sublinear functional $p: V \\to [0,\\infty)$. If we want to extend $f$ to $v \\in V \\smallsetminus U$ then we can only choose $\\tilde{f}(v) \\in [-p(-v),p(v)]$ if we want the extension $\\tilde{f}$ to be dominated by $p$. For each finite subset of $V \\smallsetminus U$ we can choose from a finite product of compact intervals plus we have linearity constraints to fulfill \u2014 which amounts to saying that finding a Hahn\u2013Banach extension is the same as finding an element in a gigantic projective limit of compact spaces. That this projective limit isn't empty follows from Tikhonov's theorem. More details are in section\u00a05 of loc. cit.\nA direct proof of the Hahn\u2013Banach theorem from the ultrafilter lemma, using the language of non-standard analysis, was given by Luxemburg in Two applications of the method of construction by ultrapowers to analysis, Bull. Amer. Math. Soc.\u00a068 (1962), 416\u2013419, see also his article Reduced powers of the real number system and equivalents of the Hahn- Banach extension theorem,\nAppl. Model Theory Algebra, Anal., Probab., Proc. Int. Sympos. Calif. Inst. Technol. 1967, 123-137 (1969) ZBlatt review.\nIn this latter article Luxemburg proves among many other things that Hahn\u2013Banach is equivalent to the statement that the unit ball in the dual of a normed space is convex-compact: every cover by open and convex sets has a finite subcover.\nThis and various other facts seem to indicate that Hahn\u2013Banach might imply Banach-Alaoglu (or the ultrafilter lemma) \u2014 at least that's the sentiment expressed by Luxemburg and others in various places.\nHowever, surprisingly enough, this turns out to be wrong: D.\u00a0Pincus, The strength of the Hahn-Banach theorem, Lecture Notes in Mathematics Volume 369 (1974), 203\u2013248 constructs a model in which Hahn-Banach holds, but both the ultrafilter lemma and Kre\u012dn\u2013Mil'man fail, so they are in fact independent of ZF+HB. In the same paper Pincus also established that the axiom of choice is independent of Hahn\u2013Banach and Kre\u012dn\u2013Mil'man (at least in ZFA). See also the announcement Independence of the prime ideal theorem from the Hahn Banach theorem, Bull. Amer. Math. Soc.\u00a078 (1972), 766-770.", "meta": {"post_id": 145271, "input_score": 41, "output_score": 40, "post_title": "Is Banach-Alaoglu equivalent to AC?"}}
{"input": "What's the difference of a monoid and a group? I'm reading this book and it says that a group is a monoid with invertibility and this property is made to solve the equation $x \\ast m=e$ and $m \\ast x=e$ for $x$, where $m$ is any element of the structure.\nI got confused because it's similar to the monoid's commutativity property which says that $m \\ast n=n*m$ for all $m, n \\in M$.", "output": "Your confusion arises from the fact that you are using the same letter in both equations. It would be better to say that invertibility is the property that for every $m$ there is a solution to the equation $m*x = e$, and a solution to the equation $y*m=e$. You can then prove that the solutions will in fact be the same, since\n$$y = y*e = y*(m*x) = (y*m)*x = e*x = x.$$\nMoreover, while it is true that the two equations together imply that $mx=xm$, this is not equivalent to commutativity. To be clear, commutativity would be \n\nFor all $a$ and all $b$, $ab=ba$.\n\nHere you have only\n\nIf $x$ is the solution to $mx=e$, then $mx=xm$.\n\nThat is, you are only guaranteed that a particular element commutes with each $m$, not that every element commutes with every element.\n\nConsider the usual \"axioms\" of a group. the ingredients are a set $S$, and a function $\\cdot\\colon S\\times S\\to S$, which we write using infix notation (so we write $a\\cdot b$ or $ab$ instead of $\\cdot(a,b)$). Then we require:\n\nAssociativity. $\\cdot$ is associative: $a(bc) = (ab)c$ for all $a,b,c\\in S$.\nExistence of neutral element. There exists an element $e\\in S$ such that for all $a\\in S$, $ae=ea=a$.\nExistence of inverses. For each $a\\in S$ there exists $b\\in S$ such that $ab=ba=e$, where $e$ is a neutral element as in 2.\n\nIf we relax the requirements that all three conditions get satisfied, we get more general structures (but the more general the structure, the less we can say about them).\n\nIf you drop all three conditions, you get a magma. \nIf you drop the second and third condition but keep the first, requiring only that the operation be associative, you get a semigroup.\nIf you drop the third condition but keep the first and second, requiring that the operation be associative and that there be a neutral element, you get a monoid.\nIf you keep all three conditions, you get a group.\n\nThere are other things you can do; it does not make sense to drop the second and keep the third condition.\nIf you drop the first (associativity), then can relax the conditions a bit and ask that all equations of the form $ax=b$ and $ya=b$ have solutions, but not requiring that the operation be associative. That gives you a quasigroup. If you require that all such equations have solutions and that there be an identity, you get a loop. This is equivalent to asking that conditions 2 and 3 be satisfied, but not condition 1.\nWithin each category you can put other conditions. There are \"cancellation semigroups\", which are semigroups in which $ax=ay$ implies $x=y$. There are \"inverse semigroups\" which, perhaps confusingly, does not mean that condition 3 is satisfied (makes no sense if we don't have condition 2), but rather that for every $a$ there exists a $b$ such that $aba=a$ and $bab=b$. And so on and so forth. Lots of different wrinkles to be seen in there.", "meta": {"post_id": 146887, "input_score": 24, "output_score": 47, "post_title": "What's the difference between a monoid and a group?"}}
{"input": "A nice observation by C.E.\u00a0Blair1, 2, 3 shows that the Baire category theorem for complete metric spaces is equivalent to the axiom of (countable) dependent choice.\nOn the other hand, the three classical consequences of the Baire category theorem in basic functional analysis \u2014 the open mapping theorem, the closed graph theorem and the uniform boundedness principle (as well as Zabreiko's lemma) \u2014 are equivalent to each other in Zermelo\u2013Fraenkel set theory without choice: that is to say, if one is added as an axiom to ZF then the others follow4.\nEach of these results has a more or less direct proof from the Baire category theorem and all the proofs \u201cavoiding Baire\u201d I'm aware of5 involve dependent choice in a way that doesn't seem to be replaceable by weaker forms of choice.\nHence I'm asking about the converse:\n\nDoes the open mapping theorem imply the Baire category theorem? \nIf not, is it at least true that the open mapping theorem implies the axiom of dependent choice for subsets of the reals?\n\nI imagine that applying any of the above results to a judiciously chosen space and/or operator(s) might yield the desired conclusion, similarly to what happens in Bell's and Fremlin's geometric version of the axiom of choice6. Unfortunately, I couldn't find a promising place to start.\nNeedless to say that I checked numerous things on the web form of Howard and Rubin's book Consequences of the Axiom of Choice, but without much success: The only articles that I found this way are J.D. Maitland Wright's articles7.\n\nFootnotes and References:\n1 Charles E. Blair, The Baire category theorem implies the principle of dependent choices, Bull. Acad. Polon. Sci. S\u00e9r. Sci. Math. Astronom. Phys.\u00a025 (1977), no. 10, 933\u2013934.\n2 Since Blair's article is hard to find, the proof can be found in the notes to chapter\u00a09, page 95 of John C. Oxtoby, Measure and Category, Springer GTM\u00a02, Second Edition, 1980.\n3 Here's the idea of Blair's argument for the implication Baire Category Theorem $\\Rightarrow$ Dependent Choice: let $S$ be a set and let $R \\subset S \\times S$ be a relation such that for all $s \\in S$ there exists $t \\in S$ such that $(s,t) \\in R$. Equip $S^{\\mathbb{N}}$ with the complete metric $d(f,g) = 2^{-\\min\\{n\\,:\\,f(n)\\neq g(n)\\}}$, put\n$$\nU_n = \\bigcup_{m = n+1}^{\\infty} \\bigcup_{(s,t) \\in R} \\{f \\in S^{\\mathbb{N}}\\,:\\,f(n) = s, \\,f(m)=t\\},\n$$\nobserve that $U_n$ is open and dense and use $f \\in \\bigcap_{n=1}^\\infty U_n$ and the well-order on $\\mathbb{N}$ to find a strictly increasing sequence $k_1 \\lt k_2 \\lt \\cdots$ such that the sequence $(x_n)_{n=1}^\\infty$ given by $x_n = f(k_n)$ satisfies $(x_n, x_{n+1}) \\in R$ for all $n \\in \\mathbb{N}$.\n4 See e.g. E.\u00a0Schechter, Handbook of Analysis and its foundations, 27.27, pp.\u00a0734ff.\n5 A good example for this is Sokal's A Really Simple Elementary Proof of the Uniform Boundedness Theorem, The American Mathematical Monthly\nVol. 118, No. 5 (May 2011), pp. 450\u2013452, ArXiV Version. While admittedly it is beautifully simple and elementary, it involves a plain application of dependent choice in the main argument.\n6 Bell and Fremlin, A Geometric Form of the Axiom of Choice, Fund. Math. vol.\u00a077 (1972), 167\u2013170.\n7 The full list of relevant articles can be obtained with this ZBlatt query two of which appeared in rather obscure proceedings, so I couldn't get my hands on them, yet. The third article is J. D. Maitland Wright, All operators on a Hilbert space are bounded, Bull. Amer. Math. Soc.\u00a079 (1973), 1247\u20131250.", "output": "EDIT after more than a year:\nAt least, the uniform boundedness principle can be proven using only the axiom of countable choice, or $\\mathbf{CC}$ for short. However, I have been unable to find a proof for the fact that any of the three facts\n\nEvery Banach space is barrelled\nOn a Banach space, a lower semi-continuous seminorm is always continuous\nThe uniform boundedness principle\n\nimplies either the closed graph theorem or the open mapping theorem (however, the two are equivalent in ZF). In particular, the argument in 27.37 of Schechter uses dependent choice, seemingly in an essential way.\nHere is the sketch for $\\mathbf{CC} \\Rightarrow$ UBP:\nWe first note that for a linear operator $T$,\n$$\n\\max\\{\\|T(x - y)\\|, \\|T(x + y)\\|\\} \\ge 1/2 (\\|T(x - y)\\| + \\|T(x + y)\\|) \\ge \\|T(y)\\|\n$$\ndue to the triangle inequality $\\|a - b\\| \\le \\|a\\| + \\|b\\|$.\nInstead of applying the axiom of dependent choice, we first pick a sequence of operators $\\|T_n\\| \\ge 4^n$ and\n$$\nx_n \\in \\left\\{x \\in X \\middle| \\|x\\| \\le 1 \\text{ and } \\|T_n(x)\\| \\ge 2/3 \\|T_n\\|\\right\\} =: S_n\n$$\nusing countable choice. Then, since dependent choice with a function instead of a relation is a theorem of ZF, we define a function which maps $(y_n, n)$ to $(y_{n+1}, n+1)$ where\n$$\ny_{n+1} = \\begin{cases}\ny_n - 3^{-(n+1)} x_{n+1} & \\|T_{n+1}(x_{n+1} - 3^{-(n+1)} x_{n+1})\\| \\ge \\frac{2}{3} 3^{-(n+1)} \\|T_{n+1}\\| \\\\\ny_n + 3^{-(n+1)} x_{n+1} & \\text{otherwise}.\n\\end{cases}\n$$\nIf we set $y_1 := 1/3 x_1$, then we may define a sequence from the repeated application of that function. The key lemma of Sokal's ensures that $\\|T_n(y_n)\\| \\ge \\frac{2}{3} 3^{-n} \\|T_n\\|$ for all $n \\in \\mathbb N$. From the triangle inequality follows if $k > n$\n$$\n\\|y_n - y_k\\| \\le \\sum_{j=n}^\\infty \\|y_j - y_{j+1}\\| \\le \\sum_{j=n}^\\infty \\frac{2}{3} 3^{-(j+1)} \\le \\frac{1}{2} 3^{-n}\n$$\nThus, $(y_l)_{l \\in \\mathbb N}$ is Cauchy (with limit $y$, say), and it also follows that\n$$\n\\|T_n(y)\\| \\ge \\left| \\|T_n(y_n)\\| - \\|T_n(y - y_n)\\| \\right| \\ge 1/6 (4/3)^n.\n$$", "meta": {"post_id": 146910, "input_score": 226, "output_score": 36, "post_title": "Does the open mapping theorem imply the Baire category theorem?"}}
{"input": "Let $T : V\\to V$ be a linear transformation such that $\\dim\\operatorname{Range}(T)=k\\leq n$, where $n=\\dim V$. Show that $T$ can have at most $k+1$ distinct eigenvalues.\n\nI can realize that the rank  will correspond to the number of non-zero eigenvalues (counted up to multiplicity) and the nullity will correspond to the 0 eigenvalue (counted up to multiplicity), but I cannot design an analytical proof of this.\nThanks for any help .", "output": "Since the nullity of $T$ is $n-k$, that means that the geometric multiplicity of $\\lambda=0$ as an eigenvalue of $T$ is $n-k$; hence, the algebraic multiplicity must be at least $n-k$, which means that the characteristic polynomial of $T$ is of the form $x^{N}g(x)$, where $N$ is the algebraic multiplicity of $0$, hence $N\\geq n-k$ (so $n-N\\leq k$), and $\\deg(g) =n-N$. Thus, $g$ has at most $n-N$ distinct roots, none of which are equal to $0$, and that means that the characteristic polynomial of $T$ has exactly:\n$$1 + \\text{# distinct roots of }g \\leq 1 + n-N \\leq 1 + k$$\ndistinct eigenvalues.\nNote that in fact we can say a bit better that $T$ has at most $\\min\\{k+1,n\\}$ distinct eigenvalues (when the rank is $n$).", "meta": {"post_id": 146927, "input_score": 40, "output_score": 36, "post_title": "Relation between rank and number of distinct eigenvalues of a matrix"}}
{"input": "I want to find the expected value of $\\text{max}\\{X,Y\\}$ where $X$ ist $\\text{exp}(\\lambda)$-distributed and $Y$ ist $\\text{exp}(\\eta)$-distributed. X and Y are independent.\nI figured out how to do this for the minimum of $n$ variables, but i struggle with doing it for 2 with the maximum.\n(The context in which this was given is waiting for the later of two trains, with their arrival times being exp-distributed).\nThanks!", "output": "The minimum of two independent exponential random variables with parameters $\\lambda$ and $\\eta$ is also exponential with parameter $\\lambda+\\eta$. \nAlso $\\mathbb E\\big[\\min(X_1,X_2)+\\max(X_1,X_2)\\big]=\\mathbb E\\big[X_1+X_2\\big]=\\frac{1}{\\lambda}+\\frac{1}{\\eta}$. Because $\\mathbb E\\big[\\min(X_1,X_2)\\big]=\\frac{1}{\\lambda+\\eta}$, we get $\\mathbb E\\big[\\max(X_1,X_2)\\big]=\\frac{1}{\\lambda}+\\frac{1}{\\eta}-\\frac{1}{\\lambda+\\eta}.$", "meta": {"post_id": 146973, "input_score": 16, "output_score": 46, "post_title": "Expected Value of the maximum of two exponentially distributed random variables"}}
{"input": "In Miles Reid's Undergraduate Commutative Algebra he defines a ring $B$ to be finite as an $A$ - algebra if it is finite as an $A$ - module. Now what I don't understand is suppose we look at the polynomial ring $k[x_1,\\ldots,x_n]$ where $k$ is a field. Then as a $k$ - algebra it is finitely generated. Is this the same as being a finite $k$ - algebra? For if it is the same this means that $k[x_1,\\ldots,x_n]$ is finitely generated as a $k$ - module which is just a $k$ - vector space. However this cannot be possible because the $x_i's$ are not even algebraic over $k$. What am I misunderstanding here?", "output": "An $A$-algebra $B$ is called finite if $B$ is a finitely generated $A$-module, i.e. there are elements $b_1,\\dotsc,b_n \\in B$ such that $B=A b_1 + \\dotsc + A b_n$. It is called finitely generated / of finite type if $B$ is a finitely generated $A$-algebra, i.e. there are elements $b_1,\\dotsc,b_n$ such that $B=A[b_1,\\dotsc,b_n]$. Clearly every finite algebra is also a finitely generated one. The converse is not true (consider $B=A[T]$). However, there is the following important connection:\n\nAn algebra $A \\to B$ is finite iff it is of finite type and integral.\n\nFor example, $\\mathbb{Z}[\\sqrt{2}]$ is of finite type over $\\mathbb{Z}$ and integral, thus finite. In fact, $1,\\sqrt{2}$ is a basis as a module. You can find the proof of the claim above in every introduction to commutative algebra.", "meta": {"post_id": 147345, "input_score": 25, "output_score": 42, "post_title": "Definition of a finitely generated $k$ - algebra"}}
{"input": "A complex analysis professor once told me that \"sheaves are all over the place\" in complex analysis. Of course one can define the sheaf of holomorphic functions: if $U\\subset \\mathbf{C}$ (or $\\mathbf{C}^n$) is a nonempty open set, let $\\mathcal{O}(U)$ denote the $\\mathbf{C}$-vector space of holomorphic functions $f:U\\to\\mathbf{C}$, and we let $\\mathcal{O}(\\varnothing)=\\{0\\}$. The restriction maps are given by restriction holomorphic functions to open subsets.  This defines a sheaf on $\\mathbf{C}$ with respect to its usual topology.\nHere are my questions: \n\nAre there interesting re-interpretations of well-known results in basic complex analysis in the language of sheaf theory (just to get one thinking about how things might translate)?\nAre there interesting new geometric insights that one gains by introducing this structure? (Feel free to reformulate the context of the question if 2 doesn't make sense).\n\nI guess I find it counter-intuitive that sheaves should say anything interesting about complex analysis, while it seems natural that they should say things about the geometry of the space on which they're defined.", "output": "As a complement to Matt's  very interesting answer, let me add a few words on the historical context of Leray's discoveries.  \nLeray was an officer in the French army and after Frances's defeat in 1940, he was sent to Oflag XVII in Edelsbach, Austria (Oflag=Offizierslager=POW camp): look here .\nThe prisoners founded a university in captivity, of which Leray was the recteur (dean).   \nLeray was a brilliant specialist in fluid dynamics (he joked that he was un m\u00e9canicien, a mechanic!), but he feared that if the Germans learned that he gave a course on that subject, they would force him to work for them and help them in their war machine (planes, submarines,...).\nSo he decided to teach  a harmless subject: algebraic topology!\nSo doing he recreated the basics on a subject in which he was a neophyte  and invented sheaves, sheaf cohomology and spectral sequences.\nAfter the war his work was examined, clarified and amplified by Henri Cartan (who introduced the definition of sheaves in terms of \u00e9tal\u00e9 spaces) and his student Koszul.\nSerre (another Cartan student) and Cartan then dazzled the world with the  overwhelming  power of these new tools applied to algebraic topology, complex analysis in several variables and algebraic geometry.  \nI find it quite interesting and moving that  the patriotism of one courageous man (officers had the option to  be freed if they agreed to work for the Nazis) changed the course of 20th century mathematics.    \nHere, finally, is Haynes Miller's fascinating article  on Leray's contributions.", "meta": {"post_id": 147561, "input_score": 28, "output_score": 37, "post_title": "Sheaves and complex analysis"}}
{"input": "Possible Duplicate:\nDistribution Functions of Measures and Countable Sets \n\nThe question at hand is:\nLet F be a distribution function on $\\mathbb{R}$. Prove that F has at most countably many discontinuities.\nMy attempt at a solution:\n$\\textrm{F is non-decreasing by assumption}\\\\\nF(\\varphi ^-)=\\lim_{t \\uparrow \\varphi}F(t),F(\\varphi ^+)=\\lim_{t \\downarrow \\varphi}F(t)\\\\\n\\textrm{The above limits exist and discontinuity points occur where}\\\\\nF(\\varphi^-)\\neq F(\\varphi)=F(\\varphi^+)\\\\\n\\textrm{let (a,b] be a \ufb01nite interval with n discontinuity points such that: }\n\\\\\na<\\varphi_1<...< \\varphi_n < b \\Rightarrow \\sum_{\\varphi =1}^{n}P(\\varphi_k) \\leq F(b)-F(a)\\\\\n\\textrm{therefore the number of discontinuity points is at most: } \\frac{1}{\\varepsilon }F(b)-F(a)$\nAs is (painfully) evident, I am just learning these concepts on my own and have little background in rigorous proof writing. I think all I have done is restrict the # of discontinuities of size $\\frac{1}{\\epsilon}$, and I'm not sure this does much for me.\nAny help would be greatly appreciated, as always.", "output": "Another approach: let $D$ be the set of points of discontinuity.  For each $x \\in D$ we have $F(x-) < F(x+)$ so we can choose a rational $q_x$ with $F(x-) < q_x < F(x+)$.  Since $F$ is increasing we can check that if $x \\ne y$ then $q_x \\ne q_y$.  So $x \\mapsto q_x$ is a one-to-one function from $D$ to $\\mathbb{Q}$, and $\\mathbb{Q}$ is countable, hence so is $D$.", "meta": {"post_id": 147612, "input_score": 18, "output_score": 42, "post_title": "Discontinuity points of a Distribution function"}}
{"input": "Would anyone can help me how to show that a finitely generated projective module over a local ring and PID are free? \n\nWhat I know about a finitely generated projective module $M$ over a PID $R$ is isomorphic to $R^k\\oplus R/(a_1)\\oplus\\dots\\oplus R/(a_n)$, and for the local ring case I don't know how to start.", "output": "For the case $R$ is a local ring it's a corollary of Nakayama's lemma.\nAs the notation in the above link, suppose $M$ is a finite generated projective module over $R$, then, first pick a minimal number of generators, i.e., $M=Rm_1+\\cdots +Rm_k$, and $k$ is the minimal number with this property, so we get a decomposition \n$$R^k=M\\oplus N,$$ then, we are left to prove $N=0$.\nFirst, applying $R/I\\otimes-$, where $I$ is the unique maximal ideal in $R$, then we get $$(R/I)^k=M/IM\\oplus N/IN,$$ and note that $M/IM$, $N/IN$ are vector spaces over the field $R/I$, so by comparing the dimension, we get $N/IN=0$, i.e., $N=IN$, then, \nwe use the Nakayama's lemma, the Statement 1 in the above link, we get $r\\in 1+I$, such that $rN=0$, but $r\\not \\in I$ and $R$ is local implies $r$ is a unit, so $N=0$.\nRemarks. 1) To get the choice of $k$, we can first assume $k=\\dim_{R/I}(M/IM)$, then use the Statement 4 in the above link to lift the basis of $M/IM$ to get a minimal set of generators of $M$.  \n2) A deep theorem of Kaplansky says that any projective modules (not necessarily finitely generated) over a local ring is free.", "meta": {"post_id": 147754, "input_score": 21, "output_score": 35, "post_title": "Finitely generated projective module"}}
{"input": "I was just wondering whether the following statement is correct.\nLet R be a ring and M a noetherian R module. Then M is finitely generated.", "output": "The three standard equivalences for Noetherian are:\n\nTheorem. Let $M$ be an $R$-module. Assuming the Axioms of Choice, the following are equivalent:\n\n$M$ has ACC on submodules.\nEvery submodule of $M$ is finitely generated.\nEvery nonempty set of submodules of $M$ has maximal elements.\n\n\nProof. 1$\\implies$2. (Uses dependent choice) Assume $N$ is a submodule of $M$ that is not finitely generated. We define a sequence of elements of $N$ inductively as follows: since $N$ is not finitely generated, $N\\neq 0$. Let $n_1\\in N$, $n_1\\neq 0$. Since $N$ is not finitely generated, $\\langle n_1\\rangle\\subsetneq N$, so there exists $n_2\\in N-\\langle n_1\\rangle$. Assume we have chosen elements $n_1,\\ldots,n_k\\in N$ such that $$\\langle n_1\\rangle \\subsetneq \\langle n_1,n_2\\rangle\\subsetneq\\cdots\\subsetneq \\langle n_1,\\ldots,n_k\\rangle.$$\nSince $N$ is not finitely generated, $\\langle n_1,\\ldots,n_k\\rangle \\subseteq N$, so there exists $n_{k+1}\\in N\\setminus\\langle n_1,\\ldots,n_k\\rangle$.\nThus, we have an infinite ascending chain of submodules in $M$, so $M$ does not satisfy ACC.\n1$\\implies 3$ (uses Zorn's Lemma): Since every ascending chain in $M$ is finite, any nonempty collection of submodules of $M$ satisfies the hypothesis of Zorn's Lemma under the partial order of inclusion (take the maximum of a chain to get an upper bound). Hence the set has maximal elements.\n2$\\implies$1 (Does not require AC) Let $N_1\\subseteq N_2\\subseteq\\cdots$ be an ascending chain of submodules. Then $N=\\cup N_i$ is a submodules of $N$, hence is finitely generated, $N=\\langle n_1,\\ldots,n_k\\rangle$. For each $i$, let $m_i$ be such that $n_i\\in N_{m_i}$. Let $m=\\max\\{m_1,\\ldots,m_k\\}$. Then $N\\subseteq N_m\\subseteq N_{m+k} \\subseteq \\cup N_i\\subseteq N$, so $N= N_m=N_{m+k}$ for all $k$; that is, the chain stabilizes after finitely many steps.\n2$\\implies$3 (Uses AC) Essentially, go through 1 to show any nonempty collection of submodules satisfies Zorn's Lemma to conclude the collection has maximal elements.\n3$\\implies$1 (Does not require AC) Given an ascending chain of submodules, by 3 the chain has maximal (hence a maximum) element. Thus, it stabilizes after finitely many steps.\n3$\\implies$2 (Does not require AC) Let $N$ be a submodule of $M$. Let $S$ be the collection of all finitely generated submodules of $N$. It is not empty (it contains $0$), hence has a maximal element $\\mathcal{N}$ which is a fortiori finitely generated. For every $n\\in N$, $\\langle \\mathcal{N},n\\rangle$ is finitely generated, and $\\mathcal{N}\\subseteq \\langle \\mathcal{N},n\\rangle$, so maximality of $\\mathcal{N}$ gives $\\mathcal{N}=\\langle \\mathcal{N},n\\rangle$. Thus, for every $n\\in N$, $n\\in \\mathcal{N}$, so $N\\subseteq \\mathcal{N}\\subseteq N$, proving that $N=\\mathcal{N}$ and so $N$ is finitely generated. $\\Box$\nIn particular, if $M$ is noetherian, then every submodule of $M$ is finitely generated, and in particular the module $M$ itself is finitely generated.", "meta": {"post_id": 147983, "input_score": 18, "output_score": 40, "post_title": "Is every Noetherian module finitely generated?"}}
{"input": "Is there a connexion between :\n1) The monodromy group of a topological space.\n2) The $\\ell$-adic monodromy theorem of Grothendieck.\n3) The $p$-adic monodromy conjecture of Fontaine (which is now proved). \nI am mainly interested in the link between 2) and 3).", "output": "A topological space does not have a monodromy group (unless someone is abusing terminology).  It has a fundamental group (more precisely, once we fix a base-point, it has a fundamental group relative to that base-point).\nIf $f:X \\to S$ is a fibre bundle, then the cohomology spaces of the fibres of $X$ (say with $\\mathbb Q$ coefficients, just to fix ideas, although any other coefficients would be okay too; and in some fixed degree $i$) glue together to form a local system over $S$ (i.e., a locally constant sheaf of $\\mathbb Q$-vector spaces), which (once we fix a base-point $s$), we can identify with a representation of $\\pi_1(S,s)$; indeed, the representation is on the vector space\n$H^i(X_s, \\mathbb Q),$ where $X_s := f^{-1}(s)$ is the fibre over $s$.\nIntuitively, if $c$ is a cohomology class on $X_s$, and $\\gamma$ is a loop based at $s$, then you can move $c$ through the fibres $X_{s'}$ as $s'$ moves along $\\gamma$, until you get back to $X_s$.\nTo understand this, you will need to think about examples.  A good one to start with is the fibre bundle $S^2 \\to \\mathbb R P^2$, taking $i = 0$, so that $H^0(X_s)$ is just the $\\mathbb Q$-vector space of dimension $2$ spanned by\nthe two points of $S^2$ lying over a point $s \\in \\mathbb R P^2$.\nA harder example, but more directly relevant to algebraic geometry, is the Legendre family of elliptic curves $y^2 := x(x-1)(x-\\lambda)$ (I mean the projective curves, although following tradition I am just writing down the affine equations) parameterized by\n$\\lambda \\in S = \\mathbb C P^1 \\setminus \\{0,1,\\infty\\}.$\nHere the interesting case is $i = 1$, i.e. the family of $H^1$'s of the fibres.\n\nEhresmann's theorem says that any smooth proper map of varieties $f: X \\to S$ over $\\mathbb C$ is topologically a fibre bundle, so this gives lots of examples of monodromy arising from algebraic geometry.\nIf the base $S$ is an algebraic curve, and $D^{\\times}$ is any copy of the punctured disk sitting inside $S$ (you should think of $S$ as being a punctured Riemann surface, like the above example of $\\mathbb C P^1 \\setminus \\{0,1,\\infty\\}$, and $D^{\\times}$ as being a neighbourhood of one of the punctures), then you can pull back $X$ to $D^{\\times}$, and consider the action of $\\pi_1(D^{\\times}) \\cong \\mathbb Z$ on the local system of $H^i$. (This is the local monodromy around the puncture.)\nGrothendieck's monodromy theorem says that this local monodromy action is always quasi-unipotent, i.e. some power of the generator of $\\pi_1(D^{\\times})$ acts unipotently.\n\nThere is a variant of all of the above working with $\\ell$-adic  cohomology  in the etale topology rather than usual cohomology in the setting of complex varieties, which makes sense over any ground field.\nThis leads one to think about $\\ell$-adic representations of $p$-adic Galois groups (such as $G_{\\mathbb Q_p}$) in geometric terms.  In this context, the analogue of Grothendieck's monodromy theorem is that the tame inertia acts quasi-unipotently; this follows from the famous relation $\\varphi N = p N \\varphi$ (where $\\varphi$ is Frobenius and $N$ is the log of a generator of tame inertia).  (Note that Grothendieck was able to deduce the monodromy theorem in its original geometric context from this rather easy and general theorem about $\\ell$-adic reps. of $p$-adic Galois groups.)\n\nIn Fontaine's $p$-adic Hodge theory, the analogue, for a $p$-adic representation of a $p$-adic Galois group, of tame inertia acting quasi-unipotently, is that the $p$-adic representation should be potentially semi-stable.  This is not true of all $p$-adic representations, but Fontaine conjectured that it was true for those that are de Rham.  This is his monodromy conjecture, now proved by Andre, Kedlaya, Mebkhout, and Berger.", "meta": {"post_id": 148250, "input_score": 26, "output_score": 35, "post_title": "What is the idea of a monodromy?"}}
{"input": "My question is related with the definition of Cauchy sequence\nAs we know that a sequence $(x_n)$ of real numbers is called Cauchy, if for every positive real number \u03b5, there is a positive integer $N \\in \\mathbb{N}$ such that for all natural numbers $m, n > N$ \n$\\mid x_m -x_n\\mid < \\epsilon$\nMy questions are\n1  : What is the significance of choosing $m, n > N$  ? \n2:  How to choose $N$?\n3: Can we see it geometrically?\nI would be pleased if someone can make me understand through examples.\nThanks", "output": "In the comment you say you know what a converging sequence is. Over the reals a Cauchy sequence is the same thing. So why do we care about them, you might ask. Here is why:\nRecall: A sequence $(a_n)$ of real numbers converges to the limit $a\\in \\mathbb R$ if $\\forall \\epsilon>0\\ \\exists N\\in\\mathbb N:\\forall n\\geq N\\ |a-a_n|<\\epsilon$. \nYou probably have seen examples of converging sequences. $\\frac 1n$ is one of them, and the limit is $0$. I assume you have also seen the reason for that (it boils down to the Archimedean axiom). In general the game works as follows: You guess the right limit and then do some algebraic manipulation till you find a suitable $N$ for every possible $\\epsilon >0$.\nIt might bother you that you have to know the limit before you can actually show something. Is there no other way to formulate convergence which doesn't rely on the right guess of the limit? Indeed there is: Cauchy sequences. Note that the limit doesn't show up in the definition and we can start proving things without assuming that some number is the limit. \nSo what makes a limit converging? The difference of subsequent elements of you sequence should definitely be arbitrarily small. So a naive guess for a condition would be\n$$|a_n-a_{n+1}|<\\epsilon$$\nfor all $n\\geq N$.\nBut this is not enough as you can see from the sequence\n$$s_n=\\sum_{i=1}^n\\frac 1i.$$\nIt turns out that the stronger assumption \n$$|a_n-a_{m}|<\\epsilon$$\nfor all $n,m\\geq N$ is enough. So you may not only compare two subsequent elements of your sequence but any two which appear after a certain time. \nSo we can relatively easy show that every converging sequence is Cauchy. What about the other direction, namely every Cauchy sequence converges? Well, depending on your set up you can define the reals precisely as the completion of $\\mathbb Q$. And here lies the true strengh of Cauchy sequences:\nImage you don't know yet what the reals are. You just know the rationals $\\mathbb Q$ (you define the natural numbers via the Peano axioms and derive first the integers and then the rationals from there). \nThen you can write down the sequence \n$$a_1=1,\\ a_{n+1}=\\frac {a_n}2+\\frac1a_n$$\nThis sequence converges to $\\sqrt 2$, but wait: we don't even know what $\\sqrt 2$ is. All we know are the rationals. But the seqeunce still converges, or doesn't it? The definition of convergence requires a limit but there is no suitable limit in $\\mathbb Q$. But we can take the closest thing to convergence available: Cauchy sequences. We can show that the sequence is Cauchy. We then say in $\\mathbb R$ any Cauchy sequence converges and have just defined the reals.\nAnd this idea goes on. You can take any space you like (you probably don't know many examples yet, but  you can take the space of all polynomials, or all continuous functions etc.) We can define distances (just as the absolut value on the reals) and can write down sequences. The notion of convergence does not always make sense. The notion of Cauchy sequence does.\nEdit: Obviously there are other definitions of the real numbers. If you want try proving the following:\nEverey bounded sequence of real numbers has a converging subsequence $\\Rightarrow$ every Cauchy sequence of real numbers converges.", "meta": {"post_id": 148713, "input_score": 32, "output_score": 36, "post_title": "Understanding the definition of Cauchy sequence"}}
{"input": "In finding the derivative of the cross product of two vectors $\\frac{d}{dt}[\\vec{u(t)}\\times \\vec{v(t)}]$, is it possible to find the cross-product of the two vectors first before differentiating?", "output": "You can evaluate this expression in two ways:\n\nYou can find the cross product first, and then differentiate it.\nOr you can use the product rule, which works just fine with the cross product:\n\n$$\n\\frac{d}{dt}(\\mathbf{u} \\times \\mathbf{v}) = \\frac{d\\mathbf{u}}{dt} \\times \\mathbf{v} + \\mathbf{u} \\times \\frac{d\\mathbf{v}}{dt}\n$$\nPicking a method depends on the problem at hand. For example, the product rule is used to derive Frenet Serret formulas.", "meta": {"post_id": 149817, "input_score": 27, "output_score": 34, "post_title": "Derivative of cross-product of two vectors"}}
{"input": "I really don't understand how to calculate ramification points for a general map between Riemann Surfaces. If anyone has a good explanation of this, would they be prepared to share it? Disclaimer: I'd prefer an explanation that avoids talking about projective space!\nI'll illustrate my problem with an example. The notion of degree for a holomorphic map is not well defined for non-compact spaces, such as algebraic curves in $\\mathbb{C}^2$. I've had advice from colleagues not to worry about this and to use the notion of degree anyway, because it works in a different setting (I don't know which). In particular consider the algebraic curve defined by \n$$p(z,w)=w^3-z(z^2-1)$$\nand the first projection map \n$$f(z,w)=z$$\nIn order to find the ramification points of this we know that generically $v_f(x)=1$ and clearly when $z\\neq0,\\pm 1$ we have $|f^{-1}(z)|=3$ so the 'degree' should be $3$. Thus $z=0,\\pm1$ are ramification points with branching order 3. I've had feedback that this is correct. Why did this work?\nNow let's look at an extremely similar example. Consider the algebraic curve defined by \n$$p(z,w)=w^2-z^3+z^2+z$$\nand the second projection map\n$$g(z,w)=w$$\nNow again we see the 'degree' of $g$ should be $3$. Now $g^{-1}(i)=\\{(1,i),(-1,i)\\}$. So by the degree argument exactly one of these is a ramification point, of branching order 2. Is this correct? If so, how do I tell which one it is?\nFinally in more generality, does this method work for the projection maps of all algebraic curves in $\\mathbb{C}^2$? Sorry for the long exposition!\nEdit: Here's an idea I just had. If our map $f$ is proper then we don't need $X$ to be compact for $\\deg(f)$ to be well defined. Now the projection map is clearly proper (I think) so that's why this works. Am I right? This of course raises the natural question - 'what standard maps are proper'? I guess I should ask this in a separate question though!", "output": "Let's look at your second example. Let $p(z, w) = w^2 - z^3 + z^2 + z$, and let $Y = \\{ p(z, w) = 0 \\}$. Then,\n$$p(z, i) = -z^3 + z^2 + z - 1 = -(z - 1)^2 (z+1)$$\nso I claim $(1, i)$ has ramification index $2$ while $(-1, i)$ has ramification index $1$. Indeed, observe that \n\\begin{align}\n\\frac{\\partial p}{\\partial z} & = -3 z^2 + 2 z + 1 \\\\\n\\frac{\\partial p}{\\partial w} & = 2 w\n\\end{align}\nso by an inverse function theorem argument, we find that $(z, w) \\mapsto z$ is locally a chart near both $(-1, i)$ and $(1, i)$. In this chart, your function $g : Y \\to \\mathbb{C}$ is given by $z \\mapsto \\sqrt{z^3 - z^2 - z}$. Let us take Taylor expansions around $\\pm 1$:\n\\begin{align}\ng(z) - i & = -i (z-1)^2 + O((z-1)^3) \\\\\ng(z) - i & = -2 i (z+1) + O((z+1)^2)\n\\end{align}\nHence, the ramification index at $(1, i)$ is indeed $2$ and at $(-1, i)$ it is $1$.\n\nMorally, what is going on is that your curves are dense open subsets of projective curves. Indeed, your first curve is given in homogeneous coordinates by\n$$w^3 - z (z^2 - u^2) = 0$$\nand your second curve is given by\n$$w^2 u - z^3 + z^2 u + z u^2 = 0$$\nand one can check by hand that these curves are smooth \"at infinity\", so we have the desired embedding of the original affine algebraic curves into projective (hence compact) algebraic curves. Degree is well-defined on the latter, so is well-defined on the former by restriction; the only trouble is that there may be \"missing\" preimages and so the equation relating degrees and ramification indices becomes an inequality:\n$$\\text{deg}(g) \\ge \\sum_{x \\in g^{-1} \\{w\\}} \\nu_x (g)$$\nFor example, take the affine hyperbola $z w - 1 = 0$ and the projection $(z, w) \\mapsto w$; this function has degree $1$ (once we embed it in the projective closure), but obviously there are no preimages of $0$ in the affine hyperbola.\n\nLet's develop a generic method of dealing with affine plane curves. Let $p : \\mathbb{C}^2 \\to \\mathbb{C}$ be a polynomial function in two variables, and suppose $Y = \\{ p(z, w) = 0 \\}$ is a smooth algebraic curve. Let $f : Y \\to \\mathbb{C}$ be the projection $(z, w) \\mapsto w$. For each fixed complex number $b$, we get a polynomial function $p(-, b)$, say of degree $d$. Now, because $\\mathbb{C}$ is algebraically closed, we can write\n$$p(z, b) = c (z - a_1)^{e_1} \\cdots (z - a_n)^{e_n}$$\nfor some distinct complex numbers $a_1, \\ldots, a_n$, $c \\ne 0$, and positive integers $e_1, \\ldots, e_n$, such that $e_1 + \\cdots + e_n = d$. Suppose also that\n$$\\frac{\\partial p}{\\partial w}(a_i, b) \\ne 0$$\nfor all $a_i$; then an inverse function theorem argument shows that $(z, w) \\mapsto z$ is a chart near each $(a_i, b)$. I claim that the ramification index of $f$ at $(a_i, b)$ is $e_i$ under these hypotheses. Indeed, when $z$ is a local parameter, we have\n$$0 = \\frac{\\partial p}{\\partial z} + \\frac{\\mathrm{d} w}{\\mathrm{d} z} \\frac{\\partial p }{\\partial w}$$\nso if $e_i > 1$, we have $\\frac{\\partial p}{\\partial z} (a_i, b) = 0$, so we must have $\\frac{\\mathrm{d} w}{\\mathrm{d} z} (a_i) = 0$ because $\\frac{\\partial p}{\\partial w} (a_i, b) \\ne 0 $ by hypothesis \u2013 implying $f(z) - b = O((z - a_i)^2)$. Playing around with total derivatives more, we eventually find that the first non-zero coefficient of $f(z) - b$ around $a_i$ is the coefficient of $(z - a_i)^{e_i}$, as required.\nOn the other hand, when we have $\\frac{\\partial p}{\\partial w} (a_i, b) = 0$, then by non-degeneracy we must have $\\frac{\\partial p}{\\partial z} (a_i, b) \\ne 0$, and we must have $e_i = 1$ and $(z, w) \\mapsto w$ is a chart near $(a_i, b)$. But then obviously the ramification index of $f$ at $(a_i, b)$ must be $1$. So in either case the ramification index of $f$ at $(a_i, b)$ is equal to $e_i$. Convenient, no?", "meta": {"post_id": 151087, "input_score": 41, "output_score": 53, "post_title": "Understanding Ramification Points"}}
{"input": "I know that for every $n\\in\\mathbb{N}$, $n\\ge 1$, there exists $p(x)\\in\\mathbb{F}_p[x]$ s.t. $\\deg p(x)=n$ and $p(x)$ is irreducible over $\\mathbb{F}_p$.\n\nI am interested in counting how many such $p(x)$ there exist (that is, given $n\\in\\mathbb{N}$, $n\\ge 1$, how many irreducible polynomials of degree $n$ exist over $\\mathbb{F}_p$).\n\nI don't have a counting strategy and I don't expect a closed formula, but maybe we can find something like \"there exist $X$ irreducible polynomials of degree $n$ where $X$ is the number of...\".\nWhat are your thoughts ?", "output": "Theorem: Let $\\mu(n)$ denote the M\u00f6bius function. The number of monic irreducible polynomials of degree $n$ over $\\mathbb{F}_q$ is the necklace polynomial\n$$M_n(q) = \\frac{1}{n} \\sum_{d | n} \\mu(d) q^{n/d}.$$\n(To get the number of irreducible polynomials just multiply by $q - 1$.)\nProof. Let $M_n(q)$ denote the number in question. Recall that $x^{q^n} - x$ is the product of all the monic irreducible polynomials of degree dividing $n$. By counting degrees, it follows that\n$$q^n = \\sum_{d | n} d M_d(q)$$\n(since each polynomial of degree $d$ contributes $d$ to the total degree). By M\u00f6bius inversion, the result follows. \nAs it turns out, $M_n(q)$ has a combinatorial interpretation for all values of $q$: it counts the number of aperiodic necklaces of length $n$ on $q$ letters, where a necklace is a word considered up to cyclic permutation and an aperiodic necklace of length $n$ is a word which is not invariant under a cyclic permutation by $d$ for any $d < n$. More precisely, the cyclic group $\\mathbb{Z}/n\\mathbb{Z}$ acts by cyclic permutation on the set of functions $[n] \\to [q]$, and $M_n(q)$ counts the number of orbits of size $n$ of this group action. This result also follows from M\u00f6bius inversion.\nOne might therefore ask for an explicit bijection between aperiodic necklaces of length $n$ on $q$ letters and monic irreducible polynomials of degree $n$ over $\\mathbb{F}_q$ when $q$ is a prime power, or at least I did a few years ago and it turns out to be quite elegant. \nLet me also mention that the above closed form immediately leads to the \"function field prime number theorem.\" Let the absolute value of a polynomial of degree $d$ over $\\mathbb{F}_q$ be $q^d$. (You can think of this as the size of the quotient $\\mathbb{F}_q[x]/f(x)$, so in that sense it is analogous to the norm of an element of the ring of integers of a number field.) Then the above formula shows that the number of monic irreducible polynomials $\\pi(n)$ of absolute value less than or equal to $n$ satisfies\n$$\\pi(n) \\sim \\frac{n}{\\log_q n}.$$", "meta": {"post_id": 152880, "input_score": 17, "output_score": 41, "post_title": "How many irreducible polynomials of degree $n$ exist over $\\mathbb{F}_p$?"}}
{"input": "Is it right to consider assigning a fundamental group to a topological space the same as having a functor from $\\mathbf{Top}$ to $\\mathbf{Grp}$ ?\nAre there any other examples of such functors ?", "output": "Assigning the fundamental group to a topological space is definitely a functor. But you have to keep in mind that a fundamental group is always taken with respect to a base point, and hence the functor assigns a pair $(X,x_0)$ consisting of a topological space $X$ and a point $x_0\\in X$ to its fundamental group $\\pi_1(X,x_0)$. As such, the functor goes from $\\mathbf{Top}_\\ast$ to $\\mathbf{Grp}$.\nIn more detail, the fundamental group $\\pi_1(X,x_0)$ is the group of homotopy classes of loops starting and ending in the base point $x_0$. It is not so hard to show that the map $[\\gamma]\\mapsto[f\\circ\\gamma]$ is well-defined for each loop $\\gamma$ from $x_0$ to $x_0$ and each morphism $f:(X,x_0)\\to(Y,y_0)$ of the category $\\mathbf{Top}_\\ast$; we take this map to be $\\pi_1(f)$. Roughly, this is a definition by post-composition, so it is immediate that this functor respects identity morphisms and compositions.\n\nThis is a side remark, because you have been asking about the fundamental group explicitly. But I feel it is in place here because it is natural to consider a functor with domain $\\mathbf{Top}$ that is `like taking the fundamental group'.\nInstead of $\\mathbf{Top}_\\ast\\to\\mathbf{Grp}$, one could also work with a functor $\\mathbf{Top}\\to\\mathbf{Grpd}$, where the category $\\mathbf{Grpd}$ is the category of groupoids (categories in which all morphisms are isomorphisms). The functor sends a topological space $X$ to the groupoid which has the points of $X$ as objects and between two points $x$ and $y$ of $X$ the morphisms are the homotopy classes of paths from $x$ to $y$. This gives you the fundamental groupoid rather than the fundamental group.\nThe n-lab has more information on the fundamental groupoid.\n\nThere are many more examples of functors from $\\mathbf{Top}$ or related categories. An important one is the singular functor to the category $\\mathbf{Sset}$ of simplicial sets. The category if simplicial sets is defined as follows: first you consider the category $\\Delta$ consisting of an object $[n]$ for each natural number $n$, where $[n]$ is the partial ordered set $\\{0,\\ldots,n\\}$ with the usual order; the morphisms are the order preserving maps. Then $\\mathbf{Sset}$ is the category of contravariant functors from $\\Delta$ to $\\mathbf{Set}$. \nFor each natural number $n$, there is the topological space\n$$\n|\\Delta^n|:=\\big\\{(t_0,\\ldots,t_n)\\in[0,1]^{n+1}:\\textstyle\\sum_{i=0}^n t_i=1\\big\\},\n$$\nwhich is called the standard $n$-simplex. To test your understanding of these definitions, you can show that the map $[n]\\mapsto|\\Delta^n|$ is a functor from $\\Delta$ to $\\mathbf{Top}$. Now we can define the functor $S:\\mathbf{Top}\\to\\mathbf{Sset}$, which is called the singular functor, by assigning to each topological space $X$ the functor\n$$\nn\\mapsto\\mathbf{Top}(|\\Delta^n|,X)\n$$\nIt turns out that the simplicial sets $S(X)$ have very nice properties. One of them is that they really are $\\infty$-groupoids. Also, the set $\\mathbf{Top}(|\\Delta^n|,X)$ is used to define the $n$-th homology group of $X$, which is gives yet another functor from the category of topological spaces. All of these functors have been (and are) important for the investigation of topological spaces.", "meta": {"post_id": 153180, "input_score": 22, "output_score": 37, "post_title": "Fundamental group as a functor"}}
{"input": "If we have a sequence of random variables $X_1,X_2,\\ldots,X_n$ converges in distribution to $X$, i.e. $X_n \\rightarrow_d X$, then is\n$$\n\\lim_{n \\to \\infty} E(X_n) = E(X)\n$$\ncorrect?\nI know that converge in distribution implies $E(g(X_n)) \\to E(g(X))$ when $g$ is a bounded continuous function.  Can we apply this property here?", "output": "With your assumptions the best you can get is via Fatou's Lemma:\n$$\\mathbb{E}[|X|]\\leq \\liminf_{n\\to\\infty}\\mathbb{E}[|X_n|]$$\n(where you used the continuous mapping theorem to get that $|X_n|\\Rightarrow |X|$).\nFor a \"positive\" answer to your question: you need the sequence $(X_n)$ to be uniformly integrable:\n$$\\lim_{\\alpha\\to\\infty} \\sup_n  \\int_{|X_n|>\\alpha}|X_n|d\\mathbb{P}= \\lim_{\\alpha\\to\\infty} \\sup_n \\mathbb{E} [|X_n|1_{|X_n|>\\alpha}]=0.$$\nThen, one gets that $X$ is integrable and $\\lim_{n\\to\\infty}\\mathbb{E}[X_n]=\\mathbb{E}[X]$.\nAs a remark, to get uniform integrability of $(X_n)_n$ it suffices to have for example:\n$$\\sup_n \\mathbb{E}[|X_n|^{1+\\varepsilon}]<\\infty,\\quad \\text{for some }\\varepsilon>0.$$", "meta": {"post_id": 153293, "input_score": 41, "output_score": 45, "post_title": "Does convergence in distribution implies convergence of expectation?"}}
{"input": "We define derivatives of functions as linear transformations of $R^n \\to R^m$. Now talking about the derivative of such linear transformation , \nas we know if $x \\in R^n$ , then\n$A(x+h)-A(x)=A(h)$, because of linearity of $A$, which implies that $A'(x)=A$ where , $A'$ is derivative of $A$ . \nWhat does this mean? I am not getting the point I think.", "output": "This is a fair question, since it is counterintuitive to the way introductory calculus is taught.\nOne looks at a typical linear function in calc 1: $f(x)=ax$, $a\\neq0$, takes the derivative, $f'(x)=a$, and thinks to themselves, \"well clearly the linear function is not equal to the constant function, one has a slope and the other is flat!\"\nSince we generalized to higher dimensions, it is wiser to pay closer attention to what we call the derivative. Merely looking at the Jacobian masks a deeper insight: the derivative is the best affine approximation to a function at a particular point. That is, $F(x)\\approx F(a) + F'(x-a)+o(|x-a|)$, which is a good approximation when $x$ is close to $a$. Notice that $F'$ acts as a \"factor\" on the tangent vector $(x-a)$.\nWhat if $F$ is already affine? Then $F(x)=Ax+b$. Plug this in the formula above, which has exact equality now, and you get: $Ax+b=Aa+b+F'(x-a)$, which gives $A(x-a)=F'(x-a)$, or if we call $h=x-a$, $Ah=F'h$. Notice what that is telling you: $A$ and $F'$ do the same thing to vectors $h$, hence they're equal. $A$ is also the derivative of $F(x)$.\nWhen, $F$ is linear, $b=0$, and thus $Fh=Ah=F'h$. Makes sense.\nWhat about our calc 1 example? The confusion stems from naming. The linear transformation is not $ax$, but $a$. View it as $fx=ax$. It's a 1x1 matrix with the entry $a$. The derivative (Jacobian), at any point, is also just $a$. Hence, $f'x=ax$ also. Thus the generalized notion of derivative is no longer \"the slope function\", but a unique linear transformation taking tangent vectors to tangent vectors which best approximates the linear behavior of a function at a particular point. In that light it makes sense that $fx=ax=f'x$ since we're viewing $f$ and $f'$ as \"factors\" at particular points rather than changing functions. This is why $Df(x)$ (which is just $a$ in our example) is used as notation for derivative at particular point $x$ rather than $f'$.\nIf you're interested there are notions of higher derivatives that take the derivative of the map that assigns to each point $x$ the matrix $Df(x)$, which differs from taking the derivative of the same matrix $Df(x)$, which is just linear and hence the same. See: http://www.math.pitt.edu/~sph/1540/1540-notes4.pdf", "meta": {"post_id": 153836, "input_score": 21, "output_score": 35, "post_title": "Derivative of a linear transformation."}}
{"input": "I am trying to prove it by induction, but I'm stuck\n$$\\mathrm{fib}(0) = 0 < 0! = 1;$$\n$$\\mathrm{fib}(1) = 1 = 1! = 1;$$\nBase case n = 2,\n$$\\mathrm{fib}(2) = 1 < 2! = 2;$$\nInductive case assume that it is true for (k+1) $k$\nTry to prove that $\\mathrm{fib}(k+1) \\leq(k+1)!$\n$$\\mathrm{fib}(k+1) = \\mathrm{fib}(k) + \\mathrm{fib}(k-1) \\qquad(LHS)$$\n$$(k+1)! = (k+1) \\times k \\times (k-1) \\times \\cdots \\times 1 = (k+1) \\times k! \\qquad(RHS)$$\n......\nHow to prove it?", "output": "$$\nF_{k+1} = F_k + F_{k-1} \\le k! + (k - 1)! \\le k! + k! \\le 2 k! \\le (k + 1) k!\n$$", "meta": {"post_id": 154756, "input_score": 8, "output_score": 43, "post_title": "How to prove that $\\mathrm{Fibonacci}(n) \\leq n!$, for $n\\geq 0$"}}
{"input": "Today in my calculus class, we encountered the function $e^{-x^2}$, and I was told that it was not integrable.\nI was very surprised. Is there really no way to find the integral of $e^{-x^2}$? Graphing $e^{-x^2}$, it appears as though it should be. \nA Wikipedia page on Gaussian Functions states that \n$$\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}$$\nThis is from -infinity to infinity. If the function can be integrated within these bounds, I'm unsure why it can't be integrated with respect to $(a, b)$.\nIs there really no way to find the integral of $e^{-x^2}$, or are the methods to finding it found in branches higher than second semester calculus?", "output": "To build on kee wen's answer and provide more readability, here is an analytic method of obtaining a definite integral for the Gaussian function over the entire real line:\nLet $I=\\int_{-\\infty}^\\infty e^{-x^2} dx$.\nThen,\n$$\\begin{align}\nI^2 &= \\left(\\int_{-\\infty}^\\infty e^{-x^2} dx\\right) \\times \\left(\\int_{-\\infty}^{\\infty} e^{-y^2}dy\\right) \\\\\n&=\\int_{-\\infty}^\\infty\\left(\\int_{-\\infty}^\\infty e^{-(x^2+y^2)} dx\\right)dy \\\\\n\\end{align}$$\nNext we change to polar form: $x^2+y^2=r^2$, $dx\\,dy=dA=r\\,d\\theta\\,dr$. Therefore\n$$\\begin{align}\nI^2 &= \\iint e^{-(r^2)}r\\,d\\theta\\,dr \\\\\n&=\\int_0^{2\\pi}\\left(\\int_0^\\infty re^{-r^2}dr\\right)d\\theta \\\\\n&=2\\pi\\int_0^\\infty re^{-r^2}dr\n\\end{align}$$\nNext, let's change variables so that $u=r^2$, $du=2r\\,dr$. Therefore,\n$$\\begin{align}\n2I^2 &=2\\pi\\int_{r=0}^\\infty 2re^{-r^2}dr \\\\\n&= 2\\pi \\int_{u=0}^\\infty e^{-u} du \\\\\n&= 2\\pi \\left(-e^{-\\infty}+e^0\\right) \\\\\n&= 2\\pi \\left(-0+1\\right) \\\\\n&= 2\\pi\n\\end{align}$$\nTherefore, $I=\\sqrt{\\pi}$.\nJust bear in mind that this is simpler than obtaining a definite integral of the Gaussian over some interval (a,b), and we still cannot obtain an antiderivative for the Gaussian expressible in terms of elementary functions.", "meta": {"post_id": 154968, "input_score": 77, "output_score": 84, "post_title": "Is there really no way to integrate $e^{-x^2}$?"}}
{"input": "I stumbled across the following problem and found it cute. \nProblem: We are given that $19$ divides $23028$, $31882$, $86469$, $6327$, and $61902$. Show that $19$ divides the following determinant: \n$$\\left|\n \\begin{matrix}\n  2 & 3&0&2&8 \\\\\n  3 & 1&8&8&2\\\\\n8&6&4&6&9\\\\\n0&6&3&2&7\\\\\n6&1&9&0&2\n \\end{matrix}\\right|$$", "output": "Integer proof\nPerform the column operation $C_5\\leftarrow 10^4C_1+10^3C_2+10^3C_3+10C_4+C_5$: the coefficient of $C_5$ is $1$ so this doesn't change the determinant.\nAll elements of $C_5$ ($23028$, $31882$, $86469$, $6327$, and $61902$) are now divisible by $19$, so we can factor out $19$: hence the determinant is divisible by $19$.\n\nModular proof\nIn $\\mathbb Z/19\\mathbb Z$, the columns $10^4C_1+10^3C_2+10^3C_3+10C_4+C_5$ sum to $0$: hence the matrix is not invertible and has determinant $0$. So in $\\mathbb Z$, the determinant is a multiple of $19$.", "meta": {"post_id": 155439, "input_score": 91, "output_score": 40, "post_title": "Cute Determinant Question"}}
{"input": "Suppose $(G,\\cdot)$ is a finite group of uneven order such that $abab=baba$ for any $a,b\\in G$. Does this mean that $G$ is commutative?", "output": "Yes. Let $|G|=2k-1$ be the order of the group and $a,b\\in G$. Then: $$ab=ab(ab)^{2k-1}=(ab)^{2k}=(abab)^k=(baba)^k=(ba)^{2k}=ba(ba)^{2k-1}=ba.$$\n(Added: I should probably mention that here we use the following fact twice: if $G$ is a finite group of order $n$ and $a\\in G$, then $a^n=e$, where $e$ is the identity element.)", "meta": {"post_id": 155814, "input_score": 15, "output_score": 35, "post_title": "Does $abab=baba$ imply commutativity in a Group of uneven order?"}}
{"input": "I have a set of points (with unknown coordinates) and the distance matrix. I need to find the coordinates of these points in order to plot them and show the solution of my algorithm.\nI can set one of these points in the coordinate (0,0) to simplify, and find the others. Can anyone tell me if it's possible to find the coordinates of the other points, and if yes, how?\nThanks in advance!\nEDIT\nForgot to say that I need the coordinates on x-y only", "output": "Doing this with angles, as Jyrki suggested, is cumbersome and difficult to generalize to different dimensions. Here is an answer that's essentially a generalization of WimC's, which also fixes an error in his answer. In the end, I show why this works, since the proof is simple and nice.\nThe algorithm\nGiven a distance matrix $D_{ij}$, define\n$$M_{ij} = \\frac {D^2_{1j}+D^2_{i1}-D^2_{ij}} 2 \\,.$$\nOne thing that is good to know in case the dimensionality of the data that generated the distance matrix is not known is that the smallest (Euclidean) dimension in which the points can be embedded is given by the rank $k$ of the matrix $M$. No embedding is possible if $M$ is not positive semi-definite.\nThe coordinates of the points can now be obtained by eigenvalue decomposition: if we write $M = USU^T$, then the matrix $X = U \\sqrt S$ (you can take the square root element by element) gives the positions of the points (each row corresponding to one point). Note that, if the data points can be embedded in $k$-dimensional space, only $k$ columns of $X$ will be non-zero (corresponding to $k$ non-zero eigenvalues of $M$).\nWhy does this work?\nIf $D$ comes from distances between points, then there are $\\mathbf x_i \\in \\mathbb R^m$ such that\n$$D_{ij}^2 = (\\mathbf x_i - \\mathbf x_j)^2 = \\mathbf x_i^2 + \\mathbf x_j^2 - 2\\mathbf x_i \\cdot \\mathbf x_j \\,.$$\nThen the matrix $M$ defined above takes on a particularly nice form:\n$$M_{ij} = (\\mathbf x_i - \\mathbf x_1) \\cdot (\\mathbf x_j - \\mathbf x_1) \\equiv \\sum_{a=1}^m \\tilde x_{ia} \\tilde x_{ja}\\,,$$\nwhere the elements $\\tilde x_{ia} = x_{ia} - x_{1a}$ can be assembled into an $n \\times m$ matrix $\\tilde X$. In matrix form,\n$$M = \\tilde X \\tilde X^T \\,.$$\nSuch a matrix is called a Gram matrix. Since the original vectors were given in $m$ dimensions, the rank of $M$ is at most $m$ (assuming $m \\le n$).\nThe points we get by the eigenvalue decomposition described above need not exactly match the points that were put into the calculation of the distance matrix. However, they can be obtained from them by a rotation and a translation. This can be proved for example by doing a singular value decomposition of $\\tilde X$, and showing that if $\\tilde X \\tilde X^T = X X^T$ (where $X$ can be obtained from the eigenvalue decomposition, as above, $X = U\\sqrt S$), then $X$ must be the same as $\\tilde X$ up to an orthogonal transformation.", "meta": {"post_id": 156161, "input_score": 39, "output_score": 54, "post_title": "Finding the coordinates of points from distance matrix"}}
{"input": "In which cases is the inverse of a matrix equal to its transpose, that is, when do we have $A^{-1} = A^{T}$? Is it when $A$ is orthogonal?", "output": "If $A^{-1}=A^T$, then $A^TA=I$. This means that each column has unit length and is perpendicular to every other column. That means it is an orthonormal matrix.", "meta": {"post_id": 156735, "input_score": 55, "output_score": 74, "post_title": "In which cases is the inverse of a matrix equal to its transpose?"}}
{"input": "Possible Duplicate:\nProve that $\\prod_{k=1}^{n-1}\\sin\\frac{k \\pi}{n} = \\frac{n}{2^{n-1}}$ \n\nI am looking for a closed form for this product of sines:\n\\begin{equation}\n\\sin \\left(\\frac{\\pi}{n}\\right)\\,\\sin \\left(\\frac{2\\pi}{n}\\right)\\dots\\sin \\left(\\frac{(n-1)\\pi}{n}\\right),\n\\end{equation}\nwhere $n$ is a fixed integer. I would like to see here a strategy that hopefully can be generalized to similar cases, not just the result (which probably can be easily found).", "output": "Use the formula $\\sin(x) = \\frac{1}{2i}(e^{ix}-e^{-ix})$ to get\n\\begin{align*}\n\\prod_{k=1}^{n-1} \\sin(k\\pi/n) &= \\left(\\frac{1}{2i}\\right)^{n-1}\\prod_{k=1}^{n-1} \\left(e^{k\\pi i/n} - e^{-k\\pi i/n}\\right) \\\\\n&= \\left(\\frac{1}{2i}\\right)^{n-1}\\left(\\prod_{k=1}^{n-1} e^{k\\pi i/n} \\right) \\prod_{k=1}^{n-1} \\left(1-e^{-2k\\pi i/n} \\right).\n\\end{align*}\nThe first product simplifies to\n$$e^{\\sum_{k=1}^{n-1} k\\pi i/n} = e^{(n-1)\\pi i/2} = i^{n-1}$$\nwhich cancels out with the $i^{n-1}$ in the denominator. The second product can be recognized as the polynomial $f(X) = \\prod_{k=1}^{n-1} (X-e^{-2k\\pi i/n})$ evaluated at $X = 1$. The roots of this polynomial are the non-trivial $n$-th roots of unity, so $f(X) = \\frac{X^n-1}{X-1} = 1+X+X^2+\\ldots+X^{n-1}$. Plugging in $1$ for $X$ yields\n$$\\prod_{k=1}^{n-1} \\left(1-e^{-2k\\pi i/n} \\right) = f(1) = n.$$\nAltogether, we have\n$$\\prod_{k=1}^{n-1} \\sin(k\\pi/n) = \\frac{n}{2^{n-1}}.$$", "meta": {"post_id": 159214, "input_score": 14, "output_score": 41, "post_title": "Evaluation of a product of sines"}}
{"input": "Referring to this lecture , I want to know what is the difference between supremum and maximum. It looks same as far as the lecture is concerned when it explains pointwise supremum and pointwise maximum", "output": "A maximum of a set must be an element of the set. A supremum need not be.\nExplicitly, if $X$ is a (partially) ordered set, and $S$ is a subset, then an element $s_0$ is the supremum of $S$ if and only if:\n\n$s\\leq s_0$ for all $s\\in S$; and\nIf $t\\in X$ is such that $s\\leq t$ for all $s\\in S$, then $s_0\\leq t$.\n\nBy contrast, an element $m$ is the maximum of $S$ if and only if:\n\n$s\\leq m$ for all $s\\in S$; and\n$m\\in S$.\n\nNote that if $S$ has a maximum, then the maximum must be the supremum: indeed, if $t\\in X$ is such that $s\\leq t$ for all $s\\in S$, then in particular $m\\in S$, so $m\\leq t$, proving that $m$ satisfies the conditions to be the supremum.\nBut it is possible for a set to have a supremum but not a maximum. For instance, in the real numbers, the set of all negative numbers does not have a maximum: there is no negative number $m$ with the property that $n\\leq m$ for all negative numbers $n$. However, the set of all negative numbers does have a supremum: $0$ is the supremum of the set of negative numbers. Indeed, $a\\leq 0$ for all negative numbers $a$; and if $a\\leq b$ for all negative numbers $a$, then $0\\leq b$.\nThe full relationship between supremum and maximum is: \n\n\nIf $S$ has a maximum $m$, then $S$ also has a supremum and in fact $m$ is also a supremum of $S$. \nConversely, if $S$ has a supremum $s$, then $S$ has a  maximum if and only if $s\\in S$, in which case the maximum is also $s$. \n\n\nIn particular, if a set has both a supremum and a maximum, then they are the same element. The set may also have neither a supremum nor a maximum (e.g., the rationals as a subset of the reals). But if it has only one them, then it has a supremum which is not a maximum and is not in the set.", "meta": {"post_id": 160451, "input_score": 65, "output_score": 84, "post_title": "Difference between supremum and maximum"}}
{"input": "I just need a bit of help clarifying the definition of a compact set.\nLet's start with the textbook definition: \n\nA set $S$ is called compact if, whenever it is covered by a collection of open sets $\\{G\\}$, $S$ is also covered by a finite sub-collection $\\{H\\}$ of $\\{G\\}$.   \n\nQuestion: Does $\\{H\\}$ need to be a proper subset of $\\{G\\}$? If, for instance, $\\{G\\}$ is already a finite collection, does that mean $S$ is automatically covered by a finite sub-collection of $\\{G\\}$? Also, is there any need for the open sets in $\\{H\\}$ to be bounded sets?", "output": "As with many statements involving nested quantifiers, it may help to think of this in terms of a game.  Suppose you are trying to prove that a certain space $G$ is compact.  $G$ is compact if, for every open covering $C$ of $G$, there is a finite subcovering.  So the game goes like this:\n\nYou say  \u201c$G$ is compact.\u201d\nYour adversary says \u201cIt is not.  Here is an open covering $C$.\u201d  (The adversary gives you a family of open sets whose union contains $G$.) \nYou reply \u201cHere is a finite subcovering of $C$.\u201d  (You reply with a finite subset of $C$ whose union still contains $G$.)\n\nIf you succeed in step 3, you win.  If you fail, you lose.  (If you're trying to prove that $G$ is not compact, you and the adversary exchange roles.)\nIf the adversary presents a finite open covering $C$ in step 2, you have an easy countermove in step 3: just hand back $C$ itself, and you win!\nBut to prove that $G$ is compact you also have to be able to find a countermove for any infinite covering $C$ that the adversary gives you.\nMust your finite subcovering be a proper subset of $C$?  No.  If this were required, the adversary would always be able to win in step 2 by handing you a covering $C$ with only a single element,  $C=\\{ G \\}$. Then the only proper subset you could hand back would be $\\lbrace\\mathstrut\\rbrace$, which is not a covering of $G$, and therefore the would be no nonempty compact sets.  That would be silly, so you have to be allowed to hand back $C$ unchanged in step 3.", "meta": {"post_id": 160578, "input_score": 19, "output_score": 35, "post_title": "Understanding the definition of a compact set"}}
{"input": "If $G$ and $H$ are groups with presentations $G=\\langle X|R \\rangle$ and $H=\\langle Y| S \\rangle$, then of course $G \\times H$ has presentation $\\langle X,Y | xy=yx \\ \\forall x \\in X \\ \\text{and} \\  y \\in Y, R,S \\rangle$.  Given two group presentations $G=\\langle X|R \\rangle$ and $H=\\langle Y| S \\rangle$ and a homomorphism $\\phi: H \\rightarrow \\operatorname{Aut}(G)$, what is a presentation for $G \\rtimes H$?  Is there a nice presentation, as in the direct product case?  Thanks!", "output": "Let $G = \\langle X \\mid R\\rangle$ and $H = \\langle Y \\mid S\\rangle$, and let $\\phi\\colon H\\to\\mathrm{Aut}(G)$.  Then the semidirect product $G\\rtimes_\\phi H$ has the following presentation:\n$$\nG\\rtimes_\\phi H \\;=\\; \\langle X, Y \\mid R,\\,S,\\,yxy^{-1}=\\phi(y)(x)\\text{ for all }x\\in X\\text{ and }y\\in Y\\rangle\n$$\nNote that this specializes to the presentation of the direct product in the case where $\\phi$ is trivial.\n\u00a0\nFor example, let $G = \\langle x \\mid x^n = 1\\rangle$ be a cyclic group of order $n$, let $H = \\langle y \\mid y^2=1\\rangle$ be a cyclic group of order two, and let $\\phi\\colon H \\to \\mathrm{Aut}(G)$ be the homomorphism defined by $\\phi(y)(x) = x^{-1}$.  Then the semidirect product $G\\rtimes_\\phi H$ is the dihedral group of order $2n$, with presentation\n$$\nG\\rtimes_\\phi H \\;=\\; \\langle x,y\\mid x^n=1,y^2=1,yxy^{-1}=x^{-1}\\rangle.\n$$", "meta": {"post_id": 160870, "input_score": 27, "output_score": 45, "post_title": "Group presentation for semidirect products"}}
{"input": "This problem was given to me by a friend: \n\nProve that $\\Pi_{i=1}^m \\mathbb{S}^{n_i}$ can be smoothly embedded in a Euclidean space of dimension $1+\\sum_{i=1}^m n_i$. \n\nThe solution is apparently fairly simple, but I am having trouble getting a start on this problem. Any help?", "output": "Note first that $\\mathbb{R}\\times\\mathbb{S}^n$ smoothly embeds in $\\mathbb{R}^{n+1}$ for each $n$, via $(t,\\textbf{p})\\mapsto e^t\\textbf{p}$.\nTaking the Cartesian product with $\\mathbb{R}^{m-1}$, we find that $\\mathbb{R}^m\\times\\mathbb{S}^n$ smoothly embeds in $\\mathbb{R}^{m}\\times\\mathbb{R}^n$ for each $m$ and $n$.\nBy induction, it follows that $\\mathbb{R}\\times\\prod_{i=1}^m \\mathbb{S}^{n_i}$ smoothly embeds in a Euclidean space of dimension $1+\\sum_{i=1}^m n_i$.\n\nThe desired statement follows.", "meta": {"post_id": 161293, "input_score": 33, "output_score": 57, "post_title": "Product of spheres embeds in Euclidean space of 1 dimension higher"}}
{"input": "I don't quite follow the rough outline Milnor gives of the fact that the 7-sphere has different differentiable structures. The video is available here, and the slides he used can be found here.\nHere's what I got from the talk. Unless otherwise stated, all manifolds are compact, orientable smooth (most of the time I state these hypotheses explicitely anyways), and $M$ is an ($n$-dimensional) manifold. Homology and cohomology are taken with coefficients in $\\mathbb{Z}$.\n\nBy a theorem of Whitney, $M$ embeds in $\\mathbb{R}^{n+k}$ from which we get a Gauss map $g:M\\rightarrow G_n(\\mathbb{R}^{n+k})\\subset G_n$, where $G_n(\\mathbb{R}^{n+k})$ is the grassmannian manifold of $n$-planes in $\\mathbb{R}^{n+k}$, and $G_n$ is the colimit of those grassmannians, i.e. the grassmannian of $n$-planes in $\\mathbb{R}^{\\infty}$. All Gauss maps $g$ (viewed as maps $M\\rightarrow G_n$) are homotopic, and we obtain a well defined homology class $\\langle M\\rangle=g_*\\mu\\in H_n(G_n)$. The letter $\\mu$ stands for the fundamental class of $M$.\nIf $M$ is a compact orientable topological manifold of dimension divisible by $4$, say $\\dim~M=4k$, it comes equipped with a symmetric bilinear form given by the cup product in dimensions $2k$:\n$$H^{2k}(M)\\times H^{2k}(M)\\rightarrow H^{4k}(M)\\simeq \\mathbb{Z},~(x,y)\\mapsto x\\cup y$$\nThis form must kill torsion, so we get a quadratic form on the finitely generated free abelian group $H^{2k}(M)/\\mathrm{Torsion}\\simeq \\mathbb{Z}\\oplus\\cdots\\oplus\\mathbb{Z}$, and we can define its signature (as a real quadratic form). The signature of the manifold $M$ is then defined as $\\sigma (M)=p-q$ where $p=\\#$ of positive eigen values and $q=\\#$ of negative eigenvalues.\nGoing back to the differentiable case, we define the Pontrjagin numbers of $M$ by looking at the cohomology ring of $G_n$. This ring is concentrated in dimensions that are multiples of $4$ and has one generator $p_i\\in H^{4i}(G_n)$ for each $i\\geq 1$, so that all $1,p_1, p_1^2,p_2,p_1^3,p_1\\cup p_2,p_3,\\dots$ generate the cohomology (plus some torsion elements). We then define the Pontrjagin numbers of $M$ by evaluating these cohomology classes on the homology class $\\langle M\\rangle$. This gives potentially non zero numbers provided $M$ has dimension $4k$. In particular, if $M$ has dimension $8$ and is smooth, compact, orientable, there are two Pontrjagin numbers: $p_1^2(M)$ and $p_2(M)$.\nBy a theorem of Hirzebruch, the signature of $M$ (with $\\dim~M=4k$) ought to be a polynomial with rational coefficients in the Pontryagine numbers of $M$, and Milnor tells us that in case $\\dim~M=8$,\n$$45\\sigma(M)=7p_2(M)-p_1^2(M).$$\nThis is somehow related to oriented cobordism. Apparantly, two smooth oriented compact manifolds $M$ and $N$ of the same dimension are cobordant iff $\\langle M\\rangle=\\langle N\\rangle$. I understand that the signature is a cobordism invariant, so that it extends to a homomorphism $\\sigma:\\Omega(n)\\rightarrow\\mathbb{Z}$ where $\\Omega(n)$ is the group of oriented cobordisms, and that since cobordism classes are determined by $\\langle M\\rangle\\in H_n(G_n)$, there ought to be a linear relation between the non torsion bits of this homology class (which can be read off the Pontrjagin numbers) and the signature. The main idea of the proof will be to calculate $$\\frac{1}{7}(45\\sigma(M)+p_1^2(M))$$ for some eight dimensional manifold, to observe that it is not an integer thus showing that its boundary, while homeomorphic to the $7$-sphere, cannot be diffeomorphic to it.\n\nPart of my confusion stems from here. Milnor considered $7$-dimensional smooth compact orientable manifolds $M$ that are the total space of a locally trivial fibre bundle with fibre $\\mathbb{S}^3$ over $\\mathbb{S}^4$. He was able to show explicitely that some of these $M$ were homeomorphic to the $7$-sphere by constructing explicit Morse functions with exactly two critical points. However, he found that some of these smooth manifolds that were topological $7$-spheres could not be diffeomorphic to the $7$-sphere. He then (as I understand it) calculated $\\frac{1}{7}(45\\sigma(E)+p_1^2(E))$ for some $8$ dimensional manifold (Which one?) and found that the result was not an integer.\nCould you help me understand how he got to exotic $7$-spheres? What manifold $E$ would one consider? Why is the result a non integer? and how could it be a non integer? since in order to make sense of $p_1^2(E)$ in the first place we need it to be smooth, and then $\\frac{1}{7}(45\\sigma(M)+p_1^2(M))$ must be equal to $p_2(M)$ which is an integer.", "output": "The method in his slides differs from his original paper. First I will explain Milnor's original construction of exotic $7$-spheres, from his article\n\nOn manifolds homeomorphic to the $7$-sphere, Annals of Mathematics, Vol. 64, No. 2, September 1956.\n\nI'll answer your question about his construction mentioned in the slides after that.\n\nFirst, he defines a smooth invariant $\\lambda(M^7)$ for closed, oriented $7$-manifolds $M^7$. To do this, we first note that every $7$-manifold bounds an $8$-manifold, so pick an $8$-manifold $B^8$ bounded by $M^7$. Let $\\mu \\in H_7(M^7)$ be the orientation class for $M^7$ and pick an \"orientation\" $\\nu \\in H_8(B^8,M^7)$, i.e. a class satisfying\n$$\\partial  \\nu = \\mu.$$\nDefine a quadratic form\n$$H^4(B^8,M^7)/\\mathrm{Tors} \\longrightarrow \\mathbb{Z},$$\n$$\\alpha \\mapsto \\langle \\alpha \\smile \\alpha, \\nu \\rangle,$$\nand let $\\sigma(B^8)$ be the signature of this form. Milnor assumes that $M^7$ has\n$$H^3(M^7) \\cong H^4(M^7) \\cong 0,$$\nso that\n$$i: H^4(B^8,M^7) \\longrightarrow H^4(B^8)$$\nis an isomorphism. Hence the number\n$$q(B^8) = \\langle i^\\ast p_1(B^8) \\cup i^\\ast p_1(B^8), \\nu \\rangle$$\nis well-defined. Then Milnor's $7$-manifold invariant is\n$$\\lambda(M^7) \\equiv 2q(B^8) - \\sigma(B^8) \\pmod 7.$$\nMilnor shows that $\\lambda(M^7)$ does not depend on the choice of $8$-manifold $B^8$ bounded by $M^7$.\nNow let us turn to the specific $7$- and $8$-manifolds that Milnor considers. As you noted, he looks at the total spaces of $S^3$-bundles over $S^4$. The total space of such a bundle is a $7$-manifold bounding the total space of the associated disk bundle. $S^3$-bundles over $S^4$ (with structure group $\\mathrm{SO}(4)$) are classified by elements of\n$$\\pi_3(\\mathrm{SO}(4)) \\cong \\mathbb{Z} \\oplus \\mathbb{Z}.$$\nAn explicit isomorphism identifies the pair $(h,j) \\in \\mathbb{Z} \\oplus \\mathbb{Z}$ with the $S^3$-bundle over $S^4$ with transition function\n$$f_{hj}: S^3 \\longrightarrow \\mathrm{SO}(4),$$\n$$f_{hj}(u) \\cdot v = u^h v u^j$$\non the equatorial $S^3$, where here we consider $u \\in S^3$ and $v \\in \\mathbb{R}^4$ as quaternions, i.e. the expression $u^h v u^j$ is understood as quaternion multiplication.\nLet $\\xi_{hj}$ be the $S^3$ bundle on $S^4$ corresponding to $(h,j) \\in \\mathbb{Z} \\oplus \\mathbb{Z}$. For each odd integer $k$, let $M^7_k$ be the total space of the bundle $\\xi_{hj}$, where\n\\begin{align*}\nh + j & = 1, \\\\\nh - j & = k.\n\\end{align*}\nMilnor shows that\n$$\\lambda(M^7_k) \\equiv k^2 - 1 \\pmod 7.$$\nFurthermore, he shows that $M^7_k$ admits a Morse function with exactly $2$ critical points, and hence is homeomorphic to $S^7$. Clearly we have\n$$\\lambda(S^7) \\equiv 0,$$\nso if\n$$k \\not\\equiv \\pm 1 \\pmod 7,$$\nthen $M^7_k$ is homeomorphic but not diffeomorphic to $S^7$, and hence is an exotic sphere. In particular, $S^7$, $M^7_3$, $M^7_5$, and $M^7_7$ are all homeomorphic to one another but all pairwise non-diffeomorphic.\n\nNow, in the slides, the space $E$ should be the total space of the disk bundle associated to an $S^3$ bundle $\\xi_{hj}$ over $S^4$ with\n\\begin{align*}\nh + j & = 1, \\\\\nh - j & = k\n\\end{align*}\nfor some odd integer $k$, as described above. Then $E$ is an $8$-manifold with boundary $\\partial E$ homeomorphic to $S^7$. Now, if $\\partial E$ is diffeomorphic to $S^7$, then we can glue $D^8$ to $E$ along their common boundary via a diffeomorphism\n$$f: \\partial E \\longrightarrow S^7$$\nin order to get a smooth manifold\n$$E' = E \\cup_f D^8.$$\nIf $f$ is not a diffeomorphism, then $E'$ is not necessarily smooth. So in showing that\n$$p_2(E') \\notin \\mathbb{Z},$$\nMilnor proves by contradiction that no such diffeomorphism $f$ can exist, since Pontrjagin numbers of a manifold are integers. So in that case $\\partial E$ would be homeomorphic to $S^7$ but not diffeomorphic, and hence an exotic $7$-sphere.", "meta": {"post_id": 162382, "input_score": 26, "output_score": 38, "post_title": "Question about Milnor's talk at the Abel Prize"}}
{"input": "I have heard that any submodule of a free module over a p.i.d. is free.\nI can prove this for finitely generated modules over a p.i.d.  But the proof involves induction on the number of generators, so it does not apply to modules that are not finitely generated.\nDoes the result still hold?  What's the argument?", "output": "Let $F$ be a free $R$-module, where $R$ is a PID, and $U$ be a submodule. Then $U$ is also free (and the rank is at most the rank of $F$). Here is a hint for the proof.\nLet $\\{e_i\\}_{i \\in I}$ be a basis of $F$. Choose a well-ordering $\\leq$ on $I$ (this requires the Axiom of Choice). Let $p_i : F \\to R$ be the projection on the $i$th coordinate. Let $F_i$ be the submodule of $F$ generated by the $e_j$ with $j \\leq i$. Let $U_i = U \\cap F_i$. Then $p_i(U_i)$ is a submodule of $R$, i.e. has the form $R a_i$. Choose some $u_i \\in U_i$ with $p_i(u_i)=a_i$. If $a_i=0$, we may also choose $u_i=0$.\nNow show that the $u_i \\neq 0$ constitute a basis of $U$. Hint: Transfinite induction.\nThe same proof shows the more general result: If $R$ is a hereditary ring (every ideal of $R$ is projective over $R$), then any submodule of a free $R$-module is a direct sum of ideals of $R$.", "meta": {"post_id": 162945, "input_score": 49, "output_score": 58, "post_title": "Submodule of free module over a p.i.d. is free even when the module is not finitely generated?"}}
{"input": "This is the problem: Determine the smallest positive integer $k$ \nsuch that there exist integers $x_1, x_2 , \\ldots , x_k$ with \n${x_1}^3+{x_2}^3+{x_3}^3+\\cdots+{x_k}^3=2002^{2002} $. How to approach these kind of problems??\nThanks in advance!!", "output": "$k=4$ is the smallest:\nCertainly, it can be done using 4 cubes, by noticing that $2002 = 10^3 + 10^3 + 1^3 +1^3$, and then using $2002^{2002} = 2002 \\times 2002^{2001} = (10^3 + 10^3 + 1^3 +1^3)\\times (2002^{667})^3$, and multiplying out the brackets.\nSince the number can be represented by 4 cubes, it suffices to show that it cannot be done with less than 4.\nSince $2002 \\equiv 4 \\pmod 9$ we have $2002^3 \\equiv 64 \\equiv 1 \\pmod 9$ so that $2002^{3n} \\equiv 1 \\pmod 9$ and so the original number is equivalent to 4 (mod 9).\nLooking at cubes mod 9, they are equivalent to 0, 1 or -1, so at least 4 are required for any number equivalent to $4 \\pmod 9$.", "meta": {"post_id": 163283, "input_score": 25, "output_score": 66, "post_title": "Expressing $2002^{2002}$ as a sum of cubes"}}
{"input": "I read from the Finnish version of the book \"Fermat's last theorem, Unlocking the Secret of an Ancient Mathematical Problem\", written by Amir D. Aczel, that genus describes how many handles there are on a given surface. But now I read the Proposition 4.1 on chapter 7.4.1 on Qing Liu's book \"Algebraic Geometry and Arithmetic Curves\". It assumes a geometrically integral projective curve $X$ over a field such that the arithmetic genus of $X$ is $p_a\\leq 0$. So is my intuition that \"genus is the number of handles\" somehow wrong as $p_a$ can be negative?", "output": "A compact  Riemann surface $X$ is in particular a compact real orientable surface. These surfaces are classified by their genus.\nThat genus  is indeed the number of handles cited in popular literature; more technically it is\n$$g(X)=\\frac {1}{2}\\operatorname {rank} H_1(X,\\mathbb Z)  =  \\frac {1}{2}\\operatorname {dim} _\\mathbb C H^1_{DR}(X,\\mathbb C)                                        $$ in terms of singular homology or De Rham cohomology.   \nUnder the pressure of arithmetic, geometers have been spurred to consider the analogue of compact Riemann surfaces over fields $k$ different from $\\mathbb C$: complete smooth algebraic curves.\nThese have a genus that must be calculated without topology.  \nThe modern definition is (for algebraically closed fields)  $$ g(X)=\\operatorname {dim} _k H^1(X, \\mathcal O_X)= \\operatorname {dim} _kH^0(X, \\Omega _X)$$\nin terms of the sheaf cohomology of the structural sheaf or of the sheaf of differential forms of the curve $X$.\nOf course this geometric  genus is always $\\geq 0$.\nThere is a more general notion of genus applicable to higher dimensional and/or  non-irreducible varieties over non algebraically closed fields: the arithmetic genus defined by $$p_a(X)=(-1)^{dim X}(\\chi(X,\\mathcal O_X)-1)\\quad  {(ARITH)}$$  (where $\\chi(X,\\mathcal O_X)$ is the Euler-Poincar\u00e9 characteristic of the structure sheaf).\n[ Hirzebruch and Serre have, for very good reasons, advocated  the modified definition $p'_a(X)=(-1)^{dim X}\\chi(X,\\mathcal O_X)$, which Hirzebruch  used in his ground-breaking  book  and Serre in his foundational FAC]  \nFor smooth projective  curves over an algebraically closed field $g(X)=p_a(X)\\geq 0$ : no problem.\nIt is only   in more general situations that the arithmetic genus $p_a(X)$ may indeed be $\\lt 0$  \nEdit\nThe simplest example of a reducible variety with negative arithmetic genus is the disjoint union $X=X_1\\bigsqcup  X_2$ of two copies $X_i$ of $\\mathbb P^1$.\nThe formula $(ARITH)$ displayed above yields: $p_a(X)=1-\\chi(X,\\mathcal O_X)=1-(dim_\\mathbb C H^0(X,\\mathcal O_X)-dim_\\mathbb C H^1(X,\\mathcal O_X))=1-(2-0)$\n so that $$p_a(X)=p_a(\\mathbb P^1\\bigsqcup  \\mathbb P^1)=-1\\lt0$$", "meta": {"post_id": 163356, "input_score": 28, "output_score": 37, "post_title": "What is an intuitive meaning of genus?"}}
{"input": "Can someone give me an idea of how R.Graham reached Graham's Number as an upper bound on the solution of the related problem ? Thanks !", "output": "This post appears long and frightening, but I hope you will not be put off, because the topic is not actually hard to understand.  It is long because I explained a lot of things from first principles. I did this because I thought the answer would be of  interest to a general audience and because the branch of mathematics is not that well-known. So there is a lot of explanation, but not much is difficult.\n\nFirst, a disclaimer.  Graham's Number is usually cited as the largest number ever to appear in a mathematical proof.  There is no evidence for this, and in fact the claim is false on its face, because Graham's Number does not actually appear in the proof that it is claimed to appear in.  (It can't be the largest number ever to appear in a mathematical proof if it doesn't actually appear in a mathematical proof.)   According to these posts by John Baez [1] [2]:\n\nI asked Graham.  And the answer was interesting.  He said he made up Graham's number when talking to Martin Gardner!  Why?   Because it was simpler to explain than his actual upper bound - and bigger, so it's still an upper bound!\n\nMartin Gardner then wrote about the number that Graham described, which is not the number from the proof, and the rest is history.\nNow what is the number from the proof?  Here there is some interesting mathematics.\n\nThe question addressed by Graham's Number belongs to the branch of mathematics known as Ramsey theory, which is not at all hard to understand. It can roughly be described as the study  of whether  a sufficiently large structure, chopped into pieces,  must still contain smaller structures.  This is a rather vague explanation, so I will give two of the canonical examples.\n\nRamsey's theorem.  Let $n$ and $k$ be given.  Then there exists a number $R(n;k)$ such that, if you take a complete graph of at least $R(n;k)$ vertices, and color its edges in $k$ different colors, then there must be a complete subgraph of $n$ vertices whose edges are all the same color.\nA frequently-cited special case of this theorem says that $R(3;2) = 6$: if you have a party with at least 6 guests, then there must be 3 guests who have all met one another before, or 3 people who have never met; it is impossible that every subset of three guests has both a pair of people who have met and a pair of people who have not.   (With only five guests, this is possible.)  Here the two \u201ccolors\u201d are \u201chave met before\u201d and \u201chave not met before\u201d.\n\nVan der Waerden's theorem. Let $n$ and $k$ be given.  Then there exists a number $W(n;k)$ such that, if you take an arithmetic progression of length $W(n;k)$, and color its elements in $k$ different colors, it must contain an arithmetic progression of $n$ elements that are all the same color.\n\n\nIn both these examples you can see the general pattern: we have some large structure (a graph of $R(n;k)$ vertices in one case, an arithmetic progression of $W(n;k)$ elements in the other) and we divide the structure into $k$ parts and ask if one of the parts still contains a sub-structure of size $n$.\nThe proofs of these theorems are constructive.  For example, the proof of van der Waerden's theorem allows one to calculate that for $W(3;2) \\ge 325$ suffices: if you color the integers $\\{1, \\ldots, 325\\}$ with $k=2$ colors, then there must be an $n=3$-term arithmetic progression whose elements are all one color, and the proof shows you how to take an arbitrary coloring of $\\{1, \\ldots, 325\\}$  and explicitly find the $3$-term subprogression of all one color.\nBut the $\\{1, \\ldots, 325\\}$ is rather silly, because in fact the same is true of $\\{1, \\ldots, 9\\}$, as is easily shown. So the proof gives an upper bound of $325$ when the correct answer is $9$.  This is typical of theorems in Ramsey theorem: the proof tells you that the number exists and is at most some large number, but then closer investigation reveals that it is really some considerably smaller number.  The corresponding overestimate for $W(3;3)$ is that the proof tells you that $$W(3;3) \\le 7\\cdot\\left(2\\cdot3^7+1\\right)\\left(2\\cdot3^{\\left(2\\cdot3^7+1\\right)}+1\\right),$$\na number with $2095$ digits, but exhaustive computer search quickly reveals that actually $W(3;3)=27$.\nThe reason for these rapidly-growing bounds is that typically the proofs proceed by induction, and one shows that if there are sufficiently many size-$n-1$ structures, then two of them must have their subcomponents  colored exactly the same, and this allows one to find sub-parts of those size-$n-1$ structures that work together to form a size-$n$ structure of all one color.  But a size-$n-1$ structure with $S(n-1)$ subcomponents will have something like $k^{S(n-1)}$ ways its components can be colored, so \u201csufficiently many\u201d means something like $k^{S(n-1)}$, and the number required looks something like an exponential tower of $k$'s of height $n$, that is something like $\\left.k^{k^{\u22f0^k}}\\right\\} \\text{height $n$}$; you can see this happening in the $W(3;3)$ example above, where the third factor is an embellished version of $3^{3^3}$.  When the structures one is forming are more complicated, then instead of needing only two size-$n-1$ structures colored the same, one might need an increasingly large number of such structures, and so perhaps you can imagine how the number required increases even faster than an exponential tower.\n(That was the crucial paragraph that really answers your question, so I apologize for being so vague; please let me know if you want me to elaborate or provide a specific example.)\nEnormous numbers are quite commonplace in Ramsey theory, and so the Graham's Number might have some competition even in its own field.\n\nThe particular problem discussed in\nthe Graham's Number paper, \u201cRamsey's theorem for $n$-parameter sets\u201d is rather general, but the enormous number (not the one described by Gardner) is an upper bound for a problem very similar to the ones I described above:\n\nWe recall that by definition $N(1, 2, 2)$ is an integer such that\nif $n\\ge N(1, 2, 2)$ and the $\\binom{2^n}{2}$ straight line segments joining all possible pairs of vertices of a unit $n$-cube are arbitrarily 2-colored, then there always exists a set of four coplanar vertices which determines six line segments of the same color.\n\nThat is, Graham and Rothschild are investigating a problem that, in this special case, involves taking a certain $n$-dimensional object, coloring its 1-dimensional subobjects with 2 colors, and looking for a single-colored  2-dimensional subobject; $N(1,2,2)$ is the smallest number of dimensions that such an object must have in order to guarantee a single-colored 2-dimensional subobject.\n\nLet $N^*$ denote the least possible value $N(1,2,2)$ can assume.\nWe introduce a calibration function $F(m,n)$ with which me may compare our estimate of $N^*$. This is defined recursively as follows:\n$$\\begin{align}\nF(1,n)=2^n \\qquad F(m,2)=4 &\\qquad m\\ge 1, n\\ge 2, \\\\\nF(m,n) = F(m-1, F(m,n-1)) & \\qquad m\\ge2, n\\ge 3.\n\\end{align} $$\nIt is recommended that the reader calculate a few small values of $F$ to get a feeling for its rate of growth, e.g. $F(5,5)$ or $F(10,3)$.\n\n\u201cA few small values\u201d here is a joke. $F(3,3)$ is already $2^{16}=65536$.  $F(3,4)$ is a tower $2^{2^{\u22f0^2}}$ of height $65536$.  $F(3,5)$ is a similar tower of height $F(3,4)$.\nFinally, the bound:\n\nThe best estimate we obtain this way is roughly\n$$N^* \\le F(F(F(F(F(F(F(12,3),3),3),3),3),3),3).$$\n\nThe authors continue with an understatement that I imagine made them chuckle:\n\nOn the other hand, it is known only that $N^*\\ge 6$.  Clearly, there is some room for improvement here.\n\nA remark a little later says\n\nin fact, the exact bound is probably $<10$.\n\nbut Sbiis Saibian's excellent discussion of this claims, unfortunately without citation:\n\nIt was recently proved that the solution could not be smaller than $11$.\n\nI will be glad to elaborate on any part of this that is not clear.", "meta": {"post_id": 163423, "input_score": 22, "output_score": 46, "post_title": "Graham's Number : Why so big?"}}
{"input": "Problem: Let $f$ be defined for all real $x$, and suppose that \n$$|f(x)-f(y)|\\le (x-y)^2$$\nfor all real $x$ and $y$. Prove $f$ is constant.\nSource: W. Rudin, Principles of Mathematical Analysis, Chapter 5, exercise 1.", "output": "For any $x\\in\\mathbb{R}$,\n$$\n\\begin{align}\n|f'(x)|\n&=\\lim_{h\\to0}\\frac{|f(x+h)-f(x)|}{|h|}\\\\\n&\\le\\lim_{h\\to0}\\frac{h^2}{|h|}\\\\\n&=0\n\\end{align}\n$$\nTherefore, $f$ is constant.", "meta": {"post_id": 164804, "input_score": 24, "output_score": 37, "post_title": "Show $f$ is constant if $|f(x)-f(y)|\\leq (x-y)^2$."}}
{"input": "I have the feeling like there are two very different definitions for what a tensor product is.  I was reading Spivak and some other calculus-like texts, where the tensor product is defined as\n$(S \\otimes T)(v_1,...v_n,v_{n+1},...,v_{n+m})= S(v_1,...v_n) * T(v_{n+1},...,v_{n+m}) $\nThe other definition I read in a book on quantum computation, its defined for vectors and matrices and has several names, \"tensor product, Kronecker Product, and Outer product\": http://en.wikipedia.org/wiki/Outer_product#Definition_.28matrix_multiplication.29\nI find this really annoying and confusing.  In the first definition, we are taking tensor products of multilinear operators (the operators act on vectors) and in the second definition the operation IS ON vectors and matrices.  I realize that matrices are operators but matrices aren't multilinear.  Is there a connection between these definitions?", "output": "Let's first set some terminology.\nLet $V$ be an $n$-dimensional real vector space, and let $V^*$ denote its dual space.  We let $V^k = V \\times \\cdots \\times V$ ($k$ times).\n\nA tensor of type $(r,s)$ on $V$ is a multilinear map $T\\colon V^r \\times (V^*)^s \\to \\mathbb{R}$.\nA covariant $k$-tensor on $V$ is a multilinear map $T\\colon V^k \\to \\mathbb{R}$.\n\nIn other words, a covariant $k$-tensor is a tensor of type $(k,0)$.  This is what Spivak refers to as simply a \"$k$-tensor.\"\n\nA contravariant $k$-tensor on $V$ is a multilinear map $T\\colon (V^*)^k\\to \\mathbb{R}$.\n\nIn other words, a contravariant $k$-tensor is a tensor of type $(0,k)$.\n\nWe let $T^r_s(V)$ denote the vector space of tensors of type $(r,s)$.  So, in particular,\n\n$$\\begin{align*}\nT^k(V) := T^k_0(V) & = \\{\\text{covariant $k$-tensors}\\} \\\\\nT_k(V) := T^0_k(V) & = \\{\\text{contravariant $k$-tensors}\\}.\n\\end{align*}$$\nTwo important special cases are:\n$$\\begin{align*}\nT^1(V) & = \\{\\text{covariant $1$-tensors}\\} = V^* \\\\\nT_1(V) & = \\{\\text{contravariant $1$-tensors}\\} = V^{**} \\cong V.\n\\end{align*}$$\nThis last line means that we can regard vectors $v \\in V$ as contravariant 1-tensors.  That is, every vector $v \\in V$ can be regarded as a linear functional $V^* \\to \\mathbb{R}$ via\n$$v(\\omega) := \\omega(v),$$\nwhere $\\omega \\in V^*$.\n\nThe rank of an $(r,s)$-tensor is defined to be $r+s$.\n\nIn particular, vectors (contravariant 1-tensors) and dual vectors (covariant 1-tensors) have rank 1.\n\nIf $S \\in T^{r_1}_{s_1}(V)$ is an $(r_1,s_1)$-tensor, and $T \\in T^{r_2}_{s_2}(V)$ is an $(r_2,s_2)$-tensor, we can define their tensor product $S \\otimes T \\in T^{r_1 + r_2}_{s_1 + s_2}(V)$ by\n$$(S\\otimes T)(v_1, \\ldots, v_{r_1 + r_2}, \\omega_1, \\ldots, \\omega_{s_1 + s_2}) = \\\\\nS(v_1, \\ldots, v_{r_1}, \\omega_1, \\ldots,\\omega_{s_1})\\cdot T(v_{r_1 + 1}, \\ldots, v_{r_1 + r_2}, \\omega_{s_1 + 1}, \\ldots, \\omega_{s_1 + s_2}).$$\nTaking $s_1 = s_2 = 0$, we recover Spivak's definition as a special case.\nExample: Let $u, v \\in V$.  Again, since $V \\cong T_1(V)$, we can regard $u, v \\in T_1(V)$ as $(0,1)$-tensors.  Their tensor product $u \\otimes v \\in T_2(V)$ is a $(0,2)$-tensor defined by\n$$(u \\otimes v)(\\omega, \\eta) = u(\\omega)\\cdot v(\\eta)$$\n\nAs I suggested in the comments, every bilinear map -- i.e. every rank-2 tensor, be it of type $(0,2)$, $(1,1)$, or $(2,0)$ -- can be regarded as a matrix, and vice versa.\nAdmittedly, sometimes the notation can be constraining.  That is, we're used to considering vectors as column vectors, and dual vectors as row vectors.  So, when we write something like $$u^\\top A v,$$\nour notation suggests that $u^\\top \\in T^1(V)$ is a dual vector and that $v \\in T_1(V)$ is a vector.  This means that the bilinear map $V \\times V^* \\to \\mathbb{R}$ given by\n$$(v, u^\\top) \\mapsto u^\\top A v$$\nis a type $(1,1)$-tensor.\nExample: Let $V = \\mathbb{R}^3$.  Write $u = (1,2,3) \\in V$ in the standard basis, and $\\eta = (4,5,6)^\\top \\in V^*$ in the dual basis.  For the inputs, let's also write $\\omega = (x,y,z)^\\top \\in V^*$ and $v = (p,q,r) \\in V$.  Then\n$$\\begin{align*}\n(u \\otimes \\eta)(\\omega, v) & = u(\\omega) \\cdot \\eta(v) \\\\\n& = \\begin{pmatrix}\n 1 \\\\\n 2 \\\\\n 3\n\\end{pmatrix} (x,y,z)\n\\cdot\n(4,5,6) \\begin{pmatrix}\n p \\\\\n q \\\\\n r\n\\end{pmatrix} \\\\\n& = (x + 2y + 3z)(4p + 5q + 6r) \\\\\n& = 4px + 5 qx + 6rx \\\\\n& \\ \\ \\ \\ \\ 8py + 10qy + 12py \\\\\n& \\ \\ \\ \\ \\ 12pz + 15qz + 18rz \\\\\n& = (x,y,z)\\begin{pmatrix}\n 4 & 5 & 6 \\\\\n 8 & 10 & 12 \\\\\n 12 & 15 & 18\n\\end{pmatrix}\\begin{pmatrix}\n p \\\\\n q \\\\\n r\n\\end{pmatrix} \\\\\n& = \\omega \\begin{pmatrix}\n 4 & 5 & 6 \\\\\n 8 & 10 & 12 \\\\\n 12 & 15 & 18\n\\end{pmatrix} v.\n\\end{align*}$$\nConclusion: The tensor $u \\otimes \\eta \\in T^1_1(V)$ is the bilinear map $(\\omega, v)\\mapsto \\omega A v$, where $A$ is the $3 \\times 3$ matrix above.\nThe Wikipedia article you linked to would then regard the matrix $A$ as being equal to the tensor product $u \\otimes \\eta$.\n\nFinally, I should point out two things that you might encounter in the literature.\nFirst, some authors take the definition of an $(r,s)$-tensor to mean a multilinear map $V^s \\times (V^*)^r \\to \\mathbb{R}$ (note that the $r$ and $s$ are reversed).  This also means that some indices will be raised instead of lowered, and vice versa.  You'll just have to check each author's conventions every time you read something.\nSecond, note that there is also a notion of tensor products of vector spaces.  Many textbooks, particularly ones focused on abstract algebra, regard this as the central concept.  I won't go into this here, but note that there is an isomorphism\n$$T^r_s(V) \\cong \\underbrace{V^* \\otimes \\cdots \\otimes V^*}_{r\\text{ copies}} \\otimes \\underbrace{V \\otimes \\cdots \\otimes V}_{s \\text{ copies}}.$$\nConfusingly, some books on differential geometry define the tensor product of vector spaces in this way, but I think this is becoming rarer.", "meta": {"post_id": 164975, "input_score": 24, "output_score": 45, "post_title": "Tensors: Acting on Vectors vs Multilinear Maps"}}
{"input": "Given a field $F$, can you necessarily construct a field extension $E \\supset F$ such that $\\operatorname{Gal}(E/F) = S_n\\,$?", "output": "We will prove that there exists a finite Galois extension $K/\\mathbb{Q}$ such that $S_n$ = $Gal(K/\\mathbb{Q})$ for every integer $n \\geq 1$.\nWe will follow mostly van der Waerden's book on algebra.\nYou can also see his proof on Milne's course note on Galois theory.\nHowever, Milne refers to his book for a crucial theorem(Proposition 1 below) whose proof uses multivariate polynomials.\nInstead, we will use elementary commutative algebra to prove this theorem.\nNotations\nWe denote by |S| the number of elements of a finite set S.\nLet $K$ be a field.\nWe denote by $K^*$ the multiplicative group of $K$.\nLet $\\tau$ = $(i_1, ..., i_m)$ be a cycle in $S_n$.\nThe set {$i_1, ..., i_m$} is called the support of $\\tau$.\nLet $\\sigma \\in S_n$.\nLet $\\sigma$ = $\\tau_1\\dots\\tau_r$, where each $\\tau_i$ is a cycle of length $m_i$ and they have mutually disjoint supports.\nThen we say $\\sigma$ is of type [$m_1, ..., m_r$].\nDefinition 1\nLet $F$ be a field.\nLet $f(X)$ be a non-constant polynomial of degree n in $F[X]$.\nLet $K/F$ be a splitting field of $f(X)$.\nSuppose $f(X)$ has distinct $n$ roots in $K$.\nThen $f(X)$ is called separable.\nSince the splitting fields of $f(X)$ over $F$ are isomorphic to each other,\nthis definition does not depend on a choice of a splitting field of $f(X)$.\nDefinition 2\nLet $F$ be a finite field.\nLet $|F| = q$.\nLet $K/F$ be a finite extension of $F$.\nLet $\\sigma$ be a map: $K \\rightarrow K$ defined by $\\sigma(x) = x^q$ for each $x \\in K$.\n$\\sigma$ is an automorphism of $K/F$.\nThis is called the Frobenius automorphism of $K/F$.\nDefinition 3\nLet $G$ be a permutation group on a set $X$.\nLet $G'$ be a permutation group on a set $X'$.\nLet $f:X \\rightarrow X'$ be a bijective map.\nLet $\\lambda:G \\rightarrow G'$ be an isomophism.\nSuppose $f(gx) = \\lambda(g).f(x)$ for any $g \\in G$ and any $x \\in X$.\nThen G and G' are said to be isomorphic as permutation groups.\nLemma 1\nLet $F$ be a field.\nLet $f(X)$ be a separable polynomial of degree $n$ in $F[X]$.\nLet $K/F$ be a splitting field of $f(X)$.\nLet $G = Gal(K/F)$.\nLet $S$ be the set of roots of $f(X)$ in K.\nThen $G$ acts transitively on $S$ if and only if $f(X)$ is irreducible in $F[X]$.\nProof:\nIf $f(X)$ is irreducible, clearly $G$ acts transitively on $S$.\nConversely, suppose $f(X)$ is not irreducible.\nLet $f(X) = g(X)h(X)$, where $g$ and $h$ are non-constant polynomials in $F[X]$.\nLet $T$ be the set of roots of $g(X)$ in $K$.\nSince $G$ acts on $T$ and $S \\neq T$, $S$ is not transitive.\nQED\nLemma 2\nLet $F$ be a field.\nLet $f(X)$ be a separable polynomial in F[X].\nLet $f(X) = f_1(X)...f_r(X)$, where $f_1(X), ..., f_r(X)$ are distinct irreducible polynomials in $F[X]$.\nLet $K/F$ be a splitting field of $f(X)$.\nLet $G = Gal(K/F)$.\nLet $S$ be the set of roots of $f(X)$ in $K$.\nLet $S_i$ be the set of roots of $f_i(X)$ in $K$ for each $i$.\nThen $S = \\cup S_i$ is a disjoint union and each $S_i$ is a $G$-orbit.\nProof:\nThis follows immediately from Lemma 1.\nLemma 3\nLet $F$ be a finite field.\nLet $K/F$ be a finite extension of $F$.\nThen $K/F$ is a Galois extension and $Gal(K/F)$ is a cyclic group generated by the Frobenius automorphism $\\sigma$.\nProof:\nLet $|F| = q$.\nLet $n = (K : F)$.\nSince $|K^*| = q^n - 1$, $x^{q^n - 1} = 1$ for each $x \\in K^*$.\nHence $x^{q^n} = x$ for each $x \\in K$.\nHence $\\sigma^n = 1$.\nLet m be an integer such that $1 \\leq m < n$.\nSince the polynomial $X^{q^m} - X$ has at most $q^m$ roots in $K$, $\\sigma^m \\neq 1$.\nHence $\\sigma$ generates a subgroup $G$ of order n of $Aut(K/F)$.\nSince $n = (K : F)$, $G = Aut(K/F)$.\nSince $|Aut(K/F)| = n$, $K/F$ is a Galois extension.\nQED\nLemma 4\nLet $F$ be a finite field.\nLet $f(X)$ be an irreducible polynomial of degree $n$ in $F[X]$.\nLet $K/F$ be a splitting field of $f(X)$.\nLet $\\sigma$ be the Frobenius automorphism of $K/F$.\nThen $Gal(K/F)$ is a cyclic group of order $n$ generated by $\\sigma$.\nProof:\nLet $\\alpha$ be a root of $f(X)$ in $K$.\nBy Lemma 3, $F(\\alpha)/F$ is a Galois extension. Hence $K = F(\\alpha)/F$\nBy Lemma 3, $Gal(F(\\alpha)/F)$ is a cyclic group of order $n$ generated by $\\sigma$.\nQED\nLemma 5\nLet $F$ be a finite field.\nLet $f(X)$ be an irreducible polynomial of degree $n$ in $F[X]$.\nLet $K/F$ be a splitting field of $f(X)$.\nLet $G = Gal(K/F)$.\nLet $\\sigma$ be the Frobenius automorphism of $K/F$.\nLet $S$ be the set of roots of $f(X)$.\nWe regard $G$ as a permutation group on $S$.\nThen $\\sigma$ is an $n$-cycle.\nProof:\nThis follows immediately from Lemma 4.\nLemma 6\nLet $F$ be a finite field.\nLet $f(X)$ be a separable polynomial in F[X].\nLet $f(X) = f_1(X)...f_r(X)$, where $f_1(X), ..., f_r(X)$ are distinct irreducible polynomials in $F[X]$.\nLet $m_i$ = deg $f_i(X)$ for each $i$.\nLet $K/F$ be a splitting field of $f(X)$.\nLet $G = Gal(K/F)$.\nLet $\\sigma$ be the Frobenius automorphism of $K/F$.\nLet $S$ be the set of roots of $f(X)$.\nWe regard $G$ as a permutation group on $S$.\nThen $\\sigma$ is a permutation of type $[m_1, ..., m_r]$.\nProof:\nThis follows immediately from Lemma 2, Lemma 3 and Lemma 5.\nLemma 7\n$S_n$ is generated by $(k, n)\u3001k = 1\u3001\uff0e\uff0e\uff0e\u3001n - 1$.\nProof:\nLet $(a, b)$ be a transpose on {$1, ..., n$}.\nIf $a \\neq n$ and $b \\neq n$, then $(a, b) = (a, n)(b, n)(a, n)$.\nSince $S_n$ is generated by transposes, we are done.\nQED\nLemma 8\nLet $G$ be a transitive permutation group on a finite set $X$.\nLet $n = |X|$.\nSuppose $G$ contains a transpose and a $(n-1)$-cycle.\nThen $G$ is a symmetric group on X.\nProof:\nWithout loss of generality, we can assume that $X$ = {$1, ..., n$} and $G$ contains\na cycle $\\tau$ = $(1, ..., n-1)$ and transpose $(i, j)$.\nSince $G$ acts transitively on $X$, there exists $\\sigma \\in G$ such that $\\sigma(j)$ = $n$.\nLet $k$ = $\\sigma(i)$.\nThen $\\sigma(i, j)\\sigma^{-1}$ = $(k, n) \\in G$.\nTaking conjugates of $(k, n)$ by powers of $\\tau$, we get $(m, n), m = 1, ..., n - 1$. Hence, by Lemma 7, $G = S_n$.\nQED \nLemma 9\nLet $F$ be a finite field.\nLet $n \\geq 1$ be an integer.\nThen there exists an irreducible polynomial of degree $n$ in $F[X]$.\nProof:\nLet $|F| = q$.\nLet $K/F$ be a splitting field of the polynomial $X^{q^n} - X$ in $F[X]$.\nLet $S$ be the set of roots of $X^{q^n} - X$ in $K$.\nIt's easy to see that $S$ is a subfield of $K$ containing $F$.\nHence $S = K$.\nSince $X^{q^n} - X$ is separable, $|S| = q^n$.\nHence $(K : F) = n$.\nSince $K^*$ is a cyclic group, $K^*$ has a generator $\\alpha$.\nLet $f(X)$ be the minimal polynomial of $\\alpha$ over $F$.\nSince $K = F(\\alpha)$, the degree of $f(X)$ is $n$.\nQED\nLemma 10\nLet $f(X) \\in \\mathbb{Z}[X]$ be a monic polynomial.\nLet $p$ be a prime number.\nSuppose $f(X)$ mod $p$ is separable in $\\mathbb{Z}/p\\mathbb{Z}[X]$.\nThen $f(X)$ is separable in $\\mathbb{Q}$.\nProof:\nSuppose $f(X)$ is not separable in $\\mathbb{Q}$.\nSince $\\mathbb{Q}$ is perfect, there exists a monic irreducible $g(X) \\in \\mathbb{Z}[X]$ such that $f(X)$ is divisible by $g(X)^2$. Then $f(X)$ (mod $p$) is divisible by $g(X)^2$ (mod $p$). This is a contradiction.\nQED\nProposition 1\nLet $A$ be an integrally closed domain and let $P$ be a prime ideal of $A$.\nLet $K$ be the field of fractions of A.\nLet $\\tilde{K}$ be the field of fractions of $A/P$.\nLet $f(X) \u2208 A[X]$ be a monic polynomial without multiple roots.\nLet $\\tilde{f}(X) \\in (A/P)[X]$ be the reduction of $f(X)$ mod $P$.\nSuppose $\\tilde{f}(X)$ is also wihout multiple roots.\nLet $L$ be the splitting field of $f(X)$ over $K$.\nLet $G$ be the Galois group of $L/K$.\nLet S be the set of roots of $f(X)$ in $L$.\nWe regard $G$ as a permutation group on $S$.\nLet $\\tilde{L}$ be the splitting field of $\\tilde{f}(X)$ over $\\tilde{K}$.\nLet $\\tilde{G}$ be the Galois group of $\\tilde{L}/\\tilde{K}$.\nLet $\\tilde{S}$ be the set of roots of $\\tilde{f}(X)$ in $\\tilde{L}$.\nWe regard $\\tilde{G}$ as a permutation group on $\\tilde{S}$.\nThen there exists a subgroup $H$ of $G$ such that $H$ and $\\tilde{G}$ are isomorphic as permutation groups.\nProof:\nSee my answer here.\nCorollary\nLet $f(X) \\in \\mathbb{Z}[X]$ be a monic polynomial of degree $m$.\nLet p be a prime number.\nSuppose $f(X)$ mod $p$ is separable in $\\mathbb{Z}/p\\mathbb{Z}[X]$.\nSuppose $f \\equiv f_1...f_r$ (mod $p$), where each $f_i$ is monic and irreducible of degree $m_i$ in $\\mathbb{Z}/p\\mathbb{Z}[X]$.\nLet $K/\\mathbb{Q}$ be a splitting field of $f(X)$.\nLet $M$ be the set of roots of $f(X)$.\n$G = Gal(K/\\mathbb{Q})$ can be regarded as a permutation group on $M$.\nThen $G$ contains an element of type [$m_1, ..., m_r$].\nProof:\nBy Lemma 10, $f(X)$ is separable in $\\mathbb{Q}[X]$.\nLet $F_p$ = $\\mathbb{Z}/p\\mathbb{Z}[X]$.\nLet $\\tilde{f}(X) \\in F_p[X]$ be the reduction of $f(X)$ mod $p$.\nLet $\\tilde{K}/F_p$  be a splitting field of $\\tilde{f}(X)$.\nLet $\\tilde{G}$ be the Galois group of $\\tilde{K}/F_p$.\nLet $\\tau$ be the Frobenius automorphism of $\\tilde{K}/F_p$.\nLet $\\tilde{M}$ be the set of roots of $\\tilde{f}(X)$.\nWe regard $\\tilde{G}$ as a permutation group on $\\tilde{M}$.\nBy Lemma 6, $\\tau$ is a permutation of type $[m_1, ..., m_r]$.\nHence the assertion follows by Proposition 1.\nQED\nTheorem\nThere exists a finite Galois extension $K/\\mathbb{Q}$ such that $S_n$ = $Gal(K/\\mathbb{Q})$ for every integer $n \\geq 1$.\nProof(van der Waerden):\nBy Lemma 9, we can find the following irreducible polynomials.\nLet $f_1$ be a monic irreducible polynomial of degree $n$ in $\\mathbb{Z}/2\\mathbb{Z}[X]$.\nLet $g_0$ be a monic polynomial of degree 1 in $\\mathbb{Z}/3\\mathbb{Z}[X]$.\nLet $g_1$ be a monic irreducible polynomial of degree $n - 1$ in $\\mathbb{Z}/3\\mathbb{Z}[X]$.\nLet $f_2 = g_0g_1$.\nIf $n - 1 = 1$, we choose $g_1$ such that $g_0 \\ne g_1$.\nHence $f_2$ is separable.\nLet $h_0$ be a monic irreducible polynomial of degree 2 in $\\mathbb{Z}/5\\mathbb{Z}[X]$.\nIf $n - 2$ is odd, Let $h_1$ be a monic irreducible polynomial of degree $n - 2$ in $\\mathbb{Z}/5\\mathbb{Z}[X]$.\nLet $f_3 = h_0h_1$.\nSince $h_0 \\neq h_1$, $f_3$ is separable.\nIf $n - 2$ is even, $n - 2 = 1 + a$ for some odd integer $a$.\nLet $h_1$ and $h_2$ be monic irreducible polynomials of degree $1$ and $a$ respectively in $\\mathbb{Z}/5\\mathbb{Z}[X]$.\nLet $f_3 = h_0h_1h_2$.\nIf a = 1, we choose $h_2$ such that $h_1 \\ne h_2$.\nHence $f_3$ is separable.\nLet $f = -15f_1 + 10f_2 + 6f_3$.\nSince each of $f_1, f_2, f_3$ is a monic of degree $n$, f is a monic of degree $n$.\nThen,\n$f \\equiv f_1$ (mod 2)\n$f \\equiv f_2$ (mod 3)\n$f \\equiv f_3$ (mod 5)\nSince $f \\equiv f_1$ (mod 2), $f$ is irreducible.\nLet $K/\\mathbb{Q}$ be the splitting field of $f$.\nLet $G = Gal(K/\\mathbb{Q})$.\nLet $M$ be the set of roots in $K$.\nWe regard $G$ as a permutation group on $M$.\nSince $f$ is irreducible, $G$ acts transitively on $M$.\nSince $f \\equiv f_2$ (mod 3), $G$ contains a $(n-1)$-cycle by Corollary of Proposition 1.\nSimilarly, since $f \\equiv f_3$ (mod 5), $G$ contains a permutation $\\tau$ of type [$2, a$] or [$2, 1, a$],\nwhere $a$ is odd.\nThen $\\tau^a$ is a transpose.\nHence $G$ contains a transpose.\nHence, $G$ is a symmetric group on $M$ by Lemma 8.\nQED", "meta": {"post_id": 165675, "input_score": 19, "output_score": 38, "post_title": "Constructing a Galois extension field with Galois group $S_n$"}}
{"input": "I have to evaluate:\n$$\\int_{0}^{\\pi/2}\\frac{\\sqrt{\\sin x}}{\\sqrt{\\sin x}+\\sqrt{\\cos x}}\\, \\mathrm{d}x. $$\nI can't get the right answer! So please help me out!", "output": "Let $I$ denote the integral and consider the substitution $u= \\frac{\\pi }{2} - x.$ Then $I = \\displaystyle\\int_0^{\\frac{\\pi }{2}} \\frac{\\sqrt{\\cos u}}{\\sqrt{\\cos u } + \\sqrt{\\sin u }} du$ and $2I = \\displaystyle\\int_0^{\\frac{\\pi }{2}} \\frac{\\sqrt{\\cos u} + \\sqrt{\\sin u }}{\\sqrt{\\cos u } + \\sqrt{\\sin u }} du = \\frac{\\pi }{2}.$ Hence $I = \\frac{\\pi }{4}.$\nIn general, $ \\displaystyle\\int_0^a f(x) dx = \\displaystyle\\int_0^a f(a-x) $ $dx$ whenever $f$ is integrable, and $\\displaystyle\\int_0^{\\frac{\\pi }{2}} \\frac{\\cos^a x}{\\cos^a x + \\sin^a x } dx = \\displaystyle\\int_0^{\\frac{\\pi }{2}} \\frac{\\sin^a x}{\\cos^a x + \\sin^a x } dx = \\frac{\\pi }{4}$ for $a>0$ (same trick.)", "meta": {"post_id": 167409, "input_score": 21, "output_score": 38, "post_title": "Evaluating $\\int_{0}^{\\frac{\\pi}{2}}\\frac{\\sqrt{\\sin x}}{\\sqrt{\\sin x}+\\sqrt{\\cos x}}\\, \\mathrm{d}x$"}}
{"input": "My father and I, on birthday cards, give mathematical equations for each others new age.  This year, my father will be turning $59$.  \nI want to try and make a definite integral that equals $59$.  So far I can only think of ones that are easy to evaluate. I was wondering if anyone had a definite integral (preferably with no elementary antiderivative) that is difficult to evaluate and equals $59$?  Make it as hard as possible, feel free to add whatever you want to it!", "output": "compact : $$\\int_0^\\infty \\frac{(x^4-2)x^2}{\\cosh(x\\frac{\\pi}2)}\\,dx$$", "meta": {"post_id": 168485, "input_score": 130, "output_score": 139, "post_title": "Help find hard integrals that evaluate to $59$?"}}
{"input": "A maximal ideal  is always a prime ideal, and the quotient ring  is always a field. In general, not all prime ideals are maximal. 1\n\nIn $2\\mathbb{Z}$, $4 \\mathbb{Z} $ is a maximal ideal. Nevertheless it is not prime because $2 \\cdot 2 \\in 4\\mathbb{Z}$ but $2 \\notin 4\\mathbb{Z}$. What is that is misunderstand?", "output": "Let $R$ be a ring, not necessarily with identity, not necessarily commutative.\nAn ideal $\\mathfrak{P}$ of $R$ is said to be a prime ideal if and only if $\\mathfrak{P}\\neq R$, and whenever $\\mathfrak{A}$ and $\\mathfrak{B}$ are ideals of $R$, then $\\mathfrak{AB}\\subseteq \\mathfrak{P}$ implies $\\mathfrak{A}\\subseteq \\mathfrak{P}$ or $\\mathfrak{B}\\subseteq \\mathfrak{P}$. \n(The condition given by elements, $ab\\in P$ implies $a\\in P$ or $b\\in P$, is stronger in the case of noncommutative rings, as evidence by the zero ideal in the ring $M_2(F)$, with $F$ a field, but is equivalent to the ideal-wise definition in the case of commutative rings; this condition is called \"strongly prime\" or \"totally prime\". Generally, with noncommutative rings, \"ideal-wise\" versions of multiplicative ideal properties are weaker than \"element-wise\" versions, and the two versions are equivalent in commutative rings).\nWhen the ring does not have an identity, you may not even have maximal ideals. But here is what you can rescue; recall that if $R$ is a ring, then $R^2$ is the ideal of $R$ given by all finite sums of elements of the form $ab$ with $a,b\\in R$ (that is, it is the usual ideal-theoretic product of $R$ with itself, viewed as ideals). When $R$ has an identity, $R^2=R$; but even when $R$ does not have an identity, it is possible for $R^2$ to equal $R$. \nTheorem. Let $R$ be a ring, not necessarily with identity, not necessarily commutative. If $R^2=R$, then every maximal ideal of $R$ is also a prime ideal. If $R^2\\neq R$, then any  ideal that contains $R^2$ is not a prime ideal. In particular,  if $R^2\\neq R$ and there is a maximal ideal containing $R^2$, this ideal is maximal but not prime.\nProof. Suppose that $R^2=R$. Let $\\mathfrak{M}$ be a maximal ideal of $R$; by assumption, we know that $\\mathfrak{M}\\neq R$. Now assume that $\\mathfrak{A},\\mathfrak{B}$ are two ideals such that $\\mathfrak{A}\\not\\subseteq \\mathfrak{M}$ and $\\mathfrak{B}\\not\\subseteq\\mathfrak{M}$. We will prove that $\\mathfrak{AB}$ is not contained in $\\mathfrak{M}$ (we are proving $\\mathfrak{M}$ is prime by contrapositive). Then by the maximality of $\\mathfrak{M}$, it follows that $\\mathfrak{M}+\\mathfrak{A}=\\mathfrak{M}+\\mathfrak{B}=R$.\nThen we have:\n$$\\begin{align*}\nR &= R^2\\\\\n &= (\\mathfrak{M}+\\mathfrak{A})(\\mathfrak{M}+\\mathfrak{B})\\\\ \n&= \\mathfrak{M}^2 + \\mathfrak{AM}+\\mathfrak{MB}+\\mathfrak{AB}\\\\\n&\\subseteq \\mathfrak{M}+\\mathfrak{M}+\\mathfrak{M}+\\mathfrak{AB}\\\\\n&=\\mathfrak{M}+\\mathfrak{AB}\\\\\n&\\subseteq R,\n\\end{align*}$$\nhence $\\mathfrak{M}\\subsetneq\\mathfrak{M}+\\mathfrak{AB}=R$. Therefore, $\\mathfrak{AB}\\not\\subseteq\\mathfrak{M}$. Thus, $\\mathfrak{M}$ is a prime ideal, as claimed.\nNow suppose that $R^2\\neq R$ and $\\mathfrak{I}$ is an ideal of $R$ that contains $R^2$. If $\\mathfrak{I}=R$, then $\\mathfrak{I}$ is not prime. If $\\mathfrak{I}\\neq R$, then $RR\\subseteq \\mathfrak{I}$, but $R\\not\\subseteq \\mathfrak{I}$, so $\\mathfrak{I}$ is not prime. In particular, if $\\mathfrak{M}$ is a maximal ideal containing $R^2$, then $\\mathfrak{M}$ is not prime. $\\Box$\nIn your example, we have $R=2\\mathbb{Z}$, $R^2=4\\mathbb{Z}\\neq R$, so any ideal that contains $R^2$ (in particular, the ideal $R^2$ itself) is not prime. And since $4\\mathbb{Z}$ is a maximal ideal containing $R^2$, exhibiting a maximal ideal that is not prime. (In fact, $2\\mathbb{Z}$ has maximal ideals containing any given ideals; this can be proven directly, or invoking the fact that it is noetherian)", "meta": {"post_id": 169188, "input_score": 28, "output_score": 55, "post_title": "A maximal ideal is always a prime ideal?"}}
{"input": "I'm a mathematics student in abstract algebra and algebraic geometry. Most of my books cover a great deal of category theory and it is an essential tool in understanding these two subjects.\nRecently, I started taking some functional analysis courses and I discovered that there is almost no category theory done in these courses. But since most of the spaces studied in functional analysis are objects in categories (e.g. the normed spaces form a category), I find it rather strange that the books leave the category theory out.\nIs there a reason for this?", "output": "There are two questions here, in reality, I think. \nFirst, in brief, I am told by many people that I \"do functional analysis in the theory of automorphic forms\", and I certainly do find a categorical viewpoint very useful. Second, in brief, it is my impression that the personality-types of many people who'd style themselves \"(functional) analysts\" might be hostile to or disinterested in the worldview of any part of (even \"naive\") category theory.\nIn more detail: as a hugely important example, I think the topology on test functions on $\\mathbb R^n$ is incomprehensible without realizing that it is a (directed) colimit (direct limit). The archetype of incomprehensible/unmotivated \"definition\" (rather than categorical characterization) is in Rudin's (quite admirable in many ways, don't misunderstand me!) \"Functional Analysis\"' definition of that topology. \nThat is, respectfully disagreeing with some other answers, I do not think the specific-ness of concrete function spaces reduces the utility of a (naive-) categorical viewpoint.\nFrom a sociological or psychological viewpoint, which I suspect is often dominant, it is not hard to understand that many people have a distaste for the structuralism of (even \"naive\", in the sense of \"naive set theory\") category theory. And, indeed, enthusiasm does often lead to excess. :)\nI might claim that we are in a historical epoch in which the scandals of late 19th and early 20th century set theory prey on our minds (not to mention the mid-19th century scandals in analysis), while some still react to the arguable excesses of (the otherwise good impulses of) Bourbaki, react to certain exuberances of category theory advocates... and haven't yet reached the reasonable equilibrium that prosaically, calmly, recognizes the utilities of all these things. \nEdit: since this question has resurfaced... in practical terms, as in L. Schwartz' Kernel Theorem in the simplest case of functions on products of circles, the strong topology on duals of Levi-Sobolev spaces is the colimit of Hilbert space topologies on duals (negatively-indexed Levi-Sobolev spaces) of (positively-indexed) Levi-Sobolev spaces. As I have remarked in quite a few other places, it was and is greatly reassuring to me that a \"naive-categorical\" viewpoint immediately shows that there is a unique (up to unique isomorphism) reasonable (!) topology there... \nSimilarly, for pseudo-differential operators, and other \"modern\" ideas, it is very useful to recast their description in \"naive-categorical\" terms, thereby seeing that the perhaps-seeming-whimsy in various \"definitions\" is not at all whimsical, but is inevitable.\nA different example is characterization of \"weak/Gelfand-Pettis\" integrals: only \"in my later years\" have I appreciated the unicity of characterization, as opposed to \"construction\" (as in a Riemann/Bochner integral).", "meta": {"post_id": 169205, "input_score": 110, "output_score": 70, "post_title": "Why don't analysts do category theory?"}}
{"input": "Possible Duplicate:\nIsometries of $\\mathbb{R}^n$ \n\nLet $X$ be a compact metric space and $f$ be an isometric map from $X$ to $X$. Prove $f$ is a surjective map.", "output": "Here is an alternative to the proof linked to in the comments: \nSuppose there existed $x \\in X\\setminus f(X)$. Then $x$ has positive distance $d$ from the compact set $f(X)$. Now consider the recursively defined sequence $$x_0 := x, \\qquad x_n := f(x_{n-1}) \\quad \\forall \\, n>0$$ \nWe have $d(x_0, x_n)\\ge d$ for all $n>0$, by assumption on $x$. This implies that we also have $d(x_k, x_{k+n}) = d(x_0, x_n) \\ge d$ for all $k,n>0$ (here we use that $f$ is an isometry). Therefore $d(x_n, x_m) \\ge d$ for all $m\\ne n$, which is in contradiction to sequential compactness of $X$.", "meta": {"post_id": 170989, "input_score": 21, "output_score": 76, "post_title": "A isometric map in metric space is surjective?"}}
{"input": "Suppose there is a well-known theorem whose usual proof uses Axiom of Choice.\nIs trying to prove it without Axiom of Choice useless?\nWhat merits can such a proof have?", "output": "It is often not useless at all. First let me give out a few reasons why it is useful:\n\nIt gives us a better understanding of how well-behaved some objects are. For example, vector spaces are not well-behaved in general, but in particular cases they are well-behaved (e.g. finitely generated ones). \nOn the other hand, compact metric spaces are quite well-behaved and many of their properties remain valid even without the axiom of choice.\nIt can be the case that a proof without the axiom of choice is a much more constructive one, and allows us to examine the features of an object which was proved only to exist if the axiom of choice were used.\nOne example coming to mind is the compactness of closed and bounded subsets of $\\mathbb R$, another is the uniform continuity of a continuous function from a compact metric space into a metric space.\nIf a proof fails it allows us a better understanding of how much choice is needed for a certain assertion. Dependent Choice for Baire's Category theorem; Countable Choice for the equivalence between different topological properties; etc.\n\nOn the other hand, it is somewhat useless to try and prove a theorem without the axiom of choice if other parts of the theory already assume it. If you already assume that every vector space has a basis, showing that a certain proposition relying on this property holds without the axiom of choice is moot.\nOne example for this is the ultrafilter theorem which, together with the Krein-Milman theorem, imply the axiom of choice [4], so if you end up using both these propositions there is no use in avoiding choice anymore. It's there.\nRegardless of the above, one should remember that not all things in modern mathematics are true in the absence of choice (that is, aside from the axiom of choice itself) and often reformulation and distinction between equivalent definitions is required. In those cases one can, and should, ask themselves how much choice is needed for a particular result.\nFor example, the assertion \"$\\mathbb R$ is not a countable union of countable sets\" requires the axiom of choice, but it is provable from a vastly weaker statement, \"countable unions of countable sets are countable\". \n\nFurther reading:\n\nWhy worry about the axiom of choice? (MathOverflow)\nAxiom of choice - to use or not to use\nAdvantage of accepting the axiom of choice\nIs Banach-Alaoglu equivalent to AC?\nMotivating implications of the axiom of choice?", "meta": {"post_id": 171090, "input_score": 7, "output_score": 38, "post_title": "Is trying to prove a theorem without Axiom of Choice useless?"}}
{"input": "Why is associativity required for groups?\nI'm doing a linear algebra paper and we're focusing on groups at the moment, specifically proving whether something is or is not a group. There are four axioms:\n\nThe set is closed under the operation.\nThe operation is associative.\nThe exists and identity in the group.\nEach element in the group has an inverse which is also in the group.\n\nWhy does the operation need to be associative?\nThanks", "output": "It is not that associativity is required for groups... That is quite backwards: the truth is actually that groups are associative.\nYour question seems to come from the idea that people decided how to define groups and then began to study them and find them interesting. In reality, it happened the other way around: people had studied groups way before actually someone gave a definition. When a definition was agreed upon, people looked at the groups they had at hand and saw that they happened to be associative (and that that was a useful piece of information about them when working with them) so that got included in the definition.\nIf I may say so, it is this which is important to understand. The way we teach abstract algebra nowdays somewhat obscures this fact, but this is how essentially everything comes to be.", "meta": {"post_id": 172694, "input_score": 20, "output_score": 57, "post_title": "Why is associativity required for groups?"}}
{"input": "In school, I have always seen sets. I was watching a video the other day about functors, and they started talking about a set being a collection, but not vice-versa. I also heard people talking about classes. What is their relation? Some background would be nice. \nIt has to do with something called Russell's paradox, but I don't know what that is. \nI think that the difference between a family and a set is that the former is a function and the latter is a set. Is this right?", "output": "The idea behind a \"collection\" is simply a notion of a bunch of mathematical objects which are collected into one big pile. Think of it as a big bin full of trash, diamonds and empty bottles of beer, it doesn't have to make sense what is in this collection, it's just a collection.\nOne of the problem to explain these things to people who are not mathematicians (or trying to \"outsmart a set theorist\", as I ran into several of those) is that the notion of a collection is not fully formal unless you already know what sets and class are, and even then it's not exactly what we mean.\nLet me start over now. Doing mathematics we often have an idea of an object that we wish to represent formally, this is a notion. We then write axioms to describe this notion and try to see if these axioms are self-contradictory. If they are not (or if we couldn't prove that they are) we begin working with them and they become a definition. Mathematicians are guided by the notion but they work with the definition. Rarely the notion and the definition coincide, and you have a mathematical object which is exactly what our [the mathematicians] intuition tells us it should be.\nIn this case, a collection is a notion of something that we can talk about, like a mystery bag. We might know that all the things inside this mystery bag are apples, but we don't know which kind; we might know they are all Granny Smith, but we cannot guarantee that none of them is rotten. A collection is just like that. We can either know something about its elements or we don't, but we know that it has some.\nMathematician began by describing these collections and calling them sets, they did that in a relatively naive way, and they described the axioms in a rather naive way. To the non-mathematician (and to most of the non-set theorists) everything is still a set, and we can always assume that there is a set theorist that assured that for what we need this is true. Indeed, if we just wanted to discuss the real numbers, there is no worry at all we can assume everything we work with is a set.\nThis naive belief can be expressed as every collection is a set. It turned out that some collections cannot be sets, this was expressed via several paradoxes, Cantor's paradox; Russell's paradox; and other paradoxes. The exact meaning is that if we use that particular axiomatic description of \"what is a set\" then we can derive from it contradiction, which is to say that these axioms are inconsistent.\nAfter this happened several people began working on ways to eliminate this problem. One method in common was to limit the way we can generate collections which are sets. This means that you can no longer derive such contradiction within the theory, namely you cannot prove that such collection even exists - or rather you can prove it doesn't.\nThe common set theory nowadays called ZFC (named after Zermelo and Fraenkel, the C denotes the axiom of choice) is relatively close to the naive way from which set theory emerges, and it still allows us to define collections which are not sets, though, for example \"the collection of all sets\". These collections are called classes, or rather proper classes.\nWhat is definable? This is a whole story altogether, but essentially it means that we can describe it with a single formula (perhaps with parameters) of one free variable. \"$x$ is taller than 1.68m\" is an example to such formula, and it defines the class of all people taller than said height.\nSo in ZFC we may define a collection which is not a set, like the collection of all the singletons, or the collection of all sets. These are not sets because they are too big, in some sense, to be sets, but they are classes, proper classes. We can talk about collections which are not definable but that requires a lot more background in logic and set theory to get into.\n\nTo sum up\n\nClasses are collections which can be defined, sets are particular classes which are relatively small and there are classes which are not sets. Collections is a notion which is expressed via both these mathematical objects, but need not be well-defined otherwise.\nOf course when we say defined we mean in the context of a theory, e.g. ZFC. In this sense, sets are things which \"really exist\" whereas classes are collections we can talk about despite their possible nonexistence.\n\n\nOne last thing remains, families. Well, as you remarked families are functions. But functions are sets, so families are sets. We can make a slight adjustment to this, and we can, in fact, talk about class functions, and an index which is not a set but a proper class. We, therefore, can talk about families which are classes.\nGenerally, speaking, if so, a family is a correspondence from one collection into another which uses one collection as indices for elements from another collection.\n\nTo read more\n\nWhat is the difference between a class and a set?\n\nWhy is \"the set of all sets\" a paradox, in layman's terms?", "meta": {"post_id": 172966, "input_score": 108, "output_score": 128, "post_title": "What are the differences between class, set, family, and collection?"}}
{"input": "Suppose we want to define a first-order language to do set theory (so we can formalize mathematics).\nOne such construction can be found here.\nWhat makes me uneasy about this definition is that words such as \"set\", \"countable\", \"function\", and \"number\" are used in somewhat non-trivial manners.\nFor instance, behind the word \"countable\" rests an immense amount of mathematical knowledge: one needs the notion of a bijection, which requires functions and sets.\nOne also needs the set of natural numbers (or something with equal cardinality), in order to say that countable sets have a bijection with the set of natural numbers.\nAlso, in set theory one uses the relation of belonging \"$\\in$\".\nBut relation seems to require the notion an ordered pair, which requires sets, whose properties are described using belonging...\nI found the following in Kevin Klement's, lecture notes on mathematical logic (pages 2-3).\n\"You have to use logic to study logic. There\u2019s no getting away from it.\nHowever, I\u2019m not going to bother stating all the logical rules that are valid in the metalanguage, since I\u2019d need to do that in the metametalanguage, and that would just get me started on an infinite regress.\nThe rule of thumb is: if it\u2019s OK in the object language, it\u2019s OK in the metalanguage too.\"\nSo it seems that, if one proves a fact about the object language, then one can also use it in the metalanguage.\nIn the case of set theory, one may not start out knowing what sets really are, but after one proves some fact about them (e.g., that there are uncountable sets) then one implicitly \"adds\" this fact also to the metalanguage.\nThis seems like cheating: one is using the object language to conduct proofs regarding the metalanguage, when it should strictly be the other way round.\nTo give an example of avoiding circularity, consider the definition of the integers.\nWe can define a binary relation $R\\subseteq(\\mathbf{N}\\times\\mathbf{N})\\times(\\mathbf{N}\\times\\mathbf{N})$, where for any $a,b,c,d\\in\\mathbf{N}$, $((a,b),(c,d))\\in R$ iff $a+d=b+c$, and then defining $\\mathbf{Z}:= \\{[(a,b)]:a,b\\in\\mathbf{N}\\}$, where $[a,b]=\\{x\\in \\mathbf{N}\\times\\mathbf{N}: xR(a,b)\\}$, as in this question or here on Wikipedia. In this definition if set theory and natural numbers are assumed, then there is no circularity because one did not depend on the notion of \"subtraction\" in defining the integers.\nSo my question is:\n\nQuestion Is the definition of first-order logic circular?\n  If not, please explain why.\n  If the definitions are circular, is there an alternative definition which avoids the circularity?\n\nSome thoughts:\n\nPerhaps there is the distinction between what sets are (anything that obeys the axioms) and how sets are expressed (using a formal language).\nIn other words, the notion of a set may not be circular, but to talk of sets using a formal language requires the notion of a set in a metalanguage.\nIn foundational mathematics there also seems to be the idea of first defining something, and then coming back with better machinery to analyse that thing.\nFor instance, one can define the natural numbers using the Peano axioms, then later come back to say that all structures satisfying the axioms are isomorphic. (I don't know any algebra, but that seems right.)\nMaybe sets, functions, etc., are too basic? Is it possible to avoid these terms when defining a formal language?", "output": "I think an important answer is still not present so I am going to type it.  This is somewhat standard knowledge in the field of foundations but is not always adequately described in lower level texts.\nWhen we formalize the syntax of formal systems, we often talk about the set of formulas. But this is just a way of speaking; there is no ontological commitment to \"sets\" as in ZFC. What is really going on is an \"inductive definition\". To understand this you have to temporarily forget about ZFC and just think about strings that are written on paper. \nThe inductive definition of a \"propositional formula\" might say that the set of formulas is the smallest class of strings such that:\n\nEvery variable letter is a formula (presumably we have already defined a set of variable letters). \nIf $A$ is a formula, so is $\\lnot (A)$. Note: this is a string with 3 more symbols than $A$. \nIf $A$ and $B$ are formulas, so is $(A \\land B)$. Note this adds 3 more symbols to the ones in $A$ and $B$. \n\nThis definition can certainly be read as a definition in ZFC. But it can also be read in a different way. The definition can be used to generate a completely effective procedure that a human can carry out to tell whether an arbitrary string is a formula (a proof along these lines, which constructs a parsing procedure and proves its validity, is in Enderton's logic textbook). \nIn this way, we can understand inductive definitions in a completely effective way without any recourse to set theory. When someone says \"Let $A$ be a formula\" they mean to consider the situation in which I have in front of me a string written on a piece of paper, which my parsing algorithm says is a correct formula. I can perform that algorithm without any knowledge of \"sets\" or ZFC.\nAnother important example is \"formal proofs\". Again, I can treat these simply as strings to be manipulated, and I have a parsing algorithm that can tell whether a given string is a formal proof.   The various syntactic metatheorems of first-order logic are also effective. For example the deduction theorem gives a direct algorithm to convert one sort of proof into another sort of proof. The algorithmic nature of these metatheorems is not always emphasized in lower-level texts - but for example it is very important in contexts like automated theorem proving. \nSo if you examine a logic textbook, you will see that all the syntactic aspects of basic first order logic are given by inductive definitions, and the algorithms given to manipulate them are completely effective. Authors usually do not dwell on this, both because it is completely standard and because they do not want to overwhelm the reader at first. So the convention is to write definitions \"as if\" they are definitions in set theory, and allow the readers who know what's going on to read the definitions as formal inductive definitions instead. When read as inductive definitions, these definitions would make sense even to the fringe of mathematicians who don't think that any infinite sets exist but who are willing to study algorithms that manipulate individual finite strings. \nHere are two more examples of the syntactic algorithms implicit in certain theorems:  \n\nG\u00f6del's incompleteness theorem actually gives an effective algorithm that can convert any PA-proof of Con(PA) into a PA-proof of $0=1$. So, under the assumption there is no proof of the latter kind, there is no proof of the former kind. \nThe method of forcing in ZFC actually gives an effective algorithm that can turn any proof of $0=1$ from the assumptions of ZFC and the continuum hypothesis into a proof of $0=1$ from ZFC alone.  Again, this gives a relative consistency result. \n\nResults like the previous two bullets are often called \"finitary relative consistency proofs\". Here \"finitary\" should be read to mean \"providing an effective algorithm to manipulate strings of symbols\".  \nThis viewpoint helps explain where weak theories of arithmetic such as PRA enter into the study of foundations. Suppose we want to ask \"what axioms are required to prove that the algorithms we have constructed will do what they are supposed to do?\". It turns out that very weak theories of arithmetic are able to prove that these symbolic manipulations work correctly. PRA is a particular theory of arithmetic that is on one hand very weak (from the point of view of stronger theories like PA or ZFC) but at the same time is able to prove that (formalized versions of) the syntactic algorithms work correctly, and which is often used for this purpose.", "meta": {"post_id": 173735, "input_score": 60, "output_score": 45, "post_title": "How to avoid perceived circularity when defining a formal language?"}}
{"input": "I just wonder if my following solution is true.\nLet $X,Y$ be sets, let $f:X\\to Y$ be a function, let $\\{Y_i\\}_{i\\in I}$ be a family of subsets of $Y$. (Note: I use equalities instead of mutual containment)\n$$\\begin{align}f^{-1}\\left[\\bigcup_{i\\in I} Y_i\\right]\n &= \\{x \\in X: \\mbox{there exists an}\\quad i \\in I\\mbox{ such that } y \\in Y_i,f(x)=y\\}\n \\\\&=\\bigcup_{i \\in  I} f^{-1}\\left[Y_i\\right]\n\\end{align}$$\nI initially do not know how to get from the left to right, but when I put both sets in set notation, they turn out to be the same, hence the one line proof. Something go ultimately wrong?", "output": "The statement is true, and your argument is essentially right, but I would say that you are skipping steps to achieve that identification.  (Also, you should not have those $\\infty$'s on top of the union symbol). I would instead add:\n$$\\begin{align*}\nf^{-1}\\left[\\bigcup_{i\\in I}Y_i\\right] &= \\left\\{ x\\in X\\;\\left|\\; f(x)\\in\\bigcup_{i\\in I}Y_i\\right\\}\\right.\\\\\n&=\\Biggl\\{x\\in X\\;\\Biggm|\\; \\exists i\\in I\\text{ such that }f(x)\\in Y_i\\Biggr\\}\\\\\n&= \\bigcup_{i\\in I}\\{x\\in X\\mid f(x)\\in Y_i\\}\\\\\n&= \\bigcup_{i\\in I}f^{-1}[Y_i].\n\\end{align*}$$\nThe first equality is by definition of inverse image; the second by definition of the union; the third is by definition of union; and the fourth by definition of inverse image.", "meta": {"post_id": 174401, "input_score": 21, "output_score": 36, "post_title": "Inverse image of a union equals the union of the inverse images"}}
{"input": "Let $\\mathbb{Z}[X]$ be the ring of polynomials in one variable over $\\Bbb Z$.\n\nMy question: Is every prime ideal of $\\mathbb{Z}[X]$ one of following types?\nIf yes, how would you prove this?\n\n$(0)$.\n\n$(f(X))$, where $f(X)$ is an irreducible polynomial.\n\n$(p)$, where $p$ is a prime number.\n\n$(p, f(X))$, where $p$ is a prime number and $f(X)$ is an irreducible polynomial modulo $p$.", "output": "Let $\\mathfrak{P}$ be a prime ideal of $\\mathbb{Z}[x]$. Then $\\mathfrak{P}\\cap\\mathbb{Z}$ is a prime ideal of $\\mathbb{Z}$: this holds whenever $R\\subseteq S$ are commutative rings. Indeed, if $a,b\\in R$, $ab\\in R\\cap P$, then $a\\in P$ or $b\\in P$ (since $P$ is prime). (More generally, the contraction of a prime ideal is always a prime ideal, and $\\mathfrak{P}\\cap\\mathbb{Z}$ is the contraction of $\\mathfrak{P}$ along the embedding $\\mathbb{Z}\\hookrightarrow\\mathbb{Z}[x]$).\nThus, we have two possibilities: $\\mathfrak{P}\\cap\\mathbb{Z}=(0)$, or $\\mathfrak{P}\\cap\\mathbb{Z}=(p)$ for some prime integer $p$.\nCase 1. $\\mathfrak{P}\\cap\\mathbb{Z}=(0)$. If $\\mathfrak{P}=(0)$, we are done; otherwise, let $S=\\mathbb{Z}-\\{0\\}$. Then $S\\cap \\mathfrak{P}=\\varnothing$, $S$ is a multiplicative set, so we can localize $\\mathbb{Z}[x]$ at $S$ to obtain $\\mathbb{Q}[x]$; the ideal $S^{-1}\\mathfrak{P}$ is prime in $\\mathbb{Q}[x]$, and so is of the form $(q(x))$ for some irreducible polynomial $q(x)$. Clearing denominators and factoring out content we may assume that $q(x)$ has integer coefficients and the gcd of the coefficients is $1$. \nI claim that $\\mathfrak{P}=(q(x))$. Indeed, from the theory of localizations, we know that $\\mathfrak{P}$ consists precisely of the elements of $\\mathbb{Z}[x]$ which, when considered to be elements of $\\mathbb{Q}[x]$, lie in $S^{-1}\\mathfrak{P}$. That is, $\\mathfrak{P}$ consists precisely of the rational multiples of $q(x)$ that have integer coefficients. In particular, every integer multiple of $q(x)$ lies in $\\mathfrak{P}$, so $(q(x))\\subseteq \\mathfrak{P}$. But, moreover, if $f(x)=\\frac{r}{s}q(x)\\in\\mathbb{Z}[x]$, then $s$ divides all coefficients of $q(x)$; since $q(x)$ is primitive, it follows that $s\\in\\{1,-1\\}$, so $f(x)$ is actually an integer multiple of $q(x)$. Thus, $\\mathfrak{P}\\subseteq (q(x))$, proving equality.\nThus, if $\\mathfrak{P}\\cap\\mathbb{Z}=(0)$, then either $\\mathfrak{P}=(0)$, or $\\mathfrak{P}=(q(x))$ where $q(x)\\in\\mathbb{Z}[x]$ is irreducible.\nCase 2. $\\mathfrak{P}\\cap\\mathbb{Z}=(p)$.\nWe can then consider the image of $\\mathfrak{P}$ in $\\mathbb{Z}[x]/(p)\\cong\\mathbb{F}_p[x]$. The image is prime, since the map is onto; the prime ideals of $\\mathbb{F}_p[x]$ are $(0)$ and ideals of the form $(q(x))$ with $q(x)$ monic irreducible over $\\mathbb{F}_p[x]$. If the image is $(0)$, then $\\mathfrak{P}=(p)$, and we are done. \nOtherwise, let $p(x)$ be a polynomial in $\\mathbb{Z}[x]$ that reduces to $q(x)$ modulo $p$ and that is monic. Note that $p(x)$ must be irreducible in $\\mathbb{Z}[x]$, since any nontrivial factorization in $\\mathbb{Z}[x]$ would induce a nontrivial factorization in $\\mathbb{F}_p[x]$ (since $p(x)$ and $q(x)$ are both monic). \nI claim that $\\mathfrak{P}=(p,p(x))$. Indeed, the isomorphism theorems guarantee that $(p,p(x))\\subseteq \\mathfrak{P}$. Conversely, let $r(x)\\in\\mathfrak{P}(x)$. Then there exists a polynomial $s(x)\\in\\mathbb{F}_p[x]$ such that $s(x)q(x) = \\overline{r}(x)$. If $t(x)$ is any polynomial that reduces to $s(x)$ modulo $p$, then $t(x)p(x)-r(x)\\in (p)$, hence there exists a polynomial $u(x)\\in\\mathbb{Z}[x]$ such that $r(x) = t(x)p(x)+pu(x)$. Therefore, $r(x)\\in (p,p(x))$, hence $\\mathfrak{P}\\subseteq (p,p(x))$, giving equality.\nThus, if $\\mathfrak{P}\\cap\\mathbb{Z}[x]=(p)$ with $p$ a prime, then either $\\mathfrak{P}=(p)$ or $\\mathfrak{P}=(p,p(x))$ with $p(x)\\in\\mathbb{Z}[x]$ irreducible.\nThis proves the desired classification.", "meta": {"post_id": 174595, "input_score": 152, "output_score": 133, "post_title": "Classification of prime ideals of $\\mathbb{Z}[X]$"}}
{"input": "After just having learned about $p$-adic numbers I've now got another question which I can't figure out from the Wikipedia page.\nAs far as I understand, the $p$-adic numbers are basically completing the rational numbers in the same way the real numbers do, except with a different notion of distance where differences in more-significant digits correspond to small distances, instead of differences in less-significant digits. So if I understand correctly, the $p$-adic numbers contain the rational numbers, but not the irrational numbers, while the non-rational $p$-adic numbers are not in $\\mathbb{R}$ (someone please correct me if I'm wrong).\nNow the real numbers do not depend on the base you write the numbers in. However the construction of the $p$-adic numbers seems to depend on the $p$ chosen. On the other hand I am sure that the construction of the real numbers can be written in a way that it apparently depends on the base, so the appearance might be misleading.\nTherefore my question: Are the $p$-adic numbers the same for each $p$ (that is, are e.g. $2$-adic and $3$-adic numbers the same numbers, only written in different bases), or are they different (except for the rational numbers, of course). For example, take the $2$-adic number $x := ...1000001000010001001011$ (i.e. $\\sum_{n=0}^\\infty 2^{n(n+1)/2}$), which IIUC isn't rational (because it's not periodic). Can $x$ also be written as $3$-adic number, or is there no $3$-adic number corresponding to this series?\nIn case they are different, is there some larger field which contains all $p$-adic numbers for arbitrary $p$?", "output": "No, the different $p$-adic number systems are not in any way compatible with one another.\nA $p$-adic number is a not a number that is $p$-adic; it is a $p$-adic number.  Similarly, a real number is not a number that is real, it is a real number.  There is not some unified notion of \"number\" that these are all subsets of; they are entirely separate things, though there may be ways of identifying bits of them in some cases (e.g., all of them contain a copy of the rational numbers).\nNow, someone here is bound to point out that if we take the algebraic closure of some $\\mathbb{Q}_p$, the result will be algebraically isomorphic to $\\mathbb{C}$.  But when we talk about $p$-adic numbers we are not just talking about their algebra, but also their absolute value, or at least their topology; and once you account for this they are truly different.  (And even if you just want algebraic isomorphism, this requires the axiom of choice; you can't actually identify a specific isomorphism, and there's certainly not any natural way to do so.)\nHow can we see that they are truly different?  Well, first let's look at the algebra.  The $5$-adics, for instance, contain a square root of $-1$, while the $3$-adics do not.  So if you write down a $5$-adic number which squares to $-1$, there cannot be any corresponding $3$-adic number.\nBut above I claimed something stronger -- that once you account for the topology, there is no way to piece the various $p$-adic number systems together, which the above does not rule out.  How can we see this?  Well, let's look at the topology when we look at the rational numbers, the various $p$-adic topologies on $\\mathbb{Q}$.  These topologies are not only distinct -- any finite set of them is independent, meaning that if we let $\\mathbb{Q}_i$ be $\\mathbb{Q}$ with the $i$'th topology we're considering, then the diagonal is dense in $\\mathbb{Q}_1 \\times \\ldots \\times \\mathbb{Q}_n$.\nPut another way -- since these topologies all come from metrics -- this means that for any $c_1,\\ldots,c_n\\in\\mathbb{Q}$, there exists a sequence of rational numbers $a_1,a_2,\\ldots$ such that in topology number 1, this converges to $c_1$, but in topology number two, it converges to $c_2$, and so forth.  (In fact, more generally, given any finite set of inequivalent absolute values on a field, the resulting topologies will be independent.)\nSo even on $\\mathbb{Q}$, the different topologies utterly fail to match up, so there is no way they can be pieced together by passing to some larger setting.", "meta": {"post_id": 174818, "input_score": 25, "output_score": 44, "post_title": "Are all $p$-adic number systems the same?"}}
{"input": "$$\\begin{bmatrix} 1235 &2344 &1234 &1990\\\\\n2124 & 4123& 1990& 3026 \\\\\n1230 &1234 &9095 &1230\\\\\n1262 &2312& 2324 &3907 \n\\end{bmatrix}$$\nClearly, its determinant is not zero and, hence, the matrix is invertible. \nIs there a more elegant way to do this?\nIs there a pattern among these entries?", "output": "Find the determinant.  To make calculations easier, work modulo $2$! The diagonal is $1$'s, the rest are $0$'s.  The determinant is odd, and therefore non-zero.", "meta": {"post_id": 175561, "input_score": 154, "output_score": 513, "post_title": "Is the following matrix invertible?"}}
{"input": "I'm having trouble verifying why the following is correct.\n$$p(x, y \\mid z)= p(x \\mid y, z) p(y \\mid z)$$\nI tried grouping the $(x, y)$ together and split by the conditional, which gives me\n$$p(x, y \\mid z) = p(z\\mid x, y) p(x, y)/p(z)$$\nHowever, this did not bring me any closer. I'm uncertain about what kind of manipulations are allowed given more than 2 variables.\nSay an expression like:\n$$p(a, b, c)$$\nThen I know from the chain rule that I can break it down to:\n$$p(a, b, c)=p(a \\mid b, c) p(b, c) = p(a \\mid b, c) p(b \\mid c) p(c)$$\nIs it allowed to split by the second comma:\n$$p(a, b, c) = p(a, b \\mid c) p(c) ?$$\nAnd even more complicated and expression like:\n$$p(a|b,c)$$\nAm I allowed to rewrite this expression by grouping (a|b) together to give me something like\n$$p(a|b,c)=p((a|b)|c)p(c)$$\nAnd does this expression even make sense?", "output": "$\\Pr(a,b,c)=\\Pr(a,b\\mid c)\\Pr(c)$ is allowed.  \nYou are simply saying $\\Pr(d,c)=\\Pr(d\\mid c)\\Pr(c)$ where $d = a \\cap b$.\nCombine this with $\\Pr(a,b,c)=\\Pr(a\\mid b,c)\\Pr(b,c)=\\Pr(a\\mid b,c)\\Pr(b\\mid c)\\Pr(c)$ and divide through by nonzero $\\Pr(c)$ to get $\\Pr(a,b\\mid c)=\\Pr(a\\mid b,c)\\Pr(b\\mid c)$.", "meta": {"post_id": 176301, "input_score": 47, "output_score": 46, "post_title": "Conditional and joint probability manipulations when there are 3 variables"}}
{"input": "I'm studying for my exam of linear algebra.. I want to prove the following corollary:\n\nIf $A$ is a symmetric positive definite matrix then each entry $a_{ii}> 0$, ie all the elements of the diagonal of the matrix are positive.\n\nMy teacher gave a suggestion to consider the unit vector \"$e_i$\", but I see that is using it. \n$a_{ii} >0$ for each $i = 1, 2, \\ldots, n$. For any $i$, define $x = (x_j)$ by $x_i =1$ and by $x_j =0$, if $j\\neq i$, since $x \\neq 0$, then:\n$0< x^TAx = a_{ii}$\nBut my teacher says my proof is ambiguous. How I can use the unit vector $e_1$ for the demonstration?", "output": "Let $e_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}$, and so on, where $e_i$ is a vector of all zeros, except for a $1$ in the $i^{\\mathrm{th}}$ place. Since $A$ is positive definite, then $x^T A x > 0$ for any non-zero vector $x \\in \\Bbb R^n$. Then, $e_1^T A e_1 > 0$, and likewise for $e_2, e_3$ and so on.\nIf the $i^{\\mathrm{th}}$ diagonal entry of $A$ was not positive, $a_{ii} < 0$, then $e_i^T A e_i = 0\\cdot a_{11}\\cdot 0 + 1\\cdot a_{12}\\cdot 0 + \\cdots + 1\\cdot a_{ii}\\cdot 1 + \\cdots + 0\\cdot a_{nn} \\cdot 0$, since $e_i$ has zeros everywhere but in the $i^{\\rm th}$ spot.\nThus, what would happen if $a_{ii}$ was positive?", "meta": {"post_id": 180173, "input_score": 36, "output_score": 40, "post_title": "Prove: symmetric positive definite matrix"}}
{"input": "When learning mathematics I tend to try to reduce all the concepts I come across to some matter of interaction between sets and functions (or if necessary the more general Relation) on them. Possibly with some extra axioms thrown in here and there if needed, but the fundamental idea is that of adding additional structure on sets and relations between them.\nI've recently tried applying this view to calculus and have been running into some confusions. Most importantly I'm not sure how to interpret Limits. I've considered viewing them as a function that takes 3 arguments, a function, the function's domain and some value (the \"approaches value\") then outputs a single value.\nHowever this \"limit function\" view requires defining the limit function over something other then the Reals or Complexes due to the notion of certain inputs and outputs being \"infinity\". This makes me uncomfortable and question whether my current approach to mathematics is really as elegant as I'd thought. Is this a reasonable approach to answering the question of what limits actually \"are\" in a general mathematical sense? How do mathematicians tend to categorize limits with the rest of mathematics?", "output": "Do you by any chance have a computer science background? Your ideal of reducing everything (even operations like limits) to function and sets has a flavor of wanting mathematics to work more or less like a programming language -- this is a flavor that I (being a computer scientist) quite approve of, but you should be aware that the ideal is not quite aligned with how real mathematicians write mathematics.\nFirst, even though everything can be reduced to sets and functions -- indeed, everything can be reduced to sets alone, with functions just being sets of a particular shape -- doing so is not necessarily a good way to think about everything all of the time. Reducing everything to set theory is the \"assembly language\" of mathematics, and while it will certainly make you a better mathematician to know how this reduction works, it is not the level of abstraction you'll want to do most of your daily work at.\nIn contrast to the \"untyped\" assembly-level set theory, the day-to-day symbol language of mathematics is a highly typed language. The \"types\" are mostly left implicit in writing (which can be frustrating for students whose temperament lean more towards the explicit typing of most typed computer languages), but they are supremely important in practice -- almost every notation in mathematics has dozens or hundreds of different meanings, between which the reader must choose based on what the types of its various sub-expressions are. (Think \"rampant use of overloading\" from a programming-language perspective). Mostly, we're all trained to do this disambiguation unconsciously.\nIn most cases, of course, the various meanings of a symbol are generalizations of each other to various degrees. This makes it a particular bad idea to train oneself to think of the symbol of denoting this or that particular function with such-and-such particular arguments and result. A fuzzier understanding of the intention behind the symbol will often make it easier to guess which definition it's being used with in a new setting, which makes learning new material easier (even though actual proofwork of course needs to be based on exact, explicit definitions).\nIn particular, even restricting our attention to real analysis, the various kinds of limits (for $x\\to a$, $x\\to \\infty$, one-sided limits and so forth) are all notated with the same $\\lim$ symbols, but they are technically different things. Viewing $\\lim_{x\\to 5}f(x)$ and $\\lim_{x\\to\\infty} f(x)$ as instances of the same joint \"limit\" function is technically possible, but also clumsy and (more importantly) not even particularly enlightening. It is better to think of the various limits as a loose grouping of intuitively similar but technically separate concepts.\nThis is not to say that there's not interesting mathematics to be made from studying ways in which the intuitive similarity between the different kind of limits can be formalized, producing some general notion of limit that has the ordinary limits as special cases. (One solution here is to say that the \"$x\\to \\cdots$\" subscript names a variable to bind while also denoting a net to take the limit over). All I'm saying is that such a general super-limit concept is not something one ought to think of when doing ordinary real analysis.\nFinally (not related to your question about limits), note that the usual mathematical language makes extensive use of abstract types. The reals themselves are a good example: it is possible to give an explicit construction of the real numbers in terms of sets and functions (and every student of mathematics deserves to know how), but in actual mathematical reasoning numbers such as $\\pi$ or $2.6$ are not sets or functions, but a separate sort of things that can only be used in the ways explicitly allowed for real numbers. \"Under the hood\" one might consider $\\pi$ to \"really be\" a certain set of functions between various other sets, but that is an implementation detail that is relevant only at the untyped set-theory level.\n(Of course, the various similarities between math and programming languages I go on about here are not coincidences. They arose from programming-language design as deliberate attempts to create formal machine-readable notations that would \"look and feel\" as much like ordinary mathematical symbolism as they could be made to. Mathematics had all of these things first; computer science was just first to need to name them).", "meta": {"post_id": 180392, "input_score": 28, "output_score": 43, "post_title": "What kind of \"mathematical object\" are limits?"}}
{"input": "For solving differential equations, especially the ones of the form\n$$g(x)dx = h(y)dy$$\nwe solve the equation by integrating on both sides to reveal the solution.\nUnderstanding this for differentiating the equation on both sides is relatively easy. We know that we can formulate an alternative equation in terms of differentials for the original equation involved and come out with a new differential equation that holds because of the properties of the differentials.\nBut how does it work for integration on both sides? Am I missing any point here? I have referred to multiple books but none give a satisfactory explanation. Integrating an equation on both sides seems really wrong, if I may dare to use the word.\nPlease help. I'm stuck with this thing and I can only begin to understand differential equations once this is cleared from my head.\nThank you very much!", "output": "The original equation was presumably \n$$h(y)\\frac{dy}{dx}=g(x),$$\nor something equivalent to this.\nYou are given the mysterious rule about \"splitting\" $\\frac{dy}{dx}$. You probably were told at one time that $\\frac{dy}{dx}$ is not a fraction, and now all of a sudden we are treating it as a fraction!\nSo let us not split it. Suppose that $H(y)$ is an antiderivative of $h(y)$, that is, a function whose derivative wiith respect to $y$ is $h(y)$. Let $G(x)$ be a function whose derivative with respect to $x$ is $g(x)$. \nWe recognize $h(y)\\frac{dy}{dx}$ as the derivative with respect to $x$ of $H(y)$ (Chain Rule).  So our equation can be written as\n$$\\frac{d}{dx} H(y)=\\frac{d}{dx}G(x).$$\nThus $H(y)$ and $G(x)$ have the same derivative with respect to $x$. So they differ by a constant, and we find\n$$H(y)=G(x)+C.$$\nNow the important part: this is exactly what we get when we \"split\" $\\frac{dy}{dx}$ and integrate on both sides. So whether or not the splitting and integrating makes sense, it gives the right answer. \nIf you wish, splitting and integrating can be treated as a senseless mnemonic that works, a \"shortcut\" to the real calculation using the Chain Rule. In fact, the individual terms $dy$ and $dx$ can be given meaning, but it is a little complicated. And Applied (and less Applied) people have an essentially correct intuition based on adding up \"infinitely small\" quantities. Unfortunately, it takes considerable effort to make that intuition rigorous.", "meta": {"post_id": 182344, "input_score": 34, "output_score": 40, "post_title": "Can anyone explain the intuitive meaning of 'integrating on both sides of the equation' when solving differential equations?"}}
{"input": "This is from my homework, I'm totally lost as to how to proceed.\nConsider the operator $T: L^2([0,1]) \\rightarrow L^2([0,1])$ defined by \n$(Tf)(x) = \\int^x_0 f(s) \\ ds$\nWhat is the adjoint of $T$?\nThis operator doesn't seem to be an orthogonal projection, nor is it self-adjoint. How does one find the adjoint of an operator in general? Thanks in advance!", "output": "Using the fact that \n\n$$ \\langle Tf , g \\rangle=\\langle f , T^{*}g \\rangle, $$ \n\nwe have\n$$ \\langle Tf, g\\rangle = \\int_{0}^{1} (Tf)(t)g(t)\\,dt =\\int_{0}^{1} \\int_{0}^{t} f(\\tau)\\,d \\tau\\, g(t)\\, dt = \\int_{0}^{1} f(\\tau)\\, \\left(\\int_{\\tau}^{1} g(t) \\,dt\\right)\\, d \\tau $$ $$ = \\langle f, T^{*}g\\rangle $$\nFrom the last integral, we can see that the adjoint is given by  \n\n$$ (T^{*}f) (x) = \\int_{x}^{1} f(s)\\, ds $$", "meta": {"post_id": 182598, "input_score": 41, "output_score": 37, "post_title": "Finding the adjoint of an operator"}}
{"input": "Possible Duplicate:\nHow do I come up with a function to count a pyramid of apples?\nProof that $\\sum\\limits_{k=1}^nk^2 = \\frac{n(n+1)(2n+1)}{6}$?\nFinite Sum of Power? \n\nI know that the sum of the squares of the first n natural numbers is $\\frac{n(n + 1)(2n + 1)}{6}$. I know how to prove it inductively. But how, presuming I have no idea about this formula, should I determine it? The sequence $a(n)=1^2+2^2+...+n^2$ is neither geometric nor arithmetic. The difference between the consecutive terms is 4, 9, 16 and so on, which doesn't help. Could someone please help me and explain how should I get to the well known formula assuming I didn't know it and was on some desert island?", "output": "This is proven, for example, in Stewart's Calculus:\nConsider the following sum:\n$$\\sum_{i=1}^n((1+i)^3-i^3).$$\nFirst, looking at it as a telescoping sum, you will get\n$$\\sum_{i=1}^n((1+i)^3-i^3)=(1+n)^3-1.$$\nOn the other hand, you also have\n$$\\sum_{i=1}^n((1+i)^3-i^3)=\\sum_{i=1}^n(3i^2+3i+1)=3\\sum_{i=1}^ni^2+3\\sum_{i=1}^ni+n.$$\nUsing these two expressions, and the fact that $\\sum_{i=1}^ni=\\frac{n(n+1)}{2}$, you can now solve for $\\sum_{i=1}^ni^2$.", "meta": {"post_id": 183316, "input_score": 39, "output_score": 38, "post_title": "How to get to the formula for the sum of squares of first n numbers?"}}
{"input": "Could any one give an example of a bijective map from $\\mathbb{R}^3\\rightarrow \\mathbb{R}$?\nThank you.", "output": "First, note that it is enough to find a bijection $f:\\Bbb R^2\\to \\Bbb R$, since then $g(x,y,z) = f(f(x,y),z)$ is automatically a bijection from $\\Bbb R^3$ to $\\Bbb R$.\nNext, note that since there is a bijection from $[0,1]\\to\\Bbb R$ (see appendix), it is enough to find a bijection from the unit square $[0,1]^2$ to the unit interval $[0,1]$. By constructions in the appendix, it does not really matter whether we consider $[0,1]$, $(0,1]$, or $(0,1)$, since there are easy bijections between all of these.\nMapping the unit square to the unit interval\nThere are a number of ways to proceed in finding a bijection from the unit square to the unit interval.  Usully one starts with the \"interleaving\" technique I mentioned in the comments, mapping $$\\langle 0.a_1a_2a_3\\ldots,\\\\\\qquad 0.b_1b_2b_3\\ldots\\rangle$$ to $$0.a_1b_1a_2b_2a_3b_3\\ldots.$$  To see why this doesn't quite work, consider the numbers $\\frac 12=0.50000\\ldots$ and $\\frac9{22} = 0.40909\\ldots$.  Both of these are the image of the pair $\\langle\\frac12, 0\\rangle$, so we have not described a bijection.\nThis problem can be fixed.\n(In answering this question, I tried many web searches to try to remember the fix, and I was amazed at how many sources I found that ignored the problem, either entirely, or by handwaving. I never did find it; I had to remember it. Sadly, I cannot remember where I saw it first.)\nFirst, we will deal with $(0,1]$ rather than with $[0,1]$; bijections between these two sets are well-known, or see the appendix. For real numbers with two decimal expansions, such as $\\frac12$, we will agree to use only the expansion that ends with nines rather than the one that ends with zeroes. So for example we represent $\\frac12$ as $0.4999\\ldots$.\nNow instead of interleaving single digits, we will break each input number into chunks, where each chunk consists of some number of zeroes (possibly none) followed by a single non-zero digit.  For example, $\\frac1{200} = 0.00499\\ldots$ is broken up as $$004\\ 9\\ 9\\ 9\\ldots,$$ and $0.01003430901111\\ldots$ is broken up as $$01\\ 003\\ 4\\ 3\\ 09\\ 01\\ 1\\ 1\\ldots.$$\nThis chunking process is well-defined since we are ignoring representations that contain infinite sequences of zeroes.\nNow instead of interleaving digits, we interleave chunks.  To interleave $0.004999\\ldots$ and $0.01003430901111\\ldots$, we get $$0.004\\ 01\\ 9\\ 003\\ 9\\ 4\\ 9\\ldots.$$  This is obviously reversible. It can never produce a result that ends with an infinite sequence of zeroes, and similarly the reverse mapping can never produce a number with an infinite sequence of trailing zeroes, so we win.  A problem example similar to the one from a few paragraphs ago is resolved as follows: $\\frac12 = 0.4999\\ldots$ is the unique image of $\\langle 0.4999\\ldots, 0.999\\ldots\\rangle$ and $\\frac9{22} = 0.40909\\ldots$ is the unique image of $\\langle 0.40909\\ldots, 0.0909\\ldots\\rangle$.\nThis is enough to answer the question posted, but I will give some alternative approaches.\nContinued fractions\nAccording to the paper \"Was Cantor Surprised?\" by Fernando Q. Gouve\u00e2, Cantor originally tried interleaving the digits himself, but Dedekind pointed out the problem of nonunique decimal representations. Cantor then switched to an argument like the one Robert Israel gave in his answer, based on continued fraction representations of irrational numbers.  He first constructed a bijection from $(0,1)$ to its irrational subset (see this question for the mapping Cantor used and other mappings that work), and then from pairs of irrational numbers to a single irrational number by interleaving the terms of the infinite continued fractions.  Since Cantor dealt with numbers in $(0,1)$, he could guarantee that every irrational number had an infinite continued fraction representation of the form $$x = x_0 + \\dfrac{1}{x_1 + \\dfrac{1}{x_2 + \\ldots}}$$\nwhere $x_0$ was zero, avoiding the special-case handling for $x_0$ in Robert Israel's solution.\nCantor-Schr\u00f6der-Bernstein mappings\nThe Cantor-Schr\u00f6der-Bernstein theorem takes an injection $f:A\\to B$ and an injection $g:B\\to A$, and constructs a bijection between $A$ and $B$.\nSo if we can find an injection $f:[0,1)^2\\to[0,1)$ and an injection $g:[0,1)\\to[0,1)^2$, we can invoke the CSB theorem and we will be done.\n$g$ is quite trivial;  $x\\mapsto \\langle x, 0\\rangle$ is one of many obvious injections.\nFor $f$ we can use the interleaving-digits trick again, and we don't have to be so careful because we need only an injection, not a  bijection. We can choose the representation of the input numbers arbitrarily; say we will take the $0.5000\\ldots$ representation rather than the $0.4999\\ldots$ representation. Then we interleave the digits of the two input numbers.  There is no way for the result to end with an infinite sequence of nines, so we are guaranteed an injection.\nThen we apply CSB to $f$ and $g$ and we are done.\nAppendix\n\nThere is a bijection from $(-\\infty, \\infty)$ to $(0, \\infty)$.  The map $x\\mapsto e^x$ is an example.\n\nThere is a bijection from $(0, \\infty)$ to $(0, 1)$.  The map $x\\mapsto \\frac2\\pi\\tan^{-1} x$ is an example, as is $x\\mapsto{x\\over x+1}$.\n\nThere is a bijection from $[0,1]$ to $(0,1]$.  Have $0\\mapsto \\frac12, \\frac12\\mapsto\\frac23,\\frac23\\mapsto\\frac34,$ and so on. That takes care of $\\left\\{0, \\frac12, \\frac23, \\frac34,\\ldots\\right\\}$.  For any other $x$, just map $x\\mapsto x$.\n\nSimilarly, there is a bijection from $(0,1]$ to $(0,1)$.", "meta": {"post_id": 183361, "input_score": 157, "output_score": 250, "post_title": "Examples of bijective map from $\\mathbb{R}^3\\rightarrow \\mathbb{R}$"}}
{"input": "I am reading a collection of problems by the Russian mathematician Vladimir Arnol'd, titled A Mathematical Trivium. I am taking a stab at this one:\n\nCalculate the $100$th derivative of the function $$\\frac{x^2 + 1}{x^3 - x}.$$\n\nThe derivative is non-trivial (in the sense that I computed it for a few rounds, and it only became more assertive). My first thought was to let\n$$f(x) = x^2 + 1, \\text{ } g(x) = \\frac{1}{x^3 - x}$$\nand apply the Leibnitz rule for products,\n$$fg^{(n)}(x) = \\sum_{k=0}^n {n\\choose k} f^{(n-k)}(x)g^{(k)}(x) .$$\nSince $f$ is vanishing after the third differentiation, we get\n$$fg^{(100)}(x) = {100 \\choose 2}f^{(2)}g^{(98)} + {100 \\choose 1}f^{(1)}g^{(99)} {100 \\choose 0}f^{(0)}g^{(100)} \\\\= 9900g^{(98)} + 200xg^{(99)} + (x^2 + 1)g^{(100)}$$\nThis would be great if we could compute the last few derivatives of $g$. Indeed, we can boil this down: notice that\n$$g(x) = h(x)i(x)j(x), \\hspace{4mm} h(x) = \\frac{1}{x-1}, \\text{  } i(x) = \\frac{1}{x}, \\text{  } j(x) = \\frac{1}{x+1};$$\nfurther, $h, i,$ and $j$ have friendly behavior under repeated differentation, e.g. $h^{(n)}(x) = \\frac{(-1)^n n!}{(x-1)^{n + 1}}$.\nSo overall, it is possible to use Leibnitz again to beat a lengthy derivative out of this function, (namely,\n$$g^{(n)}(x) = \\sum_{k=0}^n {n \\choose k} h^{(n-k)}(x) \\Bigl(\\sum_{l=0}^k {k \\choose l} i^{(k-l)}(x) j^{(l)}(x)\\Bigr)$$\nwith the details filled in). \nHowever, this is really pretty far from computing the derivative.\n\nSo, my question: does anyone know how to either improve the above argument, or generate a new one, which can resolve the problem?", "output": "We have a partial fraction decomposition\n$$\n\\frac{x^2+1}{x^3-x}=\\frac{-1}{x}+\\frac{1}{x+1}+\\frac{1}{x-1}\n$$\nIt follows that\n$$\n\\left(\\frac{d}{dx}\\right)^{100}\\frac{x^2+1}{x^3-x}=\\frac{-100!}{x^{101}}+\\frac{100!}{(x+1)^{101}}+\\frac{100!}{(x-1)^{101}}\n$$", "meta": {"post_id": 183488, "input_score": 57, "output_score": 95, "post_title": "The $100$th derivative of $(x^2 + 1)/(x^3 - x)$"}}
{"input": "I have a question about the Sobolev Space $H^1_0(U)$, where $U$ is a open subset of $\\mathbb{R}^n$. Let us denote with $H^{-1}(U)$ the dual space of $H^1_0$. \n\n\nHow is the acting of $H^{-1}$ and $H^1_0$ defined, i.e.\n  $$\n\\langle \\phi,u\\rangle\n$$ where $\\phi\\in H^{-1}$ and $u\\in H^1_0$?  \nFurthermore, if I have elements $v,u\\in H^1_0$, why is it true that\n  $$(u,v)_{L^2} = \\langle u,v\\rangle$$\n  where the latter should denote again the dual pairing of $H^1_0$ and $H^{-1}$. \n\n\nThanks for your help\nhulik", "output": "As you defined, $H^{-1}$ is an abstract space, consisting of continuous linear functionals on $H^1_0$. So let us take an element $\\phi\\in H^{-1}$. What is $\\phi$? This is a linear functional on $H^1_0$, so it can act on any function $u\\in H^1_0$ and spit out a number. We denote this number by $\\phi(u)$ or $\\langle \\phi,u\\rangle$. The latter notation has the advantage of appearing more symmetric, and cleaner when you have complicated expressions instead of $\\phi$ or $u$, as in $\\langle \\alpha\\phi+\\beta \\exp(F),uv+\\xi\\rangle$.\nNow $H^{-1}$ contains many familiar operations on functions. Remember that the elements of $H^{-1}$ are indeed operations, that do something on functions to get numbers. Evaluating a function at a given point $x$, and integrating a function against another given function are examples of such operations, which are therefore potential elements of $H^{-1}$. Let us take a function $v\\in L^2$. Then we can define a linear operation $\\phi$ by\n$$\n\\phi(u) = \\int vu = (v,u)_{L^2}.\\qquad\\qquad(*)\n$$\nIs $\\phi\\in H^{-1}$? We have to check two things: linearity, and continuity. Obviously $\\phi$ is linear: \n$$\n\\phi(\\alpha u+\\beta w)=\\int v(\\alpha u+\\beta w)=\\alpha\\int vu + \\beta\\int vw\n=\\alpha\\phi(u)+\\beta\\phi(w).\n$$\nContinuity can be checked by using the Cauchy-Bunyakowsky-Schwarz inequality:\n$$\n|\\phi(u)|=|(v,u)_{L^2}|\\leq \\|v\\|_{L^2}\\|u\\|_{L^2}\\leq \\|v\\|_{L^2}\\|u\\|_{H^1}.\n$$\nSo $\\phi\\in H^{-1}$. In other words, if we define a mapping $T:v\\mapsto \\phi$ by ($*$), then $T(L^2)\\subset H^{-1}$. Is $T:L^2\\to H^{-1}$ injective? In other words, is it possible that two different functions $v_1$ and $v_2$ to give rise to the same functional $\\phi$? This would mean that\n$$\n(v_1,u)_{L^2} = (v_2,u)_{L^2},\n\\qquad\\textrm{or}\\qquad\n(v_1-v_2,u)_{L^2} = 0,\n$$\nfor all $u\\in H^1_0$.\nIn particular, the preceding is true for all compactly supported smooth functions $u$, which implies that $v_1$ and $v_2$ must agree with each other almost everywhere. This means $v_1=v_2$ in $L^2$.  So $T:L^2\\to H^{-1}$ is injective. What all this means is that we can think the image $T(L^2)$ as being $L^2$ itself, so we can think of $L^2$ being a subset of $H^{-1}$. \nNow we write simply $v$ instead of $\\phi=T(v)$.\nHence\n$$\n\\langle v,u\\rangle = (v,u)_{L^2}.\n$$", "meta": {"post_id": 183644, "input_score": 17, "output_score": 35, "post_title": "How is the acting of $H^{-1}$ on $H^1_0$ defined?"}}
{"input": "I am currently an undergraduate math student. (In fact, freshmen.)\nI know that usually abstract algebra is taught somehow late in the undergraduate course, and curious how studies of abstract algebra at graduate level differ from studies at undergraduate level.\nSo, things like what gets new treatment, or what is learned new are what I want to know.", "output": "What constitutes a \"graduate algebra' course in the United States has undergone a lengthy evolution. As MTurgon and rschwieb said earlier, it's highly subjective from teacher to teacher. But I think the evolution of the subject gives some insight of what can be expected at most programs today for a year long graduate course. \nThe first modern graduate text was,of course, Van Der Waerden's Moderne Algebra,based on the legendary lectures of Emil Artin, Emmy Noether and Van Der Waerden himself at the the University of Gottingen before World War II. It was the first real abstract algebra textbook since these lectures emerged from the researches of the authors.The syllabus became the standard mantra for algebra courses, \"Groups,rings and fields\". Until the 1960's, algebra was considered a graduate course and it was very unusual for most undergraduates to have had much exposure to algebra with the exception of the world's top programs, such as Harvard or Yale. The first undergraduate course in algebra was developed and presented by Saunders MacLane and Garrett Birkoff at Harvard in 1941 and it eventually became,of course, the basis for their classic text, A Survey of Modern Algebra. But it was very unusual for undergraduates to have a solid course in abstract algebra before the 1960's. When undergraduate and graduate algebra courses became standard courses in math departments, the curricula was fairly well-established: undergraduate courses were based on the Survey while graduate courses where based on Van Der Waerden. \n By the late 1960's and early 1970's, Van Der Waerden's book was no longer representative of the frontiers of algebra, which were now nearly unrecognizable with the explosive growth of categorical and homological methods,noncommutative algebra, modern commutative algebra and modern algebraic geometry. The first edition of Serge Lang's Algebra was published in 1965, concurrent with the peak popularity of the Bourbaki volumes. Lang's book effectively replaced Van der Waerden as the graduate algebra text of choice at top programs due its completely modern approach and it's emphasis on categorical and homological methods in all areas of algebra. It still is,to a large degree-but its sheer difficulty and dry austerity,coupled with the mammoth size of later editions and the explosively rapid growth of algebra at the research level-has recently lead to a new generation of algebra books at the graduate level, such as Grillet (my personal favorite), Rowen and Rotman.All these books have continued the hard categorical slant of Lang while trying to bring more recent developments into the standard courses. \nFrom the prior discussion as well as my own experiences, I can state that graduate algebra generally differs from undergraduate courses in the subject in 3 ways: \n1) Much the same way undergraduate analysis covers the \"classical\" analysis of the late 19th century  and graduate analysis courses cover modern topic of the last century, graduate algebra differs mainly from undergraduate algebra in the emphasis on category theory and homological methods. There are programs that attempt to present category theory and diagram chasing to undergraduates in their algebra courses, but I think this is mainly at the top research programs,where the goal is to speed students to the frontiers as quickly as possible.In general, the categorical approach isn't tackled full on until the graduate algebra sequence and consequently the topics that are most strongly developed by these methods-i.e. homological algebra, noncommutative ring and module theory, algebraic geometry-are not discussed in depth until the graduate course. \n2) Expect the course to be much deeper, terser and problem oriented then your undergraduate course. This for 2 reasons: a) A graduate course in algebra needs to survey most of the subject as it stands today to prepare the students for research in either algebra or other fields-and unless the student is ready to learn actively, there simply will not be time to cover the bulk of this work. Also b) graduate students are now beginning to make the transition to being professional mathematicians and they can't very well do that if they're still learning simple proofs off lectures or textbooks. They have to not only learn material much more quickly,they have to learn to build vast tracts of theory themselves. The best way to do both is to give the student a large chunk of the classwork to learn themselves. \n3)Depending on whether your instructor is a prominent researcher in the field of algebra, a graduate algebra course may be much more closely tied to the frontiers of research then is usual. If so, he or she may cover the standard material in a \"need to know\" fashion in order to cover the maximal amount of material relevant to his or her research interests and a large chunk of the course would then be more like a research seminar, relying much more on published papers then standard textbooks. If the professor is not an active algebraicist, expect the course to follow a much more standard path through a conventional textbook like one of the ones stated above. \nSpecifically what topics can you expect in a graduate algebra class? At the minimum, I would expect the following topics to be covered: group theory through the Sylow theorems, free groups and presentations and the Fundamental Theorum of Abelian Groups, ring and module theory including both  the noncommutative and commutative aspects, field theory including a large section on Galois theory, linear and multilinear algebra including a full discussion of tensor products,basic category theory and homological algebra,universal algebra, semisimple rings and algebras and perhaps some algebraic geometry and algebraic number theory.            \nHope that helped-good luck!", "meta": {"post_id": 184142, "input_score": 23, "output_score": 37, "post_title": "How is graduate abstract algebra different from undergraduate abstract algebra?"}}
{"input": "I've read in many places that the Monster group was suspected to exist before it was actually proven to exist, and further that many of its properties were deduced contingent upon existence.\nFor example, in ncatlab's article,\n\nThe Monster group was predicted to exist by Bernd Fischer and Robert Griess in 1973, as a simple group containing the Fischer groups and some other sporadic simple groups as subquotients. Subsequent work by Fischer, Conway, Norton and Thompson estimated the order of $M$ and discovered other properties and subgroups, assuming that it existed.\n\nOr Wikipedia,\n\nThe Monster was predicted by Bernd Fischer (unpublished) and Robert Griess (1976) in about 1973 as a simple group containing a double cover of Fischer's baby monster group as a centralizer of an involution. Within a few months the order of $M$ was found by Griess using the Thompson order formula [...]\n\nOr The Spirit of Moonshine,\n\nIts existence is a non-trivial fact: when the original moonshine\n  conjectures were made, mathematicians suspected its existence, and had been able to work\n  out its character table, but could not prove it actually existed. They did know that the\n  dimensions of the smallest irreducible representations would be 1, 196883; and 21296876.\n\nIt surprises me that this object could have been predicted before being rigorously discovered, due to it being often described as very complicated and highly nonobvious (or at least its construction).\nTake for instance the description in this AMS review of Moonshine Beyond the Monster:\n\nThe proof of the moonshine conjectures depends on several coincidences. Even\n  the existence of the monster seems to be a \ufb02uke in any of the known constructions:\n  these all depend on long, strange calculations that just happen to work for no\n  obvious reason, and would not have been done if the monster had not already been\n  suspected to exist.\n\nIt'd be cool to be acquainted with this part of the story in some more detail at an accessible level, if possible, though I realize it may necessarily involve heavy machinery or convoluted calculations.", "output": "I'm not an expert on this topic, and have only read summaries of the proofs and techniques.  Nonetheless, let me give this a try.\nBy the odd order theorem, any non-abelian simple group has a nontrivial involution.  One of the key techniques in the classification of finite simple groups was to study the centralizers of involutions.  That is, you try to find all finite simple groups such that the centralizer of an involution has a specific form.\nOnce you prove enough results you end up in the following situation.  You look at candidate centralizers of involutions which are made up entirely of the simple groups that you already know.  You use this to try to find candidates for any new simple groups that you didn't know already.  If your search turns up a new example, you repeat the process trying to find new candidate centralizers of involution involving the new simple group.\nThus it should not be surprising that when people found the Baby Monster they started looking for candidates centralizers of involutions which you can make out of the Baby Monster.  Once you find a good candidate that you can't rule out (e.g. they couldn't find any reason why the centralizer of an involution couldn't be the double cover of the Baby Monster), then you conjecture a new simple group.\nWhat's interesting is that when you get to the Monster you can try repeating this process to try to find yet another new simple group where the centralizer of an involution is built from the Monster.  There's no reason a priori that this couldn't continue forever, with more and more sporadic groups.  But it turns out that it stops at the Monster.", "meta": {"post_id": 184463, "input_score": 78, "output_score": 44, "post_title": "How was the Monster's existence originally suspected?"}}
{"input": "$\\pi$ can be represented as $C/D$, and $C/D$ is a fraction, and the definition of an irrational number is that it cannot be represented as a fraction. \nThen why is $\\pi$ an irrational number?", "output": "A rational number is a number that can be expressed as $p/q$, where $p$ and $q$ are integers. The number $\\pi$ cannot be expressed in this form; hence it is irrational.\nIn other words, the definition of \"fraction\" does not include ratios like \"circumference/diameter\" in which the numerator and denominator are arbitrary numbers, not necessarily integers. In the case of \"circumference/diameter\" (which you denoted $\\pi = C/D$), it will always be the case that if the diameter is an integer, the circumference ($C = \\pi D$) is not an integer, and if the circumference is an integer, the diameter ($D = C/\\pi$) is not an integer: precisely because $\\pi$ is irrational.\nNote that a definition of \"fraction\" that allowed arbitrary real numbers as the numerator and denominator would be not very useful, as it would allow \"fractions\" like $\\pi = \\pi / 1$, or indeed, for any number $x$, the representation of $x$ as a \"fraction\" $x = x/1$.", "meta": {"post_id": 184675, "input_score": 51, "output_score": 49, "post_title": "Why is $\\pi$ irrational if it is represented as $C/D$?"}}
{"input": "Suppose $M$ and $N$ are two connected oriented smooth manifolds of dimension $n$. Conventionally, people use $M\\#N$ to denote the connecte sum of the two. (The connected sum is constructed from deleting an $n$-ball from each manifold and glueing the boundary.) I think there should be a general approach of finding the homology and cohomology groups of $M\\#N$ from the homology and cohomology groups of $M$ amd $N$. However, using Mayer-Vietoris sequence becomes quite complicated whent the spaces are complicated. For example: 1) $M=S^1 \\times S^3$, $N= \\mathbb{CP}^2$. The difficulty comes from understanding the map $H^i(U\\cap V)\\rightarrow H^{i+1}(M\\#N)$.\nIf we define the $M\\#_{rev}N$ to be the connected sum, however, glueing the $n$-ball in the reversed order, then we can ask the same problem. the case $n=2$ is easier to think of the picture when the orientation is reversed. It is also easier since the glueing is \"fixed\" in the sense there is only one parameter discribing the boundary of $n$-ball, aka, $S^1$. What happens when $n$ goes higher?\nI would appreciate it if someone could explain this general question with examples ($M\\#N$ and $M\\#_{rev}N$, where $M=S^1 \\times S^3$, $N= \\mathbb{CP}^2$) in the context.", "output": "The procedure for finding homology and cohomology of the spaces in question is a neat little trick. From here on out, I'll just treat the homology case, but the cohomology follows from the same arguments. Collapse the $S^{n-1}$ you're gluing along to a point- this turns $M\\# N$ into $M\\vee N$. Since $(M\\# N, S^{n-1})$ is a good pair, the homology can be identified with the relative homology of the pair $(M\\# N,S^{n-1})$. From this, we get the following long exact sequence:\n$$\\cdots\\to \\widetilde{H_i}(S^{n-1})\\to \\widetilde{H_i}(M\\# N) \\to \\widetilde{H_i}(M\\vee N)\\to\\cdots$$\nBy a simple Mayer-Vietoris argument, we have that $\\widetilde{H}_i(M\\vee N)\\cong \\widetilde{H}_i(M)\\oplus \\widetilde{H}_i(N)$. Since $\\widetilde{H_i}(S^{n-1})$ is zero except for $i=n-1$, we have automatically that $H_i(M\\# N)\\cong H_i(M\\vee N)\\cong H_i(M)\\oplus H_i(N)$ for $i\\neq n-1,n$. The only interesting case is as follows:\n$$0\\to \\widetilde{H_n}(M\\# N)\\to \\widetilde{H_n}(M\\vee N) \\to \\widetilde{H}_{n-1}(S^{n-1})\\to \\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N) \\to 0$$\nNow, we start getting into some casework depending on whether none, one, or both of $M,N$ are orientable. In the case that both are orientable, the above sequence turns into \n$$0\\to \\mathbb{Z} \\to \\mathbb{Z}\\oplus\\mathbb{Z} \\to \\mathbb{Z} \\to \\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N) \\to 0$$\nas their connected sum is also orientable. From this, we see that $\\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N)$ must be an isomorphism.\nIf just one of $M,N$ is orientable, then their connected sum is non-orientable, and the following happens:\n$$0\\to 0 \\to \\mathbb{Z}\\oplus0 \\to \\mathbb{Z} \\to \\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N) \\to 0$$\nin which case we still have that that $\\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N)$ must be an isomorphism.\nIf neither of $M,N$ are orientable, then their connected sum is non-orientable, in which case the long exact sequence does the following:\n$$0\\to 0 \\to 0 \\to \\mathbb{Z} \\to \\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N) \\to 0$$\nand thus $\\widetilde{H}_{n-1}(M\\# N)$ is an extension of $\\widetilde{H}_{n-1}(M\\vee N)$ by $\\mathbb{Z}$. To figure out what extension it is, one needs to inspect the map $S^{n-1}\\to M\\# N$ and the corresponding map on homology. Nothing too surprising can happen- $H^{n-1}(M\\# N)$ is the direct sum of a free abelian group and a finite abelian group.\nNote that during this argument, it was never necessary to talk about the orientation of the gluing- so $M\\# N$ and $M\\#_{rev}N$ have the same homology/cohomology. No description of $S^{n-1}$ was ever used except for it having reduced homology only in degree $n-1$, so the process does not care very much about what dimension your manifolds are.\nNow, for the example where $M=S^1\\times S^3$ and $N=\\mathbb{C}P^2$. $M$ has homology $H_0\\cong H_1\\cong H_3\\cong H_4\\cong \\mathbb{Z}$ and all other groups zero, while $N$ has homology $H_0\\cong H_2\\cong H_4\\cong \\mathbb{Z}$ and all other groups zero. Using the procedure above, we have that the homology of $M\\# N$ is as follows: $H_0\\cong H_1\\cong H_2\\cong H_3\\cong H_4\\cong \\mathbb{Z}$. The result is the same for $M\\#_{rev}N$.\n\nAddendum: It has been pointed out in the comments that the arguments about what happens with the maps between the various copies of $\\Bbb Z$ are not quite complete. The key gap is showing that the map from $\\widetilde{H}_n(M\\vee N)\\to \\widetilde{H}_{n-1}(S^{n-1})$ is surjective when at least one of $M,N$ are orientable. The fix is reasonable, and is presented below to make this answer as complete as possible.\nThe sequence for a good pair gives that the map $H_n(M\\# N,S^{n-1})\\to H_{n-1}(S^{n-1})$ is given by taking the boundary of relative chains. Without loss of generality, $M$ is orientable, so consider the relative class in $H_n(M\\# N,S^{n-1})$ which is the fundamental class of $M$ minus the disc we delete to perform the connected sum operation. This has boundary exactly the fundamental class of $S^{n-1}$ (up to possibly a sign change for orientation issues) by the definition of how we chose $S^{n-1}$ to glue along. As we identified $H_n(M\\# N,S^{n-1}) \\cong \\widetilde{H}_n(M\\vee N) \\cong \\widetilde{H}_n(M)\\oplus \\widetilde{H}_n(N)$, we see that both the map $\\Bbb Z\\oplus\\Bbb Z\\to \\Bbb Z$ and $\\Bbb Z\\oplus 0\\to \\Bbb Z$ are surjective. Thus the conclusion about $\\widetilde{H}_{n-1}(M\\# N)\\to \\widetilde{H}_{n-1}(M\\vee N)$ being an isomorphism when at least one of $M,N$ is orientable holds.", "meta": {"post_id": 187413, "input_score": 39, "output_score": 58, "post_title": "Computing the homology and cohomology of connected sum"}}
{"input": "Let $A$ be a $m \\times n$ matrix with entries from some field $F$. Define the determinant rank of $A$ to be the largest possible size of a nonzero minor, i.e. the size of the largest invertible square submatrix of $A$. It is true that the determinant rank is equal to the rank of a matrix, which we define to be the dimension of the row/column space.\nIt's not difficult to see that $\\text{rank} \\geq \\text{determinant rank}$. If some submatrix of $A$ is invertible, then its columns/rows are linearly indepedent, which implies that the corresponding rows/columns of $A$ are also linearly indepedent.\nIs there a nice proof for the converse?", "output": "If the matrix $A$ has rank $k$, then it has $k$ linearly independent lines. Those form an $k\\times n$ submatrix, which of course also has rank $k$. But if it has rank $k$, then it has $k$ linearly independent columns. Those form a $k\\times k$ submatrix of $A$, which of course also has rank $k$. But a $k\\times k$ submatrix with rank $k$ is a full-rank square matrix, therefore invertible, thus is has a non-zero determinant. And therefore the determinant rank has to be at least $k$.", "meta": {"post_id": 187497, "input_score": 34, "output_score": 45, "post_title": "Proof that determinant rank equals row/column rank"}}
{"input": "Let's choose an open covering for $\\left [ 0,1 \\right ]$. For example $$\\left \\{ \\left ( \\frac 1 n,1-\\frac 1 n \\right )  \\mid  n\\in \\{ 3,4,\\dots\\} \\right \\}.$$\nHow can one choose a finite open subcover to prove compactness?", "output": "Here is a quick and elegant proof of the actual result that $[0,1]$ is compact based on real induction:\nLet $\\mathcal{O}$ be an (arbitrary!) open cover. Let $P$ be the set of points $x$ in $[0,1]$ such that $[0,x]$ can be covered by finitely many elements of $\\mathcal{O}$. We have $0\\in P$ and $P$ is bounded above by $1$. Therefore, $P$ has a supremum $s$. \nWe first show that $[0,s]$ can be covered by finitely many sets in $\\mathcal{O}$. This is trivial when $s=0$, so assume $s>0$. Let $O_s\\in\\mathcal{O}$ be a set containing $s$. Then there is an $\\epsilon \\in (0, s)$ such that $(s-\\epsilon,s]\\subseteq O_s$. By assumption, there is a finite subcover of $[0,s-\\epsilon/2]$. By adding $O_s$ to that finite subcovering, we get a finite subcovering of $[0,s]$.\nWe now show that $s=1$. Suppose $s<1$ and let $O_s\\in\\mathcal{O}$ be a set containing $s$. Then there is an $\\epsilon>0$ such that $[s,s+\\epsilon)\\subseteq O_s$. So taking a finite subcover of $[0,s]$ and adding the set $O_s$ gives us a finite subcover of $[0,s+\\epsilon/2]$, contradicting the construction of $s$.", "meta": {"post_id": 188996, "input_score": 12, "output_score": 55, "post_title": "Showing that $[0,1]$ is compact"}}
{"input": "I read in the book Methods of homological algebra of Gelfand and Manin that the derived category of an abelian category $A$ is never abelian. Now to me this seems to be wrong, because if $A=0$ then $D(A)=0$ and so it is abelian. Do you know what statement is true? (Like every derived category of a non-zero category is not abelian) and do you know how to prove it?", "output": "Let $\\mathscr A$ be an abelian category. The derived category $D(\\mathscr{A})$ is abelian if and only if $\\mathscr A$ is semisimple.\n\nRecall that an abelian category is called semisimple if all short exact sequences split. Equivalently, $\\mathscr A$ is abelian and for every morphism $f\\colon A \\to B$ there is a pseudoinverse morphism $g\\colon B \\to A$, that is, a morphism $g$ such that $fgf = f$ and $gfg = g$: If $\\mathscr A$ is semisimple, factor $f$ over its image as $f = i j$, choose a left inverse $k$ for $i$ and a right inverse $l$ for $j$ and put $g = lk$ so that $f g f = (ij)(lk)(ij) = i(jl)(ki)j = ij = f$ and similarly $gfg = g$; for the other direction note that $fgf = f$ and $f$ monic imply that $gf = 1$, so every monic splits, and dually every epic splits.\nSo, the derived category of $R$-modules is abelian if and only if $R$ is a semisimple ring. See also Lam, A first course in non-commutative rings, Theorem and Definition\u00a0(2.5), page\u00a027 for this point. More explicitly, the derived category of $k$-vector spaces over a field $k$ is abelian while the derived category of abelian groups isn't abelian.\n\nThe main ingredient to answer your question is provided by the following:\n\nLemma (Verdier). A triangulated category $\\mathscr T$ is abelian if and only if every morphism $f\\colon A \\to B$ is isomorphic to $A' \\oplus I \\xrightarrow{\\begin{bmatrix} 0 & 1_I \\\\ 0 & 0\\end{bmatrix}} I \\oplus B'$.\n\nIn particular, in an abelian triangulated category $\\mathscr T$ every morphism $f$ has a pseudoinverse $g$. Since an abelian category $\\mathscr{A}$ embeds fully faithfully into its derived category $D(\\mathscr A)$ by identifying an object of $\\mathscr{A}$ with a complex concentrated in degree zero, this immediately implies that $\\mathscr{A}$ must be semisimple if $D(\\mathscr A)$ is abelian. \nConversely, if $\\mathscr{A}$ is semisimple abelian then $D(\\mathscr{A})$ is equivalent to the abelian category $\\mathscr{A}^{\\mathbb{Z}}$ via the functor that sends a complex $A$ to its homology complex $H(A^\\bullet)$ with $H^k(A)$ in degree $k$ and zero differentials. This is proved in detail in Section\u00a0III.2.3, page\u00a0146f of Gelfand\u2013Manin's Methods of Homological Algebra.\n\nThe proof of the lemma is relatively easy: Certainly, if every morphism is of the described form then $\\mathscr{T}$ is abelian because $f$ has kernel $A'$, image $I$ and cokernel $B'$ and that's all we need.\nOn the other hand, if $\\mathscr{T}$ is abelian then every morphism $f\\colon A \\to B$ factors over its image as $f = me$ with an epimorphism $e\\colon A \\twoheadrightarrow I$ and a monomorphism $m\\colon I \\rightarrowtail B$ and this reduces the lemma to the statement:\n\nIn a triangulated category all monomorphisms and all epimorphisms split.\n\nRecall that the morphism axiom [TR3] shows that two consecutive morphisms in a distinguished triangle $A \\xrightarrow{f} B \\xrightarrow{g} C \\xrightarrow{h} A[1]$ compose to zero. If $f$ happens to be monic then $fh[-1] = 0$ shows that $h[-1] =0$, so $h = 0$. Still assuming $f$ to be monic, apply the homological functor $\\operatorname{Hom}(C,{-})$ to the distinguished triangle $A \\xrightarrow{f} B \\xrightarrow{g} C \\xrightarrow{0} A[1]$ to get the exact sequence\n$$\n0 \\to \\operatorname{Hom}(C,A) \\to \\operatorname{Hom}(C,B) \\to \\operatorname{Hom}(C,C) \\to 0\n$$\nshowing that $g$ has a right inverse and applying the cohomological functor $\\operatorname{Hom}({-},A)$ to that distinguished triangle shows that $f$ has a left inverse. It follows from this that our distinguished triangle $A \\xrightarrow{f} B \\xrightarrow{g} C \\xrightarrow{0} A[1]$ with monic $f$ is isomorphic to the triangle $A \\to A \\oplus C \\to C \\to A[1]$ obtained by taking the direct sum of the distinguished triangles $A \\to A \\to 0 \\to A[1]$ and $0 \\to C \\to C \\to 0[1]$.\nComing back to our general morphism $f = me$ and applying the above observation to the epimorphism $e$ and the monomorphism $m$ gives rise to a splitting $A \\cong A'\\oplus I$ and $B \\cong I \\oplus B'$, and $f$ factors as desired.\n\nSee also these two related MO-threads:\n\nHow do I know the derived category is not abelian?\nSplitting in triangulated categories.\n\nThe above argument is essentially contained in Chapitre\u00a0II, Proposition\u00a01.2.9, p.101 and Proposition\u00a01.3.6, p.108 in this part of Verdier's thesis Des cat\u00e9gories d\u00e9riv\u00e9es des cat\u00e9gories ab\u00e9liennes, available electronically on Georges Maltsiniotis's home page who prepared the Ast\u00e9risque edition of the thesis in the mid-90ies.", "meta": {"post_id": 189769, "input_score": 33, "output_score": 47, "post_title": "When is the derived category abelian?"}}
{"input": "Forgive me if this question is quite na\u00efve; I've studied axiomatic set theory in the context of ZF, but my knowledge of NF(U) goes little beyond its axioms, what it means for a formula to be stratified, and stuff that I've read on websites here and there.\nWhat makes NF appeal to me (more so than ZF) is that it uses fewer axioms and it resolves Russell's paradox in a way which better matches my intuition of how I think a 'set' should behave; certainly it's more intuitive than Foundation+Separation+Replacement+... in ZF. The downside of NF is that it proves $\\neg$AC, which is a shame. But by adding urelements, which I can almost force myself to accept, we get NFU, which is:\n\nConsistent;\nConsistent with the Axiom of Choice;\nConsistent with the Axiom of Infinity;\nMore intuitive than ZF;\n\nAnd according to this page, NFU can \"safely be extended as far as you think ZFC can be extended\".\nNow ZF(C) has its advantages, constructing new sets from old ones and whatnot, but it still hasn't been proved to be consistent. Wikipedia [citation needed] says: \"A common argument against the use of NFU as a foundation for mathematics is that our reasons for relying on it have to do with our intuition that ZFC is correct.\" $-$ is that all there is to it?\nMy question is:\n\nWhy is ZF(C) the paradigm under which 'mathematics' is done, rather than NFU(+Inf)(+AC), which we know to be consistent?", "output": "I've never studied NF or NFU in any great detail, but I have found some points rather subtle and potentially not worth the effort to work around.\n\nStratification. NF cannot escape its roots in Russellian type theory. This means that it is difficult to directly compare sets of different \"types\". For example, consider the \"maps\" $X \\to \\mathscr{P}(X)$ used in Cantor's theorem. This is in some sense illegal in NF: you can only have maps between sets of the same type, and the type of $\\mathscr{P}(X)$ is one higher than that of $X$. The standard NF circumlocution here is to talk about maps $\\mathscr{P}_1 (X) \\to \\mathscr{P} (X)$ instead, where $\\mathscr{P}_1 (X)$ denotes the set of singleton subsets of $X$. Cantor's proof carries through without a problem once this workaround is adopted... but one has to realise that this is a different theorem. In particular, if $V$ is the universal set, we can only prove that there is no surjection $\\mathscr{P}_1 (V) \\to \\mathscr{P}(V)$ \u2013 a far cry from the indubitable fact that $\\mathscr{P}(V) \\subseteq V$! (In fact, if you have even one urelement, then $\\mathscr{P}(V) \\subsetneq V$.)\nFailure of cartesian-closedness. Consider the evaluation \"map\" $\\textrm{ev} : Y^X \\times X \\to Y$. The graph of this \"map\" cannot be a set in NF for various reasons.  [1, 2] Roughly speaking, if $X$ is a set of type $n$, then $Y$ must also be a set of type $n$, in which case $Y^X \\subseteq \\mathscr{P}(X \\times Y)$ is a set of type $n + 1$; but then we are unable to write down a stratified definition of $Y^X \\times X$. Thus, the category of NF sets is not cartesian closed!\nLocal types. What I have said so far appears to contradict the (finite) axiomatisation given by Holmes [3], which says e.g. that you can always form the set $X \\times Y$ or $Y^X$. But this is only an apparent contradiction. The fact of the matter is that sets in NF do not actually have a type. I like to think of NF as being \"locally typed\": sets only gain types when they interact with other sets. Thus, $Y^X \\times X$ exists (in some sense), but we cannot make much use of it. Holmes writes,\n\nAnother fact about stratification restrictions should be noted: a variable free in $\\phi$ in a set definition $\\{ x \\mid \\phi \\}$ may appear with more than one type without preventing the set from existing, as long as this set definition is not itself embedded in a further set definition. The reason for this is that such a set definition can be made stratified by distinguishing all free variables: the resulting definition is supposed to work for all assignments of values to those free variables, including those in which some of the free variables (even ones of different type) are identified with one another. For example, the set $\\{ x, \\{ y \\} \\}$ has a stratified definition; the set $\\{ x, \\{ x \\} \\}$ has an unstratified definition, but the existence of $\\{ x, \\{ y \\} \\}$ for all values of $x$ and $y$ ensures the existence of $\\{ x, \\{ x \\} \\}$ for any $x$. But a term $\\{ x, \\{ x \\} \\}$ cannot appear in the definition of a further set in which $x$ is bound.\n\nWeaknesses. Holmes [4] points out that bare NFU has the same consistency strength as PA, and NFU + Infinity + AC has the same consistency strength as Mac Lane set theory (MAC) (i.e. Zermelo set theory but with only $\\Delta_0$-separation). It is true that in many cases we only need $\\Delta_0$-separation to form the sets we want \u2013 but when induction seeps into the picture we have to start worrying. \nConsider the vector space $V$ of all real polynomials. There is no doubt that we can form the dual space $V^* = \\textrm{Hom}_\\mathbb{R} (V, \\mathbb{R})$ in MAC \u2013 but Mathias [5] says that MAC cannot prove \"the $n$-th iterated dual space exists for all natural numbers $n$\". This is because the cardinalities of the iterated dual space grow too fast \u2013 after all, $V_{\\omega + \\omega}$ is a model of MAC. This seems contrived, but it is enough for me to start worrying about whether important inductive constructions can be carried out in MAC. And if MAC is not strong enough to do this bit of mathematics, why should NFU + Infinity + Choice be? (The two theories are not biinterpretable, so it's not literally true that they prove the same theorems.)\n\nPersonally, I'd rather have unrestricted comprehension. But then I may as well ask for a pony...", "meta": {"post_id": 193198, "input_score": 31, "output_score": 36, "post_title": "Why use ZF over NFU?"}}
{"input": "This might be a silly question, but i was wondering, is there any topology that cannot be generated by a basis? if not, given a topology, is there a reliable way of figuring out a basis for it? it probably matters if the set $X$ the topology is on is countable or not, right? Would it matter if the topology itself is countable? Thanks for any help/advice/feedback.", "output": "Every topology is a base for itself.\nAdded: On any set $X$ the indiscrete topology $\\{X,\\varnothing\\}$ has only itself as base. There are other examples. For instance, for $n\\in\\Bbb Z^+$ let $V_n=\\{1,\\dots,n\\}$, and let $\\tau=\\{\\varnothing,\\Bbb N\\}\\cup\\{V_n:n\\in\\Bbb Z^+\\}$; then $\\tau$ is a topology on $\\Bbb N$ whose only base is itself: none of the sets $V_n$ can be written as a union of the other sets in the topology, and $\\Bbb N$ is the only open set containing $0$.\nHowever, if each point of the space $\\langle X,\\tau\\rangle$ has an open nbhd that is not the whole space, then $\\tau\\setminus\\{X\\}$ is always a base of $\\tau$ different from $\\tau$ itself. In particular this is true of every $T_1$ space with more than one point.", "meta": {"post_id": 193469, "input_score": 27, "output_score": 36, "post_title": "does every topology have a basis?"}}
{"input": "Can anyone explain to me what a blow-up is? If would be great if someone could provide a definition and some examples. Any free introductory texts are welcome too.  Thanks!", "output": "The basic idea of blowups in algebraic geometry is to remove a point from an algebraic variety and replace it by the projectivized tangent cone at that point. The result is a new space, with a projective map down to the old, such that the fibre over the \"centre\" (the point we blow up) is an effective Cartier divisor. This generalizes in a couple directions: we can blow-up on closed subvarieties, and the construction extends also to schemes (and probably to more general spaces also).\nBlowups satisfy a universal property with respect to replacing the centre by an effective Cartier divisor (i.e., a subvariety that is locally defined by a single nonzerodivisor): Let $Y\\subseteq X$ be a closed subvariety. A morphism $\\pi:\\widetilde X\\to X$ is called a blowup of X with centre Y if the two following properties hold:\n(a) $E:=\\pi^{-1}(Y)$ is an effective Cartier divisor on $\\widetilde X,$\n(b) $\\pi$ satisfies a universal property with respect to (a), namely for every morphism $\\tau:Z\\to X$ such that $\\tau^{-1}(Y)$ is an effective Cartier divisor, there is a unique morphism $\\varphi:Z\\to \\widetilde X$ such that $\\pi\\circ\\varphi=\\tau.$\nBlowups can be easily constructed in a few ways. One way is by computing charts that can be glued. For example, let $X$ be an affine variety over the field $k$ with coordinate ring $k[X],$ and let $I=(g_1,\\dots, g_l)\\subseteq k[X]$ be an ideal cutting out the subvariety $Y.$ Suppose for simplicity that $X$ is irreducible, so that $k[X]$ is a domain. Then $\\widetilde X$ is defined by the collection of charts $U_i=\\operatorname{Spec}\\left(k[X][g_1/g_i,\\ldots, g_l/g_i]\\right),$ where $k[X][g_1/g_i,\\ldots, g_l/g_i]\\subseteq k(X)$ can be viewed as a subring of the fraction field of $k[X].$\nHere is an easy example. We compute $\\widetilde X$ where $X=\\Bbb A^2$ has centre $Y=\\{(0,0)\\}.$ So $I_Y=(x,y),$ and $\\widetilde X$ has two charts: $U_x=\\operatorname{Spec}(k[x,y,y/x])=\\operatorname{Spec}(k[x,y/x]),$ and $U_y=\\operatorname{Spec}(k[x,y,x/y])=\\operatorname{Spec}(k[y,x/y]).$ That was easy.\nHere is another example, which generalizes. Knowing $\\widetilde X=\\widetilde{\\Bbb A^2}$, we can compute the blowup of say $V=V(y^2-x^3)\\subseteq X$ as the strict transform under the map $\\pi:\\widetilde X\\to X.$ What I mean is that in the $x$-chart of $\\widetilde X,$ the map $\\pi$ has a dual description given by $k[x,y]\\to k[x,y,y/x],x\\mapsto x,y\\mapsto x\\cdot y/x,$ and we can use this to get $\\widetilde V.$ In this chart, the equation for $V$ maps $y^2-x^3\\mapsto (x\\cdot y/x)^2-x^3= x^2((y/x)^2-x).$ Saying that $\\widetilde V$ is the strict transform means that it has a chart defined by $(y/x)^2-x$ inside $\\Bbb A^2=\\operatorname{Spec}(k[x,y/x]).$ There is a second chart which you can compute in exactly the same way.\nThere is much, much more that can be said about blowups, they are a central technique in algebraic geometry in my view. I think the best you can do to learn them is to compute as many examples as possible. There is a great section in Eisenbud and Harris' book on blow-ups. You should try to compute every example there.", "meta": {"post_id": 193681, "input_score": 24, "output_score": 43, "post_title": "What is a blow-up?"}}
{"input": "While studying complex variables, I could learn that $f(z)=|z|^{2}$ has only one point which is $z=0$ that $f$ being differentiable and $f$ being not differentiable at any other points.\nThen, I was wondering if there is a function $f: \\mathbb R \\to \\mathbb R$ that has only one point differentiable and not on any other points.\nIn intuition, it seems there are no such point!\nHowever, I have no idea how I can prove this...\nAdditional question is that would there be any function $f: \\mathbb R \\to \\mathbb R$ that has only one point continuous and not on any other points.\nI think this is pretty interesting things to think about! :-)", "output": "Let $$p(x)= \\begin{cases} 0,& x\\in\\mathbb Q\\\\\\\\1,& x\\in \\mathbb R-\\mathbb Q \\end{cases}$$ Now take $f(x)=x^2p(x)$.", "meta": {"post_id": 194194, "input_score": 28, "output_score": 44, "post_title": "Is there a function $f: \\mathbb R \\to \\mathbb R$ that has only one point differentiable?"}}
{"input": "I found Novikov said that algebraic topology was dead in the early 1970's in this article.\n\nSegal had been one of Atiyah's first students, working on equivariant K-theory, and \n  then other equivariant generalized cohomology theories. He was a collaborator on the second of the Annals papers on the index theorem. Well known as an algebraic topologist, he arrived in Moscow in the early 1970s to give some lectures and met S. Novikov, who told him, \u201cSo you are a topologist? Here we think that algebraic topology is dead.\u201d\n\nI wonder what he meant by it.\nAny thoughts?\nNote I heard that Thom also said so, but I could find only Japanese Wikipedia article saying he said so, which does not give a reference for that assertion.", "output": "Novikov was mentioned in the comments as a possible source. Some of his views and memories are published in the article\nhttp://arxiv.org/abs/math-ph/0004012\nHe calls 1970-80 a \"period of decay\" for topology, naming the following problems: migration of leaders to other fields; a relaxation of rigor and standards of documentation; the resulting \"informational mess\" about what had been proved and by whom.", "meta": {"post_id": 194979, "input_score": 23, "output_score": 35, "post_title": "Why did a famous mathematician say that algebraic topology was dead?"}}
{"input": "I have a doubt about the real meaning of the derivative of a vector field. This question seems silly at first but the doubt came when I was studying the definition of tangent space.\nIf I understood well a vector is a directional derivative operator, i.e.: a vector is an operator that can produce derivatives of scalar fields. If that's the case then a vector acts on a scalar field and tells me how the field changes on that point. \nHowever, if a vector is a derivative operator, a vector field defines a different derivative operator at each point. So differentiate a vector would be differentiate a derivate operator, and that seems strange to me at first. I thought for example that the total derivative of a vector field would produce rates of change of the field, but my studies led me to a different approach, where the total derivative produces rates of change only for scalar fields and for vector fields it produces the pushforward.\nSo, what's the real meaning of differentiating a vector field knowing all of this?", "output": "As I understand it, these are your questions:\n\nHow does one define the derivative of a vector field?  Do we just take the \"derivatives\" of each vector in the field?  If so, what does it mean to take the derivative of a differential operator, anyway?\nWhy does the total derivative of a scalar field give information about rates of change, while the \"total derivative\" of a vector field gives the pushforward (which doesn't seem to relate to rates of change)?\n\nI think the best way to answer these questions is to provide a broader context:\n\nIn calculus, we ask how to find derivatives of functions $F\\colon \\mathbb{R}^m \\to \\mathbb{R}^n$.  The typical answer is the total derivative $DF\\colon \\mathbb{R}^m \\to L(\\mathbb{R}^m, \\mathbb{R}^n)$, which assigns to each point $p \\in \\mathbb{R}^m$ a linear map $D_pF \\in L(\\mathbb{R}^m, \\mathbb{R}^n)$.  With respect to to the standard bases, this linear map can be represented as a matrix:\n$$D_pF = \\begin{pmatrix}\n \\left.\\frac{\\partial F^1}{\\partial x^1}\\right|_p & \\cdots & \\left.\\frac{\\partial F^1}{\\partial x^m}\\right|_p \\\\\n \\vdots & & \\vdots \\\\\n \\left.\\frac{\\partial F^n}{\\partial x^1}\\right|_p & \\cdots & \\left.\\frac{\\partial F^n}{\\partial x^m}\\right|_p\n\\end{pmatrix}$$\nPersonally, I think this encodes the idea of \"rate of change\" very well.  (Just look at all those partial derivatives!)\nLet's now specialize to the case $m = n$.  Psychologically, how does one intuit these functions $F\\colon \\mathbb{R}^n \\to \\mathbb{R}^n$?  There are two usual answers:\n\n(1) We intuit $F\\colon \\mathbb{R}^n \\to \\mathbb{R}^n$ as a map between two different spaces.  Points from the domain space get sent to points in the codomain space.\n(2) We intuit $F\\colon \\mathbb{R}^n \\to \\mathbb{R}^n$ as a vector field.  Every point in $\\mathbb{R}^n$ is assigned an arrow in $\\mathbb{R}^n$.\n\nThis distinction is important.  When we generalize from $\\mathbb{R}^n$ to abstract manifolds, these two ideas will take on different forms.  Consequently, this means that we will end up with different concepts of \"derivative.\"\n\nIn case (1), the maps $F\\colon \\mathbb{R}^m \\to \\mathbb{R}^n$ generalize to smooth maps between manifolds $F \\colon M \\to N$.  In this setting, the concept of \"total derivative\" generalizes nicely to \"pushforward.\"  That is, it makes sense to talk about the pushforward of a smooth map $F \\colon M \\to N$.\nBut you asked about vector fields, which brings us to case (2).  In this case, we first have to be careful about what we mean by \"vector\" and \"vector field.\"\n\nA vector $v_p \\in T_pM$ at a point $p$ is (as you say) a directional derivative operator at the point $p$.  This means that $v_p$ inputs a scalar field $f\\colon M \\to \\mathbb{R}$ and outputs a real number $v_p(f) \\in \\mathbb{R}$.\nA vector field $v$ on $M$ is a map which associates to each point $p \\in M$ a vector $v_p \\in T_pM$.  This means that a vector field defines a derivative operator at each point.\nTherefore: a vector field $v$ can be regarded as an operator which inputs scalar fields $f\\colon M \\to \\mathbb{R}$ and outputs scalar fields $v(f)\\colon M \\to \\mathbb{R}$.\n\nIn this setting, it no longer makes sense to talk about the \"total derivative\" of a vector field.  You've said it yourself: what would it even mean to talk about \"derivatives\" of vectors, anyway?  This doesn't make sense, so we'll need to go a different route.\n\nIn differential geometry, there are two ways of talking about the derivative of a vector field with respect to another vector field:\n\nConnections (usually denoted $\\nabla_wv$ or $D_wv$)\nLie derivatives (usually denoted $\\mathcal{L}_wv$ or $[w,v]$)\n\nIntuitively, these notions capture the idea of \"infinitesimal rate of change of a vector field $v$ in the direction of a vector field $w$.\"\nQuestion: What do these constructions look like in $\\mathbb{R}^n$?\nTaking advantage of the fact that we're in $\\mathbb{R}^n$, we can look at our vector fields in the calculus way: as functions $v\\colon \\mathbb{R}^n \\to \\mathbb{R}^n$.  As such, we can write the components as $v = (v^1,\\ldots, v^n)$.\n\nThe (Levi-Civita) connection of $v$ with respect to $w$ is defined as\n  $$\\nabla_wv = (w(v^1), \\ldots, w(v^n)),$$\n  where $$w(v^i) := w^1\\frac{\\partial v^i}{\\partial x^1} + \\ldots + w^n\\frac{\\partial v^i}{\\partial x^n}.$$\nThe Lie derivative of $v$ with respect to $w$ has a technical definition in terms of flows that I don't want to go into, but the bottom line is that it's similar to Rod Carvalho's answer.\n\nAlso, in $\\mathbb{R}^n$ we have the pleasant formula\n$$\\mathcal{L}_wv = \\nabla_wv - \\nabla_vw,$$\nwhich aids in computation.", "meta": {"post_id": 195000, "input_score": 37, "output_score": 42, "post_title": "Meaning of derivatives of vector fields"}}
{"input": "There is something I don't understand about the proof that perfect sets are uncountable. The same proof is present in Rudin's Principles of Mathematical Analysis.\nDo we assume that our construction of $U_n$ must contain all points of $S$? What if we are only collecting evenly-indexed points of $S$ ($x_{2n}$)? We would still get an infinitely countable subset of $S$, and the rest of $S$ can be used to provide points for $V$. What am I missing?", "output": "There is an alternative proof, using what is a consequence of Baire's Theorem:\n\nTHM Let $(M,d)$ be a complete metric space with no isolated points. Then $(M,d)$ is uncountable.\n\nPROOF Assume $M$ is countable, and let $\\{x_1,x_2,x_3,\\dots\\}$ be an enumeration of $M$. Since each singleton is closed, each $X_i=X\\smallsetminus \\{x_i\\}$ is open for each $i$. Moreover, each of them is dense, since each point is an accumulation point of $X$. By Baire's Theorem, $\\displaystyle\\bigcap_{i\\in\\Bbb N} X_i$ must be dense, hence nonempty, but it is readily seen it is empty, which is absurd. $\\blacktriangle$.\n\nCOROLLARY Let $(M,d)$ be complete, $P$ a perfect subset of $M$. Then $P$ is uncountable.\n\nPROOF $(P,d\\mid_P)$ is a complete metric space with no isolated points.\n\nADD It might be interesting to note that one can prove Baire's Theorem using a construction completely analogous to the proof suggested in the post. \n\nTHM Let $(X,d)$ be complete, and let $\\langle G_n\\rangle$ be a sequence of open dense sets in $X$. Then $G=\\displaystyle \\bigcap_{n\\in\\Bbb N}G_n$ is dense.\n\nPROOOF We can construct a sequence $\\langle F_n\\rangle$ of closed sets as follows. Let $x\\in X$, and take $\\epsilon >0$, set $B=B(x,\\epsilon)$. Since $G_1$ is dense, there exists $x_1\\in B\\cap G_1$. Since both $B$ and $G_1$ are open, there exists a ball $B_1=B(x_1,r_1)$ such that $$\\overline{B_1}\\subseteq B\\cap G_1$$\nSince $G_2$ is open and dense, there is $x_2\\in B_1\\cap G_2$ and again an open ball $B_2=B(x_2,r_2)$ such that $\\overline{B_2}\\subseteq B_1\\cap G_2$, but we ask now that $r_2\\leq r_1/2$. We then successively take $r_{n+1}<\\frac{r_n}2$. Inductively, we see we can construct a sequence of closed bounded sets $F_n=\\overline{B_n}$ such that $$F_{n+1}\\subseteq F_n\\\\ \\operatorname{diam}D_n\\to 0$$\nSince $X$ is complete, there exists $\\alpha\\in \\displaystyle\\bigcap_{n\\in\\Bbb N}F_n$. But, by construction, we see that $\\displaystyle\\alpha\\in \\bigcap_{n\\in\\Bbb N}G_n\\cap B(x,\\epsilon)$\nThus $G$ is dense in $X$.$\\blacktriangle.$", "meta": {"post_id": 201922, "input_score": 39, "output_score": 34, "post_title": "Proof that a perfect set is uncountable"}}
{"input": "I'm self-studying Rudin's Real and Complex Analysis. I've finished the first two chapters so far, and I don't have any major problems understanding the definitions and theorems. I can prove the theorems on my own. Exercises in chapter 1 were OK, but I'm finding the exercises in chapter 2 to be very difficult. To give you an example, one exercise expects you to come up with the generalized Cantor set on your own. Another is a proof that was published in a journal.\nAre such exercises the best way to learn for a beginner? Or is it better to start with a simpler set of exercises that test your understanding of the material before you venture into more difficult things? Should I augment my study with another book that has easier exercises?\nI'm feeling frustrated and would like some guidance here. Thank you.", "output": "I think difficult exercises are essentially bad in general: they tend to discourage the student rather than check   the understanding of the material.     \nIt so happens that I too tried to read that book by Rudin in my fourth university year.\nI found the reading very hard going and there were many exercises I couldn't do: this affected my morale very negatively.\nRetrospectively, I find this book dreadful pedagogically and the worst offence is that there are no pictures : this is a mortal sin   in a book on a geometric subject like complex analysis.\n(In fairness I should add that I do use it as a reference now: it contains sophisticated beautiful results like the theorems of M\u00fcntz-Szasz and Mergelyan, which are not often  proved in books on holomorphic functions.)   \nOn a more constructive note, let me mention two great books you might consult:\n $\\bullet $ Lang's Real and Functional Analysis  which contains an astonishing wealth in material (including the Haar integral and Schwartz's distributions)\n$\\bullet \\bullet $ Remmert's Theory of Complex Functions , written by a genuine  master and containing, apart from a perfect technical treatment,  invaluable historical vignettes.   \nFinally, for exercises  proper, an excellent source is Schaum's Outlines series .\nThe books there are very user-friendly and  the exercises  quite  reasonable, with a progression from very easy to more demanding, accompanied by  clear, detailed  solutions.\nLook here for the dirt cheap  volume ($13.41 !) on Complex Variables.", "meta": {"post_id": 202102, "input_score": 47, "output_score": 44, "post_title": "Are difficult exercises good for beginners?"}}
{"input": "The problems that appear in difficult math competitions such as the IMO or the Putnam exam are usually very difficult and require some ingenuity to solve. They also usually don't look like they can be solved by simply knowing more advanced theory and the such.\nHow do people typically come up with these problems? Do they arise naturally from advanced mathematics (the somewhat infamous 'grasshopper problem' from the 2009 IMO comes to mind - to my not exactly knowledgeable mind this problem looks like it popped out of basically nowhere)? What is the perspective that mathematicians take when seemingly \"inventing\" these problems with no theoretical motivation to them whatsoever?", "output": "When I am working on my research, or on MO/math.SE questions, I often find myself thinking in a way which reminds me of the feel of solving Olympiad problems. If I then solve the problem, I try to find a very special case of my problem which is still challenging, and can be stated and solved using only material on the Olympiad curriculum. I then e-mail Kiran Kedlaya and say \"Hey Kiran, do you think this would be a good Olympiad problem?\" If he thinks so, he proposes it to the USAMO committe.\nI wrote Problem 2 on the 2010 USAMO in this way; it is Theorem 3.2 of this paper specialized to the case that $W_0$ is the group $S_n$. The fact that the \"total number of moves\" referred to in the theorem is at most $\\binom{n}{3}$ is computed in Section 5.2.\nI think I send about one problem a year; but most of them get rejected.\nUPDATE 2014 Problem B4 of the 2014 Putnam was mine. Let $F(x,y,z) = \\sum F_{ijk} x^i y^j z^k$ be a homogenous polynomial of degree $n$ with positive real coefficients. We say that $F$ is hyperbolic with respect to the positive orthant if, for all $(u_1,v_1,w_1)$ and $(u_2,v_2,w_2) \\in \\mathbb{R}_{> 0}^3$, the polynomial $f(t) = F(tu_1+u_2,tv_1+v_2,tw_1+w_2)$ has $n$ negative real roots.\nIn this paper, I show that there are constants $V_1$ and $V_2$ (dependent on $n$) so that,\n(1) if $F$ is hyperbolic with respect to the positive orthant, then $F_{i(j+1)(k+1)} F_{(i+1)j(k+1)} > V_1  F_{i(j+1)(k+1)} F_{(i+2)jk}$ and the same for all permutations of the indices\n(2) if  $F_{i(j+1)(k+1)} F_{(i+1)j(k+1)} > V_2  F_{i(j+1)(k+1)} F_{(i+2)jk}$  and the same for all permutations of the indices, then $F$ is hyperbolic with respect to the positive orthant.\nThe proof is nonconstructive; I also (Theorem 20) give an explicit value of $V_1$. I was thinking about whether I could give a concrete value for $V_2$. The problem was too hard, so I thought instead about homogenous polynomials in two variables, which is the same as inhomogenous polynomials in one variable. At this point, I was basically looking for a converse to Newton's inequality: I wanted a constant $C$ so that, if $a_k^2 > C a_{k-1} a_{k+1}$, then all the roots of $\\sum_{k=0}^n a_k z^k$ are real. The result in one variable wasn't worth publishing, but I figured I could make a nice problem by choosing a particular polynomial and asking people to prove the roots were real.\nUPDATE 2020 Problem 6 of the 2020 USOMO was mine. It is the key computation from this MO answer, specialized to the case that the matrix $X$ has rank one, so $X_{ij} = x_i y_j$. The rank one case turned out not to be easier than the general case, which is why it doesn't show up in that MO thread, but it is one of the things I thought about when working on that answer, and I noticed at the time that it looked like a strengthening of the rearrangement inequality.", "meta": {"post_id": 203301, "input_score": 54, "output_score": 55, "post_title": "How do people come up with difficult math Olympiad questions?"}}
{"input": "There was an interesting problem asked about triples $(x,y,z)$ which are solutions of \n$$x! = y! + z!.$$ \nHere $(2,1,1)$ is a solution because $2! = 1! + 1!$, as are $(2,1,0)$ and $(2,0,1)$. \nNow I wanted to analyze this a bit further and thought of using the gamma function definition of a factorial, to see where it led and this is what I got:\n$x! = y! + z!$\n$ \\Gamma(x) = \\Gamma(y) + \\Gamma(z) $\n$\\int_{-\\infty}^{\\infty}t^xe^{-t}dt =  \\int_{-\\infty}^{\\infty}t^ye^{-t}dt + \\int_{-\\infty}^{\\infty}t^ze^{-t}dt $\n$\\int_{-\\infty}^{\\infty}t^xe^{-t}dt - \\int_{-\\infty}^{\\infty}t^ye^{-t}dt - \\int_{-\\infty}^{\\infty}t^ze^{-t}dt = 0$\n$\\int_{-\\infty}^{\\infty}(t^x - t^y - t^z)e^{-t}dt = 0$\nNow my line of thinking was that, in a manner similar to the fundamental lemma of the calculus of variations, this should imply that the above integral can only be true for arbitrary values of $x, y$ and $z$ if $t^x - t^y - t^z = 0$. \nI'm not really sure if that is justifiable so I'd appreciate some comment on this. \nThe reason I continued on despite the uncertainty is that when you're left with the polynomial $t^x - t^y - t^z = 0$ you encounter a strange fact. First off, plugging in a triple like $(2,1,1)$ results in $t^2 - t^1 - t^1 = 0$ which implies $t^2 = 2t$ thus $t = 2$. Now plugging in $t = 2$ gives $2^2 = 1^2 + 1^2$. In other words you get what you'd expect. However plugging in a triple you know is inconsistent, like $(0,0,0)$, gives\n$t^0 = t^0 + t^0$\n$1 = 1 + 1$\n$1 = 2$.\nYou get absurdities when you plug in inconsistent triples.\nSo while $(2,1,0)$, which works, gives \n$t^2 - t^1 - t^0 = 0$ \n$t^2 - t - 1 = 0$\nwhose roots are\n$\\frac{1}{2} + \\frac{\\sqrt{5}}{2}$\n$\\frac{1}{2} - \\frac{\\sqrt{5}}{2}$\nand on plugging these into $t^x - t^y - t^z = 0$ gives a consistent equality for both roots, an inconsistent triple like $(3,2,1)$ leaves us with $t^3 - t^2 - t = 0$, one of whose roots are $t = 0$ (which gives a consistent equality $0^3 - 0^2 - 0 = 0$ whereas another one of it's roots, $t = \\frac{1}{2} + \\frac{\\sqrt{5}}{2}$ does not give a consistent equality \n$(\\frac{1}{2} + \\frac{\\sqrt{5}}{2})^3 - (\\frac{1}{2} + \\frac{\\sqrt{5}}{2})^2 - (\\frac{1}{2} + \\frac{\\sqrt{5}}{2}) = 0$. \nIf all of the above is magically okay you can use this idea to show that no triples $(n, n - 1, n - 2)$ will work for $n$ greater than $2$. I'd like to go further with it but I'm afraid it's all just conceptually flawed, is it?\nA more interesting question stems from something a great lecturer told me, she basically analyzed the whole question geometrically and said something about certain triples having to do with the area of those gamma integrals canceling out. I thought I understood what she was saying but I actually don't so I'd appreciate any comment on what this could mean.\nI hope I was clear enough, thanks for your time.", "output": "$x! = y! + z!$ does not have any solutions in integers with $x \\geq 3.$ As soon as $x \\geq 3,$ we have $(x-1)! \\leq x! / 3.$ With the necessary $y,z < x,$ we get $y! \\leq x! / 3, \\; \\; z! \\leq x! / 3,$ so $y! + z! \\leq 2 x! / 3$ and $y! + z! \\neq x!$", "meta": {"post_id": 206679, "input_score": 23, "output_score": 44, "post_title": "Integer solutions of $x! = y! + z!$"}}
{"input": "In this MO post, I ran into the following family of polynomials: $$f_n(x)=\\sum_{m=0}^{n}\\prod_{k=0}^{m-1}\\frac{x^n-x^k}{x^m-x^k}.$$\nIn the context of the post, $x$ was a prime number, and $f_n(x)$ counted the number of subspaces of an $n$-dimensional vector space over $GF(x)$ (which I was using to determine the number of subgroups of an elementary abelian group $E_{x^n}$).\nAnyway, while I was investigating asymptotic behavior of $f_n(x)$ in Mathematica, I got sidetracked and (just for fun) looked at the set of complex roots when I set $f_n(x)=0$.  For $n=24$, the plot looked like this: (The real and imaginary axes are from $-1$ to $1$.)\n\nSurprised by the unusual symmetry of the solutions, I made the same plot for a few more values of $n$.  Note the clearly defined \"tails\" (on the left when even, top and bottom when odd) and \"cusps\" (both sides).\n\nYou can see that after approximately $n=60$, the \"circle\" of solutions starts to expand into a band of solutions with a defined outline.  To fully absorb the weirdness of this, I animated the solutions from $n=2$ to $n=112$. The following is the result:\n\nPretty weird, right!?  Anyhow, here are my questions:\n\n\nFirst, has anybody ever seen anything at all like this before?\nWhat's up with those \"tails?\"  They seem to occur only on even $n$, and they are surely distinguishable from the rest of the solutions.\nLook how the \"enclosed\" solutions rotate as $n$ increases.  Why does this happen? [Explained in edits.]\nAnybody have any idea what happens to the solution set as $n\\rightarrow \\infty$?\n  Thanks to @WillSawin, we now know that all the roots are contained in an annulus that converges to the unit circle, which is fantastic.  So, the final step in understanding the limit of the solution sets is figuring out what happens on the unit circle.  We can see from the animation that there are many gaps, particularly around certain roots of unity; however, they do appear to be closing.\n  \n  \nThe natural question is, which points on the unit circle \"are roots in the limit\"? In other words, what are the accumulation points of $\\{z\\left|z\\right|^{-1}:z\\in\\mathbb{C}\\text{ and }f_n(z)=0\\}$?\nIs the set of accumulation points dense?  @NoahSnyder's heuristic of considering these as a random family of polynomials suggests it should be- at least, almost surely.\n\nThese are polynomials in $\\mathbb{Z}[x]$.  Can anybody think of a way to rewrite the formula (perhaps recursively?) for the simplified polynomial, with no denominator?  If so, we could use the new formula to prove the series converges to a function on the unit disc, as well as cut computation time in half. [See edits for progress.]\nDoes anybody know a numerical method specifically for finding roots of high degree polynomials?  Or any other way to efficiently compute solution sets for high $n$? [Thanks @Hooked!]\n\n\nThanks everyone.  This may not turn out to be particularly mathematically profound, but it sure is neat.\n\nEDIT: Thanks to suggestions in the comments, I cranked up the working precision to maximum and recalculated the animation.  As Hurkyl and mercio suspected, the rotation was indeed a software artifact, and in fact evidently so was the thickening of the solution set.  The new animation looks like this:\n\nSo, that solves one mystery: the rotation and inflation were caused by tiny roundoff errors in the computation.  With the image clearer, however, I see the behavior of the cusps more clearly.  Is there an explanation for the gradual accumulation of \"cusps\" around the roots of unity?  (Especially 1.)\n\nEDIT: Here is an animation $Arg(f_n)$ up to $n=30$.  I think we can see from this that $f_n$ should converge to some function on the unit disk as $n\\rightarrow \\infty$.  I'd love to include higher $n$, but this was already rather computationally exhausting.\n\nNow, I've been tinkering and I may be onto something with respect to point $5$ (i.e. seeking a better formula for $f_n(x)$).  The folowing claims aren't proven yet, but I've checked each up to $n=100$, and they seem inductively consistent.  Here denote $\\displaystyle f_n(x)=\\sum_{m}a_{n,m}x^m$, so that $a_{n,m}\\in \\mathbb{Z}$ are the coefficients in the simplified expansion of $f_n(x)$.\n\nFirst, I found $\\text{deg}(f_n)=\\text{deg}(f_{n-1})+\\lfloor \\frac{n}{2} \\rfloor$.  The solution to this recurrence relation is $$\\text{deg}(f_n)=\\frac{1}{2}\\left({\\left\\lceil\\frac{1-n}{2}\\right\\rceil}^2 -\\left\\lceil\\frac{1-n}{2}\\right\\rceil+{\\left\\lfloor \\frac{n}{2} \\right\\rfloor}^2 + \\left\\lfloor \\frac{n}{2} \\right\\rfloor\\right)=\\left\\lceil\\frac{n^2}{4}\\right\\rceil.$$\nIf $f_n(x)$ has $r$ more coefficients than $f_{n-1}(x)$, the leading $r$ coefficients are the same as the leading $r$ coefficients of $f_{n-2}(x)$, pairwise.\nWhen $n>m$, $a_{n,m}=a_{n-1,m}+\\rho(m)$, where $\\rho(m)$ is the number of integer partitions of $m$.  (This comes from observation, but I bet an actual proof could follow from some of the formulas here.)  For $n\\leq m$ the $\\rho(m)$ formula first fails at $n=m=6$, and not before for some reason.  There is probably a simple correction term I'm not seeing - and whatever that term is, I bet it's what's causing those cusps.\n\nAnyhow, with this, we can make almost make a recursive relation for $a_{n,m}$,\n$$a_{n,m}= \\left\\{\n     \\begin{array}{ll}\n       a_{n-2,m+\\left\\lceil\\frac{n-2}{2}\\right\\rceil^2-\\left\\lceil\\frac{n}{2}\\right\\rceil^2} & : \\text{deg}(f_{n-1}) < m \\leq \\text{deg}(f_n)\\\\\n       a_{n-1,m}+\\rho(m) & : m \\leq \\text{deg}(f_{n-1})  \\text{ and } n > m \\\\\n       ? & : m \\leq \\text{deg}(f_{n-1})  \\text{ and } n \\leq m\n     \\end{array}\n   \\right.\n$$\nbut I can't figure out the last part yet.\n\nEDIT:\nSomeone pointed out to me that if we write $\\lim_{n\\rightarrow\\infty}f_n(x)=\\sum_{m=0}^\\infty b_{m} x^m$, then it appears that $f_n(x)=\\sum_{m=0}^n b_m x^m + O(x^{n+1})$.  The $b_m$ there seem to me to be relatively well approximated by the $\\rho(m)$ formula, considering the correction term only applies for a finite number of recursions.\nSo, if we have the coefficients up to an order of $O(x^{n+1})$, we can at least prove the polynomials converge on the open unit disk, which the $Arg$ animation suggests is true.  (To be precise, it looks like $f_{2n}$ and $f_{2n+1}$ may have different limit functions, but I suspect the coefficients of both sequences will come from the same recursive formula.)  With this in mind, I put a bounty up for the correction term, since from that all the behavior will probably be explained.\n\nEDIT: The limit function proposed by Gottfriend and Aleks has the formal expression $$\\lim_{n\\rightarrow \\infty}f_n(x)=1+\\prod_{m=1}^\\infty \\frac{1}{1-x^m}.$$\nI made an $Arg$ plot of $1+\\prod_{m=1}^r \\frac{1}{1-x^m}$ for up to $r=24$ to see if I could figure out what that ought to ultimately end up looking like, and came up with this:\n\nPurely based off the plots, it seems not entirely unlikely that $f_n(x)$ is going to the same place this is, at least inside the unit disc.  Now the question is, how do we determine the solution set at the limit?  I speculate that the unit circle may become a dense combination of zeroes and singularities, with fractal-like concentric \"circles of singularity\" around the roots of unity...  :)", "output": "$\\lim_{n\\to\\infty} f_n(x)/n$ exists for $x$ in the open unit disc, and is equal to the partition generating function $P(x)$. Convergence is uniform on closed unit balls.\nTo prove this, we use the following lemma:\nLet $a_{n,m}$ be a double sequence of numbers such that $a_m= \\lim_{n\\to \\infty}a_{n,m}$ exists and $|a_{n,m}| \\leq |a_m|$.  Let $R$ be the radius of convergence of $\\sum_m a_m x^m$. Then for $x<R$, we have:\n$$ \\lim_{n\\to\\infty} \\left(\\sum_m a_{n,m} x^m \\right) = \\sum_m a_m x^m$$\nand the convergence is uniform on compact subsets.\nProof of lemma: We just need to interchange the summation and limit. This follows from the dominated convergence theorem, because $a_{n,m}x^m$ is bounded by $|a_m x^m|$, which is summable as long as $|x|<R$. Because $ \\sum_m |a_m x^m|$ is bounded uniformly on compact subsets of the open disc of radius $R$, the convergence is uniform there.\nNow to get the result we want that the coefficient of $x^m$ in $f_n(x)/n$ converges to, and is bounded by, the number of partitions of $m$. To do that, we just need a combinatorial interpretation for that coefficient. One way to see the point count formula for the Grassmanian is in terms of Schubert cells - an $m$-dimensional Schubert cell contains $q^m$ points over $\\mathbb F_q$. Because the Grassmanian decomposes into Schubert cells, this gives a polynomial formula for the number of points. The coefficient of $q^m$ would be the number of $m$-dimensional Schubert cells. From the explicit description, it is easy to see that, in the Grassmanian of $k$-planes in $n-k$-dimensional space, this is equal to the number of partitions of $m$ with at most $k$ parts and with each part at most $n-k$.\nSumming over $k$ from $0$ to $n$, we get the coefficient of $x^m$, which is at most $n+1$ times the number of partitions of $n$. (In fact $n-1$ works unless $m=0$, and the error from $m=0$ is neglibigible.) For $n$ large with respect to $m$, for most $k$ between $0$ and $n$, we have $k$ and $n-k$ are both larger than $m$, so the coefficient of $x^m$ in the $k$th term is equal to the number of partitions of $m$. Hence averaging over those terms, the limit is the number of partitions of $m$. So the hypotheses of the lemma are satisfied with $a_m=$ the partition function, and the conclusion is as well.\nThis also explains why the roots seem to live in an annulus - because the partition function has no roots in the open unit disc, as $f_n(x)/n$ converges to it, the absolute value of the smallest root must converge to $1$.\nBut we are not done, because we still haven't explained why there are no roots with large norm. Unfortunately there is no limiting distribution for large $x$. However, this is easy to fix - for $x$ larger than $1$, we just divide the polynomial by the leading term: $x^{\\lfloor \\frac{n^2}{4} \\rfloor}$. Then I claim the new, normalized $f_n$, converges to one of two limits, depending on whether $f$ is even or odd.\nWe can use essentially the same strategy to compute these limits. After changing variables to $y=1/x$, we see that the coefficients of $f_n(y^{-1}) / y^{ \\deg(f_n)}$ converge to a limit depending on if $n$ is even or odd, and are bounded by this limit.\nTo get this we just use the fact that the $k$th summand of $f_n$ is a symmetric - reversing the order of its coefficients gets the same polynomial. The degree of the $k$th term is $k(n-k)$. We are interested in the coefficient of terms close to the highest degree. These come only from $k$ close to $n/2$. The contribution from the $k$th term is $y^{\\lfloor \\frac{n^2}{4}\\rfloor -k (n-k)}$ times the polynomial for the number of $k$-dimensional spaces of an $n$-dimensional vector space. By the previous discussion, the coefficients of this polynomial converge to the partition function. \nSo the reversed coefficients of $f_n$ converge to the coefficients of:\n$$\\sum_{k \\in \\mathbb Z} y^{\\lfloor \\frac{n^2}{4}\\rfloor -k (n-k)} P(y)=\\left(\\sum_{k \\in \\mathbb Z} y^{\\lfloor \\frac{n^2}{4}\\rfloor -k (n-k)}\\right) P(y) =  $$\n$$\\left(\\sum_{k\\in\\mathbb Z} y^{k^2} \\right) P(y) $$\n($n$ even)\n$$\\left(\\sum_{k\\in\\mathbb Z} y^{k^2+k} \\right) P(y) $$\n($n$ odd)\nIn other words, it is a theta function times the partition function. This power series also has radius of convergence $1$, and, I believe, no roots, and its coefficients bound the coefficients of $f$, so by the same logic we get convergence.\nThen the final conclusion is that all the roots are contained in an annulus which converges to a circle of radius $1$. Theoretically one could even get an effective bound for this annulus.", "meta": {"post_id": 206890, "input_score": 515, "output_score": 106, "post_title": "\"The Egg:\" Bizarre behavior of the roots of a family of polynomials."}}
{"input": "How to prove the measurability of convex sets in $R^n$ ? I have seen a proof, but too long and not very intuitive.If you have seen any, please post it here.", "output": "Let $C$ be your convex set, and assume without loss of generality(1) that it contains zero as an interior point and is bounded.\nThe question boils down to showing that $\\partial C$ has measure zero(2), which can be shown by squeezing the boundary between the interior $C^\\circ$, and a slightly expanded version of the interior, $\\frac{1}{1-\\epsilon}C^\\circ$.\n\nLet $p \\in \\partial C$. Since $0$ is an interior point, by convexity the point $q:=(1-\\epsilon)p$ lies in the interior of the cone $K:=\\{sp + (1-s)x: x \\in B_r(0) \\}$, and therefore $q \\in C^\\circ$. But then $p=\\frac{1}{1-\\epsilon}q \\in \\frac{1}{1-\\epsilon}C^\\circ$. \n\nThus \n$$\\partial C \\subset \\frac{1}{1-\\epsilon}C^\\circ.$$\nSince for any set the boundary and the interior are disjoint,\n$$\\partial C \\subset \\frac{1}{1-\\epsilon}C^\\circ \\setminus C^\\circ.$$\nSince the interior of a convex set is convex(3) and $C^\\circ$ contains zero, $C^\\circ$ is contained in it's dilation:\n$$C^\\circ \\subset  \\frac{1}{1-\\epsilon}C^\\circ.$$\nFinally, since we have assumed $C^\\circ$ is bounded, the measure of the boundary,\n$$\\lambda(\\partial C) \\le \\lambda(\\frac{1}{1-\\epsilon}C^\\circ \\setminus C^\\circ) = (\\frac{1}{1-\\epsilon})^n\\lambda(C^\\circ)-\\lambda(C^\\circ),$$ \ncan be made as small as desired by taking $\\epsilon \\rightarrow 0$.\n\nTying up loose ends:\n(1):\n\nIf the set is not bounded, cut it off with a countable collection of successively larger balls. Since the countable union of measurable sets is measurable, this suffices.\nIf the set $C$ contains some interior point, translate the set so that the interior point is at zero. Since the Lebesgue measure is translation invariant, this suffices.\nIf the set $C$ contains no interior points, then all it's point must lie within a $n-1$ dimensional plane, otherwise $C$ would contain a n-tetrahedron (simplex), and a simplex contains interior points. Thus $C$ would lie within a measure zero set and the result is trivial.\n\n(2):\n\nThe boundary, closure, and interior of a set are always closed, closed, and open respectively, so they are always measurable.\nIf $\\partial C$ has measure zero, then $\\partial C \\cap C$ is measurable and has measure zero by completeness of the Lebesgue measure.\nOnce you have measurability of $\\partial C \\cap C$, you have measurability of $C$ since,\n$$C=(\\partial C \\cap C) \\cup C^\\circ.$$\n\n(3):\n\nThe proof that taking interiors preserves convexity is straightforward from the definitions but a little tedious. See lemma 4 here.\n\n\nEdit: To add, the approach in the answer here:\nWhy does a convex set have the same interior points as its closure? \nis similar to the reasoning in my post, and shines some light onto what's going on. The technique there could be adapted easily to prove the result here as well, and you would get a similar proof.", "meta": {"post_id": 207609, "input_score": 35, "output_score": 37, "post_title": "The measurability of convex sets"}}
{"input": "I know that the proof that every vector space has a basis uses the Axiom of Choice, or Zorn's Lemma. If we consider an axiom system without the Axiom of Choice, are there vector spaces that provably have no basis?", "output": "If you only consider a system without the axiom of choice you cannot prove that there is such vector space, simply because while you are not assuming AC -- it might still be true.\nHowever Andreas Blass proved in 1984 that if every vector space has a basis then the axiom of choice holds [1]. In particular it means that if you assume the axiom of choice fails then there is provably a space without a basis.\n\nSemi-constructively, the proof given by Blass uses the equivalence (in ZF) between the axiom of choice the axiom of multiple choice.\n\nThe axiom of multiple choice (AMC) asserts that given a family $\\cal A$ of non-empty sets, such that every $A\\in\\cal A$ has at least two elements, then there is a function $F$ such that $F(A)$ is a non-empty, finite, proper subset of $A$ for all $A\\in\\cal A$.\n\nBlass used this equivalence as follows: given a family of non-empty sets he defines a vector space using this family and by the existence of a basis he constructs $F$ showing that AMC holds.\nIf we assume the axiom of choice fails, then also AMC fails in this model. Therefore there is a family of sets each containing at least two elements, but there is no $F$ as required. Using this family we can construct the same vector space, but now we can prove that it has no basis. If it had a basis then Blass' proof would follow and a contradiction would be found.\nNote that this is semi-constructive since we cannot constructively point out a family of non-empty sets without a choice function, simply because it is consistent that there is none of those. However if we assume that the axiom of choice fails, then we can only infer that such family exists, give it a name and move along. For more see [2]. \nWe can assume \"anti-choice\" axioms which also tell us particular sets cannot be well-ordered (or families without choice functions), for example we may assume that the real numbers cannot be well-ordered or even a stronger assumption: we can directly assume that the real numbers do not have a basis over $\\mathbb Q$. Such assumptions are indeed consistent with ZF, but they are \"focused\" versions of the negation of the axiom of choice, they tell us a lot about how it fails.\n\nBibliography:\n\nAndreas Blass, Existence of bases implies the axiom of choice. Contemporary Mathematics vol. 31 pp. 31-33, 1984.\nAsaf Karagila, Which set is unwell-orderable? Mathematics StackExchange, Sep. 2012.", "meta": {"post_id": 207990, "input_score": 34, "output_score": 39, "post_title": "Vector Spaces and AC"}}
{"input": "My professor sort of skimmed through this concept, giving only the definition and one example (i.e. $(0,1) \\subset \\mathbb{R}$ vs. $(0,1) \\subset \\mathbb{R}^2$, where $(0,1)$ is open relative to $\\mathbb{R}$ but not $\\mathbb{R}^2$), but no real explanation. I understand, more or less, this particular example, but I'm having trouble understanding this more generally.\nCan somebody please intuitively explain this concept in the context of a metric space? (If it matters, we are using Rudin's Principles Of Mathematical Analysis).", "output": "In order to know what \"open relative to\" means, you have to first know what \"open\" means:\n\nA subset S of a metric space X is called open if, for every point p of S, there is a positive real number $\\epsilon$ such that every point in X of distance less than $\\epsilon$ from p lies in S.\n\nNow, if we're working in some metric space and consider a subset A of that space, you can, if you wish, disregard the rest of the space and think of A as a metric space in itself. The notion of \"open relative to A\" is what you get when you realize, looking at the above definition, that some subsets of A that are not open subsets of the larger space may still be open subsets of A.\nFor example, suppose you look at the interval I=[0,1] as a metric space in itself, and disregard the rest of the real line. Then the subset S=(0.5,1] may seem at first not to be an open set, because you think, \"Hey, there are points arbitrarily close to 1 that are not in S, so 1 is actually a boundary point!\" Well, yes... except that none of those arbitrarily-close points are actually in I, so from the point of view of I as a space in itself, they don't exist. Every point in I sufficiently close to 1 is also in S, so if we want to work in I alone, we have to concede that S is open.\nThe moral of this story is that sets aren't inherently open: the definition depends on what metric space they are considered as subsets of. If you are only talking about one space it's harmless to just write \"open,\" but otherwise you may have to be more specific. If you are considering both $\\mathbb{R}$ and I, for example, writing \"(0.5,1] is open\" is ambiguous. So instead you write \"(0.5,1]\" is open relative to I.\"", "meta": {"post_id": 210815, "input_score": 20, "output_score": 50, "post_title": "Could someone explain the concept of a set being \"open relative\" to another set?"}}
{"input": "I don't understand this one part in the proof for convergent sequences are bounded.\n\nProof:\nLet $s_n$ be a convergent sequence, and let $\\lim s_n = s$. Then taking $\\epsilon = 1$ we have:\n$n > N \\implies |s_n - s| < 1$\nFrom the triangle inequality we see that: $ n > N \\implies|s_n| - |s| < 1 \\iff |s_n| < |s| + 1$.\nDefine $M= \\max\\{|s|+1, |s_1|, |s_2|, ..., |s_N|\\}$. Then we have $|s_n| \\leq M$ for all $n \\in N$.\n\nI do not understand the defining $M$ part. Why not just take $|s| + 1$ as the bound, since for $n > N \\implies |s_n| < |s| + 1$?", "output": "$|s|+1$ is a bound for $a_n$ when $n > N$. We want a bound that applies to all $n \\in \\mathbb{N}$. To get this bound, we take the supremum of $|s|+1$ and all terms of $|a_n|$ when $n \\le N$. Since the set we're taking the supremum of is finite, we're guaranteed to have a finite bound $M$.", "meta": {"post_id": 213936, "input_score": 41, "output_score": 43, "post_title": "Prove: Convergent sequences are bounded"}}
{"input": "Well, I need a deformation retract from $GL_n^{+}(\\mathbb{R})$ to $SO(n)$\nHere is what I tried, let $A\\in GL_n^{+}(\\mathbb{R})$ $A=(A_1,\\dots,A_n)$ where $A_i$'s are column vectors, Recall that the Gram-Schmidt algorithm turns A into an orthogonal matrix by the following sequence of steps. First normalise $A_1$ (i.e. make it unit length) $A_1\\mapsto \\frac{A_1}{|A_1|}$ next I make $A_2$ orthogonal to $A_1$ like $A_2\\mapsto A_2-\\langle A_1,A_2\\rangle A_1$ and normalize $A_2\\mapsto \\frac{A_2}{|A_2|}$  like this up to $A_n$\nBut I am not getting an explicit homotopy which  gives me a deformation retract $GL_n^{+}(\\mathbb{R})$ to $SO(n)$", "output": "Here is a geometric way to see this. To any ordered basis $(v_1,v_2,\\ldots,v_n)$ of your vector space $V$ associate the \"flag\" of subspaces $V_0=\\{0\\}$, $V_1=\\langle v_1\\rangle$, $V_2=\\langle v_1,v_2\\rangle$, ... $V_n=\\langle v_1,v_2,\\ldots,v_n\\rangle=V$. The Gram-Schmidt algorithm turns any such basis into an orthonormal basis $(b_1,\\ldots,b_n)$ that gives rise to the same flag of subspaces. It is moreover the unique such basis (orthonomal and with the same flag) for which in addition each $b_i$, inside $V_i$, is on the same side of the hyperplane $V_{i-1}$ as the original basis vector $v_i$.\nNow taking $V=\\Bbb R^n$ we can identify $GL_n^+(\\Bbb R)$ with the set of ordered bases $(v_1,v_2,\\ldots,v_n)$ with $\\det(v_1,v_2,\\ldots,v_n)>0$, and $SO(n)$ with the set of ordered orthonormal bases $(b_1,b_2,\\ldots,b_n)$ with $\\det(b_1,b_2,\\ldots,b_n)>0$. Now for such a basis $(v_1,v_2,\\ldots,v_n)$ let $(b_1,\\ldots,b_n)$ be the orthonormal basis associated to it under Gram-Schmidt, and simultaneously (or successively if you prefer) deform every $v_i$ linearly to $b_i$, as $t\\mapsto (1-t)v_i+tb_i$. The intermediate vectors stay inside $V_i$, and since $b_i$ is on the same side as $v_i$, they never enter $V_{i-1}$. This means the deformed vectors stay linearly independent at all times, so the deformation takes place inside $GL_n(\\Bbb R)$. As the determinant cannot vanish anywhere we have $\\det(v_1,v_2,\\ldots,v_n)>0\\implies \\det(b_1,b_2,\\ldots,b_n)>0$ and we have a deformation retract of $GL_n^+(\\Bbb R)$ to $SO(n)$. It is in fact a strong deformation retract: elements of $SO(n)$ remain fixed.", "meta": {"post_id": 214784, "input_score": 16, "output_score": 44, "post_title": "deformation retract of $GL_n^{+}(\\mathbb{R})$"}}
{"input": "Can anybody please help me to prove this:\n\nLet $p$ be greater than or equal to $1$.\nShow that for the space $\\ell_p=\\{(u_n):\\sum_{n=1}^\\infty |u_n|^p<\\infty\\}$ of all $p$-summable sequences (with norm $||u||_p=\\sqrt[p]{\\sum_{n=1}^\\infty |u_n|^p}\\ )$, there is an inner product $<\\_\\,|\\,\\_> $ s.t. $||u||^2=<u\\,|\\,u>$ if and only if $p=2$.", "output": "Assuming we are working with the usual norm (as OP said in comments), suppose $\\ell_{p}$ is an Hilbert space. So its must satisfy for all $u,v$: $$2\\|u\\|_{p}^2 + 2\\|v\\|_{p}^2 = \\|u + v\\|_{p}^2 + \\|u - v\\|_{p}^2.$$\nAs suggested by martini, take $u=e_{1}=(1,0,...,0,...)$ and $v=e_{2}=(0,1,0,...,0,...)$. Hence, by the last equality, we have $$4=2^{\\frac{2}{p}}+2^{\\frac{2}{p}}$$\nNow you can solve the last inequality and verify that $p=2$.\nOn the other hand, if $p=2$, you can easily check that $\\ell_{2}$ is a Hilbert space.", "meta": {"post_id": 216306, "input_score": 28, "output_score": 36, "post_title": "$\\ell_p$ is Hilbert if and only if $p=2$"}}
{"input": "I'm looking for some intuition regarding universal covers of topological spaces. \n$\\textbf{Setup:}$ For a topological space $X$ with sufficient adjectives we can construct a/the simply connected covering space of it by looking at equivalence classes of paths at a given base point. We then can put a topology in the standard way done by Hatcher - an open set around an equivalence class of paths, say $[\\gamma]$  is the set of $[\\gamma\\cdot\\eta]$ where $\\eta$ is a path starting at $\\gamma(1)$ contained in $U$ open in $X$.\nHere are my questions:\nQ: I find this topological space, as constructed above, non-intuitive. Certainly I dont know how I would manipulate it and make topological arguments in it. What is the 'right' way of thinking about the topology here? Or is this construction useful solely for proving the existence of simply connected covers?\nQ: Often times it is tractable to construct a simply connected covering by ad-hoc methods (fancy guessing). The projective plane, torus, etc all spring to mind. By universality I know that the covering space obtained by any ad-hoc method is $\\textit{the}$ universal covering space obtained by the above method, so there is an isomorphism of these two. Is there a standard way to see this isomorphism? Being really concrete, say in the cases of $\\mathbb RP^2$, or $S^1\\times S^1$.\nIn simple terms: how can I 'see' what the universal cover looks like from the general construction?", "output": "Suppose $p:\\overline{C}\\to C$ is a universal covering. By definition, around every point in $C$ is an open set that lifts up to $\\overline{C}$. So, locally, $\\overline{C}$ looks just like $C$. Suppose one wanted to cut up $C$ into small (contractible) patches and then stitch them together again to form $\\overline{C}$ - the problem is that $\\overline{C}$ is to be simply connected, so if (say) we started stitching patches around a nontrivial loop in $C$ when we wrap back around to the basepoint we can't stitch that last patch back to the original patch, instead we have to create a copy of the original patch and continue on from there.\nConsider the space $C=\\Bbb C\\setminus\\{0\\}$. If one takes a counterclockwise loop from $-1$ around $0$ back to itself, then the last patch cannot be stitched to the first, so we should make a copy of the original patch to stitch it to. In the picture below, we've literally lifted the copy above the original:\n$\\hskip 2in$ \nIf we continue this process indefinitely, then there will lots of copies of pieces of $C$ that are being stitched together. Given a point in $C$ in a patch, there will be many copies of that patch in our quilt, and so many lifts of that point - what allows us to distinguish between lifts of the same point is how we got to it from the original basepoint. Thus, we can interpret points in $\\overline{C}$ as points in the original space $C$ but with a \"memory\" of how we got there from a basepoint.\nThis inspires us to formalize our construction by letting elements of $\\overline{C}$ be paths in $C$, modulo endpoint-fixing homotopy. Points in $\\overline{C}$ are specified by points in $C$ with a memory of how we got there from the basepoint, so if we got to $x\\in C$ via a path $\\gamma$ in $C$ and $U$ is any basic nbhd of $x\\in C$, then the lift $\\overline{U}$ of that nbhd is comprised of points $\\overline{u}$, and to specify these $\\overline{u}$ we must say which points of $C$ they are (done: they lie above $U$) and how we got to them. We got to these points in $\\overline{U}$ by first travelling along $\\gamma$ from the basepoint to $x$ and then wiggled around within $U$ itself.\nAs for your other question, try lifting the paths. Say that $D\\to C$ is a covering, where $D$ is a familiar space you know well, and in particular you know $D$ is simply connected. Our construction of $\\overline{C}$ is comprised of paths emanating from (say) $x\\in C$. To see what the corresponding point of $D$ is, just lift the path from $C$ to $D$ and look at its endpoint! This is the isomorphism.", "meta": {"post_id": 216376, "input_score": 27, "output_score": 36, "post_title": "Universal cover via paths vs. ad hoc constructions"}}
{"input": "The $p$-norm on $\\mathbb R^n$ is given by $\\|x\\|_{p}=\\big(\\sum_{k=1}^n\n|x_{k}|^p\\big)^{1/p}$. For $0 < p < q$ it can be shown that $\\|x\\|_p\\geq\\|x\\|_q$ (1, 2). It appears that in $\\mathbb{R}^n$ a number of opposite inequalities can also be obtained. In fact, since all norms in a finite-dimensional vector space are equivalent, this must be the case. So far, I only found the following: $\\|x\\|_{1} \\leq\\sqrt n\\,\\|x\\|_{2}$(3),  $\\|x\\|_{2} \\leq \\sqrt n\\,\\|x\\|_\\infty$ (4). Geometrically, it is easy to see that opposite inequalities must hold in $\\mathbb R^n$. For instance, for $n=2$ and $n=3$ one can see that for $0 < p < q$, the spheres with radius $\\sqrt n$ with $\\|\\cdot\\|_p$ inscribe spheres with radius $1$ with $\\|\\cdot\\|_q$.\nIt is not hard to prove the inequality (4). According to Wikipedia, inequality (3) follows directly from Cauchy-Schwarz, but I don't see how. For $n=2$ it is easily proven (see below), but not for $n>2$. So my questions are:\n\nHow can relation (3) be proven for arbitrary $n\\,$?\nCan this be generalized into something of the form $\\|x\\|_{p} \\leq C \\|x\\|_{q}$ for arbitrary $0<p < q\\,$?\nDo any of the relations also hold for infinite-dimensional spaces, i.e. in $l^p$ spaces?\n\n\nNotes:\n$\\|x\\|_{1}^{2} = |x_{1}|^2 + |x_{2}|^2 + 2|x_{1}||x_{2}| \\leq |x_{1}|^2 + |x_{2}|^2 + \\big(|x_{1}|^2 + |x_{2}|^2\\big) = 2|x_{1}|^2 + 2|x_{2}|^2$, hence  $=2\\|x\\|_{2}^{2}$\n$\\|x\\|_{1} \\leq \\sqrt 2 \\|x\\|_{2}$.\nThis works because $|x_{1}|^2 + |x_{2}|^2 \\geq 2|x_{1}\\|x_{2}|$, but only because $(|x_{1}| - |x_{2}|)^2 \\geq 0$, while for more than two terms $\\big(|x_{1}| \\pm |x_{2}| \\pm \\dotsb \\pm |x_{n}|\\big)^2 \\geq 0$ gives an inequality that never gives the right signs for the cross terms.", "output": "Using Cauchy\u2013Schwarz inequality we get for all $x\\in\\mathbb{R}^n$\n$$\n\\Vert x\\Vert_1=\n\\sum\\limits_{i=1}^n|x_i|=\n\\sum\\limits_{i=1}^n|x_i|\\cdot 1\\leq\n\\left(\\sum\\limits_{i=1}^n|x_i|^2\\right)^{1/2}\\left(\\sum\\limits_{i=1}^n 1^2\\right)^{1/2}=\n\\sqrt{n}\\Vert x\\Vert_2\n$$\nSuch a bound does exist. Recall H\u00f6lder's inequality \n$$\n\\sum\\limits_{i=1}^n |a_i||b_i|\\leq\n\\left(\\sum\\limits_{i=1}^n|a_i|^r\\right)^{\\frac{1}{r}}\\left(\\sum\\limits_{i=1}^n|b_i|^{\\frac{r}{r-1}}\\right)^{1-\\frac{1}{r}}\n$$\nApply it to the case $|a_i|=|x_i|^p$, $|b_i|=1$ and $r=q/p>1$\n$$\n\\sum\\limits_{i=1}^n |x_i|^p=\n\\sum\\limits_{i=1}^n |x_i|^p\\cdot 1\\leq\n\\left(\\sum\\limits_{i=1}^n (|x_i|^p)^{\\frac{q}{p}}\\right)^{\\frac{p}{q}}\n\\left(\\sum\\limits_{i=1}^n 1^{\\frac{q}{q-p}}\\right)^{1-\\frac{p}{q}}=\n\\left(\\sum\\limits_{i=1}^n |x_i|^q\\right)^{\\frac{p}{q}} n^{1-\\frac{p}{q}}\n$$\nThen\n$$\n\\Vert x\\Vert_p=\n\\left(\\sum\\limits_{i=1}^n |x_i|^p\\right)^{1/p}\\leq\n\\left(\\left(\\sum\\limits_{i=1}^n |x_i|^q\\right)^{\\frac{p}{q}} n^{1-\\frac{p}{q}}\\right)^{1/p}=\n\\left(\\sum\\limits_{i=1}^n |x_i|^q\\right)^{\\frac{1}{q}} n^{\\frac{1}{p}-\\frac{1}{q}}=\\\\=\nn^{1/p-1/q}\\Vert x\\Vert_q\n$$\nIn fact $C=n^{1/p-1/q}$ is the best possible constant.\nFor infinite dimensional case such inequality doesn't hold. For explanation see this answer.", "meta": {"post_id": 218046, "input_score": 100, "output_score": 137, "post_title": "Relations between p norms"}}
{"input": "Volterra operator is defined as operator $V:L^2[0,1]\\rightarrow L^2[0,1]$ by \n\\begin{eqnarray}\n(V)(f(x))=\\int_0^xf(y)dy\n\\end{eqnarray}\nWould you help me to prove that this operator is compact but has no eigenvalues.", "output": "Note that\n$$\nVf(x)=\\int_0^1f(t)k(x,t)\\,dt,\n$$\nwhere $k(x,t)=1_{[0,x]}(t)$. It is a general fact that such an operator is Hilbert-Schmidt (and in particular compact) if and only if $k\\in L^2([0,1]^2)$. Or one can show that the measurable function $k$ is a uniform limit of simple functions, and these simple functions can be used as kernels to define operators that approximate $V$. As these operators are finite-rank, $V$ is compact.\nAs for the eigenvalues, if $\\lambda\\ne0$ and $Vf=\\lambda f$, then we get\n$$\\tag{1}\nf(x)=\\frac1\\lambda\\,\\int_0^xf(t)\\,dt.\n$$\nUsing that $f$ is in $L^2$ we have, for $x<y$,\n\\begin{align}\n|f(y)-f(x)|&=\\frac1{|\\lambda|}\\,\\left|\\int_x^yf(t)dt\\right|\\leq\\frac1{|\\lambda|}\\,\\int_x^y|f(t)|dt=\\frac1{|\\lambda|}\\,\\int_0^1|f(t)|\\,1_{[x,y]}(t)\\,dt\\\\ &\\leq\\frac{\\|f\\|_2}{|\\lambda|}\\,\\left(\\int_0^1(1_{[x,y]})^2\\,dt\\right)^{1/2}=\\frac{\\|f\\|_2}{|\\lambda|}\\,\\sqrt{y-x}.\n\\end{align}\nSo $f$ is continuous. Then looking at (1) again we get that $f$ is differentiable; thus, after differentiation, (1) is the equation $f=\\lambda f'$. This implies that $f(t)=c\\,e^{ t/\\lambda}$. But $f(0)=0$, so the only solution for (1) is $f=0$, and  $\\lambda$ cannot be an eigenvalue.\nThe case $\\lambda =0$ is trivial: if $Vf=0$, then  $f=0$.", "meta": {"post_id": 219699, "input_score": 14, "output_score": 41, "post_title": "Volterra Operator is compact but has no eigenvalue"}}
{"input": "In a lecture note from MIT on number theory says: \nTheorem 5. The greatest common divisor of a  and b  is equal to the smallest positive linear combination of a  and b.\nFor example, the greatest common divisor of 52 and 44 is 4. And, sure enough, 4 is a\nlinear combination of 52 and 44: \n6 \u00b7 52 + (\u22127) 44 = 4\nWhat about 12 and 6 their gcd is 6 but 0 which is less than 6 can be", "output": "You wrote it yourself: the gcd is the smallest positive linear combination. Smallest positive linear combination is shorthand for smallest positive number which is a linear combination. It is true that $0$ is a linear combination of $12$ and $6$ with integer coefficients, but $0$ is not positive.  \nThe proof is not difficult, but it is somewhat lengthy. We give full detail below. \nLet $e$ be the smallest positive linear combination $as+bt$ of $a$ and $b$, where $s$ and $t$ are integers. Suppose in particular that $e=ax+by$.\nLet $d=\\gcd(a,b)$. Then $d$ divides $a$ and $b$, so it divides $ax+by$. Thus $d$ divides $e$, and therefore in particular $d\\le e$.\nWe show that in fact $e$ is a common divisor of $a$ and $b$, which will imply that $e\\le d$. That, together with our earlier $d\\le e$, will imply that $d=e$.\nSo it remains to show that $e$ divides $a$ and $e$ divides $b$. We show that $e$ divides $a$. The proof that $e$ divides $b$ is essentially the same.\nSuppose to the contrary that $e$ does not divide $a$. Then when we try to divide $a$ by $e$, we get a positive remainder. More precisely,\n$$a=qe+r,$$\nwhere $0\\lt r\\lt e$. Then\n$$r=a-qe=a-q(ax+by)=a(1-qx)+b(-qy).$$\nThis means that $r$ is a linear combination of $a$ and $b$, and is positive and less than $e$. This contradicts the fact that $e$ is the smallest positive linear combination of $a$ and $b$.", "meta": {"post_id": 219941, "input_score": 22, "output_score": 51, "post_title": "Is greatest common divisor of two numbers really their smallest linear combination?"}}
{"input": "Let $G,H$ be finite groups. Suppose we have an epimorphism $$G\\times G\\rightarrow H\\times H$$  Can we find an epimorphism $G\\rightarrow H$?", "output": "Let $G=Q_8\\times D_8$, where $Q_8$ is the quaternion group and $D_8$ is the dihedral group of order $8$.\nLet $f$ be an isomorphism $$f:G\\times G =\\left(Q_8\\times D_8\\right)\\times \\left(Q_8\\times D_8\\right)\\longrightarrow \\left(Q_8\\times Q_8\\right)\\times \\left(D_8\\times D_8\\right).$$\nNow, let $\\mu$ and $\\lambda$ be epimorphisms $$\\begin{eqnarray*}\\mu:Q_8\\times Q_8&\\longrightarrow&Q_8 {\\small \\text{ Y }} Q_8\\\\ \\lambda:D_8 \\times D_8&\\longrightarrow&D_8 {\\small \\text{ Y }}D_8\\end{eqnarray*}$$\nwhere $A {\\small \\text{ Y }} B$ denotes the central product of $A$ and $B$.  Then $$\\mu\\times \\lambda:\\left(Q_8\\times Q_8\\right)\\times \\left(D_8\\times D_8\\right)\\longrightarrow \\left(Q_8 {\\small \\text{ Y }}Q_8\\right)\\times \\left(D_8 {\\small \\text{ Y }}D_8 \\right)$$\nis an epimorphism.  The key is that $D_8{\\small \\text{ Y }} D_8\\cong Q_8{\\small \\text{ Y }} Q_8$, so if we take an isomorphism $$\\phi:D_8{\\small \\text{ Y }} D_8\\longrightarrow Q_8{\\small \\text{ Y }} Q_8,$$ then we can take $H=Q_8{\\small \\text{ Y }} Q_8$ and form an isomorphism\n$$1_H\\times \\phi:\\left(Q_8 {\\small \\text{ Y }}Q_8\\right)\\times \\left(D_8 {\\small \\text{ Y }}D_8 \\right)\\longrightarrow \\left(Q_8 {\\small \\text{ Y }}Q_8\\right)\\times \\left(Q_8 {\\small \\text{ Y }}Q_8 \\right)=H\\times H.$$\nSo, all in all, we have \n$$\\newcommand{\\ra}[1]{\\kern-1.5ex\\xrightarrow{\\ \\ #1\\ \\ }\\phantom{}\\kern-1.5ex}\n\\newcommand{\\ras}[1]{\\kern-1.5ex\\xrightarrow{\\ \\ \\smash{#1}\\ \\ }\\phantom{}\\kern-1.5ex}\n\\newcommand{\\da}[1]{\\bigg\\downarrow\\raise.5ex\\rlap{\\scriptstyle#1}}\n\\begin{array}{c}\n\\left(Q_8\\times D_8\\right) \\times \\left( Q_8 \\times D_8 \\right)& \\ra{f} &\\left(Q_8\\times Q_8\\right) \\times \\left( D_8 \\times D_8 \\right)&\\\\\n& & \\da{\\mu\\times \\lambda} & &  & & \\\\\n& & \\left(Q_8 {\\small \\text{ Y }}Q_8\\right)\\times \\left(D_8 {\\small \\text{ Y }}D_8\\right) & \\ras{1_H\\times \\phi} & \\left(Q_8 {\\small \\text{ Y }}Q_8\\right)\\times \\left(Q_8 {\\small \\text{ Y }}Q_8\\right)\n\\end{array}\n$$\nand thus an epimorphism $$f(\\mu\\times\\lambda)(1_H\\times \\phi):G\\times G\\longrightarrow H\\times H.$$\nHowever, $Q_8{\\small\\text{ Y }}Q_8$ is not a homomorphic image of $Q_8\\times D_8$. So this is a counterexample.\n\nAppendix.\n\nCredit and thanks to Peter Sin for his help with the crucial step in this answer.\nSee Prop. 3.13 of these notes (\"The Theory of $p$-groups by David A. Craven\", in case the link breaks again) for a proof that $Q_8 {\\small \\text{ Y }} Q_8\\cong D_8 {\\small \\text{ Y }} D_8 \\not\\cong Q_8 {\\small \\text{ Y }} D_8$.", "meta": {"post_id": 221152, "input_score": 184, "output_score": 86, "post_title": "Can we ascertain that there exists an epimorphism $G\\rightarrow H$?"}}
{"input": "According to Wikipedia,\n\nThe collection of all algebraic objects of a given type will usually\nbe a proper class. Examples include the class of all groups, the class\nof all vector spaces, and many others. In category theory, a category\nwhose collection of objects forms a proper class (or whose collection\nof morphisms forms a proper class) is called a large category.\n\nI am aware of Russell's Paradox, which explains why not everything is a set, but how can we show the collection of all groups is a proper class?", "output": "The collection of singletons is not a set. Therefore the collection of all trivial groups is not a set.\nIf you wish to consider \"up to isomorphism\", note that for every infinite cardinal $\\kappa$ you can consider the free group, or free abelian group with $\\kappa$ generators. These are distinct (up to isomorphism, that is), and since the collection of cardinals is not a set the collection of groups cannot be a set either.", "meta": {"post_id": 226413, "input_score": 28, "output_score": 39, "post_title": "Why is the collection of all groups a proper class rather than a set?"}}
{"input": "Would someone be so kind to explain this to me:\n$$\\pi_nk=\\left\\{\\begin{array}{cl}1&\\textrm{if }k=\\arg\\min_j\\left\\Vert\\mathbf x_n-\\mu_j\\right\\Vert^2\\\\0&\\textrm{otherwise}\\end{array}\\right..$$\nEspecially the $\\arg\\min$ part.\n(It's from the $k$-means algorithm.)", "output": "$\\arg \\min$ (or $\\arg \\max$) return the input(s) for which the output is minimum (or maximum).\nFor example:\nThe graph illustrates the function $f(x)=2 \\sin(x-0.5)+\\cos(x)^2$.\nThe global minimum of $f(x)$ is $\\min(f(x)) \\approx -2$, while $\\arg \\min f(x)  \\approx 4.9$.", "meta": {"post_id": 227626, "input_score": 84, "output_score": 53, "post_title": "Meaning of \u201carg min\u201d"}}
{"input": "I am having some difficulties understanding the difference between simplicial and singular homology. I am aware of the fact that they are isomorphic, i.e. the homology groups are in fact the same (and maybe this doesnt't help my intuition), but I am having trouble seeing where in the setup they differ.\nTo my understanding, the singular chain complex on a space $X$ consists of the free abelian groups generated by the sets of $n$-simplices in X, where an $n$-simplex in this context is a continuous map $\\sigma : \\Delta^n \\to X$ from the standard geometric $n$-simplex $\\Delta^n$ to $X$, with boundary map $\\partial_n = \\sum_{i=0}^n (-1)^i d_i$ where $d_i: C_n (X) \\to C_{n-1} (X)$ is the $i$th face map (\"deleting\" the $i$th vertex).\nThe singular homology groups are then the homology groups of this complex (ie. $H_n(X)=\\ker(\\partial_n)/\\text{im}(\\partial_{n+1})$).\nNow for the simplicial homology, we have a simplicial complex $S$, which is a set of (abstract?) ordered simplices, such that a face of any simplex in $S$ is itself a simplex in $S$. Then we form the simplicial chain complex where $C_n(S) = \\mathbb{Z}[S_n]$, where $S_n \\subset S$ is the set of $n$-simplices in $S$, i.e. the free abelian group generated by $S_n$. This complex has boundary operator $\\partial_n = \\sum_{i=0}^n (-1)^i d_i$, where $d_i$ is the $i$th face map. The homology groups of this is $H^\\Delta_n(S) = \\ker(\\partial_n) / \\text{im}(\\partial_{n+1})$.\nNow for this to make any sense in a topological framework, we have the realization of $S$, $|S| = \\coprod (S_n \\times \\Delta^n) / (d_i \\sigma, y) \\sim (\\sigma, d^iy)$ for all $(\\sigma, y) \\in S_n \\times \\Delta^{n-1}$ and $d^i$ is the coface map.\n(As I understand it, $S$ is a blueprint of how to \"assemble\" the geometric $n$-simplices to form a space).\nAnd then of course, if you want to talk about a specific space $X$, you need to find a simplicial complex $S$, whose realization is homeomorphic to $X$.\nI can see very well that these two are two very different ways of building up the framework, but what I don't understand is where in practice it differs. Don't they both require that you find a way to divide $X$ into $n$-simplices? The only difference I see, is whether you map from $\\Delta^n$ into $X$ before or after you form your homology groups, but there must be something I'm missing...", "output": "Just to sum up, mostly for my own reference, but I thought others might find it useful. (I am new to the site, so please excuse me if this shouldn't be an answer...)\nFirst some preliminary notions:\nFor a topological space $X$, an $n$-simplex in $X$ is a continuous map $\\Delta^n \\to X$ from the standard geometric $n$-simplex $\\Delta^n$ into $X$. The maps $d^i: \\Delta^{n-1} \\to \\Delta^{n}$, sends $\\Delta^{n-1}$ to the face of $\\Delta^n$ sitting opposite the $i$th vertex of $\\Delta^n$.\nAn ordered $n$-simplex is a partially ordered set $n_+ = \\{ 0 < 1 < \\cdots < n \\}$. The $n+1$ elements of $n_+$ are called the vertices of $\\sigma$. The subsets of $n_+$ are called the faces of $\\sigma$. There are morphisms of simplices $d^i: (n-1)_+ \\to n_+$ called coface maps, given by $d^i((n-1)_+) = \\{ 0 < 1 < \\dots < \\hat{i} < \\cdots < n \\}$ omitting the $i$th vertex of $n_+$.\nThen for the two homologies:\nThe singular (unreduced) chain complex on a space $X$, is the chain complex\n$$\\cdots \\xrightarrow{\\partial_{n+1}} C_n(X) \\xrightarrow{\\partial_n} C_{n-1}(X) \\xrightarrow{\\partial_{n-1}} \\cdots C_1(X) \\xrightarrow{\\partial_1} C_0(X) \\to 0$$\nwhere $C_n(X)$ is the free abelian group $\\mathbb{Z}[S_n(X)]$ generated by the set $S_n(X) = \\{ \\sigma : \\Delta^n \\to X \\}$ of all $n$-simplices in $X$ (i.e. the set of all continuous maps $\\Delta^n \\to X$). The boundary maps $\\partial_n : C_n(X) \\to C_{n-1}(X)$ is given by $\\partial_n (\\sigma) = \\sum_{i=0}^{n}(-1)^i \\sigma d^i : \\Delta^{n-1} \\to \\Delta^n \\to X$.\nThe $n$th homology group $H_n(X) = \\ker(\\partial_n) / \\text{im}(\\partial_{n+1})$ of this complex is the $n$th singular homology group of $X$.\nA simplicial complex $S$ is a set $S = \\bigcup_{n=0}^{\\infty} S_n$ where $S_n = S(n_+)$ being a set of ordered $n$-simplices, such that a face of any simplex in $S$ is itself a simplex in $S$. The simplicial chain complex\n$$\\cdots \\xrightarrow{\\partial_{n+1}} C_n(S) \\xrightarrow{\\partial_n} C_{n-1}(S) \\xrightarrow{\\partial_{n-1}} \\cdots C_1(S) \\xrightarrow{\\partial_1} C_0(S) \\to 0$$\nconsists of the free abelian groups $C_n(S) = \\mathbb{Z}[S_n]$ generated by the $n$-simplices. The boundary map $\\partial_n : C_n(S) \\to C_{n-1}(S)$ is given by $\\partial_n(\\sigma) = \\sum_{i=0}^n (-1)^i d_i \\sigma$ where $d_i = S(d^i) : S_n \\to S_{n-1}$ is the face maps $d_i(\\sigma) = \\sigma \\circ d^i$.\nThe $n$th homology groups of this complex $H^\\Delta_n(S) = \\ker(\\partial_n) / \\text{im}(\\partial_{n+1})$ is the $n$th simplicial homology group of $S$.\nLastly we have the realization of $S$, $|S| = \\coprod (S_n \\times \\Delta^n) / \\left((d_i \\sigma, y) \\sim (\\sigma, d^iy) \\right)$ for all $(\\sigma, y) \\in S_n \\times \\Delta^{n-1}$, where $d_i \\sigma \\times \\Delta^{n-1}$ is identified with the $i$'th face of $\\sigma \\times \\Delta^n$.\nThen if you want to say something about a specific space $X$, you need to find a simplicial complex $S$, whose realization is homeomorphic to $X$ (i.e. you triangulate $X$ and find the homology groups of the resulting simplicial complex).\nNOTE: Feel free to edit any mistakes and clarify where you find it necessary. I'm still not 100% comfortable with it yet..", "meta": {"post_id": 229036, "input_score": 62, "output_score": 35, "post_title": "Difference between simplicial and singular homology?"}}
{"input": "I read recently that you can find the number of digits in a number through the formula $\\lfloor \\log_{10} n \\rfloor +1$ What's the logic behind this rather what's the proof?", "output": "Suppose that $n$ has $d$ digits; then $10^{d-1}\\le n<10^d$, because $10^d$ is the smallest integer with $d+1$ digits. Now take logs base $10$: $\\log_{10}10^k=k$, so the inequality becomes $d-1\\le\\log_{10}n<d$. If you now take the floor (= integer part) of $\\log_{10}n$, throwing away everything to the right of the decimal point, you get $\\lfloor\\log_{10}n\\rfloor=d-1$. Thus, $d=\\lfloor\\log_{10}n\\rfloor+1$. This isn\u2019t quite what you posted, but it\u2019s the integer part of it, and clearly the number of digits must be an integer.", "meta": {"post_id": 231742, "input_score": 47, "output_score": 75, "post_title": "Proof: How many digits does a number have? $\\lfloor \\log_{10} n \\rfloor +1$"}}
{"input": "I have a question about the following property, which I didn't know so far: \n\nWhy does the It\u014d integral have zero expectation? Is this true for every integrator and integrand? Or is this restricted to special processes, i.e. is\n  $$\\mathbb{E}\\left[\\int f \\, \\mathrm{d}M\\right]=0$$\n  for all local Martingales $M$ and predictable $f$, such that the integral is well defined? \n\nThank you for clarification.", "output": "This statement is wrong in general.\nIt may fail even when the integrator $M_t$ is a Brownian motion. In fact,\n\nGiven a probability distribution $P$ on $\\mathbb{R}$, it is possible\n  to find an adapted $t$-measurable process $f(\\omega,t)$, with\n  $\\mathbb{P}\\left(\\int_0^1 f^2(\\omega,t)\\,dt<\\infty\\right)=1$ such\n  that the random variable $$\\int_0^1 f(\\omega,t) \\, dB_t$$ has\n  distribution $P$.\n\nThis statement is known as Dudley's representation theorem (see the original paper). Hence, the expectation of the stochastic integral may take any real value, be infinite or not exist at all. \n\nAnother counterexample arises from the stochastic differential equation $$dX_t = X^2_t\\, dB_t, \\quad X_0=x, \\quad \\textrm{where } x>0.$$\nIt may be shown that the solution exists, is unique, is a strictly positive local martingale, but $\\mathbb{E} X_t \\to 0$ as $t\\to \\infty$.\nSee the details in George Lowther's blog, where this example is taken from.\n\nA sufficient condition for the integral $\\int_0^t f(\\omega, s)\\, dB_s$ to be a martingale on $[0,T]$ is that\n\n$f(\\omega,s)$ is adapted, measurable in s, and\n$\\mathbb{E}\\left(\\int_0^T f^2(\\omega,s)\\,ds\\right) < \\infty$.\n\nIn this case, indeed, $\\mathsf{E} \\left(\\int_0^T f(\\omega,s)\\, dB_s\\right)=0$.\n\nIf the integrator $M_t$ is an arbitrary martingale, and the integrand $f$ is bounded, then the integral is a martingale, and the expectation of the integral is again zero (proof).\n\nFinally, if the integrator $M_t$ is a local martingale, very little can be said about the expectation of the integral. If $f(\\omega,t)$ is sufficiently nice, the integral $\\int_0^t f(\\omega,s) \\, dM_s$ is a local martingale, but that does not guarantee that the expectation is zero, as the second counterexample above shows.", "meta": {"post_id": 232932, "input_score": 34, "output_score": 36, "post_title": "It\u014d Integral has expectation zero"}}
{"input": "What is the difference, if any, between kernel and null space?\nI previously understood the kernel to be of a linear map and the null space to be of a matrix: i.e., for any linear map $f : V \\to W$,\n$$\n\\ker(f) \\cong \\operatorname{null}(A),\n$$\nwhere\n\n$\\cong$ represents isomorphism with respect to $+$ and $\\cdot$, and\n$A$ is the matrix of $f$ with respect to some source and target bases.\n\nHowever, I took a class with a professor last year who used $\\ker$ on matrices. Was that just an abuse of notation or have I had things mixed up all along?", "output": "The terminology \"kernel\" and \"nullspace\" refer to the same concept, in the context of vector spaces and linear transformations. It is more common in the literature to use the word nullspace when referring to a matrix and the word kernel when referring to an abstract linear transformation. However, using either word is valid. Note that a matrix is a linear transformation from one coordinate vector space to another. Additionally, the terminology \"kernel\" is used extensively to denote the analogous concept as for linear transformations for morphisms of various other algebraic structures, e.g. groups, rings, modules and in fact we have a definition of kernel in the very abstract context of abelian categories.", "meta": {"post_id": 235350, "input_score": 59, "output_score": 59, "post_title": "What is the difference between kernel and null space?"}}
{"input": "I want to ask that, what is a support function intuitively. It is defined as:\n$$\\sup_{z \\in K} \\langle z, x \\rangle$$ where $z \\in K$, $K$ is a nonempty set. In this formulation, $\\langle \\cdot, \\cdot \\rangle$ is inner product.\nAs a function of $x$, what does it mean? Why it can be useful for instance? Thanks.", "output": "If you take $K$ to be convex, the support function is, in some sense, a tool for a dual representation of the set as the intersection of half-spaces.\nLet's assume that we're in $\\mathbb R^n$ for simplicity. A hyperplane can be characterized by a direction $\\boldsymbol x\\in\\mathbb R^n$ and a scalar $b\\in\\mathbb R$, let's write $H=(\\boldsymbol x;b)$ one such hyperplane, the set of points $\\boldsymbol z\\in\\mathbb R^n$ on the hyperplane $H$ are then given by\n$$ \\langle \\boldsymbol z,\\boldsymbol x\\rangle \\quad = \\quad b. $$\nThe set of points $\\boldsymbol z$ lying on one side of the hyperplane $H$ can thus always be written as $\\langle \\boldsymbol z, \\boldsymbol x\\rangle\\le b$ (modulo a change of sign). So considering \n$$\\sup_{\\boldsymbol z\\in K} \\langle \\boldsymbol z,\\boldsymbol x\\rangle $$ \namounts to finding the $b(\\boldsymbol x)$ for the direction $\\boldsymbol x$ such that set $K$ lies on one side of the hyperplane $(\\boldsymbol x,b(\\boldsymbol x))$ or equivalently, such that all $z\\in K$ verify $\\langle \\boldsymbol z, \\boldsymbol x\\rangle \\le b(\\boldsymbol x)$.\nThen $K$ can be understood as the intersection of all the half-spaces thus defined.\nIt can maybe be useful to look at a basic example: consider the region $K=[0,1]\\times [0,1]$. Then let's consider the $x$-direction with the vector $\\boldsymbol v=(1,0)^t$, we get\n$$ h_K(\\boldsymbol v) = \\max_{\\boldsymbol w\\in K} \\langle\\boldsymbol w,\\boldsymbol v \\rangle = \\max_{w_1\\in[0,1]} w_1 = 1   $$\nand the hyperplane $(\\boldsymbol v,1)$ (i.e, the vertical line $x=1$) is indeed such that $K$ lies on strictly one side of it. Doing the same thing for the direction $-\\boldsymbol v$, and the perpendicular directions will bring us the for sides of the region. This is a bit of a trivial example but hopefully it can help somewhat for the intuition.", "meta": {"post_id": 235926, "input_score": 19, "output_score": 34, "post_title": "What is the support function $h_K(x)\\equiv \\sup_{z \\in K} \\langle z, x \\rangle$ of a set $K$?"}}
{"input": "Two $n\\times n$ matrices $A, B$ are said to be simultaneously diagonalizable if there is a nonsingular matrix $S$ such that both $S^{-1}AS$ and $S^{-1}BS$ are diagonal matrices.\n\n\na) Show that simultaneously diagonalizable matrices commute: $AB = BA$.\n\n\nb) Prove that the converse is valid, provided that one of the matrices has no multiple eigenvalues. Is every pair of commuting matrices simultaneously diagonalizable?\n\nMy attempt:\na) Let\n$$M=S^{-1}AS \\qquad\\text{and}\\qquad P=S^{-1}BS.$$\nIt follows that\n$$A= S^{-1}MS \\qquad\\text{and}\\qquad B=S^{-1}PS. \\tag{Eq. 1}$$ Thus,\n\\begin{align}\nA\\,B &=S^{-1}\\,M\\,S\\,S^{-1}\\,P\\,S &&\\text{substitution of diagonal forms}\n\\\\\nA\\,B &=S^{-1}\\,M\\,I\\,P\\,S &&\\text{Identity element}\n\\\\\n&= S^{-1}\\,M\\,P\\,S  && \\text{Multiplication by identity }\n\\\\\n&= S^{-1}\\,P\\,M\\,S &&\\text{$M$ and $P$ diagonal }\n\\\\\n&= S^{-1}\\,P\\,I\\,M\\,S &&\\text{Multiplication by identity}\n\\\\\n&=S^{-1}\\,P\\, \\left[SS^{-1}\\right]\\,M\\,  S\n&& \\text{$S$ has inverse by premise}\n\\\\\n&=\\left[S^{-1}\\,P\\, S\\right] \\left[S^{-1}\\,M\\,  S\\right]\n&& \\text{associative properties of matrices}\n\\\\\n&=B\\,A &&\\text{Eq. 1}.\n\\end{align}\nb) How can I do this?", "output": "This has undoubtedly been answered (likely multiple times) here before, so I post this at the risk of beating a dead (and decaying) horse.\nLet me first link you to this page, which contains two excellent answers (I particularly recommend Keith Conrad's expository paper linked in Pierre-Yves Gaillard's answer). However, let me provide a perhaps more elementary viewpoint since, from experience, many people beginning this topic are not quite comfortable with minimal polynomial based arguments yet.\nYou seem to have covered part a quite adequately so let me focus on part b. I apologize in advance for the length, but I feel that this is a topic which requires thorough understanding.\nThe main thing to remember about commuting matrices is the fact that commuting matrices respect each other's eigenspaces. What does this mean? To talk about that, we first have to introduce the topic of an invariant subspace. \nConsider a matrix mapping $A:\\ V \\rightarrow V$ for a vector space $V$. If there is some subspace $U$ of $V$ such that the restriction of $A$ to $U$ remains an operator in the sense that $A:\\ U\\rightarrow U$, then we say that $U$ is an invariant subspace of $A$. The term stable is also sometimes used. The significance of this is that $A(U) \\subseteq U$, the image of $U$ is entirely contained within $U$. This way, it makes sense to talk about a restriction of the mapping to the smaller vector space $U$. \nThis is desirable for several reasons, the main one being that linear mappings on smaller vector spaces are easier to analyze. We can look at the action of the mapping on each invariant subspace and then piece them together to get an overall picture. This is what diagonalization does; we break down the vector space into smaller invariant subspaces, the eigenspaces, and then piece together the facts to get a simpler picture of how the mapping works. Many of the simpler, canonical representations are dependent on this fact (for example, the Jordan canonical form looks at the invariant generalized eigenspaces).\nNow, if we have two commuting, diagonalizable matrices, then each eigenspace of $B$ is not only invariant under $B$ itself, but also under $A$. This is what we mean by preserving each other's eigenspaces. To see this, let $\\mathbf{v}$ be an eigenvector of $B$ under eigenvalue $\\lambda$. Then\n$$B(A\\mathbf{v}) = A(B\\mathbf{v}) = \\lambda A\\mathbf{v}$$\nso that $A\\mathbf{v}$ is again an eigenvector of $B$ under eigenvalue $\\lambda$. In our new language, this means that the eigenspace $E_\\lambda$ of $B$ is invariant under $A$. This means it makes sense to look at the restriction of $A$ to $E_\\lambda$.\nNow consider the restriction of $A$ to $E_\\lambda$. If all the eigenvalues of $B$ are simple (multiplicity one) then that means each eigenspace of $B$ is one dimensional. We have therefore restricted $A:\\ E_\\lambda \\rightarrow E_\\lambda$ to a mapping on a one-dimensional vector space. But this means that $A$ must take each vector of $E_\\lambda$ to a scalar multiple of itself. You can check that this necessarily implies that $E_\\lambda$ is also an eigenspace of $A$. Therefore, for any eigenbasis of $B$ that we take, the corresponding vectors also form an eigenbasis of $A$. This means that the two matrices are simultaneously diagonalizable; they share a common eigenbasis.\nThe general case is a bit more involved in that the restrictions to the invariant subspaces are more complex (they're no longer one-dimensional), but the ideas are identical.\nP.S. Since you seem to be interested in physics, let me mention a crucial application of commuting operators. In quantum mechanics, you have quantities called observables, each of which is roughly speaking represented by a Hermitian matrix. Unlike in classical physics, different observables need not be simultaneously measurable (by measuring position for example, you cannot simultaneously measure momentum and vice versa) which is ultimately due to the fact that the position operator and the momentum operator do not commute (this is the underlying reasons behind the uncertainty principle). They do not have a shared basis which can represent the states of a system. Commuting operators therefore form a key element of quantum physics in that they define quantities which are compatible, i.e. simultaneously defined.", "meta": {"post_id": 236212, "input_score": 130, "output_score": 212, "post_title": "Prove that simultaneously diagonalizable matrices commute"}}
{"input": "The Whitney Embedding Theorem states that every smooth manifold can be embedded in Euclidean space.\nThe Nash Embedding Theorem states that every Riemannian manifold can be embedded in Euclidean space.\nSo I wonder: Can I regard Nash\u2019s theorem as a special case of Whitney\u2019s theorem?", "output": "Just to add something to Jesse's answer, the idea behind the proof of the Easy Whitney Embedding Theorem is to place different pieces of the given $ n $-dimensional smooth manifold in 'general position' in $ \\mathbb{R}^{2n + 1} $. The proof is not very hard to follow; I think that Munkres does a pretty good job in his book Topology. The Hard Whitney Embedding Theorem, which tries to embed a smooth $ n $-dimensional manifold in $ \\mathbb{R}^{2n} $, requires a more technical proof. A clever idea, called 'Whitney's trick' nowadays, is the main idea behind the proof. Notice that we have no notion of distance on a general smooth manifold $ M $ unless some metric on $ M $ is specified. Hence, both versions of the Whitney Embedding Theorem do not talk about preserving distances between points when constructing the required smooth embedding.\nThe Nash Embedding Theorem, however, is much harder. Not only must you embed the given Riemannian manifold in Euclidean space, you must do so isometrically, i.e., in a way that preserves distances between points. This requires the solution of a formidable system of partial differential equations that yields the required isometric embedding. Nash solved this PDE system using a special version of Newton's iteration method, called Newton's method with post-conditioning. When unmodified, Newton's iteration method, in general, fails to converge to a solution because each step of the iteration might result in the loss of derivatives, i.e., the order of differentiability is reduced. Nash recovered the lost derivatives by applying smoothing operators (defined via convolution) at each step of the iteration. This ensures that Newton's iteration method does actually converge to a solution. The application of a smoothing operator at each step is called post-conditioning. As you can see, Nash's result is definitely much harder and requires more technology to prove than Whitney's results.\nThese two results also have different natures. The Whitney Embedding Theorem is more topological in character, while the Nash Embedding Theorem is a geometrical result (as it deals with metrics). However, the structure of smooth manifolds is sufficiently rigid to ensure that they are also geometrical objects (cf. my comment below Jesse's answer), to which the Nash Embedding Theorem can be applied.", "meta": {"post_id": 236285, "input_score": 25, "output_score": 39, "post_title": "Is the Nash Embedding Theorem a special case of the Whitney Embedding Theorem?"}}
{"input": "I verified experimentally that in Java the equality\nMath.sqrt(x*x) = x\n\nholds for all long x such that x*x doesn't overflow. Here, Java long is a $64$ bit signed type and double is a IEEE binary floating point type with at least $53$ bits mantissa and sufficiently long exponent.\nMathematically, there are two imprecise functions involved:\n\nConversion from long to double which loses precision due to the mantissa being only $53$ bits where $63$ bits would be needed. This operation is guaranteed to return the closest representable result.\nComputing square root, which is also guaranteed to return the closest representable result.\n\nMathematically, this can be expressed like\n$$\n\\mathop\\forall_{x \\in {\\mathbb N} \\atop x \\le 3037000499}\n\\mathrm{round}\\left(\\sqrt{\\mathrm{round}(x^2)}\\right)\n= x\n$$\nwhere $\\mathrm{round}$ is the rounding function from $\\mathbb R$ into the set of all numbers representable as double.\nI'm looking for a proof since no experiment can assure it works across all machines.", "output": "The idea is simple: Find upper and lower bounds for\n$$X := \\sqrt{\\mathrm{round}(x^2)}$$\nand show that $\\mathrm{round}(X) = x$.\n\nLet $\\mathrm{ulp}(x)$ denote the unit of least precision at $x$\nand let $E(x)$ and $M(x)$ denote the exponent and mantissa of $x$, i.e.,\n$$x = M(x) \\cdot 2^{E(x)}$$\nwith $1 \\le M(x) < 2$ and $E(x) \\in \\mathbb Z$. Define\n$$\\Delta(x) = \\frac{\\mathrm{ulp}(x)}x = \\frac{\\mu \\cdot 2^{E(x)}}x = \\frac\\mu{M(x)}$$\nwhere $\\mu=2^{-52}$ is the machine epsilon.\nExpressing the rounding function by its relative error leads to\n$$X = \\sqrt{(1+\\epsilon) \\cdot x^2} = \\sqrt{(1+\\epsilon)} \\cdot x\n< \\big( 1+\\frac\\epsilon2 \\big) \\cdot x$$\nWe know that $|\\epsilon| \\le \\frac12\\Delta(x^2)$ and get (ignoring the trivial case $x=0$)\n$$\\frac Xx < 1 + \\frac{\\Delta(x^2)}4 = 1 + \\frac\\mu{4 M(x^2)}$$\n\nBy observing $M(x)$ and $M(x^2)$ e.g. over the interval $[1, 4]$,\nit can be easily be shown that $\\frac{M(x)}{M(x^2)} \\le \\sqrt2$ which gives us\n$$\\frac Xx < 1 + \\frac{\\mu\\sqrt2}{4 M(x)}$$\nand therefore\n$$X < x + \\frac{\\sqrt2}4 \\frac{\\mu}{M(x)} \\cdot x < x + \\frac12 \\mathrm{ulp}(x)$$\n\nAnalogously we get the corresponding lower bound. Just instead of\n$$\\sqrt{(1+\\epsilon)} < \\big( 1+\\frac\\epsilon2 \\big)$$\nwe use something like\n$$\\sqrt{(1-\\epsilon)} > \\big( 1 - (1+\\epsilon) \\cdot \\frac\\epsilon2 \\big)$$\nwhich suffices, since we used a very generous estimate ($\\sqrt2/4<\\frac12$) in the last step.\n\nBecause of $|X-x|$ being smaller than $\\frac12 \\mathrm{ulp}(x)$, $x$ is the double closest to $X$, therefore $\\mathrm{round}(X)$ must equal to $x$, q.e.d.", "meta": {"post_id": 237865, "input_score": 57, "output_score": 45, "post_title": "Show that floating point $\\sqrt{x \\cdot x} \\geq x$ for all long $x$."}}
{"input": "It\u2019s from the book \u201cLinear Algebra and its Applications\u201d by Gilbert Strang, page 260.\n$$(I-A)^{-1}=I+A+A^2+A^3+\\ldots$$\nNonnegative matrix $A$ has the largest eigenvalue $\\lambda_1<1$.\nThen, the book says $(I-A)^{-1}$ has the same eigenvector, with eigenvalue $1/(1-\\lambda_1)$.\nWhy? Is there any other formulas between inverse matrix and eigenvalue that I don\u2019t know?", "output": "A matrix $A$ has an eigenvalue $\\lambda$ if and only if $A^{-1}$ has eigenvalue $\\lambda^{-1}$. To see this, note that\n$$A\\mathbf{v} = \\lambda\\mathbf{v} \\implies A^{-1}A\\mathbf{v} = \\lambda A^{-1}\\mathbf{v}\\implies A^{-1}\\mathbf{v} = \\frac{1}{\\lambda}\\mathbf{v}$$\nIf your matrix $A$ has eigenvalue $\\lambda$, then $I-A$ has eigenvalue $1 - \\lambda$ and therefore $(I-A)^{-1}$ has eigenvalue $\\frac{1}{1-\\lambda}$.", "meta": {"post_id": 237871, "input_score": 54, "output_score": 207, "post_title": "Inverse matrix\u2019s eigenvalue?"}}
{"input": "Prove without calculus that the sequence \n$$L_{n}=\\sqrt[n+1] {(n+1)!} - \\sqrt[n] {n!}, \\space n\\in \\mathbb N$$\nis strictly decreasing.", "output": "Let $\\ell_n = \\left(n!\\right)^{1/n}$. Clearly for all $n \\in \\mathbb{N}$, $\\ell_n > 0$. The question is equivalent to showing that \n  $$\\frac{\\ell_{n+2}}{\\ell_{n+1}} + \\frac{\\ell_n}{\\ell_{n+1}} < 2 \\tag{1}$$\nLet\n$$\n    x_n = \\log \\frac{\\ell_{n+1}}{\\ell_n} = \\frac{1}{n+1} \\left( \\log(n+1) - \\frac{1}{n} \\sum_{k=1}^n \\log(k) \\right)\n$$\nThe inequality $(1)$ now reads:\n$$\n   2 > \\exp(x_{n+1}) + \\exp(-x_n) = 2 \\exp\\left(\\frac{x_{n+1}-x_n}{2}\\right) \\cosh \\left(\\frac{x_{n+1}+x_n}{2}\\right) \\tag{2}\n$$\nWe can rewrite $x_n$ a little:\n$$\n   x_n = \\frac{1}{n+1} \\left( \\log\\left(\\frac{n+1}{n}\\right) - \\underbrace{\\frac{1}{n} \\sum_{k=1}^n \\log\\left(\\frac{k}{n}\\right)}_{\\text{denote this as } s_n} \\right) \n$$\nNote that, with some straightforward algebra\n$$\n    \\frac{x_{n+1}-x_n}{2} = \\frac{1}{2(n+2)} \\log\\left(1+\\frac{1}{n+1}\\right) - \\frac{1}{(n+1)(n+2)} \\left( \\log\\left(1+\\frac{1}{n} \\right) - s_n  \\right) \\tag{3}\n$$\n$$\n  \\frac{x_{n+1}+x_n}{2} =   \\frac{1}{2(n+2)} \\log\\left(1+\\frac{2}{n}\\right)+ \\frac{1}{2(n+2)} \\log\\left(1+\\frac{1}{n}\\right) - \\frac{1}{n+2} s_n   \\tag{4}\n$$\nBounding $s_n$ \nUsing summation by parts:\n$$\n   \\sum_{k=1}^n \\left(a_{k+1}-a_k\\right) b_k = a_{n+1} b_n - a_1 b_1 -\\sum_{k=1}^{n-1} a_{k+1} \\left(b_{k+1} - b_k \\right)\n$$\nwith $a_k = \\frac{k}{n}$ and $b_k = \\log \\frac{k}{n}$, we find\n$$ \\begin{eqnarray}\n  s_n &=& 0 - \\frac{\\log n^{-1}}{n} - \\sum_{k=1}^{n-1} \\frac{k+1}{n} \\log\\left(1+\\frac{1}{k}\\right) \\\\ &=& -\\frac{n-1}{n} + \\frac{1}{2} \\frac{\\log(n)}{n} - \\frac{1}{n} \\sum_{k=1}^{n-1} \\left( \\left(k + \\frac{1}{2} \\right) \\log\\left(1+\\frac{1}{k}\\right) - 1 \\right)\n\\end{eqnarray}\n$$\nUsing elementary integral $\\int_0^1 \\frac{\\mathrm{d}x}{k+x} = \\log\\left(1+\\frac{1}{k}\\right)$ we find\n$$\n   \\left(k + \\frac{1}{2} \\right) \\log\\left(1+\\frac{1}{k}\\right) - 1 = \\int_0^1 \\left(\\frac{k+\\frac{1}{2}}{k+x}-1\\right) \\mathrm{d}x = \\int_0^1 \\frac{1-2x}{2(k+x)} \\mathrm{d}x\n$$\nchanging variables $x \\to 1-x$ and averaging with the original:\n$$\\begin{eqnarray}\n   \\left(k + \\frac{1}{2} \\right) \\log\\left(1+\\frac{1}{k}\\right) - 1 &=& \\int_0^1 \\frac{ \\left(x-\\frac{1}{2}\\right)^2}{(k+x)(k+1-x)} \\mathrm{d}x \\\\ &=& \\int_{-\\frac{1}{2}}^{\\frac{1}{2}} \\frac{ u^2}{\\left(k+\\frac{1}{2}\\right)^2 - u^2 } \\mathrm{d} u \n\\end{eqnarray}\n$$\nSince \n$$\n \\int_{-\\frac{1}{2}}^{\\frac{1}{2}} \\frac{ u^2}{\\left(k+\\frac{1}{2}\\right)^2 } \\mathrm{d} u  < \\int_{-\\frac{1}{2}}^{\\frac{1}{2}} \\frac{ u^2}{\\left(k+\\frac{1}{2}\\right)^2 - u^2 } \\mathrm{d} u  < \\int_{-\\frac{1}{2}}^{\\frac{1}{2}} \\frac{ u^2}{\\left(k+\\frac{1}{2}\\right)^2 - \\frac{1}{4} } \\mathrm{d} u\n$$\nWe have\n$$\n  \\frac{1}{12} \\frac{1}{\\left(k+\\frac{1}{2}\\right)^2} < \n   \\left(k + \\frac{1}{2} \\right) \\log\\left(1+\\frac{1}{k}\\right) - 1\n  < \\frac{1}{12 k(k+1)} = \\frac{1}{12 k} - \\frac{1}{12 (k+1)}\n$$\nSince \n$$\\frac{1}{12} \\frac{1}{\\left(k+\\frac{1}{2}\\right)^2} > \\frac{1}{12} \\frac{1}{\\left(k+\\frac{1}{2}\\right) \\left(k+\\frac{3}{2}\\right)} = \\frac{1}{12} \\frac{1}{k+\\frac{1}{2}} - \\frac{1}{12} \\frac{1}{k+\\frac{3}{2}}\n$$\nWe thus establish that \n$$\n   \\sum_{k=1}^{n-1} \\left( \\left(k + \\frac{1}{2} \\right) \\log\\left(1+\\frac{1}{k}\\right) - 1 \\right) < \\sum_{k=1}^{n-1} \\left( \\frac{1}{12 k} - \\frac{1}{12 (k+1)}  \\right) = \\frac{1}{12} - \\frac{1}{12 n} < \\frac{1}{12}\n$$\nand\n$$\n  \\sum_{k=1}^{n-1} \\left( \\left(k + \\frac{1}{2} \\right) \\log\\left(1+\\frac{1}{k}\\right) - 1 \\right) > \\sum_{k=1}^{n-1} \\left( \\frac{1}{12 \\left(k+\\frac{1}{2}\\right)} -\\frac{1}{12 \\left(k+\\frac{3}{2}\\right)}  \\right) = \\frac{1}{18} - \\frac{1}{6 (2n+1)} = \\frac{1}{9} \\frac{n-1}{2n+1}\n$$\nThe argument above suggests that $ \\sum_{k=1}^{n-1} \\left( \\left(k + \\frac{1}{2} \\right) \\log\\left(1+\\frac{1}{k}\\right) - 1 \\right)$ converges to a number $c$ such that $\\frac{1}{18} < c < \\frac{1}{12}$. Thus\n$$\n  -\\frac{n-1}{n} + \\frac{\\log(n)}{2n} - \\frac{1}{12 n}  < s_n < -\\frac{n-1}{n} + \\frac{\\log(n)}{2n} - \\frac{1}{9} \\frac{n-1}{n (2n+1)} \\tag{5}\n$$\nImplying that $s_n$ converges to $-1$ and that, for large $n$\n$$\n    s_n = -1 + \\frac{\\log(n)}{2n} + \\mathcal{O}\\left(n^{-1}\\right)\n$$\nUsing these bounds \nWe therefore conclude that $\\frac{x_{n+1}-x_n}{2} = \\mathcal{O}\\left(n^{-2}\\right)$ and $\\frac{x_{n+1}+x_n}{2} = \\mathcal{O}\\left(n^{-1}\\right)$.\nSince both the mean and difference are arbitrarily small for large enough $n$:\n$$ \\begin{eqnarray}\n    2 \\exp\\left(\\frac{x_{n+1}-x_n}{2}\\right) \\cosh \\left(\\frac{x_{n+1}+x_n}{2}\\right) &<& 2 \\frac{1}{1-\\frac{x_{n+1}-x_n}{2}} \\frac{1}{1-\\frac{1}{2} \\left(\\frac{x_{n+1}+x_n}{2}\\right)^2} \\\\ &=& 2 + 2\\left(\\frac{x_{n+1}-x_n}{2}\\right) + \\left(\\frac{x_{n+1}+x_n}{2}\\right)^2 + \\mathcal{o}\\left(n^{-3}\\right) \\\\\n   &=& 2 - \\frac{1}{2 n^3} + \\mathcal{o}\\left(n^{-3}\\right)\n\\end{eqnarray}\n$$\nThus, at least for $n$ large enough the sequence $L_n$ is decreasing.\nThis painstaking exercise just makes one appreciate the power of calculus.", "meta": {"post_id": 238245, "input_score": 95, "output_score": 43, "post_title": "Prove elementarily that $\\sqrt[n+1] {(n+1)!} - \\sqrt[n] {n!}$ is strictly decreasing"}}
{"input": "As normed division algebras, when we go from the complex numbers to the quaternions, we lose commutativity. Moving on to the octonions, we lose associativity. Is there some analogous property that we lose moving from the reals to the complex numbers?", "output": "The most obvious property that we lose is the linear (or total) ordering of the real line.", "meta": {"post_id": 240959, "input_score": 32, "output_score": 44, "post_title": "What do we lose passing from the reals to the complex numbers?"}}
{"input": "Me and my friend were arguing over this \"fact\" that we all know and hold dear. However, I do know that $1+1=2$ is an axiom. That is why I beg to differ. Neither of us have the required mathematical knowledge to convince each other.\nAnd that is why, we decided to turn to Math Stackexchange for help.\nWhat would be stack's opinion?", "output": "Those interested in pushing this question back further than Asaf Karagila did (well past logic and into the morass of philosophy) may be interested in the following comments that were written in 1860 (full reference below). Also, although Asaf's treatment here avoids this, there are certain issues when defining addition of natural numbers in terms of the successor operation that are often overlooked. See my 22 November 2011 and 28 November 2011 posts in the Math Forum group math-teach.\n\n$[\\ldots]$ consider this case. There is a world in which, whenever two pairs of things are either placed in proximity or are contemplated together, a fifth thing is immediately created and brought within the contemplation of the mind engaged in putting two and two together. This is surely neither inconceivable, for we can readily conceive the result by thinking of common puzzle tricks, nor can it be said to be beyond the power of Omnipotence, yet in such a world surely two and two would make five. That is, the result to the mind of contemplating two two\u2019s would be to count five. This shows that it is not inconceivable that two and two might make five; but, on the other hand, it is perfectly easy to see why in this world we are absolutely certain that two and two make four. There is probably not an instant of our lives in which we are not experiencing the fact. We see it whenever we count four books, four tables or chairs, four men in the street, or the four corners of a paving stone, and we feel more sure of it than of the rising of the sun to-morrow, because our experience upon the subject is so much wider and applies to such an infinitely greater number of cases.\n\nThe above passage comes from:\nJames Fitzjames Stephen (1829-1894), Review of Henry Longueville Mansel (1820-1871), Metaphysics; or, the Philosophy of Consciousness, Phenomenal and Real (1860), The Saturday Review 9 #244 (30 June 1860), pp. 840-842. [see page 842]\nStephen\u2019s review of Mansel's book is reprinted on pp. 320-335 of Stephen's 1862 book Essays, where the quote above can be found on page 333.\n(ADDED 2 YEARS LATER) Because my answer continues to receive sporadic interest and because I came across something this weekend related to it, I thought I would extend my answer by adding a couple of items.\nThe first new item, [A], is an excerpt from a 1945 paper by Charles Edward Whitmore. I came across Whitmore's paper several years ago when I was looking through all the volumes of the journal Journal of the History of Ideas at a nearby university library. Incidentally, Whitmore's paper is where I learned about speculations of James Fitzjames Stephen that are given above. The second new item, [B], is an excerpt from an essay by Augustus De Morgan that I read this last weekend. De Morgan's essay is item [15] in my answer to the History of Science and Math StackExchange question Did Galileo's writings on infinity influence Cantor?, and his essay is also mentioned in item [8]. I've come across references to De Morgan's essay from time to time over the years, but I've never read it because I never bothered trying to look it up in a university library. However, when I found to my surprise (but I really shouldn't have been surprised) that a digital copy of the essay was freely available on the internet when I searched for it about a week ago, I made a print copy, which I then read through when I had some time (this last weekend).\n[A] Charles Edward Whitmore (1887-1970), Mill and mathematics: An historical note, Journal of the History of Ideas 6 #1 (January 1945), 109-112. MR 6,141n; Zbl 60.01622\n\n(first paragraph of the paper, on p. 109) In various philosophical works one encounters the statement that J. S. Mill somewhere asserted that two and two might conceivably make five. Thus, Professor Lewis says$^1$ that Mill \"asked us to suppose a demon sufficiently powerful and maleficent so that every time two things were brought together with two other things, this demon should always introduce a fifth\"; but he gives no specific reference. {{footnote: $^1$C. I. Lewis, Mind and the World Order (1929), 250.}} C. S. Peirce$^2$ puts it in the form, \"when two things were put together a third should spring up,\" calling it a doctrine usually attributed to Mill. {{footnote: $^2$Collected Papers, IV, 91 (dated 1893). The editors supply a reference to Logic, II, vi, 3.}} Albert Thibaudet$^3$ ascribes to \"a Scottish philosopher cited by Mill\" the doctrine that the addition of two quantities might lead to the production of a third. {{footnote: $^3$Introduction to Les Id\u00e9es de Charles Maurras (1920), 7.}} Again, Professor Laird remarks$^4$ that \"Mill suggested, we remember, that two and two might not make four in some remote part of the stellar universe,\" referring to Logic III, xxi, 4 and II, vi, 2. {{footnote: $^4$John Laird, Knowledge, Belief, and Opinion (1930), 238.}} These instances, somewhat casually collected, suggest that there is some confusion in the situation.\n(from pp. 109-111) Moreover, the notion that two and two should [\"could\" intended?] make five is entirely opposed to the general doctrine of the Logic. $[\\cdots]$ Nevertheless, though these views stand in the final edition of the Logic, it is true that Mill did, in the interval, contrive to disallow them. After reading through the works of Sir William Hamilton three times, he delivered himself of a massive Examination of that philosopher, in the course of which he reverses his position--but at the suggestion of another thinker. In chapter VI he falls back on the inseparable associations generated by uniform experience as compelling us to conceive two and two as four, so that \"we should probably have no difficulty in putting together the two ideas supposed to be incompatible, if our experience had not first inseparably associated one of them with the contradictory of the other.\" To this he adds, \"That the reverse of the most familiar principles of arithmetic and geometry might have been made conceivable even to our present mental faculties, if those faculties had coexisted with a totally different constitution of external nature, is ingeniously shown in the concluding paper of a recent volume, anonymous, but of known authorship, Essays, by a Barrister.\" The author of the work in question was James Fitzjames Stephen, who in 1862 had brought together various papers which had appeared in Saturday Review during some three previous years. Some of them dealt with philosophy, and it is from a review of Mansel's Metaphysics that Mill proceeds to quote in support of his new doctrine $[\\cdots]$\nNote: On p. 111 Whitmore argues against Mill's and Stephen's empirical viewpoint of \"two plus two equals four\". Whitmore's arguments are not very convincing to me.\n(from p. 112) Mill, then, did not originate the idea, but adopted it from Stephen, in the form that two and two might make five to our present faculties, if external nature were differently constituted. He did not assign it to some remote part of the universe, nor did he call in the activity of some maleficent demon; neither did he say that one and one might make three. He did not explore its implications, or inquire how it might be reconciled with what he had said in other places; but at least he is entitled to a definite statement of what he did say. I confess that I am somewhat puzzled at the different forms in which it has been quoted, and at the irrelevant details which have been added.\n\n[B] Augustus De Morgan (1806-1871), On infinity; and on the sign of equality, Transactions of the Cambridge Philosophical Society 11 Part I (1871), 145-189.\n\nPublished separately as a booklet by Cambridge University Press in 1865 (same title; i + 45 pages). The following excerpt is from the version published in 1865.\n(footnote 1 on p. 14) We are apt to pronounce that the admirable pre-established harmony which exists between the subjective and objective is a necessary property of mind. It may, or may not, be so. Can we not grant to omnipotence the power to fashion a mind of which the primary counting is by twos, $0,$ $2,$ $4,$ $6,$ &c.; a mind which always finds its first indicative notion in this and that, and only with effort separates this from that. I cannot invent the fundamental forms of language for this mind, and so am obliged to make it contradict its own nature by using our terms. The attempt to think of such things helps towards the habit of distinguishing the subjective and objective.\nNote: Those interested in such speculations will also want to look at De Morgan's lengthy footnote on p. 20.\n\n(ADDED 6 YEARS LATER) I recently read Ian Stewart's 2006 book Letters to a Young Mathematician and in this book there is a passage (see below) that I think is worth including here.\n\n(from pp. 30-31) I think human math is more closely linked to our particular physiology, experiences, and psychological preferences than we imagine. It is parochial, not universal. Geometry's points and lines may seem the natural basis for a theory of shape, but they are also the features into which our visual system happens to dissect the world. An alien visual system might find light and shade primary, or motion and stasis, or frequency of vibration. An alien brain might find smell, or embarrassment, but not shape, to be fundamental to its perception of the world. And while discrete numbers like $1,$ $2,$ $3,$ seem universal to us, they trace back to our tendency to assemble similar things, such as sheep, and consider them property: has one of my sheep been stolen? Arithmetic seems to have originated through two things: the timing of the seasons and commerce. But what of the blimp creatures of distant Poseidon, a hypothetical gas giant like Jupiter, whose world is a constant flux of turbulent winds, and who have no sense of individual ownership? Before they could count up to three, whatever they were counting would have blown away on the ammonia breeze. They would, however, have a far better understanding than we do of the math of turbulent fluid flow.", "meta": {"post_id": 243049, "input_score": 227, "output_score": 40, "post_title": "How do I convince someone that $1+1=2$ may not necessarily be true?"}}
{"input": "I was wondering if it is possible to produce an explicit bijection $h\\colon \\mathbb{R} \\rightarrow \\mathbb{R}/\\mathbb{Q}$. If we can produce an explicit injection $i\\colon \\mathbb{R} \\rightarrow \\mathbb{R}/\\mathbb{Q}$, can the Cantor-Bernstein-Schroeder Theorem be used constructively?\nIt is clear that the two sets have the same cardinality, so the existence of such a bijection is trivial. What I am really looking for is a nice-looking bijection, or a proof that no such nice-looking bijection exists, for a definition of \"nice-looking\" which I cannot quite figure out.\nOne problem which I think makes finding such a bijection difficult is that any natural injection of $\\mathbb{R}/\\mathbb{Q}$ (ie, those injections in which one representative is chosen from each coset) produces a non-measurable set, specifically a Vitali set.\nI hate to ask such a vague question, but I'm really not sure about whether the correct answer is constructive, or whether it is a proof that any such bijection is in some sense \"very complicated.\"\nAs a final note, the motivation for this question came from this discussion, in which I was somewhat astonished to see such a clear, constructive bijection given between $\\mathbb{R}$ and $\\mathbb{R} \\setminus S$, where $S$ is countable.", "output": "It is not possible.\nIt is consistent with set theory without choice that $\\mathbb R/\\mathbb Q$ has strictly larger cardinality that $\\mathbb R$. (This looks counter-intuitive, since $\\mathbb R/\\mathbb Q$ is a quotient.) \nThis is the case because, using a fact that goes back to Sierpi\u0144ski (Sur une proposition qui entra\u00eene l\u2019existence des ensembles non mesurables, Fund. Math. 34, (1947), 157\u2013162. MR0023318 (9,338i)), in any model of $\\mathsf{ZF}$ where all sets of reals have the Baire property, it is not even possible to linearly order $\\mathbb R/\\mathbb Q$.\n(Sierpi\u0144ski argues in terms of Lebesgue measure. The argument in terms of the Baire property is analogous, and has the additional technical advantage of not requiring any discussion of consistency strength issues.)\nA couple of years ago, Mike Oliver gave a nice talk on this topic (How to have more things by forgetting where you put them); he is not exactly using $\\mathbb R/\\mathbb Q$, but the arguments easily adapt. The point of the talk is precisely to give some intuition on why we expect the quotient to be \"larger\".  \n[Of course, in the presence of choice, the two sets have the same size. The argument above shows that the use of choice is unavoidable.]", "meta": {"post_id": 243544, "input_score": 40, "output_score": 49, "post_title": "Bijection between $\\mathbb{R}$ and $\\mathbb{R}/\\mathbb{Q}$"}}
{"input": "How does one create an explicit bijection from the reals to the set of all sequences of reals? I know how to make a  bijection from $\\mathbb R$ to $\\mathbb {R \\times R}$.\nI have an idea but I am not sure if it will work. I will post it as my own answer because I don't want to anchor your answers and I want to see what other possible ways of doing this are.", "output": "The nicest trick in the book is to find a bijection between $\\mathbb R$ and $\\mathbb{N^N}$, in this case we are practically done. Why?\n$$\\mathbb{(N^N)^N\\sim N^{N\\times N}\\sim N^N}$$\nAnd the bijections above are easy to calculate (I will leave those to you, the first bijection is a simple Currying, and for the second you can use Cantor's pairing function).\nSo if we can find a nice bijection between the real numbers the infinite sequences of natural numbers we are about done. Now, we know that $\\mathbb{N^N}$ can be identified with the real numbers, in fact continued fractions form a bijection between the irrationals and $\\mathbb{N^N}$. \nWe first need to handle the rational numbers, but that much is not very difficult. Take an enumeration of the rationals (e.g. Calkin-Wilf tree) in $(0,1)$, suppose $q_i$ is the $i$-th rational in the enumeration; now we take a sequence of irrationals, e.g. $r_n = \\frac1{\\sqrt{n^2+1}}$, and we define the following function:\n$$h(x)=\\begin{cases} r_{2n} & \\exists n: x=r_n\\\\ r_{2n+1} & \\exists n: x=q_n \\\\ x &\\text{otherwise}\\end{cases}$$\nNow we can finally describe a list of bijections which, when composed, give us a bijection between $\\mathbb R$ and $\\mathbb{R^N}$.\n\n$\\mathbb{R^N\\to (0,1)^N}$ by any bijection of this sort.\n$\\mathbb{(0,1)^N\\to \\left((0,1)\\setminus Q\\right)^N}$ by the encoding given by $h$.\n$\\mathbb{\\left((0,1)\\setminus Q\\right)^N\\to \\left(N^N\\right)^N}$ by continued fractions.\n$\\mathbb{\\left(N^N\\right)^N\\to N^{N\\times N}}$ by Currying.\n$\\mathbb{N^{N\\times N}\\to N^N}$ by a pairing function.\n$\\mathbb{N^N\\to (0,1)\\setminus Q}$ by decoding the continued fractions.\n$\\mathbb{(0,1)\\setminus Q\\to (0,1)}$ by the decoding of $h$, i.e. $h^{-1}$.\n$\\mathbb{(0,1)\\to R}$ by any bijection of this sort, e.g. the inverse of the bijection used for the first step.", "meta": {"post_id": 243590, "input_score": 48, "output_score": 50, "post_title": "Bijection from $\\mathbb R$ to $\\mathbb {R^N}$"}}
{"input": "I am studying complex analysis and I have problem understanding the\nconcept of branch cut.\nThe lecturer draw this as some curve that starts from a point and goes\non and on in some direction (for example, something like $y=x$ for\n$x\\geq0$ , but it doesn't have to be straight).\nThe definition given in the lecture is \n\nA branch cut is a curve that is being presented in order to define a\n  branch \n\nand then he added a note \n\nPoints on the branch cut are singular.\n\nCan someone please explain how does such a curve define a branch of\na function ?\nAs I understand it, if $f(z)=u(r,\\theta)+iv(r,\\theta)$ then we want\n$\\alpha<\\theta\\leq\\alpha+2\\pi$ for some real $\\alpha$. How does\na curve define this $\\alpha$ ?\nWhy are the points on a branch cut singular? Are we also assuming\nsomething about the function that we are trying to define a branch\nof it?", "output": "I will demonstrate with the example of the complex logarithm.\nRecall that a complex number $z=x+iy$ can be put into \"polar form\" $z=Re^{i \\theta}$, where $R$ is the distance from $z$ to the origin and $\\theta$ is the angle to $z$ around the complex plane measured positively from the $x$-axis. The complex logarithm has this formula for $z=Re^{i \\theta}$:\n$$\\log z = ln |R| + i \\theta.$$\nYou (hopefully) know that if $z=Re^{i \\theta}$ then we can also represent $z$ as $z=Re^{i (\\theta + 2\\pi)}$ and generally as $z = Re^{i (\\theta + 2 \\pi k)}$. Consider what happens when you plug these different representations into the formula for $\\log z$. If you use $Re^{i \\theta}$ you get $\\log z = ln |R| + i \\theta$ but if you use $Re^{i (\\theta + 8\\pi)}$ you get $\\log z = ln |R| + i (\\theta + 8 \\pi)$. \nIf you plot these points you run into an apparent problem -- they don't hit the same place! The consequence of this is that the complex logarithm is not a function (because functions take a single input to a single output -- this function takes a single input to multiple outputs).\nHere is the \"Riemann surface\" for the complex logarithm:\n$\\qquad\\qquad\\qquad$ \n-- don't worry too much about how it was generated but realize what it's telling you. If you pick the point $z = Re^{i \\theta}$ on the complex plane and map it through the complex logarithm, we've seen you get these infinite number of different values; the picture just represents this. \nWhat a branch cut does is restrict the outputs of the logarithm to one particular loop around this corkscrew surface (which one it is depends on the range of values you choose for your cut). Moreover, when we define branch cuts, there's a lot of points that we make \"undefined\" -- the reason for this is that while you can pick a particular loop to \"go around\" in the Riemann surface, you can never have continuity around the place you define the cut because the limits from different sides will be at different heights and so cannot be equal (and therefore it will not be continuous, which is generally a bad thing in calculus).\nLet me give you something specific. For example, if you choose the branch cut to be the negative real axis, then you are allowing $-\\pi < \\theta < \\pi$ -- that choice will roughly correspond to the \"level\" in the Riemann surface that has the red on it. Note that $z = Re^{-i \\pi} = Re^{i \\pi}$.\nWith this, you can see what I said earlier about continuity -- if you approach $Re^{i \\pi}$ from values of $\\theta$ greater than $\\pi$, you would be on the yellow part of the surface but if you approach $Re^{i \\pi}$ from values of $\\theta$ less than $-\\pi$, you are closer to the purple part of the image. We have approached $z = Re^{-i \\pi} = Re^{i \\pi}$ two different ways and got two different answers, in contradiction to continuity which we generally like to have in calculus.\nedit: I'd like to add this domain coloring image of $\\log z$ that I found in this Mathematica stack exchange post. The colors represent the argument of the output of the logarithm -- where it switches colors at angle $\\pi$ is precisely the same jump discontinuity I'm talking about above: \n$\\qquad\\qquad\\qquad$", "meta": {"post_id": 245579, "input_score": 27, "output_score": 58, "post_title": "How does a branch cut define a branch?"}}
{"input": "I'm attempting to prove that\n$$\n\\left[ \\begin{array}{c c}\nA & B \\\\\nC & D \\\\\n\\end{array} \\right]^\\top =\n\\left[ \\begin{array}{c c}\nA^\\top & C^\\top \\\\\nB^\\top & D^\\top \\\\\n\\end{array} \\right].\n$$\nIntuitively, I can see that it's true. However, when I try to formally prove it, I quickly get lost in the indices. What tricks can I use to keep things straight?\nSource: Exercise 2.6.16, P116, Intro to Linear Algebra, 4th Ed by Strang", "output": "Most people would just claim this is obvious and omit the proof, but if you don't want to do that then perhaps you could first prove that \n\\begin{equation}\n\\begin{bmatrix} M & N \\end{bmatrix}^T \n= \\begin{bmatrix} M^T \\\\ N^T \\end{bmatrix}\n\\end{equation}\nand\n\\begin{equation}\n\\begin{bmatrix} M \\\\ N \\end{bmatrix}^T \n= \\begin{bmatrix} M^T & N^T \\end{bmatrix}.\n\\end{equation}\nThen\n\\begin{align}\n\\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}^T \n&= \\begin{bmatrix} \n\\begin{bmatrix} A \\\\ C \\end{bmatrix}^T \\\\\n\\begin{bmatrix} B \\\\ D \\end{bmatrix}^T\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix} A^T & C^T \\\\ B^T & D^T \\end{bmatrix}.\n\\end{align}", "meta": {"post_id": 246289, "input_score": 32, "output_score": 47, "post_title": "Transpose of block matrix"}}
{"input": "I was solving some problems and I came across this problem. I didn't understand how to approach this problem. Can we solve this with out actually calculating $18!\\,\\,?$", "output": "Note that $437=(19)(23)$. We prove that $19$ and $23$ divide $18!+1$.  That is enough, since $19$ and $23$ are relatively prime. \nThe fact that $19$ divides $18!+1$ is immediate from Wilson's Theorem, which says that if $p$ is prime then $(p-1)!\\equiv -1\\pmod{p}$.\nFor $23$ we need to calculate a bit. We have $22!\\equiv -1\\pmod{23}$ by Wilson's Theorem. \nNow $(18!)(19)(20)(21)(22)=22!$.\nBut $19\\equiv -4\\pmod{23}$, $20\\equiv -3\\pmod{23}$, and so on. So $(19)(20)(21)(22)\\equiv 24\\equiv 1\\pmod{23}$. It follows that $18!\\equiv 22!\\pmod{23}$, and we are finished.", "meta": {"post_id": 247879, "input_score": 18, "output_score": 44, "post_title": "how to prove $437\\,$ divides $18!+1$? (NBHM 2012)"}}
{"input": "Given any abstract group $ G $, how much is known about which types of topological/Lie group structures it might have?\nAny abstract group $ G $ will have the structure of a discrete topological group (since generally, any set can be given the discrete topology), but there are groups that have no smooth structure. An example of this from Wikipedia is the group $ \\mathbb{Q} $ with the subspace topology inherited from $ \\mathbb{R} $. Which groups can occur as Lie groups? Are there specific families of groups that are known to have no smooth structure?\nSimilarly, how much can we know about the possible topologies on an abstract group $ G $? For example, which types of abstract groups admit a nontrivial (i.e., not the usual compact case) group structure?\nIn particular, I am curious about the extent to which properties of the abstract group determine properties of any associated topology/smooth structure.\nDoes anyone have any good references or a succinct answer for this question?", "output": "Section 1\nLet us begin with the following theorem.\n\nTheorem 1 Let $ G $ be a topological group. If $ G $ admits a Lie group structure, then this structure is unique up to diffeomorphism.\n\nProof:\nSuppose that $ \\mathcal{A}_{1} $ and $ \\mathcal{A}_{2} $ are smooth structures (maximal smooth atlases) on $ G $ that make it a Lie group. Observe that the identity map $ \\text{id}_{G}: G \\to G $ defines a continuous homomorphism of Lie groups from $ (G,\\mathcal{A}_{1}) $ to $ (G,\\mathcal{A}_{2}) $. A basic fact in the theory of Lie groups is that a continuous homomorphism between Lie groups is actually smooth. Hence, $ \\text{id}_{G}: (G,\\mathcal{A}_{1}) \\to (G,\\mathcal{A}_{2}) $ is a smooth mapping between smooth manifolds. As $ \\text{id}_{G} $ is its own inverse, $ \\text{id}_{G}: (G,\\mathcal{A}_{2}) \\to (G,\\mathcal{A}_{1}) $ is also a smooth mapping between smooth manifolds. Therefore, $ (G,\\mathcal{A}_{1}) $ is diffeomorphic to $ (G,\\mathcal{A}_{2}) $. $ \\spadesuit $\nTheorem 1 says that if we fix a topology on a group $ G $, then there is at most one Lie group structure on $ G $. What happens when we vary the topology on $ G $ will be investigated in Section 3.\n\nSection 2\nIn this section, we address the issue of which topological groups admit or do not admit Lie group structures. This issue is very much related to Hilbert\u2019s Fifth Problem, which, in its original formulation, asks for the minimal hypotheses that one needs to put on a topological group so that it admits a Lie group structure. Once again, if a Lie group structure exists, then its uniqueness is guaranteed according to Section 1.\nIn their attempt to resolve Hilbert\u2019s Fifth Problem, Andrew Gleason, Deane Montgomery and Leo Zippin proved the following deep theorem in the 1950\u2019s.\n\nTheorem 2 (G-M-Z) Let $ G $ be a topological group. If $ G $ is locally Euclidean, then $ G $ admits a Lie group structure.\n\nTo appreciate the power of this theorem, realize that it says, \u201cA topological group that is merely a topological manifold is, in fact, a Lie group!\u201d A detailed and insightful proof may be found in this set of notes posted by Professor Terence Tao on his research blog. Upon reading his notes, one will notice that a key concept used in the proof is that of a group having no small subgroups.\n\nDefinition A topological group $ G $ is said to have the No-Small-Subgroup (NSS) Property iff there exists a neighborhood $ U $ of the identity element that does not contain any non-trivial subgroup of $ G $.\n\nOne can consult this other set of notes by Professor Tao in order to understand the importance of the NSS Property. The Japanese mathematicians Morikuni Got\u00f4 and Hidehiko Yamabe were the first ones to formulate this property (in a 1951 paper), which Yamabe then used to recast the G-M-Z solution of Hilbert\u2019s Fifth Problem in a manner that reflects the algebro-topological structure of Lie groups more directly.\n\nTheorem 3 (Yamabe) Any connected and locally compact topological group $ G $ is the projective limit of a sequence of Lie groups. If $ G $ is locally compact and has the NSS Property, then $ G $ admits a Lie group structure.\n\nTheorem 3 implies Theorem 2 because a locally Euclidean group is locally compact (an obvious assertion) and has the NSS Property (a non-trivial assertion). It is far from obvious why both local compactness and the NSS Property should imply locally Euclidean back; indeed, this is the main content of Yamabe\u2019s deep theorem.\nNow, a well-known example of a topological group that does not admit a Lie group structure is the group $ \\mathbb{Z}_{p} $ of the $ p $-adic integers with the $ p $-adic topology. It is a completely metrizable and compact topological group, but as it is a totally disconnected space, it cannot admit a Lie group structure for obvious topological reasons.\nIn general, profinite groups (these are topological groups that are compact, Hausdorff and totally disconnected) do not admit Lie group structures. Examples of profinite groups are the discrete finite groups (these are $ 0 $-dimensional manifolds, but we can ignore them as topologically uninteresting), \u00e9tale fundamental groups of connected affine schemes and Galois groups equipped with the Krull topology (this means that I am referring also to groups that correspond to infinite Galois extensions, not just the finite ones). In fact, every profinite group is an \u00e9tale fundamental group in disguise, in the sense that every profinite group is topologically isomorphic to the \u00e9tale fundamental group of some connected affine scheme.\nAlthough the group $ \\mathbb{Q}_{p} $ of $ p $-adic numbers is not profinite (it is locally compact, not compact), it is also a totally disconnected space, so it does not admit a Lie group structure.\nUntil now, we have stayed within the realm of locally compact topological groups. The OP has asked about the non-locally compact case, so here is an attempt at a response.\nThe Swedish functional-analyst Per Enflo did his Ph.D thesis on Hilbert\u2019s Fifth Problem by investigating to what extent the results of Montgomery and Zippin, formulated only in the finite-dimensional setting, could be carried over to the infinite-dimensional setting. He performed his investigation on topological groups that are modeled on (locally homeomorphic to) infinite-dimensional Banach spaces. The main reason for using infinite-dimensional Banach spaces is due to the following basic theorem from functional analysis.\n\nTheorem 4 A Banach space is locally compact iff it is finite-dimensional.\n\nCiting unfamiliarity with Enflo\u2019s work, we kindly request the reader to consult the references that are provided below.\nNote: The OP did say that giving references only was okay! :)\n\nSection 3\nIn this final section, we shall see how to put different topological structures on an abstract group. Toward this end, let us state the following theorem.\n\nTheorem 5 Let $ G $ be an abstract group and $ H $ a topological group. For any group homomorphism $ \\phi: G \\to H $, the pre-image topology on $ G $ induced by $ \\phi $ makes $ G $ a topological group. If $ \\phi $ is further an isomorphism, then $ G $ with the pre-image topology is topologically isomorphic to $ H $.\n\nProof: The pre-image topology on $ G $ induced by $ \\phi $ is defined as the following collection of subsets of $ G $:\n$$\n\\{ {\\phi^{\\leftarrow}}[U] \\in \\mathcal{P}(G) ~|~ \\text{$ U $ is an open subset of $ H $} \\}.\n$$\nPick an open subset $ U $ of $ H $. Then\n\\begin{align}\n    \\{ (g_{1},g_{2}) \\in G \\times G ~|~ g_{1} g_{2} \\in {\\phi^{\\leftarrow}}[U] \\}\n&=  \\{ (g_{1},g_{2}) \\in G \\times G ~|~ \\phi(g_{1} g_{2}) \\in U \\} \\\\\n&=  \\{ (g_{1},g_{2}) \\in G \\times G ~|~ \\phi(g_{1}) \\phi(g_{2}) \\in U \\} \\\\\n&=  {(\\phi \\times \\phi)^{\\leftarrow}}[\\{ (h_{1},h_{2}) \\in H \\times H ~|~ h_{1} h_{2} \\in U \\}] \\\\\n&=: {(\\phi \\times \\phi)^{\\leftarrow}}[V].\n\\end{align}\nMultiplication is continuous in $ H $, so $ V $ is an open subset of $ H \\times H $. Therefore, $ {(\\phi \\times \\phi)^{\\leftarrow}}[V] $ is an open subset of $ G \\times G $ w.r.t. the product pre-image topology. As $ U $ is arbitrary, this implies that group multiplication in $ G $ is indeed continuous w.r.t. the pre-image topology.\nNext, we have\n\\begin{align}\n    \\{ g \\in G ~|~ g^{-1} \\in {\\phi^{\\leftarrow}}[U] \\}\n&=  \\{ g \\in G ~|~ \\phi(g^{-1}) \\in U \\} \\\\\n&=  \\{ g \\in G ~|~ [\\phi(g)]^{-1} \\in U \\} \\\\\n&=  {\\phi^{\\leftarrow}}[\\{ h \\in H ~|~ h^{-1} \\in U \\}] \\\\\n&=: {\\phi^{\\leftarrow}}[W].\n\\end{align}\nInversion is continuous in $ H $, so $ W $ is an open subset of $ H $. Therefore, $ {\\phi^{\\leftarrow}}[W] $ is an open subset of $ G $ w.r.t. the pre-image topology. As $ U $ is arbitrary, this implies that inversion in $ G $ is indeed continuous w.r.t. the pre-image topology.\nThe proof of the final statement is easy enough to be left to the reader. $ \\quad \\spadesuit $\nFor distinct positive integers $ m $ and $ n $, the groups $ \\mathbb{R}^{m} $ and $ \\mathbb{R}^{n} $ are isomorphic because they are isomorphic as $ \\mathbb{Q} $-vector spaces. To prove the second assertion, first use the Axiom of Choice to deduce the existence of Hamel $ \\mathbb{Q} $-bases for $ \\mathbb{R}^{m} $ and $ \\mathbb{R}^{n} $. Then show that a Hamel $ \\mathbb{Q} $-basis $ \\beta_{m} $ for $ \\mathbb{R}^{m} $ and a Hamel $ \\mathbb{Q} $-basis $ \\beta_{n} $ for $ \\mathbb{R}^{n} $ have the same cardinality, namely $ 2^{\\aleph_{0}} $. Any bijection (there are uncountably many) from $ \\beta_{m} $ to $ \\beta_{n} $ now defines a unique vector-space isomorphism from $ \\mathbb{R}^{m} $ to $ \\mathbb{R}^{n} $.\nGiven this vector-space isomorphism, transfer the standard topology on $ \\mathbb{R}^{n} $ to $ \\mathbb{R}^{m} $. With the pre-image topology, $ \\mathbb{R}^{m} $ is a topological group that is topologically isomorphic to $ \\mathbb{R}^{n} $ with the standard topology. It follows from Invariance of Domain (a result in algebraic topology) that the new $ \\mathbb{R}^{m} $ is not topologically isomorphic to $ \\mathbb{R}^{m} $ with the standard topology.\nThe OP has asked if there is a non-trivial example involving non-Euclidean spaces. Off-hand, I do not have one in mind, but one can carry out the following procedure, which is in the same spirit as the previous example.\n\n(1) Take two known topological groups, $ G $ and $ H $, with different topological properties.\n\n\n(2) If one can find a discontinuous group isomorphism $ \\phi: G \\to H $, use $ \\phi $ to transfer the topology on $ H $ to $ G $.\n\n\n(3) Then $ G $ with the pre-image topology is not topologically isomorphic to $ G $ with the original topology.\n\n\n(4) If $ H $ further admits a Lie group structure, then this Lie group structure can be transferred to $ G $, where there might have been none before.\n\nReferences\n\nMontgomery, D; Zippin, L. Topological Transformation Groups, New York, Interscience Publishers, Inc. (1955).\n\nGot\u00f4, M. Hidehiko Yamabe (1923 - 1960), Osaka Math. J., Vol. 13, 1 (1961), i-ii.\n\nYamabe, H. On the Conjecture of Iwasawa and Gleason, Annals of Math., 58 (1953), pp. 48-54.\n\nYamabe, H. A Generalization of a Theorem of Gleason, Annals of Math., 58 (1953), pp. 351 - 365.\n\nEnflo, P. Topological Groups in which Multiplication on One Side Is Differentiable or Linear, Math. Scand., 24 (1969), pp. 195\u2013197.\n\nEnflo, P. On the Nonexistence of Uniform Homeomorphisms Between $ L^{p} $ Spaces, Ark. Math., 8 (1969), pp. 103\u2013105.\n\nEnflo, P. On a Problem of Smirnov, Ark. Math., 8 (1969), pp. 107\u2013109.\n\nEnflo, P. Uniform Structures and Square Roots in Topological Groups, I, Israel J. Math., 8 (1970), pp. 230\u2013252.\n\nEnflo, P. Uniform Structures and Square Roots in Topological Groups, II, Israel J. Math., 8 (1970), pp. 253\u2013272.\n\nMagyar, Z. Continuous Linear Representations, Elsevier, 168 (1992), pp. 273-274.\n\nBenyamini, Y; Lindenstrauss, J. Geometric Nonlinear Functional Analysis, Volume 1, AMS Publ. (1999).", "meta": {"post_id": 250292, "input_score": 54, "output_score": 96, "post_title": "Given a group $ G $, how many topological/Lie group structures does $ G $ have?"}}
{"input": "What is the geometric meaning of the determinant of a matrix? I know that \"The determinant of a matrix represents the area of \u200b\u200ba rectangle.\" Perhaps this phrase is imprecise, but I would like to know something more, please.\nThank you very much.", "output": "If you think about the matrix as representing a linear transformation, then the determinant (technically the absolute value of the determinant) represents the \"volume distortion\" experienced by a region after being transformed.  So for instance, the matrix $2I$ stretches a square of area 1 into a square with area 4, since the determinant is 4.  This idea works in all dimensions too, not just 2 or 3!  \nThis also translates well when you get in to more general mappings $f:\\Bbb{R}^n\\rightarrow\\Bbb{R}^n$; if the function is nice enough, you can represent it \"locally\" by a linear transformation.  The (absolute value of the) determinant of this linear transformation gives the \"local\" volume distortion of the function - i.e. how much the function is stretching or compressing regions of space near a point.", "meta": {"post_id": 250534, "input_score": 39, "output_score": 41, "post_title": "Geometric meaning of the determinant of a matrix"}}
{"input": "I'm working with trace of matrices. Trace is defined for square matrix and there are some useful rule, i.e. $\\text{tr}(AB) = \\text{tr}(BA)$, with $A$ and $B$ square, and more in general trace is invariant under cyclic permutation.\nI was wondering if the formula $\\text{tr}(AB) = \\text{tr}(BA)$ holds even if $A$ and $B$ are rectangular, namely $A$ is $n$-by-$m$ and $B$ is $m$-by-$n$.\nI figured out that if one completes the involved matrices to be square by adding zero entries in the right places, then the formula still works... but I want to be sure about this!", "output": "Yes, it holds true. Let $A$ be a $n\\times m$ and $B$ be a $m \\times n$ matrix over the commutative ring $R$, we have\n\\begin{align*}\n  \\mathrm{tr}(AB) &= \\sum_{i=1}^n (AB)_{ii}\\\\\n    &=\\sum_{i=1}^n \\sum_{j=1}^m A_{ij}B_{ji}\\\\\n    &= \\sum_{j=1}^m \\sum_{i=1}^n B_{ji}A_{ij}\\\\\n    &= \\sum_{j=1}^m (BA)_{jj}\\\\\n    &= \\mathrm{tr}(BA)\n\\end{align*}\nSo you can just prove the formula by computing.", "meta": {"post_id": 252272, "input_score": 30, "output_score": 42, "post_title": "Is trace invariant under cyclic permutation with rectangular matrices?"}}
{"input": "Could you guide me how to prove that any monotone function from $R\\rightarrow R$ is Borel measurable?\nSince monotone functions are continuous away from countably many points, would that be helpful in proving the measurability?", "output": "Hint: If $f$ is monotone, then, for every real number $x$, the set $$f^{-1}((-\\infty,x])=\\{t\\mid f(t)\\leqslant x\\}$$ is either $\\varnothing$ or $(-\\infty,+\\infty)$ or $(-\\infty,z)$ or $(-\\infty,z]$ or $(z,+\\infty)$ or $[z,+\\infty)$ for some real number $z$.\nTo show this, assume for example that $f$ is nondecreasing and that $u$ is in $f^{-1}((-\\infty,x])$, then show that, for every $v\\leqslant u$, $v$ is also in $f^{-1}((-\\infty,x])$.", "meta": {"post_id": 252421, "input_score": 30, "output_score": 50, "post_title": "Are Monotone functions Borel Measurable?"}}
{"input": "In \"Surely You're Joking, Mr. Feynman!,\" Nobel-prize winning Physicist Richard Feynman said that he challenged his colleagues to give him an integral that they could evaluate with only complex methods that he could not do with real methods:\n\nOne\u00a0time\u00a0I boasted, \"I can do by other\u00a0methods any integral anybody else\u00a0needs contour integration to do.\"\nSo\u00a0Paul [Olum] puts up\u00a0this tremendous damn integral he\u00a0had\u00a0obtained\u00a0by starting\u00a0out\u00a0with a\u00a0complex function that he\u00a0knew\u00a0the\u00a0answer\u00a0to, taking\u00a0out the\u00a0real part of it and\u00a0leaving\u00a0only the\u00a0complex part. He\u00a0had\u00a0unwrapped\u00a0it so it was only possible\u00a0by contour integration! He was always deflating\u00a0me\u00a0like\u00a0that. He\u00a0was a\u00a0very smart fellow.\n\nDoes anyone happen to know what this integral was?", "output": "I doubt that we will ever know the exact integral that vexed Feynman.\nHere is something similar to what he describes.\nSuppose $f(z)$ is an analytic function on the unit disk.\nThen, by Cauchy's integral formula,\n$$\\oint_\\gamma \\frac{f(z)}{z}dz = 2\\pi i f(0),$$\nwhere $\\gamma$ traces out the unit circle in a counterclockwise manner.\nLet $z=e^{i\\phi}$.\nThen\n$\\int_0^{2\\pi}f(e^{i\\phi}) d\\phi = 2\\pi f(0).$\nTaking the real part of each side we find \n$$\\begin{equation*}\n\\int_0^{2\\pi} \\mathrm{Re}(f(e^{i\\phi}))d\\phi = 2\\pi \\mathrm{Re}(f(0)).\\tag{1}\n\\end{equation*}$$\n(We could just as well take the imaginary part.)\nClearly we can build some terrible integrals by choosing $f$ appropriately.\nExample 1.\nLet $\\displaystyle f(z) = \\exp\\frac{2+z}{3+z}$.\nThis is a mild choice compared to what could be done ...\nIn any case, $f$ is analytic on the disk.\nApplying (1), and after some manipulations of the integrand, we find\n$$\\int_0^{2\\pi}\n\\exp\\left(\\frac{7+5 \\cos\\phi}{10+6\\cos\\phi}\\right)\n\\cos \\left(\n\\frac{\\sin\\phi}{10+6 \\cos\\phi}\n\\right)\nd\\phi = 2\\pi e^{2/3}.$$\nExample 2.\nLet $\\displaystyle f(z) = \\exp \\exp \\frac{2+z}{3+z}$. \nThen \n\\begin{align*}\\int_0^{2\\pi} & \n\\exp\\left(\n \\exp\\left(\n  \\frac{7+5 \\cos \\phi}{10+6 \\cos \\phi}\n \\right) \n \\cos\\left(\n  \\frac{\\sin \\phi}{10+6 \\cos \\phi}\n \\right)\n\\right) \\\\\n& \\times\\cos\\left(\n \\exp\\left(\n     \\frac{7+5 \\cos \\phi}{10+6 \\cos \\phi}\n    \\right) \n \\sin\\left(\n     \\frac{\\sin \\phi}{10+6 \\cos \\phi}\n    \\right)\n\\right) = 2\\pi e^{e^{2/3}}.\n\\end{align*}", "meta": {"post_id": 253910, "input_score": 251, "output_score": 77, "post_title": "The Integral that Stumped Feynman?"}}
{"input": "Suppose $G$ is a group. Does there always exist a group $H$, such that $\\operatorname{Aut}(H)=G$, i. e. such that $G$ is the automorphism group of $H$?\nEDIT: It has been pointed out that  the answer to the above question is no. But I would be much more pleased if someone could give an example of such a group $G$ not arising as an automorphism group together with a comparatively easy proof of this fact.", "output": "Theorem. The following cyclic groups cannot be the automorphism group of any group:\n\nThe infinite cyclic group $C_{\\infty}$ (also known as $\\mathbb{Z}$), and\nCyclic groups $C_{n}$ of odd order (also known as $\\mathbb{Z}_n$ or $\\mathbb{Z}/n\\mathbb{Z}$).\n\nThe proof is relatively straight forward, with a subtlety at the end, and consists of two lemmata. I shall leave you to pin the lemmata together and get the result. (Hint. what are the inner automorphisms isomorphic to?)\nLemma 1: If $G/Z(G)$ is cyclic then $G$ is abelian.\nProof: This is a standard undergrad question, so I'll let you figure out the proof for yourself.\nLemma 2: If $G\\not\\cong C_2$ is abelian then $\\operatorname{Aut}(G)$ has an element of order two.\n(Here, $C_2$ is the cyclic group of order two. Note that this group has trivial automorphism group.)\nProof: The negation map $n: a\\mapsto a^{-1}$ is non-trivial of order two unless $G$ comprises of elements of order two. If $G$ consists only of elements of order two then, applying the Axiom of Choice, $G$ is the direct sum of cyclic groups of order two, $$G\\cong C_2\\times C_2\\times\\ldots$$ See this question for why. Finally, because $G\\not\\cong C_2$ there are at least two copies of $C_2$, and so we can switch them (and \"switching\" has order two).\n\nThe subtlety I mentioned at the start is the use of Choice in the proof of Lemma 2. If we do not assume Choice that it is consistent that there exists a group $G$ of order greater than two such that $\\operatorname{Aut(G)}$ is trivial. This was (first) proven by Asaf Karagila in an answer to this MSE question.", "meta": {"post_id": 253936, "input_score": 51, "output_score": 42, "post_title": "Is every group the automorphism group of a group?"}}
{"input": "According to this\nWikipedia article, the expansion for $f(x\\pm h)$ is:\n$$f(x \\pm h) = f(x) \\pm hf'(x) + \\frac{h^2}{2}f''(x) \\pm \\frac{h^3}{6}f^{(3)}(x) + O(h^4)$$\nI'm not understanding how you are left with $f(x)$ terms on the right hand side.\nI tried working out, for example, the Taylor expansion for $f(x + h)$ (using $(x+h)$ as $x_0$) and got this:\n$$ f(x + h) = f(x+h) + f'(x + h)(x-(x+h)) + \\frac{f''(x+h)}{2!}(x-(x+h))^2 + \\frac{f'''(x+h)}{3!} (x - (x + h))^3 + \\cdots $$\n$$ = f(x + h) - hf'(x+h) + \\frac{h^2}{2!}f''(x + h) - \\frac{h^3}{3!} f'''(x+h) + \\cdots$$\nAm I doing this correctly?", "output": "This has already been answered, but I have seen some of the comments to the answer from Andre Nicolas and I thought I could make it clear with a step by step explanation.\nInstead of just doing a substitution, as Andre mentioned, think about the meaning of it.\nStart with the standard Taylor series expansion,\n$$ f(x) \\approx f(x_0)+f\u2032(x_0)(x\u2212x_0)+\\frac{f\u2032\u2032(x_0)}{2!}(x\u2212x_0)^2+\\frac{f\u2032\u2032\u2032(x_0)}{3!}(x\u2212x_0)^3+\u22ef. \\qquad (*)$$\nNow what does $x-x_0$ mean? For convergence, we usually need this to be small, so we can call this $h$. Now substitute $x-x_0=h$ (and obviously $x=x_0+h$) into $(*)$ to get:\n$$ f(x_0+h) \\approx f(x_0)+f\u2032(x_0)h+\\frac{f\u2032\u2032(x_0)}{2!}h^2+\\frac{f\u2032\u2032\u2032(x_0)}{3!}h^3+\u22ef.$$\nThis is in the required form but with $x_0$ instead of $x$ and since this is just a variable we can just substitute it for $x$.", "meta": {"post_id": 254792, "input_score": 55, "output_score": 42, "post_title": "How is the Taylor expansion for $f(x + h)$ derived?"}}
{"input": "Let $p$ and $q$ be polynomials (maybe in several variables, over a field), and suppose they have $m$ and $n$ non-zero terms respectively. We can assume $m\\leq n$. Can it ever happen that the product $p\\cdot q$ has fewer than $m$ non-zero terms?\nI ask this because I vaguely recall seeing a positive answer in book somewhere (probably about computation or algorithms since the polynomials were unwieldy). If anyone knows what book this is from it would be much appreciated.", "output": "Here's an elementary example. Start with the well-known identity $x^n - 1 = (x-1) (x^{n-1} + x^{n-2} + \\ldots + x + 1)$. If $n$ is odd, we can factor $x^n+1$ in a similar way by flipping the signs: $x^n + 1 = (x+1) (x^{n-1} - x^{n-2} + \\ldots - x + 1)$. Now mix and match the two:\n$$\\begin{align*}\n  x^{2n} - 1 &= (x^n - 1) (x^n + 1) \\\\\n  &= (x-1) (x^{n-1} + x^{n-2} + \\ldots + x + 1) (x+1) (x^{n-1} - x^{n-2} + \\ldots - x + 1) \\\\\n  &= (x+1) (x^{n-1} + x^{n-2} + \\ldots + x + 1) (x-1) (x^{n-1} - x^{n-2} + \\ldots - x + 1) \\\\\n  &= (x^n + 2x^{n-1} + 2x^{n-2} + \\ldots + 2x + 1) (x^n - 2x^{n-1} + 2x^{n-2} - \\ldots + 2x - 1)\n\\end{align*}$$\nI don't see an obvious generalization to even values of $n$.", "meta": {"post_id": 256028, "input_score": 49, "output_score": 46, "post_title": "Does multiplying polynomials ever decrease the number of terms?"}}
{"input": "I've found lots of resources that say this is a real number if it's not rational, but what is a real number, really?  I mean what is the definition of a real number?  If nothing else, anyone know of a resource where I could find out myself?\nThanks!", "output": "There is no \"true\" definition of the real numbers because there are several ways to think of the real numbers either as mathematical notions (i.e. we don't really care what are the objects which represent the numbers, we just care about the structure) and there are concrete ways to construct the real numbers, e.g. as sets of rational numbers or equivalence classes of sequences.\nThe structure of the real numbers is unique. It is an order field which is order-complete. It is also the unique complete Archimedean field. This means that if we construct any other field which is ordered and order complete, then we built something which is isomorphic to the real numbers.\nGenerally speaking, if we accept the rational numbers as \"atomic\" (namely, objects whose existence we take for granted, and do not investigate further) then the real numbers can be constructed either as particular sets of rationals, called Dedekind cuts, or as equivalence classes of Cauchy sequences.\nIt is a nontrivial task (at least without seeing it a couple of times before) to prove that either definition gives us this structure we seek. That complete ordered field. It is even less trivial to actually prove the uniqueness of that structure. I won't go into either subjects.\nIn either definition we can find the rationals are embedded into the real numbers, and in most cases we think about the rationals as being part of the real numbers as much as we think about integers being rational numbers.\nOne final remark is that if one prefers not to accept the rational numbers as atomic then it is possible to construct them from the integers, and we can construct those from the natural numbers, and in fact we can construct those just from the empty set.\n\nTo read more:\n\nCompletion of rational numbers via Cauchy sequences\nquestion about construction of real numbers\nConstructing $\\mathbb R$\nWhy does the Dedekind Cut work well enough to define the Reals?\nConstruction of $\\Bbb R$ from $\\Bbb Q$\nIn set theory, how are real numbers represented as sets?", "meta": {"post_id": 261074, "input_score": 21, "output_score": 41, "post_title": "True Definition of the Real Numbers"}}
{"input": "What would be an example of a function that is continuous, but not uniformly continuous?\nWill $f(x)=\\frac{1}{x}$ on the domain $(0,2)$ be an example? And why is it an example?\nPlease explain strictly using relevant definitions.", "output": "Clearly $\\,\\displaystyle{\\frac{1}{x}}\\,$ is continuous in $\\,(0,2)\\,$ as it is the quotient of two polynomials and the denominator  doesn't vanish there.\nNow, if the function was uniformly continuous there then\n$$\\forall\\,\\epsilon>0\\,\\,\\exists\\,\\delta>0\\,\\,s.t.\\,\\,|x-y|<\\delta\\Longrightarrow \\left|\\frac{1}{x}-\\frac{1}{y}\\right|<\\epsilon$$\nBut taking $\\,\\epsilon=1\\,$ , then for any $\\,\\delta>0\\,$ we take\n$$x:=\\min(\\delta,1)\\,\\,,\\,y=\\frac{x}{2}\\Longrightarrow |x-y|=\\frac{x}{2}<\\delta, \\,\\text{but nevertheless}$$\n$$\\left|\\frac{1}{x}-\\frac{1}{y}\\right|=\\left|\\frac{1}{x}-\\frac{2}{x}\\right|=\\left|\\frac{1}{x}\\right|\\geq 1=\\epsilon$$", "meta": {"post_id": 262325, "input_score": 22, "output_score": 42, "post_title": "Coming up with an example, a function that is continuous but not uniformly continuous"}}
{"input": "I am trying to understand the similarities and differences between the minimal polynomial and characteristic polynomial of Matrices.\n\nWhen are the minimal polynomial and characteristic polynomial the same\nWhen are they different\nWhat conditions (eigenvalues/eigenvectors/...) would imply 1 or 2\nPlease tell me anything else about these two polynomials that is essential in comparing them.", "output": "The minimal polynomial is quite literally the smallest (in the sense of divisibility) nonzero polynomial that the matrix satisfies. That is to say, if $A$ has minimal polynomial $m(t)$ then $m(A)=0$, and if $p(t)$ is a nonzero polynomial with $p(A)=0$ then $m(t)$ divides $p(t)$.\nThe characteristic polynomial, on the other hand, is defined algebraically. If $A$ is an $n \\times n$ matrix then its characteristic polynomial $\\chi(t)$ must have degree $n$. This is not true of the minimal polynomial.\nIt can be proved that if $\\lambda$ is an eigenvalue of $A$ then $m(\\lambda)=0$. This is reasonably clear: if $\\vec v \\ne 0$ is a $\\lambda$-eigenvector of $A$ then\n$$m(\\lambda) \\vec v = m(A) \\vec v = 0 \\vec v = 0$$\nand so $m(\\lambda)=0$. The first equality here uses linearity and the fact that $A^n\\vec v = \\lambda^n \\vec v$, which is an easy induction.\nIt can also be proved that $\\chi(A)=0$. In particular that $m(t)\\, |\\, \\chi(t)$.\nSo one example of when (1) occurs is when $A$ has $n$ distinct eigenvalues. If this is so then $m(t)$ has $n$ roots, so has degree $\\ge n$; but it has degree $\\le n$ because it divides $\\chi(t)$. Thus they must be equal (since they're both monic, have the same roots and the same degree, and one divides the other).\nA more complete characterisation of when (1) occurs (and when (2) occurs) can be gained by considering Jordan Normal Form; but I suspect that you've only just learnt about characteristic and minimal polynomials so I don't want to go into JNF.\nLet me know if there's anything else you'd like to know; I no doubt missed some things out.", "meta": {"post_id": 262334, "input_score": 39, "output_score": 50, "post_title": "Minimal polynomials and characteristic polynomials"}}
{"input": "I need help with the following problem. Suppose $Z=N(0,s)$ i.e. normally distributed random variable with standard deviation $\\sqrt{s}$. I need to calculate $E[Z^2]$. My attempt is to do something like\n\\begin{align}\nE[Z^2]=&\\int_0^{+\\infty} y \\cdot Pr(Z^2=y)dy\\\\\n=& \\int_0^{+\\infty}y\\frac{1}{\\sqrt{2\\pi s}}e^{-\\frac y{2s}}dy\\\\\n=&\\frac{1}{\\sqrt{2\\pi s}}\\int_0^{\\infty}ye^{-\\frac y{2s}}dy.\n\\end{align}\nBy using integration by parts we get\n$$\\int_0^{\\infty}ye^{-\\frac y{2s}}dy=\\int_0^{+\\infty}2se^{-\\frac y{2s}}dy=4s^2.$$\nHence $E[Z^2]=\\frac{2s\\sqrt{2s}}{\\sqrt{\\pi}},$ which does not coincide with the answer in the text. Can someone point the mistake?", "output": "This is old, but I feel like an easy derivation is in order.\nThe variance of any random variable $X$ can be written as \n$$\nV[X] = E[X^2] - (E[X])^2\n$$\nSolving for the needed quantity gives\n$$\nE[X^2] = V[X] + (E[X])^2\n$$\nBut for our case, $E[X] = 0$, so the answer of $\\sigma^2$ is immediate.", "meta": {"post_id": 264061, "input_score": 25, "output_score": 107, "post_title": "expected value calculation for squared normal distribution"}}
{"input": "Describe why norms are continuous function by mathematical symbols.", "output": "A function $f$ from a metric space to a metric space is continuous if for all $x$ in the domain, for all $\\varepsilon>0$, the exists $\\delta>0$ such that for all points $y$ in the domain, if the distance from $x$ to $y$ is less than $\\delta$, then the distance from $f(x)$ to $f(y)$ is less than $\\varepsilon$.\nIf $f$ is a norm, then it maps a vector space into $\\mathbb R$, and the distance from $x$ to $y$ is $f(x-y)$.\nIn this case it suffices to take $\\delta=\\varepsilon$, for the following reason.  Suppose the distance from $x$ to $y$ is less than $\\delta=\\varepsilon$.  Then $f(x-y)=f(y-x)<\\varepsilon$ (where the equality follows from the definition of \"norm\").  Now recall that norms satisfy a triangle inequality:\n$$\nf(x) \\le f(y) + f(x-y)\n$$\n$$\nf(y) \\le f(x) + f(y-x)\n$$\nSo\n$$\nf(y)-f(x) \\le f(y-x)<\\varepsilon\\text{ and }f(x)-f(y) \\le f(x-y)<\\varepsilon,\n$$\nso\n$$\n|f(x)-f(y)|<\\varepsilon,\n$$\ni.e.\n$$\n\\Big(\\text{distance from $f(x)$ to $f(y)$}\\Big) <\\varepsilon.\n$$", "meta": {"post_id": 265284, "input_score": 33, "output_score": 38, "post_title": "Why are norms continuous?"}}
{"input": "Let $\\| \\cdot \\|$ be a norm on $\\mathbb{R}^n$. The associated dual norm, denoted $\\| \\cdot \\|_*$ is defined\nas $\\| z \\|_* = \\sup\\{ z^{t} x : \\| x \\| < 1 \\}$. \nDoes someone know how to prove that the dual norm of the $\\mathcal l_{p}$ norm is the $\\mathcal l_{q}$ norm? I read about norms and it was stated without proof in a book.", "output": "Edit: I have edited my answer to conform more to the notation of the paper you linked.\nFirstly, suppose $||\\cdot||$ is a norm on $\\mathbb{R}^n$, and the dual norm $||\\cdot||_*$ is defined as\n$$\n||z||_* := \\sup \\{ z^{\\top} x : x \\in \\mathbb{R}^n, ||x|| \\leq 1\\}\n$$\nfor all $z \\in \\mathbb{R}^n$.\nNote that the quantity $z^{\\top} x = z\\cdot x = \\sum_{i=1}^n z_ix_i$ is just the dot product of $z$ and $x$ if thought of both as row vectors.\nThe paper you linked only mentions the case $1<p,q<\\infty$, so we will work in that framework and not worry about the extremal cases. Fix $p,q \\in (1,\\infty)$ Holder conjugates (i.e. $\\frac1p + \\frac1q = 1$).\nFurthermore, fix $z=(z_1,\\ldots,z_n) \\in \\mathbb{R}^n$. We will show that\n$$\n\\sup \\left\\{ \\sum_{i=1}^n z_ix_i : x=(x_1,\\ldots,x_n) \\in \\mathbb{R}^n, ||x||_q \\leq 1\\right\\} = ||z||_p.\n$$\nWe may assume without loss of generality that $z \\neq 0$, otherwise both norms are trivially zero.\nLet $x=(x_1,\\ldots,x_n) \\in \\mathbb{R}^n$ with $||x||_q \\leq 1$ be given.\nWe have by Holder's inequality that \n$$\n\\sum_{i=1}^n z_ix_i  \\leq \\sum_{i=1}^n |z_ix_i|=  ||zx||_1 \\leq ||z||_p ||x||_q \\leq ||z||_p\n$$\nHence the supremum in question is at most $||z||_p$.\nIn order to show that the supremum is exactly $||z||_p$, it suffices to find a single $y \\in \\mathbb{R}^n$ with $||y||_q \\leq 1$ such that $\\sum_{i=1}^n z_iy_i = ||z||_p$.\nLet $x := \\mathrm{sign}(z) |z|^{p-1}$, i.e. $x_i := \\mathrm{sign}(z_i)|z_i|^{p-1}$ for all $i=1,\\ldots,n$.\nWe calculate\n$$\n\\sum_{i=1}^n z_i x_i = \\sum_{i=1}^n z_i  \\mathrm{sign}(z_i)|z_i|^{p-1} =\n\\sum_{i=1}^n |z_i|^p =  ||z||_p^p\n$$\nwhere here we used the fact that $z_i \\mathrm{sign}(z_i) = |z_i|$.\nFurthermore, we calculate\n$$\n||x||_q^q = \\sum_{i=1}^n |x_i|^q = \\sum_{i=1}^n |\\mathrm{sign}(z_i)|z_i|^{p-1}|^q \n= \\sum_{i=1}^n |z_i|^{q(p-1)}= \\sum_{i=1}^n |z_i|^p =||z||_p^p\n$$\nwhere here we used the fact that since $\\frac1p + \\frac1q = 1$ we have $q(p-1) = p$.\nNow choose $y := \\frac{x}{||x||_q}$ (this is where we used the fact that $z \\neq 0$, so that $||x||_q > 0$). By construction we have $||y||_q = 1$, and \n$$\n\\sum_{i=1}^n z_i y_i = \\sum_{i=1}^n z_i  \\frac{x_i}{||x||_q} = \\frac{1}{||x||_q}\\sum_{i=1}^n z_i x_i\n$$\nand using the fact that $||x||_q = (||x||_q^q)^{1/q} = (||z||_p^p)^{1/q} = ||z||_p^{p/q}$ and that $\\sum_{i=1}^n z_ix_i = ||z||_p^p$ we have that\n$$\n \\frac{1}{||x||_q}\\sum_{i=1}^n z_i x_i = \\frac{1}{ ||z||_p^{p/q}}||z||_p^p = ||z||_p^{p-p/q} = ||z||_p\n$$\nwhere here we used the fact that $\\frac1p + \\frac1q = 1$ implies $p-p/q = p(1-1/q) = p(1/p) = 1$.\nThus we have found $y \\in \\mathbb{R}^n$ with $||y||_q \\leq 1$ such that $\\sum_{i=1}^n z_i y_i = ||z||_p$ as desired, completing the proof.", "meta": {"post_id": 265721, "input_score": 19, "output_score": 34, "post_title": "Proving that the dual of the $\\mathcal{l}_p$ norm is the $\\mathcal{l}_q$ norm."}}
{"input": "Given an expression such as $f(x) = x^x$, is it possible to provide a thorough and rigorous proof that there is no function $F(x)$ (expressible in terms of known algebraic and transcendental functions) such that $ \\frac{d}{dx}F(x) = f(x)$? In other words, how can you rigorously prove that $f(x)$ does not have an elementary antiderivative?", "output": "To get some background and references you may start with this SE thread.\nConcerning your specific question about $z^z$ here is an extract from a sci.math answer by Matthew P Wiener :\n\"Finally, we consider the case $I(z^z)$.\nSo this time, let $F=C(z,l)(t)$, the field of rational functions in $z,l,t$, where $l=\\log z\\,$ and $t=\\exp(z\\,l)=z^z$.  Note that $z,l,t$ are algebraically\nindependent.  (Choose some appropriate domain of definition.)  Then $t'=(1+l)t$, so for $a=t$ in the above situation, the partial fraction\nanalysis (of the sort done in the previous posts) shows that the only possibility is for $v=wt+\\cdots$ to be the source of the $t$ term on the left,\nwith $w$ in $C(z,l)$.\nSo this means, equating $t$ coefficients, $1=w'+(l+1)w$.  This is a first order ODE, whose solution is $w=\\frac{I(z^z)}{z^z}$.  So we must prove that no\nsuch $w$ exists in $C(z,l)$.\nSo suppose (as in one of Ray Steiner's posts) $w=\\frac PQ$, with $P,Q$ in $C[z,l]$ and having no common factors.  Then $z^z=\n\\left(z^z\\cdot \\frac PQ\\right)'=z^z\\cdot\\frac{[(1+l)PQ+P'Q-PQ']}{Q^2}$, or $Q^2=(1+l)PQ+P'Q-PQ'$.\nSo $Q|Q'$, meaning $Q$ is a constant, which we may assume to be one.  So we have it down to $P'+P+lP=1$.\nLet $P=\\sum_{i=0}^n [P_i l^i]$, with $P_i, i=0\\cdots n \\in C[z]$.  But then in our equation, there's a dangling $P_n l^{n+1}$ term, a contradiction.\"\n$$-$$\nFor future references here is a complete re-transcript of Matthew P Wiener's $1997$ sci.math article (converted in $\\LaTeX$ by myself : feel free to fix it!).\nA neat translation in french by Denis Feldmann is available at his homepage.\n\nWhat's the antiderivative of $\\ e^{-x^2}\\ $? of $\\ \\frac{\\sin(x)}x\\ $? of $\\ x^x\\ $?\n\nThese, and some similar problems, can't be done.\nMore precisely, consider the notion of \"elementary function\". These are the functions that can be expressed in terms of exponentals and logarithms, via the usual algebraic processes, including the solving (with or without radicals) of polynomials. Since the trigonometric functions and their inverses can be expressed in terms of exponentials and logarithms using the complex numbers $\\mathbb{C}$, these too are elementary.\nThe elementary functions are, so to speak, the \"precalculus functions\".\nThen there is a theorem that says certain elementary functions do not\nhave an elementary antiderivative.  They still have antiderivatives,\nbut \"they can't be done\".  The more common ones get their own names.\nUp to some scaling factors, \"$\\mathrm{erf}$\" is the antiderivative of $e^{-x^2}$ and \"$\\mathrm{Si}$\" is the antiderivative of $\\frac{\\sin(x)}x$, and so on.\n\nFor those with a little bit of undergraduate algebra, we sketch a proof\nof these, and a few others, using the notion of a differential field.\nThese are fields $(F,+,\\cdot,1,0)$ equipped with a derivation, that is, a\nunary operator $'$ satisfying $(a+b)'=a'+b'$ and $(a.b)'=a.b'+a'.b$.  Given\na differential field $F$, there is a subfield $\\mathrm{Con}(F)=\\{a:a'=0\\}$, called the constants of $F$.  We let $I(f)$ denote an antiderivative.  We ignore $+C$s.\nMost examples in practice are subfields of $M$, the meromorphic functions on $\\mathbb{C}$ (or some domain).  Because of uniqueness of analytic extensions, one rarely has to specify the precise domain.\nGiven differential fields $F$ and $G$, with $F$ a subfield of $G$, one calls $G$ an algebraic extension of $F$ if $G$ is a finite field extension of $F$.\nOne calls $G$ a logarithmic extension of $F$ if $G=F(t)$ for some transcendental\n$t$ that satisfies $t'=\\dfrac{s'}s$, some $s$ in $F$.  We may think of $t$ as $\\;\\log s$, but note that we are not actually talking about a logarithm function on $F$.\nWe simply have a new element with the right derivative. Other \"logarithms\" would have to be adjoined as needed.\nSimilarly, one calls $G$ an exponential extension of $F$ if $G=F(t)$ for some transcendental $t$ that satisfies $t'=t.s'$, some $s$ in $F$.  Again, we may think of $t$ as $\\;\\exp s$, but there is no actual exponential function on $F$.\nFinally, we call $G$ an elementary differential extension of $F$ if there is a finite chain of subfields from $F$ to $G$, each an algebraic, logarithmic, or exponential extension of the next smaller field.\nThe following theorem, in the special case of $M$, is due to Liouville.\nThe algebraic generality is due to Rosenlicht.  More powerful theorems have been proven by Risch, Davenport, and others, and are at the heart of symbolic integration packages.\nA short proof, accessible to those with a solid background in undergraduate algebra, can be found in Rosenlicht's AMM paper (see references).  It is probably easier to master its applications first, which often use similar techniques, and then learn the proof.\n\nMAIN THEOREM.  Let $F,G$ be differential fields, let $a$ be in $F$, let $y$ be in $G$, and suppose $y'=a$ and $G$ is an elementary differential extension field of $F$, and $\\mathrm{Con}(F)=\\mathrm{Con}(G)$.  Then there exist $c_1,...,c_n \\in \\mathrm{Con}(F), u_1,\\cdots,u_n, v\\in F$ such that\n$$a  =  c_1\\frac{u_1'}{u_1}+ ... + c_n\\frac{u_n'}{u_n}+ v'$$\nThat is, the only functions that have elementary antiderivatives are the ones that have this very specific form.  In words, elementary integrals always consist of a function at the same algebraic \"complexity\" level as the starting function (the $v$), along with the logarithms of functions at the same algebraic \"complexity\" level (the $u_i$'s).\n\nThis is a very useful theorem for proving non-integrability.  Because this topic is of interest, but it is only written up in bits and pieces, I give numerous examples.  (Since the original version of this FAQ from way back when, two how-to-work-it write-ups have appeared.  See Fitt & Hoare and Marchisotto & Zakeri in the references.)\nIn the usual case, $F,G$ are subfields of $M$, so $\\mathrm{Con}(F)=\\mathrm{Con}(G)$ always holds, both being $\\mathbb{C}$.  As a side comment, we remark that this equality is necessary.\nOver $\\mathbb{R}(x)$, $\\frac 1{1+x^2}$ has an elementary antiderivative, but none of the above form.\nWe first apply this theorem to the case of integrating $f\\cdot\\exp(g)$, with $f$ and $g$ rational functions.  If $g=0$, this is just $f$, which can be integrated\nvia partial fractions.  So assume $g$ is nonzero.  Let $t=\\exp(g)$, so $t'=g't$.\nSince $g$ is not zero, it has a pole somewhere (perhaps out at infinity), so $\\exp(g)$ has an essential singularity, and thus $t$ is transcendental over $C(z)$.  Let $F=C(z)(t)$, and let $G$ be an elementary differential extension containing an antiderivative for $f\\cdot t$.\nThen Liouville's theorem applies, so we can write\n$$f\\cdot t =  c_1\\frac{u_1'}{u_1} + \\cdots + c_n \\frac{u_n'}{u_n} + v'$$\nwith the $c_i$ constants and the $u_i$ and $v$ in $F$.  Each $u_i$ is a ratio of two $C(z)[t]$ polynomials, $\\dfrac UV$ say.  But $\\dfrac {(U/V)'}{U/V}=\\dfrac {U'}U-\\dfrac{V'}V$ (quotient rule), so we may rewrite the above and assume each $u_i$ is in $C(z)[t]$.\nAnd if any $u_i=U\\cdot V$ factors, then $\\dfrac{(U\\cdot V)'}{(U\\cdot V)}=\\dfrac {U'}U+\\dfrac {V'}V$ and so we can further assume each $u_i$ is irreducible over $C(z)$.\nWhat does a typical $\\frac {u'}u$ look like?  For example, consider the case of $u$ quadratic in $t$.  If $A,B,C$ are rational functions in $C(z)$, then $A',B',C'$ are also rational functions in $C(z)$ and\n\\begin{align}\n\\frac {(A.t^2+B.t+C)'}{A.t^2+B.t+C} &= \\frac{A'.t^2 + 2At(gt) + B'.t + B.(gt) + C'}{A.t^2 + B.t + C}\\\\\n&= \\frac{(A'+2Ag).t^2 + (B'+Bg).t + C'}{A.t^2 + B.t + C}\\\\\n\\end{align}\n(Note that contrary to the usual situation, the degree of a polynomial in $t$ stays the same after differentiation.  That is because we are taking derivatives with respect to $z$, not $t$.  If we write this out explicitly, we get $(t^n)' = \\exp(ng)' = ng'\\cdot \\exp(ng) = ng'\\cdot t^n$.)\nIn general, each $\\frac {u'}u$ is a ratio of polynomials of the same degree.  We can, by doing one step of a long division, also write it as $D+\\frac Ru$, for some $D \\in C(z)$ and $R \\in C(z)[t]$, with $\\deg(R)<\\deg(u)$.\nBy taking partial fractions, we can write $v$ as a sum of a $C(z)[t]$ polynomial\nand some fractions $\\frac P{Q^n}$ with $\\deg(P)<\\deg(Q)$, $Q$ irreducible, with each $P,Q \\in C(z)[t]$.  $v'$ will thus be a polynomial plus partial fraction like terms.\nSomehow, this is supposed to come out to just $f\\cdot t$.  By the uniqueness of partial fraction decompositions, all terms other than multiples of $t$ add up to $0$.  Only the polynomial part of $v$ can contribute to $f\\cdot t$, and this must be a monomial over $C(z)$.  So $f\\cdot t=(h\\cdot t)'$, for some rational $h$.  (The temptation to assert $v=h\\cdot t$ here is incorrect, as there could be some $C(z)$ term, cancelled by $\\frac {u'}u$ terms.  We only need to identify the terms in $v$ that contribute to $f\\cdot t$, so this does not matter.)\nSummarizing, if $f\\cdot \\exp(g)$ has an elementary antiderivative, with $f$ and $g$ rational functions, $g$ nonzero, then it is of the form $h\\cdot \\exp(g)$, with $h$ rational.\nWe work out particular examples, of this and related applications.  A bracketed function can be reduced to the specified example by a change of variables.\n$\\quad\\boxed{\\displaystyle\\exp\\bigl(z^2\\bigr)}$ $\\quad\\left[\\sqrt{z}\\cdot\\exp(z),\\frac{\\exp(z)}{\\sqrt{z}}\\right]$\nLet $h\\cdot \\exp\\bigl(z^2\\bigr)$ be its antiderivative.  Then $h'+2zh=1$.\nSolving this ODE gives $h=\\exp(-z^2)\\cdot I\\left(\\exp\\bigl(z^2\\bigr)\\right)$, which has no pole (except perhaps at infinity), so $h$, if rational, must be a polynomial.  But the derivative of $h$ cannot cancel the leading power of $2zh$, contradiction.\n$\\quad\\boxed{\\displaystyle\\frac{\\exp(z)}z}$ $\\quad\\left[\\exp(\\exp(z)),\\frac 1{\\log(z)}\\right]$\nLet $h\\cdot \\exp(z)$ be an antiderivative.  Then $h'+h=\\frac 1z$.  I know of two quick ways to prove that $h$ is not rational.\nOne can explicitly solve the first order ODE (getting\n$\\exp(-z)\\cdot I\\left(\\frac{\\exp(z)}z\\right))$, and then notice that the solution has a logarithmic singularity at zero.\nFor example, $h(z)\\to\\infty$ but $\\sqrt{z}\\cdot h(z)\\to 0$ as $z\\to 0$.  No rational function does this.\nOr one can assume $h$ has a partial fraction decomposition.  Obviously no\n$h'$ term will give $\\frac 1z$, so $\\frac 1z$ must be present in $h$ already. But $\\left(\\frac 1z\\right)'=-\\frac 1{z^2}$,\nand this is part of $h'$.  So there is a $\\frac 1{z^2}$ in $h$ to cancel this.  But $\\left(\\frac 1{z^2}\\right)'$ is $-\\frac 2{z^3}$, and this is again part of $h'$.  And again, something in $h$ cancels this, etc etc etc.  This infinite regression is impossible.\n$\\quad\\boxed{\\displaystyle\\frac {\\sin(z)}z}$ $\\quad[\\sin(\\exp(z))]$\n$\\quad\\boxed{\\displaystyle\\sin\\bigl(z^2\\bigr)}$ $\\quad\\left[\\sqrt{z}\\sin(z),\\frac{\\sin(z)}{\\sqrt{z}}\\right]$\nSince $\\sin(z)=\\frac 1{2i}[\\exp(iz)-\\exp(-iz)]$, we merely rework the above $f\\cdot \\exp(g)$ result.  Let $f$ be rational, let $t=\\exp(iz)$ (so $\\frac {t'}t=i$) and let $T=\\exp(iz^2)$ (so $\\frac{T'}T=2iz$) and we want an antiderivative of either $\\frac 1{2i}f\\cdot\\left(t-\\frac 1t\\right)$ or $\\frac 1{2i}f\\cdot(T-\\frac 1T)$.  For the former, the same partial fraction results still apply in identifying $\\frac 1{2i}f\\cdot t=(h\\cdot t)'=(h'+ih)\\cdot t$, which can't happen for $f=\\frac 1z$, as above.  In the case of $f\\cdot\\sin\\bigl(z^2\\bigr)$, we want $\\frac 1{2i}f\\cdot T=(h\\cdot T)'=(h'+2izh)\\cdot T$, and again, this can't happen for $f=1$, as above.\nAlthough done, we push this analysis further in the $f\\cdot \\sin(z)$ case, as there are extra terms hanging around.  This time around, the conclusion gives an additional $\\frac kt$ term inside $v$, so we have $-\\frac 1{2i}\\frac ft=\\left(\\frac kt\\right)'=\\frac{k'-ik}t$.\nSo the antiderivative of $\\frac 1{2i}f\\cdot\\left(t-\\frac 1t\\right)$ is $h\\cdot t+\\frac kt$.\nIf $f$ is even and real, then $h$ and $k$ (like $t=\\exp(iz)$ and $\\frac 1t=\\exp(-iz)$) are parity flips of each other, so (as expected) the antiderivative is even.\nLetting $C=\\cos(z), S=\\sin(z), h=H+iF$ and $k=K+iG$, the real (and only) part of the antiderivative of $f$ is $(HC-FS)+(KC+GS)=(H+K)C+(G-F)S$.\nSo over the reals, we find that the antiderivative of (rational even).$\\sin(x)$ is of the form (rational even).$\\cos(x)+$ (rational odd).$\\sin(x)$.\nA similar result holds for (odd)$\\cdot\\sin(x)$, (even)$\\cdot\\cos(x)$, (odd)$\\cdot\\cos(x)$.\nAnd since a rational function is the sum of its (rational) even and odd parts, (rational)$\\cdot\\sin$ integrates to (rational)$\\cdot\\sin$ + (rational)$\\cdot\\cos$, or not at all.\nLet's backtrack, and apply this to $\\dfrac {\\sin(x)}x$ directly, using reals only.\nIf it has an elementary antiderivative, it must be of the form $E\\cdot S+O\\cdot C$.\nTaking derivatives gives $(E'-O)\\cdot S+(E+O')\\cdot C$.  As with partial fractions, we have a unique $R(x)[S,C]$ representation here (this is a bit tricky, as $S^2=1-C^2$: this step can be proven directly or via solving for $t, \\frac 1t$ coefficients over $C$).  So $E'-O=\\frac 1x$ and $E+O'=0$, or $O''+O=-\\frac 1x$.\nExpressing $O$ in partial fraction form, it is clear only $(-\\frac 1x)$ in $O$ can contribute a $-\\frac 1x$.  So there is a $-\\frac 2{x^3}$ term in $O''$, so there is a $\\frac 2{x^3}$ term in $O$ to cancel it, and so on, an infinite regress.  Hence, there is no such rational $O$.\n$\\quad\\boxed{\\displaystyle\\frac{\\arcsin(z)}z}$ $\\quad[z.\\tan(z)]$\nWe consider the case where $F=C(z,Z)(t)$ as a subfield of the meromorphic functions on some domain, where $z$ is the identify function, $Z=\\sqrt{1-z^2}$, and $t=\\arcsin z$.  Then $Z'=-\\frac zZ$, and $t'=\\frac 1Z$.  We ask in the main theorem result if this can happen with $a=\\frac tz$ and some field $G$.  $t$ is transcendental over $C(z,Z)$, since it has infinite branch points.\nSo we consider the more general situation of $f(z)\\cdot \\arcsin(z)$ where $f(z)$ is rational in $z$ and $\\sqrt{1-z^2}$.  By letting $z=\\frac {2w}{1+w^2}$, note that members of $C(z,Z)$ are always elementarily integrable.\nBecause $x^2+y^2-1$ is irreducible, $\\frac{C[x,y]}{x^2+y^2-1}$ is an integral domain, $C(z,Z)$ is isomorphic to its field of quotients in the obvious manner, and $C(z,Z)[t]$ is a UFD whose field of quotients is amenable to partial fraction analysis in the variable $t$.  What follows takes place at times in various $z$-algebraic extensions of $C(z,Z)$ (which may not have unique factorization), but the terms must combine to give something in $C(z,Z)(t)$, where partial fraction decompositions are unique, and hence the $t$ term will be as claimed.\nThus, if we can integrate $f(z)\\cdot\\arcsin(z)$, we have $f\\cdot t$ = $\\sum$ of $\\frac {u'}u s$ and $v'$, by the main theorem.\nThe $u$ terms can, by logarithmic differentiation in the appropriate algebraic extension field (recall that roots are analytic functions of\nthe coefficients, and $t$ is transcendental over $C(z,Z)$), be assumed to all be linear $t+r$, with $r$ algebraic over $z$.  Then $\\frac {u'}u=\\frac {1/Z+r'}{t+r}$.\nWhen we combine such terms back in $C(z,Z)$, they don't form a $t$ term (nor any higher power of $t$, nor a constant).\nPartial fraction decomposition of $v$ gives us a polynomial in $t$, with coefficients in $C(z,Z)$, plus multiples of powers of linear $t$ terms.\nThe latter don't contribute to a $t$ term, as above.\nIf the polynomial is linear or quadratic, say $v=g\\cdot t^2 + h\\cdot t + k$, then $v'=g'\\cdot t^2 + \\left(\\frac{2g}Z+h'\\right)\\cdot t + \\left(\\frac hZ+k'\\right)$.  Nothing can cancel the $g'$, so $g$ is just a constant $c$.  Then $\\frac {2c}Z+h'=f$ or $I(f\\cdot t)=2c\\cdot t+I(h'\\cdot t)$.  The $I(h'.t)$ can be integrated by parts.  So the antiderivative works out to $c\\cdot(\\arcsin(z))^2 + h(z)\\cdot \\arcsin(z) - I\\left(\\frac{h(z)}{\\sqrt{1-z^2}}\\right)$, and as observed above, the latter is elementary.\nIf the polynomial is cubic or higher, let $v=A.t^n+B.t^{n-1}+\\cdots$, then $v'=A'.t^n + \\left(n\\cdot\\frac AZ+B'\\right).t^{n-1} +\\cdots$ $A$ must be a constant $c$.  But then $\\frac{nc}Z+B'=0$, so $B=-nct$, contradicting $B$ being in $C(z,Z)$.\nIn particular, since $\\frac 1z + \\frac c{\\sqrt{1-z^2}}$ does not have a rational in \"$z$ and/or $\\sqrt{1-z^2}$\" antiderivative, $\\frac {\\arcsin(z)}z$ does not have an elementary integral.\n$\\quad\\boxed{\\displaystyle z^z}$\nIn this case, let $F=C(z,l)(t)$, the field of rational functions in $z,l,t$, where $l=\\log z$ and $t=\\exp(z\\,l)=z^z$.  Note that $z,l,t$ are algebraically independent.  (Choose some appropriate domain of definition.)  Then $t'=(1+l)t$, so for $a=t$ in the above situation, the partial fraction analysis (of the sort done in the previous posts) shows that the only possibility is for $v=wt+\\cdots$ to be the source of the $t$ term on the left, with $w$ in $C(z,l)$.\nSo this means, equating $t$ coefficients, $1=w'+(l+1)w$.  This is a first order ODE, whose solution is $w=\\frac{I(z^z)}{z^z}$.  So we must prove that no such $w$ exists in $C(z,l)$.  So suppose (as in one of Ray Steiner's posts) $w=P/Q$, with $P,Q$ in $C[z,l]$ and having no common factors.  Then $z^z= \\left(z^z\\cdot \\frac PQ\\right)'=z^z\\cdot\\frac{[(1+l)PQ+P'Q-PQ']}{Q^2}$, or $Q^2=(1+l)PQ+P'Q-PQ'$.\nSo $Q|Q'$, meaning $Q$ is a constant, which we may assume to be one.  So we have it down to $P'+P+lP=1$.\nLet $P=\\sum_{i=0}^n [P_i l^i]$, with $P_i, i=0\\cdots n \\in C[z]$.  But then in our equation, there's a dangling $P_n l^{n+1}$ term, a contradiction.\n\nOn a slight tangent, this theorem of Liouville will not tell you that Bessel functions are not elementary, since they are defined by second order ODEs.  This can be proven using differential Galois theory.  A variant of the above theorem of Liouville, with a different normal form, does show however that $J_0$ cannot be integrated in terms of elementary methods augmented with Bessel functions.\n\nWhat follows is a fairly complete sketch of the proof of the Main Theorem.\nFirst, I just state some easy (if you've had Galois Theory 101) lemmas.\nThroughout the lemmas $F$ is a differential field, and $t$ is transcendental over $F$.\n\nLemma $1$: If $K$ is an algebraic extension field of $F$, then there exists a unique way to extend the derivation map from $F$ to $K$ so as to make $K$ into\na differential field.\nLemma $2$: If $K=F(t)$ is a differential field with derivation extending $F$'s, and $t'$ is in $F$, then for any polynomial $f(t)$ in $F[t]$, $f(t)'$ is a\npolynomial in $F[t]$ of the same degree (if the leading coefficient is not in $\\mathrm{Con}(F)$) or of degree one less (if the leading coefficient is in $\\mathrm{Con}(F)$).\nLemma $3$: If $K=F(t)$ is a differential field with derivation extending $F$'s, and $\\frac{t'}t$ is in $F$, then for any $a$ in $F$, $n$ a positive integer, there exists $h$ in $F$ such that $(a\\cdot t^n)'=h\\cdot t^n$.  More generally, if $f(t)$ is any polynomial in $F[t]$, then $f(t)'$ is of the same degree as $f(t)$, and is a multiple of $f(t)$ iff $f(t)$ is a monomial.\n\nThese are all fairly elementary.  For example, $(a\\cdot t^n)'=\\bigl(a'+a\\frac {t'}t\\bigr)\\cdot t^n$ in lemma $3$.  The final 'iff' in lemma $3$ is where transcendence of $t$ comes in.  Lemma $1$ in the usual case of subfields of $M$ is an easy consequence of the implicit function theorem.\n\nMAIN THEOREM.  Let $F,G$ be differential fields, let $a$ be in $F$, let $y$ be in $G$, and suppose $y'=a$ and $G$ is an elementary differential extension field of $F$, and $\\mathrm{Con}(F)=\\mathrm{Con}(G)$.  Then there exist $c_1,...,c_n \\in \\mathrm{Con}(F), u_1,\\cdots,u_n, v\\in F$ such that\n$$(*)\\quad a  =  c_1\\frac{u_1'}{u_1}+ ... + c_n\\frac{u_n'}{u_n}+ v'$$\nIn other words, the only functions that have elementary antiderivatives are the ones that have this very specific form.\n\nProof:\nBy assumption there exists a finite chain of fields connecting $F$ to $G$ such that the extension from one field to the next is given by performing an algebraic, logarithmic, or exponential extension.  We show that if the form $(*)$ can be satisfied with values in $F2$, and $F2$ is one of the three kinds of allowable extensions of $F1$, then the form $(*)$ can be satisfied in $F1$.  The form $(*)$ is obviously satisfied in $G$: let all the $c$'s be $0$, the $u$'s be $1$, and let $v$ be the original $y$ for which $y'=a$.  Thus, if the form $(*)$ can be pulled down one field, we will be able to pull it down to $F$, and the theorem holds.\nSo we may assume without loss of generality that $G=F(t)$.\n\nCase $1$ : $t$ is algebraic over $F$.  Say $t$ is of degree $k$.  Then there are polynomials $U_i$ and $V$ such that $U_i(t)=u_i$ and $V(t)=v$.  So we have $$a =  c_1 \\frac{U_1(t)'}{U_1(t)} +\\cdots + c_n \\frac{ U_n(t)'}{U_n(t)} + V(t)'$$ Now, by the uniqueness of extensions of derivatives in the algebraic case, we may replace $t$ by any of its conjugates $t_1,\\cdots, t_k,$ and the same equation holds.  In other words, because $a$ is in $F$, it is fixed under the Galois automorphisms.  Summing up over the conjugates, and converting the $\\frac {U'}U$ terms into products using logarithmic differentiation, we have $$k a =  c_1 \\frac{[U_1(t_1)\\times\\cdots\\times U_1(t_k)]'}{U_1(t_1)\\times \\cdots \\times U_n(t_k)}+ \\cdots + [V(t_1)+\\cdots +V(t_k)]'$$ But the expressions in $[\\cdots]$ are symmetric polynomials in $t_i$, and as they are polynomials with coefficients in $F$, the resulting expressions are in $F$.  So dividing by $k$ gives us $(*)$ holding in $F$.\n\nCase $2$ : $t$ is logarithmic over $F$.  Because of logarithmic differentiation we may assume that the $u$'s are monic and irreducible in $t$ and distinct.\nFurthermore, we may assume v has been decomposed into partial fractions.\nThe fractions can only be of the form $\\dfrac f{g^j}$, where $\\deg(f)<\\deg(g)$ and $g$ is monic irreducible.  The fact that no terms outside of $F$ appear on the left hand side of $(*)$, namely just $a$ appears, means a lot of cancellation must be occuring.\n\n\nLet $t'=\\dfrac{s'}s$, for some $s$ in $F$.  If $f(t)$ is monic in $F[t]$, then $f(t)'$ is also in $F[t]$, of one less degree.  Thus $f(t)$ does not divide $f(t)'$.  In particular, all the $\\dfrac{u'}u$ terms are in lowest terms already.  In the $\\dfrac f{g^j}$ terms in $v$, we have $a g^{j+1}$ denominator contribution in $v'$ of the form $-jf\\dfrac{g'}{g^{j+1}}$.\nBut $g$ doesn't divide $fg'$, so no cancellation occurs.  But no $\\dfrac{u'}u$ term can cancel, as the $u$'s are irreducible, and no $\\dfrac{(**)}{g^{j+1}}$ term appears in $a$, because $a$ is a member of $F$.  Thus no $\\dfrac f{g^j}$ term occurs at all in $v$.\nBut then none of the $u$'s can be outside of $F$, since nothing can cancel them. (Remember the $u$'s are distinct, monic, and irreducible.)  Thus each of the $u$'s is in $F$ already, and $v$ is a polynomial.  But $v' = a -$ expression in $u$'s, so $v'$ is in $F$ also.  Thus $v = b t + c$ for some $b$ in $\\mathrm{con}(F)$, $c$ in $F$, by lemma 2.  Then $$a=  c_1 \\frac{u_1'}{u_1} +\\cdots + c_n\\frac{u_n'}{u_n} + b \\frac{s'}s + c'$$ is the desired form.  So case 2 holds.\n\nCase $3$ : $t$ is exponential over $F$.  So let $\\dfrac {t'}t=s'$ for some $s$ in $F$.  As in case 2 above, we may assume all the $u$'s are monic, irreducible, and distinct and put $v$ in partial fraction decomposition form.  Indeed the argument is identical as in case 2 until we try to conclude what form $v$ is.  Here lemma 3 tells us that $v$ is a finite sum of terms $b\\cdot t^j$ where each coefficient is in $F$.  Each of the $u$'s is also in $F$, with the possible exception that one of them may be $t$.  Thus every $\\dfrac {u'}u$ term is in $F$, so again we conclude $v'$ is in $F$.  By lemma 3, $v$ is in $F$.  So if every $u$ is in $F$, $a$ is in the desired form.  Otherwise, one of the $u$'s, say $u_n$, is actually $t$, then $$a =  c_1\\frac{u_1'}{u_1} + \\cdots + (c_n s + v)'$$ is the desired form.  So case 3 holds.\n\n\nReferences:\nA D Fitt & G T Q Hoare \"The closed-form integration of arbitrary functions\", Mathematical Gazette (1993), pp 227-236.\nI Kaplansky An introduction to differential algebra (Hermann, 1957)\nE R Kolchin Differential algebra and algebraic groups (Academic Press, 1973)\nA R Magid \"Lectures on differential Galois theory\" (AMS, 1994)\nE Marchisotto & G Zakeri \"An invitation to integration in finite terms\", COLLEGE MATHEMATICS JOURNAL (1994), pp 295-308.\nJ F Ritt Integration in finite terms (Columbia, 1948).\nJ F Ritt Differential algebra (AMS, 1950).\nM Rosenlicht \"Liouville's theorem on functions with elementary integrals\", PACIFIC JOURNAL OF MATHEMATICS (1968), pp 153-161.\nM Rosenlicht \"Integration in finite terms\", AMERICAN MATHEMATICS MONTHLY, (1972), pp 963-972.\nG N Watson A treatise on the theory of Bessel functions (Cambridge, 1962).\n-Matthew P Wiener", "meta": {"post_id": 265780, "input_score": 163, "output_score": 193, "post_title": "How to determine with certainty that a function has no elementary antiderivative?"}}
{"input": "$ f(x) = 6x^7\\sin^2(x^{1000}) e^{x^2} $\nFind $ f^{(2013)}(0) $\nA math forum friend suggest me to use big O symbol, however have no idea what that is, so how does that helping?", "output": "Note that,\n$$ 6\\,x^{7} \\sin\\left(x^{1000}\\right)\\sin\\left(x^{1000}\\right)e^{x^2} $$\n$$ =  6\\,x^{7} \\left( x^{1000}-\\frac{x^{3000}}{3!}+\\dots \\right)\\left( x^{1000}-\\frac{x^{3000}}{3!}+\\dots \\right)\\left(1+\\frac{x^2}{1!}+\\frac{x^4}{4!}+\\dots\\right) $$\n$$ = 6x^7x^{2000}\\left( 1-\\frac{x^{2000}}{3!} +\\dots\\right)^2\\left(1+\\frac{x^2}{1!}+\\frac{x^4}{2!}+\\dots\\right) $$\n$$ = 6x^{2007}\\left(1+\\frac{x^2}{1!}+\\frac{x^4}{2!}+\\frac{x^6}{3!}+\\dots\\right)\\left( 1-\\frac{x^{2000}}{3!} +\\dots\\right)^2  $$\nNow, it is clear that the coefficient of $x^{2013}$ is $1$, which implies that \n$$ \\frac{f^{(2013)}(0)}{(2013)!} = 1 \\implies f^{(2013)}(0)=(2013)!. $$", "meta": {"post_id": 267846, "input_score": 28, "output_score": 53, "post_title": "please solve a 2013 th derivative question?"}}
{"input": "I'm having difficult time in understanding the difference between the Borel measure and Lebesgue measure. Which are the exact differences? Can anyone explain this using an example?", "output": "Not every subset of a set of Borel measure $0$ is Borel measurable. Lebesgue measure is obtained by first enlarging the $\\sigma$-algebra of Borel sets to include all subsets of set of Borel measure $0$ (that of courses forces adding more sets, but the smallest $\\sigma$-algebra containing the Borel $\\sigma$-algebra and all mentioned subsets is quite easily described directly (exercise if you like)). \nNow, on that bigger $\\sigma$-algebra one can (exercise again) quite easily show that $\\mu$ (Borel measure) extends uniquely. This extension is Lebesgue measure. \nAll of this is a special case of what is called completing a measure, so that Lebesgue measure is the completion of Borel measure. The details are just as simple as for the special case.", "meta": {"post_id": 267991, "input_score": 65, "output_score": 61, "post_title": "Differences between the Borel measure and Lebesgue measure"}}
{"input": "The Gelfand\u2013Naimark Theorem states that an arbitrary C*-algebra $ A $ is isometrically *-isomorphic to a C*-algebra of bounded operators on a Hilbert space. There is another version, which states that if $ X $ and $ Y $ are compact Hausdorff spaces, then they are homeomorphic iff $ C(X) $ and $ C(Y) $ are isomorphic as rings. Are these two related anyway?", "output": "The first result that you stated is commonly known as the Gelfand-Naimark-Segal Theorem. It is true for arbitrary C*-algebras, and its proof employs a technique known as the GNS-construction. This technique basically allows one to construct a Hilbert space $ \\mathcal{H} $ from a given C*-algebra $ \\mathcal{A} $ such that $ \\mathcal{A} $ can be isometrically embedded into $ B(\\mathcal{H}) $ as a C*-subalgebra.\nThe Gelfand-Naimark Theorem, on the other hand, states that every commutative C*-algebra $ \\mathcal{A} $, whether unital or not, is isometrically *-isomorphic to $ {C_{0}}(X) $ for some locally compact Hausdorff space $ X $. When $ X $ is compact, $ {C_{0}}(X) $ and $ C(X) $ become identical.\nNote: The assumption of commutativity is essential for stating the Gelfand-Naimark Theorem. This is because we cannot realize a non-commutative C*-algebra as the commutative C*-algebra $ {C_{0}}(X) $, for some locally compact Hausdorff space $ X $.\nWhat follows is a statement of the Gelfand-Naimark Theorem, with the utmost level of precision.\n\nGelfand-Naimark Theorem Let $ \\mathcal{A} $ be a commutative C*-algebra. If $ \\mathcal{A} $ is unital, then $ \\mathcal{A} $ is isometrically *-isomorphic to $ C(X) $ for some compact Hausdorff space $ X $. If $ \\mathcal{A} $ is non-unital, then $ \\mathcal{A} $ is isometrically *-isomorphic to $ {C_{0}}(X) $ for some non-compact, locally compact Hausdorff space $ X $.\n\nThis result is often first established for the case when $ \\mathcal{A} $ is unital. One basically tries to show that the compact Hausdorff space $ X $ can be taken to be the set $ \\Sigma $ of all non-zero characters on $ \\mathcal{A} $, where $ \\Sigma $ is equipped with a special topology. Here, a character on $ \\mathcal{A} $ means a linear functional $ \\phi: \\mathcal{A} \\to \\mathbb{C} $ satisfying $ \\phi(xy) = \\phi(x) \\phi(y) $ for all $ x,y \\in \\mathcal{A} $. A rough outline of the proof is given below.\n\nShow that every character has sup-norm $ \\leq 1 $. Hence, $ \\Sigma \\subseteq {\\overline{\\mathbb{B}}}(\\mathcal{A}^{*}) $, where $ {\\overline{\\mathbb{B}}}(\\mathcal{A}^{*}) $ denotes the closed unit ball of $ \\mathcal{A}^{*} $.\nEquip $ {\\overline{\\mathbb{B}}}(\\mathcal{A}^{*}) $ with the subspace topology inherited from $ (\\mathcal{A}^{*},\\text{wk}^{*}) $, where $ \\text{wk}^{*} $ denotes the weak*-topology. By the Banach-Alaoglu Theorem, $ {\\overline{\\mathbb{B}}}(\\mathcal{A}^{*}) $ then becomes a compact Hausdorff space.\nProve that $ \\Sigma $ is a weak*-closed subset of $ \\left( {\\overline{\\mathbb{B}}}(\\mathcal{A}^{*}),\\text{wk}^{*} \\right) $. Hence, $ \\Sigma $ becomes a compact Hausdorff space with the subspace topology inherited from $ \\left( {\\overline{\\mathbb{B}}}(\\mathcal{A}^{*}),\\text{wk}^{*} \\right) $.\nFor each $ a \\in \\mathcal{A} $, define $ \\hat{a}: \\Sigma \\to \\mathbb{C} $ by $ \\hat{a}(\\phi) \\stackrel{\\text{def}}{=} \\phi(a) $ for all $ \\phi \\in \\Sigma $. We call $ \\hat{a} $ the Gelfand-transform of $ a $.\nShow that $ \\hat{a} $ is a continuous function from $ (\\Sigma,\\text{wk}^{*}) $ to $ \\mathbb{C} $ for each $ a \\in \\mathcal{A} $. In other words, $ \\hat{a} \\in C((\\Sigma,\\text{wk}^{*})) $ for each $ a \\in \\mathcal{A} $.\nFinally, prove that $ a \\longmapsto \\hat{a} $ is an isometric *-isomorphism from $ \\mathcal{A} $ to $ C((\\Sigma,\\text{wk}^{*})) $.\n\n\nLet us now take a look at the following theorem, which the OP has asked about.\n\nIf $ X $ and $ Y $ are compact Hausdorff spaces, then $ X $ and $ Y $ are homeomorphic if and only if $ C(X) $ and $ C(Y) $ are isomorphic as C*-algebras (not only as rings).\n\nOne actually does not require the Gelfand-Naimark Theorem to prove this result. Let us see a demonstration.\nProof\n\nThe forward direction is trivial. Take a homeomorphism $ h: X \\to Y $, and define $ h^{*}: C(Y) \\to C(X) $ by $ {h^{*}}(f) \\stackrel{\\text{def}}{=} f \\circ h $ for all $ f \\in C(Y) $. Then $ h^{*} $ is an isometric *-isomorphism.\nThe other direction is non-trivial. Let $ \\Sigma_{X} $ and $ \\Sigma_{Y} $ denote the set of non-zero characters of $ C(X) $ and $ C(Y) $ respectively. As $ C(X) $ and $ C(Y) $ are isomorphic C*-algebras, it follows that $ \\Sigma_{X} \\cong_{\\text{homeo}} \\Sigma_{Y} $. We must now prove that $ X \\cong_{\\text{homeo}} \\Sigma_{X} $. For each $ x \\in X $, let $ \\delta_{x} $ denote the Dirac functional that sends $ f \\in C(X) $ to $ f(x) $. Next, define a mapping $ \\Delta: X \\to \\Sigma_{X} $ by $ \\Delta(x) \\stackrel{\\text{def}}{=} \\delta_{x} $ for all $ x \\in X $. Then $ \\Delta $ is a homeomorphism from $ X $ to $ (\\Delta[X],\\text{wk}^{*}) $ (this follows from the fact that $ X $ is a completely regular space). We will be done if we can show that $ \\Delta[X] = \\Sigma_{X} $. Let $ \\phi \\in \\Sigma_{X} $. As $ \\phi: C(X) \\to \\mathbb{C} $ is surjective (as it maps the constant function $ 1_{X} $ to $ 1 $), we see that $ C(X)/\\ker(\\phi) \\cong \\mathbb{C} $. According to a basic result in commutative ring theory, $ \\ker(\\phi) $ must then be a maximal ideal of $ C(X) $. As such,\n$$\n\\ker(\\phi) = \\{ f \\in C(X) ~|~ f(x_{0}) = 0 \\}\n$$\nfor some $ x_{0} \\in X $ (in fact, all maximal ideals of $ C(X) $ have this form; the compactness of $ X $ is essential). By the Riesz Representation Theorem, we can find a regular complex Borel measure $ \\mu $ on $ X $ such that $ \\phi(f) = \\displaystyle \\int_{X} f ~ d{\\mu} $ for all $ f \\in C(X) $. As $ \\phi $ annihilates all functions that are vanishing at $ x_{0} $, Urysohn's Lemma implies that $ \\text{supp}(\\mu) = \\{ x_{0} \\} $. Hence, $ \\phi = \\delta_{x_{0}} $, which yields $ \\Sigma_{X} \\subseteq \\Delta[X] $. We thus obtain $ \\Sigma_{X} = \\Delta[X] $, so $ X \\cong_{\\text{homeo}} \\Sigma_{X} $. Similarly, $ Y \\cong_{\\text{homeo}} \\Sigma_{Y} $. Therefore, $ X \\cong_{\\text{homeo}} Y $ because\n$$\nX \\cong_{\\text{homeo}} \\Sigma_{X} \\cong_{\\text{homeo}} \\Sigma_{Y} \\cong_{\\text{homeo}} Y.\n$$\n\n\nWe actually have the following general categorical result.\n\nLet $ \\textbf{CompHaus} $ denote the category of compact Hausdorff spaces, where the morphisms are proper continuous mappings. Let $ \\textbf{C*-Alg} $ denote the category of commutative unital C*-algebras, where the morphisms are unit-preserving *-homomorphisms. Then there is a contravariant functor $ \\mathcal{F} $ from $ \\textbf{CompHaus} $ to $ \\textbf{C*-Alg} $ such that\n(1) $ \\mathcal{F}(X) = C(X) $ for all $ X \\in \\textbf{CompHaus} $, and\n(2) $ \\mathcal{F}(h) = h^{*} $ for all proper continuous mappings $ h $. If $ h: X \\to Y $, then $ h^{*}: C(Y) \\to C(X) $, which highlights the contravariant nature of $ \\mathcal{F} $.\nFurthermore, $ \\mathcal{F} $ is a duality (i.e., contravariant equivalence) of categories.\n\nThe role of the Gelfand-Naimark Theorem in this result is to prove that $ \\mathcal{F} $ is an essentially surjective functor, i.e., every commutative C*-algebra can be realized as $ \\mathcal{F}(X) = C(X) $ for some $ X \\in \\textbf{CompHaus} $.", "meta": {"post_id": 268002, "input_score": 25, "output_score": 43, "post_title": "Gelfand-Naimark Theorem"}}
{"input": "Help me please with this question:\nThe following system of equations is given:\n\\begin{align}\nx+2y+3z&=5\\\\ \n2x-y+2z&=1\\\\ \n3x+y-2z&=-1\n\\end{align}\nCheck if the Jacoby method or Gauss-Seidel method converges?\nIf the methods or one of the methods converges how many iterations we need to apply in order to get solution with accuracy of $0.001$.\nThanks a lot for you help!\nUpdate: I tried to find spectral radius $\\rho $ of iterative matrix in both methods, and get that $\\rho>1$.\nDoes it mean that both methods diverges?", "output": "With the spectral radius, you are on the right track.\nLet us write down what we have:\n$$ A = \\left( \\begin{array}{ccc}\n&1 & 2 & 3 \\\\\n&2 & -1 & 2 \\\\\n&3 & 1 & -2 \\end{array} \\right)$$ and (less importantly) $$b = \\left( \\begin{array}{c}\n5 \\\\\n1  \\\\\n-1  \\end{array} \\right).$$\nSo how do we formulate Gauss-Seidel? Note that there are different formulation, but I will do my analysis based on this link, page 1. Let $ A = L+D+U$ be its decomposition in lower, diagonal and upper matrix. Then Gauss-Seidel works as follows:\n\\begin{align}\n(D+L)x^{k+1}&= -Ux^k+b\n\\\\ \\Leftrightarrow x^{k+1} &= Gx^k+\\tilde{b}\n\\end{align}\nwith\n$$ G = -(D+L)^{-1} U.$$\nNote that you don't actually calculate it that way (never the inverse)! Let $x$ be the solution of the system $Ax=b$, then we have an error $e^k=x^k-x$ from which it follows (see reference above) that\n$$ e^{k+1} = Ge^k$$\nThus Gauss-Seidel converges ($e^k\\rightarrow 0$ when $k\\rightarrow \\infty$) iff  $\\rho(G)<1$. When you have calculated $\\rho(G)$ and it is greater than 1, Gauss-Seidel will not converge (Matlab also gives me $\\rho(G)>1$).\nWith the Jacobi method it is basically the same, except you have $A=D+(A-D)$ and your method is\n$$ Dx^{k+1} = -(A-D)x^k+b, $$\nfrom which we obtain\n$$\nx^{k+1} = Gx^k+\\tilde{b},\n$$\nwith\n$$ G = -D^{-1} (A-D).$$\nAs before, we have $e^{k+1} = Ge^k$.\nWe again have $\\rho(G)>1$. Therefore, both methods diverge in the given case.\nIn the following I have done a simple implementation of the code in Matlab.\nfunction iterative_method\n\n\nA=[  1     2     3\n     2    -1     2\n     3     1    -2];\n \nb=[  5     1    -1]';\n\nL=[  0     0     0 \n     2     0     0\n     3     1     0];\n\nU=[  0     2     3\n     0     0     2\n     0     0     0];\n \nD=[  1     0     0\n     0    -1     0\n     0     0    -2];\n \nx0 = rand(3,1); % random start vector\nx =  A\\b;       % \"exact\" solution\n\n% Gauss Seidel\nfigure(1)\nsemilogy(1,abs(x0-x),'xr')\nhold on\n\nxi =x0;\nfor i=2:100\n    xi = (D+L)\\((-U*xi)+b);\n    semilogy(i,abs(xi-x),'xr')\nend\n\n% Jacobi\nhold off\nfigure(2)\nsemilogy(1,abs(x0-x),'ob')\nhold on\n\nxi =x0;\nfor i=2:100\n    xi = D\\(-(A-D)*xi+b);\n    semilogy(i,abs(xi-x),'ob')\nend\n\n\n\nend\n\nThis shows, that both methods diverge as expected.\nAs we see from $ e^{k+1} = G e^k = G^k e^0$, we have exponential growth in our error.\nFor comparison, I added $y(\\text{iteration number})=\\rho(G)^\\text{iteration number}$ in black. We can see that this matches the calculated errors.\n\n\nBonus\nEven though this was no longer asked, I would like to say something about successive over-relaxation (SOR).\nThis method is a modification of the Gauss-Seidel method from above. But here we introduce a relaxation factor $\\omega>1$.\nAnd rewrite our method as follows:\n$$ (D+\\omega ) x^{k+1} = -(\\omega U + (\\omega-1)D)x^k+\\omega b$$\nNormally one wants to increase the convergence speed by choosing a value for $\\omega$. It basically means, that you stretch\nthe step you take in each iteration, assuming your going in the right direction. But in our case we can make use of something similar,\ncalled under-relaxation. Here we take small steps by choosing $\\omega<1$.\nI have done some calculations, playing with different values for $\\omega$. The plot below shows the\nerror of $x^{100}-x$ for different values of $\\omega$ on the x-axis, once for $0.01<\\omega<2$ and in the second plot\nfor $0.01<\\omega<0.5$. We can see, that for a value of $\\omega\\approx 0.38$ we get optimal convergence.\nEven though this might be a little more than you asked for, I still hope it might interest you to see, that\nsmall modifications in your algorithm can yield different results.", "meta": {"post_id": 270181, "input_score": 22, "output_score": 45, "post_title": "The Convergence of Jacobi and Gauss-Seidel methods"}}
{"input": "Multiplication of matrices \u2014 taking the dot product of the $i$th row of the first matrix and the $j$th column of the second to yield the $ij$th entry of the product \u2014 is not a very intuitive operation: if you were to ask someone how to mutliply two matrices, he probably would not think of that method. Of course, it turns out to be very useful: matrix multiplication is precisely the operation that represents composition of transformations. But it's not intuitive. So my question is where it came from. Who thought of multiplying matrices in that way, and why? (Was it perhaps multiplication of a matrix and a vector first? If so, who thought of multiplying them in that way, and why?) My question is intact no matter whether matrix multiplication was done this way only after it was used as representation of composition of transformations, or whether, on the contrary, matrix multiplication came first. (Again, I'm not asking about the utility of multiplying matrices as we do: this is clear to me. I'm asking a question about history.)", "output": "Here is an answer directly reflecting the historical perspective from the paper Memoir on the theory of matrices By Authur Cayley, 1857. This paper is available here.\nThis paper is credited with \"containing the first abstract definition of a matrix\" and \"a matrix algebra defining addition, multiplication, scalar multiplication and inverses\" (source).\nIn this paper a nonstandard notation is used. I will do my best to place it in a more \"modern\" (but still nonstandard) notation. The bulk of the contents of this post will come from pages 20-21.\nTo introduce notation, $$ (X,Y,Z)= \\left( \\begin{array}{ccc}\na & b & c \\\\\na' & b' & c' \\\\\na'' & b'' & c'' \\end{array} \\right)(x,y,z)$$\nwill represent the set of linear functions $(ax + by + cz, a'x + b'y + c'z, a''x + b''y + c''z)$ which are then called $(X,Y,Z)$.\nCayley defines addition and scalar multiplication and then moves to matrix multiplication or \"composition\". He specifically wants to deal with the issue of:\n$$(X,Y,Z)= \\left( \\begin{array}{ccc}\na & b & c \\\\\na' & b' & c' \\\\\na'' & b'' & c'' \\end{array} \\right)(x, y, z) \\quad \\text{where} \\quad (x, y, z)= \\left( \\begin{array}{ccc}\n\\alpha & \\beta & \\gamma \\\\\n\\alpha' & \\beta' & \\gamma' \\\\\n\\alpha'' & \\beta'' & \\gamma'' \\\\ \\end{array} \\right)(\\xi,\\eta,\\zeta)$$\nHe now wants to represent $(X,Y,Z)$ in terms of $(\\xi,\\eta,\\zeta)$. He does this by creating another matrix that satisfies the equation:\n$$(X,Y,Z)= \\left( \\begin{array}{ccc}\nA & B & C \\\\\nA' & B' & C' \\\\\nA'' & B'' & C'' \\\\ \\end{array} \\right)(\\xi,\\eta,\\zeta)$$\nHe continues to write that the value we obtain is:\n$$\\begin{align}\\left( \\begin{array}{ccc}\nA & B & C \\\\\nA' & B' & C' \\\\\nA'' & B'' & C'' \\\\ \\end{array} \\right) &= \\left( \\begin{array}{ccc}\na & b & c \\\\\na' & b' & c' \\\\\na'' & b'' & c'' \\end{array} \\right)\\left( \\begin{array}{ccc}\n\\alpha & \\beta & \\gamma \\\\\n\\alpha' & \\beta' & \\gamma' \\\\\n\\alpha'' & \\beta'' & \\gamma'' \\\\ \\end{array} \\right)\\\\[.25cm] &= \\left( \\begin{array}{ccc}\na\\alpha+b\\alpha' + c\\alpha'' & a\\beta+b\\beta' + c\\beta'' & a\\gamma+b\\gamma' + c\\gamma'' \\\\\na'\\alpha+b'\\alpha' + c'\\alpha'' & a'\\beta+b'\\beta' + c'\\beta'' & a'\\gamma+b'\\gamma' + c'\\gamma'' \\\\\na''\\alpha+b''\\alpha' + c''\\alpha'' & a''\\beta+b''\\beta' + c''\\beta'' & a''\\gamma+b''\\gamma' + c''\\gamma''\\end{array} \\right)\\end{align}$$\nThis is the standard definition of matrix multiplication. I must believe that matrix multiplication was defined to deal with this specific problem. The paper continues to mention several properties of matrix multiplication such as non-commutativity, composition with unity and zero and exponentiation.\nHere is the written rule of composition:\n\nAny line of the compound matrix is obtained by combining the corresponding line of the first component matrix successively with the several columns of the second matrix (p. 21)", "meta": {"post_id": 271927, "input_score": 122, "output_score": 42, "post_title": "Why, historically, do we multiply matrices as we do?"}}
{"input": "Possible Duplicate:\nWhy does $\\tan^{-1}(1)+\\tan^{-1}(2)+\\tan^{-1}(3)=\\pi$? \n\nHow to prove $$\\arctan(1)+\\arctan(2)+ \\arctan(3)=\\pi$$", "output": "Once I have seen a very nice proof of this: your claim is equivalent to proving that the sum of red, green and blue angles is $\\pi$. Note that in the second picture, the blue-green triangle is right and isosceles (that is 45-45-90 triangle and thus similar to small red-black triangle in the first diagram).\n\nCheers!", "meta": {"post_id": 272208, "input_score": 11, "output_score": 73, "post_title": "Proving :$\\arctan(1)+\\arctan(2)+ \\arctan(3)=\\pi$"}}
{"input": "I found in the Wolfram MathWorld page of the Axiom of the Empty Set that this is one of the Zermelo-Fraenkel Axioms, however on the page about these ZFC Axioms I read that it is an axiom that can be deduced from the Axiom of Subsets and the Axiom of Foundation (or Axiom of Regularity), so, the existence of the Empty Set is an axiom of ZFC or not?", "output": "In short, we do not need to adopt this as an axiom. But...\nIf there are sets at all, the axiom of subsets tells us that there is an empty set: If $x$ is a set, then $\\{y\\in x\\mid y\\ne y\\}$ is a set, and is empty, since there are no elements $y$ of $x$ for which $y\\ne y$. The axiom of extensionality then tells us that there is only one such empty set.\nSo, the issue is whether we can prove that there are any sets. The axiom of infinity tells us that there is a set (which is infinite, or inductive, or whatever formalization you use). But this seems like a terrible overkill to check that there are sets, to postulate that there are infinitely many.\nSome people prefer to have an axiom that states that there are sets. Of course, some people then just prefer to have an axiom that states that there is an empty set, so we at once have that there are sets, and avoid having to apply comprehension to check that the empty set exists. \nOther people adopt a formalization of first order logic in which we can prove that there are sets. More carefully, most formalizations of logic (certainly the one I prefer) prove as a theorem that the universe of discourse is nonempty. In the context of set theory, this means \"there are sets\". This is pure logic, before we get to the axioms of set theory. Under this approach, we do not need the axiom that states that there are sets, and the existence of the empty set can be established as explained above.\n(The logic proof that there are sets is not particularly illuminating or philosophically significant. Usually, one of the axioms of first order logic is that $\\forall x\\,(x=x)$. If $\\exists x\\,(x=x)$ --the formal statement corresponding to \"there are sets\"-- is false, then $\\forall x\\,(x\\ne x)$. Instantiating, we obtain $x\\ne x$, and instantiating the axiom  $\\forall x\\,(x=x)$ we obtain $x=x$, and one of these conclusions is the negation of the other, which is a contradiction. This is not particularly illuminating, because of course we choose our logical axioms and rules of instantiation so that this silly argument can go through, it is not a deep result, and probably we do not gain much insight from it.)\nIt turns out that yet some others prefer to allow the possibility that there are empty universes of discourse, so their formalization of first order logic is slightly different, and in this case, we have to adopt some axiom to conclude that there is at least one set.\nAt the end of the day, this is considered a minor matter, more an issue of personal taste than a mathematical question.", "meta": {"post_id": 278863, "input_score": 32, "output_score": 47, "post_title": "The existence of the empty set is an axiom of ZFC or not?"}}
{"input": "Possible Duplicate:\nHow do I convince someone that $1+1=2$ may not necessarily be true? \n\nI once read that some mathematicians provided a very length proof of $1+1=2$.\nCan you think of some way to extend mathematical rigor to present a long proof of that equation? I'm not asking for a proof, but rather for some outline what one would consider to make the derivation as long as possible.\nEDIT: It seems the proof I heard about is a standard reference given here multiple times :) I stated that the proof itself is less useful than an outline for me as I know only \"physics level maths\". Can someone provide a short outline what's going on in the proof? Some outline I can look up section by section in Wikipedia to at least get a feel of what could be needed to make such a proof?", "output": "You are thinking of the Principia Mathematica, written by Alfred North Whitehead and Bertrand Russell. Here is a relevant excerpt:\n\nAs you can see, it ends with \"From this proposition it will follow, when arithmetical addition has been defined, that 1+1=2.\" The theorem above, $\\ast54\\cdot43$, is already a couple of hundred pages into the book (Wikipedia says 370 or so)\nI wrote a blog article a few years ago that discusses this in some detail. You may want to skip the stuff at the beginning about the historical context of Principia Mathematica. But the main point of the article is to explain the theorem above.\nThe article explains the idiosyncratic and mostly obsolete notation that Principia Mathematica uses, and how the proof works.  The theorem here  is essentially that if $\\alpha$ and $\\beta$ are disjoint sets with exactly one element each, then their union has exactly two elements.  This is  established based on very slightly simpler theorems, for example that if $\\alpha$ is the set that contains $x$ and nothing else, and $\\beta$ is the set that contains $y$ and nothing else, then $\\alpha \\cup \\beta$ contains two elements if and only if $x\\ne y$.\nThe main reason that it takes so long to get to $1+1=2$ is that Principia Mathematica starts from almost nothing, and works its way up in very tiny, incremental steps. The work of G. Peano shows that it's not hard to produce a useful set of axioms that can prove 1+1=2 much more easily than Whitehead and Russell do.\nThe later theorem alluded to, that $1+1=2$, appears in section $\\ast110$:", "meta": {"post_id": 278974, "input_score": 34, "output_score": 60, "post_title": "Prove that 1+1=2"}}
{"input": "What is the difference between homotopy and homeomorphism? Let X and Y be two spaces, Supposed X and Y are homotopy equivalent and have the same dimension, can it be proved that they are homeomorphic? Otherwise, is there any counterexample? Moreover, what conditions should be added to homotopy to get homeomorphism?\nWe assume additionally both X and Y are orientable.", "output": "Let $X$ be the letter\n$$\\ \\ \\ \\ \\ \\mathsf{X}\\ \\ \\ \\ \\ $$\nand $Y$ be the letter\n$$\\ \\ \\ \\ \\ \\mathsf{Y}\\ \\ \\ \\ \\ $$\nThen $X$ and $Y$ are homotopy-equivalent, but they are not homeomorphic.\n\nSketch proof: let $f:X\\to Y$ map three of the prongs of the $\\mathsf{X}$ on to the $\\mathsf{Y}$ in the obvious way, and let it map the fourth prong to the point at the centre.  Let $g:Y\\to X$ map the $\\mathsf{Y}$ into those three prongs of the $\\mathsf{X}$.  Then $f$ and $g$ are both continuous, and $f$ is a surjection but is not injective, while $g$ is an injection but is not surjective.  Now the compositions $f\\circ g$ and $g\\circ f$ are both easily seen to be homotopic to the identities on $X$ and $Y$, so $X$ and $Y$ are homotopy-equivalent.  \nIn other words, observe that $\\mathsf Y$ is a deformation retract of $\\mathsf X$.  Alternatively, observe that $\\mathsf X$ and $\\mathsf Y$ both retract on to the point at the centre.\nOn the other hand, $X$ and $Y$ are not homeomorphic.  For example, removing the point at the centre of the $\\mathsf{X}$ yields a space with four connected components, while removing any point from the $\\mathsf{Y}$ yields at most three connected components.", "meta": {"post_id": 281339, "input_score": 61, "output_score": 85, "post_title": "What is the difference between homotopy and homeomorphism?"}}
{"input": "Let $G$ be a connected Lie group and  $U$ any neighbourhood of the identity element. How to prove that $U$ generates $G$.", "output": "By replacing $U$ with $U \\cap U^{-1}$ if necessary, assume that $U = U^{-1}$.\nConsider the set generated by $U$:\n$$S = \\{g_1 \\cdots g_n : g_1,\\cdots, g_n \\in U \\text{ for some $n$} \\}$$\nWe want to show that $S = G$ by showing that $S$ is nonempty, open, and closed. Connectedness of $G$ would then imply $S = G$. \nNon-emptiness is evident.\nFor openness, note that for any $g \\in S$, $gU \\subset S$.\nFor closedness, note that if $g \\notin S$, then $gU$ is disjoint with $S$. Otherwise if $gu \\in S$, we have $g = guu^{-1} \\in S$ as well.", "meta": {"post_id": 282442, "input_score": 23, "output_score": 35, "post_title": "a neighbourhood of identity $U$ generates $G$ where $G$ is a connected lie group"}}
{"input": "How to compute the series $\\displaystyle\\sum_{x=0}^\\infty\\sum_{y=0}^\\infty\\sum_{z=0}^\\infty\\frac{1}{2^x(2^{x+y}+2^{x+z}+2^{z+y})}$ ?", "output": "By symmetry, the sum $S$ of this triple series $$\nS=\\sum_{x,y,z}\\frac{1}{2^x\\cdot(2^{x+y}+2^{x+z}+2^{z+y})}$$ is also\n$$\nS=\\sum_{x,y,z}\\frac{1}{2^\\color{red}{y}\\cdot(2^{x+y}+2^{x+z}+2^{z+y})}=\\sum_{x,y,z}\\frac{1}{2^\\color{red}{z}\\cdot(2^{x+y}+2^{x+z}+2^{z+y})}.\n$$\nFurthermore,\n$$\n\\frac1{2^x}+\\frac1{2^y}+\\frac1{2^z}=\\frac{2^{x+y}+2^{x+z}+2^{z+y}}{2^{x+y+z}}.\n$$\nHence, summing these three equivalent formulas for $S$, one gets\n$$\n3S=\\sum_{x,y,z}\\frac1{2^{x+y+z}}=\\left(\\sum_{x}\\frac1{2^x}\\right)^3,\n$$\nand, finally,\n$$\nS=\\frac13\\cdot2^3=\\frac83.\n$$\nMore generally, for every absolutely convergent series $\\sum\\limits_x\\frac1{a_x}$,\n$$\n\\sum_{x,y,z}\\frac{1}{a_x\\cdot(a_xa_y+a_xa_z+a_za_y)}=\\frac13\\left(\\sum_x\\frac{1}{a_x}\\right)^3.\n$$", "meta": {"post_id": 286072, "input_score": 19, "output_score": 45, "post_title": "How to compute the series $\\sum\\limits_{x=0}^\\infty\\sum\\limits_{y=0}^\\infty\\sum\\limits_{z=0}^\\infty\\frac{1}{2^x(2^{x+y}+2^{x+z}+2^{z+y})}$"}}
{"input": "I'm trying to see the fundamental group of the Klein bottle minus a point without success. I know how to solve the torus minus a point giving a deformation retraction to the wedge sum of two circles.\nMy solution of the torus minus a point:\n\nI need help here.\nthanks a lot", "output": "We have two representations of the Klein bottle as a fundamental polygon. The first is:\n\nAnd the second is formed by cutting this polygon across the diagonal, flipping one piece and reattaching it to give essentially two real projective planes glued together:\n\nYou should be able to see that as CW complexes and a 2-cell attached according to the diagram, these both form the Klein bottle with non-orientable genus 2. \nRemoving a point from this 2-cell produces a space that deformation rectacts onto the 1-skeleton, which in both cases obviously forms the wedge sum of two circles and the fundamental group is $\\Bbb{Z} * \\Bbb{Z}$. \nLet's see if we can develop some sort of physical intuition for this. If the point (or by deformation, hole) we remove is in the right place, we can embed this in $\\Bbb{R}^3$ to get a physical intuition. \n\nWhich then forms \nAnd you can see rather easily that this deforms to:\n\nWhich obviously has the fundamental group of $\\Bbb{Z} * \\Bbb{Z}$, as this deformation retracts onto $S^1 \\vee S^1$.", "meta": {"post_id": 287101, "input_score": 26, "output_score": 40, "post_title": "fundamental group of the Klein bottle minus a point"}}
{"input": "Let $f$ be a convex function on a convex domain $\\Omega$ and $g$ a convex non-decreasing function on $\\mathbb{R}$. prove that the composition of $g(f)$ is convex on $\\Omega$. Under what conditions is $g(f)$ strictly convex.\nMy attempt, since $f$ is convex, $$f([1-t]x_0 +ty_0)\\le [1-t]f(x_0) + tf(y_0)\\:,\\quad\n t \\in [0,1] \n\\,\\text{and} \\: x_0,y_0\\in \\Omega$$\n Since $g$ is convex $$g([1-s]x_1 +sy_1) \\le [1-s]g(x_1) + sg(y_1)\\:,\\quad s \\in [0,1]\\:and \\: x_1,y_1 \\in \\mathbb{R}$$\n So $$g([1-s]f([1-t]x_2 +ty_2) +sf([1-t]x_2 +ty_2)) \\\\\\le [1-s]g([1-t]f(x_2) + tf(y_2)) + sg([1-t]f(x_3) + tf(y_3))\\: for\\:x_2,y_2,x_3,y_3 \\in \\Omega.$$ Im not sure if this is always true.\nAny help would be appreciated. \nThanks", "output": "We want to prove that for $x, y \\in \\Omega$, $(g \\circ f)\\left(\\lambda x + (1 - \\lambda) y\\right) \\le \\lambda (g \\circ f)(x) + (1 - \\lambda)(g \\circ f)(y)$.\nWe have:\n\\begin{align}\n(g \\circ f)\\left(\\lambda x + (1 - \\lambda) y\\right) &= g\\left(f\\left(\\lambda x + (1 - \\lambda) y\\right)\\right) \\\\\n&\\le g\\left(\\lambda f(x) + (1 - \\lambda) f(y)\\right) & \\text{(} f \\text{ convex and } g \\text{ nondecreasing)} \\\\\n&\\le \\lambda g(f(x)) + (1 - \\lambda)g(f(y)) & \\text{(} g \\text{ convex)} \\\\\n&= \\lambda (g \\circ f)(x) + (1 - \\lambda)(g \\circ f)(y)\n\\end{align}", "meta": {"post_id": 287716, "input_score": 40, "output_score": 54, "post_title": "The composition of two convex functions is convex"}}
{"input": "One typically studies analysis in $\\mathbb{R}^n$ after studying analysis in $\\mathbb{R}$.  Why can't the same be said of $\\mathbb{C}$?", "output": "I would say that the main reason that several complex variables is rarely seen in the undergraduate curriculum (and even not that often in the graduate curriculum unless the department has some specialists in SCV) is that you can't get very far without lots of prerequisites.\nYou can for example start by proving Cauchy's integral formula for a polydisc, and from that Liouville's theorem and a few other well known results from one complex variable quickly follow.\nFrom Cauchy's integral forumla, it also follows that holomorphic functions of several variables admit power series expansions (but the domain of convergence is not usually a ball in $\\mathbb{C}^n$: compare $\\sum_{j,k} z^j w^k$, $\\sum_{k} (z+w)^k$ and $\\sum_{k} z^k w^k$ for a few examples of what might happen). From here you can go on and study logarithmically convex Reinhardt domains. \nNote, however, that the mere definition of a holomorphic function in several variables is a little problematic. You want to say that a function is holomorphic if is is holomorphic in each variable separately, but to show that this is equivalent to other plausible definitions (without assuming that the function is for example locally bounded or jointly continuous) is surprisingly difficult.\nYou may even get as far as showing a version of Hartogs' extension theorem: If $\\Omega$ is a domain in $\\mathbb{C^n}$ and $K$ is a compact subset such that $\\Omega \\setminus K$ is connected, every holomorphic function on $\\Omega\\setminus K$ extends to $\\Omega$. (Here $n > 1$, of course.)\nI think this is about how far you can get without bringing in tools from PDE, potential theory, algebra (sheaf theory), functional analysis, differential geometry, distribution theory and probably a few more fields.\nThe big highlight in a first course in several complex variables is usually to solve the Levi problem, i.e. to characterize the domains of existence for holomorphic functions. (Hartogs' extension theorem shows that some domains are unnatural to study, since all holomorphic functions extend to a bigger domain.) This is usually done with H\u00f6rmanders $L^2$-solution of the $\\bar\\partial$-equation. (Or via sheaf theory a la Oka.) While it's not strictly necessary to have a modern PDE course as a prerequisite, it's certainly valuable. At the very least you need to know some functional analysis (and preferably some potential theory as well), including some exposure to unbounded linear operators on Hilbert spaces to be able to understand the H\u00f6rmander solution. (For the sheaf theory solution, you need a healthy background in algebra instead.)\nSimilarly, you need some differential geometry (at least familiarity with differential forms and tangent bundles) to understand the more complicated integral formulas such as Bochner-Martnielli's formula and the geometric aspects of pseduoconvexity, which is central for a deeper understanding of SCV. In fact, the interplay between the complex geometry of the domain and the corresponding function theory is a reoccuring theme in SCV. Function theory in strictly pseudoconvex domains, for example, look rather different from function theory in weakly pseudoconvex domains. (Many finer points concerning weakly pseudoconvex domains are still open problems.)\nSumming up, to do a real meaningful course in SCV, you really need more background than what is reasonable to expect from an undergraduate. After all SCV is really a 20th century field of mathematics! The Levi problem for example wasn't solved until the early 50's (H\u00f6rmander's solution is as late as 1965).", "meta": {"post_id": 289466, "input_score": 68, "output_score": 160, "post_title": "Why isn't several complex variables as fundamental as multivariable calculus?"}}
{"input": "Could someone give an example of a \u2018very\u2019 discontinuous derivative? I myself can only come up with examples where the derivative is discontinuous at only one point. I am assuming the function is real-valued and defined on a bounded interval.", "output": "Haskell's answer does a great job of outlining conditions that a derivative $f'$ must satisfy, which then limits us in our search for an example.  From there we see the key question: can we provide a concrete example of an everywhere differentiable function whose derivative is discontinuous on a dense, full-measure set of $\\mathbb R$?  Here's a closer look at the Volterra-type functions referred to in Haskell's answer, together with a little indication as to how it might be extended.\nBasic example\nThe basic example of a differentiable function with discontinuous derivative is\n$$\nf(x) = \\begin{cases} \n  x^2 \\sin(1/x) &\\mbox{if } x \\neq 0 \\\\\n0 & \\mbox{if } x=0. \n\\end{cases}\n$$\nThe differentiation rules show that this function is differentiable away from the origin and the difference quotient can be used to show that it is differentiable at the origin with value $f'(0)=0$.  A graph is illuminating as well as it shows how $\\pm x^2$ forms an envelope for the function forcing differentiablity.\n\nThe derivative of $f$ is \n$$\nf'(x) = \\begin{cases} \n  2 x \\sin \\left(\\frac{1}{x}\\right)-\\cos \\left(\\frac{1}{x}\\right)&\\mbox{if } x \\neq 0 \\\\\n0 & \\mbox{if } x=0,\n\\end{cases}\n$$\nwhich is discontinuous at $x=0$.  Its graph looks something like so\n\nTwo points\nThe next step is to modify this example to obtain a function that is everywhere differentiable with a derivative that is continuous on all of $\\mathbb R$, except for two points.  To this end, consider\n$$\nf(x) = \\begin{cases} \n  x^2 (1-x)^2 \\sin \\left(\\frac{1}{\\pi  x (1-x)}\\right)&\\mbox{if } 0<x<1 \\\\\n0 & \\mbox{else}. \n\\end{cases}\n$$\nThe graph of $f$ and its derivative look like so.\n\nA cantor set of discontinuties\nNow that we have a way to construct a differentiable function whose derivative is discontinuous exactly at the endpoints of an interval, it should be clear how to construct a differentiable function whose derivative is discontinous on a Cantor set constructed in the interval.  For $n\\in\\mathbb N$ and $m=1,2,\\ldots,2^n$, let $I_{m,n}$ denote one of the $2^n$ intervals removed during the $n^{th}$ stage of construction of the Cantor set.  Then let $f_{m,n}$ be scaled to have support $I_{m,n}$ and to have maximum value $4^{-n}$.  The function\n$$F(x) = \\sum_{n=0}^{\\infty} \\sum_{m=1}^{2^n} f_{m,n}(x)$$\nwill be everywhere differentiable but its derivative will be discontinuous on the given Cantor set.  Assuming we do this with Cantors standard ternary set, we get a picture that looks something like so:\n\nOf course, there's really a sequence of functions here and care needs to be taken to show that the limit is truly differentiable.  Let\n$$F_N(x) = \\sum_{n=1}^{N} \\sum_{m=1}^{2^n} f_{m,n}(x).$$\nThe standard theorem then states that, as long as $F_N$ converges and $F_N'$ converges uniformly, then the limit of $F_N(x)$ will be differentiable.  This is guaranteed by the choice of $4^{-n}$ as the max for $f_{m,n}$.\nIncreasing the measure\nAgain, the last example refers to the standard Cantor ternary set but there's no reason this can't be done with any Cantor set.  In particular, it can be done with a so-called fat Cantor set, which can have positive measure arbitrarily close to the measure of the interval containing it.  We immediately produce an everywhere differentiable function whose derivative is discontinuous on a nowhere dense set of positive measure.  (Of course, care must again be taken to scale the heights of the functions go to zero quickly enough to guarantee differentiability.)\nFinally, we can fill the holes of the removed intervals with more Cantor sets (and their corresponding functions) in such a way that the union of all of them is of full measure.  This allows us to construct an everywhere differentiable function with derivative that is discontinuous on the union of those Cantor sets, which is a set of full measure.", "meta": {"post_id": 292275, "input_score": 184, "output_score": 251, "post_title": "Discontinuous derivative."}}
{"input": "What is the Topology of point-wise convergence? It has been stated in lectures but I am unfamiliar with it.", "output": "Let $F$ be a family of functions from a set $X$ to a space $Y$. $F$ might, for instance, be the set of all functions from $X$ to $Y$, or it might be the set of all continuous functions from $X$ to $Y$, if $X$ is a topological space. Each $f:X\\to Y$ can be thought of as a point in the Cartesian product $Y^{|X|}$. To see this, for each $x\\in X$ let $Y_x$ be a copy of the space $Y$. Then a function $f:X\\to Y$ corresponds to the point in $\\prod_{x\\in X}Y_x$ whose $x$-th coordinate is $f(x)$, and of course $\\prod_{x\\in X}Y_x$ is just the product of $|X|$ copies of $Y$, i.e., $Y^{|X|}$.\nThe product $Y^{|X|}$ is a topological space with the product topology; $F\\subseteq Y^{|X|}$, so $F$ inherits a topology from the product topology on $Y^{|X|}$. This inherited topology is the topology of pointwise convergence on $F$.\nIt can easily be shown that a sequence $\\langle f_n:n\\in\\Bbb N\\rangle$ in $F$ converges to some $f\\in F$ in this topology if and only if for each $x\\in X$, $\\langle f_n(x):n\\in\\Bbb N\\rangle$ converges to $f(x)$ in $Y$. (More generally, a net $\\langle f_d:d\\in D\\rangle$ in $F$ converges to some $f\\in F$ if and only if for each $x\\in X$ the net $\\langle f_d(x):x\\in D\\rangle$ converges to $f(x)$ in $Y$.) This is the reason for the pointwise in the name.\nVery often $Y$ is $\\Bbb R^n$ or $\\Bbb C^n$ for some $n$, and $X$ is some topological space. The topological structure of $X$ has no bearing on the topology of pointwise convergence, though it may help to determine the set $F$ of functions under consideration (e.g., the continuous ones).", "meta": {"post_id": 293004, "input_score": 48, "output_score": 85, "post_title": "What is the Topology of point-wise convergence?"}}
{"input": "Suppose $\\mathcal{F}$ and $\\mathcal{G}$ are sheaves on $X$. The sheaf hom from $\\mathcal{F}$ to $\\mathcal{G}$ is defined by $U \\mapsto $ Hom($\\mathcal{F}|_{U}$,$\\mathcal{G}|_{U}$), where the Hom is taken in the category of presheaves, i.e., Hom($\\mathcal{F}|_{U}$,$\\mathcal{G}|_{U}$) is the set of all natural transformations from $\\mathcal{F}|_{U}$ to $\\mathcal{G}|_{U}$.\nTo verify the sheaf hom is a sheaf , I have to show that it is a presheaf. So I need to define a restriction map from Hom($\\mathcal{F}|_{U}$,$\\mathcal{G}|_{U}$) to Hom($\\mathcal{F}|_V$,$\\mathcal{G}|_V$) if $V$ is an open subset of $U$. There seems a natural restriction map by using the restriction maps for $\\mathcal{F}$ and $\\mathcal{G}$. But how can you describe it explicitly?", "output": "Step $-1$ (unnecessary). Show that the set of sections Hom$(\\mathscr F|_U,\\mathscr G|_U)$ over an open subset $U\\subset X$ is an abelian group (so that $\\mathcal Hom(\\mathscr F,\\mathscr G)$ will be a sheaf of abelian groups). This is easy. \nStep $0$. $U\\mapsto \\textrm{Hom}(\\mathscr F|_U,\\mathscr G|_U)$ is a presheaf (note: as pointed out in a comment, a section of this presheaf is a morphism of sheaves!). The restriction is defined as follows: for fixed $U$, and an open subset $V\\subset U$, a section $\\sigma\\in\\textrm{Hom}(\\mathscr F|_U,\\mathscr G|_U)$ goes to $\\sigma|_V\\in \\textrm{Hom}(\\mathscr F|_V,\\mathscr G|_V)$, where $\\sigma|_V$ is the morphism of sheaves on $V$ defined by $\\sigma|_V(W)=\\sigma(W):\\mathscr F(W)\\to\\mathscr G(W)$ for any open subset $W\\subset V$ (which is also open in $U$! for this reason, the squares that must commute, over $V$, do commute because they already commuted over $U$).\nStep $1$. The first sheaf axiom. Let $U=\\bigcup_{i\\in I} U_i$ be an open covering of an open subset $U\\subset X$. Let $\\sigma: \\mathscr F|_U\\to\\mathscr G|_U$ be a section such that $\\sigma_i:=\\sigma|_{U_i}=0$ for all $i\\in I$. We want to show that $\\sigma=0$.\nLet $g\\in\\mathscr F(U)$ be a fixed section. Then look at the (zero!) morphisms of abelian groups\n$$\n\\sigma_i(U_i):\\mathscr F(U_i)\\to\\mathscr G(U_i) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,g_i\\mapsto 0.\n$$\nNow, because $\\mathscr{G}$ is a sheaf and the image of the section $g$ under $\\sigma(U)$ restricts to zero on every open set in an open cover $\\{U_i \\}$ (recall the usual commutative diagram for morphisms of (pre)sheaves/natural transformations), it has the property \n$$\n\\sigma(U)(g)=0.\n$$\nBecause this holds for every $g\\in\\mathscr F(U)$, we conclude that $\\sigma(U)=0$, hence $\\sigma=0$, as claimed.\nStep 2. The second sheaf axiom.\nLet again $U=\\bigcup_{i\\in I} U_i$ be an open covering of an open subset $U\\subset X$, and let $\\{\\phi_i:\\mathscr F|_{U_i}\\to\\mathscr G|_{U_i}\\}_{i\\in I}$ be a family of sections such that $\\phi_i=\\phi_j$ on $U_{ij}$. We want a global $\\phi$ (section over $U$) such that $\\phi|_{U_i}=\\phi_i$. \nIf $V\\subset U$, then $A_i:=U_i\\cap V$ cover $V$. So let us fix a section $g\\in \\mathscr F(V)$ and let us set $g_i:=g|_{A_i}$. We can give a name (say $t_i$) to the image of $g_i$ under $\\phi(A_i)$, namely\n$$\n\\phi_i(A_i):\\mathscr F(A_i)\\to\\mathscr G(A_i) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,g_i\\mapsto t_i.\n$$\nThe compatibility of the $\\phi_i$'s implies that of the $t_i$'s, and since $\\mathscr G$ is a sheaf there exists a global section $t\\in \\mathscr G(V)$ such that $t|_{A_i}=t_i$ for every $i$. We can define the $\\phi$ that we are looking for by \n$$\n\\phi(V):\\mathscr F(V)\\to\\mathscr G(V) \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, g\\mapsto t.\n$$\nfor every $V\\subset U$. In this way, by construction, $\\phi|_{U_i}=\\phi_i$, as wanted.", "meta": {"post_id": 294802, "input_score": 32, "output_score": 45, "post_title": "Prove that sheaf hom is a sheaf."}}
{"input": "What is the difference between homotopy and isotopy at the intuitive level.Some diagrammatic explanation will be helpful for me.", "output": "Isotopies are much stricter! \nA homotopy is a continuous one-parameter family of continuous functions.\nAn isotopy is a continuous one-parameter family of homeomorphisms. \nYou can think of a homotopy between two spaces as a deformation that involves bending, shrinking and stretching, but doesn't have to be one-to-one or onto. For example, a punctured torus is homotopy equivalent to a wedge of two circles (a \"figure 8\"), which can be pictured by sticking your fingers into the puncture and stretching the torus back onto the meridian and longitude lines. \nBut this map is certainly not a homeomorphism -- even the dimension is wrong, not to mention that a wedge of two circles is not a manifold. \nAn isotopy is a deformation that involves only bending. It must be one-to-one and onto at every step. In this way, any two handlebodies of equal genus are isotopic.", "meta": {"post_id": 296170, "input_score": 23, "output_score": 41, "post_title": "Isotopy and Homotopy"}}
{"input": "Ok we have 2 subgroups of G defined as $H_1$ and $H_2$ the question wants us to prove $H_1$ intersect $H_2$ must also be a subgroup of G.\nThis seems fairly intuitive, making as math usually hard to prove :)\nwe know that for any element a in $H_1$ there exists $a^{-1}$ and that $H_1$ is closed. the same holds for $H_2$ so the intersection will only contain and element c in $H_1$ Intersect $H_2$ if c and $c^{-1}$ are in $H_1$ and $H_2$ additionally we know that $H_1$ and $H_2$ must contain $e$ the identity of G thus $H_1$ intersect $H_2$ cannot be empty. my problem lies with writing this out and proving closure on this intersection.", "output": "To avoid subscripts, let $H_1 = P,\\; H_2 = Q$. Closure is addressed in Hint 2. \n\n$P$ and $Q$ are subgroups of a group $G$. Prove that $P \\cap Q$ is a subgroup. \n\nStep 1: You know that $P$ and $Q$ are subgroups of $G$. That means they each contain the identity element, say $e$ of $G$. So what can you conclude about $P\\cap Q$? If $e \\in P$ and $e \\in Q$? (Just unpack that means for their intersection.) In showing $e \\in P\\cap Q$, you also show, $P\\cap Q$ is not empty.\nStep 2: You know that $P, Q$ are subgroups of $G$. So they are both closed under the group operation of $G$. If $a, b \\in P\\cap Q$, then $a, b \\in P$ and $a, b \\in Q$. So $ab \\in P$ and $ab \\in Q$. So what can you conclude about $ab$ with respect to $P\\cap Q$? This is about proving closure under the group operation of $G$. \nStep 3: You can use similar arguments to show that for any element $c \\in P\\cap Q$, $c^{-1} \\in P\\cap Q$. If $c \\in P\\cap Q$, then $c \\in P$ and $c\\in Q$. Since $P$ and $Q$ are subgroups, each containing $c$, it follows that $c^{-1} \\in P$ AND $c^{-1} \\in Q$. Hence $c^{-1} \\in P\\cap Q$. That establishes that $P\\cap Q$ is closed under inverses.\nOnce you've completed each step above, what can you conclude about $P\\cap Q$ in $G$?", "meta": {"post_id": 297356, "input_score": 26, "output_score": 55, "post_title": "Intersection of 2 subgroups must be a subgroup proof."}}
{"input": "What is immersion and submersion at the intuitive level. What can be visually done in each case?", "output": "First of all, note that if $f : M \\to N$ is a submersion, then $\\dim M \\geq \\dim N$, and if $f$ is an immersion, $\\dim M \\leq \\dim N$.\nThe Rank Theorem may provide some insight into these concepts. The following statement of the theorem is taken from Lee's Introduction to Smooth Manifolds (second edition); see Theorem $4.12$.\n\nSuppose $M$ and $N$ are smooth manifolds of dimensions $m$ and $n$, respectively, and $F : M \\to N$ is a smooth map with constant rank $r$. For each $p \\in M$ there exist smooth charts $(U, \\varphi)$ for $M$ centered at $p$ and $(V, \\psi)$ for $N$ centered at $F(p)$ such that $F(U) \\subseteq V$, in which $F$ has a coordinate representation of the form $$\\hat{F}(x^1, \\dots, x^r, x^{r+1}, \\dots, x^m) = (x^1, \\dots, x^r, 0, \\dots, 0).$$ In particular, if $F$ is a smooth submersion, this becomes $$\\hat{F}(x^1, \\dots, x^n, x^{n+1}, \\dots, x^m) = (x^1, \\dots, x^n),$$ and if $F$ is a smooth immersion, it is $$\\hat{F}(x^1, \\dots, x^m) = (x^1, \\dots, x^m, 0, \\dots, 0).$$\n\nSo a submersion locally looks like a projection $\\mathbb{R}^n\\times\\mathbb{R}^{m-n} \\to \\mathbb{R}^n$, while an immersion locally looks like an inclusion $\\mathbb{R}^m \\to \\mathbb{R}^m\\times\\mathbb{R}^{n-m}$.", "meta": {"post_id": 297988, "input_score": 51, "output_score": 40, "post_title": "Intuitive meaning of immersion and submersion"}}
{"input": "I have a really simple differential equation: $\\frac{dx}{dt} = x^2.t$ with initial value $x(0) = x_0$. Determine the\nmaximal interval where it exists, depending on $x_0$\nThe maximal interval needs to be written in the form $(t^-,t^+)$, but I don't understand how you determine $t^+$ and $t^-$? Please could someone explain if there is a method to do this?\nThanks", "output": "An initial value problem $x' = f(x)$ with $x(0) = x_0$, has a unique solution defined on some interval $(-a, a)$.\nThis IVP has a unique solution $x(t)$ defined on a maximal interval of existence $(\\alpha,~\\beta)$.\nFurthermore, if $\\beta \\lt \\infty$ and if the limit \n$$x_1 = \\lim_{t\\rightarrow \\beta^{-}} x(t)$$\nexists then $x_1 \\in \\dot E$, the boundary of $E$. The boundary of the open set $E$, $\\dot E = \\overline E$ ~ $E$ where $\\overline E$ denotes the closure of $E$.\nOn the other hand, if the above limit exists and $x_1 \\in E$, then $\\beta = \\infty$, $f(x_1) = 0$ and $x_1$ is an equilibrium point of the IVP.\nThe domain of a particular solution to a differential equation is the largest open interval containing the initial value on which the solution satisfies the differential equation.\nTheorem (Maximal Interval of Existence). An IVP has a maximal interval of existence, and it is of the form $(t^{-}, t^{+})$, with $t^{-} \\in [-\\infty, \\infty)$ and $t^{+} \\in (-\\infty, \\infty]$. There is a unique solution $x(t)$ on $(t^{-}, t^{+})$, and $(t, x(t))$ leaves every compact subset $\\mathcal K$ of $\\mathcal D$ as $t \\downarrow t^{-}$ and as $t \\uparrow t^{+}$.\nProof See ODE Notes.\nMore Examples of Domains See the very readable section More Examples of Domains.\nExample\n$$x' = x^2, ~x(0) = 1$$\nhas the solution \n$$x(t) = \\frac{1}{1-t}$$\ndefined on its maximal interval of existence $(\\alpha, ~ \\beta) = (-\\infty, 1)$.\nFurthermore, $x_1 = \\displaystyle \\lim_{t\\rightarrow 1^{-}} x(t) = \\infty$. You can do the other side.\nOriginal Problem\nFor your problem, we have:\n$$\\tag 1 x' = x^{2}t, ~ x(0) = x_0$$\nSolving $(1)$, yields: $x(t) = \\large -\\frac{2}{c + t^{2}}$\nUsing the IC, $x(0) = x_0$, yields,\n$$\\tag 2 \\large x(t) = -\\frac{2}{t^{2} - \\frac{2}{x_0}}$$\nDepending on $c = \\large \\frac{2}{x_0}$, where $c \\in \\mathbb{R}$, there are several cases:\n\nif $c \\lt 0$, then $x(t) = \\large x(t) = -\\frac{2}{t^{2} - \\frac{2}{x_0}}$ is a global solution on $\\mathbb{R}$,\nif $c \\gt 0$, the solutions are defined on $(-\\infty, -\\sqrt{c})$, $(-\\sqrt{c}, \\sqrt{c})$, $(\\sqrt{c}, \\infty)$. The solutions are maximal solutions on $\\mathbb{R}$, but are not global solutions.\nif $c = 0$, then the maximal non global solutions on $\\mathbb{R}$ are defined on $(-\\infty, 0)$ and $(0, \\infty)$.\n\nNote for completeness, that there is another solution $x(t) = 0$, which is a global solution on $\\mathbb{R}$.\nI would strongly suggest:\n$(1)$ That you review the solution and the different results for varying \"c\" and plot those to make sure you understand them.\n$(2)$ That you use the initial definition I gave above, which works for $t^{-}$ and $t^{+}$ and make sure you can do it the way I showed and using that argument as per the theorem.\nRegards", "meta": {"post_id": 299005, "input_score": 1, "output_score": 38, "post_title": "How do you find the Maximal interval of existence of a differential equation?"}}
{"input": "My friend and I were discussing this and we couldn't figure out how to prove it one way or another.\nThe only rational values I can figure out for $\\sin(x)$ (or $\\cos(x)$, etc...) come about when $x$ is some product of a fraction of $\\pi$.\nIs $\\sin(x) $ (or other trigonometric function) necessarily irrational if $ x $ is rational?\nEdit:\nExcluding the trivial solution of 0.", "output": "If $\\sin x$ is rational (or even just algebraic), then $\\cos x=\\pm \\sqrt{1-\\sin^2 x}$ is algebraic. Therefore $e^{ix}=\\cos x+i\\sin x$ is algebraic, so by the Lindemann-Weierstrass theorem, $x$ cannot have been nonzero algebraic -- in particular not nonzero rational.", "meta": {"post_id": 299124, "input_score": 40, "output_score": 41, "post_title": "Is sin(x) necessarily irrational where x is rational?"}}
{"input": "Torsion is used to refer to elements of finite order under some binary operation. It doesn't seem to bear any relation to the ordinary everyday use of the word or with its use in differential geometry (which relates back to the ordinary use of the word). So how did it acquire this usage in algebra?\nI'm interested to understand the intuition behind why the word \"torsion\" was chosen for this notion, as well as when it was first used.", "output": "John Stillwell wrote that \"the word 'torsion'entered the theory of \nabelian groups as a result of the derivation of the one-dimensional torsion \ncoefficients by abelianization of the fundamental group in Tietze 1908\" [Classical Topology and \nCombinatorial Group Theory, 1993, Sec. 5.1.1, p. 170]. Below is an excerpt providing further context.\n\nThe appropriate notions of \"sum\" and \"boundary,\"and the correct \n  choice of k-dimensional manifolds admissible as basis elements, were found \n  only after considerable trial and error. \"Appropriate\" initially meant \n  satisfying the relation $B_k = B_{m-k}$ since this was the relation Poincare \n  tried to prove in his 1895 paper. Heegaard 1898 showed this work to be in \n  error by constructing a counterexample. Poincare then changed the definition \n  and proved the theorem again in Poincare 1899, inventing the tool of \n  simplicial decomposition for the purpose. He also made a thorough analysis \n  of his error, uncovering the important concept of torsion in Poincare 1900, and \n  exposing the breakdown of his earlier proof as failure to observe torsion. \nTorsion is present when an element a does not form a boundary taken \n  once, but does when taken more than once. An example is the curve $a$ in \n  the projective plane $P$ which generates $\\pi_1(P).$ Then $a^2$ is the boundary of \n  a disc, though a itself does not separate $P.$ Poincare justified the term \n  \"torsion\" by showing that $(m-1)$-dimensional torsion is present only in \n  an $m$-manifold which is nonorientable, and hence twisted onto itself in \n  some sense. \nIn his first topology paper, Poincare 1892 showed that the Betti numbers \n  alone did not determine a manifold up to homeomorphism. By 1900 he \n  was hoping that torsion numbers would supply the missing information, \n  and his paper of that year contains a decomposition of the homology in- \n  formation in each dimension $k$ into the Betti number $B_k$ and a finite set of \n  numbers called $k$-dimensional torsion coefficients. Since Noether 1926 it \n  has been customary to encode this information in an abelian group $H_k$\n  called the $k$-dimensional homology group, and Poincare's construction can \n  in fact be seen as the decomposition of a finitely generated abelian group \n  into cyclic factors (see the structure theorem 5.2). The word \"torsion,\" \n  which appears so inexplicably in most algebra texts, entered the theory of \n  abelian groups as a result of the derivation of the one-dimensional torsion \n  coefficients by abelianization of the fundamental group in Tietze 1908 (see \n  5.1.3. and 5.3).", "meta": {"post_id": 300586, "input_score": 68, "output_score": 39, "post_title": "Where does the word \"torsion\" in algebra come from?"}}
{"input": "Are $(\\mathbb{R},+)$ and $(\\mathbb{C},+)$ isomorphic as additive groups?\nI know that there is a bijection between $\\mathbb{R}$  and $\\mathbb{C}$, and this question asks whether they are isomorphic as abelian groups, are they referring to the additive abelian group? If so is there any simple isomorphism I can find? I know nothing about Hamel basis. Thanks.", "output": "Assuming the axiom of choice, yes.\nObserve that both these abelian groups are actually $\\mathbb Q$-vector spaces, and they have the same dimension, so they must be isomorphic as vector spaces, and such isomorphism is also a group isomorphism. This is in fact a stronger requirement than just group isomorphism, but nevermind that.\nIt is consistent with the failure of the axiom of choice that these two are not isomorphic, though. So one cannot give an explicit isomorphism between them.", "meta": {"post_id": 302514, "input_score": 41, "output_score": 38, "post_title": "Are $(\\mathbb{R},+)$ and $(\\mathbb{C},+)$ isomorphic as additive groups?"}}
{"input": "linear least-squares are convex optimization. \nAre nonlinear least squares also convex optimization? Can someone please give some simple examples?", "output": "It depends. Once you are in the nonlinear world, things can be convex or nonconvex. You can write a generic nonlinear least-squares problem as\n$$\n\\min_{x \\in \\mathbb{R}^n} \\ \\tfrac{1}{2} \\|F(x)\\|^2,\n\\qquad\n\\text{where}\n\\quad\nF(x) := (f_1(x), \\ldots, f_m(x)),\n$$\nand each $f_i : \\mathbb{R}^n \\to \\mathbb{R}$. Let's assume that they all have continuous first and second derivatives. Now the gradient of\n$$\nf(x) := \\tfrac{1}{2} \\|F(x)\\|^2 = \\tfrac{1}{2} \\sum_{j=1}^m f_j(x)^2\n$$\nis\n$$\n\\nabla f(x) = \\sum_{j=1}^m f_j(x) \\nabla f_j(x) = J(x)^T F(x),\n$$\nwhere $J(x)$ is the Jacobian of $F$, i.e., the $m$-by-$n$ matrix whose $j$-th row is $\\nabla f_j(x)^T$:\n$$\nJ(x) =\n\\begin{bmatrix}\n \\nabla f_1(x)^T \\\\\n \\vdots \\\\\n \\nabla f_m(x)^T\n\\end{bmatrix}.\n$$\nNow let's compute the second derivatives of $f$ (its Hessian). It's easiest to use the expression of $\\nabla f(x)$ as a sum (above) and differentiate that:\n$$\n\\nabla^2 f(x) =\n\\sum_{j=1}^m f_j(x) \\nabla^2 f_j(x) + \\sum_{j=1}^m \\nabla f_j(x) \\nabla f_j(x)^T =\n\\sum_{j=1}^m f_j(x) \\nabla^2 f_j(x) + J(x)^T J(x).\n$$\nThe last term, $J(x)^T J(x)$ is always a positive semi-definite matrix. If the problem were a linear least-squares problem, all the individual Hessians $\\nabla^2 f_j(x) = 0$ and $\\nabla^2 f(x)$ would itself be positive semi-definite. In this case, $f$ is convex.\nBut if each $f_j$ is nonlinear, it could very well be that some or all the terms $f_j(x) \\nabla^2 f_j(x)$ contribute against convexity.\nSuppose for example that $m=1$ (i.e., there is only one term in all the sums) and that $f_1(x) = \\sin(x)$. Then $f_1'(x) = \\cos(x)$ and $f_1''(x) = -\\sin(x)$. In this case, $f''(x) = -\\sin^2(x) + \\cos^2(x)$, which is not always positive (e.g., at $x=\\pi/2$).\nBut on the other hand, suppose $m=1$ and $f_1(x) = -x^2$. Then $f''(x) = 6 x^2 \\geq 0$. This one is convex.\nFrom the expression of the Hessian above, you can see that if either\n\neach $f_j$ is nonnegative and convex, or\neach $f_j$ is nonpositive and concave,\n\nthen $f$ is convex. But you can't reverse this implication.", "meta": {"post_id": 303991, "input_score": 14, "output_score": 37, "post_title": "is nonlinear least square a non convex optimization?"}}
{"input": "Math people:\nIt is my understanding that set theorists/logicians compare cardinalities of sets using injections rather than surjections.  Wikipedia defines countable sets in terms of injections.  Cantor's diagonal proof that the real numbers are uncountable involves showing that there is no surjection from $\\mathbb{N}$ to $(0,1)$.  So do I need the Axiom of Choice to accept Cantor's Diagonal Proof?\nI browsed the Similar Questions and I could not find an answer.  I apologize if this is a duplicate.\nStEFAN (Stack Exchange FAN)", "output": "No. You don't need choice for this.\nFor two reasons:\n\nIf there is an injection from a non-empty set $A$ into $B$ then there is a surjection from $B$ onto $A$. This does not require the axiom of choice, although the inverse implication (that a surjection has an injective inverse) is in fact equivalent to the axiom of choice.\nTo add on this, $\\mathbb N$ is well-ordered without the axiom of choice, so if there is a surjection from $\\mathbb N$ onto a set $A$, then there is an injection from $A$ into $\\mathbb N$ as well.\n\nThe axiom of choice is used when the existence of something is to be shown. In the diagonal proof you assume that you are given a certain list, and you define from that list a new function which is not on the list. This process does not require the axiom of choice.", "meta": {"post_id": 304378, "input_score": 27, "output_score": 38, "post_title": "Do you need the Axiom of Choice to accept Cantor's Diagonal Proof?"}}
{"input": "Let $A \\times \\emptyset = \\{(x,y)| x\\in A, y \\in \\emptyset \\}$. We know there is no element in $\\emptyset$. But how does it follow that $A \\times \\emptyset = \\emptyset $?", "output": "Claim:\n$A\\times B=\\emptyset$ iff $A=\\emptyset$ or $B=\\emptyset$\nProof:\nIf $A=\\emptyset$ or $B=\\emptyset$, then there is no $(a,b)$ such that $a\\in A$ and $b\\in B$. Therefore $A\\times B$, which is the set of these pairs, is empty.\nIf $A\\neq\\emptyset$ and $B\\neq\\emptyset$, there exists $a\\in A$ and $b\\in B$, thus $(a,b)\\in A\\times B$. Therefore $A\\times B\\neq\\emptyset$.", "meta": {"post_id": 305766, "input_score": 37, "output_score": 52, "post_title": "Why is the Cartesian product of a set $A$ and empty set an empty set?"}}
{"input": "What is the rule for computing $ \\text{E}[X^{2}] $, where $ \\text{E} $ is the expectation operator and $ X $ is a random variable?\nLet $ S $ be a sample space, and let $ p(x) $ denote the probability mass function of $ X $.\nIs\n$$\n\\text{E}[X^{2}] = \\sum_{x \\in S} x^{2} \\cdot p(x),\n$$\nor do I also need to square the $ x $ appearing in $ p(x) $?", "output": "In general, if $ (\\Omega,\\Sigma,P) $ is a probability space and $ X: (\\Omega,\\Sigma) \\to (\\mathbb{R},\\mathcal{B}(\\mathbb{R})) $ is a real-valued random variable, then\n$$\n\\text{E}[X^{2}] = \\int_{\\Omega} X^{2} ~ d{P}.\n$$\nAlthough this formula works for all cases, it is rarely used, especially when $ X $ is known to have certain nice properties.\nExamples:\n\nIf $ X $ is a discrete random variable (i.e., its cumulative distribution function (cdf) is a step-function) and $ p $ is its probability mass function (pmf), then we can use the formula\n$$\n\\text{E}[X^{2}] = \\sum_{x \\in \\text{Range}(X)} x^{2} \\cdot p(x).\n$$\nIf $ X $ is an absolutely continuous random variable (i.e., its cdf is an absolutely continuous function), then it possesses a probability density function (pdf) $ f $. We thus have the formula\n$$\n\\text{E}[X^{2}] = \\int_{\\mathbb{R}} x^{2} f(x) ~ d{\\mu(x)},\n$$\nwhere $ \\mu $ is the standard Borel measure on $ \\mathbb{R} $. Of course, if $ f $ is continuous, then we can simply compute the improper Riemann integral\n$$\n\\text{E}[X^{2}] = \\int_{- \\infty}^{\\infty} x^{2} f(x) ~ d{x}.\n$$", "meta": {"post_id": 306659, "input_score": 47, "output_score": 35, "post_title": "Computing the Expectation of the Square of a Random Variable: $ \\text{E}[X^{2}] $."}}
{"input": "Assume we have a sequence of rational numbers $a=(a_n)$. Assume we have a summation function $S: \\mathscr {L}^1 \\mapsto \\mathbb R, \\ \\ S(a)=\\sum a_n$ ($\\mathscr {L}^1$ is the sequence space whose sums of absolute values converges). Assume also that $S(a) \\in \\mathbb R \\setminus \\mathbb Q$. \nI would like to know if every such sequence $a$ has a subsequence $b$ (infinitely long) such that $S(b) \\in \\mathbb Q$.\nTake as an example $a_n = 1/n^2$. Then $S(a)=\\pi^2/6$. But $a$ has a subsequence $b=(b_n)=(1/(2^n)^2)$ (ie. all squares of powers of $2$). Then $S(b)=4/3$. Is this case with every such sequence?", "output": "No.  For example, take the sequence $a_n=2^{-2^n}$, $n=1$, $2$, $\\dots$.  An infinite subsequence $(a_{n_k})$ of $(a_n)$ will have sum \n$$S:=\\sum_k a_{n_k}=\\sum_k 2^{-2^{n_k}}.$$\nSo, the binary expansion of $S$ will have $1$s in positions $2^{n_1}$, $2^{n_2}$, $2^{n_3}$, $\\dots$, and $0$s everywhere else.  This is not a periodic sequence, so $S$ must be irrational.", "meta": {"post_id": 311695, "input_score": 23, "output_score": 36, "post_title": "Does every sequence of rationals, whose sum is irrational, have a subsequence whose sum is rational"}}
{"input": "This is, I'm sure, an incredibly naive question, but: is there a simple explanation for why one should be interested in 1-cocycles?\nLet me explain a bit. Given an action of a group $G$ on another group $A$ (the group structure of $A$ is respected by the action, in the sense that $\\tau(ab)=(\\tau a)(\\tau b)$ for $a, b\\in A$ and $\\tau \\in G$), a (1-)cocycle is a map $a: G\\rightarrow A$ satisfying $a(\\sigma\\tau)=a(\\sigma)\\sigma(a(\\tau))$. Now these objects are closely connected with group cohomology. My question is the following. Suppose that I didn't know the language of cohomology or homological algebra (which isn't much of a supposition). Is there a simple explanation for why these objects are interesting?\nNote: I don't mean to seem dismissive of the larger edifice of group cohomology; even knowing nothing about it, it's clear to me that it is both interesting and incredibly powerful. But the cocycle condition is so simply stated, that it seems there should be a clear picture for what these objects are doing without delving into the more abstract machinery of group cohomology.", "output": "I believe there's a very simple motivation for the cocycle condition:\nSuppose $f:X \\to R$ is a function on a space $X$ with values in a ring $R$ (if you wish, $X$ is a manifold, and $R$ is $\\mathbb{R}$ or $\\mathbb{C}$, and the function $f$ is differentiable or even holomorphic). Suppose that $G$ is a group acting on $X$ (say on the left). Then for the function to be $G$-invariant is to say that $f(gx)=f(x)$ for all $x \\in X$ and $g \\in G$.\nOften, this condition is too restrictive, i.e. there aren't enough functions satisfying this condition (as well as some other niceness conditions like holomorphic or whatever). Instead, you might want to condition the class of functions satisfying the following relation $$f(gx)=j(g,x) f(x),$$ where $j:G \\times X \\to R$ is a function. I'll give motivation for this type of relation below, but first I'd like to mention the consequences of such a relation.\nSuppose there exists a nonzero $f$ satisfying this relation. Then for all $x \\in X$ and $g_1,g_2 \\in G$, we have $$f(g_1 g_2 x) = j(g_1 g_2, x) f(x).$$ On the other hand, we have\n\\begin{eqnarray*}\nf(g_1 g_2 x) &=& j(g_1,g_2 x) f(g_2 x)\n&=& j(g_1,g_2 x ) j(g_2,x) f(x)\n\\end{eqnarray*}\nThus we find that $j(g_1 g_2, x) = j(g_1,g_2 x) j(g_2, x)$. If we let $\\mathcal{O}^\\times$ denote the group of nonzero functions on $X$ under multiplication (possibly only those satisfying some condition, like continuity or differentiability or holomorphicity), then we can think of $j$ as a map $G \\to \\mathcal{O}^\\times$. We can think of $G$ as acting on $\\mathcal{O}^\\times$ on the right by precomposition, and then this condition on $j$ is precisely the cocycle condition for a map $G \\to \\mathcal{O}^\\times$. In particular, it determines an element of $H^1(G,\\mathcal{O}^\\times)$.\nSo whence the condition $f(gx)=j(g,x) f(x)$? I'll start with some classical though possibly less conceptual motivation from special functions, then give the geometric interpretation in terms of line bundles.\nLet $L$ be a lattice in the complex plane, i.e. the $\\mathbb{Z}$-span of a $\\mathbb{R}$-basis of $\\mathbb{C}$. A theta function is a meromorphic function such that $\\theta(z+\\omega)=j(\\omega,z)\\theta(z)$ for all $z \\in \\mathbb{C}$, $\\omega \\in L$. More generally, if $X$ is a contractible Riemann surface, and $G$ is a group which acts on $X$ under sufficiently nice conditions, consider meromorphic functions $f$ on $X$ such that $f(gz)=j(g,z)f(z)$ for $z \\in X$, $g \\in G$, where $j: G \\times X \\to \\mathbb{C}$ is holomorphic for fixed $g$. In the case of theta functions, $G$ is $L$, and $X$ is $\\mathbb{C}$.\nAnother basic example is a modular form such as $G_{2k}(z)$, which satisfies $G_{2k}(g z) = (cz+d)^{2k} G_{2k}(z)$, where $g= \\left(\\begin{array}{cc} a & b \\\\ c & d \\end{array}\\right)  \\in G = SL_2(\\mathbb{Z})$ acts as a fractional linear transformation. It follows automatically that something as simple as $(cz+d)^{2k}$ is a cocycle in group cohomology, since $G_{2k}$ is, for example, nonzero. In this case $X = \\mathcal{H}$, the complex upper-half plane.\nNow for some geometric motivation. I'll stick to the case of elliptic curves, though I might rewrite this and make it more general. We define an elliptic curve to be $E=\\mathbb{C}/L$ for a two-dimensional lattice $L$. Note that the first homology group of this elliptic curve is isomorphic to $L$ precisely because it is a quotient of the universal cover $\\mathbb{C}$ by $L$. We will see that a theta function is a section of a line bundle on an elliptic curve. Since any line bundle can be lifted to $\\mathbb{C}$, the universal cover, and any line bundle over a contractible space is trivial, the line bundle is a quotient of the trivial line bundle over $\\mathbb{C}$. We can define a function $j(\\omega,z):L \\times \\mathbb{C} \\to \\mathbb{C} \\setminus \\{0\\}$. Then we identify $(z,w) \\in \\mathbb{C}^2$ (i.e. the line bundle over $\\mathbb{C}$) with $(z+\\omega,j(\\omega,z)w)$. For this equivalence relation to give a well-defined bundle over $\\mathbb{C}/L$, we need the following: Suppose $\\omega_1,\\omega_2 \\in L$. Then $(z,w)$ is identified with $(z+\\omega_1+\\omega_2,j(\\omega_1+\\omega_2,z)w$. But $(z,w)$ is identified with $(z+\\omega_1,j(\\omega_1,z)w)$, which is identified with $(z+\\omega_1+\\omega_2,j(\\omega_2,z+\\omega_1)j(\\omega_1,z)w)$. In other words, this forces $j(\\omega_1+\\omega_2,z) = j(\\omega_2,z+\\omega_1)j(\\omega_1,z)$. This means that, if we view $j$ as a function from $L$ to the set of non-vanishing holomorphic functions $\\mathbb{C} \\to \\mathbb{C}$, with (right) L-action on this set defined by $(\\omega f)(z) \\mapsto f(z+\\omega)$, then $j$ is in fact a $1$-cocyle in the language of group cohomology. Thus $H^1(L,\\mathcal{O}(\\mathbb{C}))$, where $\\mathcal{O}(\\mathbb{C})$ denotes the (additive) $L$-module of holomorphic functions on $\\mathbb{C}$, classifies line bundles over $\\mathbb{C}/L$. What's more is that this set is also classified by the sheaf cohomology $H^1(E,\\mathcal{O}(E)^{\\times})$ (where $\\mathcal{O}(E)$ is the sheaf of holomorphic functions on $E$, and the $\\times$ indicates the group of units of the ring of holomorphic functions). That is, we can compute the sheaf cohomology of a space by considering the group cohomology of the action of the homology group on the universal cover! In addition, the $0$th group cohomology (this time of the meromorphic functions, not just the holomorphic ones) is the invariant elements under $L$, i.e. the elliptic functions, and similarly the $0$th sheaf cohomology is the global sections, again the elliptic functions.\nOnce one constructs a line bundle on $E$ as a quotient of a line bundle on $\\mathbb{C}$ via the cocycle $j$, a section of that line bundle corresponds to a function $f$ on the complex plane satisfying the cocycle condition. Similarly, modular forms are sections of line bundles on modular curves.", "meta": {"post_id": 312515, "input_score": 50, "output_score": 50, "post_title": "What is the intuition between 1-cocycles (group cohomology)?"}}
{"input": "How would exponentiation be defined in Peano arithmetic? Unless $n$ is fixed natural number, $x^n$ seems to be hard to define. \nEdit 2: So, what would be the way to define $x^n+y^n = z^n$ using $\\Sigma_1^0$ formula?\nEdit: OK, I say Peano arithmetic has addition and multiplication stuffs. This allows additions to be expressed without quantifiers but as for exponentiation Peano arithmetic is silent. That's why I asked this question. Just for clarification.", "output": "The bare bones answer is something like what Hagen has said. The idea is this: Exponentiation is understood to be a function defined recursively: $y=2^x$ iff there is a sequence $t_0,t_1,\\dots,t_x$ such that \n\n$t_0=1$,\n$t_x=y$, and\nFor all $n<x$, $t_{n+1}=2\\times t_n$.\n\nIn this respect, exponentiation is hardly unique: $y=x!$ is defined similarly, for example. Now you'd say that there is a sequence $z_0,z_1,\\dots,z_x$, such that \n\n$z_0=1$,\n$z_x=y$, and\nFor all $n<x$, $z_{n+1}=(n+1)\\times z_n$.\n\n(That $t_0=z_0=1$ is coincidence. In one case it is because $2^0=1$; in the other, because $0!=1$.)\nSo, to define a formula saying that $y=2^x$, you'd like to say that there is a sequence $t_0,\\dots,t_x$ as above. \nThe problem, of course, is that in Peano Arithmetic one talks about numbers rather than sequences. G\u00f6del solved this problem when working on his proof of the incompleteness theorem: He explained how to code finite sequences by numbers, by using the Chinese remainder theorem. Recall that this result states that, given any numbers $n_1,\\dots,n_k$, pairwise relatively prime, and any numbers $m_1,\\dots,m_k$, there is a number $x$ that simultaneously satisfies all congruences \n $$ x\\equiv m_i\\pmod {n_i} $$\nfor $1\\le i\\le k$. \nIn particular, given $m_1,\\dots,m_k$, let $n=t!$ where $t=\\max(m_1,\\dots,m_k,k)$. Letting $n_1=n+1$, $n_2=2n+1,\\dots$, $n_k=kn+1$, we see that the $n_i$ are relatively prime, and we can find an $x$ that satisfies $x\\equiv m_i\\pmod{n_i}$ for all $i$. \nWe can then say that the pair $\\langle x,n\\rangle$ codes the sequence $(m_1,\\dots,m_k)$. In fact, given $x,n$, it is rather easy to \"decode\" the $m_i$: Just note that $m_i$ is the remainder of dividing $x$ by $in+1$. \nAccordingly, we can define $y=2^x$ by saying that there is a pair $\\langle a,b\\rangle$ that, in the sense just described, codes a sequence $(t_0,t_1,\\dots,t_x)$ such that $t_0=1$, $t_x=y$, and $t_{n+1}=2t_n$ for all $n<x$. (Again, \"in the sense just described\" ends up meaning simply that \"the remainder of dividing $a$ by $ib+1$ is $t_i$ for all $i\\le x$\". Note that we are not requiring $b$ to be the particular number we exhibited above using factorials.)\nOf course, one needs to prove that any two pairs coding such a sequence agree on the value of $t_x$, but this is easy to establish. And we can code a pair by a number using, for example, Cantor's enumeration of $\\mathbb N\\times\\mathbb N$, so that $\\langle a,b\\rangle$ is coded as the number \n $$ c=\\frac{(a+b)(a+b+1)}2+b. $$\nThis is a bijection, and has the additional advantages that it is definable and satisfies $a,b\\le c$ (so it is given by a bounded formula). \nAn issue that appears now is that we need to formalize the discussion of the Chinese remainder theorem and the subsequent coding within Peano arithmetic. This presents new difficulties, as again, we cannot (in the language of arithmetic) talk about sequences, and cannot talk about factorials, until we do all the above, so it is not clear how to prove or even how to formulate these results. \nThis problem can be solved by noting that we can use induction within Peano arithmetic. One then proceeds to show, essentially, that given any finite sequence, there is a pair that codes it, and that if a pair codes a sequence $\\vec s$, and a number $t$ is given, then there is a pair that codes the sequence $\\vec s{}^\\frown(t)$. That is, one can write down a formula $\\phi(x,y,z)$, \"$y$ codes a sequence, the $z$-th member of which is $x$\", such that PA proves:\n\nFor all $z$ and $y$ there is a unique $x$ such that $\\phi(x,y,z)$.\nFor all $x$ there is a $y$ such that $\\phi(x,y,0)$.\nFor all $x,y,z$ there is a $w$ such that the first $z$ terms of the sequences coded by $y$ and $w$ coincide, and the next term coded by $w$ is $x$.\n\nIn fact, we can let $\\phi$ be a bounded formula: Take $\\phi(x,y,z)$ to be\n\nThere are $a,b\\le y$ such that $y=\\langle a,b\\rangle$ (Cantor's pairing) and $x<zb+1$ and there is a $d\\le a$ such that $a=d(zb+1)+x$.\n\nOnce we have in PA the existence of coded sequences like this, implementing recursive definitions such as exponential functions is straightforward.\nThere are two excellent references for these coding issues and the subtleties surrounding them:\n\nRichard Kaye. Models of Peano arithmetic. Oxford Logic Guides, 15. Oxford Science Publications. The Clarendon Press, Oxford University Press, New York, 1991. MR1098499 (92k:03034). (See chapter 5.)\nPetr H\u00e1jek, and Pavel Pudl\u00e1k. Metamathematics of first-order arithmetic. Perspectives in Mathematical Logic. Springer-Verlag, Berlin, 1993. MR1219738 (94d:03001). (See chapter 1.)", "meta": {"post_id": 312891, "input_score": 39, "output_score": 51, "post_title": "How is exponentiation defined in Peano arithmetic?"}}
{"input": "I am not a mathematician but I am interested in big numbers.  I find them to be really interesting, almost god-like.\nI am watching a series of videos from David Metzler on YouTube.  I have a basic understanding of some fast growing functions.\nDavid does not cover $\\operatorname{TREE}(n)$ which I've read is one of the fastest growing functions.  I've looked at it explained in many places and even asked someone via Quora to help explain it to me, but I still don't get it.\nI think I need some sort of example of what $\\operatorname{TREE}(1)$, $\\operatorname{TREE}(2)$ and so on look like.", "output": "I'll try to answer.  It's a rather difficult question since (1) answering why something in mathematics is the way it is is troublesome; and (2) this subject has technical details that make it difficult to give a \"beginner's explanation\".\nIt appears that you have watched David Metzler's videos on fast-growing functions, so you know a little about ordinals, and specifically on how they give rise to fast-growing functions.  I will first describe a function on ordinals that outputs fast-growing functions;  second, I will explain how this relates to the tree(n) function, which is about sequences of unlabelled trees;  finally, I will show how the TREE(n) function, which is about labelled trees, grows even faster.  So bear with me here.\nFirst, let us assume that for every limit ordinal (a limit ordinal is an ordinal with no predecessor) in a sufficiently large initial segment of ordinals, we have defined a fundamental sequence;  that is, an increasing sequence of ordinals whose limit is the original ordinal.  So for $\\omega$ we can take the sequence {0, 1, 2, ...}; for $\\omega * 2$ we can take the sequence $\\\\{ \\omega, \\omega + 1, \\omega + 2, \\ldots, \\\\}$;  for $\\omega ^ 2$, we can take the sequence $\\\\{ 0, \\omega, \\omega * 2, \\ldots, \\\\}$; and so on.  For an ordinal $\\alpha$, define $\\alpha [n]$ to be the $n$th element of the fundamental sequence of $\\alpha$ when $\\alpha$ is a limit ordinal, and the predecessor of $\\alpha$ when $\\alpha$ is a successor ordinal.\nNotice that $\\alpha[n] < \\alpha$ for all ordinals $\\alpha$ and natural numbers $n$.  So $\\alpha, \\alpha[n], \\alpha[n][n+1], \\alpha[n][n+1][n+2], \\ldots$ is a strictly decreasing sequence of ordinals.  A defining property of ordinals is that there is no infinite decreasing sequence of ordinals;  so for $\\alpha > 0$, there exists a minimal natural number $m$ so that $\\alpha[n][n+1] \\ldots [m] = 0$.  Define $H(\\alpha,n)$ to be this minimal $m$.\nSome examples:\n$1[n] = 0$ for all $n$, so $H(1,n) = n$.\n$2[n] = 1, 2[n][n+1] = 0$ for all $n$, so $H(2, n) = n+1$.\nSimilarly, $m[n][n+1] \\ldots [n+m-1] = 0$, for all $n$, so $H(m, n) = n + m - 1$.\n$\\omega[n] = n, \\omega[n][n+1] = n-1, \\ldots, \\omega[n][n+1] \\ldots [2n] = 0$, so $H(\\omega, n) = 2n$.\n$(\\omega+1)[n] = \\omega, (\\omega+1)[n][n+1] = n+1, (\\omega+1)[n][n+1] \\ldots [2n+2] = 0$, so $H(\\omega+1, n) = 2n + 2$.\nSimilarly, $H(\\omega+m, n) = 2n + 2m$.\n$(\\omega * 2)[n] = \\omega + n, (\\omega*2)[n]\\ldots[2n] = \\omega, (\\omega*2)[n]\\ldots[2n+1] = 2n+1, (\\omega*2)[n]\\ldots[4n+2] = 0$, so $H(\\omega*2, n) = 4n + 2$.\nSimilarly, $H(\\omega * 2 + m, n) = 4n + 4m + 2$.\n$H(\\omega*3, n) = 4(2n+1) + 2 = 8n + 6$.\n$H(\\omega*4, n) = 8(2n+1) + 6 = 16n + 14$.\n$H(\\omega*m, n) = 2^m n + 2^m - 2$.\n$H(\\omega^2, n) = 2^n (n+2) - 2 > F_2 (n) > 2^n$. (Here $F_{\\alpha}(n)$ is the fast-growing hierarchy as described in Metzler's videos.)\n$H(\\omega^3, n) > F_3 (n) > 2 \\uparrow \\uparrow n$.\n...\n$H(\\omega^{\\omega}, n) > F_{\\omega}(n) > $ Ackermann($n, n$).\n$H(\\omega^{\\omega^{\\omega}}, n) > F_{\\omega^{\\omega}}(n)$.\n...\n$H(\\varepsilon_0, n) > F_{\\varepsilon_0}(n-1)$.\n...and so on.  As you can see, this construction leads to the same large numbers as the fast-growing hierarchy.  In general, $H(\\omega^{\\alpha}, n)$ is approximately $ F_{\\alpha} (n)$.\nOkay, on to trees.  Define tree(n) to be the longest sequence of unlabelled, unordered trees $T_1, T_2, \\ldots$ such that, for all $i$, $T_i$ has at most $n + i$ vertices, and for all $i, j$ with $i < j$, $T_i$ is not homeomorphically embeddable into $T_j$.\nOne might wonder why the sequence can't be infinite.  This is basically because trees, ordered under embeddability, are \"well-quasi-ordered\", or WQO.  This means that it has the same \"no infinite descending sequence\" property that ordinals have, but it may have elements that are incomparable.  In fact it does:  take one tree to be a root with two children, and another to be a tree with a child and a grandchild.  These two trees are incomparable; neither one embeds in the other.\nSo, trees under embeddability are not well-ordered.  But we can extend our partial order to a well-order;  we will use a modification of the well-ordering used by Levitz and Jervell.  (The Levitz/Jervell ordering is for ordered trees;  we need one for unordered trees.)\nWe don't really need the exact details of the well-ordering, but I will give it here for reference.  Given two trees, $S$ and $T$, we use the following comparison algorithm.  First check, inductively, if $S$ is less than or equal to any of the immediate subtrees of $T$;  if so, then $S < T$.  Similarly, check if $T$ is less than or equal to any of the immediate subtrees of $S$; if so, then $T < S$.  If neither of those checks apply, then compare the number of children of the root of $S$ to the number of children of the root of $T$;  the tree with the larger number is greater.  Finally, if the roots of $S$ and $T$ have the same number of children, compare the immediate subtrees of $S$ and $T$ one by one, starting from the smallest pair, then going to the second smallest pair, etc.  The first time you find two different immediate subtrees, the greater of the two will belong to the greater original tree.\nOkay, so now we have a well-ordering of trees, such that if $T_i < T_j$, then $T_i$ is not embeddable into $T_j$.  This gives us a strategy for constructing long sequences of trees that obey the required conditions:  take $T_{i+1}$ to be the largest tree less than $T_i$ with no more than the largest allowable number of nodes.\nSo, what kind of numbers do we get?  Well, the situation is very similar to our above sequences of decreasing ordinals!  For example, let the tree $T$ consist of a single path of $m$ vertices; this corresponds to the finite ordinal $m$. Let $n$ be the maximum allowable number of vertices in the next tree.  Then the next tree, which we will denote $T[n]$, will be a path of $m-1$ vertices, which corresponds to the ordinal $m-1$, which is exactly $m[n]$.  Similarly, the next tree, $T[n][n+1]$, will correspond to the ordinal $m[n][n+1]$, and so on, so we will reach the empty tree at the same time the corresponding ordinal reaches 0, so the length of the sequence starting from $T$ will be $H(m, n) - n + 1$.\nSimilarly, suppose $T$ is the tree where the root has two childless children.  This corresponds to the ordinal $\\omega$.  $T[n]$ will then be a path of n nodes, which is the tree that corresponds to the ordinal $\\omega[n]$.  So we reach the empty tree at $T[n] \\ldots [2n]$, since $\\omega[n] \\ldots [2n] = 0$.  So the length of the sequence starting from $T$ is $H(\\omega, n) - n + 1 = n + 1$.  For larger trees/ordinals, we won't get a length of exactly $H(\\alpha, n) - n + 1$, but it will be pretty close.\nSo, if we start from a tree corresponding to $\\omega^{\\alpha}$, we will get a function comparable to $F_{\\alpha}$ in the fast-growing hierarchy.  The natural question to ask is:  what is the ordinal corresponding to this well-ordering of trees?\nMetzler's videos decribes the Veblen hierarchy, going up to $\\Gamma_0$.  We can continue on by defining $\\phi(1, 0, \\alpha)$ to be the $\\alpha$th fixed point of $f(x) = \\phi(x, 0)$. (So $\\phi(1, 0, 0) = \\Gamma_0$.)  We then define a second Veblen hierarchy where the starting function is $\\phi(1, 0, \\alpha)$ instead of $\\omega^{\\alpha}$; so\n$\\phi(1, \\alpha+1, \\beta)$ is the $\\beta$th fixed point of $f(x) = \\phi(1, \\alpha, x)$.\nWhen $\\alpha$ is a limit ordinal,\n$\\phi(1, \\alpha, \\beta)$ is the $\\beta$th ordinal in the ranges of $f(x) = \\phi (1, \\gamma, x)$ for all $\\gamma < \\alpha$.\nWe can then define\n$\\phi(2, 0, \\alpha)$ to be the $\\alpha$th fixed point of $f(x) = \\phi(1, x, 0)$.\nMore generally, we define\n$\\phi(\\alpha+1, 0, \\beta)$ to be the $\\beta$th fixed point of $f(x) = \\phi(\\alpha, x, 0)$\nand for $\\alpha$ a limit ordinal,\n$\\phi(\\alpha, 0, \\beta)$ is the $\\beta$th ordinal in the ranges of $f(x) = \\phi(\\gamma, x, 0)$ for all $\\gamma < \\alpha$.\nNext, we can go to 4 places: define\n$\\phi (\\alpha+1, 0, 0, \\beta)$ to be the $\\beta$th fixed point of $f(x) = \\phi(\\alpha, x, 0, 0)$\nand for $\\alpha$ a limit ordinal\n$\\phi (\\alpha, 0, 0, \\beta)$ is the $\\beta$th ordinal in the ranges of $f(x) = \\phi(\\gamma, x, 0, 0)$ for all $\\gamma < \\alpha$.\nYou can see how to extend this to arbitrarily many places.  The smallest ordinal that cannot be reached by applying this extended phi notation is called the Small Veblen ordinal.\nWell, it turns out that the Small Veblen ordinal is the limit of Levitz'/Jervell's well-ordering.  In particular,\nthe tree whose root has two childless children corresponds to $\\omega$.\nthe tree whose root has three childless children corresponds to $\\epsilon_0$.\nthe tree whose root has four childless children corresponds to $\\phi(1,0,0,0)$.\nIn general, the tree whose root has n childless children for $n \\ge 4$ corresponds to $\\phi(1, 0, 0, \\ldots, 0)$ with $n-1$ $0$'s.\nNow we can finally relate this all to TREE(n).  TREE(n) is defined as the length of the longest sequence of trees $T_1, T_2, \\ldots$ labelled from {1, 2, ..., n} such that, for all $i$, $T_i$ has at most $i$ vertices, and for all $i, j$ such that $i < j$, there is no label-preserving homeomorphic embedding from $T_i$ into $T_j$.\nTREE(1) is clearly 1.  The first tree can only be the unique one-vertex tree labelled with 1.  This tree obviously embeds into any other tree, so we are done at 1.\nTREE(2) is 3.  The first tree can only be the unique one-vertex tree, labelled with either 1 or 2, it doesn't matter which. Say we label it with 1.  Then no remaining trees can use the label 1; all vertices must be labelled with 2.  The second tree can either be the unique one-vertex tree or the unique two-vertex tree.  If it is the former, we can have no more trees;  if it is the latter, the only tree that the two-vertex tree does not embed into is the one vertex tree, so it is the only choice for the third tree is the one-vertex tree, and again we are done.\nSo, so far we are not getting very long sequences!  But TREE(3) is a totally different kettle of fish.\nBefore we get to TREE(3), let's examine some smaller sequences that will build up towards it.\nTo describe a tree, I will use () to denote a vertex labelled with 1, and [] to denote a vertex labelled with 2.  The children of a vertex will be placed within the separators for the vertex; so for example\n([][][])\nmeans a vertex labelled with 1 with three children labelled with 2; and\n[(()()) ()]\nmeans a vertex labelled with 2 with two children labelled with 1; the left child has two children labelled with 1.\nWe will start by examining trees that are paths, with the root labelled with 2 and the rest of the vertices labelled with 1.\n[]\n(())\n()\nstarting from a single vertex labelled with 2 leads to a sequence of length 3.\n[()]\n(()[])\n((([])))\n(([]))\n([])\n[]\n(()()()()()()())\nthe last tree is the first tree in the sequence for tree(7) so we get a sequence of length greater than tree(7)\n[(())]\n(()[()])\n((([()])))\n(([()]))\n([()])\n[()]\n(()()()()()()()[])\nthe last tree is the first tree in the sequence for tree(8), except the last vertex is labelled with a 2.  We can thus continue with a sequence of length tree(8) of trees with all but one vertex labelled with 1, finally ending in a tree consisting of one vertex labelled with 2.  The next tree is then a tree with more than tree(8) vertices with all vertices labelled with one;  this leads to a sequence of more than tree(tree(8)) vertices.\nContinuing in this fashion, a path with one vertex labelled with 2 and n vertices labelled with 1 will lead to a sequence of more than tree$^n(n+6)$ trees.  If we define tree$_2 (n)$ to be tree$^n(n)$, then our lower bound is more than tree$_2 (n)$ trees.\nNow consider a tree consisting of a path of length $n+1$ with the bottom vertex of the path having two children.  Again, the root will have label 2 and the rest of the vertices will have label 1.  For example, with n = 3 the tree is\n[(((()())))]\nWe can construct a sequence of more than tree$_2 (n-1)$ trees by basically following the previous sequence, with the two children at the bottom added on.  This leads us to the tree [()()].  We follow that with the tree [(((...()...)))] with more than tree$_2 (n-1)$ vertices.  By our previous bound, we will wind up with a sequence of length greater than tree$_2$ (tree$_2 (n-1))$.\nIf we next consider a tree similar to the previous one, except we add a child to one of the two children at the bottom, we will get a lower bound of tree$_2$ (tree$_2$ (tree$_2 (n-1)))$.  If we add a path of length $m$ rather than a single child, we get a lower bound of tree$_2^{m+2}(n-1)$.  Define tree$_3 (n)$ to be tree$_2^n(n)$.  If $n \\ge m+3$, we have a lower bound of tree$_3 (m)$.\nNow we are ready to find a lower bound for TREE(3).  Start with:\n{}  (one vertex with label 3)\n[[]]\n([][])\n[()()()]\n[(())(())]\n[((()) ()) ()]\n[(((()()))) ()]\n[((((()()))()))]\n((((()()))()))\n((([(((()()))())])))\n(([(((()()))())]))\n([(((()()))())])\n[(((()()))())]\n(()()()()()()()[((()()))()])\nthis leads to a sequence of tree(8) trees, ending in\n[((()()))()]\nthis is followed by\n[((()())())]\nwhere the ( stands for tree(8) ('s and ) stands for tree(8) )'s.\nThis leads to a sequence of tree$_2$ (tree(8)) trees, ending in\n[(()())()]\nThis is followed by\n[(( () )())]\nwhere the ( stands for tree$_2$ (tree(8)) ('s and ) stands for tree$_2$ (tree(8)) )'s.\nThis leads to a sequence of more than tree$_3$ (tree$_2$ (tree(8))) trees.\nThus TREE(3) > tree$_3$ (tree$_2$ (tree(8))).\nAs you can imagine, the TREE(n) function clearly outpaces the tree(n) function, which is already at the level of the Small Veblen Ordinal in the fast-growing hierarchy.  This is not surprising, since labelled trees lead to more possibilities than unlabelled trees.\nI know this was very long, but I wanted to go step by step since I know you are not an expert in this subject.  Please feel free to ask about anything you are confused about.", "meta": {"post_id": 313134, "input_score": 65, "output_score": 108, "post_title": "Why is TREE(3) so big? (Explanation for beginners)"}}
{"input": "For quite a long time, I have been confused about the definitions of weak convergence and vague convergence of measures among other modes of convergence that root from functional analysis, mainly due to many different definitions and theorems from probability books. I would appreiciate it if someone can clarify the terms and give a clear picture of the concepts. (Note that Did has answered some of my related questions before. Thank you, Did!)\n\nIn Kallenberg's probability book, he defines weak convergence of a\nsequence measures to be \n\nConsider any probability measures $\\mu$ and $\\mu_1, \\mu_2, \\dots$ on some metric space $(S, \\rho)$ with Borel a-field $S$, and say\n  that $\\mu_n$ converges weakly to $\\mu$, if $\\int f d\\mu_n \\to \\int f\nd\\mu$ for every $f \\in C_b(S)$, the class of bounded, continuous\n  functions $f: S \\to \\mathbb R$.\n\nKallenberg defines vague convergence of a sequence of measures to be \n\nConsider the space $\\mathcal M = \\mathcal M(\\mathbb R^d) $of locally finite mea- sures on $\\mathbb R^d$. On $\\mathcal M$ we may\n  introduce the vague topology, generated by the mappings $\\mu\n\\mapsto \\int f d\\mu$ for all $f \\in C_K^+$, the class of continuous\n  functions $f: \\mathbb R^d \\to  \\mathbb R_+$ with compact support.\n  In particular, $\\mu_n$ is said to converge vaguely to $\\mu$  if\n  $\\mu_n f \\to \\mu f$ for all $f \\in C_K^+$. If the $\\mu_n$ are\n  probability measures, then clearly $\\mu(\\mathbb R^d) < 1$.\n\nFolland in his real ananlysis book defines vague topology and\ntherefore vague convergence for complex Radon measures on a locally compact\nHausdorff (LCH) space $X$ as weak* topology and weak* convergence wrt\n$C_0(X)$. He says the term \"vague\" is common in probability theory,\nand has the advantage of forming an adverb more gracefully than\n\"weak*\". The vague topology is sometimes called the weak topology,\nbut this terminology conflicts with his, since $C_0(X)$ is rarely\nreflexive.\nIn Kai Lai Chung's probability book, a sequence of subprobability\nmeasures $\\mu_n$ on $\\mathbb R$ are defined to vaguely converge to\nanother subprobability measure $\\mu$, if there exists a dense subset\n$D$ of $\\mathbb R$ s.t. $\\forall a, b \\in D, a < b, \\mu_n((a,b]) \\to\n\\mu((a,b])$.\nNext in Chung's, Theorem 4.4.1 says in case of subprobability measures, vague\nconvergence is equivalent to  weak* convergence wrt $C_0(\\mathbb\nR)$ and $C_K(\\mathbb\nR)$. Theorem 4.4.2 says in case of probability measures, vague\nconvergence is equivalent to  weak* convergence wrt $C_b(\\mathbb\nR)$.\n\nI was wondering if the above definitions of weak convergence and vague convergence are all weak* convergence, in the sense that the measures form (a subset of) the continuous dual of $C_b$, $C_K$, $C_K^+$, and $C_0$?\nWhen defining vague convergence and vague topology, why does kallenberg use $C_K^+$ instead of $C_K$, Folland use $C_0$, and Kai Lai Chung uses $C_K$, $C_0$ and $C_b$? Are their definitions of vague convergence consistent with each other?\nAmong the convergences of measures wrt $C_b$, $C_K$, $C_K^+$, and $C_0$, when does which imply which? When is which equivalent to which?\nThe last question is to see if there are some unifications of the above concepts. Can the above definitions be generalized to more general measures (probability measures, subprobability measures, locally finite measures are used in the definitions above), and to more general underlying spaces (metric space, $\\mathbb R^d$ and $\\mathbb R$ are used in the definitions above)?\nThanks and regards!!", "output": "Ok so there is a lot to say here, let's start with the easiest question.\nThe use of $C^+_K$ instead of $C_K$ change absolutely nothing because we can always write $f=f^+-f^-$ with $f^+=\\max(f,0)$ and $f^-=\\max(-f,0)$ which are positive and integrable.\nIn order to avoid confusions between the different names of the different convergence i will talk about $C_K$, $C_0$ or $C_b$ convergence.\nThe simple case\nFirst let's see what happen in the nicest case : Take $X$ to be a compact Hausdorff set. Then $C_K(X)=C_b(X)=C^0(X)$ with $C^0(X)$ the space of all continuous functions from $X$ to $\\mathbb R$ (or $\\mathbb C$, but it doesn't really matter). It's a bit difficult to say what $C_0(X)$ is for $X$ a topological space with no other structure but since here $X$ is compact the only natural definition is $C_0(X)=C_b(X)$. Moreover, using the Riesz-Markov theorem we can identify the (topological) dual of $C^0(X)$ and the space $\\mathcal M (X)$ of all (signed and finite) Radon measures on $X$. So now we can define the weak-* topology on $\\mathcal M (X)$ by $\\mu_n\\to \\mu$ if and only if $\\int fd\\mu_n \\to \\int f\\mu$ for every $f\\in C^0(X)$. Obviously all the definitions you have seen before are the same here because $C_K(X)=C_0(X)=C_b(X)=C^0(X)$. So in this case all those different convergences are the same and are, in fact, the weak-* convergence of measures. The weak-* topology of a Banach has a very important propety : the closed unit ball for the strong topology is compact for the weak-* topology (it's the Banach alaoglu theorem). Which imply the following : given a bounded sequence of Radon measures $\\mu_n$ there exists a Radon measure $\\mu$ and a subsequence $\\mu_{n_k}$ such that $\\mu_{n_k}$ is weak-* convergent to $\\mu$. Moreover we can prove that if the $\\mu_n$ are probability measures then $\\mu$ is also a probability measure (we'll see that later).\nNow in the more general case when $X$ is Hausdorff but only localy compact it's more difficult. First of all the spaces $C_K$, $C_b$ and $C^0$ are all different. And when $C_0(X)$ is easily definable (when $X$ is a metric space or a topological vector space for example) it's also different from all the previous spaces. So the chances are that the different definitions of convergences won't coincide anymore. First let's see what are the differences with the previous case.\nThe things that still work, or not\nIf we look at the Riesz Markov theorem it still work with $X$ only being locally compact, but the Radon measure are not finite anymore, only locally finite. So the convergence using $C_K(X)$ functions is still the weak-* convergence of measures. The Banach Alaoglu theorem also still work but the property about probability measure doesn't hold anymore : in $\\mathbb R$ the sequence $\\delta_n$ is weak-* convergent to $O$, which is not a probability measure. However if the sequence $\\mu_n$ is bounded and converge to $\\mu$ then $\\mu$ is also bounded and $\\mu(X)\\leq \\lim\\mu_n(X)$ (with the possibility of a strict inequality, as in the example before).\nFor the convergence with $C_0(X)$ functions we also have a representation theorem for the dual of $C_0(X)$ in terms of measures (see the wikipedia page of the Riesz-Markov theorem). So what i've said for $C_K(X)$ convergence will still hold. However even if the $C_K$ and the $C_0$ convergences can be viewed as weak-* convergences they are not the same. Take the sequence $n\\delta_n$, it's $C_K$ convergent to $0$ but not $C_0$ convergent (take for example $X=\\mathbb R$ and $f(x)=sin(x)/x$).\nFor the convergence with $C_b(X)$ functions it's different. There exists elements of the dual of $C_b(X)$ which cannot be represented as measures on $X$, for an exemple of that you can look at \"Banach limit\", the Banach limit of the $l^\\infty$ sequence $(f(n))_{n\\in\\mathbb N}$ is one of those weird element of the dual of $C_b(X)$. So the $C_b$ convergence is not a weak-* convergence. \nLast thing to say : since $C_K(X)\\subset C_0(X) \\subset C_b(X)$ we immediatly see that the $C_b$ convergence imply $C_0$ convergence which in turn imply $C_K$ convergence but the converses are all false. For example $\\delta_n$ in $\\mathbb R$ is $C_0$ convergent to $0$ but not $C_b$ convergent (take the bounded function sinus for example).\nHow to get things to work again\nThe almost obvious answer is : we have to work on a compact again. The usefull notion here is the notion of tight sequence (especially when dealing with probability): $(\\mu_n)$ is called tight if, for all $\\varepsilon>0$ there exists a compact $K_\\varepsilon$ such that $\\mu_n(K^C_\\varepsilon)<\\varepsilon$ for all $n$. So the sequence of measures are all almost supported in a compact set, so there is no possibility of mass \"escaping at infinity\" as it was the case with $(\\delta_n)$. If $(\\mu_n)$ is tight then the $\\mu_n$ are finite (we assume that all the measures are locally finite)\nNow you can read this answer of mine about tight sequences : Defining weak* convergence of measures using compactly supported continuous functions\nTo sum up things : if your bounded sequence of measures is tight then the 3 definitions are equivalent. Moreover if the $\\mu_n$ are probability measure then so is the limit, and if you have a sequence of probability measure converging to a probability measure $\\mu$ in the $C_K$ definition then it is tight. Moreover if $(\\mu_n)$ is $C_b$ convergent then is is automatically tight.\nThe other questions\nLocally finite measure that are note finite are usually not in the dual of $C_b$ and $C_0$. So the only definition of convergence that make sense here is the $C_K$ convergence.\nThe definition 4 is consistent with the other definitions of vague convergence because the linear combinations of $\\mathbf 1_{[a;b[}$ are dense in $C_0$ and because the sequence $(\\mu_n)$ is bounded.\nIf you read french you can learn more on http://www.proba.jussieu.fr/cours/dea/telehtml/telehtml.html", "meta": {"post_id": 313986, "input_score": 52, "output_score": 47, "post_title": "Are vague convergence and weak convergence of measures both weak* convergence?"}}
{"input": "How do you prove that the Sobolev space $H^s(\\mathbb{R}^n)$ is an algebra if $s>\\frac{n}{2}$, i.e. if $u,v$ are in $H^s(\\mathbb{R}^n)$, then so is $uv$? Actually I think we should also have $\\lVert uv\\rVert_s \\leq C \\lVert u\\rVert_s \\lVert v\\rVert_s$. Recall that $\\lVert f\\rVert_s=\\lVert(1+|\\eta|^2)^{s/2}\\,\\hat{f}(\\eta)\\rVert$, the norm on $H^s(\\mathbb{R}^n)$. This is an exercise from Taylor's book, Partial differential equations I.", "output": "Note that\n$$\n\\begin{split}\n(1+|\\xi|^2)^p\n&\\leq (1+2|\\xi-\\eta|^2+2|\\eta|^2)^p\\\\\n&\\leq 2^p(1+|\\xi-\\eta|^2+1+|\\eta|^2)^p\\\\\n&\\leq c(1+|\\xi-\\eta|^2)^p + c(1+|\\eta|^2)^p,\n\\end{split}\n$$\nfor $p>0$, where $c=\\max\\{2^{p},2^{2p-1}\\}$. \nPut $\\langle\\xi\\rangle=\\sqrt{1+|\\xi|^2}$.\nThen we have\n$$\n\\begin{split}\n\\langle\\xi\\rangle^s |\\widehat{uv}(\\xi)|\n&\\leq \\int \\langle\\xi\\rangle^s |\\hat{u}(\\xi-\\eta)\\hat{v}(\\eta)|\\,\\mathrm{d}\\eta\\\\\n&\\leq c\\int \\langle\\xi-\\eta\\rangle^s |\\hat{u}(\\xi-\\eta)\\hat{v}(\\eta)|\\,\\mathrm{d}\\eta\n+ c\\int \\langle\\eta\\rangle^s |\\hat{u}(\\xi-\\eta)\\hat{v}(\\eta)|\\,\\mathrm{d}\\eta\\\\\n&\\leq c|\\langle\\cdot\\rangle^s\\hat u|*|\\hat v| + c|\\hat u|*|\\langle\\cdot\\rangle^s\\hat v|,\n\\end{split}\n$$\nwhich, in light of Young's inequality, implies\n$$\n\\|uv\\|_{H^s} \\leq c\\|u\\|_{H^s} \\|\\hat v\\|_{L^1} + c\\|\\hat u\\|_{L^1}\\|v\\|_{H^s}.\n$$\nFinally, we note that $\\|\\hat u\\|_{L^1}\\leq C\\,\\|u\\|_{H^s}$ when $s>\\frac{n}2$.", "meta": {"post_id": 314820, "input_score": 38, "output_score": 39, "post_title": "Sobolev space $H^s(\\mathbb{R}^n)$ is an algebra with $2s>n$"}}
{"input": "When can we say a multiplicative group of integers modulo $n$, i.e., $U_n$ is  cyclic?\n\n$$U_n=\\{a \\in\\mathbb  Z_n \\mid \\gcd(a,n)=1 \\}$$\nI searched the internet but did not get a clear idea.", "output": "So $U_n$ is the group of units in $\\mathbb{Z}/n\\mathbb{Z}$.\nWrite the prime decomposition\n$$\nn=p_1^{\\alpha_1}\\cdots p_r^{\\alpha_r}.\n$$\nBy the Chinese remainder theorem\n$$\n\\mathbb{Z}/n\\mathbb{Z}=\\mathbb{Z}/p_1^{\\alpha_1}\\mathbb{Z}\\times\\ldots\\times\\mathbb{Z}/p_r^{\\alpha_r}\\mathbb{Z}\n$$\nso\n$$\nU_n=U_{p_1^{\\alpha_1}}\\times\\ldots\\times U_{p_r^{\\alpha_r}}.\n$$\nFor powers of $2$, we have\n$$\nU_2=\\{1\\}\n$$\nand for $k\\geq 2$\n$$\nU_{2^k}=\\mathbb{Z}/2\\mathbb{Z}\\times \\mathbb{Z}/2^{k-2}\\mathbb{Z}.\n$$\nFor odd primes $p$,\n$$\nU_{p^\\alpha}=\\mathbb{Z}/\\phi(p^\\alpha)\\mathbb{Z}=\\mathbb{Z}/p^{\\alpha-1}(p-1)\\mathbb{Z}.\n$$\nSo you see now that $U_n$ is cyclic if and only if\n$$\nn=2,4,p^\\alpha,2p^{\\alpha}\n$$\nwhere $p$ is an odd prime.\nHere is a reference.", "meta": {"post_id": 314846, "input_score": 49, "output_score": 43, "post_title": "For what $n$ is $U_n$ cyclic?"}}
{"input": "Do we rely on certain intuition or is there an unofficial general crude checklist I should follow?\n\nI had a friend telling me that if the sum of the powers on the numerator is smaller then the denominator, there is a higher chance that it may not exists.\nAnd if the sum of powers on the numerator is higher then the denominator, most likely, it exists.\nAlso if there is a sin or cos or e-constant, it most likely exists.\n\nHow can I know what to do at the first glance given so little time exists for me in exam to ponder? If I spend all my time on figuring out a two-path test when the limit exists, that would be a huge disaster.\nIs this one of those cases where practice makes perfect?\nExample: $$\\lim_{(x,y)\\to(0,0)}\\frac{(\\sin^2x)(e^y-1)}{x^2+3y^2}$$\nPlease give me a hint and where do you get the hint.\nExample: $$\\lim_{(x,y)\\to(0,0)}\\exp\\left(-\\frac{x^2+y^2}{4x^4+y^6}\\right)$$\nI need a hint for this too.\n\nCommon methods I have learnt for reference:\nTwo-Path test, Polar Coordinates, Spherical Coordinates, Mean Value Theorem using inequalities.", "output": "I wouldn't say there's a \"step by step\" method for all limits, as many require individual analysis and sometimes a clever observation, but I've assembeled a list of general techniques (I also used this to answer this question).\nIn general, it is much easier to show that a limit does not exist than it is to show a limit does exist, and either case might require a clever insight or tricky manipulation. There are a few common ways of working with multi-variable functions to obtain the existence or nonexistence of a limit:\n\nTry different paths. That is, parameterize $x$ and $y$ as $x = x(t)$, $y = y(t)$ such that $(x(0),y(0)) = (a,b)$, where $(a,b)$ is the point you want to approach in the limit. This is usually the first resort, and if the paths are chosen judiciously, you will obtain two different answers, which implies the nonexistence of the limit, because for the limit to exist, it must have the same value along every possible path. Note that this test can only be used to show nonexistence: to prove a limit exists requires more work.\nUse polar or spherical coordinates. This approach can prove that the limit exists in special cases, and it can also show that limits do not exist, because they may depend on the path ($\\theta$). It's a good idea to use when you have something that looks like $x^2 + y^2$ or $x^2 + y^2 + z^2$ that is troublesome, as these simply become $r^2$ after the substitution. We write $x = r\\cos\\theta$, $y = r\\sin\\theta$, and as the limits are usually taken as $(x,y)\\to (0,0)$, we now must look at what happens as $r\\to 0^+$. Sometimes, this will depend on $\\theta$, which corresponds to a specific path ($\\theta$ controls the direction), and sometimes, the $r$ will dominate and leave you with an expression where $\\theta$ does not matter - in this case, the limit exists. However, one must be careful, because there are some expressions that might seem to be independent of $\\theta$ as $r\\to 0^+$, but are not: for example, take\n$$\nr\\frac{\\cos^2\\theta\\sin^2\\theta}{\\cos^3\\theta + \\sin^3\\theta}.\n$$\nFor any constant $\\theta$ such that the denominator exists (and is nonzero), the limit as $r\\to 0^+$ is $0$, but there are certain paths $\\theta = \\theta(r)$ along which the value of the limit will not be $0$.\n$\\delta - \\epsilon$ proofs: When correct, these show the existence of a limit. However, one must already know what the limit is before this type of proof is possible. If you are unfamiliar with $\\delta - \\epsilon$ arguments, the statement is:\nGiven a function $f : \\mathbb{R}^n\\to\\mathbb{R}$, we say\n$$\n\\lim_{\\vec{x}\\to\\vec{x}_0} f(\\vec{x}) = L\n$$\nif for every $\\epsilon > 0$, there exists a $\\delta > 0$ such that $\\left|\\,f(\\vec{x}) - L\\right| < \\epsilon$ whenever $d(\\vec{x}, \\vec{x}_0) < \\delta$. Here, $d(\\vec{x}, \\vec{x}_0)$ refers to the distance in Euclidean $n$-space (for example, with $n = 2$ we have $d((a,b), (c,d)) = \\sqrt{(a - c)^2 + (b - d)^2}$.) This approach should be used if you are already convinced that the limit exists and is equal to $L$.\nUse algebra and theory. You have probably seen that the product and sum of continuous functions are continuous, and that for continuous functions, the limit can be evaluated by \"plugging in.\" Furthermore, if $f$ is continuous, then you can move a limit on the outside to a limit on the inside: $\\lim_{p\\to a} f(g(p)) = f(\\lim_{p\\to a} g(p))$. If you can identify that your function is continuous, or at least becomes continuous after algebraic manipulation (e.g. canceling a \"bad factor\" from the denominator), you can use the theorems to say that the limit exists. Another useful theorem is the squeeze theorem: if you can cleverly bound your function on both sides by two functions tending to the same limit, you know the limit exists.\nAs Norbert notes, another useful technique is expanding your non-elementary functions in Taylor series and using big $O$ notation. You can usually convert the troublesome limit into a quotient and expand the non-elementary functions near the point in question to get a polynomial of sufficiently high degree in both the numerator and the denominator (plus an error term that doesn't play much of a role): i.e. your function $\\frac{f(t)}{g(t)}$, where $f$ and $g$ are combinations of functions whose growth you don't know much about, becomes $\\frac{P(t) + O(t^n)}{Q(t) + O(t^n)}$, where $P$ and $Q$ are polynomials, and\n$$\n\\lim_{t\\to 0}\\frac{P(t) + O(t^n)}{Q(t) + O(t^n)} = \\lim_{t\\to 0}\\frac{P(t)}{Q(t)},\n$$\nbecause the error terms ($O$ terms) are negligible. Some examples for the one variable case can be found here, and the multivariable case works the same, only using the multivariable version of Taylor's theorem.", "meta": {"post_id": 316806, "input_score": 63, "output_score": 55, "post_title": "Is there a step by step checklist to check if a multivariable limit exists and find its value?"}}
{"input": "How to justify the convergence and calculate the sum of the series:\n$$\\sum_{n=1}^{+\\infty}\\frac{1}{1^2+2^2+\\cdots+n^2}.$$", "output": "$$\\begin{array}{lcl}\n\\sum_{n=1}^\\infty \\frac{1}{1^2+2^2+\\cdots+n^2}&=& \\sum_{n=1}^\\infty\\frac{6}{n(n+1)(2n+1)} \\\\ &=& 6\\sum_{n=1}^\\infty \\frac{1}{2n+1} \\left( \\frac{1}{n}-\\frac{1}{n+1}\\right) \\\\ &=& 12\\sum_{n=1}^\\infty \\frac{1}{2n(2n+1)} -12\\sum_{n=1}^\\infty \\frac{1}{(2n+1)(2n+2)} \\\\ &=& 12\\sum_{n=1}^\\infty \\left[ \\frac{1}{2n}-\\frac{1}{2n+1} \\right] - 12\\sum_{n=1}^\\infty \\left[ \\frac{1}{2n+1}-\\frac{1}{2n+2} \\right]\\\\\n&=& 12(1-\\ln 2)- 12\\left(\\ln 2-\\frac{1}{2}\\right)\\\\ &=& 18-24\\ln 2\n\\end{array}\n$$", "meta": {"post_id": 317219, "input_score": 22, "output_score": 35, "post_title": "The series $\\sum_{n=1}^{+\\infty}\\frac{1}{1^2+2^2+\\cdots+n^2}.$"}}
{"input": "My teachers have gone over rules for dealing with fractional exponents. I was just wondering how someone would compute say: $$(-5)^{2/3}$$ I have tried a couple ways to simplify this and I am not sure if the number stays negative or turns into a positive. I know that if a negative number is raised to an odd power it is negative, but fractional powers are neither odd or even. Is there a general rule for dealing with these types of problems?", "output": "This question is several years old. Nevertheless, I'm bothered that none of the answers provided were at an introductory level. Let me then contribute an answer, which is not rigorous, but will serve as a general heuristic for students who have only recently encountered fractional exponents with real numbers.\n\nWe can interpret a base raised to a simplified$^\\dagger$ fractional exponent with this heuristic:\n  $$x^{\\frac ab} = x^{\\frac{\\text{'power'}}{\\text{'root'}}}$$\n  That's saying that $a$ acts like a standard integer power and $b$ acts like a standard integer root.$^{\\dagger\\dagger}$\n\nYour example, $(-5)^{2/3}$, can be interpreted as squaring $-5$ and then taking the third root. Or, in the opposite order, taking the cube root of $-5$ and then squaring that result.\n\\begin{align}\n(-5)^{2/3} &= ((-5)^2)^{1/3} = \\sqrt[3]{25} \\approx 2.92 \n\\\\\\\\\n\\text{or}&\n\\\\\\\\\n(-5)^{2/3} &= (\\sqrt[3]{-5})^{2} \\approx (-1.71)^2 \\approx 2.92  \n\\end{align}\nNotice that in this particular example our base was negative. Since the denominator of the fraction was odd, we were able to solve for a real number. If the denominator were even, though, we would have no real solution, since the even root of a negative number is undefined for real numbers. Instead, we would have to turn to complex numbers for a more adequate interpretation (see the accepted answer by Hurkyl).\n\n$^\\dagger$ The fractional exponent must be simplified for the upcoming process to make sense. To see why, consider the example $(-8)^\\frac 24$. What happens if you don't simplify? If you do?\n$^{\\dagger\\dagger}$ We're assuming $a$ and $b$ are integers such that $a/b$ is  a rational number. That's likely what a student has seen when first encountering fractional exponents. If $a,b$ are not integers, then the meaning is less obvious.", "meta": {"post_id": 317528, "input_score": 78, "output_score": 34, "post_title": "How do you compute negative numbers to fractional powers?"}}
{"input": "Is there any upper bound for an expression like:\n$$\\left( a_1 + a_2 + \\cdots + a_n\\right)^{1/2} ?$$\nI need it for $n=3$. I know Hardy's inequality but it is for exponent greater than 1. Is there anything for the square root?", "output": "Elementary proof from scratch: $$(\\sqrt{a_1}+\\sqrt{a_2})^2 = a_1+a_2+2\\sqrt{a_1a_2}\\ge a_1+a_2 $$ hence $$\\sqrt{a_1+a_2}\\le \\sqrt{a_1}+\\sqrt{a_2}$$\nFor general $n$, by induction: \n$$\\sqrt{(a_1+\\dots+a_{n-1})+a_n}\\le \\sqrt{a_1+\\dots+a_{n-1}}+\\sqrt{a_n} \\le \\sqrt{a_1}+\\dots+\\sqrt{a_n}$$\n\nMore generally, the function $f(x)=  x^p$ is subadditive for $0<p<1$, meaning $f(a+b)\\le f(a)+f(b)$. A fun way to prove this is \n$$\nf(a+b)-f(b)=\\int_b^{a+b} f'(x)\\,dx = \\int_0^{a} f'(x+b)\\,dx\\le \\int_0^{a} f'(x)\\,dx = f(a)\n$$ \nwhere the inequality holds because $f'$ is decreasing.", "meta": {"post_id": 318649, "input_score": 28, "output_score": 51, "post_title": "Upper bound for $( a_1 + a_2 + \\cdots + a_n)^{1/2}$"}}
{"input": "I've been taught that $1^\\infty$ is undetermined case. Why is it so? Isn't $1*1*1...=1$ whatever times you would multiply it? So if you take a limit, say $\\lim_{n\\to\\infty} 1^n$, doesn't it converge to 1? So why would the limit not exist?", "output": "It isn\u2019t: $\\lim_{n\\to\\infty}1^n=1$, exactly as you suggest. However, if $f$ and $g$ are functions such that $\\lim_{n\\to\\infty}f(n)=1$ and $\\lim_{n\\to\\infty}g(n)=\\infty$, it is not necessarily true that\n$$\\lim_{n\\to\\infty}f(n)^{g(n)}=1\\;.\\tag{1}$$\nFor example, $$\\lim_{n\\to\\infty}\\left(1+\\frac1n\\right)^n=e\\approx2.718281828459045\\;.$$\nMore generally,\n$$\\lim_{n\\to\\infty}\\left(1+\\frac1n\\right)^{an}=e^a\\;,$$\nand as $a$ ranges over all real numbers, $e^a$ ranges over all positive real numbers. Finally,\n$$\\lim_{n\\to\\infty}\\left(1+\\frac1n\\right)^{n^2}=\\infty\\;,$$\nand\n$$\\lim_{n\\to\\infty}\\left(1+\\frac1n\\right)^{\\sqrt n}=0\\;,$$\nso a limit of the form $(1)$ always has to be evaluated on its own merits; the limits of $f$ and $g$ don\u2019t by themselves determine its value.", "meta": {"post_id": 319764, "input_score": 32, "output_score": 42, "post_title": "1 to the power of infinity, why is it indeterminate?"}}
{"input": "Why isn't an infinite direct product of copies of $\\Bbb Z$ a free module?\n\nActually I was asked to show that it's not projective, but as $\\Bbb{Z}$ is a PID, so it suffices to show it's not free. \nI am stuck here. I saw some questions in SE, but there is no satisfactory answer at all.", "output": "This failure of freeness is a non-trivial result.  One way to prove it is to begin with a lemma: If $F$ is a free abelian group and $C$ is a countable subgroup, then the quotient $F/C$  is the direct sum of a countable group and a free group.  (I'm omitting \"abelian\" because I'm lazy and all groups here will be abelian.)  [Proof of lemma: Fix a basis $B$ for $F$, let $B_0$ be the countable subset consisting of the basis elements that occur when you expand elements of $C$ in terms of your basis $B$.  Then $F$ is the direct sum of $F_0$ freely generated by $B_0$ and $F_1$ freely generated by $B-B_0$.  As $C\\subseteq F_0$, it follows that $F/C$ is the direct sum of the countable group $F_0/C$ and the free group $F_1$.]  \nAs a corollary, under the hypotheses of the lemma, any divisible subgroup of $F/C$ must be included in the countable summand and must therefore be countable.\nNow suppose the direct product $P$ of countably infinitely many copies of $\\mathbb Z$ were free.  The elements of $P$ are the all of the infinite sequences of integers.  Let $C$ be the subgroup of $P$ consisting of those sequences that have non-zero entries in only finitely many positions.  Then $C$ is countable, so the divisible part of $P/C$ would have to be countable.  But this divisible part contains the cosets (in $P/C$) of all the sequences (in $P$) of the form $n\\mapsto n!\\cdot a_n$ for arbitrary sequences of integers $(a_n)$.  So the divisible part of $P/C$ has the cardinality of the continuum. This contradiction shows that $P$ is not free.\nIf your question was not only about $P$ but also about products of uncountably many copies of $\\mathbb Z$, notice that such a product contains a copy of $P$, so you're done if you know that subgroups of free (abelian) groups are free.  If you don't know that, just re-run the argument in the preceding paragraph within a copy of $P$ inside your bigger product.\nBy the way, a theorem of Specker shows that $P$ is not only not free but very far from free.  Since $P$ has cardinality $2^{\\aleph_0}$, if it were free any basis for it would also have cardinality $2^{\\aleph_0}$, so there would be $2^{2^{\\aleph_0}}$ homomorphisms from $P$ to $\\mathbb Z$ (because you could choose the images of the $2^{\\aleph_0}$ basis elements arbitrarily).  Specker showed that there are only countably many homomorphisms $P\\to\\mathbb Z$, namely the $\\mathbb Z$-linear combinations of the projections.", "meta": {"post_id": 320444, "input_score": 45, "output_score": 43, "post_title": "Why isn't an infinite direct product of copies of $\\Bbb Z$ a free module?"}}
{"input": "I'm trying to find an example of a continuously differentiable function from $\\mathbb{R}$ to $\\mathbb{R}$ that is injective but not surjective. \nI can easily find one from $\\mathbb{Z}$ to $\\mathbb{Z}$ but I'm having difficulty finding one for the reals.", "output": "Hint: It might be easier than expected.", "meta": {"post_id": 320760, "input_score": 8, "output_score": 34, "post_title": "Injective function that is not surjective"}}
{"input": "One of fundamental inequalities on logarithm is:\n$$ 1 - \\frac1x \\leq \\log x \\leq x-1 \\quad\\text{for all $x > 0$},$$\nwhich you may prefer write in the form of\n$$ \\frac{x}{1+x} \\leq \\log{(1+x)} \\leq x \\quad\\text{for all $x > -1$}.$$\nThe upper bound is very intuitive -- it's easy to derive from Taylor series as follows:\n$$ \\log(1+x) = \\sum_{n=1}^\\infty (-1)^{n+1}\\frac{x^n}{n} \\leq (-1)^{1+1}\\frac{x^1}{1} = x.$$\nMy question is: \"what is the intuition behind the lower bound?\" I know how to prove the lower bound of $\\log (1+x)$ (maybe by checking the derivative of the function $f(x) = \\frac{x}{1+x}-\\log(1+x)$ and showing it's decreasing) but I'm curious how one can obtain this kind of lower bound. My ultimate goal is to come up with a new lower bound on some logarithm-related function, and I'd like to apply the intuition behind the standard logarithm lower-bound to my setting.", "output": "If you don't already know that $\\log(x)=\\int_1^x\\frac1t\\,dt$, one way to define the logarithm function is: $$\\text{$\\log(x)$ is the area under the curve $y=\\frac1t$ from $t=1$ to $t=x$.}$$ The picture below shows that this area is sandwiched between two rectangles, each of width $x-1$. The smaller rectangle has height $1/x$, while the larger one has height $1$. In other words, we have the following inequalities:\n$$(x-1)\\cdot \\frac1x\\le\\log(x)\\le(x-1)\\cdot 1$$", "meta": {"post_id": 324345, "input_score": 68, "output_score": 44, "post_title": "Intuition behind logarithm inequality: $1 - \\frac1x \\leq \\log x \\leq x-1$"}}
{"input": "I want to show that every an infinite-dimensional separable (contains countable dense set) Hilbert space has a countable orthonormal basis.\nI know that every orthogonal set in a separable Hilbert space is countable, it is help me with the proof?", "output": "This is actually an if and only if statement. For future students looking for a full answer to this question, I am posting a full proof below:\nLet $H$ be an infinite-dimensional Hilbert space. Show that $H$ has a countable orthonormal basis if and only if $H$ has a countable dense subset.\nFirst let us assume that $H$ has a countable orthonormal basis $\\{e_i\\}$. Then any $x$ can be uniquely written as\n$$x=\\sum\\limits_{i=1} c_ie_i \\quad \\text{where} \\quad c_i= \\langle x,e_i \\rangle$$\nRecall that $S:=\\mathbb{Q}+\\mathbb{Q}i$ is a countable dense subset of $\\mathbb{C}$. Now for every $n \\in \\mathbb{N}$, consider the following subset of $H$:\n$$A_n=\\left\\{\\sum\\limits_{i=1}^n s_ie_i \\quad \\text{where} \\quad s_i \\in S \\, \\, \\forall i\\right\\}.$$\nBeing a finite union of countable sets, each $A_n$ is countable. Then define\n$$A:= \\bigcup_{n=1}^\\infty A_n$$\nBeing a countable union of countable sets, $A$ is countable. Let us show that $A$ is a dense subset of $H$; its being countable will imply separability of $H$.\nLet $x\\in H$. Then\n$$x=\\sum\\limits_{i=1}^\\infty c_ie_i \\quad \\text{where} \\quad c_i=\\langle x,e_i\\rangle \\in \\mathbb{C}.$$\nSince this infinite sum is convergent in the norm of $H$, fixing an arbitrary $\\epsilon >0$, we can find a big enough $N$ for which\n$$\\left\\| \\sum\\limits_{i=N+1}^\\infty c_ie_i\\right\\|< \\frac{\\epsilon}{2}.$$\nAlso, Since $S$ is a dense subset of $\\mathbb{C}$, for every $i \\leq N$, we can find $s_i\\in S$ such that\n$$|c_i-s_i|<\\frac{\\epsilon}{2^{i+1}}.$$\nNow consider the following element\n$$x_N=\\sum\\limits_{i=1}^N s_ie_i \\in A.$$\nWe know that $\\sum\\limits_{i=N+1}^\\infty c_ie_i$ is the difference of two elements of $H$ and thus is in $H$ and therefore by using triangle inequality and Perseval's inequality, we have\n\\begin{equation}\n\\begin{split}\n\\|x-x_N\\|\n& =\\left\\|\\sum\\limits_{i=1} c_ie_i-\\sum\\limits_{i=1}^N s_ie_i\\right\\|\\\\\n& =\\left\\|\\sum\\limits_{i=1}^N (c_i-s_i)e_i+\\sum\\limits_{i=N+1} c_ie_i\\right\\|\\\\\n& \\leq \\left\\|\\sum\\limits_{i=1}^N (c_i-s_i)e_i\\right\\|+\\left\\|  \\sum\\limits_{i=N+1} c_ie_i\\right\\|\\\\\n& \\leq \\sum\\limits_{i=1}^N \\left|(c_i-s_i)\\right|+\\frac{\\epsilon}{2}\\\\\n& < \\sum\\limits_{i=1}^N \\frac{\\epsilon}{2^{i+1}}+\\frac{\\epsilon}{2}\\\\\n& \\leq \\sum\\limits_{i=1}\\frac{\\epsilon}{2^{i+1}}+\\frac{\\epsilon}{2}\\\\\n& = \\epsilon\n.\n\\end{split}\n\\end{equation}\nTherefore, $x_n$ is an element of the set $A$ that is in the ball of radius $\\epsilon$ around $x$. Since both $x$ and $\\epsilon$ were arbitrary, this implies that $A$ is dense in $H$.\nConversely, assume that $H$ has a countable dense subset $\\{a_j\\}$, where $j \\in \\mathbb{N}$. Let $\\{e_i\\}_{i \\in I}$ be an orthonormal basis of $H$ (we know that such a basis exists by Zorn's lemma).\nProceeding by contradiction, let us assume that the orthonormal basis is uncountable. This implies that for any $e_n \\neq e_m$, $n,m \\in I$, by orthogonality we have\n\\begin{equation}\n\\begin{split}\n\\|e_n-e_m\\|^2\n& =\\langle e_n-e_m, e_n-e_m \\rangle\n\\\\\n& =\\langle e_n, e_n \\rangle+\\langle e_m, e_m \\rangle-2\\text{Re}\\big(\\langle e_n, e_m \\rangle\\big)\\\\\n& =\\|e_n\\|+\\|e_m\\|\\\\\n& =2\\\\\n\\end{split}\n\\end{equation}\nTherefore, any two elements of our orthonormal basis are $\\sqrt{2}$ apart. Now for all $i \\in I$, consider the following balls:\n$$B\\left(e_i, \\frac{1}{2}\\right)$$\nEach of such balls has diameter less than $1$. Thus, each one of them can contain at most one element of the basis, namely only the center itself. Also, these ball are disjoint, since if there exists an element in two balls, then by the triangle inequality the distance between the centers of the balls is less than $1$, which is a contradiction. Since $\\{a_j\\}$ is a dense subset, it has to have at least one element in each such ball. Since by the above remarks the balls are disjoint, we have a surjective function from some $\\{a_j\\}$ to the balls. However, our balls are indexed by an uncountable set. Thus, there has to be at least an uncountable amount of $a_j$, a contradiction. Thus, any orthonormal basis has to be countable.", "meta": {"post_id": 324538, "input_score": 48, "output_score": 79, "post_title": "Separable Hilbert space have a countable orthonormal basis"}}
{"input": "If we are in a sequence space, then the $ l^{p} $-norm of the sequence $ \\mathbf{x} = (x_{i})_{i \\in \\mathbb{N}} $ is $ \\displaystyle \\left( \\sum_{i=1}^{\\infty} |x_{i}|^{p} \\right)^{1/p} $.\nThe $ l^{\\infty} $-norm of $ \\mathbf{x} $ is $ \\displaystyle \\sup_{i \\in \\mathbb{N}} |x_{i}| $.\nProve that the limit of the $ l^{p} $-norms is the $ l^{\\infty} $-norm.\nI saw an answer for $ L^{p} $-spaces, but I need one for $ l^{p} $-spaces. Besides, I didn\u2019t really understand the $ L^{p} $-answer either.\nThanks for your help!", "output": "Let me state the result properly:\n\nLet $x=(x_n)_{n \\in \\mathbb{N}} \\in \\ell^q$ for some $q \\geq 1$. Then $$\\|x\\|_{\\infty} = \\lim_{p \\to \\infty} \\|x\\|_p. \\tag{1}$$\n\nNote that $(1)$ fails, in general, not hold if $x=(x_n)_{n \\in \\mathbb{N}} \\notin \\ell^q$ for all $q \\geq 1$ (consider for instance $x_n := 1$ for all $n \\in \\mathbb{N}$.)\nProof of the result: Since $$|x_k| \\leq \\left(\\sum_{j=1}^{\\infty} |x_j|^p \\right)^{\\frac{1}{p}}=\\|x\\|_p$$ for all $k \\in \\mathbb{N}$, $p \\geq 1$, we have $\\|x\\|_{\\infty} \\leq \\|x\\|_p$. Thus, in particular $$\\|x\\|_{\\infty} \\leq \\liminf_{p \\to \\infty} \\|x\\|_p. \\tag{1}$$\nOn the other hand, we know that $$\\|x\\|_p = \\left( \\sum_{j=1}^{\\infty} |x_j|^{p-q} \\cdot |x_j|^q \\right)^{\\frac{1}{p}} \\leq \\|x\\|_{\\infty}^{\\frac{p-q}{p}} \\cdot \\left( \\sum_{j=1}^{\\infty} |x_j|^q \\right)^{\\frac{1}{p}} = \\|x\\|_{\\infty}^{1-\\frac{q}{p}} \\cdot \\|x\\|_q^{\\frac{q}{p}}$$ for all $q<p$ where we used $|x_j| \\leq \\|x\\|_{\\infty}$ for all $j \\in \\mathbb{N}$. Therefore, we arrive at\n$$ \\limsup_{p \\to \\infty} \\|x\\|_p \\leq \\limsup_{p \\to \\infty} \\left( \\|x\\|_{\\infty}^{1-\\frac{q}{p}} \\cdot \\|x\\|_q^{\\frac{q}{p}}\\right) = \\|x\\|_{\\infty} \\cdot 1. \\tag{2}$$\nHence, $$\\limsup_{p \\to \\infty} \\|x\\|_p \\leq \\|x\\|_{\\infty} \\leq \\liminf_{p \\to \\infty} \\|x\\|_p.$$ This shows that $\\lim_{p \\to \\infty} \\|x\\|_p$ exists and equals $\\|x\\|_{\\infty}$.", "meta": {"post_id": 326172, "input_score": 38, "output_score": 62, "post_title": "The $ l^{\\infty} $-norm is equal to the limit of the $ l^{p} $-norms."}}
{"input": "Please scroll down to the bold subheaded section called Exact questions if you are too bored to read through the whole thing. \nI am a physics undergrad, trying to carry out some research work on topological solitons. I have been trying to read a paper that uses K\u00e4hler Manifolds. My guide just expects me to learn the mathematical definitions, without understanding it(or he expects me to study complex manifolds by scratch by myself), within 3-4 days, with exams going on, but I find this highly discomforting. So, it would be great if someone could tell me what is a K\u00e4hler manifold, highlighting the essential features of the definition and what they do, and intuitive explanations behind each. Why are they mathematically important? Also, some reasons as to why they are used in Physics?\nThe definition on Wikipedia is very obscure, linking you to 7-8 pages, and you forget what you are actually looking for. I have read the following definition from Nakahara: \n\nA K\u00e4hler Manifold is an hermitian manifold, whose K\u00e4lher form is closed i.e. $d\\Omega=0$. \n\nAfter searching the internet, I know the following: \n\nA Hermitian manifold is a complex manifold equipped with a metric $g$,\n  such that $g_p(X,Y)=g_p(J_pX,J_p Y)$, where $p \\in M$ and $X,Y \\in\n T_pM$\n\nAgain the web tells me that, $J$ is a linear map between the tangent spaces at a point such that $J^2=-1$. Lastly, the K\u00e4hler form $\\Omega$ is a tensor field whose action is given by $\\Omega_p(X,Y)=(J_pX,Y)$. \nExact questions: \nThis is what I would really really want to understand. What is the meaning and motivation for $J^2=-1$? What is the intuitive meaning and motivation for the definition of the Hermitian manifold, and the K\u00e4hler form? Most importantly, what does the K\u00e4hler form is closed really mean?\nI am sorry for the long question, and would be delighted, even if I got a partial answer. Looking forward for the replies. \nI am not looking for exact arguments, but an intuitive overall picture. \nBackground: I understand definitions of real manifolds, tangent spaces, and a differential forms. I have no intuition about exterior derivatives. I have a fair understanding of what is a complex manifold, and a few examples of Riemann surfaces.", "output": "First, as I think you know, K\u00e4hler manifolds are just special cases of Hermitian manifolds (complex manifolds with a Hermitian metric).  There are a number of very different, but equivalent, ways to define K\u00e4hler manifolds.  Here are a couple that might help:\n\nOn a Riemannian manifold $X$, we can always choose 'Riemann normal coordinates' at any point $p \\in X$.  These are coordinates in which the metric takes its canonical form $g_{ab} = \\delta_{ab}$ at $p$, and all its first derivatives vanish at $p$.  On a general Hermitian manifold, it may not be possible to find holomorphic coordinates in which this is true. \nK\u00e4hler manifolds are exactly those manifolds on which we can always find a holomorphic change of coordinates which, at some given point, sets the metric to its canonical form, and its first derivatives to zero.\nAnother characterisation of K\u00e4hler manifolds is as Hermitian manifolds for which the Christoffel symbols of the Levi-Civita connection are pure.  In other words, $\\Gamma^i_{jk}$ and $\\Gamma^{\\bar i}_{\\bar j\\bar k}$ may be non-zero, but all 'mixed' symbols like $\\Gamma^{\\bar i}_{jk}$ vanish.  This means that (anti-)holomorphic vectors get parallel transported to (anti-)holomorphic vectors.\nEquivalent to the above is to say that $n$-dimensional K\u00e4hler manifolds are precisely $2n$-dimensional Riemannian manifolds with holonomy group contained in $U(n)$.\n\nAll of this can be found in Moroianu's \"Lectures on K\u00e4hler Geometry\", which is quite a nice concise book.\nThere is a lot more to be said about K\u00e4hler manifolds, but hopefully this gives you some intuition about them.", "meta": {"post_id": 329342, "input_score": 50, "output_score": 45, "post_title": "What exactly is a K\u00e4hler Manifold?"}}
{"input": "Given a random vector $\\mathbf x \\sim N(\\mathbf{\\bar x}, \\mathbf{C_x})$ with normal distribution. $\\mathbf{\\bar x}$ is the mean value vector and $\\mathbf{C_x}$ is the covariance matrix of $\\mathbf{x}$.\nAn affine transformation is applied to the $\\mathbf{x}$ vector to create a new random $\\mathbf{y}$ vector:\n$$\n\\mathbf{y} = \\mathbf{Ax} + \\mathbf{b}\n$$\nCan we find mean value $\\mathbf{\\bar y}$ and covariance matrix $\\mathbf{C_y}$ of this new vector $\\mathbf{y}$ in terms of already given parameters ($\\mathbf{\\bar x}$, $\\mathbf{C_x}$, $\\mathbf{A}$ and $\\mathbf{b}$)?\nCan you please show the steps. Once I learn the method, I will use it on several other distributions myself.", "output": "We find the mean of $\\mathbf{y}$ by using the fact that $\\mathbb{E}\\{\\}$ is a linear operator.\n$$\n\\mathbf{\\bar{y}} = \\mathbb{E}\\{\\mathbf{y}\\} = \\mathbb{E}\\{\\mathbf{A}\\mathbf{x}+\\mathbf{b}\\} = \\mathbf{A}\\mathbb{E}\\{\\mathbf{x}\\}+\\mathbf{b} = \\mathbf{A}\\mathbf{\\bar{x}}+\\mathbf{b}\n$$\nThen we find covariance of\n$$\n\\begin{array}{rcl}\n\\mathbf{C_y} & \\triangleq & \\mathbb{E}\\{(\\mathbf{y}-\\mathbf{\\bar{y}})(\\mathbf{y}-\\mathbf{\\bar{y}})^\\top\\} \\\\\n& = & \\mathbb{E} \\Big\\{ \\Big[ (\\mathbf{A}\\mathbf{x}+\\mathbf{b})-(\\mathbf{A}\\mathbf{\\bar{x}}+\\mathbf{b}) \\Big] \\Big[ (\\mathbf{A}\\mathbf{x}+\\mathbf{b})-(\\mathbf{A}\\mathbf{\\bar{x}}+\\mathbf{b}) \\Big] ^\\top \\Big\\} \\\\\n& = & \\mathbb{E} \\Big\\{ \\Big[ \\mathbf{A}(\\mathbf{x}-\\mathbf{\\bar{x}}) \\Big] \\Big[ \\mathbf{A}(\\mathbf{x}-\\mathbf{\\bar{x}}) \\Big] ^\\top \\Big\\} \\\\\n& = & \\mathbb{E} \\Big\\{ \\mathbf{A}(\\mathbf{x}-\\mathbf{\\bar{x}}) (\\mathbf{x}-\\mathbf{\\bar{x}})^\\top \\mathbf{A}^\\top \\Big\\} \\\\\n& = & \\mathbf{A} \\mathbb{E} \\Big\\{ (\\mathbf{x}-\\mathbf{\\bar{x}}) (\\mathbf{x}-\\mathbf{\\bar{x}})^\\top  \\Big\\} \\mathbf{A}^\\top \\\\\n& = & \\mathbf{A}\\mathbf{C_x}\\mathbf{A}^\\top\n\\end{array}\n$$\nThen, $\\mathbf{y}$ is defined as,\n$$\n\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{A}\\mathbf{\\bar{x}+\\mathbf{b}, \\mathbf{A}\\mathbf{C_x}\\mathbf{A}^\\top})\n$$\nThat is,\n$$\nf_\\mathbf{Y}(\\mathbf{y)}\n= {1 \\over \\sqrt{\\lvert2\\pi\\mathbf{A}\\mathbf{C_x}\\mathbf{A}^\\top\\rvert}}\n\\exp\\left(- {1 \\over 2} \\big[\\mathbf{y}-(\\mathbf{A}\\mathbf{\\bar{x}}+\\mathbf{b}) \\big]^\\top (\\mathbf{A}\\mathbf{C_x}\\mathbf{A}^\\top)^{-1} \\big[\\mathbf{y}-(\\mathbf{A}\\mathbf{\\bar{x}}+\\mathbf{b}) \\big] \\right)\n$$", "meta": {"post_id": 332441, "input_score": 49, "output_score": 60, "post_title": "Affine transformation applied to a multivariate Gaussian random variable - what is the mean vector and covariance matrix of the new variable?"}}
{"input": "I am reading (as a supplement) the book Basic Real Analysis, by Anthony Knapp. Before I proceed into reading a proof, I want to be sure that the result seems obvious. Yet, I am having trouble seeing through this one. It annoys me too much in order to disregard it:\nTheorem.\nA continuous function $f$ from a closed bounded interval $[a, b]$ into $\\mathbb{R}$ is uniformly continuous.\nWhat gives? Why can't we provide the counterexample $f(x)=x^2$ and $[a,b] \\subset [1,+\\infty)$, for $b < +\\infty$ sufficiently large and show that the theorem is incorrect? \nDoesn't it seem to be an insufficient statement? \nI'm having trouble picturing it, is all.\nHints are fine.", "output": "I can provide you with a proof. We use the lemma that $[a,b]$ is compact. The general statement is that if $f:X\\to Y$ is continuous and $X$ is a compact metric space, then i$f$ is uniformly continuous. This is usually known as the Heine Cantor theorem, while the fact that $[a,b]$is compact might be found as Borel's Lemma, if memory serves. So\nTHEOREM (Spivak) Let $f:[a,b]\\to \\Bbb R$ be continuous. Then it is uniformly continuous.\nWe first prove the \nLEMMA Let $f$ be a continuous function defined on $[a,c]$. If, given $\\epsilon >0$, there exists $\\delta_1>0$ such that, for each pair $$x,y\\in[a,b]\\text{ ; } |x-y|<\\delta_1 \\implies |f(x)-f(y)|<\\epsilon$$  and $\\delta_2>0$ such that for each\n$$x,y\\in[b,c]\\text{ ; } |x-y|<\\delta_2 \\implies |f(x)-f(y)|<\\epsilon$$\nThen there exists $\\delta $ such that for each \n$$x,y\\in[a,c]\\text{ ; } |x-y|<\\delta  \\implies |f(x)-f(y)|<\\epsilon$$\nP\nSince $f$ is continuous at $x=b$, there exists a $\\delta_3$ such that for every $x$ with $|b-x|<\\delta_3$, we have $|f(b)-f(x)|<\\frac{\\epsilon}2$.\nThus, whenever $|x-b|<\\delta_3$ and $|y-b|<\\delta_3$ we will certainly have $$|f(x)-f(y)|<\\epsilon$$ \nWe take $\\delta=\\min\\{\\delta_1,\\delta_2,\\delta_3\\}$. Then $\\delta$ works: indeed, consider any pair $x,y\\in[a,c]$. If $x,y\\in[a,b]$ or $x,y\\in[b,c]$, we're done. If $x<b<y$ or $y<b<x$. In any case, since $|x-y|<\\delta$, we must have $|x-b|,|y-b|<\\delta$, so that $|f(x)-f(y)|<\\epsilon$, as claimed.\nPROOF1 Fix $\\epsilon >0$. Let's agree to call $f$ $\\epsilon$-good on an interval $[a,b]$ if for this $\\epsilon$ there exists a $\\delta$ such that for any $x,y\\in[a,b]$, $|x-y|<\\delta\\implies |f(x)-f(y)|<\\epsilon$. We thus want to prove that $f$ is $\\epsilon$-good on $[a,b]$ for any $\\epsilon >0$. Let $\\epsilon >0$ be given, and consider the set $$A(\\epsilon)=\\{x\\in[a,b]:f \\text{ is } \\epsilon \\text{-good on}: [a,x]\\}$$ Then $A\\neq \\varnothing$ for $a\\in A(\\epsilon)$, and $A(\\epsilon)$ is bounded above by $b$. Thus $\\sup A=\\alpha $ exists. Suppose that $\\alpha <b$. Since $f$ is continuous at $\\alpha$ there exists a $\\delta'$ such that $|y-\\alpha|<\\delta'$ implies $|f(y)-f(\\alpha)|<\\epsilon/2$. Thus, if $|y-\\alpha|,|x-\\alpha|<\\delta'$, we'll have  $|f(y)-f(x)|<\\epsilon$. Thus $f$ is $\\epsilon$-good on $[\\alpha-\\delta,\\alpha+\\delta]$. Since $\\alpha=\\sup A(\\epsilon)$, it is clear $f$ is $\\epsilon$-good on $[a,\\alpha+\\delta]$, which is absurd. Thus $\\alpha\\geq b$, which means $\\alpha =b$. It suffices to show that $b$ is also an element of $A(\\epsilon)$. But since $f$ is continuous on $b$, there exists a $\\delta_0$ such that $|b-y|<\\delta_0$ implies $|f(b)-f(y)|<\\epsilon/2$. Thus, $f$ is $\\epsilon$-good on $[b-\\delta_0,b]$. The lemma implies $f$ is $\\epsilon$-good on $[a,b]$. Since $\\epsilon$ was arbitrary, the result follows. $\\blacktriangle$ \n\nPROOF2 Let $\\epsilon >0$ be given. Assign, to each $x\\in [a,b]$ a $\\delta_x>0$ such that for each $y\\in(x-2\\delta_x,x+2\\delta_x)$, we have $|f(x)-f(y)|< \\epsilon/2$, to obtain a open cover of $[a,b]$, namely the set $\\mathcal O$ of intervals $(x-\\delta_x,x+\\delta_x)$. This is possible since $f$ is continuous at each $x$. Since $[a,b]$ is compact, there is a finite number of $x_i\\in [a,b]$ such that $$\\bigcup_{i=1}^n (x_i-\\delta_{x_i},x_i+\\delta_{x_i})\\supset [a,b]$$\nChoose now $\\delta =\\min{\\delta_{x_i}}$, and let $x,y\\in [a,b]$ with $ |y-x|<\\delta$. Since $\\mathcal O$ is a cover, for some $x_i$ we have that $|x-x_i|<\\delta_{x_i}$. Then, we'll have $$|y-x_i|\\leq |y-x|+|x-x_i|<\\delta+\\delta_i\\leq 2\\delta_i$$ It follows that $$|f(x_i)-f(x)|<\\epsilon/2$$\n$$|f(y)-f(x)|<\\epsilon/2$$\nwhich means by the triangle inequality that $$|f(x)-f(y)|<\\epsilon$$\nThen for any $x,y\\in[a,b]$,  $|x-y|<\\delta$ will imply $|f(x)-f(y)|<\\epsilon$; and $f$ is uniformly continuous. $\\blacktriangle$ \n\nThere is yet another way of proving this. \nLEMMA (Mendelson) Let $X$ be a metric space such that every infinite subset of $X$ has an accumulation point in $X$. Then for each covering $\\mathcal O=\\{O_\\beta\\}_{\\beta\\in I}$ there exists a positive $\\epsilon$ such that each ball $B(x;\\epsilon)$ is contained in an element $O_\\beta$ of this covering. \nPROOF  If it wasn't the case, we'd obtain for each $n$ an point $x_n$ and an open ball $B(x_n;1/n)\\not\\subseteq O_\\beta$ for each $\\beta \\in I$. Let $A=\\{x_1,\\dots\\}$. If $A$ is finite, $x_n=x$ infinitely often for some $x\\in X$. Since $\\mathcal O$ is a cover, $x\\in O_\\alpha$ for some $\\alpha$. Since the cover is open, there is a $\\delta >0$ for which $B(x;\\delta)\\subseteq O_\\alpha$. We can take $n$ such that $1/n<\\delta$ and $x_n=x$, in whichcase we get a contradiction $$B\\left(x;\\frac 1n \\right)\\subseteq B\\left(x;\\delta\\right)\\subseteq O_\\alpha$$ If $A$ is infinite, there is an accumulation point $x\\in X$. Thus $x\\in O_\\beta$ for some index, and there are infinitely many points of $A$ in $B(x:\\delta /2)\\subseteq O_\\beta$. We can take $n$ such that $1/n<\\delta /2$ and we'd have $B(x_n;1/n)\\subseteq B(x;\\delta)\\subseteq O_\\beta$, a contradiction. \nAfter having proven that for metric spaces, the existence of accumulation points for infinite subsets is equivalent to compactness. \nPROOF3 Let $f:X\\to Y$ be a continuous function from a compactum $X$ to a metric space $Y$. Then $f$ is uniformly continuous.\nPROOF Given $\\epsilon >0$, for each $x\\in X$ there is a $\\delta_x>0$ such that if $y\\in B(x:\\delta_x)$, $f(y)\\in B\\left(f(x);\\epsilon /2\\right)$. These balls are an open cover for $X$, thus there exists such a number $\\delta_L$ as in the previous lemma (usually called a Lebesgue number). Choose $\\delta$ to be positive yet smaller than $\\delta_L$. If $z,z'\\in X$ and $d(z,z')<\\delta$ (so that $z,z'$ are in a ball of radius less than $\\delta$), we have $z,z'\\in B(x,\\delta_x)$ for some $x\\in X$. In that case $f(z),f(z')\\in B(f(x),\\epsilon/2)$ so $d'(f(z),f(z'))<\\epsilon$ by the triangle inequality. $\\blacktriangle$.", "meta": {"post_id": 333125, "input_score": 25, "output_score": 50, "post_title": "A continuous function $f$ from a closed bounded interval $[a, b]$ into $\\mathbb{R}$ is uniformly continuous"}}
{"input": "how to show image of a non constant entire function is dense in $\\mathbb{C}$? is there any smallest proof? I have seen this as a theorem in some books but I want some elementary proof.", "output": "It can be done with the help of the $\\color{blue}{Casorati-Weierstrass\\ theorem}$ or by the $\\color{blue} {Liouville's\\ theorem, }$\nWith the help of $\\color{blue}{Casorati-Weierstrass\\ theorem}$\nSuppose $f$ is an entire function whose image is not dense in $\\mathbb{C}$. Then there exists a complex number $\\alpha$ and a number $s> 0$  such that |$f(z)-\\alpha|>s$ $\\forall  \nz \\in \\mathbb{C}$.\nWrite $f(z) =\\sum_{n=0}^{\\infty} a_nz^n$ and suppose that there are infinitely many nonzero \nterms in this expansion. Then, for all $z \\not=0$, let $g(z)=f(1/z)$ .We see that g has \nan essential singularity at $0$ so by the Casorati-Weierstrass theorem for some $z\\ \nnear\\ 0$ we have $|g(z)-\\alpha|<s $ then$|f(1/z)-\\alpha|<s$ This contradiction implies \nthat the power series expansion of $f$ can have only finitely many terms. Then the \nfundamental theorem of algebra guarantees that $f\\ is\\ constant$. This contradicts the \nhypothesis. \nWith the help of $\\color{blue}{Liouville's\\ theorem, }$\nSuppose there exists a complex number $\\alpha$ and $s\\in \\mathbb{R^+}$ such \nthat $|f(z) \u2014 \\alpha|>s, \\forall z \\in \\mathbb{C}$. Then the function $g(z) = 1/(f(z) \u2014 \\alpha)$ is \nentire and bounded, so by Liouville's theorem, g is constant. Hence $f$ is constant, \nagain contradicting the hypothesis.", "meta": {"post_id": 337154, "input_score": 26, "output_score": 40, "post_title": "how to show image of a non constant entire function is dense in $\\mathbb{C}$?"}}
{"input": "I read that Euler proved $2^{31} -1$ is prime. What techniques did he use to prove this so early on in history? Isn't very large number stuff done with computers? Do you know if Euler had a team of people to follow algorithms for him, dubbed \"computers\"?", "output": "Euler proved $\\rm\\: M_{31}= 2^{31}\\!-1\\:$ is prime by showing that all prime divisors are $\\rm\\equiv 1$ or $\\rm\\, 63\\:\\ (mod\\ 248),\\:$ then test dividing by all primes of this form below $\\rm\\sqrt{M_{31}}.\\:$ The constraint on the prime divisors is an immediate consequence of the (now) well-known Mersenne factor theorem: for odd primes $\\rm\\:p,q,\\:$ $\\rm\\: p\\mid M_q\\:\\Rightarrow\\: p\\equiv 1\\,\\ (mod\\ q),\\,\\ p\\equiv \\pm 1\\,\\ (mod\\ 8).\\:$ \nBelow is an excerpt from his 1772 letter to Bernoulli describing this.  See also this page.", "meta": {"post_id": 337973, "input_score": 74, "output_score": 38, "post_title": "How did Euler prove the Mersenne number $2^{31}-1$ is a prime so early in history?"}}
{"input": "I don't understand questions that involve a binomial expression where you have a fraction choose $k$ or a negative number choose $k$. I understand and am able to do it when there are no fractions and they are all positive. We learned the generalized formula but I get the wrong answer when the question involves fractions or negative numbers. \neg: $$2/3 \\choose 2$$\nor $$-4 \\choose 3$$\nI am definitely not understanding something here, please help.", "output": "You know that $$\\binom{x}k=\\frac{x^{\\underline k}}{k!}\\;,$$ where $x^{\\underline k}$ is the falling factorial: $x^{\\underline k}=x(x-1)(x-2)\\dots(x-k+1)$. Thus,\n$$\\binom{2/3}2=\\frac{(2/3)^{\\underline 2}}{2!}=\\frac{\\left(\\frac23\\right)\\left(\\frac23-1\\right)}2=\\frac{\\left(\\frac23\\right)\\left(-\\frac13\\right)}2=-\\frac19\\;,$$\nand\n$$\\binom{-4}3=\\frac{(-4)^{\\underline 3}}{3!}=\\frac{(-4)(-4-1)(-4-2)}6=-\\frac{4\\cdot5\\cdot6}6=-20\\;.$$\nWith specific small numbers you can always just do the arithmetic, as I\u2019ve done here. Some more general calculations are also possible without too much difficulty. For instance:\n$$\\begin{align*}\n\\binom{1/2}n&=\\frac{(1/2)^{\\underline n}}{n!}\\\\\n&=\\frac{\\left(\\frac12\\right)\\left(-\\frac12\\right)\\left(-\\frac32\\right)\\dots\\left(-\\frac{2n-3}2\\right)}{n!}\\\\\n&=(-1)^{n-1}\\frac{(2n-3)!!}{2^nn!}\\\\\n&=(-1)^{n-1}\\frac{2^{n-1}(n-1)!(2n-3)!!}{2^{2n-1}n!(n-1)!}\\\\\n&=(-1)^{n-1}\\frac{(2n-2)!!(2n-3)!!}{2^{2n-1}n!(n-1)!}\\\\\n&=\\frac{(-1)^{n-1}}{2^{2n-1}n}\\frac{(2n-2)!}{(n-1)!^2}\\\\\n&=\\frac{(-1)^{n-1}}{2^{2n-1}n}\\binom{2n-2}{n-1}\n\\end{align*}$$", "meta": {"post_id": 340124, "input_score": 17, "output_score": 34, "post_title": "Binomial coefficients $1/2\\choose k$"}}
{"input": "What's the reason we agreed to setting the number of degrees of a full circle to 360? Does that make any more sense than 100, 1000 or any other number? Is there any logic involved in that particular number?", "output": "As it has been replied here - on Wonder Quest (webarchive link):\n\nThe Sumerians watched the Sun, Moon, and the five visible planets\n(Mercury, Venus, Mars, Jupiter, and Saturn), primarily for omens. They\ndid not try to understand the motions physically. They did, however,\nnotice the circular track of the Sun's annual path across the sky and\nknew that it took about 360 days to complete one year's circuit.\nConsequently, they divided the circular path into 360 degrees to track\neach day's passage of the Sun's whole journey. This probably happened\nabout 2400 BC.\nThat's how we got a 360 degree circle. Around 1500 BC, Egyptians\ndivided the day into 24 hours, though the hours varied with the\nseasons originally. Greek astronomers made the hours equal. About 300\nto 100 BC, the Babylonians subdivided the hour into base-60 fractions:\n60 minutes in an hour and 60 seconds in a minute. The base 60 of their\nnumber system lives on in our time and angle divisions.\nAn 100-degree circle makes sense for base 10 people like ourselves.\nBut the base-60 Babylonians came up with 360 degrees and we cling to\ntheir ways-4,400 years later.\n\nThen, there's also this discussion on Math Forum:\n\nIn 1936, a tablet was excavated some 200 miles from Babylon. Here one\nshould make the interjection that the Sumerians were first to make one\nof man's greatest inventions, namely, writing; through written\ncommunication, knowledge could be passed from one person to others,\nand from one generation to the next and future ones.  They impressed\ntheir cuneiform (wedge-shaped) script on soft clay tablets with a\nstylus, and the tablets were then hardened in the sun.  The mentioned\ntablet, whose translation was partially published only in 1950, is\ndevoted to various geometrical figures, and states that the ratio of\nthe perimeter of a regular hexagon to the circumference of the\ncircumscribed circle equals a number which in modern notation is given\nby $ \\frac{57}{60} + \\frac{36}{60^2} $\n(the Babylonians used the sexagesimal system, i.e., their\nbase was 60 rather than 10).\nThe Babylonians knew, of course, that the perimeter of a hexagon is\nexactly equal to six times the radius of the circumscribed circle, in\nfact that was evidently the reason why they chose to divide the circle\ninto 360 degrees (and we are still burdened with that figure to this\nday).  The tablet, therefore, gives ... $\\pi = \\frac{25}{8} = 3.125$.", "meta": {"post_id": 340467, "input_score": 34, "output_score": 35, "post_title": "Why is a full circle 360\u00b0 degrees?"}}
{"input": "prove that$$\\sum_{n=0}^{\\infty}\\left(\\frac{(-1)^n}{2n+1}\\sum_{k=0}^{2n}\\frac{1}{2n+4k+3}\\right)=\\frac{3\\pi}{8}\\log(\\frac{1+\\sqrt5}{2})-\\frac{\\pi}{16}\\log5 $$\nThis problem, I think use \n$$\\sum_{k=0}^{2n}\\dfrac{1}{2n+4k+3}=H_{10n+3}-H_{2n+3}$$\nThank you everyone help", "output": "First of all let us introduce the function\n$$ F(x)=\\sum_{n=0}^{\\infty}\\frac{(-1)^n}{2n+1}\\sum_{k=0}^{2n}\\frac{x^{2n+4k+3}}{2n+4k+3} $$\nWe need to compute $F(1)$. Let us start by calculating\n$$ x^{-2}F'(x)=\\sum_{n=0}^{\\infty}\\frac{(-1)^n}{2n+1}\\sum_{k=0}^{2n}x^{2n+4k}=\n\\sum_{n=0}^{\\infty}\\frac{(-1)^n}{2n+1}\\frac{x^{2n}-x^{4+10n}}{1-x^4}=\\frac{\\arctan x-\\arctan x^5}{x(1-x^4)},$$\nwhere the first step is obtained by straightforward differentiation, the second equality follows by summation of finite geometric series, and the third one is obtained from the series representation $\\arctan x=\\sum_{n=0}^{\\infty}\\frac{(-1)^nx^{2n+1}}{2n+1}$. Since it is clear from the definition that $F(0)=0$, we find\n$$ F(1)=\\int_0^1\\frac{x\\left(\\arctan x-\\arctan x^5\\right) dx}{1-x^4}$$\nIntegrate by parts using that $\\int\\frac{xdx}{1-x^4}=\\frac14\\ln\\frac{1+x^2}{1-x^2}$ (and saying appropriate words about the limits). This gives\n\\begin{align*}\nF(1)=\\frac14\\int_0^1\\left(\\frac{1}{1+x^2}-\\frac{5x^4}{1+x^{10}}\\right)\\ln\\left(\\frac{1-x^2}{1+x^2}\\right)dx=\\qquad\\qquad\\qquad\\\\\n=\\frac14\\int_0^1\\frac{(1+x^2)(1-3x^2+x^4)}{x^8-x^6+x^4-x^2+1}\\ln\\left(\\frac{1-x^2}{1+x^2}\\right)dx.\\qquad\\qquad (\\mathrm{A})\n\\end{align*}\nNotice that the change of integration variable $x\\rightarrow 1/x$ and parity allow to rewrite (A) as \n$$F(1)=\\frac{1}{16}\\int_{-\\infty}^{\\infty}\\frac{(1+x^2)(1-3x^2+x^4)}{x^8-x^6+x^4-x^2+1}\\ln\\left|\\frac{1-x^2}{1+x^2}\\right|dx $$\nNext we write this in terms of complex contour integral:\n$$F(1)=\\frac{1}{16}\\mathrm{Re}\\left\\{\\int_C f(z)dz\\right\\}\\qquad\\qquad (\\mathrm{B})$$\nwith \n$$ f(z)=\\frac{(1+z^2)(1-3z^2+z^4)}{z^8-z^6+z^4-z^2+1}\\ln\\left(\\frac{1-z^2}{1+z^2}\\right) $$\nAt this point, some comments are necessary:\n\nthe function $f(z)$ has a number of singularities in the complex $z$-plane. First of all one has 8 simple poles $z=\\exp\\pm\\frac{ik\\pi}{10}$, $k=1,3,7,9$. One also has logarithmic branch points $z=\\pm1,\\pm i$. Accordingly, we introduce 4 branch cuts: $B_{1}=[1,\\infty)$, $B_{-1}=(-\\infty,-1]$, $B_i=[i,i\\infty)$, $B_{-i}=(-i\\infty,-i]$.\nthe contour of integration $C$ runs from $-\\infty$ to $+\\infty$ slightly above the real axis (we could put it slightly below and modify what follows). The logarithms in $f(z)$ will be defined on their main sheets for $z\\in(-1,1)$, then $f(z)$ is unambigously defined in the cut plane by analytic continuation.\nGoing slightly above the branch cuts $B_{\\pm1}$ produces (irrelevant) imaginary parts in the logarithms. This explains the need to take real part in (B).\n\nNow the idea is to pull the contour $C$ to $i\\infty$. The integral will then be given by the sum of residues at $z=\\exp\\frac{ik\\pi}{10}$, $k=1,3,7,9$, plus integral of the jump of $f(z)$ on the cut $B_i$. It is not difficult to understand that the latter is real (rational prefactor $\\frac{(1+z^2)(1-3z^2+z^4)}{z^8-z^6+z^4-z^2+1}$ is real on the cut, the logarithm jumps by $2\\pi i$ and the integral is along $B_i$\nso that the integration variable is pure imaginary). Therefore we are left with\n$$F(1)=\\frac{1}{16}\\left[2\\pi\\int_1^{\\infty}\\frac{(1-x^2)(1+3x^2+x^4)dx}{x^8+x^6+x^4+x^2+1}+\\mathrm{Re}\\left\\{2\\pi i \\sum_{k=1,3,7,9}\\mathrm{res}_{z=\\exp\\frac{ik\\pi}{10}}f(z)\\right\\}\\right]\\qquad(\\mathrm{C})$$\nIn the first integral we have a rational function, so it can be calculated by elementary means:\n$$\\int_1^{\\infty}\\frac{(1-x^2)(1+3x^2+x^4)dx}{x^8+x^6+x^4+x^2+1}=\\left[\\frac12\\ln\\frac{1+x+x^2+x^3+x^4}{1-x+x^2-x^3+x^4}\\right]_{x=1}^{x=\\infty}=-\\frac{\\ln5}{2}$$\nThe residues are also relatively easily computed:\n\\begin{align}\n\\mathrm{res}_{z=\\exp\\frac{i\\pi}{10}}f(z)=\\frac{i}{2}\\left(\\ln\\tan\\frac{\\pi}{10}-\\frac{i\\pi}{2}\\right),\\\\\n\\mathrm{res}_{z=\\exp\\frac{3i\\pi}{10}}f(z)=-\\frac{i}{2}\\left(\\ln\\tan\\frac{3\\pi}{10}-\\frac{i\\pi}{2}\\right),\\\\\n\\mathrm{res}_{z=\\exp\\frac{7i\\pi}{10}}f(z)=-\\frac{i}{2}\\left(\\ln\\tan\\frac{3\\pi}{10}+\\frac{i\\pi}{2}\\right),\\\\\n\\mathrm{res}_{z=\\exp\\frac{9i\\pi}{10}}f(z)=\\frac{i}{2}\\left(\\ln\\tan\\frac{\\pi}{10}+\\frac{i\\pi}{2}\\right).\n\\end{align}\nSubstituting this into (C), we finally obtain\n$$ F(1)=\\frac{\\pi}{8}\\left(\\ln\\frac{\\tan\\frac{3\\pi}{10}}{\\tan\\frac{\\pi}{10}}-\\frac{\\ln 5}{2}\\right) $$\nThe statement now follows from the easily verified identity $\\frac{\\tan\\frac{3\\pi}{10}}{\\tan\\frac{\\pi}{10}}=\\left(\\frac{1+\\sqrt{5}}{2}\\right)^3$. $\\blacksquare$", "meta": {"post_id": 341302, "input_score": 23, "output_score": 47, "post_title": "How can I find $\\sum\\limits_{n=0}^{\\infty}\\left(\\frac{(-1)^n}{2n+1}\\sum\\limits_{k=0}^{2n}\\frac{1}{2n+4k+3}\\right)$?"}}
{"input": "Cauchy's Hypothesis or Noll's theorem states that $\\vec{t}(\\vec{X},t;\\partial \\Omega) = \\vec{t}(\\vec{X},t;\\vec{N})$ where $\\vec{N}$ is the outward unity normal to the positively oriented surface $\\partial \\Omega$. This translates to words as the dependence of the surface interaction vector on the surface on which it acts is only through the normal $\\vec{N}$. My question is what is the significance of the semicolon (;)? How does it differ from the comma (,) used to separated the function's first two arguments?", "output": "A semicolon is used to separate variables from parameters. Quite often, the terms variables and parameters are used interchangeably, but with a semicolon the meaning is that we are defining a function of the parameters that returns a function of the variables.\nFor example, if I write $f(x1,x2,\\ldots;p1,p2,\\ldots)$ then I mean that by supplying the parameters $(p1, p2,\\ldots)$, I create a new function whose arguments are $(x1, x2,\\ldots)$.\nSo the general syntax is $\\operatorname{functionName}(\\mathrm{variables};\\mathrm{parameters})$.\nIn Noll's theorem it says that the function created by supplying $\\partial \\Omega$ is the same as that created by supplying $\\vec{N}$. That's rather a nice way of saying that the function created only depends on $\\vec{N}$.", "meta": {"post_id": 342268, "input_score": 35, "output_score": 75, "post_title": "What does the semicolon ; mean in a function definition"}}
{"input": "What is the difference between minimum and infimum?\nI have a great confusion about this.", "output": "The minimum is attained, the infimum isn't necessarily. \nExample.\nLet $f(x) = \\frac{1}{x}$. Then $f$ has no minimum value on the interval $(0,\\infty)$. The minimum is the smallest element in the set. That is\n$$\n\\min\\{f(x)\\mid x\\in (0,\\infty)\\}\n$$\ndoesn't exist because there is not smallest number in the set.\nAnother example is the minimum of the set $S = (0,1) = \\{x\\mid 0<x<1\\}$. Here again there isn't a smallest number\n$$\n\\min\\{x\\mid 0<x<1\\}\n$$\ndoesn't exist.\nThe infimum of a set $S$ is defined as the greatest number that is less than or equal to all elements of S (from Wikipedia). The infimum is also sometimes called the greatest lower bound.\nIt is a fact that every non empty set (bounded below) of real numbers has an infimum. But, as we saw, not every real set has a minimum.\nSo in the example\n$$\n\\inf\\{f(x)\\mid x\\in (0,\\infty)\\} = 0.\n$$\nNote that the infimum and the minimum can be the same. Consider for example $S = \\{1,2,3,\\dots\\}$. Then the infimum and minimum is both $1$.\nConsider this other example. If $f$ is a continuous function on a closed interval $[a,b]$,  then it is a fact that $f$ attains a minimum over that interval. So here again\n$$\n\\inf\\{f(x)\\mid x\\in [a,b]\\} = \\min\\{f(x)\\mid x\\in [a,b]\\}.\n$$", "meta": {"post_id": 342749, "input_score": 64, "output_score": 97, "post_title": "What is the difference between minimum and infimum?"}}
{"input": "How to show that  $$\\lim_{n\\to\\infty}\\left[\\sum^n_{k=1}\\frac{1}{k}-\\ln(n)\\right]=0.5772\\ldots$$\nNo clue at all. Need help!  Appreciated!", "output": "Showing that\n$\\displaystyle\\lim_{n\\to\\infty}\\left(\\sum_{k=1}^n\\frac1k-\\log(n)\\right)$\nexists\nNote that\n$$\n\\frac1{n+1}\\le\\int_n^{n+1}\\frac1x\\,\\mathrm{d}x\\le\\frac1n\\tag{1}\n$$\nSumming the right hand inequality shows that\n$$\n\\sum_{k=1}^n\\frac1k-\\log(n+1)\\tag{2}\n$$\nis increasing. Summing the left hand inequality shows that\n$$\n\\sum_{k=1}^n\\frac1k-\\log(n)\\tag{3}\n$$\nis decreasing.\nSince $\\displaystyle\\lim_{n\\to\\infty}\\log\\left(\\frac{n+1}{n}\\right)=0$, $(2)$ and $(3)$ show that $\\displaystyle\\gamma=\\lim_{n\\to\\infty}\\left(\\sum_{k=1}^n\\frac1k-\\log(n)\\right)$ exists and is between $0$ and $1$.\n\nEvaluating $\\gamma$\nDefine the Harmonic Numbers\n$$\nH(n)=1+\\frac12+\\frac13+\\frac14+\\dots+\\frac1n\\tag{4}\n$$\nand the Alternating Harmonic Numbers\n$$\nAH(n)=1-\\frac12+\\frac13-\\frac14+\\dots+(-1)^{n-1}\\frac1n\\tag{5}\n$$\nand the Alternating Harmonic Tails\n$$\nAHT(n)=\\frac1{n+1}-\\frac1{n+2}+\\frac1{n+3}-\\frac1{n+4}+\\dots\\tag{6}\n$$\nRemember that\n$$\n\\begin{align}\n\\log(2)&=1-\\frac12+\\frac13-\\frac14+\\frac15-\\frac16+\\dots\\\\\n&=AH(n)+(-1)^nAHT(n)\\tag{7}\n\\end{align}\n$$\nNotice that for $n\\gt0$,\n$$\n\\begin{align}\n&H(2^n)-H(2^{n-1})\\\\\n&=\\left(1+\\frac12+\\frac13+\\frac14+\\dots+\\frac1{2^n}\\right)-\\left(1+\\frac12+\\frac13+\\frac14+\\dots+\\frac1{2^{n-1}}\\right)\\\\\n&=\\left(1+\\frac12+\\frac13+\\frac14+\\dots+\\frac1{2^n}\\right)-\\left(\\frac22+\\frac24+\\frac26+\\frac28+\\dots+\\frac2{2^n}\\right)\\\\\n&=\\left(1-\\frac12+\\frac13-\\frac14+\\dots-\\frac1{2^n}\\right)\\\\\n&=AH(2^n)\\tag{8}\n\\end{align}\n$$\nTherefore,\n$$\n\\begin{align}\nH(2^n)\n&=1+\\sum_{k=1}^n\\left(H(2^k)-H(2^{k-1})\\right)\\\\\n&=1+\\sum_{k=1}^nAH(2^k)\\tag{9}\n\\end{align}\n$$\nBecause $\\displaystyle\\gamma=\\lim_{n\\to\\infty}\\Big(H(2^n)-\\log(2^n)\\Big)$, subtract $n\\log(2)$ from $(9)$ and rearrange terms to get\n$$\nH(2^n)-\\log(2^n)\n=1+\\sum_{k=1}^n\\left(AH(2^k)-\\log(2)\\right)\\tag{10}\n$$\nFor $k\\gt0$, $2^k$ is even, so we have from $(7)$\n$$\n\\log(2)-AH(2^k)=AHT(2^k)\\tag{11}\n$$\nUsing $(10)$ and $(11)$, we get\n$$\n\\gamma=1-\\sum_{k=1}^\\infty AHT(2^k)\\tag{12}\n$$\nExpedite the computation of $(12)$ using the following integral,\n$$\n\\begin{align}\nAHT(m)\n&=\\sum_{j=1}^\\infty\\frac{(-1)^{j-1}}{m+j}\\\\\n&=\\int_0^1\\frac{x^m}{1+x}\\,\\mathrm{d}x\\\\\n&=\\sum_{j=1}^\\infty\\frac1{j\\binom{m+j}{j}2^j}\\tag{13}\n\\end{align}\n$$\nwhere the last sum in $(13)$ is gotten by repeatedly integrating by parts. It converges far more quickly than the alternating harmonic tail, two lines above.\nUsing $(12)$ and $(13)$, it is not unreasonable to compute\n$$\n\\gamma=0.57721566490153286060651209\\tag{14}\n$$\nIn fact, I used $(12)$ and $(13)$ to compute $10000$ digits of $\\gamma$.\n\nA Slightly Different Sum\nSince $AHT(2^k-1)=2^{-k}-AHT(2^k)$, we can rewrite $(12)$ as\n$$\n\\gamma=\\sum_{k=1}^\\infty AHT(2^k-1)\\tag{15}\n$$\nand use $(13)$ and $(15)$ to get $(14)$, as well.\n\nA Mathematica Implementation\nHere is an implementation of the algorithm above to compute $d$ digits of $\\gamma$.\nAHT[m_,d_]:=Module[{p=10^(d+2), j=1, b=1, t, s=SetPrecision[1,d+4]-1}, \n  While[b=b*2(m+j)/j; (t=j b)<=p, s=s+1/t; j=j+1]; SetPrecision[s,d]]\nEulerG[d_]:=Module[{n=Ceiling[10d/3]}, Sum[AHT[2^k-1,d], {k,1,n}]]", "meta": {"post_id": 344314, "input_score": 31, "output_score": 45, "post_title": "Showing that $\\lim\\limits_{n\\to\\infty}\\sum^n_{k=1}\\frac{1}{k}-\\ln(n)=0.5772\\ldots$"}}
{"input": "When $A, B$ are $K$-modules, then $A \\times B$ is the same as $A \\oplus B$. \n\nLet $A, B$ be two $K$-algebras, where $K$ is a field. Is $A \\times B$ the same as $A \\oplus B$? \n\nThank you very much.\nEdit: $A \\times B$ is direct product and $A \\oplus B$ is direct sum.\nEdit: I am asking this question because I am confused with Lemma 1.6 on page 46 of the book Elements of representations of associative algebras. It is said that $\\{e_1, \\ldots, e_n\\}$ is a complete set of primitive orthogonal idempotents. This means that $A=e_1A\\oplus \\cdots \\oplus e_nA$. But it is said that $A$ is not nessesarily connected (connected means $A$ is a product of two non-trivial algebras, the definition of connected is on page 18 of the book, above example 4.1).", "output": "To my very regret, old-fashioned terminologies are all over the place in mathematics and still prevent us from using the universal benefit of (the deep idea of) category theory, for example the unification of various scattered notions in mathematics. So let me answer what $A \\times B$ and $A \\oplus B$ should denote (although most books have not adopted this yet).\nIn any category, $\\times$ denotes the product, and $\\oplus$ or $\\sqcup$ denotes the coproduct. Even in the category of $K$-modules, they are not the same. Only the underlying objects are the same. A (co)product is more than just an object, it is a a special (co)limit (co)cone. Thus, a product comes equipped with projections, and a coproduct comes equipped with inclusions. Of course there is this usual abuse of notation. But in this case, in my opinion, it is important to realize that (even the underlying objects) $A \\times B$ and $A \\oplus B$ really have a different flavor, and that it really is a coincidence that they agree. Instead of being surprised that they don't agree for, say, groups, one should be surprised that they agree for abelian groups. Unfortunately, the current literature and courses for beginners do not support this. The only reason why one can write $A \\times B = A \\oplus B$ safely is that there is a canonical isomorphism between the underlying objects, and in fact there is the notion of a biproduct which fuses product and coproduct in arbitrary linear categories.\nNow let $k$ be a commutative ring. For me, rings are always unital unless otherwise specified. The product in the category of $k$-algebras is as usual: We just take the cartesian product of the underlying sets, and then declare addition, scalar multiplication, multiplication and unit pointwise. This works for every algebraic category. So the elements of $A \\times B$ are easy to understand and easy to manipulate. The coproduct $A \\oplus B$ is more complicated. The underlying $k$-module is some quotient of $k \\oplus A \\oplus B \\oplus A \\otimes_k B \\oplus B \\otimes_k A \\oplus A \\otimes_k A \\otimes_k B \\oplus \\dotsc$, where we basically identify $x$ with $x \\otimes 1$ and $1 \\otimes x$. The multiplication is defined in an obvious way. Of course, in the category of commutative $k$-algebras this simplifies a lot, namely we may reduce to $A \\otimes_k B$.\nMany people still write $A \\oplus B$ for what should be denoted by $A \\times B$, because the elements of $A \\times B$ may be written as $(a,0) + (0,b)$ and there is no harm in identifiying $(a,0)$ with $a$ etc. But meanwhile it should be well-known that morphisms are as important as elements, in the context of universal constructions even far more important than elements, and more importantly it glosses over the fact that $a \\mapsto (a,0)$ is not a homomorphism of algebras, since it does not preserve the unit. So this construction usually denoted by $A \\oplus B$ actually takes place in the category of non-unital $k$-algebras:\nLet more generally $(A_i)_{i \\in I}$ be a family of non-unital $k$-algebras. As a $k$-module, their coproduct $\\oplus_{i \\in I} A_i$ is again a large direct sum of tensor products (for example $A \\oplus B = A \\oplus B \\oplus A \\otimes_k B \\oplus B \\otimes_k A \\oplus \\dotsc$ with a suitable multiplication). But many people denote by $\\oplus_{i \\in I} A_i$ the subalgebra $C$ of $\\prod_{i \\in I} A_i$ consisting of those tuples with finite support (see Frank McGovern's answer). But this is not a coproduct and therefore I find it confusing to denote it as such, contradicting the usual use of this symbol (I will suggest a new symbol below). Nevertheless, it has an interesting universal property. In some sense, it is the universal coproduct with the additional requirement that $A_i \\cdot A_j = 0$ for $i \\neq j$. More precisely, there are inclusions $A_i \\to C$ such that $A_i \\otimes_k A_j \\to C$  is zero for $i \\neq j$, and for every family of homomorphisms of non-unital $k$-algebras $A_i \\to B$ such that $A_i \\otimes_k A_j \\to B$ is zero for $i \\neq j$, there is a homomorphism $C \\to B$ making the obvious diagrams commute. Sometimes elements $x,y$ of a ring with $xy=0$ are called orthogonal (motivated by the case of projections on some space). Therefore, I would call $C$ the orthogonal coproduct and denote it by $\\oplus_{i \\in I}^{\\perp} A_i$. What do you think?\nSee here for a quite similar construction for groups.\nNotation should be consistent and work for all categories. It only gets confusing when for different categories the notation differs.\nEdit: Surprisingly, my point of view also appears on Wikipedia: Direct sum of rings", "meta": {"post_id": 345501, "input_score": 29, "output_score": 51, "post_title": "Is $A \\times B$ the same as $A \\oplus B$?"}}
{"input": "I have a simple question answer to which would help me more deeply understand the concept of (non)commutative structures. Let's take for example (our teacher's definition of) a ring:\nLet $R\\neq \\emptyset$ be a set, let $\\oplus:A\\times A \\to A$ and $\\bullet :A\\times A \\to A$ be binary operations. Moreover, let $(R, \\oplus)$ be a commutative group, $(R, \\bullet)$ be a monoid and following property holds for all $a, b, c\\in R$:\n$$a\\bullet(b\\oplus c) = (a\\bullet b)\\oplus(a \\bullet c)$$\n$$(b\\oplus c)\\bullet a = (b\\bullet a)\\oplus(c \\bullet a)$$\nThen ordered triple $\\mathbf R = (R, \\oplus, \\bullet \\mathbf)$ is called a (unitary) ring.\nMoreover, we call ring $\\mathbf R$ commutative iff $(R, \\bullet)$ is a commutative monoid. Commutativity of a ring is always a matter of its multiplicative operation because the additive operation is always assumed to be commutative.\nCould anyone explain me the bold part? Why do we even in non-commutative rings (and fields etc.) assume the addition to be always commutative? Is there some serious reason? Would it make any trouble? Or studying of structures with non-commutative addition just doesn't give us anything new so we can take addition as commutative simply because of our comfort?", "output": "Perhaps the comment refers to the fact that in order to generalize rings to structures with noncommutative addition, we cannot  simply delete the axiom that addition is commutative,  since, in fact, other axioms force addition to be commutative (Hankel, 1867 [1]). The proof is simple: apply both the left and right\ndistributive law in different order to the term  $\\rm\\:(1\\!+\\!1)(x\\!+\\!y),\\:$ viz.\n$$\\rm (1\\!+\\!1)(x\\!+\\!y) = \\bigg\\lbrace \\begin{eqnarray}\\rm (1\\!+\\!1)x\\!+\\!(1\\!+\\!1)y\\, =\\, x + \\color{#C00}{x\\!+\\!y} + y\\\\\n\\rm 1(x\\!+\\!y)\\!+1(x\\!+\\!y)\\, =\\, x + \\color{#0A0}{y\\!+\\!x} + y\\end{eqnarray}\\bigg\\rbrace\\Rightarrow\\, \\color{#C00}{x\\!+\\!y}\\,=\\,\\color{#0A0}{y\\!+\\!x}\\ \\ by\\ \\ cancel\\ \\ x,y$$\nThus commutativity of addition, $\\rm\\:x+y = y+x,\\:$ is implied by these axioms:\n$(1)\\ \\  *\\,$ distributes over $\\rm\\,+\\!:\\ \\   x(y+z)\\, =\\, xy+xz,\\ \\   (y+z)x\\, =\\, yx+zx$\n$(2)\\ \\,  +\\,$ is cancellative: $\\rm\\ \\      x+y\\, =\\, x+z\\:\\Rightarrow\\: y=z,\\ \\  y+x\\, =\\, z+x\\:\\Rightarrow\\: y=z$\n$(3)\\ \\,  +\\,$ is associative:  $\\rm\\ \\    (x+y)+z\\, =\\, x+(y+z)$\n$(4)\\ \\  *\\,$ has a neutral element $\\rm\\,1\\!:\\ \\      1x = x$\nIn order to state this result concisely, recall that a SemiRing is\nthat generalization of a Ring whose additive structure is relaxed\nfrom a commutative Group to merely a SemiGroup, i.e. here the only\nhypothesis on addition is that it be associative (so in SemiRings,\nunlike Rings, addition need not be commutative, nor need every\nelement $\\rm\\,x\\,$ have an additive inverse $\\rm\\,-x).\\,$ Now the above result may\nbe stated as follows: a semiring with $\\,1\\,$ and cancellative addition\nhas commutative addition. Such semirings are simply subsemirings\nof rings (as is $\\rm\\:\\Bbb N \\subset \\Bbb Z)\\,$ because any commutative cancellative\nsemigroup embeds canonically into a commutative group, its group\nof differences (in precisely the same way $\\rm\\,\\Bbb Z\\,$ is constructed from $\\rm\\,\\Bbb N,\\,$\ni.e. the additive version of the fraction field construction).\nExamples of SemiRings include:  $\\rm\\,\\Bbb N;\\,$ initial segments of cardinals;\ndistributive lattices (e.g. subsets of a powerset with operations $\\cup$ and $\\cap$;\n$\\rm\\,\\Bbb R\\,$ with + being min or max, and $*$ being addition; semigroup semirings\n(e.g. formal power series); formal languages with union, concat; etc.\nFor a nice survey of SemiRings and SemiFields see [2]. See also Near-Rings.\n[1] Gerhard Betsch. On the beginnings and development of near-ring theory.\npp. 1-11 in:\nNear-rings and near-fields. Proceedings of the conference\nheld in Fredericton, New Brunswick, July 18-24, 1993. Edited by Yuen Fong,\nHoward E. Bell, Wen-Fong Ke, Gordon Mason and  Gunter Pilz.\nMathematics and its Applications, 336. Kluwer Academic Publishers Group,\nDordrecht, 1995. x+278 pp. ISBN: 0-7923-3635-6 Zbl review\n[2] Hebisch, Udo; Weinert, Hanns Joachim. Semirings and semifields. $\\ $ pp. 425-462 in:  Handbook of algebra. Vol. 1. Edited by M. Hazewinkel.\nNorth-Holland Publishing Co., Amsterdam, 1996. xx+915 pp. ISBN: 0-444-82212-7\nZbl review,\nAMS review", "meta": {"post_id": 346375, "input_score": 29, "output_score": 35, "post_title": "Commutative property of ring addition"}}
{"input": "I want to compute the inverse Laplace transform of a function\n$$\n   F(z) = e^{-\\sqrt{z}}.\n$$\nThis problem seems very nontrivial to me. Here one can find the answer: the inverse Laplace transform of one variable function $e^{-\\sqrt{z}}$ is equal to\n$$\n   \\mathcal{L}^{-1}[e^{-\\sqrt{z}}](x) = \\frac{1}{2 \\sqrt{\\pi}} x^{-\\frac{3}{2}} \\exp \\left( -\\frac{1}{4x} \\right).\n$$\nBut what is the simpliest way to do it? Post's formula requires knowledge of all degree derivatives of $e^{-\\sqrt{z}}$ and I think that it isn't a good way. The classical inversion formula is of the form\n$$\n    \\mathcal{L}^{-1}[F(z)](x) = \\frac{1}{2 \\pi i}\\int\\limits_{\\sigma - i \\infty}^{\\sigma + i \\infty} F(z) e^{zx}\\,dz = \\frac{1}{2 \\pi i} \\int\\limits_{\\sigma - i \\infty}^{\\sigma + i \\infty} e^{-\\sqrt{z}+zx} \\, dz.\n$$\nTo compute it I make a substitution $p = \\sqrt{z}$. Then I'm looking for the image of the line $\\sigma + i \\mathbb{R}$. If I'm not mistaken it is the angle with vertice at $\\sqrt{\\sigma}$ and with rays $\\sqrt{\\sigma} + e^{i \\frac{\\pi}{4}} [0,\\infty)$ and $\\sqrt{\\sigma}+e^{-i\\frac{\\pi}{4}} [0,\\infty)$ (not exactly, these rays are curvilinear, but I think that this doesn't matter because of Cauchy formula). I will denote it $\\Lambda$. So \n$$\n   \\mathcal{L}^{-1}[e^{-\\sqrt{z}}] = \\frac{1}{\\pi i} \\int\\limits_{\\Lambda} e^{-p + p^2 x}p \\, dp.\n$$\nThen I should look for residues, but integrand doesn't have them in finite part of $\\mathop{\\mathrm{conv}} \\Lambda$. Please help me wiith it.", "output": "You can use a contour integration without that substitution as follows by deforming the Bromwich contour about the negative real axis and exploiting a branch cut of $\\sqrt{z}$ about that axis.  So, consider the integral\n$$\\oint_C dz \\: e^{-\\sqrt{z}} e^{z t}$$\nwhere $C$ is a keyhole contour about the negative real axis, as pictured below.\n\nWe will define $\\text{Arg}{z} \\in (-\\pi,\\pi]$, so the branch is the negative real axis.  There are $6$ pieces to this contour, $C_k$, $k \\in \\{1,2,3,4,5,6\\}$, as follows.\n$C_1$ is the contour along the line $z \\in [c-i R,c+i R]$ for some large value of $R$.\n$C_2$ is the contour along a circular arc of radius $R$ from the top of $C_1$ to just above the negative real axis.\n$C_3$ is the contour along a line just above the negative real axis between $[-R, -\\epsilon]$ for some small $\\epsilon$.\n$C_4$ is the contour along a circular arc of radius $\\epsilon$ about the origin.\n$C_5$ is the contour along a line just below the negative real axis between $[-\\epsilon,-R]$.\n$C_6$ is the contour along the circular arc of radius $R$ from just below the negative real axis to the bottom of $C_1$.\nWe will show that the integral along $C_2$,$C_4$, and $C_6$ vanish in the limits of $R \\rightarrow \\infty$ and $\\epsilon \\rightarrow 0$.\nOn $C_2$, the real part of the argument of the exponential is\n$$R t \\cos{\\theta} - \\sqrt{R} \\cos{\\frac{\\theta}{2}}$$\nwhere $\\theta \\in [\\pi/2,\\pi)$.  Clearly, $\\cos{\\theta} < 0$ and $\\cos{\\frac{\\theta}{2}} > 0$, so that the integrand exponentially decays as $R \\rightarrow \\infty$ and therefore the integral vanishes along $C_2$.\nOn $C_6$, we have the same thing, but now $\\theta \\in (-\\pi,-\\pi/2]$.  This means that, due to the evenness of cosine, the integrand exponentially decays again as $R \\rightarrow \\infty$ and therefore the integral also vanishes along $C_6$.\nOn $C_4$, the integral vanishes as $\\epsilon$ in the limit $\\epsilon \\rightarrow 0$.  Thus, we are left with the following by Cauchy's integral theorem (i.e., no poles inside $C$):\n$$\\left [ \\int_{C_1} + \\int_{C_3} + \\int_{C_5}\\right] dz \\: e^{-\\sqrt{z}} e^{z t} = 0$$\nOn $C_3$, we parametrize by $z=e^{i \\pi} x$ and the integral along $C_3$ becomes\n$$\\int_{C_3} dz \\: e^{-\\sqrt{z}} e^{z t} = e^{i \\pi} \\int_{\\infty}^0 dx \\: e^{-i \\sqrt{x}} e^{-x t}$$\nOn $C_5$, however, we parametrize by $z=e^{-i \\pi} x$ and the integral along $C_5$ becomes\n$$\\int_{C_5} dz \\: e^{-\\sqrt{z}} e^{z t} = e^{-i \\pi} \\int_0^{\\infty} dx \\: e^{i \\sqrt{x}} e^{-x t}$$\nWe may now write\n$$-\\frac{1}{i 2 \\pi} \\int_0^{\\infty} dx \\: e^{- x t} \\left ( e^{i \\sqrt{x}} - e^{-i \\sqrt{x}} \\right ) + \\frac{1}{i 2 \\pi} \\int_{c-i \\infty}^{c+i \\infty} ds \\: e^{-\\sqrt{s}} e^{s t} = 0$$\nTherefore, the ILT of $\\hat{f}(s) = e^{-\\sqrt{s}}$ is given by\n$$\\begin{align}\\frac{1}{i 2 \\pi} \\int_{c-i \\infty}^{c+i \\infty} ds \\: e^{-\\sqrt{s}} e^{s t} &= \\frac{1}{i 2 \\pi} \\int_0^{\\infty} dx \\: e^{- x t} \\left ( e^{i \\sqrt{x}} - e^{-i \\sqrt{x}} \\right )\\\\ &= \\frac{1}{\\pi} \\int_{-\\infty}^{\\infty} du\\: u \\,e^{-t u^2} \\sin{u}\\end{align}$$\nThe last step involved substituting $x=u^2$ and exploiting the evenness of the integrand.  This integral may be evaluated as follows:\n$$\\begin{align}\\frac{1}{\\pi} \\int_{-\\infty}^{\\infty} du\\: u \\,e^{-t u^2} \\sin{u} &= \\frac{1}{\\pi} \\Im{\\left [\\int_{-\\infty}^{\\infty} du\\:u\\, e^{-t u^2} e^{i u} \\right]}\\\\ &= \\frac{1}{\\pi} \\Im{\\left [\\int_{-\\infty}^{\\infty} du\\:u\\, e^{-t (u-i/(2 t))^2} e^{-1/(4 t)}\\right ]}\\\\ &= \\frac{1}{\\pi} e^{-1/(4 t)} \\Im{\\left [\\int_{-\\infty}^{\\infty} dv \\: \\left ( v + \\frac{i}{2 t} \\right ) e^{-t v^2} \\right]}\\\\ &= \\frac{1}{\\pi} e^{-1/(4 t)}  \\frac{1}{2 t} \\sqrt{\\frac{\\pi}{t}} \\end{align}$$\nTherefore the result is that\n$$\\mathcal{L}^{-1}[e^{-\\sqrt{z}}](t) = \\frac{1}{i 2 \\pi} \\int_{c-i \\infty}^{c+i \\infty} dz \\: e^{-\\sqrt{z}} e^{z t} = \\frac{1}{2 \\sqrt{\\pi}} t^{-3/2} e^{-\\frac{1}{4 t}}$$\nas was to be shown.", "meta": {"post_id": 347933, "input_score": 23, "output_score": 39, "post_title": "Compute the inverse Laplace transform of $e^{-\\sqrt{z}}$"}}
{"input": "I'm trying to understand intuitively the notion of Lipschitz function. \nI can't understand why bounded function doesn't imply Lipschitz function.\n\nI need a counterexample or an intuitive idea to clarify my notion of Lipschitz function.\nI need help\nThanks a lot", "output": "It means that your function does not explode at some point - made mathematically rigorous.\nThis idea can also be shown geometrically like so (picture from wikipedia):\n\nThe Lipschitz condition (or Lipschitz continuity) ensures that your function always remains entirely outside the white cone, so it cannot e.g. become infinitely steep at one point.\nConcerning your picture: Think of the function $sin(1/x)$ (source W|A):\n\nYou see that it is becoming infinitely steep at the origin (and would therefore run into the white cone!) but it is still bounded.\nThis condition is e.g. used in the theory of differential equations where it guarantees the existence and uniqueness of solutions to certain types of differential equations.", "meta": {"post_id": 353276, "input_score": 38, "output_score": 43, "post_title": "Intuitive idea of the Lipschitz function"}}
{"input": "Is Serge Lang's famous book Algebra nowadays still worth reading, or are there other, more modern books which are better suited for an overview over all areas of algebra?\nEDIT: My main concern is that the first edition of Algebra is already 48 years old. Even if there have probably been no fundamental new insight in Algebra which can be included in a first-year-graduate algebra course, the passage of the decades may have helped clarify what are the most important results and techniques as well as how they can be achieved with the least effort.\nIn addition, I'm wondering whether the terminology and notation is nonstandard or out of fashion (for example Lang, calls integral domains entire rings, an expression which I have never seen anywhere else).", "output": "Yes. ${}{}{}{}{}{}{}{}{}{}{}{} $", "meta": {"post_id": 354221, "input_score": 66, "output_score": 94, "post_title": "Is Serge Lang's Algebra still worth reading?"}}
{"input": "If $A$ and $B$ are $n\\times n$ matrices such that $AB = BA$ (that is, $A$ and $B$ commute), show that \n\n$$ e^{A+B}=e^A e^B$$ \n\nNote that $A$ and $B$ do NOT have to be diagonalizable.", "output": "$$\\begin{align*}e^{A}e^{B} &= \\left(\\sum \\frac{A^{n}}{n!}\\right)\\left(\\sum\\frac{B^{n}}{n!}\\right)\\\\\n          &=\\sum^{\\infty}_{m=0}\\sum^{\\infty}_{n=0}\\frac{A^{m}B^{n}}{m!n!}\\\\\n          &=\\sum^{\\infty}_{l=0}\\sum^{l}_{m=0}\\frac{A^{m}B^{l-m}}{m!(l-m)!}\\\\\n          &=\\sum^{\\infty}_{l=0}\\frac{1}{l!}\\sum^{l}_{m=0}\\frac{l!}{m!(l-m)!}A^{m}B^{l-m}\\\\\n          &=\\sum^{\\infty}_{l=0}\\frac{(A+B)^{l}}{l!}\\\\\n          &= e^{A+B}\\end{align*}$$\nNote:A and B have to commute.\nAlso, I set l=m+n.  I did this because we want to use the binomial theorem to simplify this.", "meta": {"post_id": 356752, "input_score": 23, "output_score": 54, "post_title": "Show that $ e^{A+B}=e^A e^B$"}}
{"input": "Are there some good overviews of basic facts about images and inverse images of sets under functions?", "output": "This big list is included in Appendix A of Introduction to Topological Manifolds by John M. Lee:\nLet $f:X\\to Y$ and $g:W\\to X$ be maps, and suppose $R\\subseteq W$, $S,S'\\subseteq X$, and $T,T'\\subseteq Y$.\n\n$T\\supseteq f(f^{-1}(T))$.\n$T\\subseteq T' \\Rightarrow f^{-1}(T)\\subseteq f^{-1}(T')$.\n$f^{-1}(T\\cup T')=f^{-1}(T)\\cup f^{-1}(T')$.\n$f^{-1}(T\\cap T')=f^{-1}(T)\\cap f^{-1}(T')$.\n$f^{-1}(T\\setminus T')=f^{-1}(T)\\setminus f^{-1}(T')$.\n$S\\subseteq f^{-1}(f(S))$.\n$S\\subseteq S' \\Rightarrow f(S)\\subseteq f(S')$.\n$f(S\\cup S')=f(S)\\cup f(S')$.\n$f(S\\cap S')\\subseteq f(S)\\cap f(S')$.\n$f(S\\setminus S')\\supseteq f(S)\\setminus f(S')$.\n$f(S)\\cap T=f(S\\cap f^{-1}(T))$.\n$f(S)\\cup T\\supseteq f(S\\cup f^{-1}(T))$.\n$S\\cap f^{-1}(T)\\subseteq f^{-1}(f(S)\\cap T)$.\n$S\\cup f^{-1}(T)\\subseteq f^{-1}(f(S)\\cup T)$.\n$(f\\circ g)^{-1}(T)=g^{-1}(f^{-1}(T))$.\n$(f\\circ g)(R)=f(g(R))$.", "meta": {"post_id": 359693, "input_score": 204, "output_score": 53, "post_title": "Overview of basic results about images and preimages"}}
{"input": "I want to show that if a function $f:[a,b]\\rightarrow \\mathbb R$ satisfies a H\u00f6lder condition of order $\\alpha > 1 $ then it is constant.\nThe way I think of it is as follows:\n$$|f(x) - f(y)| < K|x-y|^\\alpha$$\n$$\\frac{|f(x) - f(y)|} {|x-y]}  < K|x-y|^{\\alpha -1}$$\n$$\\lim_{y\\rightarrow x} \\frac{|f(x) - f(y)|} {|x-y]} \\le \\lim_{y\\rightarrow x}  K|x-y|^{\\alpha -1} =0 $$\nAs the limit is $0$, we can remove the modulus, so we get:\n$$\\lim_{y\\rightarrow x} \\frac{f(x) - f(y)} {x-y} = 0$$\nSo $f$ is derivable and $f'(x) = 0$ for all $x$ in $[a,b]$.\nNote that the reason we can add the limit $y\\rightarrow x$ is because $[a,b]$ is closed in $\\mathbb R$.\nHowever, the question gives as a hint using the mean value theorem. I am not sure why one should do that. You would first have to prove that $f$ is derivable in a similar manner to what I did, and then prove that $f$ is constant. Or is there a simpler way to do it and I am missing it?\nAlso please inform me of any mistakes I did in the proof (if any)/\nThank you!", "output": "Or is there a simpler way to do it (...)?\n\nThis was asked on the site before and requires no differentiability argument nor the mean value theorem, nor anything but the hypothesis, really...\nFor every $x\\ne y$, split the interval $[x,y]$ into $n$ subintervals of length $\\frac1n\\cdot|x-y|$. The H\u00f6lder condition on each interval yields a bound $K\\cdot \\left(\\frac1n\\cdot|x-y|\\right)^\\alpha$. By the triangular inequality used $n-1$ times,\n$$\n|f(x)-f(y)|\\leqslant n\\cdot K\\cdot \\left(\\tfrac1n\\cdot|x-y|\\right)^\\alpha=K\\cdot|x-y|^\\alpha\\cdot \\frac1{n^{\\alpha-1}}.\n$$\nNow, consider the limit $n\\to\\infty$.", "meta": {"post_id": 361400, "input_score": 19, "output_score": 39, "post_title": "Function on $[a,b]$ that satisfies a H\u00f6lder condition of order $\\alpha > 1 $ is constant"}}
{"input": "Given a countable collection of metric spaces $\\{(X_n,\\rho_n)\\}_{n=1}^{\\infty}$.  Form the Cartesian Product of these sets $X=\\displaystyle\\prod_{n=1}^{\\infty}X_n$, and define $\\rho:X\\times X\\rightarrow\\mathbb{R}$ by\n$$\\rho(x,y)=\\displaystyle\\sum_{n=1}^{\\infty}\\frac{\\rho_n(x_n,y_n)}{2^n[1+\\rho_n(x_n,y_n)]}.$$\nShow that $\\rho$ is a metric on $X$ whose induced topology is equivalent to the product topology on $X$.\nSo basically what this problem is saying is that there's a canonical way to define a metric on the countable product of metric spaces.  I showed in a previous problem that the topology induced by $\\rho_n$ is equivalent to that induced by $\\frac{\\rho_n(x_n,y_n)}{1+\\rho_n(x_n,y_n)}$.  And thus we can go ahead and just assume that $\\rho_n< 1$ for all $n$ and replace our infinite series by\n$$\\rho(x,y)=\\displaystyle\\sum_{n=1}^{\\infty}\\frac{\\rho_n(x_n,y_n)}{2^n}.$$\nNow comes the interesting part: how should I go about showing that the product topology on $X$ and the topology induced by $\\rho$ are equivalent?  \nThe basis for the product topology given to me in my book's definition is that of cartesian products made up the $X_n$ except for finitely many which are $O_n$ for some open subset of $X_n$.  However I believe I was able to improve upon this and show that I could decompose these into a basis where the $O_n$ were all open balls induced by their respect $\\rho_n$ metric.\nFor $\\rho$ I'm using the basis of open balls that it induces, as I can see no other reasonable choice.  \nHowever I can't seem to match these two bases up.  There are many different points $\\{x_n\\}\\in X$ which make my infinite series less than a certain value and there is so much freedom in which terms in the series I choose to reduce in size that it seems hopeless to try and fit an open ball induced by $\\rho$ into any one of my basis elements for the product topology.  \nIs there a more appropriate strategy for proving these two topologies are equivalent?", "output": "I'll use the $\\rho_n$ that are bounded by 1, and $\\rho(x,y) = \\sum_n \\frac{\\rho_n(x_n,y_n)}{2^n}$ metric on the product $X = \\prod_n X_n$.\nLet $O$ be a basic open product set, so $O = \\prod_n O_n$, all $O_n$ are open in $X_n$ and where we have a finite set $F \\subset \\mathbb{N}$ such that $n \\notin F$ iff $X_n = O_n$. We want to show it is open in the $\\rho$-topology, so pick $x \\in O$, and we want to find $r>0$ such that $B_{\\rho}(x, r) \\subset O$. This would show that all basic product open sets are $\\rho$-open, and thus all product open sets are $\\rho$-open.\nNow, for every $n \\in F$, we have that $x_n \\in O_n$, which is a (non-trivial) open subset in $X_n$, so we have $r_n > 0$ such that $B_{\\rho_n}(x_n, r_n) \\subset O_n$, from the fact that the topology on $X_n$ is induced by the metric $\\rho_n$. As we have finitely many $r_n$ to consider, we can find $0 < r < 1$ such that $r \\le \\frac{r_n}{2^n}$ for all $n \\in F$.\nThe claim now is that this $r$ is as required, in the sense that $B_{\\rho}(x, r) \\subset O$.\nTo see this, take any $y$ with $\\rho(x,y) < r$. For $n \\in F$, we know that $\\frac{\\rho_n(x_n, y_n)}{2^n} \\le \\rho(x, y) < r \\le \\frac{r_n}{2^n}$, which implies that for such $n$ we have that $\\rho_n(x_n, y_n) < r_n$, and so $y_n \\in B_{\\rho_n}(x_n, r_n) \\subset O_n$. Hence, for all $n \\in F$, $y_n \\in O_n$, and as the other $O_n$ equal $X_n$ by the form of $O$, we have that indeed $y \\in O$, and as $y$ was arbitrary, $B_\\rho(x, r) \\subset O$, as required.\nNow for the other part: we start with an open ball $B_\\rho(x,r)$, a basic open subset of the $\\rho$-topology, for some arbitrary $x \\in X$ and $r>0$, and try to find a basic open subset in the product topology $O$ such that $x \\in O \\subset B_\\rho(x,r)$. This would then show that any $\\rho$-open ball is open in the product topology and would show the other inclusion we need: every $\\rho$-open set is product open.\nThe intuition is that the tail of a series like the one that defines $\\rho$ is essentially irrelevant (we can get it as small as we like) and this corresponds to the idea that basic open subsets only depend on finitely many non-trivial open sets. So we first pick $N \\in \\mathbb{N}$ such that $\\frac{1}{2^N} < \\frac{r}{2}$. This $N$ defines our tail. For $1 \\le k \\le N$ we consider the open balls $O_k = B_{\\rho_k}(x_k, \\frac{r}{2N})$, and we set $O_k = X_k$ for $k \\ge N+1$. \nThe claim now is that $O = \\prod_k O_k \\subset B_\\rho(x, r)$, as required. Note that $O$ is indeed a basic open subset in the product topology on $X$ and $x \\in O$. To verify the latter claim, we simply estimate: let $y$ be in $O$, then for $k \\le N$, $\\rho_k(x_k, y_k) < \\frac{r}{2N}$, so $$\\sum_{k=1}^{N} \\frac{\\rho_k(x_k,y_k)}{2^k} \\le \\sum_{k=1}^{N} \\rho_k(x_k,y_k) < N\\cdot \\frac{r}{2N} = \\frac{r}{2}\\mbox{,}$$ while $$\\sum_{k=N+1}^{\\infty} \\frac{\\rho_k(x_k, y_k)}{2^k} \\le \\sum_{k=N+1}^{\\infty} \\frac{1}{2^k} = \\frac{1}{2^N} < \\frac{r}{2}\\mbox{.}$$ \nPutting it together, we indeed get that for $y \\in O$ we have $\\rho(x,y) < \\frac{r}{2} + \\frac{r}{2} = r$, as required.", "meta": {"post_id": 361778, "input_score": 54, "output_score": 50, "post_title": "Show that the countable product of metric spaces is metrizable"}}
{"input": "What are the basic differences between $\\mathbb C$ and $\\mathbb R^2$?\nThe points in these two sets are written as ordered pairs, I mean the structure looks similar to me. So what is the reason to denote these two sets differently?", "output": "The answer to this question depends on what you mean by $\\mathbf{R}^2$. You can write $\\mathbf{R}\\times\\mathbf{R}$, but the \"$\\times$\" can have several different meanings depending on which category you are working in.\nAs sets:\nIf you view $\\mathbf{R}\\times\\mathbf{R}$ and $\\mathbf{C}$ as sets, this means you are ignoring any possible structure on these things besides their elements (so forget about multiplication, addition, anything.) From this perspective, $\\mathbf{R}\\times\\mathbf{R}$ and $\\mathbf{C}$ are the same object (in technical terms they are isomorphic in the category of sets) because there is a bijection\n$$(a,b)\\ \\leftrightarrow\\ a+bi.$$\nA key thing to note about this is that \"$\\times$\" refers to a direct product of sets, i.e. the Cartesian product. This will change as we add more structure.\nAs real vector spaces:\nYou can give each of $\\mathbf{R}\\times\\mathbf{R}$ and $\\mathbf{C}$ the structure of a real vector space, meaning you can add vectors and multiply by real numbers. Then from the theory of linear algebra, we know that $\\mathbf{R}\\times\\mathbf{R}$ has a basis of $\\{(1,0), (0,1)\\}$ and $\\mathbf{C}$ has a (real) basis of $\\{1, i\\}$. Since these real vector spaces both have dimension 2, they are isomorphic (in the linear algebra sense, i.e. in the category of $\\mathbf{R}$-modules). So from this perspective they are again the same object.\nNote here that $\\mathbf{R}\\times\\mathbf{R}$ can be interpreted as $\\mathbf{R}\\oplus \\mathbf{R}$ which may be more familiar to linear algebra students. The point is that now we are requiring more from the operation (it has to preserve addition of vectors now).\nAs rings:\nHere is where the difference comes in. We can think of $\\mathbf{R}$ and $\\mathbf{C}$ as rings, meaning we can add and multiply elements together according to some axioms. Then if you write $\\mathbf{R}\\times\\mathbf{R}$, you mean a direct product in the category of rings, so now multiplication in $\\mathbf{R}\\times\\mathbf{R}$ has to satisfy\n$$(a,b)\\cdot (c,d)=(ac,bd).$$\nBut in particular this means things like\n$$(1,0)\\cdot (0,1)=(0,0),$$\nwhich means it is possible for two nonzero things to have a product zero. In contrast, if $z_1z_2=0$ in $\\mathbf{C}$, then either $z_1=0$ or $z_2=0$. In this way, $\\mathbf{R}\\times\\mathbf{R}$ and $\\mathbf{C}$ have fundamentally different behavior as rings. Because of this, there is no isomorphism of rings between the two objects.\nAs fields:\nA field is a commutative ring with more structure (we can invert multiplication for nonzero things). It turns out that $\\mathbf{C}$ can be given the structure of a field because $z^{-1}$ exists for any nonzero $z\\in\\mathbf{C}$, but $\\mathbf{R}\\times\\mathbf{R}$ cannot be a field because equations like $(1,0)\\cdot (0,1)=(0,0)$ mess everything up (try to cancel something from the left side).\n\n\ntl;dr -- You have to specify what you mean by \"$\\times$\". $\\mathbf{C}$ and $\\mathbf{R}\\times\\mathbf{R}$ are exactly the same until you start saying you want to do things like multiply elements together.", "meta": {"post_id": 364044, "input_score": 41, "output_score": 73, "post_title": "Difference between $\\mathbb C$ and $\\mathbb R^2$"}}
{"input": "Say I have two topological spaces given by $(X,\\mathscr{T}_X)$ and $(Y,\\mathscr{T}_Y)$ where $Y$ is Hausdorff. In addition say I have a function $f:X\\rightarrow Y$, and let it be continuous. I want to show that $Gr(f):=\\{(x,f(x))\\mid x\\in X\\}$ is a closed subset of $(X\\times Y)$. \nIn answering this question, could you also provide the \"chain of thought\" that brought you to the solution. I am aware of what the continuity of the function and the fact that $Y$ is Hausdorff provides me in terms of definition. I am however, finding it hard to find a -link- towards answering the question. Thank you all in advance for your help.", "output": "Let $G=\\operatorname{Gr}(f)$. In this case the easiest way to show that $G$ is closed in $X\\times Y$ is to show that its complement is open, so let $\\langle x,y\\rangle\\in(X\\times Y)\\setminus G$. Since $\\langle x,y\\rangle\\notin G$, $y\\ne f(x)$. Thus, $y$ and $f(x)$ are distinct points in $Y$. And $Y$ is Hausdorff, so there are disjoint open sets $U$ and $V$ in $Y$ such that $y\\in U$ and $f(x)\\in V$. Finally, $f$ is continuous, so there is an open nbhd $W$ of $x$ such that $f[W]\\subseteq V$. \nUp to here I\u2019ve just done what comes naturally: I\u2019ve used the hypotheses in the most obvious way without really thinking about where I\u2019m going. I\u2019m trying to show that $\\langle x,y\\rangle$ has an open nbhd contained in $(X\\times Y)\\setminus G$; do I have an open nbhd of $\\langle x,y\\rangle$ hanging about anywhere? Yes: by definition of the product topology, $W\\times U$ is an open nbhd of $\\langle x,y\\rangle$ in $X\\times Y$. If I\u2019m lucky, this nbhd $W\\times U$ will turn out to be disjoint from $G$, showing that $\\langle x,y\\rangle$ is not in the closure of $G$. And since $\\langle x,y\\rangle$ was an arbitrary point of $(X\\times Y)\\setminus G$, this would show that $(X\\times Y)\\setminus G$ is open and hence that $G$ is closed.\nLet $\\langle z,f(z)\\rangle$ be any point of $G$. If $z\\notin W$, then clearly $\\langle z,f(z)\\rangle\\notin W\\times U$. If $z\\in W$, then $f(z)\\in V$, so $f(z)\\notin U$, and therefore $\\langle z,f(z)\\rangle\\notin W\\times U$. Thus, in all cases $\\langle z,f(z)\\rangle\\notin W\\times U$, and it follows that $(W\\times U)\\cap G=\\varnothing$. Thus, each point of $(X\\times Y)\\setminus G$ has an open nbhd disjoint from $G$, and $G$ is therefore closed.", "meta": {"post_id": 367155, "input_score": 33, "output_score": 73, "post_title": "Why is the graph of a continuous function to a Hausdorff space closed?"}}
{"input": "Quivers are directed graphs where loops and multi-arrows are allowed. And we can talk about representations of quivers by assigning each vertex a vector space and each arrow a homomorphism. Moreover, Gabriel gives a complete classification of quivers of finite type using just five Dynkin diagrams.\nAlthough these are both deep and surprising, but I am not sure why quivers deserve so much attention. The only potential application I can think of (although highly unlikely to be true) that they might be useful to answer certain questions in category theory since the notion of quivers are similar to categories, and a representation is very much like a functor from a quiver to some $\\mathcal{k}$-$\\operatorname{vect}$. \nSo I wonder whether someone can give a hint why quivers deserve so much attention? Do they naturally show up in problems? And do representations of quivers really help to solve these problems?", "output": "Morita Equivalence This is a supplement to the aspect of quiver representations mentioned in Alistair's answer. Every associative finite dimensional $k$-algebra $A$ is Morita equivalent to a path algebra $kQ/I$ (this is another Gabriel's theorem). In particular, you have a very nice equivalence (so nice that the functors giving such equivalence are given by tensoring finitely-generated projective bimodules) of abelian categories $A\\text{-Mod}$ and $kQ/I\\text{-Mod}$. So basically (almost) everything you want to know about representations of $A$ ($A$-modules) can be learnt from studying $kQ/I$-modules. And studying quiver representation is arguably much easier because path algebras are basic, meaning all simple modules are one-dimensional. This is equivalent to saying all projective indecomposable are only of multiplicity $1$ in $kQ/I$, one can say that this makes the homological behaviour of the modules much easier to study. In particular, many things can be done combinatorially.\nAuslander-Reiten Theory This part does not relate directly to \"why quiver representation\", but to \"why quivers\".  It turns out we can treat abelian categories (or in fact functorially finite categories) pretty much the same way as we treat an algebra: you can talk about irreducible maps. In particular, there is a combinatorial gadget called the Auslander-Reiten quiver, where vertices are in correspondence with indecomposable $A$-module and arrows are given by irreducible maps. In such a way, one can \"visualise\" the category nicely with a very nice form of quivers.  And surprisingly, the form of quiver appearing in this construction also follows Dynkin classification.\nCluster Theory One of the most exciting developments in representation theory in the last decade is the cluster theory introduced by Fomin and Zelevinsky. The centre of the theory is an algebra called the cluster algebra, which is some sort of dual to the picture we see in the theory of Lie algebras (I do not know the exact argument to this). But the algebraic setting for clusters theory is pretty difficult to work with sometimes, and it turns out we can use an operation on quivers called a quiver mutation to substitute basis elements of the cluster algebra. Now people \"categorify\" this setting (which is in fact an incarnation of the Auslander-Reiten theory I mentioned above) and found out that we can use the derived category of the module category of the path algebra $kQ$ to study properties of cluster algebras.\nHall Algebras There is one construction of algebra called the Hall algebra of an abelian category. Ringel proved the amazing theorem in the 90s that if you take a Dynkin quiver $Q$ and consider the module category $kQ\\text{-mod}$, then take the Hall algebra $H_Q$ of $kQ\\text{-mod}$, it turns out $H_Q$ is isomorphic as an Hopf algebra to one half of the quantum group of the Lie algebra of type $Q$. I.e. studying quiver representations allow us to dig out more unknown properties of quantum group.\nQuiver Varieties & Geometric Representation Theory (This is the impression I have got. Please correct me if I am wrong) If you recall from the proof of Gabriel's theorem on the classification of finite-type (unquotiented) path algebras, you will see there is some action of general linear group on the quiver. Nakajima extended this idea and developed a whole new approach for doing representation theory via geometric methods, using the so called Nakajima quiver variety.", "meta": {"post_id": 373578, "input_score": 58, "output_score": 44, "post_title": "Why are (representations of ) quivers such a big deal?"}}
{"input": "$$(26+15\\cdot\\sqrt3)^{1/3}+(26-15\\cdot\\sqrt3)^{1/3}$$\nI'm trying to get the result of this number. Through some algebra I found that it is close to $52^{1/3}$. Through some observation I found that it is a root of this cubic equation $x^3-3x-52=0$ and I found that the only real solution of that eq. is $4$ so now I know that the number I'm looking for is $4$ (close to my first try $52^{1/3}$). My question is, is there any algebraic process to get from the original expression and simplify it to $4$?\nThank you all very much in advance.", "output": "$26 + 15 \\sqrt{3} = 8 + 12 \\sqrt{3} + 18 + 3 \\sqrt{3} = 2^3 + 3 \\cdot 2^2 \\sqrt{3} + 3 \\cdot 2 \\sqrt{3}^2 + \\sqrt{3}^3 = (2 + \\sqrt{3})^3.$\n$26 - 15 \\sqrt{3} = 8 - 12 \\sqrt{3} + 1 8- 3 \\sqrt{3} = 2^3 - 3 \\cdot 2^2 \\sqrt{3} + 3 \\cdot 2 \\sqrt{3}^2 - \\sqrt{3}^3 = (2 - \\sqrt{3})^3.$\n$(26 + 15 \\sqrt{3})^{\\frac{1}{3}} + (26 - 15 \\sqrt{3})^{\\frac{1}{3}} = 2 + \\sqrt{3} + 2 - \\sqrt{3} = 4.$", "meta": {"post_id": 373918, "input_score": 11, "output_score": 40, "post_title": "What is $(26+15(3)^{1/2})^{1/3}+(26-15(3)^{1/2})^{1/3}$?"}}
{"input": "Suppose that $f$ is a real-valued function on $\\Bbb R$ whose derivative exists at each point and is bounded. Prove that $f$ is uniformly continuous.", "output": "Since $f'$ is bounded then there's $M>0$ s.t.\n$$|f'(x)|\\leq M\\quad \\forall x\\in\\mathbb{R}$$\nhence by mean value theorem we find\n$$|f(x)-f(y)|\\leq M|x-y|\\quad \\forall x,y\\in\\mathbb{R}$$\nso $f$ is a lipschitzian function on $\\mathbb{R}$ and therefore it's s uniformly continuous on $\\mathbb{R}$.", "meta": {"post_id": 377531, "input_score": 25, "output_score": 34, "post_title": "Prove that a function whose derivative is bounded is uniformly continuous."}}
{"input": "Each rational number (fraction) can be written as a decimal periodic number. Is there a method or hint to derive the length of the period of an arbitrary fraction? For example $1/3=0.3333...=0.(3)$ has a period of length 1. \nFor example: how to determine the length of a period of $119/13$?", "output": "This answer seeks to explain why Ross Millikan's answer works, and provides further information on techniques to speed up the process of seeking the period:\nConsider the fraction $\\frac17$. The decimal expansion of this is\n$$\n\\frac17 = 0.\\overline{142857}\n$$\nfor a period of 6. Now consider what happens when we multiply it by $10^6$:\n$$\n10^6\\times\\frac17 = 142857.\\overline{142857}\n$$\nSubtracting the original fraction from this gives\n$$\n(10^6-1)\\times\\frac17 = 142857\n$$\nAnd so, we have\n$$\n\\frac17 = \\frac{142857}{10^6-1}\n$$\nAs you can see, the denominator is one less than a power of 10, and the power is the period of the decimal expansion. This is no accident, and works for any fraction - if you can rewrite it in this form, the denominator reveals the period.\nNow, rearrange the equation:\n$$\n10^6-1 = 142857\\times 7\n$$\nSo $10^6$ must be one more than a multiple of 7 (or, more generally, $10^n$ must be one more than a multiple of $d$, where $d$ is the denominator of the fraction and $n$ is the period of the decimal expansion) - indeed, it must be the smallest power of 10 (larger than 1) that has this property.\nAs such, we can use modular arithmetic to look for the period. Since $a\\times d\\equiv 0 \\pmod d$, we have that $10^n-1\\equiv0 \\pmod d$, or\n$$10^n \\equiv 1\\pmod d$$\nAnd therefore you can just look for the smallest $n>0$ satisfying this.\nOf course, there are other approaches to gain the same result, but they're all fundamentally variants of the same idea. That said, if you can factor $\\phi(d)$ - the euler totient function of the denominator - then you can accelerate the process of looking for the smallest $n$. For example, when checking 13, you have $\\phi(13)=12$, so $n\\in\\{1,2,3,4,6,12\\}$ (as these are the factors of 12) - this can save you a lot of computation (especially where $\\phi(d)$ has only a few large factors and 2).\nFor example, $\\phi(167)=166 = 2\\times83$, so $n\\in\\{1,2,83,166\\}$. Therefore, we need to check only these four, and we can do it quite efficiently. Obviously, neither $10$ nor $100=10^2$ are equivalent to 1 mod 167, so we only need to actually check 83. For this we can use binary exponentiation. Note that $83 = 2^6 + 2^4 + 2^1 + 2^0$. So we can write\n$$\\begin{align}\n10^{83} &= 10^{2\\times(2^5 + 2^3 + 1)}\\times 10\\\\\n&= (10^{2^3\\times(2^2+1)}\\times 10)^2 \\times 10\\\\\n&= ((10^{2^2}\\times10)^{2^3}\\times 10)^2 \\times 10\n\\end{align}$$\nSo, working in modular arithmetic, we can go\n$$\\begin{align}\n10^{83} &\\equiv ((10^{2^2}\\times10)^{2^3}\\times 10)^2 \\times 10 \\mod 167\\\\\n&\\equiv ((100^2\\times 10)^{2^3}\\times 10)^2\\times10\\mod167\\\\\n&\\equiv ((147\\times 10)^{2^3}\\times 10)^2\\times10\\mod167\\\\\n&\\equiv (134^{2^3}\\times 10)^2\\times10\\mod167\\\\\n&\\equiv (87^{2^2}\\times 10)^2\\times10\\mod167\\\\\n&\\equiv (54^2\\times 10)^2\\times10\\mod167\\\\\n&\\equiv (77\\times 10)^2\\times10\\mod167\\\\\n&\\equiv 102^2\\times10\\mod167\\\\\n&\\equiv 50\\times10\\mod167\\\\\n&\\equiv 166\\mod167\n\\end{align}$$\nThis is the same as $-1\\pmod{167}$, so $n=166$ is the only possible period, and $\\frac1{167}$ has a period of 166.\nAlso note that you don't actually have to expand out the product like that. You can just write the number in binary ($83_{10} = 1010011_2$), then work through the binary digits left-to-right - start with 1, and for each digit, $b$, multiply by $10^b$. Square it after each digit except the last one.", "meta": {"post_id": 377683, "input_score": 36, "output_score": 39, "post_title": "Length of period of decimal expansion of a fraction"}}
{"input": "I'm trying to solve the well known Coupon Collector's Problem by explicitly finding the probability distribution (so far all the methods I read involve using some sort of trick). However, I'm not having much luck getting anywhere as combinatorics is not something I'm particularly good at. \nThe Coupon Collector's Problem is stated as:\nThere are $m$ different kinds of coupons to be collected from boxes. Assuming each type of coupon is equally likely to be found per box, what's the expected amount of boxes one has to buy to collect all types of coupons?\nWhat I'm attempting:\nLet $N$ be the random variable associated with the number of boxes one has to buy to find all coupons. Then $P(N=n)=\\frac{|A_n|}{|\\Omega _n|}$, where $A_n$ is the set of all outcomes such that all types of coupons are observed in $n$ buys, and $\\Omega _n$ is the set of all the possible outcomes in $n$ buys. I think $|\\Omega _n| = m^n$, but I'm not even sure about that anymore, as all my attempts so far led to garbage probabilities that either diverged or didn't sum up to 1.", "output": "As Henry pointed out in a comment, the probability has been determined elsewhere as\n$$\n\\def\\stir#1#2{\\left\\{#1\\atop#2\\right\\}}\nP(N=n)=\\frac{m!}{m^n}\\stir{n-1}{m-1}\\;,\n$$\nwhere\n$$\\stir nk=\\frac1{k!}\\sum_{j=0}^k(-1)^{k-j}\\binom kjj^n$$\nis a Stirling number of the second kind and counts the number of partitions of a set of $n$ labeled objects into $k$ non-empty unlabeled subsets.\nTo get the expected value, it's slightly more convenient to work with the probability\n$$\nP(N\\gt n)=1-\\frac{m!}{m^n}\\stir nm\\;,\n$$\nwhich can be derived in much the same manner: There are $m^n$ sequences of length $n$; choose one of $\\stir nm$ partitions into $m$ non-empty subsets and one of $m!$ assignments of the coupons types to the subsets.\nThen\n\\begin{align}\nE[N]={}&\\sum_{n=0}^\\infty P(N\\gt n)\\\\\n={}&\\sum_{n=0}^\\infty\\left(1-\\frac{m!}{m^n}\\stir nm\\right)\\\\\n={}&\\sum_{n=0}^\\infty\\left(1-\\frac{m!}{m^n}\\frac1{m!}\\sum_{j=0}^m(-1)^{m-j}\\binom mjj^n\\right)\\\\\n={}&\\sum_{n=0}^\\infty\\frac1{m^n}\\sum_{j=0}^{m-1}(-1)^{m-j+1}\\binom mjj^n\\\\\n={}&\\sum_{j=0}^{m-1}(-1)^{m-j+1}\\binom mj\\sum_{n=0}^\\infty\\frac{j^n}{m^n}\\\\\n={}&\\sum_{j=1}^m(-1)^{j+1}\\binom mj\\sum_{n=0}^\\infty\\frac{(m-j)^n}{m^n}\\\\\n={}&\\sum_{j=1}^m(-1)^{j+1}\\binom mj\\frac mj\\\\\n={}&-m\\sum_{j=1}^m\\int_0^{-1}\\mathrm dq'\\binom mjq'^{j-1}\\\\\n={}&-m\\int_0^{-1}\\mathrm dq'\\sum_{j=1}^m\\binom mjq'^{j-1}\\\\\n={}&-m\\int_0^{-1}\\mathrm dq'\\frac{(1+q')^m-1}{q'}\\\\\n={}&-m\\int_0^{-1}\\mathrm dq'\\frac{(1+q')^m-1}{(1+q')-1}\\\\\n={}&-m\\int_0^{-1}\\mathrm dq'\\sum_{j=0}^{m-1}(-q')^j\\\\\n={}&-m\\sum_{j=0}^{m-1}\\int_0^{-1}\\mathrm dq'(-q')^j\\\\\n={}&m\\sum_{j=1}^m\\frac1j\\;.\n\\end{align}\nI'll leave it to you to decide whether this counts as \"using some sort of trick\". :-)", "meta": {"post_id": 379525, "input_score": 28, "output_score": 39, "post_title": "Probability distribution in the coupon collector's problem"}}
{"input": "Is any closed-form representation known for the sum $\\sum\\limits_{n=1}^{\\infty}\\frac{\\mu(n)\\log n}{n^2}$, where $\\mu(n)$ is the M\u00f6bius $\\mu$-function?", "output": "You can use a series for the reciprocal Riemann $\\zeta$-function: $\\zeta(s)^{-1}=\\sum\\limits_{n=1}^{\\infty}\\mu(n)n^{-s}$, then take a derivative with respect to $s$ and let $s=2$. This gives you $\\sum\\limits_{n=1}^{\\infty}\\frac{\\mu(n)\\log n}{n^2}=\\frac{\\zeta'(2)}{\\zeta(2)^2}$. It is well-known that $\\zeta(2)=\\frac{\\pi^2}{6}$, and a closed form of the derivative is given in OEIS A073002: \n$\\zeta'(2)=\\frac{\\pi^2}{6}\\left(\\gamma+\\log(2\\pi)-12 \\log A\\right)$, \nwhere $\\gamma$ is the Euler-Mascheroni constant, and $A$ is the Glaisher-Kinkelin constant. Taking it all together, the result is:\n$\\sum\\limits_{n=1}^{\\infty}\\frac{\\mu(n)\\log n}{n^2}=\\frac{6}{\\pi^2}\\left(\\gamma+\\log(2\\pi)-12 \\log A\\right)$.", "meta": {"post_id": 380798, "input_score": 26, "output_score": 37, "post_title": "Is any closed-form representation known for the sum $\\sum\\limits_{n=1}^{\\infty}\\frac{\\mu(n)\\log n}{n^2}$?"}}
{"input": "I remember there is a special rule for this kind of function, but I can't remember what it was. \nDoes anyone know?", "output": "Caveat: I'm using the normalization $\\hat f(\\omega) = \\int_{-\\infty}^\\infty f(t)e^{-it\\omega}\\,dt$.\nA cute way to to derive the Fourier transform of $f(t) = e^{-t^2}$ is the following trick: Since $$f'(t) = -2te^{-t^2} = -2tf(t),$$\ntaking the Fourier transfom of both sides will give us\n$$i\\omega \\hat f(\\omega) = -2i\\hat f'(\\omega).$$\nSolving this differential equation for $\\hat f$ yields\n$$\\hat f(\\omega) = Ce^{-\\omega^2/4}$$\nand plugging in $\\omega = 0$ finally gives\n$$ C = \\hat f(0) = \\int_{-\\infty}^\\infty e^{-t^2}\\,dt = \\sqrt{\\pi}.$$\nI.e. $$ \\hat f(\\omega) = \\sqrt{\\pi}e^{-\\omega^2/4}.$$", "meta": {"post_id": 381597, "input_score": 15, "output_score": 36, "post_title": "What is the Fourier transform of $f(x)=e^{-x^2}$?"}}
{"input": "The set of positive semidefinite symmetric real matrices forms a cone. We can define an order over the set of matrices by saying $X \\geq Y$ if and only if $X - Y$ is positive semidefinite. I suspect that this order does not have the lattice property, but I would still like to know which matrices are candidates for the meet and join of two matrices.\nIn other words, let $P$ be the cone of positive semidefinite matrices. Is there a nice characterization of the set $(X+P)\\cap (Y+P)$, for two given matrices? What are the minimal points in this intersection?", "output": "Geometric Interpretation. Consider a positive definite matrix $A$. It defines the ellipsoid\n$${\\cal E}_A = \\{u: u^T A u \\leq 1\\}.$$\nNote that the correspondence between $A$ and ${\\cal E}_A$ is one-to-one. Moreover, $A\\succeq B$ if and only if ${\\cal E}_B$ contains ${\\cal E}_A$. Using this fact, we can give the following characterization.\n\n$C \\succeq A$ and $C \\succeq B$ if and only if ${\\cal E}_C \\subset {\\cal E}_A \\cap {\\cal E}_B$.\n\nSimilarly,\n\n$C \\preceq A$ and $C \\preceq B$ if and only if ${\\cal E}_C \\supset {\\cal E}_A \\cup {\\cal E}_B$.\n\nNow a matrix $C$ is a minimal matrix s.t. $C \\succeq A$ and $C \\succeq B$ if and only if ${\\cal E}_C \\subset {\\cal E}_A \\cap {\\cal E}_B$ and there is no ellipsoid \u201csandwiched\u201d between ${\\cal E}_C$ and ${\\cal E}_A \\cap {\\cal E}_B$. It is easy to see that that happens iff $\\partial{\\cal E}_C$ intersects $\\partial{\\cal E}_A$ by a $k$-dimensional ellipsoid and   $\\partial{\\cal E}_C$ intersects $\\partial{\\cal E}_B$ by an $n-k$ dimensional ellipsoid. This can be stated in terms of matrices $A$, $B$, and $C$.\n\nConsider a minimal $C$ s.t. $C \\succeq A$ and $C \\succeq B$. Then there are two subspaces $U$ and $V$ with ${\\mathbb R}^n = U \\oplus V$ such that $u^T Cu = u^T Au$ for $u\\in U$ and $u^T Cu = u^T Bu$ for $u\\in V$; in particular, $\\operatorname{rank}(C-A) + \\operatorname{rank}(C-B) \\leq n$.\n\nVisualization. The following animation shows ellipses ${\\cal E}_C$ inscribed in ${\\cal E}_A \\cap {\\cal E}_B$ in two dimensions. Every ellipse ${\\cal E}_C$ corresponds to a minimal matrix $C$ ($C \\succeq A$ and $C \\succeq B$)\n\nAnd this animation shows minimum ellipses ${\\cal E}_C$ containing ${\\cal E}_A \\cup {\\cal E}_B$.\n\nI'm afraid that there is no more explicit characterization of sets $\\{C: C \\succeq A \\text{ and } C \\succeq B\\}$ and $\\{C: C \\preceq A \\text{ and } C \\preceq B\\}$. \nCorrespondence between \u201cmeet\u201d and \u201cjoin\u201d matrices. Note that if $C_1$ is a \u201cjoin\u201d then $C_2 = A+B-C_1$ is a \u201cmeet\u201d and vice versa. That follows from the fact that $C_2 \\preceq A$ iff $A+B-C_1 \\preceq A$ iff $B \\preceq C_1$; similarly, $C_2 \\preceq B$ iff $A \\preceq C_1$.\nCharacterization for $2\\times 2$ matrices. If $A$ and $B$ are $2\\times 2$ matrices (s.t. $A\\not\\preceq B$ and $A\\not\\succeq B$) then meet and join matrices $C$ must satisfy the following equations $\\det(C-A) = 0$ and $\\det(C-A) =0$. The set of symmetric matrices that satisfy this system of equations forms a one dimensional curve in the space of matrices. Let us write \n$$C_{xyz} = \\begin{pmatrix}x&y\\\\y&z\\end{pmatrix}.$$ Then the set\n$$\\{(x,y,z): \\det(C_{xyz} - A) = \\det(C_{xyz} - B) = 0\\}$$\nis a hyperbola (that lies in a plane in ${\\mathbb R}^3$). Points on one branch of the hyperbola correspond to join matrices; points on the other branch correspond to meet matrices.\nNotes. Note that by changing the basis we can always assume that $A=I$ and $B$ is a diagonal matrix, but I don't think that this observation leads to a very explicit characterization of  $\\{C: C \\succeq A \\text{ and } C \\succeq B\\}$. In particular, $C$ does not have to be a diagonal matrix. For example, let\n$$A=\\begin{pmatrix} 1& 0\\\\0& 1\\end{pmatrix}\\quad B =\\begin{pmatrix} 2& 0\\\\0& 1/2\\end{pmatrix}.$$ Then the following matrices $C$ are minimal matrices greater ($\\succeq$) than $A$ and $B$:\n$$C=\\begin{pmatrix} 2& 0\\\\0& 1\\end{pmatrix}\\quad C=\\begin{pmatrix} 3& 1\\\\1& 3/2\\end{pmatrix} \\quad C=\\begin{pmatrix} 3& -1\\\\-1& 3/2\\end{pmatrix}.$$\n(The set of all such matrices $C$ forms a one dimensional curve in the space of all $2\\times 2$ matrices.)", "meta": {"post_id": 383895, "input_score": 26, "output_score": 34, "post_title": "Properties of the cone of positive semidefinite matrices"}}
{"input": "I got the number\n$$\\frac{\\Gamma\\left(\\frac{1}{5}\\right)\\Gamma\\left(\\frac{4}{15}\\right)}{\\Gamma\\left(\\frac{1}{3}\\right)\\Gamma\\left(\\frac{2}{15}\\right)}=0.824326275998351470388591998726842...$$\nin the process of some quite long calculations that are not pertinent to my question. \nQuestion 1: Is this number a rational, algebraic irrational or transcendental? \nI performed some numeric calculations and they did not find a close match among roots of polynomials of degree less than $100$ and integer coefficients of absolute value less than $10^{12}$.\nSome people I've asked this question told me that it is very likely unknown and probably will be unknown (in the rigorous sense) for a long time. But many of them were ready to bet that this number is transcendental. Their reasoning was that there are $2^{\\aleph_0}$ transcendental numbers and only $\\aleph_0$ algebraics. So, if the number with a simple definition is not explicitly constructed to be algebraic, and simple tests do not suggest that it is algebraic, then this just would be highly unlikely coincidence for it to be algebraic. And mathematical intuition and common sense says that there are no such coincidences. Actually, I do not hope very much to get the answer to my first question, and would like to answer another (more philosophical) one:\nQuestion 2: Is this common sense reasoning valid? What is the role of such intuition in mathematics?", "output": "I'll lead off with the bombshell, and follow with some notes.\nYour constant is algebraic.\nIndeed, it satisfies the (irreducible) degree $120$ equation:\n$$729 + 914166000 x^{30} + 3529576586250 x^{60} - 1259674334325000 x^{90} + \n 3125 x^{120}.$$\nIt is also expressible via radicals:\n$$\\frac{\\sqrt{2}\\cdot 3^{1/20}}{5^{1/6}\n  {\\left(5-\\frac{7}{\\sqrt{5}}+\\sqrt{6-\\frac{6}{\\sqrt{5}}}\\right)^{1/4}}}.$$\nAll of this follows from a recent paper, Expessions for Values of the Gamma Function, in which expressions for $\\Gamma(m/n)$ with $n$ either dividing $24$ or $60$ are given in terms of some algebraic constants, $\\pi$, and the following ten $\\Gamma$-values:\n$$\\Gamma\\left(\\frac{1}{3}\\right),\\Gamma\\left(\\frac{1}{4}\\right),\\Gamma\\left(\\frac{1}{5}\\right),\\Gamma\\left(\\frac{2}{5}\\right),\\Gamma\\left(\\frac{1}{8}\\right),\\Gamma\\left(\\frac{1}{15}\\right),\\Gamma\\left(\\frac{1}{20}\\right),\\Gamma\\left(\\frac{1}{24}\\right),\\Gamma\\left(\\frac{1}{60}\\right),\\Gamma\\left(\\frac{7}{60}\\right).$$\nIt is conjectured (Lang) that these constants are algebraically independent over $\\mathbb{Q}(\\pi)$.", "meta": {"post_id": 386207, "input_score": 41, "output_score": 73, "post_title": "What is the role of mathematical intuition and common sense in questions of irrationality or transcendence of values of special functions?"}}
{"input": "Is it always possible to find the limit of a function without using L'H\u00f4pital Rule or Series Expansion?\nFor example, \n$$\\lim_{x\\to0}\\frac{\\tan x-x}{x^3}$$\n$$\\lim_{x\\to0}\\frac{\\sin x-x}{x^3}$$\n$$\\lim_{x\\to0}\\frac{\\ln(1+x)-x}{x^2}$$\n$$\\lim_{x\\to0}\\frac{e^x-x-1}{x^2}$$\n$$\\lim_{x\\to0}\\frac{\\sin^{-1}x-x}{x^3}$$\n$$\\lim_{x\\to0}\\frac{\\tan^{-1}x-x}{x^3}$$", "output": "Using only trigonometric identities, in this answer, it is shown that\n$$\n\\lim_{x\\to0}\\frac{x-\\sin(x)}{x-\\tan(x)}=-\\frac12\\tag{1}\n$$\nTherefore, if we subtract from $1$, we get\n$$\n\\lim_{x\\to0}\\frac{\\tan(x)-\\sin(x)}{\\tan(x)-x}=\\frac32\\tag{2}\n$$\nUsing the limits proven geometrically in this answer, we can derive\n$$\n\\begin{align}\n\\lim_{x\\to0}\\frac{\\tan(x)-\\sin(x)}{x^3}\n&=\\lim_{x\\to0}\\frac{\\tan(x)(1-\\cos(x))}{x^3}\\\\\n&=\\lim_{x\\to0}\\frac{\\tan(x)}x\\frac{\\sin^2(x)}{x^2}\\frac1{1+\\cos(x)}\\\\\n&=\\frac12\\tag{3}\n\\end{align}\n$$\nwe can divide $(3)$ by $(2)$ to get\n$$\n\\bbox[5px,border:2px solid #C0A000]{\\lim_{x\\to0}\\frac{\\tan(x)-x}{x^3}=\\frac13}\\tag{4}\n$$\nand we can multiply $(1)$ by $(4)$ to get\n$$\n\\bbox[5px,border:2px solid #C0A000]{\\lim_{x\\to0}\\frac{\\sin(x)-x}{x^3}=-\\frac16}\\tag{5}\n$$\nNote that $(4)$ implies\n$$\n\\begin{align}\n\\lim_{x\\to0}\\frac{\\tan(x)-x}{\\tan^3(x)}\n&=\\lim_{x\\to0}\\frac{\\tan(x)-x}{x^3}\\lim_{x\\to0}\\frac{x^3}{\\tan^3(x)}\\\\\n&=\\frac13\\cdot1\\tag{6}\n\\end{align}\n$$\nTherefore, substituting $x\\mapsto\\tan^{-1}(x)$,\n$$\n\\bbox[5px,border:2px solid #C0A000]{\\lim_{x\\to0}\\frac{\\tan^{-1}(x)-x}{x^3}=-\\frac13}\\tag{7}\n$$\nSimilarly, $(5)$ implies\n$$\n\\begin{align}\n\\lim_{x\\to0}\\frac{\\sin(x)-x}{\\sin^3(x)}\n&=\\lim_{x\\to0}\\frac{\\sin(x)-x}{x^3}\\lim_{x\\to0}\\frac{x^3}{\\sin^3(x)}\\\\\n&=-\\frac16\\cdot1\\tag{8}\n\\end{align}\n$$\nTherefore, substituting $x\\mapsto\\sin^{-1}(x)$,\n$$\n\\bbox[5px,border:2px solid #C0A000]{\\lim_{x\\to0}\\frac{\\sin^{-1}(x)-x}{x^3}=\\frac16}\\tag{9}\n$$\n\nUsing the Binomial Theorem, we have\n$$\n\\left(1+\\frac xn\\right)^n-1-x\n=\\frac{n-1}{2n}x^2+\\sum_{k=3}^n\\binom{n}{k}\\frac{x^k}{n^k}\\tag{10}\n$$\nand for $|x|\\le1$,\n$$\n\\begin{align}\n\\left|\\sum_{k=3}^n\\binom{n}{k}\\frac{x^k}{n^k}\\right|\n&=|x|^3\\left|\\sum_{k=3}^n\\binom{n}{k}\\frac{x^{k-3}}{n^k}\\right|\\\\\n&\\le |x|^3\\sum_{k=3}^\\infty\\frac1{k!}\\\\[6pt]\n&=|x|^3\\left(e-\\tfrac52\\right)\\tag{11}\n\\end{align}\n$$\nCombining $(10)$ and $(11)$ and taking the limit as $n\\to\\infty$ yields\n$$\n\\frac{e^x-1-x}{x^2}=\\frac12+O(|x|)\\tag{12}\n$$\nand therefore,\n$$\n\\bbox[5px,border:2px solid #C0A000]{\\lim_{x\\to0}\\frac{e^x-1-x}{x^2}=\\frac12}\\tag{13}\n$$\nA simple corollary of $(13)$ is\n$$\n\\lim_{x\\to0}\\frac{e^x-1}x=1\\tag{14}\n$$\nTherefore, it follows that\n$$\n\\begin{align}\n\\lim_{x\\to0}\\frac{e^x-1-x}{(e^x-1)^2}\n&=\\lim_{x\\to0}\\frac{e^x-1-x}{x^2}\\lim_{x\\to0}\\frac{x^2}{(e^x-1)^2}\\\\\n&=\\frac12\\tag{15}\n\\end{align}\n$$\nIf we substitute $x\\mapsto\\log(1+x)$ in $(15)$, we get\n$$\n\\lim_{x\\to0}\\frac{x-\\log(1+x)}{x^2}=\\frac12\\tag{16}\n$$\nTherefore,\n$$\n\\bbox[5px,border:2px solid #C0A000]{\\lim_{x\\to0}\\frac{\\log(1+x)-x}{x^2}=-\\frac12}\\tag{17}\n$$", "meta": {"post_id": 387333, "input_score": 125, "output_score": 62, "post_title": "Are all limits solvable without L'H\u00f4pital Rule or Series Expansion"}}
{"input": "I need help with calculating this integral:\n$$\\int_0^1\\arctan\\,_4F_3\\left(\\frac{1}{5},\\frac{2}{5},\\frac{3}{5},\\frac{4}{5};\\frac{1}{2},\\frac{3}{4},\\frac{5}{4};\\frac{x}{64}\\right)\\,\\mathrm dx,$$\nWhere $_pF_q$ is a generalized hypergeometric function. \nI was told it has a closed-form representation in terms of elementary functions and integers.", "output": "This hypergeometric function is not an elementary function, but its inverse is - see Bring radical.\n\\begin{align}\nI&= \\int_0^1\\arctan{_4F_3}\\left(\\frac15,\\frac25,\\frac35,\\frac45;\\frac12,\\frac34,\\frac54;\\frac{x}{64}\\right)\\,dx \\\\\n&=\\frac{3125}{48}\\left(5+3\\pi+6\\ln2-3\\alpha^4+4\\alpha^3+6\\alpha^2-12\\alpha\\\\-12\\left(\\alpha^5-\\alpha^4+1\\right)\\arctan\\frac1\\alpha-6\\ln\\left(1+\\alpha^2\\right)\\right)\\\\\n&=0.7857194\\dots\n\\end{align}\nwhere $\\alpha$ is the positive root of the polynomial $625\\alpha^4-500\\alpha^3-100\\alpha^2-20\\alpha-4$. It can be expressed in radicals as follows:\n$$\\alpha=\\frac15+\\sqrt\\beta+\\sqrt{\\frac15-\\beta +\\frac1{25\\sqrt\\beta}},$$\nwhere\n$$\\beta=\\frac1{30}\\left(\\frac\\gamma5-\\frac4\\gamma+2\\right),$$\nwhere\n$$\\gamma=\\sqrt[3]{15\\sqrt{105}-125}.$$", "meta": {"post_id": 388890, "input_score": 41, "output_score": 40, "post_title": "$\\int_0^1\\arctan\\,_4F_3\\left(\\frac{1}{5},\\frac{2}{5},\\frac{3}{5},\\frac{4}{5};\\frac{1}{2},\\frac{3}{4},\\frac{5}{4};\\frac{x}{64}\\right)\\,\\mathrm dx$"}}
{"input": "I need to calculate the following integral:\n$$\\int_0^\\infty \\left(\\left(2\\ S(x)-1\\right)^2+\\left(2\\ C(x)-1\\right)^2\\right)^2 x\\ \\mathrm dx,$$\nwhere \n$$S(x)=\\int_0^x\\sin\\frac{\\pi z^2}{2}\\mathrm dz,$$\n$$C(x)=\\int_0^x\\cos\\frac{\\pi z^2}{2}\\mathrm dz$$\nare the Fresnel integrals. \nNumerical integration gives an approximate result $0.31311841522422385...$ that is close to $\\frac{16\\log2-8}{\\pi^2}$, so it might be the answer.", "output": "Step 1. Reduction of the integral\n\nLet $I$ denote the integral in question. With the change of variable $v = \\frac{\\pi x^2}{2}$, we have\n$$ I = \\frac{1}{\\pi} \\int_{0}^{\\infty} \\left\\{ (1 - 2 C(x) )^{2} + (1 - 2S(x))^{2} \\right\\}^{2} \\, dv $$\nwhere $x = \\sqrt{2v / \\pi}$ is understood as a function of $v$. By noting that\n$$ 1-2 S(x) = \\sqrt{\\frac{2}{\\pi}} \\int_{v}^{\\infty} \\frac{\\sin u}{\\sqrt{u}} \\, du \\quad \\text{and} \\quad 1-2 C(x) = \\sqrt{\\frac{2}{\\pi}} \\int_{v}^{\\infty} \\frac{\\cos u}{\\sqrt{u}} \\, du, $$\nwe can write $I$ as\n$$ I = \\frac{4}{\\pi^{3}} \\int_{0}^{\\infty} \\left| A(v) \\right|^{4} \\, dv \\tag{1} $$\nwhere $A(v)$ denotes the function defined by \n$$ A(v) = \\int_{v}^{\\infty} \\frac{e^{iu}}{\\sqrt{u}} \\, du. $$\n\n\nStep 2. Simplification of $\\left| A(v) \\right|^2$.\n\nNow we want to simplify $\\left| A(v) \\right|^2$. To this end, we note that for $\\Re u > 0$,\n$$ \\frac{1}{\\sqrt{u}}\n= \\frac{1}{\\Gamma\\left(\\frac{1}{2}\\right)} \\frac{\\Gamma\\left(\\frac{1}{2}\\right)}{u^{1/2}}\n= \\frac{1}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\frac{e^{-ux}}{\\sqrt{x}} \\, dx\n= \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{\\infty} e^{-ux^{2}} \\, dx \\tag{2} $$\nUsing this identity,\n\\begin{align*}\nA(v)\n&= \\frac{2}{\\sqrt{\\pi}} \\int_{v}^{\\infty} e^{iu} \\int_{0}^{\\infty} e^{-u x^2} \\, dx du\n = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\int_{v}^{\\infty} e^{-(x^2-i)u} \\, du dx \\\\\n&= \\frac{2 e^{iv}}{\\sqrt{\\pi}} \\int_{0}^{\\infty} e^{-v x^2} \\int_{0}^{\\infty} e^{-(x^2-i)u} \\, du dx\n = \\frac{2 e^{iv}}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\frac{e^{-v x^2}}{x^2-i} \\, dx.\n\\end{align*}\nThus by the polar coordinate change $(x, y) \\mapsto (r, \\theta)$ followed by the substitutions $r^2 = s$ and $\\tan \\theta = t$, we obtain\n\\begin{align*}\n\\left| A(v) \\right|^2\n&= A(v) \\overline{A(v)}\n = \\frac{4}{\\pi} \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\frac{e^{-v (x^2+y^2)}}{(x^2-i)(y^2 + i)} \\, dxdy \\\\\n&= \\frac{4}{\\pi} \\int_{0}^{\\infty} \\int_{0}^{\\frac{\\pi}{2}}  \\frac{r e^{-v r^2}}{(r^2 \\cos^{2}\\theta-i)(r^2 \\sin^{2}\\theta + i)} \\, d\\theta dr \\\\\n&= \\frac{2}{\\pi} \\int_{0}^{\\infty} \\int_{0}^{\\frac{\\pi}{2}}  \\frac{e^{-v s}}{(s \\cos^{2}\\theta-i)(s \\sin^{2}\\theta + i)} \\, d\\theta ds \\\\\n&= \\frac{2}{\\pi} \\int_{0}^{\\infty} \\frac{e^{-v s}}{s} \\int_{0}^{\\frac{\\pi}{2}} \\left( \\frac{1}{s \\cos^{2}\\theta-i} + \\frac{1}{s \\sin^{2}\\theta + i} \\right) \\, d\\theta ds \\\\\n&= \\frac{2}{\\pi} \\int_{0}^{\\infty} \\frac{e^{-v s}}{s} \\int_{0}^{\\infty} \\left( \\frac{1}{s -i(t^2 + 1)} + \\frac{1}{s t^2 + i (t^2 + 1)} \\right) \\, dt ds.\n\\end{align*}\nEvaluation of the inner integral is easy, and we obtain\n\\begin{align*}\n\\left| A(v) \\right|^2\n&= 2 \\int_{0}^{\\infty} \\frac{e^{-v s}}{s} \\Re \\left( \\frac{i}{\\sqrt{1 + is}} \\right) \\, ds.\n\\end{align*}\nApplying $(2)$ again, we find that\n\\begin{align*}\n\\left| A(v) \\right|^2\n&= 2 \\int_{0}^{\\infty} \\frac{e^{-v s}}{s} \\Re \\left( \\frac{i}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\frac{e^{-(1+is)u}}{\\sqrt{u}} \\, du \\right) \\, ds \\\\\n&= \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\frac{e^{-v s}}{s} \\int_{0}^{\\infty} \\frac{e^{-u} \\sin (su)}{\\sqrt{u}} \\, du\\, ds \\\\\n&= \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\frac{e^{-u}}{\\sqrt{u}} \\int_{0}^{\\infty}  \\frac{\\sin (su)}{s} \\, e^{-v s} \\, ds\\, du \\\\\n&= \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{\\infty} \\frac{e^{-u}}{\\sqrt{u}} \\arctan \\left( \\frac{u}{v} \\right) \\, du \\\\\n&= \\frac{4\\sqrt{v}}{\\sqrt{\\pi}} \\int_{0}^{\\infty} e^{-vx^{2}} \\arctan (x^2) \\, dx \\qquad (u = vx^2) \\tag{3}\n\\end{align*}\nHere, we exploited the identity\n$$ \\int_{0}^{\\infty} \\frac{\\sin x}{x} e^{-sx} \\, dx = \\arctan \\left(\\frac{1}{s}\\right), $$\nwhich can be proved by differentiating both sides with respect to $s$.\n\n\nStep 3. Evaluation of $I$.\n\nPlugging $(3)$ to $(1)$ and applying the polar coordinate change, $I$ reduces to\n\\begin{align*}\nI\n&= \\frac{64}{\\pi^{4}} \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\int_{0}^{\\infty} v e^{-v(x^{2}+y^{2})} \\arctan (x^2) \\arctan (x^2) \\, dx dy dv \\\\\n&= \\frac{64}{\\pi^{4}} \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\frac{\\arctan (x^2) \\arctan (x^2)}{(x^2 + y^2)^2} \\, dx dy \\\\\n&= \\frac{64}{\\pi^{4}} \\int_{0}^{\\frac{\\pi}{2}} \\int_{0}^{\\infty} \\frac{\\arctan (r^2 \\cos^2 \\theta) \\arctan (r^2 \\sin^2 \\theta)}{r^3} \\, dr d\\theta \\\\\n&= \\frac{32}{\\pi^{4}} \\int_{0}^{\\frac{\\pi}{2}} \\int_{0}^{\\infty} \\frac{\\arctan (s \\cos^2 \\theta) \\arctan (s \\sin^2 \\theta)}{s^2} \\, ds d\\theta. \\qquad (s = r^2) \\tag{4}\n\\end{align*}\nNow let us denote\n$$ J(u, v) = \\int_{0}^{\\infty} \\frac{\\arctan (us) \\arctan (vs)}{s^2} \\, ds. $$\nThen a simple calculation shows that\n$$ \\frac{\\partial^{2} J}{\\partial u \\partial v} J(u, v) = \\int_{0}^{\\infty} \\frac{ds}{(u^2 s^2 + 1)(v^2 s^2 + 1)} = \\frac{\\pi}{2(u+v)}. $$\nIndeed, both the contour integration method or the partial fraction decomposition method work here. Integrating, we have\n$$ J(u, v) = \\frac{\\pi}{2} \\left\\{ (u+v) \\log(u+v) - u \\log u - v \\log v \\right\\}. $$\nPlugging this to $(4)$, it follows that\n\\begin{align*}\nI\n&= -\\frac{64}{\\pi^{3}} \\int_{0}^{\\frac{\\pi}{2}} \\sin^2 \\theta \\log \\sin \\theta \\, d\\theta\n = -\\frac{16}{\\pi^{3}} \\frac{\\partial \\beta}{\\partial z}\\left( \\frac{3}{2}, \\frac{1}{2} \\right)\n\\end{align*}\nwhere $\\beta(z, w)$ is the beta function, satisfying the following beta function identity\n$$ \\beta (z, w) = 2 \\int_{0}^{\\infty} \\sin^{2z-1}\\theta \\cos^{2w-1} \\theta \\, d\\theta = \\frac{\\Gamma(z)\\Gamma(w)}{\\Gamma(z+w)}. $$\nTherefore we have\n\\begin{align*}\nI\n&= \\frac{16}{\\pi^{3}} \\frac{\\Gamma\\left(\\frac{3}{2}\\right)\\Gamma\\left(\\frac{1}{2}\\right)}{\\Gamma(2)} \\left\\{ \\psi_{0} (2) - \\psi_{0} \\left(\\tfrac{3}{2} \\right) \\right\\}\n = \\frac{8}{\\pi^2} \\int_{0}^{1} \\frac{\\sqrt{x} - x}{1 - x} \\, dx\n = \\frac{8 (2 \\log 2 - 1)}{\\pi^2},\n\\end{align*}\nwhere $\\psi_0 (z) = \\dfrac{\\Gamma'(z)}{\\Gamma(z)}$ is the digamma function, satisfying the following identity\n$$ \\psi_{0}(z+1) = -\\gamma + \\int_{0}^{1} \\frac{1 - x^{z}}{1 - x} \\, dx. $$", "meta": {"post_id": 390847, "input_score": 47, "output_score": 59, "post_title": "An integral involving Fresnel integrals $\\int_0^\\infty \\left(\\left(2\\ S(x)-1\\right)^2+\\left(2\\ C(x)-1\\right)^2\\right)^2 x\\ \\mathrm dx,$"}}
{"input": "I stumbled upon this short article on last weekend, it introduces an integral trick that exploits differentiation under the integral sign. On its last page, the author, Mr. Anonymous, left several exercises without any hints, one of them is to evaluate the Gaussian integral\n$$\n\\int^\\infty_0 e^{-x^2} \\,dx= \\frac{\\sqrt{\\pi}}{2}\n$$\nusing this parametrization trick. I had been evaluating it through trial and error using different paramatrizations, but no luck so far. \n\nHere are what I have tried so far:\n\nA first instinct would be do something like:$$\nI(b) = \\int^\\infty_0 e^{-f(b)x^2}\\,dx\n$$\nfor some permissible function $f(\\cdot)$, differentiating it will lead to a simple solvable ode:\n$$\n\\frac{I'(b)}{I(b)} = -\\frac{f'(b)}{2f(b)}\n$$\nwhich gives:\n$$\nI(b) = \\frac{C}{\\sqrt{f(b)}}.\n$$\nHowever, finding this constant $C$ basically is equivalent to evaluating the original integral, we are stuck here without leaving this parametrization trick framework.\nA second try involves an exercise on the same page:\n$$\nI(b) = \\int^\\infty_0 e^{-\\frac{b^2}{x^2}-x^2}dx.\n$$\nTaking derivative and rescaling the integral using change of variable we have:\n$$\nI'(b) = -2I(b).\n$$\nThis gives us another impossible to solve constant $C$ in:\n$$\nI(b) = C e^{-2b}\n$$\nwithout leaving this framework yet again.\nThe third try is trying modify Am\u00e9rico Tavares's answer in this MSE question:\n$$\nI(b) = \\int^\\infty_0 be^{-b^2x^2}\\,dx.\n$$\nIt is easy to show that:\n$$\nI'(b) = \\int^\\infty_0 e^{-b^2x^2}\\,dx - \\int^\\infty_0 2b^2 x^2 e^{-b^2x^2}\\,dx = 0\n$$\nby an integration by parts identity:\n$$\n\\int^\\infty_0 x^2 e^{- c x^2}\\,dx = \\frac{1}{2c}\\int^\\infty_0 e^{- c x^2}\\,dx .\n$$\nThen $I(b) = C$, ouch, stuck again at this constant.\n\n\nNotice in that Proving $\\displaystyle\\int_{0}^{\\infty} e^{-x^2} dx = \\frac{\\sqrt \\pi}{2}$ question, Bryan Yocks's answer is somewhat similar to the idea of parametrization, however he has to introduce another parametric integration to produce a definite integral leading to $\\arctan$.\nIs there such a one shot parametrization trick solution like the author Anonymous claimed to be \"creative parameterizations and a dose of differentiation under the integral\"?", "output": "Just basically independently reinvented Bryan Yock's solution as a more 'pure' version of Feynman.\nLet $$I(b) = \\int_0^\\infty \\frac {e^{-x^2}}{1+(x/b)^2} \\mathrm d x = \\int_0^\\infty \\frac{e^{-b^2y^2}}{1+y^2} b\\,\\mathrm dy$$ so that $I(0)=0$, $I'(0)= \\pi/2$ and $I(\\infty)$ is the thing we want to evaluate.\nNow note that rather than differentiating directly, it's convenient to multiply by some stuff first to save ourselves some trouble. Specifically, note\n$$\\left(\\frac 1 b e^{-b^2}I\\right)' = -2b \\int_0^\\infty e^{-b^2(1+y^2)} \\mathrm d y = -2 e^{-b^2} I(\\infty)$$\nThen usually at this point we would solve the differential equation for all $b$, and use the known information at the origin to infer the information at infinity. Not so easy here because the indefinite integral of $e^{-x^2}$ isn't known. But we don't actually need the solution in between; we only need to relate information at the origin and infinity. Therefore, we can connect these points by simply integrating the equation definitely; applying $\\int_0^\\infty \\mathrm d b$ we obtain\n$$-I'(0)= -2 I(\\infty)^2 \\quad \\implies \\quad I(\\infty) = \\frac{\\sqrt \\pi} 2$$", "meta": {"post_id": 390850, "input_score": 47, "output_score": 48, "post_title": "Integrating $\\int^{\\infty}_0 e^{-x^2}\\,dx$ using Feynman's parametrization trick"}}
{"input": "I want to know if there is a way to simplify, or a closed form solution of $tr(\\Sigma^{-1})$ where $\\Sigma$ is a symmetric positive definite matrix.", "output": "Let $A$ be symmetric positive definite matrix hence $\\exists$ a diagonal matrix $D$ whose diagonal entries are nonzero and $A=P D P^{-1}$ so $A^{-1} = P D^{-1} P^{-1}$ and $Tr(A^{-1})= Tr(D^{-1})$.  Now $D$ being diagonal matrix with non zero diagonal entries $D^{-1}$ has diagonal entries reciprocal of the diagonal entries of $D$ so $Tr(D^{-1})$ is sum of the inverses of the diagonal entries of $D$.", "meta": {"post_id": 391128, "input_score": 29, "output_score": 42, "post_title": "Trace of an Inverse Matrix"}}
{"input": "I want to prove that\n\n$p(x):=x^n-x-1 \\in \\mathbb Q[x]$ for $n\\ge 2$ is irreducible.\n\nMy attempt.\n\nGCD of coefficients is $1$, $\\mathbb Q$ is the field of fractions of $\\mathbb Z$, and $\\mathbb Z$ is UFD. Hence, $p(x)$ is irreducible over $\\mathbb Q$ iff it's irreducible over $\\mathbb Z$ (Gauss's lemma).\nLet $m\\in \\mathbb Z$ such that $\\varphi(m)=n$ (Euler's totient). Make reduction of $p(x)$ by modulo $m$. Because of $\\overline{x^n}=\\overline{x^{\\varphi(m)}}=\\overline{1}$, we get $\\overline{p(x)}=\\overline{1-x-1}=\\overline{-x}$, which is irreducible. Hence, $p(x)$ is irreducible.\n\nDoes this proof is correct?\nUPDATE. Thanks to Calvin Lin. My mistake is: not for all $n$ we can find such $m$. OK, but as for the rest, does my proof is correct for such $n$, that $n=\\varphi(m)$ for an integer $m$? And can it be some changed for all $n$, i.e. can we find such modulo that $\\overline{p(x)}$ is irreducible for every $n\\ge 2$?", "output": "I will sketch a proof of the irreducibility of $x^n - x - 1$ over ${\\mathbf Q}$ for $n \\geq 2$ that is simpler than Selmer's argument. I learned of this approach from David Rohrlich, who learned it from Michael Filaseta.\nLet $f(x) = a_nx^n + a_{n-1}x^{n-1} + \\cdots + a_1x + a_0$ be nonzero in $F[x]$ for any field $F$. Let $\\tilde{f}(x) = x^{\\deg f}f(1/x)$ be its reciprocal polynomial:\n$\\tilde{f}(x) = a_0x^n + a_1x^{n-1} + \\cdots + a_{n-1}x + a_n$. If $f(0) \\not= 0$ then $f$ and $\\tilde f$ have the same degree. Easily if $f = gh$ then $\\tilde{f} = \\tilde{g}\\tilde{h}$, and $\\widetilde{cf} = c\\tilde{f}$ for any constant $c$.\nIn terms of factorization over roots, if $f(0) \\not= 0$ then\n$$\nf(x) = c(x-r_1)\\cdots(x-r_n) \\Longrightarrow \\tilde{f}(x) = f(0)c(x - 1/r_1)\\cdots(x-1/r_n).\n$$\nStep 1: Let $f(x) \\in {\\mathbf Z}[x]$ satisfy (i) $f(0) \\not= 0$ and (ii) $f(x)$ and $\\tilde{f}(x)$ have no common roots. If $f(x) = g(x)h(x)$ for some nonconstant $g(x)$ and $h(x)$ in $\\mathbf Z[x]$, show there is a $k(x)$ in $\\mathbf Z[x]$ with $\\deg k = \\deg f$ such that $f\\tilde{f} = k\\tilde{k}$ and $k \\not= \\pm f$ or $\\pm\\tilde f$. If $f(x)$ is monic and $f(0) = \\pm 1$, show you can take $k$ to be monic. (Hint: Use $k = \\pm g\\tilde{h}$ for a suitable choice of sign.)\nStep 2: For $n \\geq 2$, show the polynomial $x^n - x - 1$ doesn't have any roots in common with its reciprocal.\nStep 3: Let $f(x) = x^n - x - 1$ for $n \\geq 2$. Suppose $f\\tilde{f} = k\\tilde{k}$ for some monic $k \\in \\mathbf Z[x]$ of degree $n$.  Compare the degree $n$ coefficients on both sides of the equation $f\\tilde{f} = k\\tilde{k}$ to show that $k$ must be a sum of 3 monomials whose coefficients are all $\\pm 1$. Then look at the top 3 nonzero terms on both sides to show $k = f$ or $k = -\\tilde{f}$.\nStep 4: Combine all the previous steps to deduce irreducibility of $x^n - x - 1$ over $\\mathbf Q$.\nExercise: Use the same argument to determine when $x^n + x + 1$, $x^n - x + 1$, and $x^n + x - 1$ are irreducible over $\\mathbf Q$. (With a computer you can find in each case that there is a congruence condition on $n$ for it to be reducible, and this turns out to be exactly the condition for $f(x)$ and $\\tilde{f}(x)$ to have a common root, which will be either a 3rd or 6th root of unity.)\nUpdate: for another application of this method to prove irreducibility of a family of polynomials, see the MO post here.", "meta": {"post_id": 393646, "input_score": 36, "output_score": 49, "post_title": "Irreducibility of $x^n-x-1$ over $\\mathbb Q$"}}
{"input": "I need to find a closed form for these nested definite integrals:\n$$I=\\int_0^\\infty\\left(\\int_0^1\\frac1{\\sqrt{1-y^2}\\sqrt{1+x^2\\,y^2}}\\mathrm dy\\right)^3\\mathrm dx.$$\nThe inner integral can be represented using the hypergeometric function $_2F_1$ or the complete elliptic integral of the 1st kind $K$ with an imaginary argument:\n$$\\int_0^1\\frac1{\\sqrt{1-y^2}\\sqrt{1+x^2\\,y^2}}\\mathrm dy=\\frac\\pi2   {_2F_1}\\left(\\frac12,\\frac12;1;-x^2\\right)=K(x\\,\\sqrt{-1}).$$\nMy conjecture is the integral $I$ has a closed-form representation:$$I\\stackrel{?}{=}\\frac{3\\,\\Gamma (\\frac14)^8}{1280\\,\\pi^2}=7.09022700484626946098980237...,$$\nbut I was neither able to find a proof of it, nor disprove the equality using numerical integration. Could you please help me with resolving this question?", "output": "Using \n$$ K(ik) =\n\\frac{1}{\\sqrt{1+k^2}}K\\left(\\sqrt{\\frac{k^2}{k^2+1}}\\right) $$\nand a substitution $t^2 = \\frac{k^2}{1+k^2}$, rewrite the integral as\n$$ \\int_0^\\infty K(i k)^3\\,dk = \\int_0^1 K(t)^3\\,dt. $$\nThere is a paper \"Moments of elliptic integrals and critical L-values\"\nby Rogers, Wan and Zucker (http://arxiv.org/abs/1303.2259; also one of\nthe authors' earlier papers: http://arxiv.org/abs/1101.1132), and the\nauthors, by relating this integral to an L-series of a modular form (their theorems 1 and 2),\nshow that\n$$ \\int_0^1 K(k)^3\\,dk = \\frac{3}{5}K(1/\\sqrt{2})^4 =\n\\frac{3\\Gamma(\\frac14)^8}{1280\\pi^2}, $$\nusing $K(1/\\sqrt{2}) = \\frac14 \\pi^{-1/2}\\Gamma(\\frac14)^2$.", "meta": {"post_id": 395085, "input_score": 34, "output_score": 36, "post_title": "Closed form for $\\int_0^\\infty\\left(\\int_0^1\\frac1{\\sqrt{1-y^2}\\sqrt{1+x^2\\,y^2}}\\mathrm dy\\right)^3\\mathrm dx.$"}}
{"input": "Consider the following integral:\n$$\\mathcal{I}(\\mu,\\nu)=\\int_0^\\infty\\ln\\frac{J_\\mu(x)^2+Y_\\mu(x)^2}{J_\\nu(x)^2+Y_\\nu(x)^2}\\mathrm dx,$$\nwhere $J_\\mu(x)$ is the Bessel function of the first kind:\n$$J_\\mu(x)=\\sum _{n=0}^\\infty\\frac{(-1)^n}{\\Gamma(n+1)\\Gamma(n+\\mu+1)}\\left(\\frac x2\\right)^{2n+\\mu}$$\nand $Y_\\mu(x)$ is the Bessel function of the second kind:\n$$Y_\\mu(x)=\\frac{J_\\mu(x)\\cos(\\mu\\pi)-J_{-\\mu}(x)}{\\sin(\\mu\\pi)}.$$\nI was not able to rigorously establish a closed form for $\\mathcal{I}(\\mu,\\nu)$, but based on numerical integration I made a conjecture:\n$$\\forall\\mu,\\nu\\in\\mathbb{R},\\hspace{1cm}\\mathcal{I}(\\mu,\\nu)\\stackrel{?}{=}\\frac{\\pi}{2}(\\mu^2-\\nu^2).$$\nCould you please help me to find out if this conjecture is true?\n\nIf the conjecture is true, then taking the derivative with respect to $\\mu$ at $\\mu=1$ we get the following corollary:\n$$\\int_0^\\infty\\frac{J_0(x)J_1(x)+Y_0(x)Y_1(x)}{J_1(x)^2+Y_1(x)^2}x^{-1}\\mathrm dx\\stackrel{?}{=}\\frac{\\pi}{2}.$$\n\nAs pointed by O.L., the conjecture is equivalent to\n$$\\int_0^{\\infty}\\ln\\frac{\\pi\\,x\\,H^{(1)}_\\nu(x)H^{(2)}_\\nu(x)}{2}\\mathrm dx\\stackrel{?}{=}\\frac{\\pi\\,(4\\nu^2-1)}{8},$$\nwhere $H^{(1)}_\\nu(x)=J_\\nu(x)+i\\,Y_\\nu(x)$ and $H^{(2)}_\\nu(x)=J_\\nu(x)-i\\,Y_\\nu(x)$ are the Hankel functions of the first and second kind.", "output": "I will assume that $\\nu$ is real, as in the formulation\nof the question. Something similar may be true for\ncomplex $\\nu$ by a similar argument, but there would be an  extra complication (perhaps\nonly notational) of working with $\\nu$ and\nits conjugate $\\overline{\\nu}$ instead of just $\\nu$.\nThe Hankel functions $H^{(i)}_{\\nu}(z)$ are entire\nexcept for a branch cut along the negative real axis.\nLet\n$$A^{(1)}_{\\nu}(z) =  \\frac{H^{(1)}_{\\nu-1}(z)}{ H^{(1)}_{\\nu}(z)}, \\qquad\nA^{(2)}_{\\nu}(z) =  \\frac{H^{(2)}_{\\nu-1}(z)}{ H^{(2)}_{\\nu}(z)},$$\n$$ B^{(1)}_{\\nu}(z) =  \\frac{H^{(1)}_{\\nu+1}(z)}{ H^{(1)}_{\\nu}(z)}, \\qquad\nB^{(2)}_{\\nu}(z) =  \\frac{H^{(2)}_{\\nu+1}(z)}{ H^{(2)}_{\\nu}(z)}.$$\nWe introduce the following notation:\nfor a function $F(z)$ with a branch cut along the negative axis, we let $F(x^{+})$\nand $F(x^{-})$ denote the limit of $F(z)$ as $z \\rightarrow x$ from the region\n$\\mathrm{Im}(z) > 0$ and $\\mathrm{Im}(z) < 0$ respectively.\nThere are identities as follows:\n$$\\text{Eq. 1:} \\quad  A^{(1)}_{\\nu}(x^{+}) = \\overline{A^{(2)}_{\\nu}(x^{-})} = -A^{(2)}_{\\nu}(-x)\n= - \\overline{A^{(1)}_{\\nu}(-x)},$$\n$$\nA^{(2)}_{\\nu}(x^{+}) = \\overline{A^{(1)}_{\\nu}(x^{-})}.$$\nand the same equations hold for $B$. Note that\nit's not true that\n$A^{(2)}_{\\nu}(x^{+}) = -A^{(2)}_{\\nu}(-x)$, the lack of symmetry\nhere is related to the branch cut. This is an important point.  The behavior of  $A^{(1)}_{\\nu}(z)$ is bad in the region near $x^{-}$, and correspondingly\n $A^{(2)}_{\\nu}(z)$ is bad near $x^{+}$.\nThe Hankel function has nice  asymptotic expansions for large $z$.\nThe ratio of such functions at arguments\n$\\nu$ differing by integers is particularly nice, because the complex phase cancels.\nIn particular, one has the following:\n$$A^{(1)}_{\\nu}(z) \\sim \ni \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right)\n+ \\frac{(2 \\nu - 1)}{2} \\cdot \\frac{1}{z}  + O(z^{-3}),$$\n$$A^{(2)}_{\\nu}(z) \\sim \n-i \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right)\n+ \\frac{(2 \\nu - 1)}{2} \\cdot \\frac{1}{z}  + O(z^{-3}),$$\n$$  B^{(2)}_{\\nu}(z)  \\sim \ni \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right)\n+ \\frac{(2 \\nu + 1)}{2} \\cdot \\frac{1}{z}  + O(z^{-3}),$$\n$$  B^{(1)}_{\\nu}(z)  \\sim \n-i \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right)\n+ \\frac{(2 \\nu + 1)}{2} \\cdot \\frac{1}{z}  + O(z^{-3}).$$\nThis holds outside the bad regions mentioned above. In particular,\nit holds for $A^{(1)}_{\\nu}(z)$ and $B^{(1)}_{\\nu}(z)$ for $z$ with argument\nin $[-\\pi + \\epsilon,\\pi]$, and for $A^{(2)}_{\\nu}(z)$ and $B^{(2)}_{\\nu}(z)$ with argument\nin $[-\\pi,\\pi - \\epsilon]$.\nLet $C_R$ be the semi-circle\nwith centre $0$ and radius $R$ in the upper half plane, oriented anti-clockwise, and thought of as lying\nabove the branch cut in $(-\\infty,0])$. Note that this is contained within the range where the asymptotic holds for $A^{(1)}_{\\nu}(z)$, \nand hence\n$$\\lim_{R \\rightarrow \\infty} \\int_{C_R} z  \\left(\\frac{H^{(1)}_{\\nu-1}(z)}{ H^{(1)}_{\\nu}(z)} - i \\right) \n-  \\frac{(2 \\nu - 1)}{2} dz\n=   \\pi i \\cdot \\left( -i  \\cdot \\frac{(4 \\nu^2 - 1)}{8} \\right) = \n\\frac{\\pi (4 \\nu^2 - 1)}{8}  $$\nThe main term comes from the residue theorem\n(applied to a half circle, hence the $\\pi i$ rather than $2 \\pi i$ factor), and\nthe error term comes from the fact that the integral of $O(z^{-2})$ over a \nhalf-circle of radius\n$R$ and circumference $\\pi R$ is $O(R^{-1})$.\nWe now use the following fact: $H^{(1)}_{\\nu}(z)$ has no zeros in the upper  half plane.\nI say that it is a fact, but I couldn't find a reference (Edit: proof of this fact included at the end of this answer). I proved it rigorously by an explicit contour integral \ncomputation for various ranges of values of $\\nu$, however. (Certainly, by the asymptotic expansion which is valid in the entire upper half plane, it follows that any such zeros, if they exist, must be within some small radius, which one can eliminate by computing $(2 \\pi i)^{-1} \\oint d \\log(f)$.) \nBy the residue theorem (taking $C$ above to be the circle in the upper half plane), we get,\nfor any holomorphic integrand,\n$$0 = \\oint  = \\int_{C} + \\int^{R}_{-R},$$\nand hence\n$$ \\lim_{R \\rightarrow \\infty}  \\int^{R}_{-R} z  \\left(\\frac{H^{(1)}_{\\nu-1}(z)}{ H^{(1)}_{\\nu}(z)} - i \\right) \n-  \\frac{(2 \\nu - 1)}{2} dz\n = - \\frac{\\pi (4 \\nu^2 - 1)}{8} .$$\nNote that the integrand has order $O(1/z)$, so one really has to take the integrand\nfrom $-R$ to $R$ and then take the limit for this to make sense.\nOne may apply the same analysis to $H^{(2)}_{\\nu}$, except now the zero free region\nof $H^{(2)}_{\\nu}$\nis in the lower half plane --- this follows by symmetry from the\nidentity $H^{(1)}_{\\nu}(\\overline{z}) = \\overline{H^{(2)}_{\\nu}(z)}$, noting\nthat we are once again in the correct region as far as asymptotics goes.\n  Hence we deduce that\n$$\\lim_{R \\rightarrow \\infty} \\int^{-R}_{R} z  \\left(\\frac{H^{(2)}_{\\nu+1}(z)}{ H^{(2)}_{\\nu}(z)} - i \\right) \n-  \\frac{(2 \\nu + 1)}{2} dz = - \\frac{\\pi (4 \\nu^2 - 1)}{8} .$$\nNote that the direction of the integral has changed, for orientation reasons.\nWarning! There's also another difference between this and the previous integral. The first integral was above the branch cut and this integral is below\n the branch cut. However, in the first case,\nwe were integrating values of the form $A^{(1)}_{\\nu}(x^+)$, which was the good value (in the\nsense that it was related to three other values by symmetry in equation 1), and\nhere we are integrating $B^{(2)}_{\\nu}(x^{-})$, which also is related to three\nother values by the same equations.\nCorrecting\nthe order of the second integrand and then subtracting the results,  we get\n$$\\lim_{R \\rightarrow \\infty} \\int^{R}_{-R} 1 + z \n\\left(\\frac{H^{(1)}_{\\nu-1}(z)}{ H^{(1)}_{\\nu}(z)} - \\frac{H^{(2)}_{\\nu+1}(z)}{ H^{(2)}_{\\nu}(z)} \\right)\n dz = - 2 \\cdot  \\frac{\\pi (4 \\nu^2 - 1)}{8}.$$\nWe now make two observations: the integrand is now $O(z^{-2})$ for large $z$, and hence\nit exists as a definite integral. Moreover, the integrand is even. In light of the warning,\nwe should really specify that the\nintegrand for values $x \\in (-\\infty,0]$ is precisely:\n$$1 + x \\left(A^{(1)}_{\\nu}(x^{+}) - B^{(2)}_{\\nu}(x^{-})\\right).$$\n(One should check this is the correct function to make the integrand\neven.) We deduce that\n$$\n- \\int^{\\infty}_{0} 1 +  z \\left(\\frac{H^{(1)}_{\\nu-1}(z)}{ H^{(1)}_{\\nu}(z)} - \n\\frac{H^{(2)}_{\\nu+1}(z)}{ H^{(2)}_{\\nu}(z)} \\right) dz = \\frac{\\pi (4 \\nu^2 - 1)}{8}.$$\nLet \n$$I(\\nu) = \\int^{\\infty}_{0} \\log \\frac{\\pi x H^{(1)}_{\\nu}(x)  H^{(2)}_{\\nu}(x)}{2}  \\cdot dx.$$\nIntegrating by parts, and being a little bit careful about what happens at $0$, and\nexpressing the derivatives on Hankel functions in terms of Hankel  functions of other arguments, we find that\n$$I(\\nu) = -  \\int^{\\infty}_{0} 1 +  x \\left(\\frac{H^{(1)}_{\\nu-1}(x)}{ H^{(1)}_{\\nu}(x)} - \n\\frac{H^{(2)}_{\\nu+1}(x)}{ H^{(2)}_{\\nu}(x)} \\right) dx = \\frac{\\pi (4 \\nu^2 - 1)}{8}.$$\nAs noted in the comments, this was the identity to be proved.\nAlternatively, integration by parts also shows that\n$$\\frac{\\pi (\\mu^2 - \\nu^2)}{2} = I(\\mu) - I(\\nu)\n=  \\int^{\\infty}_{0} \\log \\frac{H^{(1)}_{\\mu}(x)  H^{(2)}_{\\mu}(x)}{H^{(1)}_{\\nu}(x)  H^{(2)}_{\\nu}(x)} \\cdot dx,$$\nand hence\n$$ \\int^{\\infty}_{0} \\log \\frac{J_{\\mu}(x)^2 + Y_{\\mu}(x)^2}{J_{\\nu}(x)^2 + Y_{\\nu}(x)^2} \\cdot\ndx = \\frac{\\pi (\\mu^2 - \\nu^2)}{2}.$$\nEdit: Proof that $H^{(1)}_{\\nu}(z)$ has no zeros in the upper half plane for real $\\nu > 0$.\nI realized that the proof can be completed in a similar manner. Let\n$$G^{(1)}_{\\nu}(z) =  d \\log H^{(1)}_{\\nu}(z)\n=\\frac{1}{2} \\left(A^{(1)}_{\\nu}(z) -  B^{(1)}_{\\nu}(z)\\right).$$\nThen we have an asymptotic formula, as before:\n$$A^{(1)}_{\\nu}(z) \\sim \ni \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right)\n+ \\frac{(2 \\nu - 1)}{2} \\cdot \\frac{1}{z}  + O(z^{-3}), $$\n$$  B^{(1)}_{\\nu}(z)  \\sim \n- i \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right)\n+ \\frac{(2 \\nu + 1)}{2} \\cdot \\frac{1}{z}  + O(z^{-3}), $$\nand thus\n$$G^{(1)}_{\\nu}(z) \\sim  i \\left(1 - \\frac{(4 \\nu^2 - 1)}{8} \\cdot \\frac{1}{z^2} +  \\ldots \\right) - \\frac{1}{2z} + O(z^{-3}),$$\nThis is valid for $z$ with argument in $[-\\pi + \\epsilon,\\pi]$, so in particular is valid in the upper half plane.\nIf $C_R$ denotes the circle in the upper half plane, we find that:\n$$\\lim_{R \\rightarrow \\infty} \\int_{C_R} (G^{(1)}_{\\nu}(z) - i) dz = - \\frac{\\pi i}{2},$$\nbecause it is $O(R^{-1})$ plus the contribution from the $1/(2z)$ term.\nLet $\\Omega_R$ denote the boundary of the  region bounded by by the interval $[-R,R]$ and $C_R$.\nThe function $H^{(1)}_{\\nu}(z)$ and thus $H^{(1)}_{\\nu}(z) e^{-iz}$ is holomorphic in $\\Omega_R$ \naway from $z = 0$. At zero (and $\\nu > 0$)  we have an asymptotic\n$$H^{(1)}_{\\nu}(z) \\sim  - i \\cdot \\frac{\\Gamma(\\nu)}{\\pi} \\left(\\frac{2}{z}\\right)^{\\nu}.$$\nIt follows that the number of zeros\nin the upper half plane is given, accounting for the singularity at $0$ (modified by a factor\nof two since this integral only accounts for half of the singularity) by\n$$\\frac{\\nu}{2} + \\lim_{R \\rightarrow \\infty} \\frac{1}{2 \\pi i} \\oint_{\\Omega_R} d  \\log (H^{(1)}_{\\nu}(z) e^{-iz}) dz$$\n$$= \\frac{\\nu}{2} + \\frac{1}{2 \\pi i} \\left( \\lim_{R \\rightarrow \\infty} \\int_{C_R}  (G^{(1)}_{\\nu}(z) - i) dz + \n\\lim_{R \\rightarrow \\infty} \\int_{-R}^{R} (G^{(1)}_{\\nu}(z) - i) dz\\right).$$\nWe computed the first integral above, hence it suffices to compute:\n$$\\frac{\\nu}{2}    -\\frac{1}{4} + \\frac{1}{2 \\pi i} \\lim_{R \\rightarrow \\infty} \\int_{-R}^{R} (G^{(1)}_{\\nu}(z) - i) dz.$$\nWe may write this as the integral\n$$\\frac{\\nu}{2}  -\\frac{1}{4} + \\frac{1}{2 \\pi i} \\int_{0}^{\\infty} \\left( G^{(1)}_{\\nu}(z) + G^{(1)}_{\\nu}(-z) - 2 i \\right) dz.$$\nThis expression evaluates to an integer, which is the number of zeros of $H^{(1)}_{\\nu}(z)$\nin the upper half plane.\nIn particular, evaluating this integral numerically for a random value (say $\\nu = \\pi$) shows\nthat, for this value, it is equal to zero. Now suppose that we vary $\\nu$. Since this integral\nevaluates to an integer, to complete the proof, it suffices to show that it varies continuously.\nFor the integral over $[R,\\infty)$ this is clear  for large enough $R$ from the asymptotic formula. For small values, it suffices to show that $H^{(1)}_{\\nu}(z)$ doesn't have any zeros on\nthe real line $[0,R]$, since otherwise the integrand is continuous in $\\nu$, and the continuity\nof the integral is clear. Now on the real axis, we have, by definition,\n$$H^{(1)}_{\\nu}(z) = J_{\\nu}(z) + i \\cdot Y_{\\nu}(z).$$\nSince $\\nu$ is real, it suffices to show that $J_{\\nu}(z)$ and $Y_{\\nu}(z)$\ndo not have any simultaneous zeros.  However, the zeros of these Bessel functions\nare well known to interlace (see Watson, A treatise on the theory of Bessel functions),\nand the result is established. Edit: Actually, there's a much easier proof that  $J_{\\nu}(z)$ and $Y_{\\nu}(z)$\ndo not have any common zeros --- they are linearly independent solutions to a second order ODE!", "meta": {"post_id": 395818, "input_score": 82, "output_score": 63, "post_title": "Closed form for $\\int_0^\\infty\\ln\\frac{J_\\mu(x)^2+Y_\\mu(x)^2}{J_\\nu(x)^2+Y_\\nu(x)^2}\\mathrm dx$"}}
{"input": "I have heard $\\varphi$ called the most irrational number. Numbers are either irrational or not though, one cannot be more \"irrational\" in the sense of a number that can not be represented as a ratio of integers. What is meant by most irrational? Define what we mean by saying one number is more irrational than another, and then prove that there is no $x$ such that $x$ is more irrational than $\\varphi$.\nNote: I have heard about defining irrationality by how well the number can be approximated by rational ones, but that would need to formalized.", "output": "How well can a number $\\alpha$ be approximated by rationals?\nTrivially, we can find infinitely many $\\frac pq$ with $|\\alpha -\\frac pq|<\\frac 1q$, so something better is needed to talk about a good approximation.\nFor example, if $d>1$, $c>0$ and there are infinitely many $\\frac pq$ with $|\\alpha-\\frac pq|<\\frac c{q^d}$, then we can say that $\\alpha$ can be approximated better than another number if it allows a higher $d$ than that other number. Or for equal values of $d$, if it allows a smaller $c$.\nIntriguingly, numbers that can be approximated exceptionally well by rationals are transcendental (and at the other end of the spectrum, rationals can be approximated exceptionally poorly  - if one ignores the exact approximation by the number itself). On the other hand, for every irrational $\\alpha$, there exists $c>0$ so that for infinitely many rationals $\\frac pq$ we have $|\\alpha-\\frac pq|<\\frac c{q^2}$. The infimum of allowed $c$ may differ among irrationals and it turns out that it depends on the continued fraction expansion of $\\alpha$.\nEspecially, terms $\\ge 2$ in the continued fraction correspond to better approximations than those for terms $=1$. Therefore, any number with infinitely many terms $\\ge 2$ allows a smaller $c$ than a number with only finitely many terms $\\ge2$ in the continued fraction. But if all but finitely many of the terms are $1$, then $\\alpha$ is simply a rational transform of $\\phi$, i.e. $\\alpha=a+b\\phi$ with $a\\in\\mathbb Q, b\\in\\mathbb Q^\\times$.", "meta": {"post_id": 395938, "input_score": 64, "output_score": 53, "post_title": "Why is $\\varphi$ called \"the most irrational number\"?"}}
{"input": "This question came up in a recent video series of lectures by Mike Freedman available through Max Planck Institut's website. He proves the \"difficult\" converse direction, that $X\\times \\mathbb R\\cong Y\\times \\mathbb R$  implies $X\\times S^1\\cong Y\\times S^1$ using a subtle \"push-pull\" argument, when $X$ and $Y$ are compact. He goes on to make the remark that the converse holds in the simply connected case by taking universal covers, but it is not obvious in general and there may even be a counterexample. So my question is whether anyone has any ideas about this question. Maybe someone can spot a counterexample in the non-simply connected case?\nEdit: Here's a link to the video series. If I recall correctly, the push-pull argument is in the first video.", "output": "No, it is not possible to conclude that $X\\times\\mathbb{R}\\cong Y\\times\\mathbb{R}$ from $X\\times S^1\\cong Y\\times S^1$, even in the case where $X$ and $Y$ are compact differentiable manifolds. In fact, Charlap1 showed the following in 1965.\n\nThere exist compact manifolds $X$, $Y$ of different homotopy types, but with $X\\times S^1$ diffeomorphic to $Y\\times S^1$.\n\nAs $X\\times\\mathbb{R}$ has the same homotopy type as $X$ for any topological space $X$, this implies that $X\\times\\mathbb{R}$ and $Y\\times\\mathbb{R}$ have different homotopy types, so they are certainly not homeomorphic.\nSee also Hilton, Mislin & Roitberg2 for examples of non-homotopy equivalent manifolds with diffeomorphic products with higher dimensional spheres.\nI just found those references by a bit of searching around, so I am not familiar with all of the details. However, I will now construct an explicit example of compact manifolds with different fundamental groups (hence, different homotopy types), but with diffeomorphic products with the circle. I think this example works along similar lines to the examples which could be constructed with the results of Charlap. In my example, $X$ and $Y$ will be 10-dimensional manifolds with covering space $S^9\\times\\mathbb{R}$.\nSince the products $X\\times S^1$ and $Y\\times S^1$ are to be diffeomorphic, they must have the same fundamental group. So, the first thing is to find two non-isomorphic groups $G$, $H$ whose direct products with the infinite cyclic group $Z$ (i.e., the integers under addition) are isomorphic. As it is always possible to cancel direct products with $Z$ for abelian groups, it is necessary that $G$ and $H$ are non-abelian. Expressed in terms of generators and relations, one such example is,\n$$\n\\begin{align}\nG &= \\langle x,y\\mid yx = xy^2, y^{32}=y\\rangle,\\cr\nH &= \\langle x,y\\mid yx = xy^4, y^{32}=y\\rangle.\n\\end{align}\n$$\nThis is a very slightly simplified version of the example given in Hirshon3.\nNote that each element of $G$ (resp., $H$) can be uniquely expressed as $x^ry^s$ with $s$ taken modulo 31, and satisfy the multiplication rule $(x^{r_1}y^{s_1})(x^{r_2}y^{s_2})=x^{r_1+r_2}y^{2^{r_2}s_1+s_2}$ (resp., $x^{r_1+r_2}y^{4^{r_2}s_1+s_2}$).\nThese groups are not isomorphic. Indeed, if $\\bar x$, $\\bar y$ are elements generating $G$ with $\\bar y$ of order 31, then we must have $\\bar y$ a power of $y$ and $\\bar x=x^{\\pm1}y^s$, so that $\\bar x^{-1}\\bar y\\bar x = \\bar y^2$ or $\\bar y^{16}$. In either case, this does not equal $\\bar y^4$, so $G\\not\\cong H$.\nOn the other hand, the direct product $G\\times Z$ can be formed by adding an additional generator $z$ to $G$ which commutes with $x$ and $y$ and satisfies no further relations. Setting $\\bar x = x^2z$ and $\\bar z = x^5z^2$, then $\\bar x, y, \\bar z$ also generate $G\\times Z$, $\\bar z$ commutes with $\\bar x$ and $y$, and $\\bar x,y$ satisfy the relation $y\\bar x=\\bar xy^4$. From this it can be seen that $G\\times Z\\cong H\\times Z$.\nI'll now define the spaces $X,Y$. The simply connected manifold $\\hat X\\equiv S^9\\times\\mathbb{R}$ can be realized as the submanifold of $\\mathbb{C}^5\\times\\mathbb{R}$ consisting of the points $(z,s)$ with $\\lVert z\\rVert = 1$. Define the diffeomorphisms $R,S$ on $\\hat X$ as\n$$\n\\begin{align}\n& R(z_0,z_1,z_2,z_3,z_4,s)=(\\omega z_0,\\omega^2z_1,\\omega^4z_2,\\omega^8z_3,\\omega^{16}z_4,s),\\cr\n& S(z_0,z_1,z_2,z_3,z_4,s)=(z_4,z_0,z_1,z_2,z_3,s+1)\n\\end{align}\n$$\nwhere $\\omega=e^{2\\pi i/31}$. These satisfy the relations $RS=SR^2$ and $R^{32}=R$, so the groups $\\Lambda_G\\equiv\\langle S,R\\rangle$ and $\\Lambda_H\\equiv\\langle S^2,R\\rangle$ are isomorphic to $G$ and $H$ respectively. Define $X$ and $Y$ to be the quotients $\\hat X/\\Lambda_G$ and $\\hat X/\\Lambda_H$. These are compact manifolds with non-isomorphic fundamental groups $\\Lambda_G\\cong G$ and $\\Lambda_H\\cong H$.\nFinally, I'll show that $X\\times S^1$ and $Y\\times S^1$ are diffeomorphic. Note that $X\\times S^1$ is just the manifold $X\\times \\mathbb{R}$ quotiented out by the translations $(x,t)\\mapsto(x,t+n)$ ($n\\in\\mathbb{Z}$). This can be written as a quotient $\\hat X\\times\\mathbb{R}/\\Lambda^\\prime$, where $\\Lambda^\\prime=\\langle S\\times I, R\\times I, \\hat I\\times T\\rangle$. Here, $\\hat I, I$ are the identities on $\\hat X$ and $\\mathbb{R}$, and $T$ is the translation on $\\mathbb{R}$ given by $t\\mapsto t+1$. However, writing $\\bar S=S^2\\times T$ and $\\bar T=S^5\\times T^2$, then $\\bar S, R\\times I,\\bar T$ generate $\\Lambda^\\prime$ and we have $$\n\\begin{align}\nU^{-1}\\bar SU&=S^2\\times I,\\cr\nU^{-1}(R\\times I)U&=R\\times I,\\cr\nU^{-1}\\bar TU&=\\hat I\\times T,\n\\end{align}\n$$\nwhere $U$ is the diffeomorphism on $S^9\\times\\mathbb{R}\\times\\mathbb{R}$ given by $U(z,s,t)=(z,s+5t,s/2+2t)$.\nSo, with $\\cong$ denoting diffeomorphism,\n$$\n\\begin{align}\nX\\times S^1 &\\cong S^9\\times\\mathbb{R}\\times\\mathbb{R}/\\langle S\\times I,R\\times I,\\hat I\\times T\\rangle\\cr\n&\\cong S^9\\times\\mathbb{R}\\times\\mathbb{R}/\\langle S^2\\times I,R\\times I,\\hat I\\times T\\rangle\\cr\n&\\cong Y\\times S^1.\n\\end{align}\n$$\nAside: As was noted in the question and shown in the linked lecture series, the implication $X\\times\\mathbb{R}\\cong Y\\times\\mathbb{R}\\Rightarrow X\\times S^1\\cong Y\\times S^1$ holds for compact spaces $X,Y$. However, it does not hold if the spaces are not compact. Although there are rather involved counterexamples, such as $X=\\mathbb{R}^3$ and $Y$ being the Whitehead manifold, there are also much simpler counterexamples, and I'll just mention one here that I thought of. Take $X$ to be the sphere minus three points and $Y$ to be a torus minus one point. Then, $X$ is just the same as the open disc minus two points, so $X\\times S^1$ embeds in $\\mathbb{R}^3$. On the other hand, there exists an embedded closed surface and curve in $Y\\times S^1$ with intersection number 1 (if it was embeddable in $\\mathbb{R}^3$, the intersection number would have to be zero). So, $X\\times S^1\\not\\cong Y\\times S^1$. I'll leave the construction of the closed surface and curve as an interesting exercise, and also the homeomorphism showing that $X\\times\\mathbb{R}\\cong Y\\times\\mathbb{R}$.\n\n1Compact Flat Riemannian Manifolds: I, Leonard S. Charlap, Annals of Mathematics\nSecond Series, Vol. 81, No. 1 (Jan., 1965), pp. 15-30. (link)\n2Sphere Bundles Over Spheres and Non-Cancellation Phenomena, P. Hilton, G. Mislin & J. Roitberg. J. London Math. Soc. (1972) s2-6(1): 15-23 (link)\n3On Cancellation in Groups, R. Hirshon, The American Mathematical Monthly. Vol. 76, No. 9 (Nov., 1969), pp. 1037-1039. (link) (Alt. freely available link from R. Hirshon's home page)", "meta": {"post_id": 396608, "input_score": 88, "output_score": 69, "post_title": "Does $X\\times S^1\\cong Y\\times S^1$ imply that $X\\times\\mathbb R\\cong Y\\times\\mathbb R$?"}}
{"input": "I'm not sure wether or not the following sum uniformly converge on $\\mathbb{R}$ :\n$$\\sum_{n=1}^{\\infty} \\frac{\\sin(n x) \\sin(n^2 x)}{n+x^2}$$\nCan someone help me with it? (I can't use Dirichlet' because of the areas where $x$ is close to $0$)", "output": "The series does converge uniformly. For the proof, put $S_n(x) = \\sum_{k = 0}^n \\sin{(kx)}\\sin{(k^2 x)}$ for $n\\geq 0$. The general idea is to use summation by parts to reduce ourselves to showing that $S_n(x)$ is bounded uniformly, and then to prove that by giving a closed form for $S_n(x)$.\nFirst, the summation by parts (I write $S_n$ in place of $S_n(x)$ for brevity):\n$$\n\\begin{align}\n\\sum_{n = 1}^N{\\sin{(nx)}\\sin{(n^2 x)}\\over n + x^2} & = \\sum_{n = 1}^N {1\\over n+x^2}(S_n - S_{n-1}) \\\\\n& = \\sum_{n = 1}^N {S_n\\over n+x^2} - \\sum_{n = 1}^N {S_{n-1}\\over n+x^2} \\\\\n& = \\sum_{n = 1}^N {S_n\\over n+x^2} - \\sum_{n = 0}^{N-1} {S_{n}\\over n+1+x^2} \\\\\n& = {S_N\\over N+x^2} - {S_0\\over 1 + x^2} + \\sum_{n = 1}^{N-1} S_n\\left({1\\over n+x^2} - {1\\over n+1+x^2}\\right) \\\\\n& = {S_N\\over N+x^2} + \\sum_{n = 1}^{N-1} {S_n\\over (n+x^2)(n+1+x^2)}.\n\\end{align}\n$$\nFrom here it is clear that it is enough to prove that $S_n = S_n(x)$ is uniformly bounded in $x$.\nTo do this, note that \n$$\n\\begin{align}\n2\\sin{(kx)}\\sin{(k^2 x)} &= \\cos{\\{(k^2 - k)x\\}} - \\cos{\\{(k^2 + k)x\\}} \\\\ \n& = \\cos{\\{k(k - 1)x\\}} - \\cos{\\{(k+1)k)x\\}},\n\\end{align}\n$$ \nand therefore that\n$$\n\\begin{align}\n2S_n(x) & = \\sum_{k = 0}^n 2\\sin{(kx)}\\sin{(k^2 x)} \\\\\n& = \\sum_{k = 0}^n\\left(\\cos{\\{k(k - 1)x\\}} - \\cos{\\{(k+1)k)x\\}}\\right)\n\\end{align}\n$$\nThe last sum telescopes, and we are left with\n$$\n2S_n(x) = 1 - \\cos{\\{n(n+1)x\\}},\n$$\nwhich is plainly bounded uniformly in $x$. So we're done.", "meta": {"post_id": 397097, "input_score": 43, "output_score": 67, "post_title": "Uniform convergence of $\\sum_{n=1}^{\\infty} \\frac{\\sin(n x) \\sin(n^2 x)}{n+x^2}$"}}
{"input": "We're all familair with this beautiful proof whether or not an irrational number to an irrational power can be rational. It goes something like this:\nTake $(\\sqrt{2})^{\\sqrt{2}}$\nIf it's rational, then you proved it, if it's irrational, take $((\\sqrt{2})^{\\sqrt{2}} ){^\\sqrt{2}} = 2$ and you've proved it.\nI'm wondering if you can raise $\\pi$ or $e$ to a certain non-trivial real power to make it rational? And if not, where is the proof that it can't be done? \np.s. - I almost left out the real part, but then I realized that $e^{i\\pi} = -1$.", "output": "Of course. Pick any positive rational $p$ and let $x=\\log_\\pi p$, then $\\pi^x=p$.", "meta": {"post_id": 399478, "input_score": 22, "output_score": 44, "post_title": "Can you raise $\\pi$ to a real power to make it rational?"}}
{"input": "Browsing the web I came across this:\n\nThe conjugacy class of an element $g\\in A_{n}$:\n\nsplits if the cycle decomposition of $g\\in A_{n}$ comprises cycles of distinct odd length. Note that the fixed points are here treated as cycles of length $1$, so it cannot have more than one fixed point; and\ndoes not split if the cycle decomposition of $g$ contains an even cycle or contains two cycles of the same length.\n\n\nAnybody with a proof?", "output": "Note the following: (1) The conjugacy class in $S_n$ of an element $\\sigma \\in A_n$ splits, iff there is no element $\\tau \\in S_n\\setminus A_n$ commuting with $\\sigma$. For if there is one, for each $\\tau' \\in S_n \\setminus A_n$ we have\n$$ \\tau'\\sigma{\\tau'}^{-1} = \\tau'\\sigma\\tau\\tau^{-1}\\tau'{}^{-1}\n    = (\\tau'\\tau)\\sigma(\\tau'\\tau)^{-1}\n$$\nand $\\tau\\tau' \\in A_n$. On the other hand, if $\\tau\\sigma\\tau^{-1}$ and $\\sigma$ with $\\tau \\in S_n\\setminus A_n$ are conjugate in $A_n$, then for some $\\tau' \\in A_n$, we have $\\tau\\sigma\\tau^{-1} = \\tau'\\sigma\\tau'^{-1}$, giving\n$$ \\tau'{}^{-1}\\tau \\sigma = \\sigma\\tau'{}^{-1}\\tau $$\nand hence $\\tau'{}^{-1}\\tau \\in S_n\\setminus A_n$ commutes with $\\sigma$.\nNow suppose, $\\sigma$ has a cycle $c_i$ of even length. A cycle of even length is an element of $S_n \\setminus A_n$, and as $\\sigma$ commutes with its cycles, we are done by the above. If $\\sigma$ has two cycles $(a_1\\ldots a_\\ell)$ and $(b_1 \\ldots b_\\ell)$ of the same odd length $\\ell$, then $(a_1b_1) \\ldots (a_\\ell b_\\ell)$ is a product of $\\ell$ transpositions (hence odd, so an element of $S_n \\setminus A_n$) commuting with $\\sigma$.\nNow suppose $\\sigma = c_1 \\cdots c_s$ is a product of odd cycles $c_i$ of distinct length $d_i$. Let $\\tau \\in S_n$ be a permutation commuting with $\\sigma$. Then $\\tau$ must fix each of the $c_i$, that is, $\\tau$ must be of the form $\\tau = c_1^{a_1} \\cdots c_s^{a_s}$ for some $a_i \\in \\mathbb Z$. But as the $c_i$ are even permutations (as cycles of odd length), we have $\\tau \\in A_n$. So no $\\tau \\in S_n \\setminus A_n$ commutes with $\\sigma$ and we are done.", "meta": {"post_id": 404656, "input_score": 22, "output_score": 40, "post_title": "Splitting of conjugacy class in alternating group"}}
{"input": "Please correct me if I'm wrong but here is what I understand from the theory of cardinal numbers :\n1) The definition of $\\aleph_1$ makes sense even without choice as $\\aleph_1$ is an ordinal number (whose construction doesnt depend on the axiom of choice) with some minimal property. With or without choice, there is no cardinal number $\\mathfrak{a}$ such that $\\aleph_0 < \\mathfrak{a} < \\aleph_1$.\n2) In ZFC, all cardinal numbers are $\\aleph$ and are comparable (by the trichotomic property of the ordinals). The continuum hypothesis states that $\\aleph_1 = 2^{\\aleph_0}$.\n3) In ZF, $2^{\\aleph_0}$ need not be an $\\aleph$. However, I don't know if talking about  $2^{\\aleph_0}$ in ZFC makes sense or not.\nDoes it make sense in ZF to define CH to be the statement that if a set is larger than the natural numbers, then it must contain a copy of the reals (up to a relabeling of the elements) no matter what the cardinality of the reals is ?", "output": "You are correct that without the axiom of choice $2^{\\aleph_0}\\newcommand{\\CH}{\\mathsf{CH}}$ may not be an $\\aleph$. Therefore the continuum hypothesis split into two inequivalent statements:\n\n$(\\CH_1)$ $\\aleph_0<\\mathfrak p\\leq2^{\\aleph_0}\\rightarrow2^{\\aleph_0}=\\frak p$.\n$(\\CH_2)$ $\\aleph_1=2^{\\aleph_0}$.\n\nWhereas the second variant implies that the continuum is well-ordered, the first one does not.\nYou suggested a third variant:\n\n$(\\CH_3)$ $\\aleph_0<\\mathfrak b\\rightarrow 2^{\\aleph_0}\\leq\\mathfrak b$.\n\nLet's see why $\\CH_3\\implies\\CH_2\\implies\\CH_1$, and that none of the implications are reversible.\nNote that if we assume $\\CH_3$, then it has to be that $2^{\\aleph_0}\\leq\\aleph_1$ and therefore must be equal to $\\aleph_1$. If we assume that $\\CH_2$ holds, then every cardinal less or equal to the continuum is finite or an $\\aleph$, so $\\CH_1$ holds as well.\nOn the other hand, there are models of $\\sf ZF+\\lnot AC$, such that $\\CH_1$ holds and $\\CH_2$ fails. For example, Solovay's model in which all sets are Lebesgue measurable is such model.\nBut $\\CH_2$ does not imply $\\CH_3$ either, because it is consistent that $2^{\\aleph_0}=\\aleph_1$, and there is some infinite Dedekind-finite set $X$, that is to say $\\aleph_0\\nleq |X|$. Therefore we have that $\\aleph_0<|X|+\\aleph_0$. Assuming $\\CH_3$ would mean that if $X$ is infinite, then either $\\aleph_0=|X|$ or $2^{\\aleph_0}\\leq|X|$. This is certainly false for infinite Dedekind-finite sets (one can make things stronger, and use sets that have no subset of size $\\aleph_1$, while being Dedekind-infinite).\n\nOne can also think of the continuum hypothesis as a statement saying that the continuum is a certain kind of successor to $\\aleph_0$. As luck would have it, there are $3$ types of successorship between cardinals in models of $\\sf ZF$, and you can find the definitions in my answer here.\nIt is easy to see that $\\CH_1$ states \"$2^{\\aleph_0}$ is a $1$-successor or $3$-successor of $\\aleph_0$\", and $\\CH_3$ states that \"$2^{\\aleph_0}$ is a $2$-successor of $\\aleph_0$\" -- while not explicitly, it follows from the fact that I used to prove $\\CH_3\\implies\\CH_2$.\nSo where does $\\CH_2$ gets here? It doesn't exactly get here. Where $\\CH_1$ and $\\CH_3$ are statements about all cardinals, $\\CH_2$ is a statement only about the cardinality of the continuum and $\\aleph_1$. So in order to subsume it into the $i$-successor classification we need to add an assumption on the cardinals in the universe, for example every cardinal is comparable with $\\aleph_1$ (which is really the statement \"$\\aleph_1$ is a $2$-successor of $\\aleph_0$\").\nAll in all, the continuum hypothesis can be phrased and stated in many different ways and not all of them are going to be equivalent in $\\sf ZF$, or even in slightly stronger theories (e.g. $\\sf ZF+AC_\\omega$).\n\nWithout the axiom of choice we can have two notions of ordering on the cardinals, $\\leq$ which is defined by injections and $\\leq^*$ which is defined by surjections, that is to say, $A\\leq^* B$ if there is a surjection from $B$ onto $A$, or if $A$ is empty. These notions are clearly the same when assuming the axiom of choice but often become different without it (often because we do not know if the equivalence of the two orders imply the axiom of choice, although evidence suggest it should -- all the models we know violate this).\nSo we can formulate $\\CH$ in a few other ways. An important fact is that $\\aleph_1\\leq^*2^{\\aleph_0}$ in $\\sf ZF$, so we may formulate $\\CH_4$ as $\\aleph_2\\not\\leq^*2^{\\aleph_0}$. This formulation fails in some models while $\\CH_1$ holds, e.g. in models of the axiom of determinacy, as mentioned by Andres Caicedo in the comments.\nOn the other hand, it is quite easy to come up with models where $\\CH_4$ holds, but all three formulations above fail. For example the first Cohen model has this property.\nAll in all, there are many many many ways to formulate $\\CH$ in $\\sf ZF$, which can end up being inequivalent without some form of the axiom of choice. I believe that the correct way is $\\CH_1$, as it captures the essence of Cantor's question. \n\nInteresting links:\n\nWhat's the difference between saying that there is no cardinal between $\\aleph_0$ and $\\aleph_1$ as opposed to saying that...\nRelationship between Continuum Hypothesis and Special Aleph Hypothesis under ZF", "meta": {"post_id": 404807, "input_score": 29, "output_score": 34, "post_title": "How to formulate continuum hypothesis without the axiom of choice?"}}
{"input": "The dual space to the Banach space $L^1(\\mu)$ for a sigma-finite measure $\\mu$ is $L^\\infty(\\mu)$, given by the correspondence \n$\\phi \\in L^\\infty(\\mu) \\mapsto I_\\phi$, where $I_\\phi(f) = \\int f \\cdot \\phi \\,d\\mu$ for $f \\in L^1(\\mu)$.\nNow, we consider this for more general measures $\\mu$. If $\\mu$ is not semifinite, then there is a measurable subset $A$ with $\\mu(A) = \\infty$ but $\\mu(B) = 0$ for every subset $B$ of $A$ with $\\mu(B) < \\infty$. Any $\\phi$ supported on $A$ will then map to the zero functional, so the correspondence is not one-to-one.\nIf $\\mu$ is semifinite, the correspondence is one-to-one, but is it onto? \nUsing the Radon-Nikodym Thorem, any member of $L^1(B)^*$ for $\\mu(B) < \\infty$ can be represented by an $L^\\infty(B)$ function class $[\\phi_B]$. Unlike in the $\\sigma$-finite measure case, however, I do not see an easy way to form a \"patchwork\" $\\phi$ with $\\phi|_B = \\phi_B$ a.e.$[\\mu]$. I suspect this may have an easy answer, but it eludes me. Or perhaps there is a well-known counterexample?", "output": "To answer the question in the title, there is the following general theorem:\n\nThe natural map $I \\colon L^\\infty(X) \\to (L^1(X))^\\ast$ is\n\nan isometric injection if and only if $(X,\\Sigma,\\mu)$ is semifinite.\nan isometric isomorphism if and only if $(X,\\Sigma,\\mu)$ is localizable.\n\n\nYou already covered point 1 in your question (the isometry property being straightforward). The second point is more delicate.\nA measure space $(X,\\Sigma,\\mu)$ is called localizable if it is semifinite and in addition the following condition holds:\nFor every family $\\mathcal{F} \\subseteq \\Sigma$ there is $H \\in \\Sigma$ such that \n\n$F \\setminus H$ is a null set for all $F \\in \\mathcal{F}$.\nIf $G \\in \\Sigma$ is such that $F \\setminus G$ is a null set for all $F \\in \\mathcal{F}$ then $H \\setminus G$ is a null set.\n\nLoosely speaking, this property asserts that every family $\\mathcal{F}$ of measurable sets has a smallest measurable envelope $H$ (up to null sets).\nThe definition of localizability implies via a slightly technical argument that one can glue together measurable functions. More precisely:\n\nLet $\\mathscr{F}$ be a family of functions such that each $f \\in \\mathscr{F}$ is defined and measurable on a measurable subset $D_f$ of the localizable measure space $(X,\\Sigma,\\mu)$. Suppose in addition that $f_1 = f_2$ a.e. on $D_{f_1} \\cap D_{f_2}$ whenever $f_1,f_2 \\in \\mathscr{F}$. Then there is a measurable function $g \\colon X \\to \\mathbb{R}$ whose restriction to $D_f$ satisfies $g|_{D_f} = f$ a.e. for all $f \\in \\mathscr{F}$.\n\nUsing this result one can patch together the Radon-Nikodym derivatives one obtains from restricting a continuous linear functional $\\varphi \\colon L^1(X) \\to \\mathbb{R}$ to the subspaces $L^1(F) \\subseteq L^1(X)$ where $F$ runs through the subsets of finite measure of $X$.\nThe proof of the converse direction proceeds by a direct verification of the localizability property of $(X,\\Sigma,\\mu)$ the fact that $I$ is isometric implies semi-finiteness by point 1. of the theorem and the \"envelope condition\" uses surjectivity of $I$.\nDetails can be found in 243G on page 153 of this PDF. The gluing property is proved in 213N on page 28.", "meta": {"post_id": 405357, "input_score": 46, "output_score": 40, "post_title": "When exactly is the dual of $L^1$ isomorphic to $L^\\infty$ via the natural map?"}}
{"input": "Is it possible to simplify this expression?\n$$\\frac{\\displaystyle\\Gamma\\left(\\frac{1}{10}\\right)}{\\displaystyle\\Gamma\\left(\\frac{2}{15}\\right)\\ \\Gamma\\left(\\frac{7}{15}\\right)}$$\nIs there a systematic way to check ratios of Gamma-functions like this for simplification possibility?", "output": "Amazingly, this can be greatly simplified.  I'll state the result first:\n$$\\frac{\\displaystyle\\Gamma\\left(\\frac{1}{10}\\right)}{\\displaystyle\\Gamma\\left(\\frac{2}{15}\\right)\\Gamma\\left(\\frac{7}{15}\\right)} = \\frac{\\sqrt{5}+1}{3^{1/10} 2^{6/5} \\sqrt{\\pi}}$$\nThe result follows first from a version of Gauss's multiplication formula:\n$$\\displaystyle\\Gamma(3 z) = \\frac{1}{2 \\pi} 3^{3 z-1/2} \\Gamma(z) \\Gamma\\left(z+\\frac13\\right) \\Gamma\\left(z+\\frac{2}{3}\\right)$$\nor, with $z=2/15$:\n$$\\Gamma\\left(\\frac{2}{15}\\right)\\Gamma\\left(\\frac{7}{15}\\right) = 2 \\pi \\,3^{1/10} \\frac{\\displaystyle\\Gamma\\left(\\frac{2}{5}\\right)}{\\displaystyle\\Gamma\\left(\\frac{4}{5}\\right)}$$\nNow use the duplication formula\n$$\\Gamma(2 z) = \\frac{1}{\\sqrt{\\pi}}\\, 2^{2 z-1}  \\Gamma(z) \\Gamma\\left(z+\\frac12\\right)$$\nor, with $z=2/5$:\n$$\\frac{\\displaystyle\\Gamma\\left(\\frac{2}{5}\\right)}{\\displaystyle\\Gamma\\left(\\frac{4}{5}\\right)} = \\frac{\\sqrt{\\pi} \\, 2^{1/5}}{\\displaystyle\\Gamma\\left(\\frac{9}{10}\\right)}$$\nPutting this all together, we get\n$$\\frac{\\displaystyle\\Gamma\\left(\\frac{1}{10}\\right)}{\\displaystyle\\Gamma\\left(\\frac{2}{15}\\right)\\Gamma\\left(\\frac{7}{15}\\right)} = \\frac{\\displaystyle\\Gamma\\left(\\frac{1}{10}\\right) \\Gamma\\left(\\frac{9}{10}\\right)}{\\sqrt{\\pi^3} \\, 2^{6/5} \\, 3^{1/10}}$$\nAnd now, we may use the reflection formula:\n$$\\Gamma(z) \\Gamma(1-z) = \\frac{\\pi}{\\sin{\\pi z}}$$\nWith $z=1/10$, and noting that\n$$\\sin{\\left(\\frac{\\pi}{10}\\right)} = \\frac{\\sqrt{5}-1}{4} = \\frac{1}{\\sqrt{5}+1}$$\nthe stated result follows.  This has been verified numerically in Wolfram|Alpha.", "meta": {"post_id": 406200, "input_score": 60, "output_score": 109, "post_title": "Is it possible to simplify $\\frac{\\Gamma\\left(\\frac{1}{10}\\right)}{\\Gamma\\left(\\frac{2}{15}\\right)\\ \\Gamma\\left(\\frac{7}{15}\\right)}$?"}}
{"input": "I observed the following limit empirically. Let $p_n$ be the $n$-th prime and $c_n$ be the $n$-th composite number then,  \n\n$$\n\\lim_{n \\to \\infty}\\frac{1}{n}\\sum_{i=1}^{n}\\frac{p_n c_n}{p_n c_n + p_i c_i} = \\frac{\\pi}{4}.\n$$\n\nI am looking for a proof.", "output": "It turns out that\n\nThe limit is correct, but\nIt's not saying anything that's very special to primes and composites.\n\nNote that (inspired by DonAntonio's answer)\n$$\n\\lim_{n\\to\\infty} \\frac1n \\sum_{k=1}^n \\frac{n^2}{n^2 + k^2} \n= \\lim_{n\\to\\infty} \\frac1n \\sum_{k=1}^n \\frac{1}{1 + \\left(\\frac{k}{n}\\right)^2} \n= \\int_{0}^{1} \\frac{dx}{1 + x^2} = \\frac{\\pi}{4}\n$$\nIt just so happens that $p_n$ and $c_n$ are (at a very loose level of approximation) on the order of $n$ each, so that $p_n c_n$ is of the order of $n^2$, and therefore your sum\n$$\n\\frac{1}{n}\\sum_{k=1}^{n}\\frac{p_n c_n}{p_n c_n + p_k c_k}\n\\approx \\frac{1}{n}\\sum_{k=1}^{n}\\frac{n^2}{n^2 + k^2},\n$$\nthe approximation turning exact in the limit.\nTo prove this rigorously, we have from the prime number theorem, that $p_n \\sim n\\ln n$, or to be precise\n$$p_n = n\\left(\\ln n + \\ln\\ln n - 1 + O\\left(\\frac{\\ln\\ln n}{\\ln n}\\right)\\right) = n\\left(\\ln n + o(\\ln n)\\right).$$\nSimilarly for the $n$th composite number $c_n$, we have \n$$c_n = n\\left(1 + \\frac1{\\ln n} + O\\left(\\frac{1}{\\ln^2 n}\\right)\\right) = n\\left(1 + o(1)\\right).$$\nSo $$p_nc_n = n^2\\left( \\ln n + o(\\ln n) \\right).$$\nConsider a particular value of $\\frac{k}{n}$ (say $\\alpha$) so that $k = \\alpha n$. Then \n$$\n\\frac{p_kc_k}{p_nc_n} \n= \\frac{k^2 (\\ln k + o(\\ln k))}{n^2(\\ln n + o(\\ln n))} \n= \\frac{k^2}{n^2}\\frac{\\ln n + \\ln \\alpha + o(\\ln n)}{\\ln n + o(\\ln n)}\n= \\frac{k^2}{n^2}(1 + o(1))\n$$\nTherefore\n$$\n\\lim_{n\\to\\infty} \\frac{1}{n}\\sum_{k=1}^{n}\\frac{p_n c_n}{p_n c_n + p_k c_k}\n= \\lim_{n\\to\\infty} \\frac{1}{n}\\sum_{k=1}^{n}\\frac{1}{1 + \\frac{p_k c_k}{p_nc_n}}\n= \\lim_{n\\to\\infty} \\frac{1}{n}\\sum_{k=1}^{n}\\frac{1}{1 + \\frac{k^2}{n^2}(1 + o(1))}\n= \\int_{0}^{1} \\frac{dx}{1 + x^2} = \\frac{\\pi}{4}.\n$$", "meta": {"post_id": 406550, "input_score": 35, "output_score": 45, "post_title": "A beautiful limit involving primes and composites"}}
{"input": "In 1960, Kervaire found the first example of a PL-manifold which does not admit a smooth structure. Since then, I understand that there are many examples of non-smoothable manifolds that can be built. My question is: Which is the \"easiest\" non-smoothable manifold? Easiest in the sense that among all the non-smoothable manifolds, this manifold has the easiest construction process.\nThanks everyone for your help !!\nCheers...", "output": "Here are explicit equations for nonsmoothable manifolds (all of which admit  triangulations). I do not know if these are the \"easiest\" but they are surely much more explicit than a description of the E8-manifolds, which is constructed as a result of some infinite, and very implicit, process (Freedman's work).\nConsider the equation\n$$\nz_1^5 + z_2^3 + z_3^2 +z_4^2 + z_5^2 +\\sum_{j=1}^5 e^{j-1} z_j^6=0\n$$\nin the complex affine space ${\\mathbb C}^5$. Here instead of $e$ one can take any transcendental number. Then the solution set of this equation is a piecewise-linear complex 4-dimensional (real 8-dimensional) manifold which is not homeomorphic to a smooth manifold.\nSee \"Algebraic equations for  nonsmoothable 8-dimensional manifolds\" by N.Kuiper, Math. Publ. of IHES, 1967.", "meta": {"post_id": 408221, "input_score": 45, "output_score": 35, "post_title": "The \"Easiest\" non-smoothable manifold"}}
{"input": "[Theorem]\nLet $V$ be an inner product space, and let $S$ be an orthogonal subset of $V$ consisting of nonzero vectors. Then $S$ is linearly independent.   \nAlso, orthogonal set and linearly independent set both generate the same subspace. \n(Is that right?)  \nThen\northogonal $\\rightarrow$ linearly independent\nbut\northogonal $\\nleftarrow$ linearly independent\nis that right?  \nOne more question.\nFor T/F,\nEvery orthogonal set is linearly independent (F)\nEvery orthonormal set is linearly independent (T)\nWhy?", "output": "For the theorem: \nHint: let $v_{1}, v_{2}, \\ldots, v_{k}$ be the vectors in $S$, and suppose there are $c_{1}, \\ldots, c_{k}$ such that $v_{1}c_{1} + \\cdots + v_{k}c_{k} = 0$. Then take the inner product of both sides with any vector in the set $v_{j}, 1 \\leq j \\leq k$. Conclude something about the coefficient $c_{j}$ using the fact that $v_{j} \\neq 0$ for all vectors $v_{j}$ in the set.\nFor your next question, orthogonal set implies linearly independent set with the condition that all the vectors in the set are nonzero - we need this in the above proof! (I'll address that in your true false questions). \nYou're right that linearly independent need not imply orthogonal. To see this, see if you can come up with two vectors which are linearly independent over $\\mathbb{R}^{2}$ but have nonzero dot product. (It shouldn't be too hard to do so!)\nFor your true false question, every orthogonal set need not be linearly independent, as orthogonal sets can certainly include the '$0$' vector, and any set which contains\nthe '$0$' vector is necessarily linearly dependent.\nHowever, every orthonormal set is linearly independent by the above theorem, as every orthonormal set is an orthogonal set consisting of nonzero vectors.", "meta": {"post_id": 409810, "input_score": 35, "output_score": 39, "post_title": "Orthogonality and linear independence"}}
{"input": "Lemma $1.92$ in Rotman's textbook (Advanced Modern Algebra, second edition) states,\nLet $G = \\langle a \\rangle$ be a cyclic group.\n(i) Every subgroup $S$ of $G$ is cyclic.\n(ii) If $|G|=n$, then $G$ has a unique subgroup of order $d$ for each divisor $d$ of $n$.\n\nI understand how every subgroup must be cyclic and that there must be a subgroup for each divisor of $d$. But how is that subgroup unique? I'm having trouble understanding this intuitively. For example, if we look at the cyclic subgroup $\\Bbb{7}$, we know that there are $6$ elements of order $7$. So we have six different cyclic subgroups of order $7$, right?\nThanks in advance.", "output": "Let $d$ be a divisor of $n=|G|$. Consider $H=\\{ x \\in G : x^d =1 \\}$. Then $H$ is a subgroup of $G$ and $H$ contains all elements of $G$ that have order $d$ (among others).\nIf $K$ is a subgroup of $G$ of order $d$, then $K$ is cyclic, generated by an element of order $d$. Hence, $K\\subseteq H$.\nOn the other hand, $x\\in H$ iff $x=g^k$ with $0\\le k < n$ and $g^{kd}=1$, where $g$ is a generator of $G$. Hence, $kd=nt$ and so $k=(n/d) t$. The restriction $0\\le k<n$ implies $0\\le t<d$, and so $H$ has exactly $d$ elements. Therefore, $K=H$.", "meta": {"post_id": 410389, "input_score": 40, "output_score": 50, "post_title": "Subgroups of a cyclic group and their order."}}
{"input": "I can't solve this problem:\nSuppose $f$ is polynomial function of even degree $n$ with always $f\\geq0$.\nProve that\n$f+f'+f''+\\cdots+f^{(n)}\\geq 0$.", "output": "Alternatively, by a standard Maximum Value Theorem argument, the even-degree polynomial $g=f+f'+\\dots+f^{(n)}$ has a global minimum at some point $a$. Then $g(a)=f(a)+g'(a)\\ge 0$, so $g\\ge g(a)\\ge 0$.", "meta": {"post_id": 414541, "input_score": 26, "output_score": 49, "post_title": "Prove that If $f$ is polynomial function of even degree $n$ with always $f\\geq0$ then $f+f'+f''+\\cdots+f^{(n)}\\geq 0$."}}
{"input": "Let $f:[0,1]\\to[0,1]$ be a continuous function such that its derivative $f'$ exists on $(0,1)$. Inspired by a similar question of myself here, I want to ask: \n\nIf $E\\subset[0,1]$ is a Borel set, is $f(E)$ still a Borel set? \n\n\nRemark: \n\nIt is known that $f$ maps sets of Lebesgue measure zero to sets of Lebesgue measure zero(for example, one may refer to this post), so it follows easily that $f$ maps Lebesgue measurable sets to Lebesgue measurable sets.\nEven if $f$ is assumed to be $C^1$, the answer is unclear to me.\nAs pointed by George Lowther in a comment in the same post, it is well known that for the natural projection $p:\\mathbb R^2\\to \\mathbb R$, $p(x,y)=x$, which is clearly real analytic, the image of  a Borel set of $\\mathbb R^2$ under $p$ may not be a Borel set in $\\mathbb R$. More details can be found here. It follows that for a high dimensional analog of the question, i.e. for $f:[0,1]^n\\to\\mathbb R^m$, $n\\ge 2$, even when $f$ is real analytic, the answer is negative. \n\n\nAny hint or suggestion is appreciated. Thanks in advance.", "output": "Even for $C^\\infty$ functions $f\\colon[0,1]\\to[0,1]$, it is possible that $f(E)$ is not Borel for some Borel sets $E$.\nStarting with the simpler case where $f$ is only required to be continuous, one way to construct a counterexample is to start with a space filling curve $g\\colon[0,1]\\to[0,1]^2$ whose image is the entire unit square. Then write $f=\\pi\\circ g$ where $\\pi\\colon[0,1]^2\\to[0,1]$ is the standard projection $\\pi(x,y)=x$. It is then a standard fact that there exist Borel sets $F\\subset[0,1]^2$ whose projection $\\pi(F)$ is not Borel. In fact, the projections of Borel sets are precisely the analytic sets, and there do exist analytic sets which are not Borel. Then, taking $E=g^{-1}(F)$, this is Borel set whose image under $f$ is the non-Borel set $\\pi(F)$.\nNow, moving on to the differentiable - and, even, smooth - case, the first thing to note is that the image of a differentiable function $g\\colon[0,1]\\to[0,1]^2$ must have zero Lebesgue measure, so cannot fill the unit square. However, this is not necessary. All we need is that the image of $g$ contains $C^2$ for some closed uncountable set $C\\subset[0,1]$ (e.g., a Cantor set). Such sets are always Borel isomorphic to the interval $[0,1]$ (as are all uncountable Polish spaces). So, we can procede as above and let $E=g^{-1}(F)$ where $F\\subset C^2$ is Borel such that $\\pi(F)$ is not Borel.\nSo, it remains to find a closed uncountable $C\\subset[0,1]$ and smooth $g\\colon[0,1]\\to[0,1]^2$ containing $C^2$ in its image. I will choose $C$ to be a 'thin Cantor set'. By this, I mean that $C$ is constructed in the same way as the fat Cantor set, except we remove larger and larger proportions of the set at each stage of the construction instead of smaller and smaller proportions. Choose a sequence $\\alpha_0,\\alpha_1,\\ldots\\in(0,1/2)$ with $\\alpha_n\\to0$ as $n\\to\\infty$. Then define a sequence of closed sets as follows. Let $C_0=[0,1]$. For each $n\\ge0$, once $C_n$ has been chosen (and is a finite union of disjoint closed intervals), we remove the centre $1-2\\alpha_n$ proportion of each interval in $C_n$ to obtain $C_{n+1}$. Then, $C=\\bigcap_nC_n$. Equivalently, $C$ is the set of points which can be written as\n$$\n(1-\\alpha_0)x_0+\\alpha_0(1-\\alpha_1)x_1+\\alpha_0\\alpha_1(1-\\alpha_2)x_2+\\alpha_0\\alpha_1\\alpha_2(1-\\alpha_3)x_3+\\cdots\n$$\nfor $x_0,x_1,\\ldots\\in\\lbrace0,1\\rbrace$. Note that if we had $\\alpha_0=\\alpha_1=\\cdots=1/3$ then this is the usual Cantor middle thirds set.\nNow, I'll describe the function $g$. This will be done in an inductive fashion using self-similarity. We can set $g=g_0$ where, for each $n\\ge0$, $g_n\\colon[0,1]\\to[0,1]^2$ contains scaled and maybe reflected copies of another such map $g_{n+1}$ in each of the four corner sections $[0,\\alpha_n]^2$, $[0,\\alpha_n]\\times[1-\\alpha_n,1]$, $[1-\\alpha_n,1]^2$, $[1-\\alpha_n,1]\\times[0,\\alpha_n]$ of the unit square, and follows straight lines in between these regions. This is as in the image below.\n\nLet $\\theta\\colon[0,1]\\to[0,1]$ be a smooth function with $\\theta(0)=0$, $\\theta(1)=1$ and such that its derivatives all vanish at the endpoints. For example, $\\theta(x)$ could be taken to be the integral of $\\exp(-1/(x(1-x)))$ appropriately scaled. Then, to be precise, we define $g=g_0$ where $g_n=(g_{n,1},g_{n,2})\\colon[0,1]\\to[0,1]^2$ are defined inductively by\n$$\ng_n(x)=\\begin{cases}\n\\alpha_n(g_{n+1,2}(7x),g_{n+1,1}(7x))&{\\rm if\\ } 0\\le x\\le 1/7,\\\\[.3em]\n(0,\\alpha_n+(1-2\\alpha_n)\\theta(7x-1))&{\\rm if\\ } 1/7 < x < 2/7,\\\\[.3em]\n\\alpha_n(g_{n+1,1}(7x-2),g_{n+1,2}(7x-2)-1)+(0,1)&{\\rm if\\ } 2/7\\le x\\le 3/7,\\\\[.3em]\n(\\alpha_n+(1-2\\alpha_n)\\theta(7x-3),1-\\alpha_n)&{\\rm if\\ } 3/7 < x < 4/7,\\\\[.3em]\n\\alpha_n(g_{n+1,1}(7x-4)-1,g_{n+1,2}(7x-4)-1)+(1,1)&{\\rm if\\ } 4/7 \\le x \\le 5/7,\\\\[.3em]\n(1,1-\\alpha_n-(1-2\\alpha_n)\\theta(7x-5))&{\\rm if\\ } 5/7 < x < 6/7,\\\\[.3em]\n\\alpha_n(-g_{n+1,2}(7x-6),1-g_{n+1,1}(7x-6))+(1,0)&{\\rm if\\ } 6/7 \\le x \\le 1.\n\\end{cases}\n$$\nThis does uniquely define $g$. Note that if $\\tilde g_n$ was any other sequence of functions to the unit square satisfying the same recurrence equations, then we have $\\lVert\\tilde g_n - g_n\\rVert=\\alpha_n\\lVert\\tilde g_{n+1} - g_{n+1}\\rVert$, where $\\lVert\\cdot\\rVert$ is the supremum norm. Also, as they both map into the unit square, $\\lVert \\tilde g_n-g_n\\rVert\\le\\sqrt{2}$. Putting these together,\n$$\n\\lVert\\tilde g_n-g_n\\rVert\\le\\alpha_n\\alpha_{n+1}\\cdots\\alpha_{n+r-1}\\sqrt{2}\n$$\nfor all $r\\ge0$. Letting $r$ go to infinity shows that $\\tilde g_n=g_n$.\nWe can also construct a sequence of smooth approximations to $g$. For each $N\\ge0$ define a map $g_N^{(N)}(x)=(\\theta(x),0)$. Then, inductively define $g_{N-1}^{(N)},g_{N-2}^{(N)},\\ldots,g_0^{(N)}$ using the recurrence equation above. Note that, at each step, $g^{(N)}_n$ is a smooth function with $g^{(N)}_n(0)=(0,0)$ and $g^{(N)}_n(1)=(1,0)$, and whose derivatives vanish to all orders at the end points. Next, using $D^rg$ to represent the r'th order derivatives of a function $g$, we have\n$$\n\\lVert D^rg_n^{(N)}\\rVert=7^r\\max\\left(\\alpha_n\\lVert D^rg_{n+1}^{(N)}\\rVert,(1-2\\alpha_n)\\lVert D^r\\theta\\rVert\\right)\n$$\nfor all $r\\ge1$. From this, we can see that $\\lVert D^rg_n^{(N)}\\rVert$ is bounded by $(7^r\\alpha_n)\\cdots(7^r\\alpha_{n+k-1})7^r\\lVert D^r\\theta\\rVert$ for some $k\\ge0$. As we chose $\\alpha_n$ to be tending to zero, this is bounded by some constant $L_r$ independently of $n,k$. Next, for any natural numbers $n < M\\le N$, the recurrence equation gives\n$$\n\\begin{align}\n\\lVert D^rg_n^{(M)}-D^rg_n^{(N)}\\rVert &= 7^r\\alpha_n\\lVert D^rg_{n+1}^{(M)}-D^rg_{n+1}^{(N)}\\rVert\\cr\n&=(7^r\\alpha_n)(7^r\\alpha_{n+1})\\cdots(7^r\\alpha_{M-1})\\lVert D^rg_M^{(M)}-D^rg_M^{(N)}\\rVert\\cr\n&\\le(7^r\\alpha_n)(7^r\\alpha_{n+1})\\cdots(7^r\\alpha_{M-1})2L_r\n\\end{align}\n$$\nAgain since we chose $\\alpha_n$ tending to zero, this shows that $\\lVert D^rg_n^{(M)}-D^rg_n^{(N)}\\rVert$ tends to zero as $M,N$ go to infinity. Therefore, the sequence of functions $g_n^{(N)}$ converges uniformly as $N\\to\\infty$, along with its derivatives to all orders. The limit, $g_n$, satisfies the defining recurrence equations above. So, $g$ is infinitely differentiable.\nNote, finally, that by choosing $x=x_0/7+x_1/7^2+x_2/7^3+\\cdots$ for appropriate $x_n\\in\\lbrace0,2,4,6\\rbrace$ we can find $x$ so that $g(x)$ is equal to any desired element of $C^2$.", "meta": {"post_id": 415759, "input_score": 29, "output_score": 36, "post_title": "Is the image of a Borel subset of $[0,1]$ under a differentiable map still a Borel set?"}}
{"input": "I'm currently learning about exact sequences in grad sch Algebra I course, but I really can't get the intuitive picture of the concept and why it is important at all.\nCan anyone explain them for me? Thanks in advance.", "output": "There are many good answers here.  I'd just like to add one example that made exact sequences 'click' for me, related to \"Euler's Formula\" relating the number of vertices ($V$), edges ($E$), and faces ($F$) of a simple non-self-intersecting polyhedron:\n$$ |F| - |E| + |V| = 2$$\nNow what does this have to do with exact sequences, you may well ask!  Well if you consider the free abelian groups generated by the set of faces, edges, and vertices separately, and create certain linear maps between them (see 'boundary maps' for simplicial homology), then you almost get an exact sequence:\n$$\n \\mathbb{Z}[F] \\to \\mathbb{Z}[E] \\to \\mathbb{Z}[V]\n$$\nIn fact, this sequence is exact at the middle term.  If we append two rank $1$ groups on the left and right (one with a generator the the whole solid $S$, and one generated by the symbol $e =$ '$\\emptyset$'), then you do get an exact sequence:\n$$\n  0 \\to \\mathbb{Z}[S] \\to \\mathbb{Z}[F] \\to \\mathbb{Z}[E] \\to \\mathbb{Z}[V]\n  \\to \\mathbb{Z}[e] \\to 0\n$$\nThen Euler's Formula is the statement just that the alternating sum of ranks is $0$ (because there is no torsion to keep track of).\n$$\n -1 + |F| - |E| + |V| - 1 = 0,\n$$\nor\n$$\n  |F| - |E| + |V| = 2.\n$$\nHope this helps!", "meta": {"post_id": 419329, "input_score": 97, "output_score": 44, "post_title": "Intuitive meaning of Exact Sequence"}}
{"input": "The Cauchy functional equation asks about functions $f \\colon \\mathbb R \\to \\mathbb R$ such that\n$$f(x+y)=f(x)+f(y).$$\nIt is a very well-known functional equation, which appears in various areas of mathematics ranging from exercises in freshman classes to constructing useful counterexamples for some advanced questions. Solutions of this equation are often called additive functions.\nAlso a few other equations related to this equation are often studied. (Equations which can be easily transformed to Cauchy functional equation or can be solved by using similar methods.)\nIs there some overview of basic facts about Cauchy equation and related functional equations - preferably available online?", "output": "I have no doubt that many resources about Cauchy functional equations and its relatives are available. But many properties of them have been shown in MSE posts, I will try to provide here list of links to the questions I am aware of. I have made this post CW, feel free to add more links and improve this answer in any way. Several links have been collected also in this post: Functional Equation $f(x+y)=f(x)+f(y)+f(x)f(y)$\n$\\newcommand{\\R}{\\mathbb{R}}\\newcommand{\\Q}{\\mathbb{Q}}\\newcommand{\\Zobr}[3]{{#1}\\colon{#2}\\to{#3}}$\n\nCauchy's additive functional equation\nWe are interested in functions $\\Zobr f{\\R}{\\R}$ such that\n$$f(x+y)=f(x)+f(y) \\tag{0}\\label{0}$$\nholds for each $x,y\\in\\R$.\n\n\nIf $f$ is a solution of \\eqref{0} and $q$ is a rational number, then $f(qx)=qf(x)$ holds for each $x\\in\\R$.\n\n\nSee e.g. If $f(x + y) = f(x) + f(y)$ showing that $f(cx) = cf(x)$ holds for rational $c$\n\n\nEvery continuous solution $\\Zobr f{\\R}{\\R}$ of \\eqref{0} has the form $f(x)=cx$ for some constant $c\\in\\R$.\n\n\nSee e.g. this question: I want to show that $f(x)=x.f(1)$ where $f:R\\to R$ is additive. or I need to find all functions $f:\\mathbb R \\rightarrow \\mathbb R$ which are continuous and satisfy $f(x+y)=f(x)+f(y)$\n\n\nIf $\\Zobr f{\\R}{\\R}$ is a solution of \\eqref{0} which is continuous at some point $x_0$, then it is continuous everywhere.\n\n\nSee e.g. Proving that an additive function $f$ is continuous if it is continuous at a single point\n\n\nIf a solution $f$ of \\eqref{0} is bounded on some interval, then $f(x)=cx$ for some $c\\in\\R$.\n\n\nOne part of this question is about the proof that locally bounded solution of \\eqref{0} is necessarily continuous: Real Analysis Proofs: Additive Functions\n\n\nEvery monotonic solution $\\Zobr f{\\R}{\\R}$ of \\eqref{0} has the form $f(x)=cx$ for some constant $c\\in\\R$.\n\n\nSee e.g. this question: Suppose $f(x)$ is linear i.e. $f(x + y) = f(x) +f(y) $ and monotone on $[-\\infty, +\\infty]$, then $f(x) = ax$, a real.\n\n\nIf we have the equation \\eqref{0} for function $\\Zobr f{[0,\\infty)}{[0,\\infty)}$, then every solution is of the form $f(x)=cx$.\n\n\nSee If $f:[0,\\infty)\\to [0,\\infty)$ and $f(x+y)=f(x)+f(y)$ then prove that $f(x)=ax$\n\n\nThere exist non-continuous solutions of \\eqref{0}.\n\n\nSee non-continuous function satisfies $f(x+y)=f(x)+f(y)$,\nDo there exist functions satisfying $f(x+y)=f(x)+f(y)$ that aren't linear?,\nOn sort-of-linear functions, Many other solutions of the Cauchy's Functional Equation\nNOTE: The proof of this fact needs at least some form of Axiom of Choice - it is not provable in ZF.\nFor the role of AC in this result, see this MO thread, and theses questions: Cauchy functional equation with non choice, A question concerning on the axiom of choice and Cauchy functional equation, Is there a non-trivial example of a $\\mathbb Q-$endomorphism of $\\mathbb R$?, What is $\\operatorname{Aut}(\\mathbb{R},+)$?\n\n\nThe graph $G(f)=\\big\\{\\big(x,f(x)\\big)\\big| x\\in\\R\\big\\}$ is a dense subset of $\\R^2$ for every non-continuous solution of \\eqref{0}.\n\n\nSee e.g. Graph of discontinuous additive function is dense in $ \\mathbb R ^ 2 $\n\n\nEvery anti-differentiable solution $\\Zobr f{\\R}{\\R}$ of \\eqref{0} has the form $f(x)=cx$ for some constant $c\\in\\R$.\n\n\nSee e.g. Solution of Cauchy functional equation which has an antiderivative\n\n\nEvery locally integrable solution $\\Zobr f{\\R}{\\R}$ of \\eqref{0} has the form $f(x)=cx$ for some constant $c\\in\\R$.\n\n\nSee e.g. How to prove $f(x)=ax$ if $f(x+y)=f(x)+f(y)$ and $f$ is locally integrable\n\n\nEvery Lebesgue measurable solution $\\Zobr f{\\R}{\\R}$ of \\eqref{0} has the form $f(x)=cx$ for some constant $c\\in\\R$.\n\n\nSee e.g. Additivity + Measurability $\\implies$ Continuity or Show that $f(x+y)=f(x)+f(y)$ implies $f$ continuous $\\Leftrightarrow$ $f$ measurable or Measurable Cauchy Function is Continuous or Prove that if a particular function is measurable, then its image is a rect line or Proving that $f$ is measurable with $f(x+y)= f(x)+f(y)$ then $f(x) =Ax$ for some $A\\in\\Bbb R$?.\n\nRelated equations\nThere are several functional equations that are closely related to Cauchy equation. They can be reduced to it by some methods or solved by very similar methods as the Cauchy equation.\nCauchy's exponential functional equation\n$$f(x+y)=f(x)f(y) \\tag{1}\\label{1}$$\n\n\nIf $f$ is a solution of \\eqref{1}, then either $f=0$ (i.e., $f$ is constant function equal to zero) or $f(x)>0$ for each $x\\in\\R$.\n\n\nSee e.g. Is there a name for function with the exponential property $f(x+y)=f(x) \\cdot f(y)$? and A non-zero function satisfying $g(x+y) = g(x)g(y)$ must be positive everywhere\n\n\nIf $f$ is a solution of \\eqref{1}, $x\\in\\Q$ and if we denote $a=f(1)$, then $f(x)=a^x$.\n\nEvery continuous solution of \\eqref{1} has the form $f(x)=a^x$ for some $a\\ge 0$.\n\n\n\nSee e.g. continuous functions on $\\mathbb R$ such that $g(x+y)=g(x)g(y)$ or How do I prove that $f(x)f(y)=f(x+y)$ implies that $f(x)=e^{cx}$, assuming f is continuous and not zero?\n\n\nIf a solution of \\eqref{1} is continuous at $0$, then it is continuous everywhere.\n\n\nIf $f\\colon \\mathbb{R} \\to \\mathbb{R}$ is such that $f (x + y) = f (x) f (y)$ and continuous at $0$, then continuous everywhere\n\n\nThere are non-continuous solutions of \\eqref{1}.\n\n\nThey can be obtained from non-continuous solutions of \\eqref{1}.\nSee Is the group isomorphism $\\exp(\\alpha x)$ from the group $(\\mathbb{R},+)$ to $(\\mathbb{R}_{>0},\\times)$ unique?\n\n\nIf a solution of \\eqref{1} is differentiable at $0$, then it is differentiable everywhere and $f'(x)=f(x)f'(0)$.\n\n\nSee Prove that $f'$ exists for all $x$ in $R$ if $f(x+y)=f(x)f(y)$ and $f'(0)$ exists and Differentiability of $f(x+y) = f(x)f(y)$ There are also posts searching for differentiable solutions: Solution for exponential function's functional equation by using a definition of derivative\nCauchy's logarithmic functional equation\n$$f(xy)=f(x)+f(y) \\tag{2}\\label{2}$$\nSee Examples of functions where $f(ab)=f(a)+f(b)$ and Is the product rule for logarithms an if-and-only-if statement?\nCauchy's multiplicative functional equation\n$$f(xy)=f(x)f(y) \\tag{3}\\label{3}$$\n\n\nEvery continuous solution of \\eqref{3} has the form $f(x)=x^a$\n\n\nSee If $f(xy)=f(x)f(y)$ then show that $f(x) = x^t$ for some t\n\n\nA solution of \\eqref{3} which is continuous at $1$ is continuous for each $x>0$.\n\n\nSee Functional equation $f(xy)=f(x)+f(y)$ and continuity\n\n\nIf a function fulfills both \\eqref{0} and \\eqref{3}, then either $f(x)=0$ or $f(x)=x$ (i.e., it is either zero function or the identity).\n\n\nNotice that we do not require continuity here. See Functional equations $f(x+y)= f(x) + f(y)$ and $f(xy)= f(x)f(y)$", "meta": {"post_id": 423492, "input_score": 154, "output_score": 167, "post_title": "Overview of basic facts about Cauchy functional equation"}}
{"input": "Let $R$ be a commutative ring and $I$ be an ideal of $R$.\nIs it true that $I$ is a principal ideal if and only if $I$ is a free $R$-module?", "output": "It is true that, if $R$ is a commutative ring and $I$ is a nonzero ideal of $R$, then $I$ is free iff $I$ is principal and generated by a non-zerodivisor.\nProof: Say $I$ is free.  By way of contradiction, suppose $I$ has an $R$-basis containing more than one element.  Let $e_1$ and $e_2$ be distinct elements of this basis.  Then we have $e_2e_1-e_1e_2=0$, which is impossible, since the $e_i$ are linearly independent (this is where we use commutativity).  Thus, $I$ has finite rank, and its rank is $1$.  Say $I$ is generated over $R$ by $e \\in R$; notice that $e$ must be a non-zerodivisor, or else $\\{e\\}$ would not be linearly independent.\nConversely, suppose $I$ is a principal ideal generated by a non-zerodivisor $e$.  Then the map $R \\to I$ given by $r \\mapsto re$ is an isomorphism of $R$-modules.", "meta": {"post_id": 423641, "input_score": 22, "output_score": 41, "post_title": "Principal ideal and free module"}}
{"input": "One may define the derivative of $f$ at $x$ as $\\lim\\limits_{h\\to0}\\cdots\\cdots\\cdots$ etc., and show that that has certain properties, but it also has a \"physical\" interpretation: it is an instantaneous rate of change.\nHow much money do I need to put in the bank today to have $\\$1$, $t$ years from now, assuming continuous compounding at a constant rate?  The answer is $(e^{-st} \\times \\$1)$ where $s$ is the annual interest rate.  So how much do I need to deposit today to get paid at a rate of $f(t)$ in dollars per year, $t$ years from now?  The answer is\n$$\n\\int_0^\\infty e^{-st} f(t)\\,dt.\n$$\nThis is a \"physical\" interpretation of the Laplace transform as the \"present value\" of a revenue stream, as a function of the interest rate.\nIs that pretty much the whole story of how to \"physically\" interpret the Laplace transform, or can more be said?", "output": "There's really a lot that can be said, but I will only delve into one geometric idea: the laplace transform, like many integral transforms, is a change of basis (\"coordinate system\"). I consider this a \"physical\" interpretation because it is geometric- you will be able to imagine the laplace transform's actions on a function much like you imagine how a matrix can geometrically transform a vector. This description is also mathematically precise.\nIf you have studied linear algebra you have likely run into the concept of a change of basis. Imagine you have a vector on $\\mathbb{R}^2$. You construct a basis $\\{\\hat{a}_x, \\hat{a}_y\\}$ to represent your vector as\n$$\nv = a_1\\hat{a}_x+a_2\\hat{a}_y\n$$\nTo express $v$ on an alternative basis, $\\{\\hat{b}_x, \\hat{b}_y\\}$, we make use of the inner product (or \"dot\" product) that geometrically does the job of computing projections. Basically, we are asking:\nGiven $[a_1, a_2]^T$, $\\{\\hat{a}_x, \\hat{a}_y\\}$, and $\\{\\hat{b}_x, \\hat{b}_y\\}$, compute $[b_1, b_2]^T$ so that,\n$$a_1\\hat{a}_x+a_2\\hat{a}_y = b_1\\hat{b}_x+b_2\\hat{b}_y$$\nIf we take the inner product of both sides of this equation with $\\hat{b}_x$, we get,\n$$\\hat{b}_x\\cdot(a_1\\hat{a}_x+a_2\\hat{a}_y) = \\hat{b}_x\\cdot(b_1\\hat{b}_x+b_2\\hat{b}_y)$$\n$$a_1(\\hat{b}_x\\cdot\\hat{a}_x)+a_2(\\hat{b}_x\\cdot\\hat{a}_y) = b_1$$\nwhere for the final simplification we made the common choice that our basis $\\{\\hat{b}_x, \\hat{b}_y\\}$ is orthonormal,\n$$\\hat{b}_x\\cdot\\hat{b}_x=1,\\ \\ \\ \\ \\hat{b}_x\\cdot\\hat{b}_y=0$$\nNow we have $b_1$ in terms of $a_1$ and $a_2$ with some weights $\\hat{b}_x\\cdot\\hat{a}_x$ and $\\hat{b}_x\\cdot\\hat{a}_y$ that are really equal to the cosines of the angles between these vectors, which we know since we were the ones who chose these coordinate systems. We do the same thing again but dotting with $\\hat{b}_y$ to compute $b_2$.\nWe can construct a matrix representation of this process,\n$$R \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}$$\n$$R = \\begin{bmatrix} \\hat{b}_x\\cdot\\hat{a}_x & \\hat{b}_x\\cdot\\hat{a}_y \\\\ \\hat{b}_y\\cdot\\hat{a}_x & \\hat{b}_y\\cdot\\hat{a}_y \\end{bmatrix}$$\nWhat does this have to do with the laplace transform? We need to generalize our understanding of vectors now. 3Blue1Brown does a great job of explaining this. Put concisely: a function can be thought of as a member of an infinite-dimensional vector space. It has one element for every value it can take on, in an ordered array.\n$$f(x) = \\begin{bmatrix} \\vdots \\\\ f(0.05) \\\\ f(0.051) \\\\ f(0.052) \\\\ \\vdots \\end{bmatrix}$$\nwhere of course, it has elements for values in between 0.0500000... and 0.051, etc. The indexing is uncountable, but the point is that they are vectors because they satisfy the properties of vectors. Just consider that what we can do with functions is identical to what we do with typical vectors in finite-dimensional spaces: when you add two functions you add them \"element-wise\", scalar multiplication does what it should, and furthermore, we even have an inner product! (So these functions are not just members of a vector space, but actually a Hilbert space).\n$$f(x)\\cdot g(x) = \\int_{-\\infty}^\\infty f(x)g(x) \\, dx$$\nWhat that integral means: for each $x$ (index), multiply $f(x)$ and $g(x)$ and sum that result down all the $x$. Look familiar?\n$$\\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\end{bmatrix} \\cdot \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots\\end{bmatrix} = a_1b_1 + a_2b_2 + \\cdots$$\nIf functions are vectors, then don't they need to be expressed on bases of other functions? Yes. How many basis functions do you need to span an infinite dimensional space? Infinitely many. Is it hard to describe an infinite number of unique functions? No. One example:\n$$f_n(x)=x^n\\ \\ \\ \\ \\forall n \\in \\mathbb{R}$$\nThough notice that we don't have both $x^n$ and $cx^n$, because they are linearly dependent; they span the same direction. We sometimes call linearly dependent functions \"like-terms\" in the sense that we can combine $x^n + cx^n = (1+c)x^n$ as opposed to linearly independent functions we cannot combine $x^n + x^{n+1}$.\nIf we took the inner product of one of these $f_i(x)$ with $f_j(x)$ we would certainly not get 0, so they don't have that nice orthogonal property where $\\hat{b}_x\\cdot\\hat{b}_y=0$. The polynomials don't form an orthogonal basis. However, the purely complex exponentials $e^{i\\omega}$ do.\nNow lets look at that mysterious laplace transform.\n$$\\mathscr{L}(f(x)) = \\int_{-\\infty}^\\infty e^{-sx}f(x) \\, dx$$\nImagine all possible values of $e^{-sx}$ in a big matrix, where each row corresponds to plugging in a specific $s$ and each column corresponds to plugging in a specific $x$. (This matrix is orthonormal if $s=i\\omega$ is purely imaginary, i.e. the Fourier transform).\nIf you select some $s$, you are plucking out a specific value of the function that resulted from the multiplication of this matrix with the vector $f(x)$, a function we call $F(s):=\\mathscr{L}(f(x))$. Specifically,\n$$F(s=3) = f(x) \\cdot e^{-3x}$$\n(where that dot is an inner product, not ordinary multiplication). We say that $F(s)$ is just $f(x)$ expressed on a basis of exponential functions. Choosing a specific value of $s=s_1$ is picking out the value of $f(x)$ in the $e^{-s_1x}$ direction. The entire $e^{-sx}$ can be viewed as the change of basis matrix.\nPhysical applications of thinking about things this way:\nDifferential equations. They model, like, everything. The exponential has a special relationship with the derivative. If we view the derivative as some operator $D$, we see that $D$ applied to an exponential just returns a scaled version of that same exponential.\n$$D(e^{\\lambda x}) = \\lambda e^{\\lambda x}$$\n$$D(f) = \\lambda f$$\n$$Av = \\lambda v$$\nLooks like one of them eigen-somethin equations. The exponential is the eigen-function of the derivative. Imagine we have an equation with just derivative operators and constants (throwing in anything else could potentially break the eigen relationship with the exponential). Things would be easier if we changed coordinates to the eigenbasis.\nOn the eigenbasis, the action of the original operators becomes simple scaling. We do this all the time to solve real world problems on finite-dimensional vector spaces (change your coordinates to make the problem simpler), now we're just doing it for infinite-dimensional vector spaces. This is why the laplace transform is useful for solving linear differential equations among many other things.\nYou should now also be able to explain Fourier transforms, Hankel transforms, and explain why the Legendre polynomials are good stuff, all at face value.\nRead up more on functional analysis if this interests you. You may also find linear systems theory and the concept of convolution cool and approachable now too. Good luck!\nP.S. This wasn't meant to be a book on this topic, and I wasn't being nearly as rigorous as I should be on Math Stack Exchange, but I figured that since you wanted intuition, a less rigorous more intuitive introduction to the functional analysis perspective was the right thing to provide.\nA follow-up Q/A can be found here!", "meta": {"post_id": 428408, "input_score": 43, "output_score": 49, "post_title": "Physical interpretation of Laplace transforms"}}
{"input": "Suppose we are given the following: $$\\left| \\int_{a}^{b} f(x) g(x) \\ dx \\right| \\leq \\int_{a}^{b} |f(x)|\\cdot |g(x)| \\ dx$$\nHow would we prove this? Does this follow from Cauchy Schwarz? Intuitively this is how I see it: In the LHS we could have a negative area that reduces the positive area. In the RHS the area can only increase because we take the absolute values of the functions first.", "output": "The big idea here is this:\nFirst: it is enough to show that\n$$\n\\left\\lvert\\int_a^b f(x)\\,dx\\right\\rvert\\leq\\int_a^b\\lvert f(x)\\rvert dx,\n$$\nsince you can replace $f(x)$ by $f(x)\\cdot g(x)$ to get the desired result.\nNow, notice that\n$$\n-\\lvert f(x)\\rvert\\leq f(x)\\leq \\lvert f(x)\\rvert\n$$\nfor all $x$; hence\n$$\n-\\int_a^b\\lvert f(x)\\rvert\\,dx\\leq \\int_a^b f(x)\\,dx\\leq\\int_a^b\\lvert f(x)\\rvert\\,dx.\n$$\nCan you finish it from here?", "meta": {"post_id": 429220, "input_score": 44, "output_score": 56, "post_title": "Integral Inequality Absolute Value: $\\left| \\int_{a}^{b} f(x) g(x) \\ dx \\right| \\leq \\int_{a}^{b} |f(x)|\\cdot |g(x)| \\ dx$"}}
{"input": "If $x$ is a positive rational number, but not an integer, then can $x^{x^{x^x}}$ be a rational number ?\n\nWe can prove that if $x$ is a positive rational number but not an integer, then $x^x$ can not be rational: \nDenote $x=\\dfrac{b}{a},(a,b)=1,x^x=\\dfrac{d}{c},(c,d)=1,$\n$$\\left(\\dfrac{b}{a}\\right)^\\dfrac{b}{a}=\\dfrac{d}{c} \\hspace{12pt}\\Rightarrow  \\hspace{12pt}\\left(\\dfrac{b}{a}\\right)^b=\\left(\\dfrac{d}{c}\\right)^a \\hspace{12pt}\\Rightarrow  \\hspace{12pt}b^b c^a=d^a a^b$$\nSince $(a,b)=1,(c,d)=1,$ we have $c^a\\mid a^b$ and $a^b\\mid c^a$, hence $a^b=c^a.$\nSince $(a,b)=1$, $a^b$ must be an $ab$-th power of an integer, assume that $a^b=t^{ab},$ then $a=t^a,$ where $t$ is a positive integer, this is impossible if $t>1,$ so we get $t=1,a=1$, hence $x$ is an integer.\nThen from Gelfond\u2013Schneider theorem , we can prove that if $x$ is a positive rational number but not an integer, then $x^{x^x}$ can not be rational. In fact, it can not be an algebraic number, because both $x$ and $x^x$ are algebraic numbers and $x^x$ is not a rational number.\n\nCan we prove that $x^{x^{x^x}}$ is irrational?\nCan $x^{x^{\\dots (n-th)^{\\dots x}}}~(n>1)$ be rational?", "output": "Let $x_1 = x$ and by induction $x_{n+1} = x^{x_n}$: so $x_1 = x$ is rational by hypothesis, $x_2 = x^x$ is algebraic irrational, $x_3 = x^{x^x}$ is transcendental by the Gelfond-Schneider theorem, and the question is to prove that $x_4, x_5,\\ldots$ are transcendental (or at least, irrational).\nI will assume Schanuel's conjecture and use it to prove by induction on $n\\geq 2$ that $x_3,x_4,\\ldots,x_n$ are algebraically independent (and, in particular, transcendental).  For $n=2$ there is nothing to prove: so let me assume the statement true for $n$ and prove it for $n+1$.\nSince $x_2$ is irrational, $x_1$ and $x_2$ are linearly independent over $\\mathbb{Q}$.  The induction hypothesis implies that $1,x_3,\\ldots,x_n$ are linearly independent over $\\mathbb{Q}^{\\mathrm{alg}}$ (the algebraics), so in particular $x_1,\\ldots,x_n$ are linearly independent over $\\mathbb{Q}$, and, of course, this implies that $x_1\\cdot\\log(x),\\ldots,x_n\\cdot\\log(x)$ are also such.\nNow Schanuel's conjecture then implies that among the $2n$ quantities $x_1 \\log(x),\\ldots,x_n \\log(x), x_2,\\ldots,x_{n+1}$ at least $n$ are algebraically independent.  Of course, we can remove $x_2$ from that list since it is algebraic, we can similarly replace both $x_1 \\log(x)$ and $x_2 \\log(x)$ by simply $\\log(x)$: so among $\\log(x),x_3,\\ldots,x_{n+1},x_3 \\log(x),\\ldots,x_n \\log(x)$ at least $n$ are algebraically independent.  But (for any $i$) this independent set cannot contain all three of $x_i$, $\\log(x)$ and $x_i\\log(x)$, and if it contains two of them then we can choose any two (namely, $x_i$ and $\\log(x)$): so that, in fact, the $n$ quantities $\\log(x),x_3,\\ldots,x_n,x_{n+1}$ are algebraically independent, which concludes the induction step (and moreover shows that $\\log(x)$ is also independent with the rest).", "meta": {"post_id": 430797, "input_score": 128, "output_score": 48, "post_title": "Can $x^{x^{x^x}}$ be a rational number?"}}
{"input": "Consider a function $f(t)$ with Fourier Transform $F(s)$. So $$F(s) = \\int_{-\\infty}^{\\infty} e^{-2 \\pi i s t} f(t) \\ dt$$\nWhat is the Fourier Transform of $f'(t)$? Call it $G(s)$.So $$G(s) = \\int_{-\\infty}^{\\infty} e^{-2 \\pi i s t} f'(t) \\ dt$$\nWould we consider $\\frac{d}{ds} F(s)$ and try and write $G(s)$ in terms of $F(s)$?", "output": "A simpler way, using the anti-transform:\n$$f(t) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} F(\\omega) \\, e^{i \\omega t} d\\omega$$\n$$f'(t) = \\frac{d}{dt}\\!\\left( \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} F(\\omega) \\, e^{i \\omega t} d\\omega \\right)= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty}   i \\omega \\, F(\\omega) \\, e^{i \\omega t} d\\omega$$\nHence the Fourier transform of $f'(t)$ is $ i \\omega \\, F(\\omega)$", "meta": {"post_id": 430858, "input_score": 75, "output_score": 132, "post_title": "Fourier Transform of Derivative"}}
{"input": "How to get the radius of an ellipse at a specific angle by knowing its semi-major and semi-minor axes?\nPlease take a look at this picture :", "output": "The polar form of the equation for an ellipse with \"horizontal\" semi-axis $a$ and \"vertical\" semi-axis $b$ is\n$$r = \\frac{ab}{\\sqrt{a^2\\sin^2\\theta+b^2\\cos^2\\theta}}$$\nHere, $\\theta$ represents the angle measured from the horizontal axis ($30.5^\\circ$ in your case), and $r$ is the distance from the center to the point in question (the radius you seek).", "meta": {"post_id": 432902, "input_score": 28, "output_score": 35, "post_title": "How to get the radius of an ellipse at a specific angle by knowing its semi-major and semi-minor axes?"}}
{"input": "Suppose I have: $w=2+3i$ and $x=1+2i$. What does it really mean to divide $w$ by $x$? \nEDIT: I am sorry that I did not tell my question precisely. (What you all told me turned out to be already known facts!) I was trying to ask the geometric intuition behind the division of complex numbers.", "output": "When you divide $a$ by $b$, you're asking \"What do I multiply $b$ by in order to get $a$?\".\nMultiplying two complex numbers multiplies their magnitudes and adds their phases:\n\nSo when you divide a complex $a$ by a complex $b$ you are asking: \"How much do I need to scale $b$ and rotate $b$ in order to get $a$? Please give me a complex number with a magnitude equivalent to how much I must scale and a phase equivalent to how much I must rotate.\".\nExample\nConsider $\\frac{1 + i}{1 - i}$. How much do we need to scale and rotate $1-i$ in order to make it the same as $1+i$?\nWell, when graphed on the complex plane you can see that $1-i$ has a 45 degree clockwise rotation and a magnitude of $\\sqrt{2}$. $1+i$, on the other hand, has a 45 degree counter-clockwise rotation and the same magnitude of $\\sqrt{2}$.\nSince the magnitudes are the same, we don't need any scaling. Our result's magnitude will be 1.\nTo rotate from 45 degrees clockwise to 45 degrees counter-clockwise, we must rotate 90 degrees counter-clockwise. Thus our result will have a phase of 90 degrees counter-clockwise (which is upwards along the imaginary Y axis).\nMove a distance of 1 up the imaginary Y axis and you get the answer... $\\frac{1 + i}{1 - i} = i$. We can confirm this by doing the multiplication: $(1-i) \\cdot i = i+1$.", "meta": {"post_id": 434275, "input_score": 40, "output_score": 58, "post_title": "What does it mean to divide a complex number by another complex number?"}}
{"input": "Evaluate the integral $$\\int^{\\frac{\\pi}{2}}_0 \\frac{\\sin^3x}{\\sin^3x+\\cos^3x}\\, \\mathrm dx.$$\nHow can i evaluate this one? Didn't find any clever substitute and integration by parts doesn't lead anywhere (I think).\nAny guidelines please?", "output": "As $$\\int_a^bf(x)dx=\\int_a^bf(a+b-x)dx,$$\nIf $$\\begin{eqnarray}I &=& \\int^{\\frac{\\pi}{2}}_0 \\frac{\\sin^nx}{\\sin^nx+\\cos^nx} \\,dx\\\\\n&=& \\int^{\\frac{\\pi}{2}}_0 \\frac{\\sin^n\\left(\\frac\\pi2-x\\right)}{\\sin^n\\left(\\frac\\pi2-x\\right)+\\cos^n\\left(\\frac\\pi2-x\\right)}\\, dx\\\\\n&=& \\int^{\\frac{\\pi}{2}}_0 \\frac{\\cos^nx}{\\cos^nx+\\sin^nx}\\, dx\n\\end{eqnarray}$$\n$$\\implies I+I=\\int_0^{\\frac\\pi2}dx$$ assuming $\\sin^nx+\\cos^nx\\ne0$ which is true as $0\\le x\\le \\frac\\pi2 $ \nGeneralization :\n$$\\text{If }J=\\int_a^b\\frac{g(x)}{g(x)+g(a+b-x)}dx, J=\\int_a^b\\frac{g(a+b-x)}{g(x)+g(a+b-x)}dx$$\n$$\\implies J+J=\\int_a^b dx$$ provided $g(x)+g(a+b-x)\\ne0$\nIf $a=0,b=\\frac\\pi2$ and $g(x)=h(\\sin x),$ \n$g(\\frac\\pi2+0-x)=h(\\sin(\\frac\\pi2+0-x))=h(\\cos x)$\nSo, $J$ becomes $$\\int_0^{\\frac\\pi2}\\frac{h(\\sin x)}{h(\\sin x)+h(\\cos x)}dx$$", "meta": {"post_id": 439851, "input_score": 21, "output_score": 37, "post_title": "Evaluate the integral $\\int^{\\frac{\\pi}{2}}_0 \\frac{\\sin^3x}{\\sin^3x+\\cos^3x}\\,\\mathrm dx$."}}
{"input": "Related: Can a sum of square roots be an integer?\n\nExcept for the obvious cases $n=0,1$, are there any values of $n$ such that $\\sum_{k=1}^n\\sqrt k$ is an integer? How does one even approach such a problem? (This is not homework - just a problem I thought up.)", "output": "No, it is not an integer.\nLet $p_1=2<p_2<p_3<\\cdots <p_k$ be all the primes $\\le n$. It is known that $$K=\\mathbb{Q}(\\sqrt{p_1},\\sqrt{p_2},\\ldots,\\sqrt{p_k})$$ is a Galois extension of the rationals of degree $2^k$.\nThe Galois group $G$ is an elementary abelian 2-group. An automorphism $\\sigma\\in G$ is fully determined by a sequence of $k$ signs $s_i\\in\\{+1,-1\\}$, $\\sigma(\\sqrt{p_i})=s_i\\sqrt{p_i}$, $i=1,2,\\ldots,k$. \nSee this answer/question for a proof of the dimension of this field extension. There are then several ways of getting the Galois theoretic claims. For example we can view $K$ as a compositum of linearly disjoint quadratic Galois extensions, or we can use the basis given there to verify that all the above maps $\\sigma$ are distinct automorphisms.\nFor the sum $S_n=\\sum_{\\ell=1}^n\\sqrt{\\ell}\\in K$ to be a rational number, it has to be fixed by all the automorphisms in $G$. This is one of the basic ideas of Galois correspondence. But clearly $\\sigma(S_n)<S_n$ for all the non-identity automorphisms $\\sigma\\in G$, so this is not the case.", "meta": {"post_id": 442259, "input_score": 73, "output_score": 87, "post_title": "Is $\\sqrt1+\\sqrt2+\\dots+\\sqrt n$ ever an integer?"}}
{"input": "My father recently lent me an old textbook of his, called Mathematical Methods of Physics by Mathews and Walker. I am working on the following exercise.\n\nConsider the differential equation\n$$y'' + p(x)y' + q(x)y = 0$$\non the interval $x \\in [a, b]$. Suppose we know two solutions,\n  $y_1(x)$ and $y_2(x)$, such that\n$$\\begin{align*} y_1(a) = 0 && y_2(a)\\neq0\\\\ y_1(b) \\neq 0 && y_2(b)=0\\\\ \\end{align*}$$\nGive the solution of the equation $$y'' + p(x)y' + q(x)y = f(x)$$\n  which satisfies $y(a)=y(b)=0$, in the form $$y(x) = \\int_a^b G(x, s)\\,f(s) \\, ds$$ \n  where $G(x, s)$, the so-called Green's function,\n  involves only the solutions $y_1$ and $y_2$ and assumes different\n  functional forms for $x<s$ and $s<x$.\n\nEarlier in the chapter, the authors begin to describe the general method for solving these types of equations and leave the completion to the student. This is what I have tried so far, following their advice:\nFirst we seek a solution of the form\n$$y=u_1(x)y_1(x) + u_2(x)y_2(x)$$\nwhere the $u_i(x)$ functions are to be determined. We will need the first and second derivatives of this expression in order to solve the differential equation. Thus,\n$$y' = u_1y_1' + u_2y_2' + u_1'y_1 + u_2'y_2$$\nBefore calculating $y''$, the authors suggest to set $u_1'y_1+u_2'y_2=0$ for the sake of convenience. (Anyone have a more sophisticated reasoning for this step?) In any case, I follow suit, and thus we have\n$$\\begin{align*}\ny' &= u_1y_1' + u_2y_2'\\\\\ny'' &= u_1y_1'' + u_2y_2'' + u_1'y_1'+u_2'y_2'\n\\end{align*}$$\nAt this point, I plug these representations back into the inhomogeneous equation.\n$$[u_1y_1'' + u_2y_2'' + u_1'y_1'+u_2'y_2'] + p(x)[u_1y_1' + u_2y_2'] + q(x)[u_1y_1+u_2y_2] = f(x)$$\nNext I reorganize the equation to take advantage of the fact that $y_1$ and $y_2$ solve the homogeneous equation.\n$$u_1[y_1''+p(x)y_1'+q(x)y_1] + u_2[y_2''+p(x)y_2'+q(x)y_2] + u_1'y_1'+u_2'y_2'= f(x)$$\nThe first two terms in the sum equal zero by assumption, thus\n$$u_1'y_1'+u_2'y_2'= f(x)$$\nWe also have the following equation, by our \"convenient assumption\"\n$$u_1'y_1+u_2'y_2=0$$\nHence, $u_1'=\\dfrac{-u_2'y_2}{y_1}$, which we then substitute to find $u_2'$\n$$\\begin{align*}\n\\dfrac{-u_2'y_2}{y_1}y_1'+u_2'y_2'&= f(x)\\\\\nu_2'&= \\dfrac{y_1f(x)}{y_2'y_1-y_2y_1'}\\\\\n\\end{align*}$$\nwhich gives us $u_2'$ purely in terms of the $y_i$'s and $f$, allowing us to find a similar expression for $u_1'$\n$$ u_1'=-\\dfrac{y_2f(x)}{y_2'y_1-y_2y_1'} $$\n[EDIT: This is where I was originally stuck. The rest of the solution is as follows, courtesy comment below from @icurays1]\nFinally we have our expression for $y$,\n$$\\begin{align*}\ny&=-y_1\\int_a^b\\dfrac{y_2f(s)}{y_2'(s)y_1(s)-y_2(s)y_1'(s)}ds + y_2\\int_a^b\\dfrac{y_1f(s)}{y_2'(s)y_1(s)-y_2(s)y_1'(s)} ds\\\\\ny(x)&=-\\int_a^b\\dfrac{y_1(x)y_2(s)f(s)}{y_2'y_1-y_2y_1'}ds + \\int_a^b\\dfrac{y_2(x)y_1(s)f(s)}{y_2'(s)y_1(s)-y_2(s)y_1'(s)} ds\\\\\n\\end{align*}$$\nand we reach the desired form:\n$$y(x)=\\int_a^b\\dfrac{y_2(x)y_1(s)-y_1(x)y_2(s)}{y_2'y_1-y_2y_1'}f(s)ds$$", "output": "Well, the problem of finding the solution to\n$$\ny'' + p(x) y' + q(x) y = f(x)\n$$\nwhen the solutions $y_1$ and $y_2$ of the homogeneous problem are \"known\" is in some thinking, the following:\nI know $y(x) = c_1 y_1(x) + c_2 y_2(x)$ solves\n$$\ny'' + p(x) y' + q(x) y = 0.\n$$\nWhat if I let $c_1$ and $c_2$ vary, in order to satisfy point by point, the non-homogeneous equation. I propose then $y(x) = c_1(x) y_1(x) + c_2(x) y_2(x)$, and see where it leads me. For that, we calculate\n$$\ny'(x) = c_1(x) y_1'(x) + c_2(x) y_2'(x) + c_1'(x) y_1(x) + c_2'(x) y_2(x),\n$$\nand $y''$. Before doing that, let's take a look on $y'(x)$. If we let the terms of the first derivative of $c_1$, and $c_2$ to subsist, then $y''$ will have terms of the second derivative on $c_1$ and $c_2$, and we will achieve nothing in terms of simplicity. What was a second order ODE with one unknown ($y$), will become a second order ODE with two unknowns ($c_1$ and $c_2$), with the lack of a second equation. This is why, in the construction, one asks for \n$$\nc_1'(x) y_1(x) + c_2'(x) y_2(x) = 0.\n$$\nIf this condition is fulfilled, we achieve one very important thing: to reduce the non-homogeneous second order ODE, to a system of two first order ODEs, which, incidentally, we know how to solve exaclty (modulus integration). This would be one rational explanation in choosing the above condition.\nNow back to the problem at hand. When one has initial conditions, the problem is stated in the following way:\n$$\ny'' + p(x) y' + q(x) y = f(x), \\quad y(a) = y_0, \\quad y'(a) = y_0', \\quad a < x < b.\n$$\nThe easiest way to solve the full problem is to take advantage of the linearity and split it in two:\n$$\ny_h'' + p(x) y_h' + q(x) y = 0, \\quad y_h(a) = y_0, \\quad y_h'(a) = y_0',\n$$\nand\n$$\ny_p'' + p(x) y_p' + q(x) y = f(x), \\quad y_p(a) = 0, \\quad y_p'(a) = 0,\n$$\nand then $y(x) = y_h(x) + y_p(x)$. In other words, we solve the homogeneous ODE with non-homogeneous initial conditions, and then the non-homogeneous ODE with homogeneous initial conditions, treating the non-homogeneities separately.\nThe homogeneous solution will be\n$$\ny_h(x) = k_1 y_1(x) + k_2 y_2(x),\n$$\nwhere $y_1$ and $y_2$ are the two linearly independent solutions, and $k_1$ and $k_2$ are determined by solving the system\n$$\n\\begin{pmatrix} y_1(a) & y_2(a) \\\\ y_1'(a) & y_2'(a) \\end{pmatrix} \\begin{pmatrix} k_1 \\\\ k_2 \\end{pmatrix} = \\begin{pmatrix} y_0 \\\\ y_0' \\end{pmatrix}.\n$$\nFor $y_p$, we propose the solution\n$$\ny_p(x) = c_1(x) y_1(x) + c_2(x) y_2(x)\n$$\nwith the condition $c_1'(x) y_1(x) + c_2(x) y_2(x) = 0$ (explained above). Then, substituting in its ODE, we can determine $c_1(x)$ and $c_2(x)$ by solving the system\n$$\n\\begin{pmatrix} y_1(x) & y_2(x) \\\\ y_1'(x) & y_2'(x) \\end{pmatrix} \\begin{pmatrix} c_1'(x) \\\\ c_2'(x) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ f(x) \\end{pmatrix}\n$$\nand then\n\\begin{align}\nc_1'(x) &= -\\frac{y_2(x) f(x)}{W(y_1,y_1)(x)} \\\\\n\\\\\nc_2'(x) &= \\frac{y_1(x) f(x)}{W(y_1,y_2)(x)}\n\\end{align}\nwhere $W(y_1,y_2)(x) = y_1 y_2'- y_1' y_2$ is the Wronskian of $y_1$ and $y_2$.\nThe key here is that, in order to obtain $c_1$ and $c_2$, one has to integrate. To do this, we need limits of integration. Conveniently, we know the initial conditions for $y_p$; they can be read as\n$$\n\\begin{pmatrix} y_1(a) & y_2(a) \\\\ y_1'(a) & y_2'(a) \\end{pmatrix} \\begin{pmatrix} c_1(a) \\\\ c_2(a) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis means that $c_1(a) = c_2(a) = 0$. Finally, performing the integration,\n$$\n\\int_{a}^x c_1'(s) ds = c_1(x) = -\\int_{a}^x \\frac{y_2(s)}{W(s)} f(s) ds\n$$\n$$\n\\int_{a}^x c_2'(s) ds = c_2(x) = \\int_{a}^x \\frac{y_1(s)}{W(s)} f(s) ds\n$$\nand then\n$$\ny_p(x) = \\int_{a}^x \\frac{y_1(s) y _2(x) - y_1(x)y_2(s)}{W(s)} f(s) ds\n$$\nor\n$$\ny_p(x) = \\int_{a}^{b} g(x,s) f(s) ds\n$$\nwhere\n$$\ng(x,s) = \\begin{cases} \\frac{y_1(s) y _2(x) - y_1(x)y_2(s)}{W(s)} & \\mbox{if }a < s < x \\\\\n0 & \\mbox{if } x < s < b \\end{cases}\n$$\nFinally\n$$\ny(x) = k_1(x) y_1(x) + k_2(x) y_2(x) + \\int_a^b g(x,s) f(s) ds.\n$$\nThe function $g(x,s)$ is called Green's function, and is completely associated with the problem \n$$\nL y = \\frac{d^2 y}{d x^2} + p(x) \\frac{d y}{d x} + q(x) y = f(x), \\quad B y = \\begin{pmatrix} y(a) \\\\ y'(a) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad a < x < b\n$$\nThe Green's functions is some sort of \"inverse\" of the operator $L$ with boundary conditions $B$.\nWhat happens with boundary conditions on $a$ and $b$?\nWell, in this case, the boundary conditions $B$ are of the form\n$$\nBy = \\begin{pmatrix} \\alpha_1 y(a) + \\beta_1 y'(a) + \\gamma_1 y(b) + \\delta_1 y'(b) \\\\ \\alpha_2 y(a) + \\beta_2 y'(a) + \\gamma_2 y(b) + \\delta_2 y'(b) \\end{pmatrix}\n$$\nand need not to be homogeneous, i.e. $B y = (r_1, r_2)^T$. If the boundary conditions are linearly independent, then the problem is well defined, and we have to solve\n$$\nL y = f(x), \\quad B y = (r_1, r_2)^T, \\quad a < x < b.\n$$\nThe case where $\\alpha_2 = \\beta_1 = \\gamma_{1,2} = \\delta_{1,2} = 0$ is the well known initial condition problem, solved above. The case $\\gamma_1 = \\delta_1 = \\alpha_2 = \\beta_2 = 0$ is called unmixed, while the other is mixed. Examples of unmixed boundary conditions are Dirichlet conditions $y(a) = y(b) = 0$, Neumann conditions $y'(a) = y'(b)$ and Robin conditions $y(a) + \\alpha y'(a) = y(b) + \\beta y'(b) = 0$. Examples of mixed boundary conditions are periodic $y(a) = y(b)$, $y'(a) = y'(b)$, and antiperiodic $y(a) = -y(b)$, $y'(a) = -y'(b)$.\nAs with initial conditions, the best way to work is to split the problem in two:\n$$\nL y_h = 0, \\quad B y_h = (r_1, r_2)^T\n$$\nand\n$$\nL y_p = f(x), \\quad B y_p = (0, 0)^T\n$$\nSolving for $y_h$ is equivalent for all types of boundary conditions, and I'll ignore this problem (if you have doubts is a good exercise to work out the details yourself).\nUnmixed boundary conditions\n$$\nB y = \\begin{pmatrix} B_1 y \\\\ B_2 y \\end{pmatrix} = \\begin{pmatrix} \\alpha_1 y(a) + \\beta_1 y'(a) \\\\ \\gamma_2 y(b) + \\delta_2 y'(b) \\end{pmatrix} = 0\n$$\nLet $w_1(x) = h_{11} y_1(x) + h_{22} y_2(x)$, and $w_2(x) = h_{12} y_1(x) + h_{22} y_2(x)$ with $h_{ij}$ constants, such that $B_1 w_1 = 0$ and $B_2 w_2 = 0$. We propose\n$$\ny_p(x) = c_1(x) w_1(x) + c_2(x) w_2(x)\n$$\nwith the (very natural) condition $c_1'(x) w_1(x) + c_2'(x) w_2(x) = 0$. As in the initial conditions problem, it's easy to see that\n\\begin{align}\nc_1'(x) &= -\\frac{w_2(x) f(x)}{W(w_1,w_2)(x)}\\\\\n\\\\\nc_2'(x) &= \\frac{w_1(x) f(x)}{W(w_1,w_2)(x)}\n\\end{align}\nwhere $W(w_1,w_2)(x) = W(x)$ is the Wronskian of $w_1$ and $w_2$. To determine the limits of integration, we simply evaluate the boundary condtions:\n\\begin{align} \nB_1 y_p &= c_1(a) B_1 w_1 + c_2(a) B_1 w_2 = c_2(a) B_1 w_2 = 0\\\\\nB_2 y_p &= c_1(b) B_2 w_1 + c_2(b) B_2 w_2 = c_1(b) B_2 w_1 = 0\n\\end{align}\nThis relation tells us that $c_1$ and $c_2$ need to fulfill the conditions $c_2(a) = c_1(b) = 0$, in order to satisfy the boundary conditions. Then\n$$\n\\int_x^b c_1'(s) ds = - c_1(x) = \\int_x^b \\frac{w_2(s)}{W(s)}f(s) ds\n$$\nand\n$$\n\\int_a^x c_2(s) ds = c_2(x) = \\int_a^x \\frac{w_1(s)}{W(s)}f(s) ds\n$$\nFinally\n$$\ny_p(x) = \\int_x^b \\frac{w_1(x)w_2(s)}{W(s)} f(s) ds + \\int_a^x \\frac{w_2(x) w_1(s)}{W(s)} f(s) ds\n$$\nor\n$$\ny_p(s) = \\int_a^b g(x,s) f(s) ds\n$$\nwhere\n$$\ng(x,s) = \\begin{cases} \\frac{w_1(s) w_2(x)}{W(s)} & \\mbox{if } a < s < x < b\\\\\n\\\\\n\\frac{w_1(x) w_2(s)}{W(s)} & \\mbox{if } a < x < s < b\\end{cases}\n$$\nMixed boundary conditions\nThe easiest way here would be to use the result above. Let\n$$\ny_1(x) = c_{11}(x) w_{11}(x) + c_{12}(x) w_{12}(x)\n$$\nand\n$$\ny_2(x) = c_{21}(x) w_{21}(x) + c_{22}(x) w_{22}(x)\n$$\nwhere $w_{11}$, $w_{12}$, $w_{21}$, $w_{22}$, are chosen in order to fulfill\n$$\nB_1 y_1 = \\begin{pmatrix} \\alpha_1 y_1(a) + \\beta_1 y_1'(a) \\\\ \\gamma_2 y_1(b) + \\delta_2 y_1'(b)\\end{pmatrix} = 0\n$$\nand\n$$\nB_2 y_2 = \\begin{pmatrix} \\gamma_1 y_2(b) + \\delta_1 y_2'(b) \\\\ \\alpha_2 y_2(a) + \\beta_2 y_2'(a)\\end{pmatrix} = 0\n$$\nBy construction, let\n\\begin{align}\ny_1(x) &= \\frac{1}{2}\\int_a^b g_1(x,s) f(s) ds\\\\\n\\\\\ny_2(x) &= \\frac{1}{2}\\int_a^b g_2(x,s) f(s) ds\n\\end{align}\nwhere\n\\begin{align}\ng_1(x,s) &= \\begin{cases} \\frac{w_{11}(s) w_{12}(x)}{W(s)} & \\mbox{if } a < s < x < b\\\\\n\\\\\n\\frac{w_{11}(x) w_{12}(s)}{W(s)} & \\mbox{if } a < x < s < b\\end{cases}\n\\\\ \\\\\ng_2(x,s) &= \\begin{cases} \\frac{w_{21}(s) w_{22}(x)}{W(s)} & \\mbox{if } a < s < x < b\\\\\n\\\\\n\\frac{w_{21}(x) w_{22}(s)}{W(s)} & \\mbox{if } a < x < s < b\\end{cases}\n\\end{align}\nThen, taking $y(x) = y_h(x) + y_1(x) + y_2(x)$, where $y_h(x) = k_1 y_1(x) + k_2 y_2(x)$, we have\n$$\nL y = L y_h + L y_1 + L y_2 = L y_1 + L y_2 = \\frac{1}{2}f(x) + \\frac{1}{2}f(x) = f(x)\n$$\nand\n\\begin{align}\nB y &= B y_h + B y_1 + B y_2 \\\\\n&= B y_h + B_1 y_1 + B_1 y_2 + B_2 y_1 + B_2 y_2 \\\\\n&= B y_h + B_2 y_1 + B_1 y_2 = \\begin{pmatrix} r_1 \\\\ r_2 \\end{pmatrix}\n\\end{align}\nIf we choose $k_1$ and $k_2$ in such way that\n$$\nB y_h = \\begin{pmatrix} r_1 \\\\ r_2 \\end{pmatrix} - B_2 y_1 - B_1 y_2\n$$\nthen\n$$\ny(x) = y_h(x) + y_1(x) + y_2(x) = y_h(x) + \\int_a^b g(x,s) f(s) ds\n$$\nwhere\n$$\ng(x,s) = \\begin{cases} \\frac{w_{11}(s) w_{12}(x) + w_{21}(s) w_{22}(x)}{2 W(s)} & \\mbox{if } a < s < x < b\\\\\n\\\\\n\\frac{w_{11}(x) w_{12}(s) + w_{21}(x) w_{22}(s)}{2W(s)} & \\mbox{if } a < x < s < b\\end{cases}\n$$\nA very thorough treatment of the construction of Green's functions for second order ODEs can be found in the fantastic book Principles and Techniques of Applied Mathematics, by Bernard Friedman.", "meta": {"post_id": 442277, "input_score": 26, "output_score": 41, "post_title": "General Solution of a Differential Equation using Green's Function"}}
{"input": "Ok, so the Chi-Squared distribution with $n$ degrees of freedom is the sum of the squares of $n$ independent Gaussian random variables.\nThe trouble is, my Gaussian random variables are not independent. They do however all have zero mean and the same variance. Supposing I have a covariance matrix---which again is not a diagonal matrix because they aren't independent, but all the elements along the diagonal are equal to each other because they have the same variance, and in fact the covariance matrix is a symmetric Toeplitz matrix (and I'm not saying that this is important to the solution if there is one, but if it's a necessary property to get anywhere, by all means use that fact)---is there some way to decompose this sum of squares of these Gaussian random variables into perhaps a sum of chi-squared random variables and possibly Gaussian random variables? In other words, I can't directly just square them all and add them together and call it a chi squared distribution because a chi squared distribution is a sum of independent Gaussian squares, and they aren't independent.\nI know how to find a linear transformation of the Gaussian random variables which are $n$ independent Gaussians, but that's no help because they aren't the things being squared, you see.", "output": "Lets assume you have $X=(X_1, \\dots, X_n)$ a random vector with multinormal distribution with expectation vector $\\mu$ and covariance matrix $\\Sigma$. We are interested in the quadratic form $Q(X)= X^T A X  = \\sum \\sum a_{ij} X_i X_j$. Define $Y = \\Sigma^{-1/2} X$ where we  are assuming $\\Sigma$ is invertible.  Write also $Z=(Y-\\Sigma^{-1/2} \\mu)$, which will have expectation zero and variance matrix the identity. \nNow\n$$\n  Q(X) = X^T A X= (Z+\\Sigma^{-1/2} \\mu)^T \\Sigma^{1/2}A\\Sigma^{1/2} (Z+\\Sigma^{-1/2} \\mu).\n$$\nUse the spectral theorem now and write $\\Sigma^{1/2}A \\Sigma^{1/2} = P^T \\Lambda P$\nwhere $P$ is an orthogonal matrix (so that$P P^T=P^T P=I$) and $\\Lambda$ is diagonal with positive diagonal elements $\\lambda_1, \\dotsc, \\lambda_n$.  Write $U = P Z$ so that $U$ is multivariate normal with identity covariance matrix and expectation zero. \nWe can compute\n$$\n Q(X) = (Z+\\Sigma^{-1/2} \\mu)^T \\Sigma^{1/2}A\\Sigma^{1/2} (Z+\\Sigma^{-1/2} \\mu)  \\\\\n      = (Z+\\Sigma^{-1/2} \\mu)^T  P^T \\Lambda P  (Z+\\Sigma^{-1/2} \\mu)  \\\\\n      = (PZ+P\\Sigma^{-1/2} \\mu)^T \\Lambda   (PZ+P\\Sigma^{-1/2} \\mu)   \\\\\n  = (U+b)^T  \\Lambda  (U+b)\n$$\nwhere now\n$b = P \\Sigma^{-1/2} \\mu $. (There was a small typo in above defs of $U$ and $b$, now corrected.)  So:\n$$\n  Q(X) = X^T A X = \\sum_{j=1}^n \\lambda_j (U_j+b_j)^2\n$$\nIn your case, $\\mu=0$ so $b=0$ so your quadratic form is a linear combination of independent chi-square variables, each with one degree of freedom.  In the general case, we will get a linear combination of independent non-central chisquare variables.\nIf you want to work numerically with that distribution, there is an CRAN package (that is, package for R) implementing it, called  CompQuadForm.\nIf you want (much) more detail, there is a book dedicated to the topic, Mathai & Provost: \"Quadratic forms in random variables\".", "meta": {"post_id": 442472, "input_score": 40, "output_score": 54, "post_title": "Sum of squares of dependent Gaussian random variables"}}
{"input": "The question is to find a purely probabilistic proof of the following identity, valid for every integer $n\\geqslant1$, where $(S_n)_{n\\geqslant0}$ denotes a standard simple random walk:\n\n$$\nE[(S_n)^2;S_{2n}=0]=\\frac{n}2\\,P[S_{2n-2}=0].\n$$\n\nStandard simple random walk is defined as $S_0=0$ and $S_n=\\sum\\limits_{k=1}^nX_k$ for every $n\\geqslant1$, where $(X_k)_{k\\geqslant1}$ is an i.i.d. sequence such that $P[X_k=+1]=P[X_k=-1]=\\frac12$.\nOf course, the RHS of the identity is\n$$\n\\frac{n}{2^{2n-1}}\\,{2n-2\\choose n-1}.\n$$\nFor a combinatorial proof, see this MSE question and its comments. \nFor an accessible introduction to the subject, see the corresponding chapter of the Chance project.", "output": "I don't know if what I will write is a \"purely probabilistic proof\" as the question requests, or a combinatorial proof, but Did should decide that. At the end I do use combinatorial identities (UPDATE 12-1-2014: an alternative final step of the proof has been found that does not use the identities. The initial final step is preserved at the end, in grey).\nThe LHS of the identity can be re-written as\n$$E[(S_n)^2;S_{2n}=0] = E\\left[\\left(\\sum_{i=1}^nX_i\\right)^2 ; S_{2n}=0\\right]=E\\left[\\sum_{i=1}^nX_i^2 + \\sum_{i\\neq j}X_iX_j ; S_{2n}=0\\right]$$\nhence\n$$E[(S_n)^2;S_{2n}=0]=E\\left[\\sum_{i=1}^nX_i^2 ; S_{2n}=0\\right]+ E\\left[\\sum_{i\\neq j}X_iX_j ; S_{2n}=0\\right] $$\nBut $X_i^2=1$ almost surely for every $i$. Likewise, the distribution of $(X_i,X_j)$ conditionally on $S_{2n}=0$ does not depend on $i\\ne j$ since the $X_i$s are i.i.d and $S_{2n}$ is a symmetric function of $(X_i)$. So we arrive at\n$$E[(S_n)^2;S_{2n}=0] = nP[S_{2n}=0] +n(n-1)E\\left(X_{2n-1}X_{2n}; S_{2n}=0\\right)$$\nThe random variable $X_{2n-1}X_{2n}$ takes on the values $\\{-1,1\\}$. Let $q \\equiv P[X_{2n-1}X_{2n}=-1; S_{2n}=0]$. Then $P[X_{2n-1}X_{2n}=1; S_{2n}=0]=P[S_{2n}=0]-q$ hence\n$$E\\left(X_{2n-1}X_{2n};S_{2n}=0\\right) = -1\\cdot q + 1\\cdot(P[S_{2n}=0]-q) = P[S_{2n}=0]-2q$$\nNote that if $X_{2n-1}X_{2n}=-1$ then $X_{2n-1}+X_{2n} = 0$ and $S_{2n-2}=S_{2n}$, therefore \n$$q= P[S_{2n-2}=0,S_{2n}=0]= P[S_{2n-2}=0,X_{2n-1}+X_{2n} = 0]= \\frac 12 P[S_{2n-2}=0]$$\nthe last equality being due to the fact that $S_{2n-2}$ and $X_{2n-1}+X_{2n}$ are independent and that $P[X_{2n-1}+X_{2n} = 0]= \\frac 12$.\nSo\n$$ E\\left(X_{2n-1}X_{2n}; S_{2n}=0\\right) =P[S_{2n}=0]-P[S_{2n-2}=0]$$\nInserting into the main expression and multiplying out we have\n$$E[(S_n)^2;S_{2n}=0] = nP[S_{2n}=0] +n(n-1)\\Big (P[S_{2n}=0] - P[S_{2n-2}=0]\\Big) $$\n$$=n^2P[S_{2n}=0] - n(n-1)P[S_{2n-2}=0] \\tag{A}$$\nGuided by what we want to prove, the issue now is to express $P[S_{2n}=0]$ in terms of $P[S_{2n-2}=0]$. We note that the event $\\{S_{2n}=0\\}$ can only occur and so has strictly positive probability, if and only if one of the following events occur: \n1) $E_0 \\equiv [\\{S_{2n-2}=0\\}\\; \\cap \\{X_{2n-1}+X_{2n}= 0\\}]$\n2) $E_1 \\equiv [\\{S_{2n-2}=2\\}\\; \\cap \\{X_{2n-1}+X_{2n}= -2\\}]$\n3) $E_2 \\equiv [\\{S_{2n-2}=-2\\}\\; \\cap \\{X_{2n-1}+X_{2n}= 2\\}]$\nThe events inside the brackets, if we look at them as elements of a $3\\times 2$ matrix, are (pardon my language) ... \"horizontally independent and vertically mutually exclusive\". Combined these imply that \n$$P[S_{2n}=0] = P[E_0 \\cup E_1 \\cup E_2] = P[E_0] + P[E_1]+P[E_2]$$\nwhile the \"horizontal\" independence implies that the joint probabilities can be separated as the product of the probabilities of the two events involved in each case. More over, by symmetry we have that $P[S_{2n-2}=-2]= P[S_{2n-2}=2]$, while\n$P[X_{2n-1}+X_{2n}= 0]= 1/2  $ , $P[X_{2n-1}+X_{2n}= -2]= 1/4  $ , $P[X_{2n-1}+X_{2n}= 2]= 1/4$. Using all these results we arrive at \n$$ P[S_{2n}=0] = \\frac 12 P[S_{2n-2}=0] + \\frac 12 P[S_{2n-2}=2]$$\nInserting this into equation $[A]$ we obtain\n$$E[(S_n)^2;S_{2n}=0] = n^2\\left( \\frac 12 P[S_{2n-2}=0] + \\frac 12 P[S_{2n-2}=2]\\right) - n(n-1)P[S_{2n-2}=0] $$\n$$\\Rightarrow E[(S_n)^2;S_{2n}=0] = \\frac n2 (2-n)P[S_{2n-2}=0] + \\frac {n^2}2P[S_{2n-2}=2] \\tag{B}$$\nEquation $[B]$ is an improvement over eq. $[A]$ because, through probabilistic arguments, only $S_{2n-2}$ is now present in the RHS. Now, if we denote by $Y_k$ a binomial random variable with probability of success $1/2$, $Y_k = Bin(k,1/2)$, then, the probabilities related to  $S_{2n-2}$ can be expressed in terms of $Y_{2n-2}$,  \n$$P[S_{2n-2}=0] = P[Y_{2n-2}=n-1],\\;\\; P[S_{2n-2}=2] = P[Y_{2n-2}=n]$$\nIt is then a natural step to use the probability generating function of $Y_{2n-2}$, which is\n$$G_Y(z) = \\left (\\frac 12 +\\frac 12z\\right)^{2n-2}$$ \nwhile we have\n$$P[S_{2n-2}=0] = P[Y_{2n-2}=n-1] = \\frac 1 {(n-1)!}G_Y^{(n-1)}(0)$$\n$$P[S_{2n-2}=2] = P[Y_{2n-2}=n] = \\frac 1 {n!}G_Y^{(n)}(0)$$\nwhere the superscript parentheses denote the order of derivative taken. But\n$$G_Y^{(n)}(0) = (n-1)G_Y^{(n-1)}(0) = (n-1)\\cdot[(n-1)!]P[Y_{2n-2}=n-1]$$\nand so \n$$P[S_{2n-2}=2] = P[Y_{2n-2}=n] = \\frac {n-1}{n} P[Y_{2n-2}=n-1] = \\frac {n-1}{n}P[S_{2n-2}=0]$$\nInserting this into equation $[B]$ we obtain\n$$E[(S_n)^2;S_{2n}=0] = \\frac n2 (2-n)P[S_{2n-2}=0] + \\frac {n^2}2\\frac {n-1}{n}P[S_{2n-2}=0] $$\n$$\\Rightarrow \\frac n2P[S_{2n-2}=0] \\cdot (2-n+n-1) = \\frac n2P[S_{2n-2}=0]$$\nwhich is what we wanted to prove. One could argue that the use of the probability generating function is too \"algebraic\" a step, and that in general, the moment we enter binomial distribution territory, combinatorics rage in the background -but at least now they rage only in the background. \n--  \nAnd don't forget, on the side, rearranging the relations above we can obtain the conditional expectation\n$$E[(S_n)^2 \\mid S_{2n}=0] = \\frac {n^2}{2n-1}$$\n\n\nINITIAL final step of the proof   (using combinatorial identities)  \nWe have arrived at equation $[A]$.  \nUsing $Y_k$ to denote a binomial random variable with probability of\n  success $1/2$, $Y_k = B(k,1/2)$, the probabilities related to  $S_n$\n  can be expressed as probabilities of binomial random variables,\n  $$P[S_{2n}=0] = P[Y_{2n}=n],\\qquad P[S_{2n-2}=0] =P[Y_{2n-2}=n-1]$$\nSince we want at the end to have only $P[S_{2n-2}=0]$ present, we must\n  somehow express $P[Y_{2n}=n]$ in terms of $P[Y_{2n-2}=n-1]$, and here\n  is where I use the combinatorial identities, since one can arrive\n  through them at\n$$P[Y_{2n}=n] = \\frac 1{2^{2n}}{2n \\choose n} = \\frac 12 \\frac\n {2n-1}{n}\\frac 1{2^{2n-2}}{2n-2 \\choose n-1} = \\frac\n {2n-1}{2n}P[Y_{2n-2}=n-1]$$\nReverting back to $S$- notation and inserting in the main expression,\n  we have\n$$E[(S_n)^2;S_{2n}=0] = n^2\\frac {2n-1}{2n}P[S_{2n-2}=0] -\n n(n-1)P[S_{2n-2}=0]$$\n$$=\\frac{n}2\\,P[S_{2n-2}=0]\\Big (2n-1 - 2(n-1)\\Big) =\n \\frac{n}2\\,P[S_{2n-2}=0]$$\nwhich is what we wanted to prove. Maybe there are other ways to relate\n  $P[S_{2n}=0]$ and  $P[S_{2n-2}=0]$, but they currently escape me.", "meta": {"post_id": 443343, "input_score": 51, "output_score": 39, "post_title": "Identity for simple 1D random walk"}}
{"input": "I am studying an undergraduate text about math logic.\nThe proofs of the two G\u00f6del's incompleteness theorems are not completely formal: they are admittedly simpler that the real proofs.\nFor what I understood, I deduce the two theorems are valid for both classical and intuitionistic logic.\nIs my deduction correct?", "output": "The usual proof of G\u00f6del's First Incompleteness Theorem is entirely constructive. We don't have to rely on excluded middle, or have to rely on proving an existential quantification for which we can't produce a witness. For recall: the proof consists in (a) giving a recipe which takes a suitable specification of a sufficiently strong theory $T$ and constructs a certain sentence $G_T$ and then (b) showing $G_T$ is undecidable in that theory. The construction of $G_T$ is clever though simple when you see how, and involves no infinitary ideas. The proof of undecidability involves a pair of reductios, but both of the non-contentious type [like  \"Suppose $T \\vdash G_T$: then contradiction; so $T \\nvdash G_T$\"]. So overall the proof is intuitionistically acceptable. \nThe usual proof of the Second Incompleteness Theorem then consists, at heart, in showing that the proof of the First Theorem can be coded up in arithmetic. Again it's all constructive, and so is intuitionistically acceptable.", "meta": {"post_id": 448527, "input_score": 24, "output_score": 34, "post_title": "Are the G\u00f6del's incompleteness theorems valid for both classical and intuitionistic logic?"}}
{"input": "I want to know if there is some systematic way (using some combinatorial argument)\nto find the number of elements of conjugacy classes of $S_n$ for some given $n$.\nFor example, let's consider $S_5$. If the representative for the conjugacy class\nis an $m$-cycle then Dummit and Foote gives a formula on how to compute the number of elements in the conjugacy class. This is not a problem. But what about when the representative is not an $m$-cycle. As an example we can consider the conjugacy class that gives rise by the partition $2+3$ of $5$. A representative for the conjugacy class would be $(1 2)(3 4 5)$. How can I find the number of such elements?.\nQuestion?: \n\nDoes $ {5\\choose 2}\\cdot { 3 \\choose 3}\\cdot 2$ give me what I want?\n\nReasoning: For the first parenthesis I need to choose $2$ elements out of $5$ and for the second set of parenthesis I need to choose $3$ out of the remaining $3$ (noting that they can't be repeats). Finally we can permute these two parenthesis in two ways, thus giving me the above number. \nIs this reasoning correct?. If not how does one find the number of elements of such conjugacy classes.\nAs always, any help is greatly appreciated.", "output": "More systematically, you have $n!$ choices to arrange $1,\\ldots, n$. Place them into the parentheses pattern in this order to obtain an element of the conjugacy class.\nFor each $r$-cycle, you divide by $r$ as only the cyclic order within a cycle plays a role, not which element we start with. Then, if there are $n_r$ cycles of length $r$, divide by $n_r!$ as the order in which the cycles are listed is not important. Note that this must also be done for the cycles of length $1$!\nThis gives us\n$$\n\\frac{n!}{\\prod_{r}r^{n_r}n_r!} $$\nThus in $S_5$, there are $\\frac{5!}{2\\cdot 3}$ conjugates of $(1\\,2)(3\\,4\\,5)$. Similarly, there are $\\frac{7!}{2\\cdot2\\cdot 2!\\cdot 3!}$ conjugates of $(1\\,2)(3\\,4)$ in $S_7$.", "meta": {"post_id": 449041, "input_score": 27, "output_score": 50, "post_title": "counting the number of elements in a conjugacy class of $S_n$"}}
{"input": "I suggested the following problem to my friend: prove that there exist irrational numbers $a$ and $b$ such that $a^b$ is rational. The problem seems to have been discussed in this question.\nNow, his inital solution was like this: let's take a rational number $r$ and an irrational number $i$. Let's assume\n$$a = r^i$$\n$$b = \\frac{1}{i}$$\nSo we have\n$$a^b = (r^i)^\\frac{1}{i} = r$$\nwhich is rational per initial supposition. $b$ is obviously irrational if $i$ is. My friend says that it is also obvious that if $r$ is rational and $i$ is irrational, then $r^i$ is irrational. I quickly objected saying that  $r = 1$ is an easy counterexample. To which my friend said, OK, for any positive rational number $r$, other than 1 and for any irrational number $i$ $r^i$ is irrational. Is this true? If so, is it easily proved? If not, can someone come up with a counterexample?\nLet's stick to real numbers only (i.e. let's forget about complex numbers for now).", "output": "Consider $2^{\\log_2 3}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}{}$", "meta": {"post_id": 449431, "input_score": 30, "output_score": 78, "post_title": "Rational number to the power of irrational number = irrational number. True?"}}
{"input": "I was reading Topology from Munkres and got confused by the definition of a subbasis. What is/are the difference between basis and subbasis in a topology?", "output": "Consider $S=\\{\\{0,1\\},\\{0,2\\}\\}$. What is the topological space $T(S)$ generated by $S?$ By definition, $S$ will then be a subbasis of $T(S)$.\nWell, we want to all requirements to hold true and find that $T(S) = \\{\\emptyset, \\{0\\}, \\{0,1\\}, \\{0,2\\}, \\{0,1,2\\}\\}$ (check this!). \nIs $S$ a basis? No, because you cannot write $\\{0\\}$ as a union of any elements in $S$.\nSo you see that subbasis and basis are two different notions, even for a very basic example. \nA subbasis can be thought of, and is actually defined to be, the \"smallest set that becomes my topological space if I complete it under the property of being a topological space, i.e. fulfiling the axioms of topological space\".\nThe two terms are related nevertheless. Every basis is a subbasis, and in one of the equivalent definitions of subbasis you will find that you already get a basis from your subbasis.", "meta": {"post_id": 449554, "input_score": 69, "output_score": 39, "post_title": "Difference between basis and subbasis in a topology?"}}
{"input": "In Thurston's superb essay On proof and progress in mathematics, he makes this observation:\n\n\n\nOf course there is always another subtlety to be gleaned, but I would like to at least think that I have absorbed the main intuition behind each element of the above list. However:\n\n\n\nDifferential geometry is not my strong suit, unfortunately, so I have had trouble trying to unravel this even at a formal level. Manifolds and vector bundles themselves I am comfortable with, but with connections and connection forms I have trouble moving between formalism and intuition, and \"Lagrangian section\" is not a term I've come across (though I can find its definition online).\nSo, I have some questions about Thurston's 37th conception of the derivative:\n\nTo use Thurston's words: can someone \"translate into precise, formal, and explicit definitions\" making the \"differences start to evaporate\" between 37 and the differential of a smooth map?\nWhat is the intuition behind it - why should the notion of \"Lagrangian section\" appear here, what does it mean (intuitively) when a connection makes the graph of $f$ parallel, etc.?\n\nMy hope is also for answers that are as accessible to as many people as possible, though of course, any explanation has to assume some level of background knowledge.", "output": "Zev, I honestly think Thurston's tongue was firmly implanted in his cheek when he wrote this. So the key point is that a connection on a vector bundle gives you (a) a means of differentiating sections (generalizing the covariant derivative for a Riemannian manifold as a connection on the tangent bundle) and (b) a notion of parallelism (generalizing the notion of parallel transport of tangent vectors).\nAs you suggested, the differential of $f\\colon D\\to\\mathbb R$ gives you a $1$-form, hence a section of the cotangent bundle $T^*D$. With the standard symplectic structure on $T^*D$, Lagrangian sections (i.e., ones that pull back the symplectic $2$-form to $0$) are precisely closed $1$-forms. [This is tautological: If $q_i$ are coordinates on $D$, a $1$-form on $D$ is given by $\\omega = \\sum p_i\\,dq_i$ for some functions $p_i$. By definition, $d\\omega = \\sum dp_i\\wedge dq_i$, and this is (negative of) the pullback by the section $\\omega$ of the standard symplectic form $\\sum dq_i\\wedge dp_i$ (with canonical coordinates $(q_i,p_i)$ on $T^*D$).]\nNow, a connection form on a rank $k$ vector bundle $E\\to M$ is a map $\\nabla\\colon \\Gamma(E)\\to\\Gamma(E\\otimes T^*M)$ (i.e., a map from sections to one-form valued sections) that satisfies the Leibniz rule $\\nabla(gs) = dg\\otimes s + g\\nabla s$ for all sections $s$ and functions $g$. In general, one specifies this by covering $M$ with open sets $U$ over which $E$ is trivial and giving on each $U$ a $\\mathfrak{gl}(k)$-valued $1$-form, i.e., a $k\\times k$ matrix of $1$-forms; when we glue open sets these matrix-valued $1$-forms have to transform in a certain way in order to glue together to give a well-defined $\\nabla$.\nOK, so Thurston takes the trivial line bundle $D\\times\\mathbb R$. A connection is determined by taking the global section $1$ and specifying $\\nabla 1$ to be a certain $1$-form on $D$. The standard flat connection will just take $\\nabla 1 = 0$ and then $\\nabla g = dg$. I'm now going to have to take some liberties with what Thurston says, and perhaps someone can point out what I'm missing. Assume now that our given function $f$ is nowhere $0$ on $D$. We can now define a connection by taking $\\nabla 1 = -df/f$. Then the covariant derivative of the section given by the function $f=f\\otimes 1$ [to which he refers as the graph of $f$] will be $\\nabla(f\\otimes 1) = df - f(df/f) = 0$, and so this section is parallel.\nSlightly less tongue-in-cheek, parallelism is the generalization of constant (in a vector bundle, we cannot in general say elements of different fibers are equal), and covariant derivative $0$ is the generalization of $0$ derivative.", "meta": {"post_id": 449740, "input_score": 81, "output_score": 77, "post_title": "Thurston's 37th way of thinking about the derivative"}}
{"input": "Is it possible to integrate a matrix?\nI've been working through a problem and come up with $$\\int_{t_0}^t  \\begin{pmatrix}\\sin(s)\\cdot\\cos(\\beta s)\\\\ \\cos(s)\\cdot\\cos(\\beta s)\\end{pmatrix}ds$$\nI'm integrating from $t_0$ to $t$\nCan this be done or do we think I went wrong somewhere?", "output": "Matrices form a vector space. Therefore, you can simply integrate them componentwise. \nIn detail. Let $A:t\\mapsto A(t)$ be a function from a real interval $I$ to the space of $m\\times n$ real matrices. \nEvery entry $a_{ij}$ is a real function of a real variable. If all entries are integrable functions, then you can define the integral of the matrix as the matrix of the integrals:\n$$\\int A(t)\\,dt := \\left( \\int a_{ij}(t)\\,dt \\right).$$\nFor your problem:\n$$\\int  \\begin{pmatrix}\\sin(s)&\\cos(\\beta s)\\\\ \\cos(s)&\\cos(\\beta s)\\end{pmatrix}ds = \n\\begin{pmatrix}\\int\\sin(s)\\,ds &\\int\\cos(\\beta s)\\,ds\\\\ \\int\\cos(s)\\,ds&\\int\\cos(\\beta s)\\,ds\\end{pmatrix} = \\begin{pmatrix}-\\cos(s)&\\frac{1}{\\beta}\\sin(\\beta s)\\\\ \\sin(s)&\\frac{1}{\\beta}\\sin(\\beta s)\\end{pmatrix}. $$\nThere is a more sophisticated operation, in case the matrix in question belongs to a Lie algebra: ordered exponentiation. It is to integration as exponentiation is to multiplication, and permits to go from a Lie algebra element (intuitively, a differential transformation) to a group element (a whole transformation). \nIn this case, you need a $n\\times n$ matrix-valued function.\nIt is explained quite well here:\nhttp://en.wikipedia.org/wiki/Ordered_exponential.", "meta": {"post_id": 450560, "input_score": 23, "output_score": 42, "post_title": "Integrating a matrix"}}
{"input": "Given a real function $f\\in L^1_{\\text{loc}}(\\Omega)$, we define both weak or distributional derivatives by\n$\\int f'\\phi = - \\int f \\phi'$ for all test functions $\\phi$. \nNow, take $\\Omega = (-1,1)$, and $f(x) = I_{x>0}$, an indicator function. \nThen, according to Example 2 of Section 5.2 of Evans' PDE book, there is no weak derivatives.\nBut, it is well known that $f' = \\delta_0$ as a distribution. In fact, every distribution has its derivative according to Rudin's book on Functional Analysis, see Section 6.1.\nAt this far, can anybody clarify the following questions?\n\nweak derivatives is stronger than distributional derivatives? If\nyes, how strong?\nIs $\\delta_0$ a $L^1_{loc}(-1,1)$, a locally integrable function?\nsee also this question.\nMost PDE books use weak derivatives, not distributional one?", "output": "(1) One usually wants to be weak derivatives to be functions, that is distributions represented by $\\def\\loc{\\mathrm{loc}}$$L^1_\\loc$-functions, that is we say that $f$ is weakly differentiable iff there is an $f' \\in L^1_\\loc$ such that \n$$ \\int_\\Omega f\\phi' = -\\int_\\Omega f'\\phi\\quad \\phi\\in C_c^\\infty(\\Omega) $$\nNow some functions, such as $I_{x>0}$ don't have a weak derivative. As every distribution $T \\in \\mathcal D'(\\Omega)$ has derivatives of any order given by  $T^{(n)}(\\phi) = (-1)^n T(\\phi^{(n)})$ in this case, the distributional derivatives cannot be represented by functions, see (2).\n(2) No. $\\delta$ cannot be represented by a function $g \\in L^1_\\loc(\\Omega)$. To see this, suppose $g \\in L^1_\\loc(\\Omega)$ were representing $\\delta$, that is \n$$ \\int_{-1}^1 g\\phi = \\delta(\\phi) = \\phi(0) $$\nfor all $\\phi \\in C^\\infty_c(-1,1)$. Let $\\phi_n$ be a sequence in $C^\\infty_c$ such that $0\\le \\phi_n \\le 1$, $\\mathop{\\rm supp} \\phi_n \\subseteq[-\\frac 1n, \\frac 1n]$, $\\phi_n(0) = 1$. Then $\\phi_n \\to 0$ almost everywhere, hence \n$$ 1 = \\phi_n(0) = \\int_{-1}^1 \\phi_n g \\to 0 $$\ncontradiction.\n(3) Also distributional derivatives which cannot be represented by functions play a role in PDE theory, so I cannot say, this holds for most books, for some, it surely does.", "meta": {"post_id": 451746, "input_score": 45, "output_score": 46, "post_title": "Are weak derivatives and distributional derivatives different?"}}
{"input": "Wikipedia give sheaf property using equalizer diagram by saying sheaf property means for any open cover $\\{U_i\\}$ of $U$\n$$F(U) \\rightarrow \\prod_{i} F(U_i) {{{} \\atop \\longrightarrow}\\atop{\\longrightarrow \\atop {}}} \\prod_{i, j} F(U_i \\cap U_j)$$\nis an equalizer. What means two arrows? If $i = 1,2,3$ then what means\n$$F(U) \\rightarrow F(U_1) \\times F(U_2) \\times F(U_3) {{{} \\atop \\longrightarrow}\\atop{\\longrightarrow \\atop {}}} F(U_1 \\cap U_2) \\times F(U_2 \\cap U_3) \\times F(U_1 \\cap U_3)?$$", "output": "Given maps $f:X\\to Y$ and $g:X\\to Y$, a natural way of writing them together in the same diagram would be like this:\n$$X{{f\\atop {\\Large\\longrightarrow}}\\atop{{\\Large\\longrightarrow}\\atop g}}Y$$\nA map $\\mathrm{eq}:E\\to X$ is an equalizer (Wikipedia) of the maps $f$ and $g$ if it is final in the category of maps to $X$ that equalize $f$ and $g$. Depicting it together with the maps $f$ and $g$, we say that \n$$E\\xrightarrow{\\;\\mathrm{eq}\\;} X{{f\\atop {\\Large\\longrightarrow}}\\atop{{\\Large\\longrightarrow}\\atop g}}Y$$\nis an equalizer diagram.\n\nLet $X$ be a topological space, let $F$ be a presheaf on $X$, and let $\\{U_i\\}_{i\\in I}$ be an open cover of $X$. The presheaf $F$ comes with restriction maps $\\mathrm{res}_{V}^{U}:F(U)\\to F(V)$ for each pair of open sets $U,V$ with $V\\subseteq U$. In particular, for each pair of $i,j\\in I$, we have restriction maps $$\\large\\mathrm{res}_{U_i\\cap U_j}^{U_i}:F(U_i)\\to F(U_i\\cap U_j)$$ and  $$\\large\\mathrm{res}_{U_i\\cap U_j}^{U_j}:F(U_j)\\to F(U_i\\cap U_j).$$\nBy definition, for each $i\\in I$, the product (Wikipedia) $\\prod_{i\\in I}F(U_i)$ comes with a projection map \n$$p_i:\\prod_{i\\in I}F(U_i)\\longrightarrow F(U_i).$$ Moreover, $F(U_i)$ comes with a restriction map $\\mathrm{res}_{U_i\\cap U_j}^{U_i}:F(U_i)\\to F(U_i\\cap U_j)$ for each $j\\in I$. Composing them, we have for each pair of $i,j\\in I$, a map $a_{i,j}$, as follows:\n$$\\underbrace{(\\mathrm{res}_{U_i\\cap U_j}^{U_i}\\circ p_i)}_{\\Large a_{i,j}}:\\prod_{i\\in I}F(U_i)\\longrightarrow F(U_i\\cap U_j)$$\nBy the definition of a product, these $a_{i,j}$'s induce a map (let's call it $\\alpha$) from $\\prod_{i\\in I}F(U_i)$ to the product of all of the $F(U_i\\cap U_j)$'s together:\n$$\\alpha:\\prod_{i\\in I}F(U_i)\\longrightarrow \\prod_{i,j\\in I}F(U_i\\cap U_j).$$\nSimilarly, for each pair of $i,j\\in I$, we have a map $b_{i,j}$:\n$$\\underbrace{(\\mathrm{res}_{U_i\\cap U_j}^{U_j}\\circ p_j)}_{\\Large b_{i,j}}:\\prod_{i\\in I}F(U_i)\\longrightarrow F(U_i\\cap U_j)$$\nwhich come together to form a single map\n$$\\beta:\\prod_{i\\in I}F(U_i)\\longrightarrow \\prod_{i,j\\in I}F(U_i\\cap U_j).$$\nThese maps $\\alpha$ and $\\beta$ are the top and bottom arrows in the diagram\n$$F(U) \\longrightarrow \\prod_{i\\in I} F(U_i) {{{}\\atop {\\Large\\longrightarrow}}\\atop{{\\Large\\longrightarrow}\\atop {}}}\\prod_{i, j\\in I} F(U_i \\cap U_j)$$\nWe say that $F$ is a sheaf (Wikipedia) if, for any open cover $\\{U_i\\}_{i\\in I}$ of our space $X$, this diagram is an equalizer diagram.\n\nNote that $\\alpha$ and $\\beta$ are not the same map; they take different \"paths\". Let's trace out what happens: the top-right path is $a_{i,j}$, and the bottom-left path is $b_{i,j}$.  $\\require{AMScd}$\n$$\\require{AMScd}\n\\begin{CD}\n\\prod_{i\\in I}F(U_i) @>{\\Large p_i}>> F(U_i);\\\\\n@V{\\Large p_j}VV @VV{\\Large\\mathrm{res}_{U_i\\cap U_j}^{U_i}}V \\\\\nF(U_j) @>>{\\Large\\mathrm{res}_{U_i\\cap U_j}^{U_j}}> F(U_i\\cap U_j);\n\\end{CD}$$\nThe two maps $\\alpha$ and $\\beta$ to the product $\\prod_{i,j\\in I}F(U_i\\cap U_j)$ agree on a specific element $(s_i)_{i\\in I}$ of the product $\\prod_{i\\in I}F(U_i)$ if and only if $a_{i,j}$ and $b_{i,j}$ agree on it for all pairs of $i,j\\in I$. But $a_{i,j}$ and $b_{i,j}$ are clearly going to be different maps in general, so this is not a trivial condition.", "meta": {"post_id": 453203, "input_score": 22, "output_score": 37, "post_title": "Definition of sheaf using equalizer"}}
{"input": "Will product of path connected topological spaces be necessarily path connected? Why or why not?\nGive me some hints. Thank you.", "output": "This is very straightforward from applying the definitions involved:\nAssume $X=\\prod_{i\\in I}X_i$ with $X_i$ path-connected.\nLet $x=(x_i)_{i\\in I}$, $y=(y_i)_{i\\in I}$ be two points in $X$.\nBy assumption, thee exist continuous paths $\\gamma_i\\colon[0,1]\\to X_i$ with $\\gamma_i(0)=x_i$ and $\\gamma_i(1)=y_i$.\nBy definition of product, there exists a unique continuous $\\gamma\\colon[0,1]\\to X$ such that $\\pi_i\\circ\\gamma=\\gamma_i$ for all $i\\in I$ where $\\pi_i$ is the projection $X\\to X_i$. That makes $\\gamma$ a path from $x$ to $y$.", "meta": {"post_id": 454627, "input_score": 15, "output_score": 34, "post_title": "Product of path connected spaces is path connected"}}
{"input": "Simplify $$\\frac{\\displaystyle\\sum_{k=1}^{2499}\\sqrt{10+{\\sqrt{50+\\sqrt{k}}}}}{\\displaystyle\\sum_{k=1}^{2499}\\sqrt{10-{\\sqrt{50+\\sqrt{k}}}}}$$\nI don't have any good idea. I need your help.", "output": "Here is my answer. \nI've just got the following result:$$\\frac{\\sum_{k=1}^{2499}\\sqrt{10+{\\sqrt{50+\\sqrt k}}}}{\\sum_{k=1}^{2499}\\sqrt{10-{\\sqrt{50+\\sqrt k}}}}=1+\\sqrt2+\\sqrt{4+2\\sqrt2}=\\cot\\frac{\\pi}{16}.$$\nProof: Suppose that $\\sum$ represents $\\sum_{k=1}^{2499}$. Let the numerator and the denominator be $A$ and $B$ respectively. Letting $a_k=\\sqrt{10+\\sqrt{50+\\sqrt k}}, b_k=\\sqrt{10-\\sqrt{50+\\sqrt k}}$, we can represent $A, B$ as $A=\\sum a_k, B=\\sum b_k.$ Letting $p_k=\\sqrt{50+\\sqrt k}$ and $q_k=\\sqrt{50-\\sqrt k}$, since ${p_k}^2+{q_k}^2=10^2$ and $p_k\\gt0, q_k\\gt0$, there exists a real number $0\\lt x_k\\lt \\frac{\\pi}{2}$ such that $p_k=10\\cos x_k, q_k=10\\sin x_k$. Then, we get $$a_k=\\sqrt{10+10\\cos x_k}=\\sqrt{10+10\\left(2{\\cos^2{\\frac{x_k}{2}}}-1\\right)}=\\sqrt{20}\\cos \\frac{x_k}{2},$$$$b_k=\\sqrt{10-10\\cos x_k}=\\sqrt{10-10\\left(1-2{\\sin^2{\\frac{x_k}{2}}}\\right)}=\\sqrt{20}\\sin \\frac{x_k}{2}.$$ Then, since $\\sum a_k=\\sum a_{2500-k}$, let's consider $a_{2500-k}$. $$\\begin{align}a_{2500-k}&=\\sqrt{10+\\sqrt{50+\\sqrt{(50+\\sqrt k)(50-\\sqrt k)}}}\\\\&=\\sqrt{10+\\sqrt{50+{p_kq_k}}}\\\\&=\\sqrt{10+\\sqrt{50+100\\cos {x_k}\\sin {x_k}}}\\\\&=\\sqrt{10+\\sqrt{50(\\cos {x_k}+\\sin {x_k})^2}}\\\\&=\\sqrt{10+\\sqrt{50}\\cdot\\sqrt2\\sin \\left(x_k+\\frac{\\pi}{4}\\right)}\\\\&=\\sqrt{10+10\\cdot2\\cos \\left(\\frac{x_k}{2}+\\frac{\\pi}{8}\\right)\\sin \\left(\\frac{x_k}{2}+\\frac{\\pi}{8}\\right)}\\\\&=\\sqrt{10\\left(\\cos \\left(\\frac{x_k}{2}+\\frac{\\pi}{8}\\right)+\\sin \\left(\\frac{x_k}{2}+\\frac{\\pi}{8}\\right)\\right)^2}\\\\&=\\sqrt{10}\\left(\\cos \\left(\\frac{x_k}{2}+\\frac{\\pi}{8}\\right)+\\sin \\left(\\frac{x_k}{2}+\\frac{\\pi}{8}\\right)\\right)\\\\&=\\frac{\\left(\\cos \\left(\\frac{\\pi}{8}\\right)+\\sin \\left(\\frac{\\pi}{8}\\right)\\right)a_k+\\left(\\cos \\left(\\frac{\\pi}{8}\\right)-\\sin \\left(\\frac{\\pi}{8}\\right)\\right)b_k}{\\sqrt2}\\\\&=\\sqrt{\\frac{\\sqrt2+1}{2\\sqrt2}}a_k+\\sqrt{\\frac{\\sqrt2-1}{2\\sqrt2}}b_k.\\end{align}$$ Hence, $$A=\\sqrt{\\frac{\\sqrt2+1}{2\\sqrt2}}A+\\sqrt{\\frac{\\sqrt2-1}{2\\sqrt2}}B.$$\nSo, the proof is completed with $$\\frac AB=1+\\sqrt2+\\sqrt{4+2\\sqrt2}. $$\nAfter a while, I got the following theorem in the same way as above.\nTheorem: For any natural number $n$, $$\\frac{\\sum_{k=1}^{n^2+2n}\\sqrt{\\sqrt{2n+2}+{\\sqrt{n+1+\\sqrt k}}}}{\\sum_{k=1}^{n^2+2n}\\sqrt{\\sqrt{2n+2}-{\\sqrt{n+1+\\sqrt k}}}}=1+\\sqrt2+\\sqrt{4+2\\sqrt2}=\\cot {\\frac{\\pi}{16}}.$$\nNote that the case $n=49$ in this theorem is the question at the top.\nP.S. I think it's worth adding a link where user mercio provided a background why the theorem holds for any $n$. (it's a background, not a proof. The theorem has already been proved.)", "meta": {"post_id": 460331, "input_score": 26, "output_score": 38, "post_title": "Simplify $\\left({\\sum_{k=1}^{2499}\\sqrt{10+{\\sqrt{50+\\sqrt{k}}}}}\\right)\\left({\\sum_{k=1}^{2499}\\sqrt{10-{\\sqrt{50+\\sqrt{k}}}}}\\right)^{-1}$"}}
{"input": "I have some trouble solving this problem:\nDoes there exist a holomorphic function $f$ on $\\mathbb C\\setminus \\{0\\}$ such that\n$$|f(z)|\\geq \\frac{1}{\\sqrt{|z|}}$$\nfor all $z\\in\\mathbb C \\setminus \\{0\\}$?\nI don't know where to start. My intuition is that you would get a problem with the singularity near $0$, but I am not sure how to prove it. Any help would be appreciated! Thanks!", "output": "Such an inequality guarantees that $f$ has no zeros, hence we can invert the inequality and find\n$$\\left\\lvert\\frac{1}{f(z)}\\right\\rvert \\leqslant \\sqrt{\\lvert z\\rvert},$$\nand that says that $1/f$ has a removable singularity (with value $0$) in the origin, thus, without loss of generality, $g = 1/f$ is an entire funtion that grows at most as fast as $\\sqrt{\\lvert z\\rvert}$. But such an estimate forces $g$ to be constant, hence $g \\equiv 0$. That on the other hand means $f \\equiv \\infty$, so $f$ was not an analytic function.\n\nAddendum: The Cauchy integral for the derivative of $g$ yields, for $\\lvert z\\rvert \\leqslant R$:\n$$\\begin{align}\n\\lvert g'(z)\\rvert &= \\left\\lvert \\frac{1}{2\\pi i} \\int_{\\lvert\\zeta\\rvert = 2R} \\frac{g(\\zeta)}{(\\zeta-z)^2}\\,d\\zeta\\right\\rvert\\\\\n&\\leqslant \\frac{1}{2\\pi} \\int_0^{2\\pi} \\frac{\\lvert g(2Re^{it})\\rvert}{\\lvert2Re^{it}-z\\rvert^2}2R\\,dt\\\\\n&\\leqslant \\frac{1}{2\\pi}\\int_0^{2\\pi}\\frac{2R\\sqrt{2R}}{R^2}\\,dt = \\frac{2\\sqrt{2}}{\\sqrt{R}},\n\\end{align}$$\nand letting $R \\to \\infty$ shows $g' \\equiv 0$.\nSimilarly, when $h$ is an entire function, and you have an estimate $\\lvert h(z)\\rvert \\leqslant c\\cdot \\lvert z\\rvert^\\alpha$ for all $\\lvert z\\rvert \\geqslant K$, the Cauchy integral for the $n$-th derivative of $h$ yields an estimate $\\lvert h^{(n)}(z)\\rvert \\leqslant C\\cdot R^{\\alpha - n}$ for all $\\lvert z\\rvert \\leqslant R/2$, where the constant $C$ depends on $n$ but not on $R$, and thus $h^{(n)} \\equiv 0$ if $n > \\alpha$, i.e. if you have such an estimate, then $h$ is a polynomial of degree $\\leqslant \\lfloor \\alpha\\rfloor$.", "meta": {"post_id": 463729, "input_score": 17, "output_score": 36, "post_title": "Does there exist an holomorphic function such that $|f(z)|\\geq \\frac{1}{\\sqrt{|z|}}$?"}}
{"input": "I am trying to grasp the Riemann curvature tensor, the torsion tensor and their relationship. \nIn particular, I'm interested in necessary and sufficient conditions for local isometry with Euclidean space (I'm talking about isometry of an open set - not the tangent space - with Euclidean space) - and I'd especially like to grasp these tensors in terms of their measuring the failure of Euclid's parallel postulate in a particular manifold.\nIn a Riemannian manifold $M$, we can choose the Levi-Civita connexion and null out the torsion. So then the curvature wholly determines whether or not we can find the Euclidean open set: we seek an open set wherein $R(X_p,Y_p)Z_p = 0; \\forall X_p,Y_p, Z_p \\in T_p M, \\forall p \\in U \\subset M$ and we are done.\nQuestion 1.\nHowever, what happens to the curvature $R$ in the Riemannian case if we use other connexions with the same geodesic sprays but with different, nonzero torsions $T$? \nQuestion 2.\nIs there a formula showing how $R$ and $T$ in the case of connexions with the same geodesic sprays?\nQuestion 3.\nCan we still test the curvature alone as above to see whether there is a local Euclidean open set, or do we now need to make sure that torsion also vanishes in that open set too?\nQuestion 4.\nIf we relax the Riemannian condition and talk abstractly about a connexion alone, which of the above answers change?\nand lastly:\nQuestion 5.\nI'd really like to have an example of a space with zero curvature but nonzero torsion over a whole, open set, not just at a point, if such a thing exists. I think this would help build intuition.", "output": "Your question is great! Before answering your question, I would like to visualize some concepts for you:\nWe consider the Riemann tensor first. A crucial observation is that if we parallel\ntransport a vector $u$ at $p$ to $q$ along two different pathes $vw$ and $wv$, the resulting\nvectors at $q$ are different in general (following figure). If, however, we parallel transport\na vector in a Euclidean space, where the parallel transport is defined in our\nusual sense, the resulting vector does not depend on the path along which it\nhas been parallel transported. We expect that this non-integrability of parallel\ntransport characterizes the intrinsic notion of curvature, which does not depend\non the special coordinates chosen.\n\n\nIt is useful to say that in this sense visualization of the first Bianchi identity is very easy:\n\n\nFor visualizing Lie bracket: if $X$ and $Y$ are two vector fields in a neighborhood of $p$, then for sufficiently small $h$ we can\n(1) follow the integral curve of $X$\nthrough $p$ for time $h$ ;\n(2) starting from that point, follow the\nintegral curve of $Y$ for time $h$;\n(3) then follow the integral curve of $X$\nbackwards for time $h$ ;\n(4) then follow the integral curve of $Y$\nbackwards for time $h$.\nWhen $X$ and $Y$ are (linearly\nindependent) vector fields with $[X, Y]\\ne 0$, the parallelogram is not closed.\n\n\nWe next look at the geometrical meaning of the torsion tensor. Let $p \\in M$\nbe a point whose coordinates are $\\{x^\u03bc\\}$. Let $X = \\varepsilon^\u03bc e_\u03bc$ and $Y = \\delta^\u03bc e_\u03bc$ be\ninfinitesimal vectors in $T_pM$. If these vectors are regarded as small displacements,\nthey define two points $q$ and $s$ near $p$, whose coordinates are $\\{x^\u03bc + \u03b5^\u03bc\\}$ and\n$\\{x^\u03bc + \u03b4^\u03bc\\}$ respectively (following figure). If we parallel transport $X$ along the line $ps$,\nwe obtain a vector $sr_1$ whose component is $\\varepsilon^\u03bc \u2212 \\varepsilon^{\\lambda} \\Gamma^{\\mu}_{\\nu \\lambda} \\delta^{\\nu}$\n. The displacement\nvector connecting $p$ and $r_1$ is\n$$pr_1 = ps + sr_1 = \u03b4^\u03bc + \u03b5^\u03bc \u2212 \\Gamma^{\\mu}_{\\nu \\lambda} \\varepsilon^{\\lambda} \\delta^{\\nu} .$$\nSimilarly, the parallel transport of $\u03b4^\u03bc$ along $pq$ yields a vector\n$$pr_2 = ps + sr_2 = \u03b5^\u03bc + \u03b4^\u03bc \u2212 \\Gamma^{\\mu}_{\\lambda \\nu} \\varepsilon^{\\lambda} \\delta^{\\nu} .$$\nIn general, $r_1$ and $r_2$ do not agree and the difference is\n$$r_2r_1=pr_2-pr_1=(\\Gamma^{\\mu}_{\\nu \\lambda}\u2212 \\Gamma^{\\mu}_{\\lambda \\nu})\\varepsilon^{\\lambda} \\delta^{\\nu}=T^{\\mu}_{\\lambda \\nu} \\varepsilon^{\\lambda} \\delta^{\\nu} \\qquad(*)$$\nThus, the torsion tensor measures the failure of the closure of the parallelogram\nmade up of the small displacement vectors and their parallel transports.\n\nNow with respect to the above description it is easy to imagine the Torsion tensor in terms of the Lie bracket and connection:\n$$T(u,v)=\\nabla_u v \u2212\\nabla_v u\u2212[u,v]$$\n\n\nI hope the above explanations have cleared the matter\nnow I will get to answering your question:\nSuppose we are navigating on the surface of the Earth. We define a\nvector to be parallel transported if the angle between the vector and the latitude is\nkept fixed during the navigation. [Remarks: This definition of parallel transport\nis not the usual one. For example, the geodesic is not a great circle but a straight\nline on Mercator\u2019s projection.] Suppose we navigate along\na small quadrilateral $pqrs$ made up of latitudes and longitudes (following figure).\n\nWe parallel transport a vector at $p$ along $pqr$ and $psr$, separately. According\nto our definition of parallel transport, two vectors at $r$ should agree, hence the\ncurvature tensor vanishes. To find the torsion, we parametrize the points $p$, $q$, $r$\nand $s$ as in following figure. \n\nWe find the torsion by evaluating the difference between\n$pr_1$ and $pr_2$ as in $(*)$. If we parallel transport the vector $pq$ along $ps$, we\nobtain a vector $sr_1$, whose length is $R \\sin \\theta d\\varphi$. However, a parallel transport\nof the vector $ps$ along $pq$ yields a vector $qr_2 = qr$. Since $sr$ has a length\n$R \\sin(\\theta \u2212 d\\theta) d\\varphi \\simeq R \\sin \\theta d\\varphi \u2212 R \\cos \\theta d\\theta d\\varphi$, we find that $r_1r_2$ has a length\n$R \\cos \\theta d\\theta d\\varphi$. Since $r_1r_2$ is parallel to $-\\frac{\\partial}{\\partial \\varphi}$, the connection has a torsion\n$T^{\\varphi}_{\\theta \\varphi}$, see $(*)$. From $g_{\\varphi \\varphi} = R^2 \\sin^2 \\theta$, we find that $r_1r_2$ has components\n$(0,\u2212\\cot \\theta d\\theta d\\varphi)$. Since the $\\varphi$-component of $r_1r_2$ is equal to $T^{\\varphi}_{\\theta \\varphi} d\\theta d\\varphi$, we\nobtain $T^{\\varphi}_{\\theta \\varphi} = \u2212\\cot \\theta$.", "meta": {"post_id": 465603, "input_score": 61, "output_score": 79, "post_title": "Are there simple examples of Riemannian manifolds with zero curvature and nonzero torsion"}}
{"input": "Let $X_1, \\dots, X_n \\sim N(\\mu,\\sigma)$ be normal random variables. Find the expected value of random variables $\\max_i(X_i)$  and $\\min_i(X_i)$.\n\nThe sad truth is I don't have any good idea how to start and I'll be glad for a hint.", "output": "As Robert Israel says, you only need to consider standard $N(0,1)$s. If $E_n$ is the expected value of the maximum of $n$ independent $N(0,1)$s, then the expected value of $\\max_i(X_i)$ is $\\mu+\\sigma E_n$ and the expected value of $\\min_i(X_i)$ is $\\mu-\\sigma E_n$.\nActually this is quite a cute problem, because there is a closed form up to $n=5$ given by $E_1=0$, $E_2=\\pi^{-1/2}$, $E_3=(3/2)\\pi^{-1/2}$, $E_4=3\\pi^{-3/2}\\cos^{-1}(-1/3)$, $E_5=(5/2)\\pi^{-3/2}\\cos^{-1}(-23/27)$.\nBut I don't think you will get a neat closed form for general $n$. Depending on your application, maybe an approximation formula would be useful, or maybe a computer program would be good. The first approximation formula you might use is $E_n\\sim\\sqrt{2\\log(n)}$, but there are more accurate (and elaborate) ones if that is what you need (in which case, please say so).\n\n\nTo show the first five values of $E_n$ are as above, let $\\phi(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}$ be the standard normal density and $\\Phi(x)$ the standard cdf. Then\n\n\\begin{equation}\\begin{split}\nE_n&=\\int_{-\\infty}^\\infty x\\dfrac{d}{dx}\\Phi(x)^ndx\\\\\n   &=n\\int_{-\\infty}^\\infty x\\phi(x)\\Phi(x)^{n-1}dx\\\\\n   &=n(n-1)\\int_{-\\infty}^\\infty \\phi(x)^2\\Phi(x)^{n-2}dx\\quad\\text{(by parts)}\\\\\n   &=n(n-1)\\int_{-\\infty}^\\infty \\phi(x)^2\\left(A(x)+\\tfrac12\\right)^{n-2}dx\\quad\\text{(where $A(x)=\\Phi(x)-\\tfrac12$),}\\\\\n   &=\\frac{n(n-1)}{2\\pi}\\sum_{r=0}^{[n/2]-1}(\\tfrac12)^{n-2-2r}\\binom{n-2}{2r}\\int_{-\\infty}^\\infty e^{-x^2}A(x)^{2r}dx\\quad\\text{(using antisymmetry of $A(x)$)}.\n\\end{split}\\end{equation}\n\n\nSo the $E_n$ come in pairs, and $E_{2n}$ and $E_{2n+1}$ can be written in terms of $\\int_{-\\infty}^\\infty e^{-x^2}A(x)^{2r}dx$ for $r=0, \\ldots, n-1$. We can get a closed form for this integral for $r=0$ and also, slightly surprisingly, for $r=1$, giving $E_1, \\ldots, E_5$ above.\n\nThe only way I know how to do this is by introducing $$I_n(s)=\\int_{-\\infty}^\\infty e^{-sx^2/2}A(x)^{2n}dx.$$\nThen $I_n(s)$ satisfies a strange reduction rule\n$$I_{n+1}(s)=\\frac{2n+1}{2\\pi\\sqrt{s}}\\int_0^{1/\\sqrt{s}}\\left(1+y^2\\right)^{-1}I_{n}\\left(1+s(1+y^2)\\right)dy.$$\nAs a consequence,\n\\begin{equation}\\begin{split}\nI_0(s)&=\\sqrt\\frac{2\\pi}{s},\\\\\nI_1(s)&=\\frac{1}{\\sqrt{2\\pi s}}\\tan^{-1}\\left(\\frac{1}{\\sqrt{s(s+2)}}\\right),\\\\\n%I_2(s)&=3(2\\pi)^{-3/2}s^{-1/2}\\int_0^{s^{-1/2}}\\left(1+y^2\\right)^{-1}\\left(3+2y^2\\right)^{-1/2}\\tan^{-1}\\left(\\left((3+2y^2)(5+2y^2)\\right)^{-1/2}\\right)dy\n\\end{split}\\end{equation}\nand the above formulae for $E_1, \\ldots, E_5$ follow from the values of $I_0(2)$ and $I_1(2)$, using standard trig equivalences.\n\n\nTo prove the reduction rule,\n\\begin{equation}\\begin{split}\n\\int_0^{1/\\sqrt{s}}\\left(1+y^2\\right)^{-1}&I_{n}\\left(1+s(1+y^2)\\right)dy\\\\\n&=\\int_0^{1/\\sqrt{s}}\\int_{-\\infty}^\\infty\\left(1+y^2\\right)^{-1}e^{-(1+s(1+y^2))x^2/2}A(x)^{2n}dxdy\\\\\n&=\\int_0^{1/\\sqrt{s}}\\int_{-\\infty}^\\infty\\left(1+y^2\\right)^{-1}e^{-s(1+y^2)x^2/2}\\sqrt{2\\pi}\\phi(x)A(x)^{2n}dxdy\\\\\n&=\\int_0^{1/\\sqrt{s}}\\int_{-\\infty}^\\infty\\left(1+y^2\\right)^{-1}e^{-s(1+y^2)x^2/2}\\frac{\\sqrt{2\\pi}}{2n+1}\\dfrac{d}{dx}A(x)^{2n+1}dxdy\\\\\n&=s\\frac{\\sqrt{2\\pi}}{2n+1}\\int_0^{1/\\sqrt{s}}\\int_{-\\infty}^\\infty xe^{-s(1+y^2)x^2/2}A(x)^{2n+1}dxdy\\quad\\text{(by parts in $x$)}\\\\\n&=s\\frac{\\sqrt{2\\pi}}{2n+1}\\int_{-\\infty}^\\infty xe^{-sx^2/2}A(x)^{2n+1}\\left(\\int_0^{1/\\sqrt{s}} e^{-sy^2x^2/2}dy\\right)dx\\\\\n&=s\\frac{\\sqrt{2\\pi}}{2n+1}\\int_{-\\infty}^\\infty xe^{-sx^2/2}A(x)^{2n+1}\\left(\\int_0^x \\frac{e^{-y^2/2}}{\\sqrt{s}x}dy\\right)dx\\\\\n&=\\sqrt{s}\\frac{2\\pi}{2n+1}\\int_{-\\infty}^\\infty e^{-sx^2/2}A(x)^{2n+2}dx\\\\\n&=\\sqrt{s}\\frac{2\\pi}{2n+1}I_{n+1}(s).\n\\end{split}\\end{equation}", "meta": {"post_id": 473229, "input_score": 22, "output_score": 39, "post_title": "Expected value of maximum and minimum of $n$ normal random variables"}}
{"input": "Let $G$ be a Lie group with Lie algebra $\\mathfrak{g}$ and let $\\exp :\\mathfrak{g}\\rightarrow G$ be the exponential map.\nIn his blog, Terence Tao notes that if a Lie group is not simply-connected, then $\\exp$ will not be injective.  Conversely, is it true that if a Lie group is simply-connected, then $\\exp$ is injective?  If not, what is a counter-example?", "output": "There is a complete characterization, in a large part due to Dixmier and Saito (both independently in 1957):\n\nIf $G$ is a real (finite-dimensional) Lie group with Lie algebra $\\mathfrak{g}$, then the following are equivalent:\n\n$\\exp$ is injective;\n$\\exp$ is bijective;\n$\\exp$ is a real analytic diffeomorphism;\n$G$ is solvable, simply connected, and $\\mathfrak{g}$ does not admit $\\mathfrak{e}$ as subalgebra of a quotient;\n$G$ is solvable, simply connected, and $\\mathfrak{g}$ does not admit $\\mathfrak{e}$ or $\\tilde{\\mathfrak{e}}$ as subalgebra;\n$G$ has no closed subgroup isomorphic to either the circle $\\mathbf{R}/\\mathbf{Z}$, the universal covering $\\widetilde{\\mathrm{SL}_2(\\mathbf{R})}$, $E$ or $\\tilde{E}$.\n\n\nHere $\\mathfrak{e}$ is the 3-dimensional Lie algebra with basis $(H,X,Y)$ and bracket $[H,X]=Y$, $[H,Y]=-X$, $[X,Y]=0$. It is isomorphic to the Lie algebra of the group of isometries of the plane. Its central extension $\\tilde{\\mathfrak{e}}$ is defined as the 4-dimensional Lie algebra defined by adding a central generator $Z$ and the additional nonzero bracket $[X,Y]=Z$. And $E$ and $\\tilde{E}$ are the 3-dimensional and 4-dimensional simply connected solvable Lie groups associated to $\\mathfrak{e}$ and $\\tilde{\\mathfrak{e}}$ respectively.\n \nOn the proof:\nInjectivity of the exponential implies (as mentioned in Qiaochu's post) that there is no closed subgroup isomorphic to the circle, which means that the maximal compact subgroup in $G$ is trivial, that is, $G$ is contractible. A contractible Lie group is always isomorphic to $R\\rtimes S^k$ where $R$ is a simply connected solvable Lie group, $k$ is a non-negative integer and $S$ is the universal covering $\\widetilde{\\mathrm{SL}_2(\\mathbf{R})}$. The latter has a non-injective exponential map, as we see by unfolding two distinct circle groups from $\\mathrm{SL}_2(\\mathbf{R})$. So if the exponential map is injective we have $k=0$, i.e. $G$ is a simply connected solvable Lie group (for a solvable Lie group, contractible and simply connected are equivalent assumptions).\nThis is not enough since in the simply connected Lie group associated to $\\mathfrak{e}$, the exponential map is not injective (this can been seen concretely, as in can be realized as the group of motions of the 3-dimensional Euclidean space generated by horizontal translations and a given 1-parameter group of vertical screwings).\nThat (4) implies (2) and (3) is due to Dixmier (Numdam freely available link) (Bull. SMF, 1957, in French). Dixmier also proved that (2), (3) and (4) are equivalent for simply connected solvable Lie groups, which together with the previous paragraph shows the equivalence between (2), (3), and (4) in general. \nTo complete the proof of the equivalences, one needs to show that for a simply connected solvable Lie group $G$, (1) implies the last (sub-quotient) condition in (4). A careful look at Dixmier's proof seems to show this: if $G$ does not satisfy (4) he even obtains that the exponential map is not locally injective.\nThat (4) implies (5) is easy, the converse is a bit harder but was done by Saito (M. Saito. Sur certains groupes de Lie r\u00e9solubles. Scientific Papers of the College of Arts and\nSciences. The University of Tokyo, 7:1-11, 1957; available here; in French too). To obtain that (1) implies (5), it is enough to check by hand that the simply connected Lie groups $E$ and $\\tilde{E}$ associated to $\\mathfrak{e}$ and $\\tilde{\\mathfrak{e}}$ have a non-injective exponential map, which is easy (not locally injective is a bit harder).\nThe equivalence with (6), which is stated in terms of the 4 minimal counterexamples, does not seem to have be stated in printed form, but follows from the proof.", "meta": {"post_id": 475385, "input_score": 29, "output_score": 40, "post_title": "Under what conditions is the exponential map on a Lie algebra injective?"}}
{"input": "Let $\\mathcal{L}$ denote the $\\sigma$-algebra of Lebesgue measurable sets on $\\mathbb{R}$.  Then, if memory serves, there is an example (and of course, if there is one, there are many) of a continuous function $f:\\mathbb{R}\\rightarrow \\mathbb{R}$ that is not measurable in the sense that $f:(\\mathbb{R},\\mathcal{L})\\rightarrow (\\mathbb{R},\\mathcal{L})$ is measurable, but unfortunately, I was not able to recall the example.  Could somebody please enlighten me?\nNote that this is not in contradiction with the usual \"Every continuous function is measurable.\", because in this statement it is implicit that the co-domain is equipped with the Borel sets, not the Lebesgue measurable sets.", "output": "The standard example is given by  the function $g(x)=f(x)+x$, where $f$ is the devil's staircase function of Cantor. It turns out that the function  $g$ is a homeomorphism from $[0,1]$ onto $[0,2]$  and has the property that $\\mu(g(C))=1$ (where $C$ is the Cantor set). Pick a non measurable $A\\subset g(C)$. First note that  $B=g^{-1}(A)$ is measurable since $B\\subset C$. It follows that $g^{-1}$ is continuous, $B$ is Lebesgue measurable but $(g^{-1})^{-1}(B)$ is non measurable.", "meta": {"post_id": 479441, "input_score": 48, "output_score": 39, "post_title": "Example of a continuous function that is not Lebesgue measurable"}}
{"input": "I read somewhere that $xy=1$ is a closed set in $\\Bbb{R}^2$. \nA closed set is defined as the complement of an open set, or one which contains all its limit points. In metric spaces, it is defined as the complement of the union of balls $B(x,\\epsilon)$, where $\\epsilon>0$.  For example, $(-\\infty,0)$ is open in $\\Bbb{R}$ as it is $\\bigcup_{i=-1}^{-\\infty}B(i,1)$, where $i\\in \\Bbb{Z}^-$. \nIs $xy=1$, or any graph for that matter, closed because we can take any point $( x,y)$ outside of it and draw an open set centred on it such that $B((x,y),\\epsilon)$ lies completely inside the complement of the graph, thus saying the complement is the union of open sets (hence open)?\nThanks in advance!", "output": "More simply, note that $(x,y)\\mapsto xy$ is a continuous function $\\Bbb R^2\\to\\Bbb R,$ and that $\\bigl\\{(x,y)\\in\\Bbb R^2:xy=1\\bigr\\}$ is the preimage of the closed set $\\{1\\},$ so is closed by continuity.", "meta": {"post_id": 480355, "input_score": 13, "output_score": 38, "post_title": "How is $xy=1$ closed in $\\Bbb{R}^2$?"}}
{"input": "For each $t \\in [0,b]$, let $M(t)$ be an $n \\times n$ matrix with entries $m_{ij}(t).$ The matrix $M(t)$ is invertible and positive-definite, so the eigenvalues of $M(t)$ exist and are positive for every $t$.\nThe entries are continuous functions: $m_{ij} \\in C^0[0,b]$. Does this mean the eigenvalues $\\lambda_i(t)$ of $M(t)$ are also continuous functions? \nIt is true for $n=1$ and $n=2$. I can't do it for the general case.", "output": "The eigenvalues of $M(t)$ are continuous functions of $t$, and this holds whether or not $M(t)$ is invertible and/or positive definite.\nIt follows from the fact that the roots of any polynomial $p(z) \\in \\Bbb C[z]$ are continuous functions of the coefficients $p_i \\in \\Bbb C$, where we have \n$$p(z) =\\sum_{i = 0}^{i = \\deg p} p_i z^i.$$\nThis result is in fact quite well known.  An excellent paper covering it and more, similar results, is Continuity and Location of Zeroes of Linear Combinations of Polynomials, by Mishael Zedek, Proc. Amer. Math. Soc. 16 (1965), 78-84, a copy of which is available for free download here.  \nNow consider the \"characteristic polynomial\" $p(\\mu, t) = \\det (\\mu I - M(t))$; this is in fact a monic polynomial in $C^0[0, b][\\mu]$, the ring of polynomials with coefficients in $C[0, b]$; indeed, for each individual $t \\in [0, b]$, $p(\\mu, t)$ is in fact the usual characteristic polynomial of $M(t)$.  To actually see that $p(\\mu, t) \\in C^0[0, b][\\mu]$, simply note that the coefficients $p_i(t)$ of $p(\\mu, t) = \\sum_0^n p_i(t) \\mu^i$\ndepend continuously on the $m_{ij}(t)$, being themselves in fact mulitvariate polynomials in the coefficients $m_{ij}(t)$ of the matrix $M(t)$; and since the $m_{ij}(t)$ are themselves continuous, we simply invoke that old adage ($\\equiv$, in the present context, well-known mathematical truth) that a continuous function of a continuous function is continuous to conclude that we have $p_i(t) \\in C^0[0, b]$ for all $i$, $0 \\le i \\le n$.  We see that in fact the the one-parameter family of characteristic polynomials  $p(\\mu, t) = \\det (\\mu I - M(t))$ has coefficients continuous in $t$; thus the zereos $\\mu(t)$ of $p(\\mu,t)$, i.e. the eigenvalues of $M(t) = [m_{ij}(t)]$ are themselves continuous functions of $t$ by the result of Zedek cited above.\nIn closing, I should add the following note:  in the event that the $m_{ij}(t) \\in C^1[0. b]$, i.e., are differentiable functions of $t$, we can say a lot more; for then, the coefficients $p_i(t)$ of the characteristic polynomial will themselves be differentiable functions of $t$; we can thus use the implicit function theorem applied to the equation $p(\\mu, t) = \\det(\\mu I - M(t))$ to conclude that, as long as \n$\\frac {\\partial p}{\\partial \\mu}(\\mu, t) = \\sum_1^n p_i(t)i \\mu^{i - 1} \\ne 0 \\tag{A}$\nthe eigenvalue functions $\\mu(t)$ are in fact differentiable.  We can even compute their\nderivative:  since\n$p(\\mu(t), t) =  \\sum_0^n p_i(t) \\mu^i(t) = 0$,\ntaking the $t$-derivative (denoted by the \"dot\") yields\n$\\dot p(\\mu, t) + \\frac{\\partial p}{\\partial \\mu} \\dot{\\mu}(t) =  \\sum_0^n \\dot{p}_i(t) \\mu^i(t) + (\\sum_1^n p_i(t)i \\mu^{i - 1}) \\dot{\\mu}(t) =0; \\tag{B}$\nwhen (A) holds, (B) can obviously be solved for $\\dot \\mu(t)$; and (A) is precisely the condition that $\\mu(t)$ is only a simple zero of $p(\\mu(t), t)$, that is, of multiplicity one.  In this case we can actually track the motion of $\\mu(t)$ by looking at $\\dot \\mu(t)$, which will work until the multiplicity of $\\mu(t)$ jumps up to two or more; this happens when the zeroes of $p(\\mu, t)$ \"collide\".\nFun stuff, and useful, too, I'd type all afternoon but my night job beckons, gotta run!\nHope this helps!\nCheers!", "meta": {"post_id": 480619, "input_score": 24, "output_score": 40, "post_title": "Eigenvalues of matrix with entries that are continuous functions"}}
{"input": "In relation to my previous question, I am curious about what exactly are the normal subgroups of a dihedral group $D_n$ of order $2n$.\nIt is easy to see that cyclic subgroups of $D_n$ is normal. But I suspect that case analysis is needed to decide whether dihedral subgroups of $D_n$ is normal.\nA little bit of Internet search suggests the use of semidirect product $(\\mathbb Z/n\\mathbb Z) \\rtimes (\\mathbb Z/2\\mathbb Z) \\cong D_n$, but I do not know the condition for subgroups of a semidirect product to be normal.\nI would be grateful if you could suggest a way to enumerate the normal subgroups of $D_n$ that does not resort to too much of case analysis.", "output": "Here is a nice answer: the dihedral group is generated by a rotation $R$ and a reflection $F$ subject to the relations $R^n=F^2=1$ and $(RF)^2=1$.\nFor $n$ odd the normal subgroups are given by $D_n$ and $\\langle R^d \\rangle$ for all divisors $d\\mid n$. If $n$ is even, there are two more normal subgroups, i.e., $\\langle R^2,F \\rangle$ and $\\langle R^2,RF \\rangle$.", "meta": {"post_id": 484828, "input_score": 32, "output_score": 42, "post_title": "Normal subgroups of dihedral groups"}}
{"input": "I'm confused how it can be true that the product of an infinite number of Hausdorff spaces $X_\\alpha$ can be Hausdorff. \nIf $\\prod_{\\alpha \\in J} X_\\alpha$ is a product space with product topology, the basis elements consists of of products $\\prod_{\\alpha \\in J} U_{\\alpha}$ where $U_{\\alpha}$ would equal $X_{\\alpha}$ for all but finitely many $\\alpha$'s. If this is the case and we had two distinct points, $x$ and $y$, in $\\prod_{\\alpha \\in J} X_{\\alpha}$ and a basis element, $B_x$ containing $x$ then In looking for a basis element,$B_y$ that contains $y$ but is disjoint from $B_x$ then for every $\\alpha$ such that the open set $U_\\alpha$ of $B_x$ is equal to $X_\\alpha$ the open set $U_\\alpha$ for $B_y$ would have to be empty. But then it couldn't possibly contain $y$. (or any other point of $\\prod_{\\alpha \\in J} X_\\alpha$ for that matter). How then is it possible that $\\prod_{\\alpha \\in J} X_\\alpha$ is Hausdorff in the product topology?\nWhat am I missing?", "output": "If $x$ and $y$ are distinct points of $\\prod_{\\alpha\\in J}X_\\alpha$, then there is at least one $\\alpha_0\\in J$ on which they differ, meaning that $x_{\\alpha_0}\\ne y_{\\alpha_0}$. $X_{\\alpha_0}$ is Hausdorff, so there are open sets $U_{\\alpha_0}$ and $V_{\\alpha_0}$ in $X_{\\alpha_0}$ such that $x_{\\alpha_0}\\in U_{\\alpha_0}$, $y_{\\alpha_0}\\in V_{\\alpha_0}$, and $U_{\\alpha_0}\\cap V_{\\alpha_0}=\\varnothing$. Now let $U_\\alpha=V_\\alpha=X_\\alpha$ for each $\\alpha\\in J\\setminus\\{\\alpha_0\\}$, let $U=\\prod_{\\alpha\\in J}U_\\alpha$, and let $V=\\prod_{\\alpha\\in J}V_\\alpha$; then $U$ and $V$ are basic open sets in $\\prod_{\\alpha\\in J}X_\\alpha$, $x\\in U$, $y\\in V$, and $U\\cap V=\\varnothing$. The only statement there that might not be immediately evident is that $U\\cap V=\\varnothing$; to see this, note that if $z\\in U\\cap V$, then $z_{\\alpha_0}\\in U_{\\alpha_0}\\cap V_{\\alpha_0}=\\varnothing$, so no such $z$ can exist.\nThus, $\\prod_{\\alpha\\in J}X_\\alpha$ is Hausdorff.", "meta": {"post_id": 487626, "input_score": 31, "output_score": 50, "post_title": "The product of Hausdorff spaces is Hausdorff"}}
{"input": "I always confused by whether tautological bundle is $\\mathcal{O}(1)$ or $\\mathcal{O}(-1)$, and definitions from different sources tangled in my brain. However, I thought this might not be simply a matter of convention.\nLet $\\mathbb{P}^n$ be a projective space of dimensional $n$, if we realize a point $[l]$ in $\\mathbb{P}^n$ as a line $l \\subset \\mathbb{C}^{n+1}$ passing through origin. Then the tautological bundle $S$ of $\\mathbb{P}^n$ is defined as a subbundle of $\\mathbb{P}^n\\times \\mathbb{C}^{n+1}$ by\n$$[l] \\times l \\subset [l] \\times \\mathbb{C}^{n+1}$$\nIn many books, the convention is $S \\cong \\mathcal{O}(-1)$ on $\\mathbb{P}^n$. Here $\\mathcal{O}(-1)$ is defined as it is in Hartshorne which is the sheaf of modules associated to $\\mathbb{C}[x_0,\\dots,x_n](-1)$ (for example $x_i^{-1}$ is degree $0$ element in $\\mathbb{C}[x_0,\\dots,x_n](-1)$). I know something need to be clarified in $S \\cong \\mathcal{O}(-1)$: for $S$, I mean the sheaf associated to the tautological line bundle S. So it seems the problem becomes to show $S$ does not have global sections (because $\\mathcal{O}(-1)$ is different from $\\mathcal{O}(1)$ by does not have global sections)?", "output": "As a complement to Matt's fine answer let me explain why $\\mathcal O(-1)$ has only zero as global section.  \nA section $s\\in \\Gamma(\\mathbb P^n,\\mathcal O(-1))$ is in particular a section of the trivial bundle $\\mathbb P^n \\times \\mathbb C^{n+1}$, so that it is of the form $s(x)=(x,\\sigma (x)) $ with $\\sigma:\\mathbb P^n \\to  \\mathbb C^{n+1}$ a regular map.\nBut such a map $\\sigma$ is a constant, since any regular map $\\mathbb P^n \\to  \\mathbb C$ is constant by completeness of $\\mathbb P^n$.\nSo $\\sigma (x)=v\\in \\mathbb C^{n+1}$, a fixed vector independent of $x$.\nHowever for $x=[l]$, we must have $\\sigma (x)=v\\in l$.\nIn other words, that constant vector $v\\in \\mathbb C^{n+1}$ must lie on all lines  $l\\subset \\mathbb C^{n+1}$, which forces $v=0$ .\nWe have thus proved that $$\\Gamma(\\mathbb P^n,\\mathcal O(-1))=0$$", "meta": {"post_id": 487923, "input_score": 48, "output_score": 59, "post_title": "Is tautological bundle $\\mathcal{O}(1)$ or $\\mathcal{O}(-1)$?"}}
{"input": "What is the Fourier transform of the indicator of the unit ball in $\\mathbb R^n$?\nI think it is known as one of special functions, so I would be happy to know which one.", "output": "Let $\\alpha_d = \\dfrac{\\pi^{d/2}}{\\Gamma\\left(\\frac{d}{2}+1\\right)}$ the volume of the $d$-dimensional unit ball. Since the characteristic function of the unit ball is rotationally symmetric, so is its Fourier transform, hence let's compute it at the point $\\xi = (0,\\,\\dotsc,\\,0,\\,\\rho)$ with $\\rho > 0$:\n$$\\begin{align}\n\\hat{\\chi}_B(\\xi) &= \\frac{1}{(2\\pi)^{n/2}} \\int_{\\lVert x\\rVert < 1} e^{-i x_n\\rho}\\, dx_1\\, \\dotsc\\, dx_n\\\\\n&= \\frac{1}{(2\\pi)^{n/2}} \\int_{-1}^1 \\left(1-x_n^2\\right)^{(n-1)/2}\\alpha_{n-1} e^{-i x_n\\rho}\\,dx_n\\\\\n&= \\frac{1}{2^{n/2}\\sqrt{\\pi}\\Gamma\\left(\\frac{n+1}{2}\\right)} \\int_0^\\pi \\sin^n \\varphi e^{-i\\rho\\cos\\varphi}\\,d\\varphi.\n\\end{align}$$\nRecalling that we have the Bessel functions\n$$J_p(x) = \\frac{(x/2)^p}{\\sqrt{\\pi}\\Gamma\\left(p + \\frac12\\right)}\\int_0^\\pi \\sin^{2p} \\varphi e^{-ix\\cos\\varphi}\\,d\\varphi,$$\nwe see that\n$$\\hat{\\chi}_B(\\xi) = \\lVert \\xi\\rVert^{-n/2}J_{n/2}(\\lVert\\xi\\rVert).$$", "meta": {"post_id": 489391, "input_score": 27, "output_score": 36, "post_title": "Fourier transform of the indicator of the unit ball"}}
{"input": "Let $A$ be a ring with $0 \\neq 1 $, which has $2^n-1$ invertible elements and less non-invertible elements. Prove that $A$ is a field.", "output": "Step 1: The characteristic of $A$ is $2$\n(Credit for this observation goes to Jyrki Lahtonen)\nThe mapping $x\\mapsto -x$ is an involution on $A^\\times$. Since $\\lvert A^\\times\\rvert = 2^n - 1$ is odd, it has a fixed point.\nSo $a = -a$ for an $a\\in A^\\times$. Multiplication with $a^{-1}$ yields $1 = -1$.\nStep 2: $\\lvert A\\rvert = 2^n$\nFrom the preconditions on $A$ we know\n$$2^n - 1 < \\lvert A \\rvert < 2(2^n - 1) = 2^{n+1} - 2.$$\nBy step 1, the additive group of $A$ is a $2$-group, so $\\lvert A\\rvert$ is a power of $2$. The only remaining possibility is $\\lvert A\\rvert = 2^n$.\nStep 3: $A$ is a field\nFrom step 2 and $\\lvert A^\\times\\rvert = 2^n - 1$, we know that all non-zero elements of $A$ are invertible. Hence $A$ is a finite skew-field. Now by Wedderburn's little theorem, $A$ is a field.", "meta": {"post_id": 489893, "input_score": 29, "output_score": 41, "post_title": "A ring with few invertible elements"}}
{"input": "I have very little understanding on how complex functions work but was wondering if someone could show what the summation of the zeta function simplifies to when $s$ is the first non-trivial zero of the Riemann zeta function, $\\zeta(s)$.\nIn other words, show how one gets 0 when plugging in the first non-trivial zero of the zeta function into the zeta function.", "output": "[update: see updated image/consideration at the end]\n\nThis an extension to @Dietrich's answer. \nThe convergence of the above summation, using the alternating zeta (or \"Dirichlet's eta\"), based on the terms of the series can be much improved using Euler-summation to some order. For my own exercise with this I've made an excel-file to experiment with it and I show here a couple of pictures which illustrate the acceleration-power of Euler-summation, when even its order can be optimally adapted to fractional or complex orders.     \nWith Euler-summation/-acceleration in principle one calculates the finitely truncated Dirichlet-series with some weighting coefficients $e_n(o,t)$ which are determined by the order of the Euler-summation. This can approximate the final result much better than the \"unaccelerated\" series with the same number of terms.      \nCall the number of used terms  t then we get the approximation\n$$ \\eta(z)=\\lim_{t\\to \\infty} \\sum_{n=1}^t e_n(o,t) {(-1)^{n-1}\\over n^z} \\tag 1$$ where the $e_n(o,t)$ are the coefficients by the Euler-summation-procedure of order $o$ for number-of-terms $t$. Then even at the first nontrivial root $z=\\rho_0$ we need few terms of the partial sums to \"see/extrapolate the result\" .       \nFor the trajectories of the partial sums $s_n$ we simply compute the above (1) with the same order $o$ to increasing number $t$ of terms :\n$$ s_t=\\sum_{n=1}^t e_n(o,t) {(-1)^{n-1}\\over n^z} \\tag 2$$ \nand the trajectory is then given by the sequence $s_1,s_2,s_3,...,s_{48}$      \nHere is the picture of the trajectory of the partial sums in the complex plane without any Euler-summation (or $o=0$) first. The trajectory begins at $s_1=1+0i$ which is the partial sum using only the first term, then proceeds along the blue line from point to point to arrive eventually at $s_\\infty=0+0i$     (update: upps, I see I've the argument in the pictures the letter $s$ which should be $z$ for this discussion here because $s$ denotes here the partial sums, sorry)\n\nWe see the spiralling around the expected final value of $0+0i$ and we need hundreds of terms to be near to only three or four decimal digits.        \nEuler-summation with the weighting coefficients can improve this image dramatically.\nHere is the same computation, but with a small Euler-order $o=0.1$ and number of terms $t=48$: \n\nObviously the convergence to the final value appears much faster and even smoother.     \nA higher order of Eulersummation ($o=0.5$) improves this further:\n\nThis gives even the idea, that the spiralling may be defeated!     \nHere is the detail around the origin, where I rescaled the absolute distance to the origin logarithmically so we see, that the spiralling even seem to stop after only 32 or so terms of the partial sums (but this might be a numerical artifact here).\n\nThe standard Eulersummation uses order $o=1$ and this gives this impression:\n\nwhich does not give much improvement (if at all).  In the logarithmic rescale around the origin we have no more artifact but can still observe that the arclengthes per step seem to decrease.      \nFinally, it seems that there is an \"optimal\" Eulerorder, but which is even complex. I've the impression, that $o=0.5 - 0.3i$ is in some sense optimal: the final steps of the 48-term-trajectory seem to approach the final value in a nearly linear curve:\n \nand the detail around the origin (here just zoomed in) looks very promising:\n \n\n\nSimilarly this can be used to work with the summation for the nontrivial roots with larger imaginary value - and as well for arguments $\\eta(z)$ with smaller (and even negative) real part.\n\n[update] Possibly the last step (the optical improvement using the negative imaginary component of $-0.3i$ for the order $o$ of the Euler-summation) was due to Excel-rounding issues. Using multiprecision Pari/GP I produced the following picture for the order of exactly $o=1/2$ and it seems to be always the best/fastest approximation; the circling around the origin seems unavoidable even if the imaginary component is varied a bit. The following picture shows the approximation of the $s_t$ to the origin in absolute distance (x-axis) and angle (y-axis) for 216 terms of the partial sums $s_t$ (compare the Excel-picture with $o=0.5$. Here I'm leaving out the first $8$ partial sums to keep the picture concise).        \n \nHere is an overlay of the numerical more precise computation focusing the angular and the absolute deviation from the origin in rectancular, but logarithmically scaled terms, for the $o=1/2$ and $o=1/2 - 0.3 \u00ee$ orders. We see, that both orders approximate similarly in terms of absolute deviation (radii decrease very similarly) but with the $o=0.5$-order the sequence of difference of angles seems to decrease nicely:", "meta": {"post_id": 490308, "input_score": 27, "output_score": 41, "post_title": "Show how to calculate the Riemann zeta function for the first non-trivial zero"}}
{"input": "Continuum hypothesis (CH) states that there can be no set whose cardinality is strictly between that of integers and real numbers. Godel, 1940 and Paul Cohen,1963 showed that CH can neither be proved nor be disproved. \nHow can we assert that a statement is true if it cannot be proven? What are the bases to make such assertions? I know this is a deep subject, and I confess that I don't have technical tools to understand everything on this topic. But I would like to have some understanding on why CH has to be true if it cannot be proven. \nDo you have any other examples where statements are absolutely true (i.e with 100% certainty) but cannot be proved??\nEDIT: Is \"CH cannot be proved\" the same as \"proof will not exist ever in future\"? Or is it that \"CH cannot be proved\" implies the insufficient human knowledge to prove it?", "output": "In mathematics, and in particular when talking about the incompleteness phenomenon, there is a grave danger of confusing the two terms \"True\" and \"Provable\".\nBeing \"true\" is a semantic property of a statement. Statements are true in a particular interpretation of the language, or a particular model of a theory. Whereas being provable is a syntactical property which means that there is a sequence statements which are axioms, or inferences from the axioms which is both finite, and its final statement is the statement we wanted to prove.\nThe completeness theorem tells us that $T$ proves a statement if and only if the statement is true in every model of $T$. So we sometimes abuse the meaning and say things like \"Cantor's theorem is true in $\\sf ZF$\" when we mean that $\\sf ZF$ can prove Cantor's theorem.\nSometimes, as in the case of arithmetic, there is an intended interpretation or a standard model for the theory. With the integers we have a standard model that we know to characterize. That is the model we care about. So in the context of Peano's axioms of arithmetic we say that $\\varphi$ is true if it is true in the standard model, and provable if it is true in every model. However $\\sf ZFC$ is far from $\\sf PA$ in this context, and it does not have a standard or intended interpretation (in set theory the term \"standard model\" means something which is far more arbitrary than in $\\sf PA$).\nSo finally we ask what does it mean for $\\sf CH$ to be true? Since we don't have a standard model for set theory, we can't associate some \"Platonic truth\" to the statement in the language of set theory. Some will be true in some models, and some will be false in other models. The continuum hypothesis - and its negation - are both such statement. And when we say that the continuum hypothesis is not provable from the axioms of $\\sf ZFC$, we mean that mathematically we have proved that this statement does not have a proof from the axioms. How did we do that? We showed that there models where it is true, and models where it is false (assuming there are models to begin with, of course).\nSo saying that $\\sf CH$ is true, but not saying where is meaningless in the sense that it doesn't offer sufficient information to properly evaluate the claim. It is true in $L$, which is Godel's constructible universe (i.e. in every model of set theory satisfying the statement $V=L$ the continuum hypothesis is true). But it need not be true in other models of set theory (e.g. models of $\\sf PFA$).\nEdit: \nFrom the question's comments it shows that the question rose after reading a statement of the form \"the continuum hypothesis is true for all practical purposes\" and \"the continuum hypothesis is true for Borel sets\". The statement simply say that if we are only concerned about Borel sets (or some other indicated family of sets) then they are either countable, or have the cardinality of the continuum.\nHistorically when Cantor set to prove his continuum hypothesis, it was simply to find a bijection between open intervals and the real numbers; and using a clever method he showed that uncountable closed sets must have the cardinality of the continuum. Cantor expected that these proofs can carry on on some \"complexity\" of sets, and eventually cover all the sets. However the construction of taking complement and countable unions only gives us the Borel sets, and indeed shortly after leaving the Borel sets one can already run into classes which do not have to satisfy the continuum hypothesis.\n\nFurther reading:\n\nWhy is the Continuum Hypothesis (not) true?\nUnprovable things\nNeither provable nor disprovable theorem", "meta": {"post_id": 494099, "input_score": 14, "output_score": 50, "post_title": "Why is CH true if it cannot be proved?"}}
{"input": "Is there any simple explanation for Haar Measure and its geometry?\nhow do we understand analogy Between lebesgue measure and Haar Measure?\nHow to show integration with respect to Haar Measure?\nwhat do we mean by integrating with respect to Measure?", "output": "The question leaves your background a bit unclear, so I chat descriptively about the three Haar measures you are likely to be aware of without even knowing that they are Haar measures. Nothing in what follows is rigorous, but rather seeks to give you a taste of what Haar measure is about.\nWe can measure the size $m(S)$ of a subset $S$ of $\\mathbb{R}$ simply by the integral\n$$\nm(S)=\\int_S 1\\,dx.\n$$\nIf $S=[a,b]$ is an interval, this gives the length $m(S)=b-a$. Here we can think of that $dx$ as a measure (technically it's not, but I ignore that here).\nWhat makes this into a Haar measure is the fact that it is translation invariant.\nIf $c\\in\\mathbb{R}$ is a constant, and $c+S=\\{c+s\\mid s\\in S\\}$, then\n$$\nm(c+S)=\\int_{c+S}1\\,dx=\\int_S1\\,dx=m(S),\n$$\nbecause the substitution $t=x+c$ transforms one integral to the other. Effectively we use\n$$\nd(x+c)=dx\n$$\nand say that $dx$ is a translation invariant measure, i.e. a Haar measure of the additive group of reals.\nWhat about the multiplicative group of positive reals? A Haar measure should be invariant under the group operation. So if $S\\subseteq\\mathbb{R}$ (is a measurable subset), we want\n$m(S)=m(cS)$ to hold for all $c>0$. Obviously the above measure $\\int_S\\,dx$ won't do. For example, the length of an interval is not invariant under scaling.\nThis time we should use the definition\n$$\nm(S)=\\int_S\\frac{dx}x\n$$\ninstead. Here the 1-form $x^{-1}dx$ works. Basically because\n$$\n\\frac{d(cx)}{cx}=\\frac{c\\,dx}{cx}=\\frac{dx}x.\n$$\nYou can check that whenever $0<a<b$, we have\n$$\nm([a,b])=\\int_a^b\\frac{dx}x=\\ln b-\\ln a=\\int_{ca}^{cb}\\frac{dx}x=m([ca,cb]).\n$$\nThus $x^{-1}\\,dx$ is a Haar measure of the multiplicative group of reals.\nA third instance of Haar measure that you have surely seen is that of the unit circle of the complex plane $C=\\{e^{i\\phi}\\mid 0\\le\\phi <2\\pi\\}$. Here the group operation is multiplication. Multiplication by the number $e^{i\\phi_0}$ amounts to rotating the circle counterclockwise by the angle $\\phi_0$. Here\nthe \"measure\" $d\\phi$ will be invariat under such rotations as\n$$\nd(\\phi+\\phi_0)=d\\phi,\n$$\nor, if $S\\subseteq C$, then\n$$\nm(S)=\\int_Sd\\phi=\\int_{e^{i\\phi_0}S}d(\\phi+\\phi_0)=\\int_{e^{i\\phi_0}S}d\\phi=m(e^{i\\phi_0}S).\n$$\nIn the last example (as the group $C$ is compact as a topological space), it is customary (but not necessary for all purposes) to divide the measure by $2\\pi$ so that $m(C)=1$.\nThe fun facts are that for Lie groups and locally compact topological groups a Haar measure always exists. It is unique up to a constant multiplier in many important cases such as compact. For non-commutative groups (e.g. matrix groups) there is a distinction between invariance under group operation from the left or from the right. The two notions coincide in the compact case.\n\nSketching an example, where there is a difference between left and right invariant.\nConsider the group $G$ of real upper triagular matrices of the form\n$$\nG=\\left\\{\\left(\\begin{array}{cc}\\sqrt{y}&\\frac{x}{\\sqrt y}\\\\0&\\frac1{\\sqrt{y}}\n\\end{array}\\right)\\mid x,y\\in\\mathbb{R},y>0\\right\\}.\n$$\nLet us denote the above element of $G$ by $g(x,y)$.\nConsider the differential\n$$\ndg(x,y)=\\frac{\\partial}{\\partial y}g(x,y)\\,dy+\\frac{\\partial}{\\partial x}g(x,y)\\,dx.\n$$\nFor any fixed element $g(r,s)\\in G$ we see that\n$$\n(g(r,s)g(x,y))^{-1}d(g(r,s)g(x,y))=g(x,y)^{-1}g(r,s)^{-1}g(r,s)dg(x,y)\n=g(x,y)^{-1}dg(x,y),\n$$\nso the entries of the matrix of 1-forms \n$$\ng(x,y)^{-1}dg(x,y)=\\left(\n\\begin{array}{cc}\n\\frac{dy}{2y}&\\frac{dx}y\\\\0&-\\frac{dy}{2y}\\end{array}\\right)\n$$\nare left invariant. Therefore the exterior product of the top row entries\n$$\n\\frac{dy}{2y}\\wedge\\frac{dx}y=\\frac12\\frac{dy\\wedge dx}{y^2}\n$$\nis a left-invariant 2-form. Number theorists will recognize this (up to a scalar factor) as the\nhyperbolic metric of the upper half plane. We can define an action of $G$\non the upper half plane by the recipe of fractional linear\ntransformations. Let $\\tau$ be an arbitrary element of the upper half-plane \nand define $g\\cdot\\tau=z_1/z_2$, where \n$$\n\\left(\\begin{array}{c}z_1\\\\\nz_2\\end{array}\\right)\n=g\\left(\n\\begin{array}{c}\\tau\\\\1\\end{array}\\right).\n$$\nHere $g(y,x)\\cdot i=x+iy$, so the entire upper half plane is the orbit of $i$ under $G$.\nWe get a right invariant 2-form similarly by using\n$$\ndg(x,y)g(x,y)^{-1}=\\left(\n\\begin{array}{cc}\n\\frac{dy}{2y}&dx-\\frac{x\\,dy}y\\\\0&-\\frac{dy}{2y}\\end{array}\\right).\n$$\nAgain the wedge product of the entries of the upper row gives a right invariant 2-form\n$$\n\\frac{dy}{2y}\\wedge \\left(dx-\\frac{x\\,dy}y\\right)=\\frac{dy\\wedge dx}{2y}.\n$$\nWe see that this time the left and right invariant measures are not scalar multiples of each other.", "meta": {"post_id": 494225, "input_score": 60, "output_score": 130, "post_title": "What is Haar Measure?"}}
{"input": "It would be extremely helpful if anyone gives me the formal definition of conditional probability and expectation in the following setting, given probability space \n$ (\\Omega, \\mathscr{A}, \\mu ) $ with $\\mu(\\Omega) = 1 $, and a random variable $ X : \\Omega \\rightarrow \\mathbb{R}^n $, where for any borel set $ A \\in \\mathscr{B}(\\mathbb{R}^n) $ we define \n$$ \\mathbb{P}(X \\in A) = (X_*\\mu)(A) = \\mu(X^{-1}(A))=\n\\mu(\\{\\omega\\in \\Omega\\ \\ |\\ \\ X(\\omega)\n\\in A\\})\\ \\ \\text{and}\\ \\ \\mathbb{E}(X) = \\int_\\Omega Xd\\mu $$\nRegardless of $X, Y$ being discrete or continuous (with density $f_X, f_Y $ and joint density $f_{X,Y} $ w.r.t some measure $\\nu$ on $\\mathbb{R}^n $), I am asking for the definition \nof $ \\mathbb{P}(Y\\in B\\ |\\ X \\in A) $ and $ \\mathbb{E}(Y|X) $  for all Borel sets $ A, B \\in \\mathscr{B}(\\mathbb{R}^n) $, keeping in mind that $ \\mathbb{P}(X \\in A) $ may well be zero. \nIn our probability class some thing of the following sort was mentioned, where\n$\\delta_x$ is the Dirac distribution at $ x $, then we have\n$$ \\mathbb{E}(Y|X = x) = \\frac{\\mathbb{E}(\\delta_x(X)Y)}{\\mathbb{P}(X=x)}$$\nout of which I can't make any sense. Any appropiate reference for these is also very much welcome. \nThank you.", "output": "Let throughout this post $(\\Omega,\\mathcal{F},P)$ be a probability space, and let us first define the conditional expectation ${\\rm E}[X\\mid\\mathcal{G}]$ for integrable random variables $X:\\Omega\\to\\mathbb{R}$, i.e. $X\\in L^1(P)$, and sub-sigma-algebras $\\mathcal{G}\\subseteq\\mathcal{F}$.\n\nDefinition: The conditional expectation ${\\rm E}[X\\mid\\mathcal{G}]$ of $X$ given $\\mathcal{G}$ is the random variable $Z$ having the following properties:\n(i) $Z$ is integrable, i.e. $Z\\in L^1(P)$.\n(ii) $Z$ is ($\\mathcal{G},\\mathcal{B}(\\mathbb{R}))$-measurable.\n(iii) For any $A\\in\\mathcal{G}$ we have \n  $$\n\\int_A Z\\,\\mathrm dP=\\int_A X\\,\\mathrm dP.\n$$\n\nNote: It makes sense to talk about the conditional expectation since if $U$ is another random variable satisfying (i)-(iii) then $U=Z$ $P$-a.s.\n\nDefinition: If $X\\in L^1(P)$ and $Y:\\Omega\\to\\mathbb{R}$ is any random variable, then the conditional expectation of $X$ given $Y$ is defined as\n  $$\n{\\rm E}[X\\mid Y]:={\\rm E}[X\\mid\\sigma(Y)],\n$$\n  where $\\sigma(Y)=\\{Y^{-1}(B)\\mid B\\in\\mathcal{B}(\\mathbb{R})\\}$ is the sigma-algebra generated by $Y$.\n\nI'm not aware of any other definition of $P(Y\\in B\\mid X\\in A)$ than the obvious, i.e.\n$$\nP(Y\\in B\\mid X\\in A)=\\frac{P(Y\\in B,X\\in A)}{P(X\\in A)}\n$$\nprovided that $P(X\\in A)>0$. The only exception being when $A$ contains a single point, i.e. $A=\\{x\\}$ for some $x\\in\\mathbb{R}$. In this case, the object $P(Y\\in B\\mid X=x)$ is defined in terms of a regular conditional distribution. \nLet us first define regular conditional probabilities. Let $X:\\Omega\\to\\mathbb{R}$ be a random variable.\n\nDefinition: A regular conditional probability for $P$ given $X$ is a function \n  $$\n\\mathcal{F}\\times \\mathbb{R} \\ni(A,x)\\mapsto P^X(A\\mid x)\n$$\n  satisfying the following three conditions:\n(i) The mapping $A\\mapsto P^X(A\\mid x)$ is a probability measure on $(\\Omega,\\mathcal{F})$ for all $x\\in \\mathbb{R}$.\n(ii) The mapping $x\\mapsto P^X(A\\mid x)$ is $(\\mathcal{B}(\\mathbb{R}),\\mathcal{B}(\\mathbb{R}))$-measurable for all $A\\in\\mathcal{F}$.\n(iii) The defining equation holds: For any $A\\in\\mathcal{F}$ and $B\\in\\mathcal{B}(\\mathbb{R})$ we have\n  $$\n\\int_B P^X(A\\mid x)\\,P_X(\\mathrm dx)=P(A\\cap\\{X\\in B\\}).\n$$\n\nNote: A mapping satisfying (i) and (ii) is often called a Markov kernel. Furthermore, since $(\\mathbb{R},\\mathcal{B}(\\mathbb{R}))$ is a nice space, the regular conditional probability is unique in the sense that if $\\tilde{P}^X(\\cdot\\mid\\cdot)$ is another regular conditional probability of $P$ given $X$, then we have that $P^X(\\cdot\\mid x)=\\tilde{P}^X(\\cdot\\mid x)$ for $P_X$-a.a. $x$. Here $P_X=P\\circ X^{-1}$ is the distribution of $X$.\n\nConnection: Let $P^X(\\cdot\\mid\\cdot)$ be a regular conditional probability of $P$ given $X$. Then for any $A\\in\\mathcal{F}$ we have\n  $$\n{\\rm E}[1_A\\mid X]=\\varphi(X),\n$$\n  where $\\varphi(x)=P^X(A\\mid x)$. In short we write ${\\rm E}[1_A\\mid X]=P^X(A\\mid X)$.\n\nNow let us introduce another random variable $Y:\\Omega\\to\\mathbb{R}$, and $P^X(\\cdot\\mid \\cdot)$ still denotes a regular conditional probability of $P$ given $X$.\n\nDefinition: For $B\\in\\mathcal{B}(\\mathbb{R})$ and $x\\in\\mathbb{R}$ we define the regular conditional distribution of $Y$ given $X$ by\n  $$\nP_{Y\\mid X}(B\\mid x):=P^X(Y\\in B\\mid x).\n$$\n\nInstead of $P_{Y\\mid X}(B\\mid x)$ one often writes $P(Y\\in B\\mid X=x)$.\nAn easy consequence of this definition is that $(B,x)\\mapsto P_{Y\\mid X}(B\\mid x)$ is a Markov kernel and for any $A,B\\in\\mathcal{B}(\\mathbb{R})$ we have\n$$\n\\int_A P_{Y\\mid X}(B\\mid x)\\,P_X(\\mathrm dx)=P(\\{X\\in A\\}\\cap\\{Y\\in B\\}). \\tag{1}\n$$\nIn fact, $P_{Y\\mid X}(\\cdot \\mid \\cdot)$ is a regular conditional distribution of $Y$ given $X$ if and only if $P_{Y\\mid X}(\\cdot\\mid\\cdot)$ is a Markov kernel and satisfies $(1)$. Again $(1)$ is often referred to as the defining equation.\n\nDefinition: Let $P^X(\\cdot\\mid\\cdot)$ be a regular conditional probability of $P$ given $X$. Furthermore, let $U:\\Omega\\to\\mathbb{R}$ be another random variable that is assumed bounded (to ensure the following expectations exist). Then we define the (regular) conditional mean of $U$ given $X=x$ by \n  $$\n{\\rm E}[U\\mid X=x]:=\\int_\\Omega U(\\omega)\\, P^X(\\mathrm d\\omega\\mid x).\n$$\n\nLet us denote $\\psi(x)={\\rm E}[U\\mid X=x]$. Then we have the following:\n\nConnection: The mapping $\\mathbb{R}\\ni x\\mapsto \\psi(x)$ is $(\\mathcal{B}(\\mathbb{R}),\\mathcal{B}(\\mathbb{R}))$-measurable, and\n  $$\n{\\rm E}[U\\mid X]=\\psi(X).\n$$\n\nThe following is an extremely useful rule when calculating with conditional distributions:\n\nRule: Let $X$ and $Y$ be as above, and let $\\xi:\\mathbb{R}^2\\to\\mathbb{R}$ be $(\\mathcal{B}(\\mathbb{R}^2),\\mathcal{B}(\\mathbb{R}))$-measurable. Then\n  $$\nP(\\xi(X,Y)\\in D\\mid X=x)=P(\\xi(x,Y)\\in D\\mid X=x),\\quad D\\in\\mathcal{B}(\\mathbb{R}),\n$$\n  holds for $P_X$-a.a. $x$. This is saying that \"conditional on $X=x$ we may replace $X$ by $x$\".\n\nThe following example shows how this rule can be useful: Let $X$ and $Y$ be independent $\\mathcal{N}(0,1)$ random variables, and let $U=X+Y$. Then we claim that $U\\mid X=x\\sim \\mathcal{N}(x,1)$ for $P_X$-a.a. $x$. To see this, note that by the rule above, the distribution of $U\\mid X=x$ and $Y+x\\mid X=x$ is the same. But since $Y$ is independent of $X$ we have that $Y+x\\mid X=x$ is distributed as $Y+x$. We can write it as follows:\n$$\nU\\mid X=x\\sim Y+x\\mid X=x\\sim Y+x\\sim\\mathcal{N}(x,1).\n$$", "meta": {"post_id": 496608, "input_score": 61, "output_score": 86, "post_title": "Formal definition of conditional probability"}}
{"input": "A quote from Enderton:\n\nOne might well question whether there is any meaningful sense in which one can say that the continuum hypothesis is either true or false for the \"real\" sets. Among those set-theorists nowadays who feel that there is a meaningful sense, the majority seems to feel that the continuum hypothesis is false.\n\nI am interested in the motivation behind this belief. In other words, what advances in set-theory could make one favor this option over the other?", "output": "I have written extensively on very related matters. I suggest you take a look here. What I will say now is more or less there or in the links there (some times with additional details), but I hope you find the description here useful, even if it is somewhat of a caricature.\n\nNaturally, there are set theorists that consider the question of the truth value of $\\mathsf{CH}$ meaningless. Set theory is the study of the consequences of the Zermelo Fraenkel (+Choice) axioms, and that's it. Part of what one studies is consistency results, and their associated consistency strength, but truth only makes sense in the context of a proof, which means a formal proof in first order logic, and these proofs only make sense within the context of an axiomatic theory, and the theory that has been agreed upon (more or less universally) is $\\mathsf{ZFC}$, which is not enough to settle the continuum hypothesis.\nIn fact, there may be a somewhat agnostic minority within this group, that does not even consider the issue of the consistency of $\\mathsf{ZFC}$ a settled matter. All that we know is that either it is consistent, but its consistency is unprovable, or else it is inconsistent, and we have not yet uncovered its contradictions. (Within this group, most seem to consider that restricting replacement to $\\Sigma_2$ formulas ought to be on the \"safe\" side. Others do not even agree on this. But this is another matter.)\n(From a philosophical point of view, I consider this view unsatisfactory, and the equating of proof with first-order proof problematic, but let's not get into this. Note that mathematically there is nothing wrong with this view in that in no way restricts what kind of results one would consider valid, or what kind of structures one would study.)\n\nAmong set theorists that consider that set theory is not just the study of the consequences of $\\mathsf{ZFC}$, there is a more or less unified consensus that one of the goals of work in the field is to identify natural extensions of the current list of axioms that will eventually be recognized as part of what we should consider the basic list. There is a program essentially initiated by G\u00f6del that hopes to identify these extensions, and obtain as a result a more complete description of the universe of sets. The main success of the program so far is the identification of the large cardinal hierarchy, that (I believe) most set theorists agree are part of the set theoretic universe, even if some traditional adherence to $\\mathsf{ZFC}$ still leads us to explicitly mention their use every time they are invoked. \nWe cannot yet fully explain the coherency of the theories given by large cardinals, but we have partial results. This coherency (at the arithmetic, and later, at the projective level), and the companion generic absoluteness results, is one of the main sources of their acceptance. The problem with large cardinals is that they do not settle $\\mathsf{CH}$. \nOne can think of large cardinals as providing the universe with \"height\". One may expect that the true universe of sets should also be \"wide\", and so after large cardinals one would pursue extensions that imply this width. The most successful set of principles providing the universe with such richness comes in the form of reflection principles. These principles (typically, more or less natural generalizations of reflection principles outright provable in $\\mathsf{ZFC}$) tend to imply that $\\mathsf{CH}$ is false (in fact, they usually imply that the continuum is $\\aleph_2$). To my mind, the strongest current arguments for $\\lnot\\mathsf{CH}$ come from the embracing of strong reflection principles.\n(These principles come in several flavors. One can see many large cardinal hypotheses formulated in terms of elementary embeddings to be essentially giving us local reflection principles. Other establish that many initial stages of the universe resemble the full universe in non-trivial ways. There are yet others.)\nOther arguments have been proposed. Determinacy, which contradicts choice but is true in some inner models if we accept large cardinals, implies that the continuum is very large. We expect that this should be reflected at the definable (projective) level, so not only we expect $\\mathsf{CH}$ to fail, we expect a theory that would provide us with explicit counterexamples. It was explicitly stated by Donald Martin that, if one is to argue against $\\mathsf{CH}$, this would be a reasonable program to achieve this goal, see for example\n\nDonald A. Martin. Hilbert's first problem: the continuum hypothesis. In Mathematical developments arising from Hilbert problems (Proc. Sympos. Pure Math., Northern Illinois Univ., De Kalb, Ill., 1974), pp. 81\u201392. Amer. Math. Soc., Providence, R. I., 1976. MR0434826 (55 #7790). \n\nThe most successful result in this direction seems to be Woodin's realization that $\\mathsf{MM}$, Martin's maximum (a strengthening of reflection principles, describing in a precise way how the universe satisfies a sort of \"saturation\"), implies that the projective ordinal $\\mathbf{\\delta}^1_2$ is $\\omega_2$, thus indicating that there are projective pre-well-orderings of the reals that witness the falsehood of $\\mathsf{CH}$.\nFor an account of other arguments against $\\mathsf{CH}$, see Believing the axioms, by Penelope Maddy, particularly the second part: J. Symbolic Logic 53 (1988), no. 2, 481-511 and no. 3, 736-764. These papers are also an excellent introduction to the question of what counts as evidence when a formal proof is impossible. \nA recent argument against $\\mathsf{CH}$ was proposed by Woodin, see here and here. As explained in the first link above, there were some problems with this argument, that weaken its conclusion. \n\nThere are also set theorists that expect $\\mathsf{CH}$ to be true. I do not consider the argument that $\\mathsf{CH}$ trivializes the theory of cardinal characteristics of the continuum to carry much weight, for two reasons. One, the theory of Tukey reductions remains regardless of $\\mathsf{CH}$. Second, $\\mathsf{CH}$ provides us with a rich variety of objects in analysis, the theory of linear orders, algebra, etc, that more than makes up for the coincidence of a few cardinals. In fact, there is an empirical dichotomy that gives us much control (\"few objects\") in the presence of forcing axioms such as $\\mathsf{MM}$, while it gives us great chaos (\"as many examples as possible\") if we accept $\\mathsf{CH}$.\nThe strongest arguments for $\\mathsf{CH}$ that I know of come from three fronts: One, $\\mathsf{CH}$ gives us a rich theory of generic embeddings. This view is detailed in the work of Matt Foreman. Foreman identified a natural hierarchy of generic embeddings, and (in the 1980s) showed that some of these embeddings actually imply $\\mathsf{CH}$. These are natural statements that in a sense generalize large cardinals. On the other hand, there are also natural generic embedding statements that contradict $\\mathsf{CH}$, so this argument, even if appealing, is at best inconclusive.\nTwo, rather than $\\mathsf{CH}$, we pursue a view of the universe as an \"$L$-like\" model, and it is a basic consequence of the resulting fine-structural theory that $\\mathsf{CH}$ (in fact, $\\mathsf{GCH}$) holds. The second view fits naturally with the inner model program of the California school, and any progress in this program can be seen as strengthening the case of $\\mathsf{CH}$. However, this second approach loses some strength, as the pursuit of the inner model program is largely independent of the \"background\" theory , and conflating the two may be seen as a distraction rather than an advantage. Certainly, the fact that there are nice inner models in the universe does not immediately suggest that the universe itself is one of them. (On the other hand, one could argue that $\\mathsf{HOD}$ should be but, again, that's another matter.)\nThere is yet a third argument, perhaps the strongest. Woodin has a nice result showing that $\\mathsf{CH}$, itself a $\\Sigma^2_1$ statement, is \"maximal\" among $\\Sigma^2_1$ statements, in that any such statement that can be proved consistent with large cardinals via forcing is actually a consequence of $\\mathsf{CH}$(+large cardinals). \nThere is not really a corresponding result for the negation of $\\mathsf{CH}$. There are, however, maximality results of another kind (found through the theory of $\\mathbb P_{max}$ forcing).  \nThe result in itself is of course not \"proof\" that $\\mathsf{CH}$ is true. If one wants to make a case, one should aim at presenting this maximality as part of a larger picture, and there have been many problems trying to carry this out. (See here for an extension.)\n\nA very attractive alternative has emerged recently. We understand a good set theory should allow us to interpret other theories. So, if we want to study models of $\\mathsf{CH}$, the theory should be able to provide us with such models, without any artificial restrictions. Instead, if we prefer models of $\\mathsf{MM}$, displaying such models should be possible as well. The method of forcing gives us a tool for carrying out these interpretations. If it turns out that two theories are mutually interpretable, then on mathematical grounds there is really no reason to prefer one over the other. Such choice would in principle be guided by extra-mathematical considerations, perhaps of an aesthetic nature.\nTaking this point of view seriously leads to a multiverse theory. There are several candidate theories here (there are some additional details at the first link). In all, it is the case that $\\mathsf{CH}$ is true in some models and false in others, and it does not quite make sense to ask whether it is true or false in absolute terms. The multiverse approach I prefer gives us a \"partial order\" of universes and their forcing extensions. If a model is a forcing extension of another one, then we also consider the latter. Part of the formalization of the theory may be in deciding whether there is a \"distinguished ground model\" or not. If there is, of course it makes sense to ask whether $\\mathsf{CH}$ holds in it, but deciding this does not appear to me to constitute a solution to the continuum problem. In fact, one could say that the continuum problem is not a \"real question\" in this view of set theory. The clearest presentation of this approach that I am aware of is this essay by Steel.\n\nIs there really a majority preferring the negation of $\\mathsf{CH}$? The picture provided by reflection principles, forcing axioms, Woodin's results with $\\mathbb P_{max}$, etc, is very compelling. On the other hand, the search for inner models of large cardinals gives us very nice theories implying $\\mathsf{CH}$, including what Woodin calls \"ultimate $L$\", and then there are Woodin's maximality results. It appears to me forcing axioms are more popular (though I may be biased). Certainly the proofs of their consistency (though involved) are less technical that the proofs of maximality from $\\mathsf{CH}$ and its proposed generalizations, so it seems natural more set theorists study their consequences and consider them.  \nPopularity, of course, cannot be the arbiter of truth: Whatever axioms we decide best explain the universe of sets ought not to be chosen based on it. But the popularity of forcing axioms and reflection principles is due to mathematical reasons: They have great explanatory power. Many classification results can be carried out in their presence. They serve as organizing principles. These are all benchmarks of a good theory. \nThe principles discussed by Maddy in her essays suggest that principles giving us a rich universe of sets should be preferable over the alternative. This explains why principles such as $V=L$ (that implies $\\mathsf{CH}$) that some model theorists prefer (but see here), are actually not given much weight, since they unnecessarily restrict the interpretative power of the theory. (For an opposing view to this argument against $V=L$, see here.)\nI expect in the future the multiverse view will gain weight. Beyond this, I cannot really say what the ultimate status of $\\mathsf{CH}$ will be.", "meta": {"post_id": 499560, "input_score": 42, "output_score": 79, "post_title": "Why is the continuum hypothesis believed to be false by the majority of modern set theorists?"}}
{"input": "Let $f_n(x)$ be recursively defined as\n$$f_0(x)=1,\\ \\ \\ f_{n+1}(x)=\\sqrt{x+f_n(x)},\\tag1$$\ni.e. $f_n(x)$ contains $n$ radicals and $n$ occurences of $x$:\n$$f_1(x)=\\sqrt{x+1},\\ \\ \\ f_2(x)=\\sqrt{x+\\sqrt{x+1}},\\ \\ \\ f_3(x)=\\sqrt{x+\\sqrt{x+\\sqrt{x+1}}},\\ \\dots\\tag2$$\nThe functions $f_0(x)$, $f_1(x)$ and $f_2(x)$ are integrable in elementary functions, e.g.:\n$$\\int\\sqrt{x+\\sqrt{x+1}}\\,dx=\\left(\\frac{2\\,x}3+\\frac{\\sqrt{x+1}}6-\\frac14\\right)\\sqrt{x+\\sqrt{x+1}}+\\frac58\\ln\\left(2\\,\\sqrt{x+1}+2\\,\\sqrt{x+\\sqrt{x+1}}+1\\right).\\tag3$$\n\n\nQuestion: Is there an integer $n>2$ such that $f_n(x)$ is integrable in elementary functions?\n\n\nUpdate: The question is reposted at MathOverflow as was suggested by moderator.", "output": "I posted an answer to this question as an answer to the related question on MathOverflow. See my (second) answer at https://mathoverflow.net/questions/171733.  However, as Krokop pointed out, it would be better to have a copy here on Math.SE to the original question.  Thanks to Krokop for copying my answer over to Math.SE.\n\nI am going to show that there is no elementary antiderivative of $f_n$ when  $n>2$.\n\nAssume $n>2$ (NB: This is important, because the argument below will not work for $n\\le2$; the reader may enjoy finding where it breaks down), and let $K_n = {\\mathbb C}\\bigl(x,f_n(x)\\bigr)$ be the elementary differential field generated by $x$ and $f_n(x)$.  Then $K_n$ is the field of meromorphic functions on the normalization $\\hat C_n$ of the algebraic curve $C_n$ defined by the minimal degree $y$-monic polynomial $P_n(x,y)$ that satisfies $P_n\\bigl(x,f_n(x)\\bigr) \\equiv 0$.  This minimal degree is $2^n$; for example, $P_2(x,y) = (y^2-x)^2-x-1$ and $P_3(x,y) = \\bigl((y^2-x)^2-x\\bigr)^2-x-1$, etc.\n\nSince $P_{n+1}(x,y) = (P_n(x,y)+1)^2-x-1$ for $n\\ge 1$ with $P_1(x,y)=y^2-x-1$, one sees, by applying the Eisenstein Criterion to $P_n(x,y)$ regarded as an element of $D[y]$ with $D$ being the integral domain ${\\mathbb C}[x]$, that $P_n(x,y)$ is irreducible for all $n\\ge 1$.  Hence, $\\hat C_n$ is connected. \n\nIt will be important in what follows to observe that $K_n$ has an involution $\\iota$ that fixes $x$ and sends $f_n(x)$ to $-f_n(x)$; this is because $P_n(x,y)$ is an even polynomial in $y$.  The fixed field of $\\iota$ is ${\\mathbb C}\\bigl(x,\\,f_n(x)^2\\bigr)$, and the $(-1)$-eigenspace of $\\iota$ is ${\\mathbb C}\\bigl(x,\\,f_n(x)^2\\bigr)f_n(x) = K_{n-1}{\\cdot}f_n(x)$.\n\nNow, the curve $C_n\\subset \\mathbb{CP}^2$ has only one point on the line at infinity, namely $[1,0,0]$, but the normalization $\\hat C_n$ has $2^{n-1}$ points lying over this point.  They can be parametrized as follows:  First, establish the convention that $\\sqrt{u}$ means the unique analytic function on the complex $u$-plane minus its negative axis and $0$ that satisfies $\\sqrt1 = 1$ and $\\bigl(\\sqrt{u}\\bigr)^2 = u$.  Let $\\epsilon = (\\epsilon_1,\\ldots,\\epsilon_{n-1})$ be any sequence with ${\\epsilon_k}^2=1$ and consider the sequence of functions $g^\\epsilon_k(t)$ defined by the criteria $g^\\epsilon_1(t) = \\sqrt{1+t^2}$ and $g^\\epsilon_{k+1}(t) = \\sqrt{1+\\epsilon_{n-k}t g^\\epsilon_k(t)}$ for $1\\le k < n$.  Choose, as one may, a $\\delta_n>0$ sufficiently small so that, when $t$ is complex and satisfies $|t|<\\delta_n$, all of the functions $g^\\epsilon_k$ are analytic when $|t|<\\delta_n$.  In particular, one finds an expansion\n$$\ng^\\epsilon_n(t) \n= 1+\\tfrac12\\epsilon_1\\,t + \\tfrac18(2\\epsilon_1\\epsilon_2-1)t^2 + O(t^3).\n$$\n\nAlso, it is easy to verify that the disk in $\\mathbb{CP}^2$ defined by\n$$\n[x,y,1] = [1,\\ t g^\\epsilon_n(t),\\ t^2]\\qquad\\text{for}\\quad |t|<\\delta_n\n$$\nis a nonsingular parametrization of a branch of $C_n$ in a neighborhood of the point $[1,0,0]$.  In the normalization $\\hat C_n$, this is then a local parametrization of a neighborhood of a point $p_\\epsilon\\in \\hat C_n$.\nObviously, this describes $2^{n-1}$ distinct points on $\\hat C_n$. \n\nWhen $x$ and $f_n$ are regarded as meromorphic functions on $\\hat C_n$, \nit follows that there is a unique local coordinate chart $t_\\epsilon:D_\\epsilon\\to D(0,\\delta_n)\\subset \\mathbb{C}$ of an open disk $D_\\epsilon\\subset \\hat C_n$ about $p_\\epsilon$ such that $t_\\epsilon(p_\\epsilon)=0$ and on which one\nhas formulae\n$$\nx = \\frac1{{t_\\epsilon}^2}\n\\quad\\text{and}\\quad\nf_n(x) = \\frac{g^\\epsilon_n(t_\\epsilon)}{t_\\epsilon} \n= \\frac{1+\\tfrac12\\epsilon_1\\ t_\\epsilon \n         +\\tfrac18(2\\epsilon_1\\epsilon_2-1)\\ {t_\\epsilon}^2}\n   {t_\\epsilon} \n+ O({t_\\epsilon}^2).\n$$ \nIn particular, it follows that $f_n(x)$, as a meromorphic function on $\\hat C_n$,\nhas polar divisor equal to the sum of the $p_\\epsilon$ and hence has degree $2^{n-1}$. Of course, this implies that the zero divisor of $f_n(x)$ on $\\hat C_n$ must be of degree $2^{n-1}$ as well. \n\nNote that the functions $g^\\epsilon_k$ satisfy $g^{-\\epsilon}_k(-t) = g^{\\epsilon}_k(t)$, where $-\\epsilon = (-\\epsilon_1,\\ldots,-\\epsilon_{n-1})$.\nThis implies that $\\iota(p_\\epsilon) = p_{-\\epsilon}$ and that\n$t_\\epsilon\\circ\\iota = -t_{-\\epsilon}$.\n\nNow, the $2^{n-1}$ zeroes of $f_n(x)$ on $\\hat C_n$ are distinct, for they are the zeros of the polynomial $q_n(x) = P_n(x,0) = (q_{n-1}+1)^2-x-1$, and the discriminant of $q_n$, being the resultant of $q_n$ and $q_n'$, is clearly an odd integer, and hence is not zero.  Thus, $C_n$ is a branched double cover of $C_{n-1}$, branched exactly where $f_{n}$ has its zeros. This induces a branched cover $\\pi_n:\\hat C_n\\to \\hat C_{n-1}$ that is exactly the quotient of $\\hat C_n$ by the involution $\\iota$ (whose fixed points are where $f_n$ has its zeros).  Since one then has the Riemann-Hurwitz formula\n$$\n\\chi(\\hat C_n) = 2\\chi(\\hat C_{n-1}) - B_n = 2\\chi(\\hat C_{n-1}) - 2^{n-1},\n$$\nand $\\chi(\\hat C_1) = \\chi(\\hat C_2) = 2$, induction gives $\\chi(\\hat C_n) = \n(3{-}n)2^{n-1}$, so the genus of $\\hat C_n$ is $(n{-}3) 2^{n-2} + 1$.  (This won't actually be needed below, but it is interesting.)\n\nThe only poles of $x$ and $f_n(x)$ on $\\hat C_n$ are the points $p_\\epsilon$, \nand computation using the above expansions shows that, \nin a neighborhood of $p_\\epsilon$, one has an expansion of the form\n$$\nf_n(x)\\,\\mathrm{d} x \n- \\mathrm{d}\\left(f_n(x)\\bigl(\\tfrac12\\ x + \\tfrac16\\ f_n(x)^2\\bigr) \\right)\n= \\left(\\frac{ (1-\\epsilon_1\\epsilon_2) }\n         {4{t_\\epsilon}^2}\n          + O({t_\\epsilon}^{-1})\\right)\\ \\mathrm{d} t_\\epsilon\\ .\n$$\nThus, the meromorphic differential $\\eta$ on $\\hat C_n$ \ndefined by the left hand side of this equation has, at worst, double poles \nat the points $p_\\epsilon$ and no other poles.\n\nNow, by Liouville's Theorem, $f_n$ has an elementary antiderivative if and only if $f_n(x)\\ \\mathrm{d} x$ and, hence, the form $\\eta$ are expressible as finite linear combinations of exact differentials and log-exact differentials.\nThus, $f_n(x)$ has an elementary antiderivative if and only if $\\eta$ is expressible in the form\n$$\n\\eta = \\mathrm{d} h + \\sum_{i=1}^m c_i\\,\\frac{\\mathrm{d} g_i}{g_i}\n$$\nfor some $h,g_1,\\cdots g_m\\in K_n$ and some constants $c_1,\\ldots,c_m$.  Suppose that these exist.  Since $\\eta$ has, at worst, double poles at the $p_\\epsilon$ and no other poles, it follows that $h$ must have, at worst, simple poles at the points $p_\\epsilon$ and no other poles; in fact, $h$ is uniquely determined up to an additive constant because its expansion at $p_\\epsilon$ in terms of $t_\\epsilon$ must be of the form\n$$\nh = \\frac{\\epsilon_1\\epsilon_2-1}{4t_\\epsilon} + O(1).\n$$\nMoreover, because $\\eta$ is odd with respect to $\\iota$, it follows that $h$ (after adding a suitable constant if necessary) must also be odd with respect to $\\iota$.  This implies, in particular, that $h$ vanishes at each of the zeros of $f_n$ (which, by the argument above, are simple zeros).  This implies that $h = r\\,f_n$ for some $r\\in K_{n-1}$ that has no poles and satisfies $r(p_\\epsilon) = (\\epsilon_1\\epsilon_2-1)/4$ for each $\\epsilon$.  However, since $r$ has no poles and $\\hat C_n$ is connected, it follows that $r$ is constant.  Thus, it cannot take the two distinct values $0$ and $-1/2$, as the equation $r(p_\\epsilon) = (\\epsilon_1\\epsilon_2-1)/4$ implies.\n\nThus, the desired $h$ does not exist, and $f_n$ cannot be integrated in elementary terms for any $n>2$.", "meta": {"post_id": 500589, "input_score": 80, "output_score": 50, "post_title": "Integrals of $\\sqrt{x+\\sqrt{\\phantom|\\dots+\\sqrt{x+1}}}$ in elementary functions"}}
{"input": "$R$ is a commutative ring. $p(x)$ is an irreducible polynomial of $R[x]$. Is the ideal $(p(x))$ generated by $p(x)$ in $R[x]$ prime?\nIf not, under what conditions of $R$ is $(p(x))$ prime? How about maximal?", "output": "@kahen gave a good example. Here is more detail, note that $R[x]$ is the ring we look at. Also when there is no implication means there is a counterexample.\nIn a commutative ring $R$ with 1\n\\begin{array}{|ccccc|}\n\\hline\nR/(a) \\text{ integral domain} &\\iff &(a) \\text{ prime ideal} & \\iff & a \\text{ prime}\\\\\n&&&  & \\Downarrow & \\\\\n\\Uparrow&&(a) \\text{ maximal among principal} & \\Longleftarrow & a \\text{ irreducible} &\\\\\n && &  &  & \\\\\nR/(a) \\text{ is a field} & \\iff &(a) \\text{ maximal ideal}\\\\\n\\hline\n\\end{array}\nIn an integral domain $R$\n\\begin{array}{|ccccc|}\n\\hline\nR/(a) \\text{ integral domain} &\\iff &(a) \\text{ prime ideal} & \\iff & a \\text{ prime}\\\\\n&&&  & \\Downarrow & \\\\\n\\Uparrow&&(a) \\text{ maximal among principal} & \\iff & a \\text{ irreducible} &\\\\\n && &  &  & \\\\\nR/(a) \\text{ is a field} & \\iff &(a) \\text{ maximal ideal} \\\\\n\\hline\n\\end{array}\nIn a UFD $R$\n\\begin{array}{|ccccc|}\n\\hline\nR/(a) \\text{ integral domain} &\\iff &(a) \\text{ prime ideal} & \\iff & a \\text{ prime}\\\\\n&&&  & \\Updownarrow & \\\\\n\\Uparrow &&(a) \\text{ maximal among principal} & \\iff & a \\text{ irreducible} &\\\\\n && &  &  & \\\\\nR/(a) \\text{ is a field} & \\iff &(a) \\text{ maximal ideal} \\\\\n\\hline\n\\end{array}\nIn a PID $R$\n\\begin{array}{|ccccc|}\n\\hline\nR/(a) \\text{ integral domain} &\\iff &(a) \\text{ prime ideal} & \\iff & a \\text{ prime}\\\\\n&& &  & \\Updownarrow & \\\\\n\\Uparrow &&(a) \\text{ maximal among principal} & \\iff & a \\text{ irreducible} &\\\\\n &&\\Downarrow &  &  & \\\\\nR/(a) \\text{ is a field} & \\iff &(a) \\text{ maximal ideal} \\\\\n\\hline\n\\end{array}", "meta": {"post_id": 501275, "input_score": 24, "output_score": 74, "post_title": "Is the ideal generated by an irreducible polynomial prime?"}}
{"input": "Let $f: X \\to Y$ be continuous and proper (a map is proper iff the preimage of a compact set is compact). Furthermore, assume that $Y$ is locally compact and Hausdorff (there are various ways of defining local compactness in Hausdorff spaces, but let's say this means each point $y \\in Y$ has a local basis of compact neighborhoods).\nProve that $f$ is a closed map.\nI know that this proof cannot require much more than a basic topological argument. But there's just something that I'm missing. \nWe can start with $C \\subseteq X$ closed, and then try to show that $Y \\setminus F(C)$ is open (for each $q \\in Y \\setminus F(C)$, we would want to find an open set $V_q$ with $q \\in V_q \\subseteq Y \\setminus F(C)$). \nHints or solutions are greatly appreciated.", "output": "Let $C \\subset X$ be closed. Let $y \\in Y - f(C)$. Since $Y$ is locally compact, $y$ has a neighborhood $V$ with compact closure. Since $f$ is proper, $f^{-1}(\\overline{V})$ is compact in $X$. Let $E = C \\cap f^{-1}(\\overline{V})$. $E$ is compact; thus, $f(E)$ is compact. Since $Y$ is Hausdorff, $f(E)$ is closed. Let $\\hat V = V - f(E)$. $\\hat V$ is a neighborhood of $y$ disjoint from $f(C)$ as desired.", "meta": {"post_id": 501510, "input_score": 38, "output_score": 46, "post_title": "Show that a proper continuous map from $X$ to locally compact $Y$ is closed"}}
{"input": "When I study Topology, I met with a problem. On my book, it says 'we cannot admit that there exists a set whose members are all the topological spaces. That will lead to a logical contradiction, that there will be a set who is a member of itself.' But why we cannot have a set who is a member of itself?", "output": "We cannot admit that there exists a set whose members are all the\n  topological spaces. That will lead to a logical contradiction, that\n  there will be a set who is a member of itself.\n\nThis is not quite untrue, but at the very least, its a deceptive statement. Sure, in the usual formulation of set theory (namely, ZFC), no set can be a member of itself, but this is basically because (to oversimplify a little) we declare via axiom that \"there are no infinite descending membership chains\" (see also, axiom of regularity). This means that, in particular, if there was a set $x$ such that $x \\in x$, then we'd have\n$$\\cdots x \\in x \\in x$$\nwhich is an infinite decreasing membership chain. So, that would be a contradiction. The point, though, is that there are perfectly good variants of ZFC in which there exist sets $x$ satisfying $x \\in x$, see also non-well-founded set theory.\nIndeed, topology can be developed pretty much independently of whether or not well-founded sets exist. Therefore, that quote is not an example of good mathematical writing.\nHowever, the author is correct that there cannot be a set of all topological spaces. Now you may say: \n\nWait, I can define the notion of a topological space. Its a pair\n  $(X,\\tau)$ such that [whatever]. And if I can define a concept, then the set of all instances of that concept must exist. So, the set of all topological spaces must exist.\n\nSounds convincing, right? Indeed for quite a long time, mathematicians believed the basic idea that \"if you can define a concept, then the set of all instances that concept must exist.\" It was Russell whom first realized that this principle, which is formalized by the (contradictory) axiom schema of unrestricted comprehension, is untenable. The proof goes something like this. Suppose that for any formula $\\phi(x)$ I can write down, there is a set of all $x$ satisfying $\\phi(x)$. Then, there is a set of all $x$ satisfying $x \\notin x$, call it $R$. Thus $y \\in R$ iff $y \\notin y$, for all $y$. So $R \\in R$ iff $R \\notin R$, a contradiction. Basically, this happens because we cannot consistently decide whether or not $R$ should be an element of itself. See also, Russell's Paradox.\nSo, we need some new set-existence principles, which (hopefully!) don't lead to an outright contradiction like that. The standard collection of set-existence principles is called ZFC. What you've got to understand about ZFC is, its based on the philosophy that \"the only reason a set shouldn't exist, is if its too large.\" So for example, we have an axiom schema (namely, replacement) stating that, if the domain of a (definable) function exists, then its range (aka image) must also exist. This makes sense in light of the philosophy of ZFC, because the range of a function always has cardinality less-than-or-equal-to the cardinality of its domain. So, using this principle, we can always use a set $X$ to prove the existence of a lot of smaller sets (as well as different sets of equal size). \nReturning to the issue of topological spaces, it turns out that ZFC cannot prove the existence of a set of all topological spaces. Indeed, ZFC can prove that there is no set of all topological spaces, because such a set would be \"too big.\" The problem with a set thats too big is that we can use the aforementioned principle to prove the existence of a whole slew of smaller sets, some of which will be paradoxical, like the aforementioned $\\{x : x \\notin x\\}$. So, this is the usual approach to (hopefully) avoiding the paradoxes, and it requires that big collections, like the class of all topological spaces, cannot exist. For a similar reason, there is no \"set of all things,\" nor a \"set of all sets,\" nor anything like that.\nOf course, there are other set theories where the \"set of all sets\" actually does exist. This is true in NFU, for example; and, if I'm not mistaken, it is also the case that the set of all topological spaces exists according to NFU. However, these sorts of set theories tend to have their own issues, which is why most people tend to prefer relatively \"tame\" ZFC (and its extensions) over the relatively \"crazy\" set theories like NFU.\nIn conclusion, its true that there is no set of all topological spaces, at least in \"tame\" theories like ZFC. However, the fundamental reason for this is issues of size, not issues of self-membership.", "meta": {"post_id": 502259, "input_score": 25, "output_score": 35, "post_title": "Why cannot a set be its own element?"}}
{"input": "What is the integral of\n\n$$\\int_{-\\infty}^{\\infty}e^{-x^2/2}dx\\,?$$\n\nMy working is here:\n= $-e^(-1/2x^2)/x$ from negative infinity to infinity.\nWhat is the value of this? Not sure how to carry on from here.  Thank you.", "output": "$$\\left(\\int\\limits_{-\\infty}^\\infty e^{-\\frac12x^2}dx\\right)^2=\\int\\limits_{-\\infty}^\\infty e^{-\\frac12x^2}dx\\int\\limits_{-\\infty}^\\infty e^{-\\frac12y^2}dy=\\int\\limits_{-\\infty}^\\infty\\int\\limits_{-\\infty}^\\infty e^{-\\frac12(x^2+y^2)}dxdy=$$\nChange now to polar coordinates:\n$$=\\int\\limits_0^{2\\pi}\\int\\limits_0^\\infty re^{-\\frac12r^2}drd\\theta=\\left.-2\\pi e^{-\\frac12r^2}\\right|_0^\\infty=2\\pi$$\nSo your integral equals $\\;\\sqrt{2\\pi}\\;$", "meta": {"post_id": 502313, "input_score": 7, "output_score": 34, "post_title": "What is the integral of $e^{-x^2/2}$ over $\\mathbb{R}$"}}
{"input": "Can you please help me by giving an example of a stochastic process that is Martingale but not Markov process for discrete case?", "output": "Markov chains have a finite memory, Martingales can have an infinite one.\nPick a random value for $X_0$. Let the sequence of random variables $\\{\\epsilon_n,\\,n>0\\}$ be i.i.d. with mean$=E[\\epsilon_{n}]=0$ and independent of $X_0$.\nThe process governed by $X_{n+1}=X_n+\\epsilon_{n+1}X_0$\n\nis a martingale as \n\\begin{align}\n&E[X_{n+1}|X_0,\\dots,X_n]\n=E[X_{n}|X_0,\\dots,X_n]+E[\\epsilon_{n+1}X_0|X_0,\\dots,X_n]\\\\\n&=X_n+E[\\epsilon_{n+1}|X_0,\\dots,X_n]E[X_0|X_0,\\dots,X_n]\\\\\n&= X_n+E[\\epsilon_{n+1}]X_0\\\\\n&= X_n+0X_0\\\\&=X_n\n\\end{align}\nHere, it is key that $\\epsilon_n$ is independent of the $\\{X_i, 0\\leq i\\leq n\\}$.\nis not Markov as it is clear that $Pr[X_n+\\epsilon_{n+1}X_0|X_n]\\neq Pr[X_n+\\epsilon_{n+1}X_0|X_0,\\dots,X_n]$.\nTo determine $X_{n+1}$ not only the value of $X_n$ but the entire path to it (at which value did the path start at $X_0$) is needed.\n\nMartingale are about expectation and the Markov property about probability, which of course is also an expectation, but that's stuff for another post.\nInspitration was drawn from http://djalil.chafai.net/blog/2012/01/20/martingales-which-are-not-markov-chains/", "meta": {"post_id": 503245, "input_score": 15, "output_score": 36, "post_title": "Stochastic process that is Martingale but not Markov?"}}
{"input": "This is taken from the Car Talk puzzler of the week, seen here: http://www.cartalk.com/content/mathematic-mistake-0?question\nI'll summarize it thusly:\nA hotshot mathematician calls a press conference because he's found a counterexample to Fermat's Last Theorem (which claims that $A^x + B^x = C^x$ has no integer solutions for $A, B$ and $C$ when $x > 2$). However, just to be dramatic (and annoying), he doesn't reveal the whole counterexample, but just the values of $A, B$ and $C$, which are 91, 56 and 121, respectively. The 10-year-old child of one of the reporters attending the press conference raises his hand, and says \"Sorry, sir, but you're wrong.\"\nThe question is: How did the child know?", "output": "7 divides both 91 and 56, but not 121.", "meta": {"post_id": 504954, "input_score": 17, "output_score": 40, "post_title": "Mistaken counterexample to FLT; where's the mistake?"}}
{"input": "Let $A$ be a a square matrix. Is it possible \nto express $\\operatorname{trace}(A^2)$ by means of $\\operatorname{trace}(A)$ ? or at least\nsomething close?", "output": "In general,\n$\\text{Tr}(A^2) = (\\text{Tr}A)^2 - 2 \\sigma_2(A), \\tag{1}$\nwhere $\\text{Tr}$ denotes the trace of $A$, and $\\sigma_2(A)$ is the coefficient of $N - 2$ in the characteristic polynomial $p_A(\\lambda)$ of $A$, where $N$ is the size of $A$.  We have\n$\\sigma_2(A) = \\sum_{i <  j}\\lambda_i \\lambda_j, \\tag{2}$\nwhere $\\lambda_1, \\lambda_2, . . ., \\lambda_N \\in \\Bbb C$ are the eigenvalues of $A$.  In this formula, repeated eigenvalues are admitted but are assigned distinct indices.\nThis result may be seen as follows:  factoring $p_A(\\lambda)$, we have\n$p_A(\\lambda) = \\prod_1^N (\\lambda - \\lambda_i) = \\sum_0^N (-1)^i\\sigma_i(\\lambda_1, \\lambda_2, . . . \\lambda_N) \\lambda^{N - i}, \\tag{3}$\nwhere the $\\sigma_i(\\lambda_1, \\lambda_2, . . . \\lambda_N)$ are the so-called elementary symmetric functions/polynomials in the $\\lambda_i$.  This result is very well-known and is thoroughly discussed in this Wikipedia entry.  Inspecting (3), it is easily seen that\n$\\sigma_1(\\lambda_1, \\lambda_2, . . . \\lambda_N) = \\text{Tr}A; \\tag {4}$\nand\n$\\sigma_2(\\lambda_1, \\lambda_2, . . . \\lambda_N) =  \\sum_{i <  j}\\lambda_i \\lambda_j. \\tag{5}$\n$\\sigma_k(\\lambda_1, \\lambda_2, . . . \\lambda_N)$ is the sum of $C_k^N = \\frac{N!}{k! (N - k)!}$ terms, each being the product of precisely $k$ of the $\\lambda_i$ with distinct $i$.  It is a homogeneous polynomial of degree $k$, and is evidently invariant under any permutation of the indices of the $\\lambda_i$.  We also take\n$\\sigma_0(\\lambda_1, \\lambda_2, . . . \\lambda_N) = 1. \\tag{6}$\nAn important fact for the present purposes is that, though the $\\sigma_k(\\lambda_1, \\lambda_2, . . . \\lambda_N)$ may be expressed in terms of the $\\lambda_i$, in the case of $p_A(\\lambda)$ they may be had without explicit knowledge of the eigenvalues simply by obtaining the coefficients of $p_A(\\lambda)$ from the defining equation\n$p_A(\\lambda) = \\det(\\lambda I - A); \\tag{7}$\nthus there is no ambiguity in referring to the $\\sigma_k(A)$, as we have done above for the cases $k = 1, 2$.\nBearing these observations in mind, we recall that the eigenvalues of $A^2$ are precisely the $\\lambda_i^2$ and thus\n$(\\text{Tr}(A))^2 = (\\sum_i \\lambda_i)^2 = \\sum_i \\lambda_i^2 + 2\\sum_{i < j}\\lambda_i \\lambda_j = \\text{Tr}(A^2) + 2\\sigma_2(A); \\tag{8}$\n(1) follows by way of a minor re-arrangement of (8).  QED.\nThe full machinery of symmetric polynomials can actually be avoided by means of a simple induction whereby we may directly show that, for any polynomial $p(\\lambda)$ with complex coefficients and $\\deg p = N$, the coefficient of $\\lambda^{N - 2}$ is $\\sigma_2(\\lambda_1, \\lambda_2, . . . \\lambda_N)$.  The case $N = 2$ is easily ratified, and serves as our base case:\n$(\\lambda - \\lambda_1)(\\lambda - \\lambda_2) = \\lambda^2 - (\\lambda_1 + \\lambda_2)\\lambda + \\lambda_1 \\lambda_2; \\tag{9}$\nnow suppose that\n$\\prod_1^k (\\lambda - \\lambda_i) = \\lambda^k - (\\sum_1^k \\lambda_i)\\lambda^{k - 1} + (\\sum_{1 \\le i < j \\le k} \\lambda_i \\lambda_j) \\lambda^{k - 2} + r(\\lambda), \\tag{10}$\nwhere if $r(\\lambda) \\ne 0$ we have $\\deg r(\\lambda) \\le k - 3$.  For $\\lambda_{k + 1}$ arbitrary,\n$\\prod_1^{k + 1} (\\lambda - \\lambda_i) = (\\lambda - \\lambda_{k + 1}) (\\lambda^k - (\\sum_1^k \\lambda_i)\\lambda^{k - 1} + (\\sum_{1 \\le i < j \\le k} \\lambda_i \\lambda_j) \\lambda^{k - 2} + r(\\lambda))$\n$= \\lambda^{k + 1} - (\\sum_1^k \\lambda_i)\\lambda^k + (\\sum_{1 \\le i < j \\le k} \\lambda_i \\lambda_j) \\lambda^{k - 1} + \\lambda r(\\lambda)$\n$- \\lambda_{k + 1} \\lambda^k + (\\sum_1^k \\lambda_i \\lambda_{k + 1})\\lambda^{k - 1} - (\\sum_{1 \\le i < j \\le k} \\lambda_i \\lambda_j \\lambda_{k + 1}) \\lambda^{k - 2} - \\lambda_{k + 1}r(\\lambda)$\n$=\\lambda^{k + 1} -(\\sum_1^{k + 1} \\lambda_i) \\lambda^k +  (\\sum_{1 \\le \n i < j \\le k + 1} \\lambda_i \\lambda_j) \\lambda^{k - 1}$\n$-(\\sum_{1 \\le i < j \\le k} \\lambda_i \\lambda_j \\lambda_{k + 1}) \\lambda^{k - 2} + \\lambda r(\\lambda) - \\lambda_{k + 1} r(\\lambda). \\tag{11}$\nInspection of (11) reveals that the last three summands are all of degree $k - 2$ or less, since $\\deg r(\\lambda) \\le k - 3$; thus (11) shows that the coefficient of $\\lambda^{k - 1}$ in $\\prod_1^{k + 1} (\\lambda - \\lambda_i)$ is in fact $\\sigma_2(\\lambda_1, \\lambda_2, . . . \\lambda_{k + 1})$, and the induction is complete.  QED.\nHope this helps.  Cheerio,\nand as always,\nFiat Lux!!!", "meta": {"post_id": 506962, "input_score": 19, "output_score": 35, "post_title": "Expressing the trace of $A^2$ by trace of $A$"}}
{"input": "The wikipedia tells that it is not known that $\\pi+e$ is irrational? \nImmediately after reading this my mind came with this proof- \nLet $x =\\sqrt{\\pi^2}+\\sqrt{e^2}$  be rational, then\n$  \\quad (x-\\sqrt{\\pi^2})^2=\\sqrt{e^2}$ is rational , now\n$\\quad x^2-2x\\sqrt{\\pi^2}-\\pi^2=e^2$ is rational , now \n$\\quad \\frac{e^2-x^2-\\pi^2}{-2x} = \\sqrt{\\pi^2}=\\pi$ is rational.\nThis is a contradiction as $\\pi$ is irrational! Thus we prove by contradiction that $\\pi+e $ is irrational.\n Please tell me me if this is a valid proof? And also tell my mistake if I am wrong somewhere?", "output": "It's not clear how you can deduce the first step of your proof.\nLet $a=\\sqrt[4]{2}$ and $b=2-\\sqrt[4]{2}$. Then from:\n$$2=a+b$$\ncan we conclude that:\n$$(2-b)^2=a^2=\\sqrt{2}$$ is rational?\nIt's certainly the case that $(2-b)^2=a^2$. But why is it rational?", "meta": {"post_id": 507032, "input_score": 1, "output_score": 49, "post_title": "Proof of $\\pi+e$ irrational"}}
{"input": "I always see this word $\\mathcal{F}$-measurable, but really don't understand the meaning. I am not able to visualize the meaning of it.\nNeed some guidance on this.\nDon't really understand $\\sigma(Y)$-measurable as well. What is the difference?", "output": "Let $(\\Omega,\\mathcal{F},P)$ be a probability space, i.e. $\\Omega$ is a non-empty set, $\\mathcal{F}$ is a sigma-algebra of subsets of $\\Omega$ and $P:\\mathcal{F}\\to [0,1]$ is a probability measure on $\\mathcal{F}$. Now, suppose we have a function $X:\\Omega\\to\\mathbb{R}$ and we want to \"measure\" the probability of $X$ belonging to some subset of $\\mathbb{R}$. That is, we want to assign the probability to sets of the form $$\\{X\\in A\\}:=X^{-1}(A)=\\{\\omega\\in\\Omega\\mid X(\\omega)\\in A\\}$$ for Borel sets $A\\in\\mathcal{B}(\\mathbb{R})$. For this to make sense, we need to make sure that $\\{X\\in A\\}\\in\\mathcal{F}$ for all $A\\in\\mathcal{B}(\\mathbb{R})$, otherwise we can't assign a probability to it (recall that $P$ is only defined on $\\mathcal{F}$). \nWhenever $X:\\Omega\\to\\mathbb{R}$ satisfies that $X^{-1}(A)\\in\\mathcal{F}$ for all $A\\in\\mathcal{B}(\\mathbb{R})$ we say that $X$ is $(\\mathcal{F},\\mathcal{B}(\\mathbb{R}))$-measurable or just $\\mathcal{F}$-measurable when there is no chance of confusion. Thus, for a random variable $X$, it makes sense to assign the probability to any set of the form $\\{X\\in A\\}$, and this defines the distribution of $X$:\n$$\nP_X(A):=P(\\{X\\in A\\}),\\quad A\\in\\mathcal{B}(\\mathbb{R}).\n$$ \nNote that a random variable is a synonym for an $\\mathcal{F}$-measurable function.\nIf $Y:\\Omega\\to\\mathbb{R}$ is a random variable, then $\\sigma(Y)$ is, by definition, given as\n$$\n\\sigma(Y)=\\sigma(\\{Y^{-1}(A)\\mid\\ A\\in\\mathcal{B}(\\mathbb{R})\\}),\n$$\ni.e. the smallest sigma-algebra containing all sets of the form $Y^{-1}(A)$. Another way of characterizing $\\sigma(Y)$ is by saying that it is the smallest sigma-algebra we can put on $\\Omega$ that makes $Y$ measurable.", "meta": {"post_id": 508790, "input_score": 13, "output_score": 35, "post_title": "What does it mean by $\\mathcal{F}$-measurable?"}}
{"input": "Proof.  Let $\\sum_{k = 0}^N c_k \\rightarrow s$, let $\\sigma_N = (S_0 + \\dots + S_{N-1})/N$ be the $Nth$ Cesaro sum where $S_K$ is the $Kth$ partial sum of the series.  Then $s - \\sigma_N \\\\= s - c_0 - c_1(N-1)/N + c_2(N-2)N +\\dots+c_{N-1}/N \\\\ =c_1/N + c_2 2/N + \\dots + c_{N-1}(N-1)/N + c_N + \\dots$\nWhere do I go from here?", "output": "You don't need the fact that it's a series, which is maybe why this is confusing for you. \nSuppose $S_n \\to S$ is a converging sequence. Then $\\frac 1n \\sum_{k=1}^n S_k \\to S$ also. Roughly speaking, if you take $n$ large enough, then all the big terms (big index, not big in value) are close to $S$ ; all the small terms (small index) will get killed when $n$ goes to infinity. \nNon-roughly speaking, \n$$\n\\left| \\left( \\frac 1n \\sum_{k=1}^n S_k \\right) - S \\right| = \\frac 1n \\left| \\sum_{k=1}^n (S_k - S) \\right| \\le \\frac 1n \\sum_{k=1}^n |S_k - S| = \\frac {\\sum_{k=1}^{\\ell} |S_k - S|}{n} + \\frac {\\sum_{k=\\ell+1}^n |S_k - S|}{n}\n$$\nLet $\\varepsilon > 0$, and choose $\\ell$ such that for all $k > \\ell$, $|S_k - S| < \\varepsilon/2$ by convergence of $S_k$ to $S$. Now that $\\ell$ is fixed, choose $N$ large enough so that for all $n > N$, \n$$\n\\frac{\\sum_{k=1}^{\\ell} |S_k - S|}{n} < \\varepsilon / 2.\n$$\n(Note that the numerator does not depend on $n$ so we still have freedom.) It follows that for all $n > N$,\n$$\n\\frac {\\sum_{k=1}^{\\ell} |S_k - S|}{n} + \\frac {\\sum_{k=\\ell+1}^n |S_k - S|}{n} \\le \\frac {\\sum_{k=1}^{\\ell} |S_k - S|}{n} + \\frac{(n-\\ell) (\\varepsilon/2)}n  \\le \\varepsilon.\n$$\nFor your particular problem, put $S_n = \\sum_{k=0}^n c_k$. \nHope that helps,", "meta": {"post_id": 514802, "input_score": 18, "output_score": 48, "post_title": "Convergence of series implies convergence of Cesaro Mean."}}
{"input": "By definition, we have\n$$\n\\|V\\|_p\n:= \\sqrt[p]{\\displaystyle \\sum_{i=1}^{n}|v_i|^p}\n\\qquad \\text{and} \\qquad\n\\|A\\|_p\n:= \\sup_{x\\not=0}\\frac{||Ax||_p}{||x||_p}\n$$\nand if $A$ is finite, we change sup to max.\n\nHowever I don't really get how we get to the definition of $||A||_1$ as the maximum absolute column sum of the matrix as stated in Wikipedia\n\nFor example, assume $A=\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{bmatrix}$.\nThen\n$$\n||A||_1\n= \\max_{x\\not=0} \\frac{\\left\\|\\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22}\\end{bmatrix}\\cdot \\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\right\\| }{\\left\\|\\begin{bmatrix} x_{1} \\\\ x_{2}\\end{bmatrix}\\right\\|}\n= \\max_{x\\not=0} \\frac{|a_{11}x_1+a_{12}x_2|+|a_{21}x_1+a_{22}x_2|}{|x_1|+|x_2|}\n$$\nThat's what I have gotten so far, but I don't really see how this is related to the max of the column sum. Can anyone help me explain this?", "output": "Let's denote the columns of $A$ by $A_1,\\, \\dotsc,\\, A_n$. Then for every $x \\in \\mathbb{R}^n$, we have\n$$\\begin{align}\n\\lVert Ax \\rVert_1 &= \\left\\lVert\\sum_{\\nu=1}^n x_\\nu\\cdot A_\\nu \\right\\rVert_1\\\\\n&\\leqslant \\sum_{\\nu=1}^n \\lVert x_\\nu\\cdot A_\\nu\\rVert_1\\\\\n&= \\sum_{\\nu=1}^n \\lvert x_\\nu\\rvert\\cdot\\lVert A_\\nu\\rVert_1\\\\\n&\\leqslant \\max \\left\\{\\lVert A_\\nu\\rVert_1 : 1 \\leqslant \\nu \\leqslant n\\right\\} \\left(\\sum_{\\nu=1}^n \\lvert x_\\nu\\rvert\\right)\\\\\n&= \\max \\left\\{\\lVert A_\\nu\\rVert_1 : 1 \\leqslant \\nu \\leqslant n\\right\\}\\cdot \\lVert x\\rVert_1.\n\\end{align}$$\nThat shows that\n$$\\lVert A\\rVert_1 \\leqslant \\max \\left\\{\\lVert A_\\nu\\rVert_1 : 1 \\leqslant \\nu \\leqslant n\\right\\},$$\nand choosing $x = e_m$, where $m$ is the index where the absolute column sum has its maximum shows the converse inequality, hence equality.", "meta": {"post_id": 519279, "input_score": 31, "output_score": 48, "post_title": "Why is the matrix norm $||A||_1$ maximum absolute column sum of the matrix?"}}
{"input": "Consider the following expression:\n(a - b) mod N\n\nWhich of the following is equivalent to the above expression?\n1) ((a mod N) + (-b mod N)) mod N\n\n2) ((a mod N) - (b mod N)) mod N\n\nAlso, how is (-b mod N) calculated, i.e., how is the mod of a negative number calculated?\nThanks.", "output": "Other answers have addressed the immediate question, so I'd like to address a philosophical one.\nI think that the way you're thinking of \"mod\" is a bit misleading. You seem to be thinking of \"mod\" as an operator: so that \"13 mod 8\" is another way to write the number \"5\". This is the way that modulo operators often work in programming languages: in Python you can write \"13 % 8\" and get back the number 5.\nMathematically, though, I think it is better to think of \"mod 8\" as an adverb modifying \"=\": when we say \"5 = 13 (mod 8)\" we are really saying \"5 is equal to 13, if you think of equality as working modulo 8\". When you think of \"mod\" this way, it doesn't really make sense to ask about the expression \"((a mod N) + (-b mod N)) mod N\": it's not even really an expression, under this interpretation.\nI'm not trying to say that you are wrong for thinking of \"mod\" as an operation, because the operation of \"taking a residue mod $m$\" is a useful operation. However, I think it is also useful to keep the other meaning of \"mod\" in mind.\n(After writing this answer I see that the question was posted more than a year ago. Well, maybe someone else will find this helpful.)", "meta": {"post_id": 519845, "input_score": 74, "output_score": 77, "post_title": "How to calculate $\\,(a-b)\\bmod n\\,$ and $ {-}b \\bmod n$"}}
{"input": "Definition: The c.d.f. $F$ of a random variable $X$ is a function defined for each real number $x$ as follows:$$F(x)=\\Pr(X\\leq x) \\text{ for } -\\infty<x<\\infty$$\n\nLet $$F(x^-)=\\lim_{y\\rightarrow x,\\,y<x}F(y)$$ and $$F(x^+)=\\lim_{y\\rightarrow x,\\,y>x}F(y)$$\n\n\nProperty of cumulative distribution function: A c.d.f. is always continuous from the right; that is , $F(x)=F(x^+)$ at every point $x$.\n\n\n\nProof: Let $y_1>y_2>\\dots$ be a sequence of numbers that are decreasing such that $$\\lim_{n\\rightarrow \\infty}y_n=x.$$Then the event $\\{X\\leq x\\}$ is the intersection of all the events $\\{X\\leq y_n\\}$ for $n=1,2,\\dots$ .Hence, $$F(x)=\\Pr(X\\leq x)=\\lim_{n\\rightarrow \\infty} \\Pr(X\\leq y_n)=F(x^+).$$\n\n\nNow I think the left inequality can also be proved in the similar way as:\nLet $y_1<y_2<\\dots$ be a sequence of numbers that are increasing such that $$\\lim_{n\\rightarrow \\infty}y_n=x.$$Then the event $\\{X\\leq x\\}$ is the union of all the events $\\{X\\leq y_n\\}$ for $n=1,2,\\dots$ .Hence, $$F(x)=\\Pr(X\\leq x)=\\lim_{n\\rightarrow \\infty}\\Pr(X\\leq y_n)=F(x^-).$$\nWhere am I wrong?", "output": "You write:\n\nThen the event $\\{X\\leq x\\}$ is the union of all the events $\\{X\\leq y_n\\}$ for $n=1,2,\\dots$.\n\nThis is the faulty step. To wit:\n\nIf $y_n\\lt x$ for every $n$ and $y_n\\to x$, then $\\bigcup\\limits_n\\{X\\leqslant y_n\\}$ is equal to $\\{X\\lt x\\}$, not to $\\{X\\leqslant x\\}$. \n\nYou might want to check that $x$ is in $(-\\infty,y_n]$ for no $n$ whatsoever hence $x$ is not in $\\bigcup\\limits_n(-\\infty,y_n]$, and in fact $\\bigcup\\limits_n(-\\infty,y_n]$ is equal to $(-\\infty,x)$, not to $(-\\infty,x]$.", "meta": {"post_id": 522270, "input_score": 20, "output_score": 34, "post_title": "Why left continuity does not hold in general for cumulative distribution functions?"}}
{"input": "Is there a closed form (possibly, using known special functions) for the Fourier transform of the function $f(x)=\\left|\\frac{\\sin x}{x}\\right|$?\n$\\hspace{.7in}$\nI tried to find one using Mathematica, but it ran for several hours without producing any result.", "output": "Since $\\left|\\frac{\\sin(x)}{x}\\right|$ is not in $L^1$, there is no Fourier transform in the strict sense. However, we can get a Fourier transform in the sense of distributions (via Plancherel's Theorem).\nThe standard result about the sinc function is that\n$$\n\\int_{-\\infty}^\\infty\\frac{\\sin(ax)}{ax}e^{-2\\pi ix\\xi}\\,\\mathrm{d}x\n=\\frac\\pi{a}\\left[|\\xi|\\le\\frac{a}{2\\pi}\\right]\\tag{1}\n$$\nwhere $[\\,\\cdot\\,]$ are Iverson brackets.\n$$\n\\int_{-\\infty}^\\infty\\frac{\\sin(2(k+1)\\pi^2x)-\\sin(2k\\pi^2x)}{\\pi x}e^{-2\\pi ix\\xi}\\,\\mathrm{d}x\n=\\Big[k\\pi\\le|\\xi|\\le (k+1)\\pi\\Big]\\tag{2}\n$$\nNote that\n$$\n\\mathrm{sgn}\\left(\\frac{\\sin(x)}{x}\\right)=\\sum_{k=0}^\\infty(-1)^k\\Big[k\\pi\\le|\\xi|\\le (k+1)\\pi\\Big]\\tag{3}\n$$\nCombining $(2)$ and $(3)$ yields the Fourier transform of $\\mathrm{sgn}\\left(\\frac{\\sin(x)}{x}\\right)$ in the sense of distributions\n$$\n\\begin{align}\n&\\sum_{k=0}^\\infty(-1)^k\\frac{\\sin(2(k+1)\\pi^2x)-\\sin(2k\\pi^2x)}{\\pi x}\\\\\n&=\\frac2{\\pi x}\\sum_{k=0}^\\infty(-1)^k\\sin(2(k+1)\\pi^2x)\\\\\n&=\\frac{\\tan(\\pi^2x)}{\\pi x}\\tag{4}\n\\end{align}\n$$\nSince the Fourier transform of a product is the convolution of the Fourier transforms, the Fourier transform of $\\left|\\frac{\\sin(x)}{x}\\right|$ is the convolution\n$$\n\\left[|\\xi|\\le\\frac1{2\\pi}\\right]\\ast\\frac{\\tan(\\pi^2\\xi)}{\\xi}\n=\\mathrm{PV}\\int_{\\xi-\\frac1{2\\pi}}^{\\xi+\\frac1{2\\pi}}\\frac{\\tan(\\pi^2t)}{t}\\,\\mathrm{d}t\\tag{5}\n$$\nusing the Cauchy Principal Value in $(5)$.\n\nPlots of the Fourier Transform:\n$\\hspace{8mm}$\n$\\hspace{5mm}$", "meta": {"post_id": 523886, "input_score": 22, "output_score": 35, "post_title": "Fourier transform of $\\left|\\frac{\\sin x}{x}\\right|$"}}
{"input": "A friend of mine sent me an integral that she had not been able to crack, and me neither. It comes with a result, but without a proof (I suppose it originated in some math contest). Could you please suggest an approach to prove the result?\n$$\\int_0^1\\frac{x^9\\left(x^4+x^2-x-1-5\\ln x\\right)}{\\left(x^{10}-1\\right)\\ln x}\\mathrm dx=\\frac12\\gamma+\\frac{11}5\\ln2-\\frac54\\ln5+\\frac12\\ln\\pi-\\frac12\\ln\\phi,$$\nwhere $\\gamma$ is the Euler\u2013Mascheroni constant, and $\\phi$ is the golden ratio.", "output": "Let's denote the integral in question as\n$$I=\\int_0^1\\frac{x^9\\left(x^4+x^2-x-1-5\\ln x\\right)}{\\left(x^{10}-1\\right)\\ln x}dx.\\tag1$$\nChanging the variable $x=y^{1/10}$ and renaming $y$ back to $x$ we get\n$$I=\\int_0^1\\frac{x^{2/5}+x^{1/5}-x^{1/10}-1-\\ln\\sqrt x}{(x-1)\\ln x}dx.\\tag2$$\nSome elementary transformations show that\n$$I=\\mathcal{J}(2/5)+\\mathcal{J}(1/5)-\\mathcal{J}(1/10),\\tag3$$\nwhere we introduced notation\n$$\\mathcal{J}(q)=\\int_0^1\\frac{x^q-1-q\\,\\ln x}{(x-1)\\ln x}dx.\\tag4$$\nThe integral $\\mathcal{J}(q)$ can be evaluated as follows:\n$$\\begin{align}\\mathcal{J}(q)=\\int_0^1\\int_0^q\\frac{x^p-1}{x-1}dp\\,dx=\\int_0^q\\underbrace{\\int_0^1\\frac{x^p-1}{x-1}dx}_{\\text{DLMF 5.9.16}}\\,dp\\\\=\\int_0^q H_p\\,dp=q\\cdot\\gamma+\\ln\\Gamma(q+1),\\end{align}\\tag5$$\nwhere $H_p$ are harmonic numbers: $H_p$$\\,=\\,$$\\gamma$$\\,+\\,$$\\psi_0$$(p+1)$, and $\\psi_0$ is the digamma function: $\\psi_0(x)=\\frac{d}{dx}\\ln\\,$$\\Gamma$$(x)$. Let me mention that the formula DLMF 5.9.16 becomes particularly obvious for positive integer $p$, when $H_p=\\sum_{n=1}^pn^{-1}$.\nPluging $(5)$ back into $(3)$, we get\n$$I=\\frac12\\gamma+\\ln\\frac45+\\ln\\frac{\\Gamma\\left(\\frac15\\right)\\Gamma\\left(\\frac25\\right)}{\\Gamma\\left(\\frac1{10}\\right)}.\\tag6$$\nFrom the formula $(74)$ on this MathWorld page we know that\n$$\\frac{\\Gamma\\left(\\frac15\\right)\\Gamma\\left(\\frac25\\right)}{\\Gamma\\left(\\frac1{10}\\right)}=\\frac{\\sqrt[5]2\\,\\sqrt\\pi}{\\sqrt[4]5\\,\\sqrt\\phi}.\\tag7$$\n(see the paper Raimundas Vid\u016bnas, Expressions for values of the gamma function for a proof).\nMaking use of this formula, we get the final result\n$$I=\\frac12\\gamma+\\frac{11}5\\ln2-\\frac54\\ln5+\\frac12\\ln\\pi-\\frac12\\ln\\phi.\\tag8$$", "meta": {"post_id": 525384, "input_score": 41, "output_score": 42, "post_title": "Integral $\\int_0^1\\frac{x^9\\left(x^4+x^2-x-1-5\\ln x\\right)}{\\left(x^{10}-1\\right)\\ln x}\\mathrm dx$"}}
{"input": "I am an 3rd year undergrad interested in mathematics and theoretical physics. I have been reading some classical differential geometry books and I want to pursue this subject further. I have three questions:\n1) What are the current research topics in differential geometry? How is scope in those areas?\n2) How should I go about  to pursue research in this area? I mean to say what background I should have if I want to start research in this area? How I should go about it i.e. what books  I should read and what are some important concepts?\n3) As now I am reading classical differential geometry(Do' Carmo's book), how much time it will take me to start my research?", "output": "Here is a partial answer based on my comment: \nAt the very least, you should read the other do Carmo's book: \"Riemannian Geometry\". I do not know though if you have enough background to handle it. The book you are reading is basically about mathematics of 18th and 19th century. \n(It is good that you are reading it, keep doing so, but it prepares you for the current research about as much as reading about classical mechanics and electromagnetism prepares you for research in, say, string theory.) \nIn contrast, the \"R.G.\" is about math of 20th century, until mid 1960s. Without knowing at least the first 4 chapters of it, you will be facing basic linguistical problems understanding what the modern Differential Geometry is about. I can write about current research in geometric flows or in minimal surfaces, or in K\u00e4hler-Einstein metrics, but it would be mostly useless at this point. \nTo do serious research in modern differential geometry you also need strong background in:\n\nAlgebraic topology (say, to the extent covered in Hatcher's \"Algebraic Topology\" book). \nFunctional Analysis, Sobolev spaces, etc.\nLinear and Nonlinear PDEs (primarily elliptic and parabolic), at least if you will be doing geometric analysis. See for instance D. Gilbarg, N. Trudinger, \"Elliptic Partial Differential Equations of Second Order\".\nPossibly other fields, depending on the differential geometry subarea: Complex analysis, algebraic geometry, geometric topology, measure theory... \n\nHow long would it take you to get the right background (determined by your future PhD advisor) to start research, is impossible to tell, it depends on too many factors.", "meta": {"post_id": 531932, "input_score": 35, "output_score": 39, "post_title": "Research in differential geometry"}}
{"input": "From Wikipedia:\n\nAn event in George Dantzig's life became the origin of a famous\n  story in 1939 while he was a graduate student at UC Berkeley. Near the\n  beginning of a class for which Dantzig was late, professor Jerzy\n  Neyman wrote two examples of famously unsolved statistics problems on\n  the blackboard. When Dantzig arrived, he assumed that the two problems\n  were a homework assignment and wrote them down. According to Dantzig,\n  the problems \"seemed to be a little harder than usual\", but a few days\n  later he handed in completed solutions for the two problems, still\n  believing that they were an assignment that was overdue.\n\nWhat were the two unsolved problems which Dantzig had solved?", "output": "I'm a few years late to the party, but in fact the problem in the first, solo paper is easy to state with only elementary background, and the arguments in it are entirely reasonable for a talented young grad student to come up with. I have not taken the time to read the second paper. This topic comes up from time to time with interest from a very broad array of people, and nobody seems to have written a straightforward description of either problem, so I'll provide such a description for the first one.\nFor those with some background: Dantzig showed that in the situation of Student's t-test, the only way to get a hypothesis test whose power for any given alternative is independent of the standard deviation is to use a silly test which always has an equal probability of rejecting or failing to reject, which is obviously not useful.\nIn an unusual amount of detail, aimed at those with no statistical knowledge:\nLots of data is approximately normally distributed (\"bell-shaped\"), like IQ scores, birthweights, or people's height. The classical Central Limit Theorem gives one explanation for this phenomenon: complicated traits like birthweight can often be thought of as the result of adding up a large number of competing effects, like the presence or absence of specific genes. It is a statistical fact that under very general hypotheses, adding up many such effects tends to result in a normal distribution. For such data, you'll \"usually\" get the average value, and with enough observations, you can predict with high accuracy just how likely it is to get a certain amount above or below that average.\nA century ago, William Gosset was Head Experimental Brewer at Guinness. He came up against something like the following problem. Certain strains of barley have approximately normally distributed yields. Using only a few data points, how could he tell which type of barley is better, and more importantly, how could he quantify his certainty that his conclusion wasn't simply due to random chance?\nA little more formally, say our current strain of barley has an average yield of 100 units, and we're only interested in switching to the new strain if its yield is at least 105 units. So, we have two specific hypotheses:\n\n(\"Null hypothesis.\") The new strain's average yield is 100 units.\n(\"Alternative hypothesis.\") The new strain's average yield is 105 units.\n\nAt the end of the day, we're going to need to pick one strain of barley or the other. There are hence four probabilities of interest:\n\nIn a world where the new strain's average yield is actually 100 units...\n\n\nA. ...the probability that we correctly keep using the old strain.\nB. ...the probability that we mistakenly switch to the new strain.\n\nIn a world where the new strain's average yield is actually 105 units...\n\n\nC. ...the probability that we mistakenly keep using the old strain.\nD. ...the probability that we correctly switch to the new strain.\n\n\nWe want to somehow minimize the probability of the two types of mistakes, B and C, but doing so requires a trade-off.\nGosset developed a clever test where you can specify in advance the probability of making mistake B--often it's set at 5%. This is called the significance level of the test. Gosset published the it under the pseudonym \"Student\", and it is now called Student's t-test. One excellent thing about his procedure is that you don't need to know in advance how variable the yield actually is in the sense that the probability of mistake B is always your pre-set value.\nIf you use his procedure, you can also compute the probability of making mistake C. The power of the test is probability D (namely 1-C), which is thought of as the ability of the test to correctly tell us to switch to the new method. Unlike the significance level, the power of Gosset's procedure does depend on the true variability of the yield.\nThis dependence makes some intuitive sense, too. Suppose the new strain does have an average yield of 105 units. If that yield had almost no variation, you would expect it to be much easier to correctly switch to the new strain than if the yield had enormous variation which \"muddies your data\". Of course, expecting something and proving it are two different things! As mentioned above, in the world where the average yield is 100 units, the error probability of Student's t-test is independent of the variation of the yield, so there is certainly something interesting going on.\nHere's where Dantzig came in. We could ask if there is any test whatsoever which has the property that, for every fixed alternative, the power does not depend on the true variability of the yield. Dantzig showed that, while such tests technically exist, they are uninteresting in that probabilities A, B, C, and D are all 50%.\nClosing remarks:\nFinally, I wanted to comment on the tendency towards hyperbole. In Dantzig's 1986 College Mathematics Journal interview, Dantzig is quoted as calling the problems \"two famous unsolved problems in statistics\". In Dantzig's obituary (repeated on Wikipedia currently), this turned into \"two of the most famous unsolved problems in statistics\". While this is not my field and I am not old, I'm extremely dubious about the \"most famous\" claim. For instance, there seems to have been no rush to publish the second solution (it waited for Dantzig's thesis and an accident of someone else solving it). MathSciNet has only 5 citations for the first paper, three historical, and 7 citations for the second, again three historical. These are not the citation counts I would expect from solutions to a field's \"most famous unsolved problems\", even accounting for recent citation bias.\nThese exaggerations are frankly not necessary. Dantzig's reputation is enormous already, and the true story of a talented young grad student cleverly finding a few pages of brilliant argument that had eluded his teacher---something he never would have looked for if he knew that what he was working on was unsolved---is enough.", "meta": {"post_id": 533146, "input_score": 70, "output_score": 45, "post_title": "Dantzig's unsolved homework problems"}}
{"input": "A student asked me the following today :\n\nIs $S:= \\{\\tan(x) : x\\in \\mathbb{Q}\\}$ a group under addition?\n\nI am quite perplexed by it. Clearly, the only non-trivial part is to check\n\nFor any $x, y\\in \\mathbb{Q}$, does there exist $z \\in \\mathbb{Q}$ such that\n  $$\n\\tan(z) = \\tan(x) + \\tan(y)\n$$\n\nA couple of things I have learnt from various sources that appear to be relevant are :\n\n(Source) If $x \\in \\mathbb{Q}\\setminus\\{0\\}$, then $\\tan(x) \\in \\mathbb{Q}^c$.\n(Source) $\\arctan(\\alpha)$ is a rational multiple of $\\pi$ iff $\\exists n \\in \\mathbb{Z}$ such that $(1+i\\alpha)^n \\in \\mathbb{R}$\n\nMy guess is that it is not a group, and my initial goal was to find two $x,y \\in \\mathbb{Q}$ such that $\\tan(x) + \\tan(y) \\in \\mathbb{Q}$, but that seems to be going nowhere. \nPerhaps I am missing something obvious, but it doesn't seem to strike me. Does anyone have a suggestion on how to tackle this? Thanks for your help.", "output": "It follows rather easily from this fact: for every $0\\neq n\\in\\mathbb N$ the number $e^{i/n}$ is transcendental. If we suppose it: if $\\tan y=2\\tan x$ then $(e^{2iy}-1)(e^{2ix}+1)=2(e^{2ix}-1)(e^{2iy}+1)$. If $x\\neq0$ and $y$ are rational and $n$ is the least common denominator of $x$ and $y$, this becomes a polynomial equation for $e^{i/n}$, hence we get a contradiction.\n[the used result is a special case of the Lindemann-Weierstrass theorem which says, in particular, that $e^\\alpha$ is transcendental for every algebraic $\\alpha\\neq0$. There is probably a simpler proof in this case.]", "meta": {"post_id": 533171, "input_score": 48, "output_score": 38, "post_title": "Is $\\{\\tan(x) : x\\in \\mathbb{Q}\\}$ a group under addition?"}}
{"input": "Let\n$$\\alpha=\\sqrt{6}\\ \\sqrt{12+7\\,\\sqrt3}-3\\,\\sqrt3-6\\,=\\,\\big(2+\\sqrt{3}\\big) \\big(\\sqrt{2} \\sqrt[4]{27}-3\\big)\\,=\\,\\frac{3\\sqrt{3}}{3+\\sqrt2\\ \\sqrt[4]{27}}.\\tag1$$\nNote that $\\alpha$ is the unique positive root of the polynomial equation\n$$\\alpha^4+24\\,\\alpha^3+18\\,\\alpha^2-27=0.\\tag2$$\nNow consider the following integral:\n$$I=\\int_0^1\\frac{dx}{\\sqrt[3]x\\ \\sqrt[6]{1-x}\\ \\sqrt{1-x\\,\\alpha^2}}.\\tag3$$\nI have a conjectured elementary value for it\n$$I\\stackrel?=\\frac\\pi9\\Big(3+\\sqrt2\\ \\sqrt[4]{27}\\Big)=\\color{blue}{\\frac{\\pi}{\\sqrt{3}\\,\\alpha}}.\\tag4$$\n\nActually, the integral $I$ can be evaluated in an exact form using Mathematica or manually, using formula DLMF 15.6.1:\n$$I=\\frac{4\\,\\sqrt\\pi}{\\sqrt3}\\cdot\\frac{\\Gamma\\left(\\frac56\\right)}{\\Gamma\\left(\\frac13\\right)}\\cdot{_2F_1}\\left(\\frac12,\\frac23;\\ \\frac32;\\ \\alpha^2\\right),\\tag5$$\nbut I could not find a way to simplify this result to $(4)$.\nSo, my conjecture can be restated in a different form:\n$${_2F_1}\\left(\\frac12,\\frac23;\\ \\frac32;\\ \\alpha^2\\right)\\stackrel?=\\frac{\\sqrt2+\\sqrt[4]3}{4\\,\\sqrt[4]{27}}\\cdot\\frac{\\Gamma\\left(\\frac13\\right)}{\\Gamma\\left(\\frac56\\right)}\\cdot\\sqrt\\pi\\tag6$$\nor\n$$\\sum_{n=0}^\\infty\\frac{\\Gamma\\left(n+\\frac23\\right)}{(2\\,n+1)\\,\\Gamma(n+1)}\\alpha^{2\\,n}\\stackrel?=\\frac{3+\\sqrt2\\,\\sqrt[4]{27}}{18}\\cdot\\frac{\\pi^{3/2}}{\\Gamma\\left(\\frac56\\right)}.\\tag7$$\n\nThe conjecture can also be given in terms of the incomplete beta function:\n$$B\\left(\\alpha^2;\\ \\frac12,\\frac13\\right)\\stackrel?=\\frac{\\sqrt\\pi}2\\cdot\\frac{\\Gamma\\left(\\frac13\\right)}{\\Gamma\\left(\\frac56\\right)}.\\tag8$$\n\n\nQuestion: Is the conjecture indeed true?\n\nNote: It holds numerically up to at least $10^4$ decimal digits.\n\nConjecture 2\nLet\n$$\n{\\small\n\\begin{multline}\n\\beta=\\frac{21}4+\\frac{9\\,\\sqrt5}4-\\frac{15}8\\sqrt{750-330\\,\\sqrt5}-\\frac{33}8\\sqrt{150-66\\,\\sqrt5} \\\\ \n+ \\frac12\\sqrt{3\\left(165+75\\,\\sqrt5-46\\,\\sqrt{750-330\\,\\sqrt5}-103\\,\\sqrt{150-66\\,\\sqrt5}\\right)}.\n\\end{multline}}\\tag9$$\nAdded later: We can simplify it to\n$$\n\\small\\beta=\\frac 3 4 \\left(7+3 \\sqrt 5-\\sqrt[4] 5 \\sqrt{66+30 \\sqrt 5}\\right)+\\frac 1 2 \\sqrt{495+225 \\sqrt 5-3 \\sqrt{6 \\big(8545+3821 \\sqrt  5\\big)}}.\\tag{$9'$}\n$$\nI conjecture that:\n$$\\int_0^1\\frac{dx}{\\sqrt[3]x\\ \\sqrt[6]{1-x}\\ \\sqrt{1-x\\,\\beta^2}}\\stackrel?=\\color{blue}{\\frac{2\\,\\pi}{5\\,\\sqrt3\\,\\beta}}.\\tag{10}$$\nI can imagine that if these conjectures are true, then there are generalizations for some other algebraic numbers.", "output": "Let $t = 1 + y^3$, we can rewrite $B\\left(\\alpha^2; \\frac12, \\frac13 \\right)$ as\n$$\\int_0^{\\alpha^2} t^{-1/2} (1-t)^{-2/3} dt \n= \\int_{-1}^{-\\sqrt[3]{1-\\alpha^2}} \\frac{3y^2dy}{\\sqrt{1+y^3} y^2}\n= 3 \\int_{-1}^{-\\sqrt[3]{1-\\alpha^2}} \\frac{dy}{\\sqrt{1+y^3}}$$\nFollowing the setup in my answer to a related question. Let\n$\\;\\displaystyle\\eta = \\frac{\\Gamma\\left(\\frac13\\right)\\Gamma\\left(\\frac16\\right)}{\\sqrt{3\\pi}}\\;$ and $\\wp(z)$ be the Weierstrass elliptic $\\wp$ function with fundamental periods $1$ and $e^{i\\pi/3}$. $\\wp(z)$ is known to satisfy an ODE of the form\n$$\\wp'(z)^2 = 4 \\wp(z)^3 - g_2 \\wp(z) - g_3\\quad\\text{ where }\\quad g_2 = 0 \\;\\text{ and }\\;g_3 = \\frac{\\eta^6}{16}$$\nIf one perform variable substitution $\\;\\displaystyle y = -\\frac{4}{\\eta^2} \\wp\\left(\\frac{iz}{\\eta}\\right)$, one has\n$$\\frac{dy}{\\sqrt{1+y^3}} = -dz\\quad\\text{ and }\\quad\n\\begin{cases}\ny\\left(\\frac{\\sqrt{3}\\eta}{3}\\right) \n= -\\frac{4}{\\eta^2}\\wp\\left(i\\frac{\\sqrt{3}}{3}\\right) =  0\\\\\n\\\\\ny\\left(\\frac{\\sqrt{3}\\eta}{2}\\right) \n= -\\frac{4}{\\eta^2}\\wp\\left(i\\frac{\\sqrt{3}}{2}\\right) = -1\n\\end{cases}$$\nUsing this, we can express conjecture $(8)$ in terms of $y(\\cdot)$ and/or $\\wp(\\cdot)$:\n$$\\begin{align}\n & B\\left(\\alpha^2; \\frac12, \\frac13 \\right) \n\\stackrel{?}{=} \\frac{\\sqrt{\\pi}}{2}\\frac{\\Gamma\\left(\\frac13\\right)}{\\Gamma\\left(\\frac56\\right)} = \\frac{\\sqrt{3}}{4}\\eta\\\\\n\\iff & 3\\left[y^{-1}(-1) - y^{-1}(-\\sqrt[3]{1-\\alpha^2})\\right] \n\\stackrel{?}{=} \\frac{\\sqrt{3}}{4}\\eta\\\\\n\\iff & y^{-1}(-\\sqrt[3]{1-\\alpha^2}) \n\\stackrel{?}{=} \\frac{5\\sqrt{3}}{12}\\eta\\\\\n\\iff & \\frac{4}{\\eta^2}\\wp\\left(i\\frac{5\\sqrt{3}}{12}\\right) \n\\stackrel{?}{=} \\sqrt[3]{1-\\alpha^2}\n\\end{align}\n$$\nLet $u_0 = i\\frac{\\sqrt{3}}{3}$, $u_{-1} = i\\frac{\\sqrt{3}}{2}$ and $u = i\\frac{5\\sqrt{3}}{12} = \\frac12(u_0 + u_{-1})$. Using the addition formula for $\\wp$ function,\nwe have\n$$\n\\wp(2u) = \\wp(u_0 + u_{-1}) \n= \\frac14\\left[\\frac{\\wp'(u_0)-\\wp'(u_{-1})}{\\wp(u_0)-\\wp(u_{-1})}\\right]^2 - \\wp(u_0) - \\wp(u_{-1})\\\\\n=\\frac14\\left[\\frac{-i\\frac{\\eta^3}{4} - 0}{0 - \\frac{\\eta^2}{4}}\\right]^2 - 0  - \\frac{\\eta^2}{4}\n= -\\frac{\\eta^2}{2}\n$$\nUsing the duplication formula of $\\wp$ function, we get\n$$\n-\\frac{\\eta^2}{2} = \\wp(2u) = \\frac14\\left(\\frac{(6\\wp(u)^2-\\frac12 g_2)^2}{4\\wp(u)^3-g_2\\wp(u)-g_3}\\right) -2\\wp(u)\n= \\frac{9\\wp(u)^4}{4\\wp(u)^3-\\frac{\\eta^4}{16}} - 2\\wp(u)\n$$\nLet $Y = \\frac{4}{\\eta^2}\\wp(u)$ and $A^2 = 1 - Y^3$, above condition is equivalent to\n$$\\begin{align}\n& Y^4 + 8 Y^3 + 8 Y - 8 = 0\\tag{*1a}\\\\\n\\iff & Y(8+Y^3) = 8(1-Y^3)\\tag{*1b}\\\\\n\\implies & (9-A^2)^3(1-A^2) = 512A^6\\tag{*1c}\\\\\n\\iff & (A^4-24A^3+18A^2-27)(A^4+24A^3+18A^2-27) = 0\\tag{*1d}\n\\end{align}$$\n\nSince $\\alpha$ is a root for one of the factors in $(*1d)$, $A = \\alpha$ satisfies $(*1d)$ and hence $(*1c)$.  \nSince $0 < \\alpha < 1$ implies $1 - \\alpha^2 > 0$, $(*1c) \\implies (*1b)$ in this particular case.\ni.e. $Y = \\sqrt[3]{1-\\alpha^2}$ satisfies $(*1b)$ and hence $(*1a)$. \nSince $u$ lies between $u_0$ and $u_{-1}$, $\\frac{4}{\\eta^2}\\wp(u) > 0$. Using\nthe fact $(*1a)$ has only one positive root, we find $\\frac{4}{\\eta^2}\\wp(u) = \\sqrt[3]{1-\\alpha^2}$. i.e. conjecture $(8)$ is true.", "meta": {"post_id": 538564, "input_score": 90, "output_score": 57, "post_title": "Conjecture $\\int_0^1\\frac{dx}{\\sqrt[3]x\\,\\sqrt[6]{1-x}\\,\\sqrt{1-x\\left(\\sqrt{6}\\sqrt{12+7\\sqrt3}-3\\sqrt3-6\\right)^2}}=\\frac\\pi9(3+\\sqrt2\\sqrt[4]{27})$"}}
{"input": "From F. Klein's books, It seems that one can find the roots of a quintic equation\n$$z^5+az^4+bz^3+cz^2+dz+e=0$$\n(where $a,b,c,d,e\\in\\Bbb C$)  by elliptic  functions.  How to get that?\nUpdate: How to transform a general higher degree five or higher equation to normal form?", "output": "To solve the general quintic using elliptic functions, one way is to reduce it to Bring-Jerrard form,\n$$x^5-x+ d = 0\\tag{1}$$\na transformation which can be done in radicals. (See this post.) To solve $(1)$, define,\n$$k = \\tan\\left(\\tfrac{1}{4}\\arcsin\\Big(\\frac{16}{25\\sqrt{5}\\,d^2}\\Big)\\right)\\tag{2}$$\n$$p = i\\frac{K(k')}{K(k)} = i\\,\\frac{\\text{EllipticK[1-m]}}{\\text{EllipticK[m]}}\\tag3$$\nwith the complete elliptic integral of the first kind $K(k)$ and elliptic parameter $m=k^2$ (with $p$ also given in Mathematica syntax above).\nMethod 1: For $j=0,1,2,3,4$, let,\n$$S_j={u}^j \\frac{\\sqrt{2}\\,\\eta(\\tau_j)\\,\\eta^2(4\\,\\tau_j)}{\\eta^3(2\\,\\tau_j)}$$\n$$\\tau_j = \\tfrac{1}{10}(p+2j)$$\n$$u=\\zeta_8 = \\exp(2 \\pi i/8)$$\n$$S_5=\\frac{\\sqrt{2}\\,\\eta(\\frac{5p}{2})\\,\\eta^2(10p)}{\\eta^3(5p)}$$\nwhere $\\eta(\\tau)$ is the Dedekind eta function, then,\n$$x = \\frac{\\pm 1}{2\\cdot 5^{3/4}}\\frac{(k^2)^{1/8}}{\\sqrt{k(1-k^2)}}(S_0+S_5)(S_1+i\\,S_4)(i\\,S_2+S_3) \\tag{4}$$\nwith the sign chosen appropriately.\n$\\color{blue}{\\text{Remark}}$: The five roots $x_n$can be found by using $p_n = i\\frac{K(k')}{K(k)}+16n$ for $n = 0,1,2,3,4$.\nMethod 2:  For $j=0,1,2,3,4$, let,\n$$T_j =\\left(\\frac{\\vartheta_2(0,w^j q^{1/5})}{\\vartheta_3(0,w^j q^{1/5})}\\right)^{1/2}$$\n$$q = \\exp(i \\pi p_0)$$\n$$w = \\zeta_5 = \\exp(2 \\pi i/5)$$\n$$T_5 =\\frac{q^{5/8}}{(q^5)^{1/8}}\\left(\\frac{\\vartheta_2(0,q^{5})}{\\vartheta_3(0, q^{5})}\\right)^{1/2}$$\nwith $\\vartheta_n(0,q)$ as the Jacobi theta functions, then,\n$$x = \\frac{\\pm{\\zeta_8}}{2\\cdot 5^{3/4}}\\frac{(k^2)^{1/8}}{\\sqrt{k(1-k^2)}}(T_0+T_5)(T_1-i\\,T_4)(T_2+T_3) \\tag{5}$$\nand the sign of $\\zeta_8$ chosen appropriately. Note that the two methods are connected via $(S_j)^8 = (T_j)^8$ as can be seen more in the Afterword.\n$\\color{blue}{\\text{Remark}}$: One can also find the other roots $x_i$, but is not as simple as in Method 1. (As one can see, you need much more than radicals to solve the general quintic.)\nExample. Let,\n$$x^5-x+1 = 0\\tag6$$\nso $d = \\pm1$. Plugging it into $(2)$ gives $k \\approx 0.072696$, and $m=k^2$, so $p \\approx 2.550572\\,i$. Since both methods use a square root, using the formulas one eventually finds,\n$$x = \\mp\\,1.1673039\\dots$$\n$\\color{blue}{\\text{Afterword}}$ (Added June 2015): Given the nome $q = \\exp(i \\pi \\tau)$, the two methods involve the function known either as the modular lambda function or elliptic lambda function, $\\lambda(\\tau)$ which has a beautiful q-continued fraction,\n$$\\big(\\lambda(\\tau)\\big)^{1/8} = \\frac{\\sqrt{2}\\,\\eta(\\tfrac{\\tau}{2})\\,\\eta^2(2\\tau)}{\\eta^3(\\tau)} = \\left(\\frac{\\vartheta_2(0,q)}{\\vartheta_3(0,q)}\\right)^{1/2} = \\cfrac{\\sqrt{2}\\,q^{1/8}}{1+\\cfrac{q}{1+q+\\cfrac{q^2}{1+q^2+\\cfrac{q^3}{1+q^3+\\ddots}}}}$$\nstudied by Ramanujan (who also had his own method to solve solvable quintics).", "meta": {"post_id": 540964, "input_score": 81, "output_score": 79, "post_title": "How to solve fifth-degree equations by elliptic functions?"}}
{"input": "Suppose $f$ and $g$ are Lebesgue measurable, we want to show $f+g$ is measurable. So, the hint is to consider the continuous functions $F : \\mathbb{R}^2 \\to \\mathbb{R} $ given by $h(x) = F(f ,g ) $. If we can show $F$ Is measurable, then Taking $F = f +g $ would solve our problem.\nIn other words, I want to show that the set $R = \\{ (f,g) : F(f,g) > a $ } is lebesgue measurable.. But this set is just a rectangle in the plane. And since $F$ is continuous, then $R$ must be open, and hence a union of open rectangles which are measurable and hence $R$ must be measurable. Is this a correct approach to the problem? Can someone help me to make this formal? thanks", "output": "This is a method I used in my analysis class. Note that $f(x) + g(x) < t$ iff $f(x) < t-g(x)$ iff there exists a rational  number $r$ such that $f(x) < r < t-g(x)$. \nTherefore $\\{x : f(x) + g(x) < t\\} = \\bigcup_{r\\in\\Bbb Q} [f^{-1}((-\\infty, r)) \\cap g^{-1}((-\\infty, t-r))]$. \nBoth the sets being intersected in the union are measurable sets. Hence the set on the left is also measurable, meaning that $f+g$ is measurable.", "meta": {"post_id": 541118, "input_score": 42, "output_score": 74, "post_title": "Proving that sum of two measurable functions is measurable."}}
{"input": "This question can look like a duplicate of this one, but it's kind of different. I'm trying to relate some geometrical meanings I've seem in some books to the definition of differential forms in $\\mathbb{R}^n$ as mappings $p \\mapsto \\omega(p)\\in \\Lambda^k(\\mathbb{R}^n_{\\phantom{n}p})$.\nDifferential forms seems to be object with high geometrical importance. However, I'm failing to grasp what they really represent. Many books, mainly on Physics, try to give one geometrical interpretation for differential forms as \"families of surfaces\" such that the value on a vector is the number of surfaces the vector crosses.\nThis confuses me a little. Why do this interpretation makes any sense? I mean, if I want to construct an object with this geometrical property, why it should be a function associating skew-symmetric tensors to each point in space?\nAlso, vector fields are easy to understand. We know what each vector is at each point, we picture as a small arrow, and we know that they can describe things with directions, they can describe rates of change being derivatives, and so on. Now this geometrical interpretation they give does not allow us to picture differential forms at points, just the association at each point.\nMy understanding was the following as I see: Differential forms replace the classical $dx$, $dA$, $dV$ and so on, that were considered infinitesimal objects. My idea is that in that case, $\\omega(p)$ would represent just a small patch of the surfaces $\\omega$ represents and because of that, we could think of $\\omega$ really relating to those infinitesimal objects. I'm unsure of this intuition, and I can't see how this would lead us towards the rigorous definition of differential forms.\nSo, what's the true geometrical meaning of differential forms and how this meaning implies that the algebraic definition we give is a good one?\nThanks very much in advance!", "output": "A 2-form is a function that eats a parallelogram (technically it eats 2 vectors, which you should think of as spanning a parallelogram) and spits out a number proportional to its area. A 3-form eats a parallelepiped (the 3-dimensional analog of a parallelogram) and spits out a number proportional to its volume. A 4-form eats a 4-dimensional parallelotope and spits out a number proportional to its hypervolume. A 1-form eats a line segment (which you can think of as a 1-dimensional parallelogram) and spits out a number proportional to its length. A 0-form eats a single point (which you can think of as a 0-dimensional parallelogram) and spits out a number, though there's nothing for it to be proportional to since a point has no extension in space. I think you get the picture. In general an n-form eats n vectors, which you should think of as spanning an n-dimensional parallelotope, and spits out a number proportional to its hypervolume.\nUsually books that teach differential forms obscure this. They will define an n-form as a \"real-valued multilinear, skew-symmetric function of n vectors\". But it means the same thing. Multilinearity and skew-symmetry = output is proportional to length/area/volume/hypervolume. The determinant, which is used to compute the volume of a parallelepiped (and its higher and lower dimensional analogs), has the same two properties.\nSo why do we require forms to have this property? Well it's just because it's needed for integration. Imagine a curve you want to integrate over. The first step is to approximate it with line segments. Then you apply some function to each line segment in order to get a number. You need that number to shrink as the size of the line segment shrinks otherwise the sum won't converge. Think about it, if the output of the function was independent of the length of the input, then as more segments were added to the approximation the sum would just shoot up to infinity. Now think of a surface you want to integrate over. You can approximate it with parallelograms, imagine the scales of an armadillo. Then for each parallelogram you apply some function that spits out a number. We need the numbers to shrink as the scales do so the sum actually converges. If you want to integrate over some 3-dimensional volume, approximate it with parallelepipeds and again evaluate a function for each parallelepiped. The output of this function needs to shrink with its input for the sum to converge. These functions that we integrate over curves/surfaces/volumes/hypervolumes are forms.\nNow let me explain why you write forms as linear combinations of elementary forms. It has to do with the generalized Pythagorean theorem, which I'll just call the GPT. In the same way that the length of a line segment is equal to the sum of the squared lengths of its projections onto the various coordinate axes, the area of an arbitrary parallelogram is equal to the sum of the squared areas of its projections onto the various coordinate planes. And the volume of a parallelepiped is equal to the sum of the squared volumes of its projections onto the various 3-dimensional subspaces. And so on. So the Pythagorean theorem applies to more than just line segments.\nSo let's look at the example of a 1-form that eats line segments embedded in 3-dimensional space. In general it's gonna look like $adx + bdy + cdz$ (if you forgot, $dx$, $dy$, and $dz$ are just functions that eat a line segment and spit out its projections on the x axis, y axis, and z axis respectively). All that's happening is you're taking the dot product of a vector $(a,b,c)$ with another vector $(dx,dy,dz)$ which equals the projection of $(a,b,c)$ onto $(dx,dy,dz)$ times the length of $(dx,dy,dz)$ (the length of $(dx,dy,dz)$ is $\\sqrt{dx^2 + dy^2 + dz^2}$ ie the length of the line segment by the GPT). In other words $adx + bdy + cdz$ is literally just another way of writing: (projection of $(a,b,c)$ onto $(dx,dy,dz)$) times (length of the line segment). Since the length of the line segment is a factor in this product, the function is obviously proportional to the length of the line segment. Any 1-form can be written like this.\nAnother example: A 2-form that eats parallelograms embedded in 3-dimensional space is gonna have the form $a(dx \\wedge dy) + b(dx \\wedge dz) + c(dy \\wedge dz)$ (if you forgot, $dx \\wedge dy$, $dx \\wedge dz$, and $dy \\wedge dz$ are just functions that eat parallelograms and spit out the areas of their projections on the xy, xz, and yz planes respectively). So this is just another way of writing the dot product of $(a,b,c)$ and $(dx \\wedge dy, dx \\wedge dz, dy \\wedge dz)$ which is just the projection of $(a,b,c)$ onto $(dx \\wedge dy, dx \\wedge dz, dy \\wedge dz)$ times the length of $(dx \\wedge dy, dx \\wedge dz, dy \\wedge dz)$ (which is $\\sqrt{(dx \\wedge dy)^2 + (dx \\wedge dz)^2 + (dy \\wedge dz)^2}$ ie the area of the parallelogram by the GPT). In other words the linear combination is just equal to: (projection of $(a,b,c)$ onto $(dx \\wedge dy, dx \\wedge dz, dy \\wedge dz)$) times (area of the parallelogram). Which is clearly a function proportional to the area of the parallelogram.\nAnother example: A 2-form that eats parallelograms in the plane. It has the general form $a(dx \\wedge dy)$. You only need one term because $dx \\wedge dy$ already gives you the area of the parallelogram. In the same way $dx$ gives you the length of your line segment if you're only in 1 dimension. It's only when you're in a dimension higher than the dimension of the line segment/parallelogram/parallelepiped/parallelotope that you're gonna have to invoke the GPT ie have a linear combination of multiple elementary forms.\nSo hopefully you see that differential forms are actually very simple objects. They're merely generalized integrands. Other things in exterior calculus like the exterior derivative, the generalized stokes theorem, etc are similarly very simple when explained properly.\nedit: a slightly cleaned up version of this post with some pictures can be found here: https://simplermath.wordpress.com/2020/02/13/understanding-differential-forms/", "meta": {"post_id": 548131, "input_score": 48, "output_score": 43, "post_title": "What's the geometrical intuition behind differential forms?"}}
{"input": "Let $\\large{I} = \\Large \\int\\limits_{-\\infty}^{\\infty} \\normalsize e^{\\small -\\frac{y^2}{2}}\\  dy$. \nThen, my textbook says,\n$$\\tag{1}\nI^2 = \\left( \\int\\limits_{-\\infty}^{\\infty} \\normalsize e^{\\small -\\frac{y^2}{2}}\\  dy\\ \\right) \\left( \\int\\limits_{-\\infty}^{\\infty} \\normalsize e^{\\small -\\frac{x^2}{2}}\\  dx\\ \\right) = \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} e^{-(y^2 +x^2)/2}\\ dy\\ dx $$\nI am not seeing how we reach the rightmost, iterated integral, and I do not remember the calculus how or why this works. Why can we restate the product of two integrals, each having funcions of  different variable, as an iterated integral including both variables? \nLet $\\large{I} = \\Large \\int\\limits_{-\\infty}^{\\infty} \\normalsize f_y(y)\\  dy$ for any continuous, differentiable $ f_y: \\mathbb{R} \\to \\mathbb{R}$\nNow, does (2) generally hold, $\\forall x, y \\in \\mathbb{R}$ ?\n$$\\tag{2}\nI^2 = \\left( \\int\\limits_{-\\infty}^{\\infty} \\normalsize f_y(y)\\  dy\\ \\right) \\left( \\int\\limits_{-\\infty}^{\\infty} \\normalsize f_x(x)\\  dx\\ \\right) = \\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty} f_y(y)f_x(x) dy\\ dx $$\nor is result(1) specific to the particular function $e^{-(y^2)/2}$?\nPlease show the work or reasoning behind how and why this transformation is valid.", "output": "Note first, that for any integrable $f \\colon \\def\\R{\\mathbb R}\\R \\to \\R$ and any $\\alpha \\in \\R$ we have\n$$ \\int_\\R \\alpha f(x)\\, dx = \\alpha \\int_\\R f(x) \\, dx $$\nNow note that with $\\alpha := \\int_\\R f(y)\\, dy$ (this depends on $f$, but given $f$ it is a constant) this gives\n$$ \\int_\\R  \\left(\\int_\\R f(y)\\, dy\\right)\\, f(x)\\, dx = \\int_\\R f(y)\\, dy  \\cdot \\int_\\R f(x)\\, dx $$\nNow, for every fixed $x \\in \\R$, we apply the above again, now for $\\alpha = f(x)$ (which is not depending on $y$), giving\n$$  \\left(\\int_\\R f(y)\\, dy\\right)\\, f(x) =  \\int_\\R f(x)f(y)\\, dy $$\naltogether\n$$ \\int_\\R  \\int_\\R f(y)f(x)\\, dy\\, dx = \\int_\\R f(y)\\, dy  \\cdot \\int_\\R f(x)\\, dx $$", "meta": {"post_id": 549923, "input_score": 60, "output_score": 61, "post_title": "How the product of two integrals is iterated integral? $\\int\\cdot \\int = \\iint$"}}
{"input": "I need your assistance with evaluating the integral\n$$\\int_0^\\infty\\frac{1}{x\\,\\sqrt{2}+\\sqrt{2\\,x^2+1}}\\cdot\\frac{\\log x}{\\sqrt{x^2+1}}dx$$\nI tried manual integration by parts, but it seemed to only complicate the integrand more. I also tried to evaluate it with a CAS, but it was not able to handle it.", "output": "It is easy to see that the integral is equivalent to\n$$\n\\begin{align*}\n\\int_0^\\infty \\frac{1}{x\\sqrt{2}+\\sqrt{2x^2+1}}\\frac{\\log x}{\\sqrt{1+x^2}}dx &= \\sqrt{2}\\int_0^\\infty \\frac{\\sqrt{x^2+\\frac{1}{2}}-x}{\\sqrt{1+x^2}}\\log x\\; dx\\tag{1}\n\\end{align*}\n$$\nThis integral is a special case of the following generalised equation:\n$$\\begin{align*}\\mathcal{I}(k) :&= \\int_0^\\infty \\frac{\\sqrt{x^2+k^2}-x}{\\sqrt{1+x^2}}\\log x\\; dx \\\\ &= E'(k)-\\left(\\frac{1+k^2}{2} \\right)K'(k)+\\left(k^2 K'(k)-E'(k) \\right)\\frac{\\log k}{2}+\\log 2-1 \\tag{2}\\end{align*}$$\nwhere $K'(k)$ and $E'(k)$ are complementary elliptic integrals of the first and second kind respectively.\nPutting $k=\\frac{1}{\\sqrt{2}}$ in equation $(2)$,\n$$\n\\begin{align*}\n\\mathcal{I}\\left(\\frac{1}{\\sqrt{2}}\\right)&=E'\\left(\\frac{1}{\\sqrt{2} }\\right)-\\frac{3}{4}K'\\left(\\frac{1}{\\sqrt{2}} \\right)-\\left\\{\\frac{1}{2} K'\\left(\\frac{1}{\\sqrt{2}} \\right)-E'\\left(\\frac{1}{\\sqrt{2}}  \\right)\\right\\}\\frac{\\log 2}{4}+\\log 2-1 \n\\end{align*}\n$$\nUsing the special values,\n$$\n\\begin{align*}\nE'\\left(\\frac{1}{\\sqrt2} \\right) &= \\frac{\\Gamma\\left(\\frac{3}{4} \\right)^2}{2\\sqrt\\pi}+\\frac{\\sqrt{\\pi^3}}{4\\Gamma\\left(\\frac{3}{4} \\right)^2}\\\\\nK'\\left(\\frac{1}{\\sqrt2} \\right) &= \\frac{\\sqrt{\\pi^3}}{2\\Gamma\\left(\\frac{3}{4} \\right)^2}\n\\end{align*}\n$$\nwe get\n$$\n\\mathcal{I}\\left(\\frac{1}{\\sqrt{2}}\\right)=\\frac{1+\\log\\sqrt[4]2}{2\\sqrt{\\,\\pi}}\\Gamma\\left(\\frac34\\right)^2-\\frac{\\sqrt{\\,\\pi^3}}8\\Gamma\\left(\\frac34\\right)^{-2}+(\\log 2-1)\\, \\tag{3}\n$$\nPutting this in equation $(1)$, we get the answer that Cleo posted.\n\nHow to prove Equation $(2)$?\nWe begin with Proposition 7.1 of \"The integrals in Gradshteyn and Ryzhik:\nPart 16\" by Boettner and Moll.\n$$\\int_0^\\infty \\frac{\\log x}{\\sqrt{(1+x^2)(m^2+x^2)}}dx = \\frac{1}{2}K'(m)\\log m$$\nMultiplying both sides by $m$ and integrating from $0$ to $k$:\n$$\n\\begin{align*}\n\\int_0^\\infty \\frac{\\sqrt{x^2+k^2}-x}{\\sqrt{1+x^2}}\\log x\\; dx &= \\frac{1}{2}\\int_0^k m K'(m)\\log(m)\\; dm\n\\end{align*}\n$$\nThe result follows since\n$$\\begin{align*} \\int m K'(m)\\log(m)\\; dm &= 2E'(m)-\\left(1+m^2 \\right)K'(m)+\\left(m^2 K'(m)-E'(m) \\right)\\log m\\\\ &\\quad +\\text{constant} \\tag{4}\n\\end{align*}$$\nOne can verify equation $(4)$ easily by differentiating both sides with respect to $m$ and using the identities\n$$\n\\begin{align*}\n\\frac{dE'(k)}{dk}&= \\frac{k}{k^{'2}}(K'(k)-E'(k))\\\\\n\\frac{dK'(k)}{dk}&= \\frac{k^2 K'(k)-E^{'}(k)}{kk^{'2}}\n\\end{align*}\n$$", "meta": {"post_id": 554624, "input_score": 35, "output_score": 39, "post_title": "Integral $\\int_0^\\infty\\frac{1}{x\\,\\sqrt{2}+\\sqrt{2\\,x^2+1}}\\cdot\\frac{\\log x}{\\sqrt{x^2+1}}\\mathrm dx$"}}
{"input": "Is it possible to evaluate this integral in a closed form?\n$$I=\\int_0^1\\frac{\\arctan^2x}{\\sqrt{1-x^2}}\\mathrm dx$$\nIt also can be represented as\n$$I=\\int_0^{\\pi/4}\\frac{\\phi^2}{\\cos \\phi\\,\\sqrt{\\cos 2\\phi}}\\mathrm d\\phi$$", "output": "Okay, finally I was able to prove it.\nStep 0. Observations. In view of the following identity\n$$ \\int_{0}^{\\frac{\\pi}{2}} \\arctan (r \\sin\\theta) \\, d\\theta = 2 \\chi_{2} \\left( \\frac{\\sqrt{1+r^{2}} - 1}{r} \\right), $$\nVladimir's result suggests that there may exists a general formula connecting\n$$ I(r, s) = \\int_{0}^{\\frac{\\pi}{2}} \\arctan (r \\sin\\theta) \\arctan (s \\sin\\theta) \\, d\\theta $$\nand the Legendre chi function $\\chi_{2}$. Indeed, inspired by Vladimir's result, I conjectured that\n$$ I(r, s) = \\pi \\chi_{2} \\left( \\frac{\\sqrt{1+r^{2}} - 1}{r} \\cdot \\frac{\\sqrt{1+s^{2}} - 1}{s} \\right). \\tag{1} $$\nI succeeded in proving this identity, so I post a solution here.\nStep 1. Proof of the identity $\\text{(1)}$. It is easy to check that the following identity holds:\n$$ \\arctan(ab) = \\int_{1/b}^{\\infty} \\frac{a \\, dx}{a^{2} + x^{2}}. $$\nSo it follows that\n\\begin{align*}\nI(r, s)\n&= \\int_{1/r}^{\\infty} \\int_{1/s}^{\\infty} \\int_{0}^{\\frac{\\pi}{2}} \\frac{\\sin^{2}\\theta}{(x^{2} + \\sin^{2}\\theta)(y^{2} + \\sin^{2}\\theta)} \\, d\\theta dy dx \\\\\n&= \\int_{1/r}^{\\infty} \\int_{1/s}^{\\infty} \\int_{0}^{\\frac{\\pi}{2}} \\frac{1}{x^{2} - y^{2}} \\left( \\frac{x^{2}}{x^{2} + \\sin^{2}\\theta} - \\frac{y^{2}}{y^{2} + \\sin^{2}\\theta} \\right) \\, d\\theta dy dx \\\\\n&= \\frac{\\pi}{2} \\int_{1/r}^{\\infty} \\int_{1/s}^{\\infty} \\frac{1}{x^{2} - y^{2}} \\left( \\frac{x}{\\sqrt{x^{2} + 1}} - \\frac{y}{\\sqrt{y^{2} + 1}} \\right) \\, dy dx.\n\\end{align*}\nFor the convenience of notation, we put\n$$ \\alpha = \\frac{\\sqrt{r^{2} + 1} - 1}{r} \\quad \\text{and} \\quad \\beta = \\frac{\\sqrt{s^{2} + 1} - 1}{s}. $$\nThen it is easy to check that $\\mathrm{arsinh}(1/r) = - \\log \\alpha$ and likewise for $s$ and $\\beta$. Thus with the substitution $x \\mapsto \\sinh x$ and $y \\mapsto \\sinh y$, we have\n\\begin{align*}\nI(r, s)\n&= \\frac{\\pi}{2} \\int_{-\\log\\alpha}^{\\infty} \\int_{-\\log\\beta}^{\\infty} \\frac{\\sinh x \\cosh y - \\sinh y \\cosh x}{\\sinh^{2}x - \\sinh^{2}y} \\, dy dx.\n\\end{align*}\nApplying the substitution $e^{-x} \\mapsto x$ and $e^{-y} \\mapsto y$, it follows that\n\\begin{align*}\nI(r, s)\n&= \\pi \\int_{0}^{\\alpha} \\int_{0}^{\\beta} \\frac{dydx}{1 - x^{2}y^{2}} \\\\\n&= \\pi \\sum_{n=0}^{\\infty} \\left( \\int_{0}^{\\alpha} x^{2n} \\, dx \\right) \\left( \\int_{0}^{\\beta} y^{2n} \\, dx \\right)\n = \\pi \\sum_{0}^{\\infty} \\frac{(\\alpha \\beta)^{2n+1}}{(2n+1)^{2}} \\\\\n&= \\pi \\chi_{2}(\\alpha \\beta)\n\\end{align*}\nas desired, proving the identity $\\text{(1)}$.\n\nEDIT. I found a much simpler and intuitive proof of $\\text{(1)}$. We first observe that $\\text{(1)}$ is equivalent to the following identity\n$$ \\int_{0}^{\\frac{\\pi}{2}} \\arctan\\left( \\frac{2r\\sin\\theta}{1-r^{2}} \\right) \\arctan\\left( \\frac{2s\\sin\\theta}{1-s^{2}} \\right) \\, d\\theta = \\pi \\chi_{2}(rs). $$\nNow we first observe that from the addition formula for the hyperbolic tangent, we obtain the following formula\n$$ \n\\operatorname{artanh}x - \\operatorname{artanh} y = \\operatorname{artanh} \\left( \\frac{x - y}{1 - xy} \\right) $$\nwhich holds for sufficiently small $x, y$. Thus\n\\begin{align*}\n\\arctan\\left( \\frac{2r\\sin\\theta}{1-r^{2}} \\right)\n&= \\frac{1}{i} \\operatorname{artanh}\\left( \\frac{2ir\\sin\\theta}{1-r^{2}} \\right) \n = \\frac{\\operatorname{artanh}(re^{i\\theta}) - \\operatorname{artanh}(re^{-i\\theta})}{i} \\\\\n&= 2 \\Im \\operatorname{artanh}(re^{i\\theta})\n = 2 \\sum_{n=0}^{\\infty} \\frac{\\sin(2n+1)\\theta}{2n+1} r^{2n+1}.\n\\end{align*}\nWe readily check this holds for any $|r| < 1$. Therefore\n\\begin{align*}\n&\\int_{0}^{\\frac{\\pi}{2}} \\arctan\\left( \\frac{2r\\sin\\theta}{1-r^{2}} \\right) \\arctan\\left( \\frac{2s\\sin\\theta}{1-s^{2}} \\right) \\, d\\theta \\\\\n&\\quad = 4 \\sum_{m=0}^{\\infty} \\sum_{n=0}^{\\infty} \\frac{r^{2m+1}s^{2n+1}}{(2m+1)(2n+1)} \\int_{0}^{\\frac{\\pi}{2}} \\sin(2m+1)\\theta \\sin(2n+1)\\theta \\, d\\theta\\\\\n&\\quad = 2 \\sum_{m=0}^{\\infty} \\sum_{n=0}^{\\infty} \\frac{r^{2m+1}s^{2n+1}}{(2m+1)(2n+1)} \\int_{0}^{\\frac{\\pi}{2}} \\{ \\cos(2m-2n)\\theta - \\cos(2m+2n+2)\\theta \\} \\, d\\theta\\\\\n&\\quad = \\pi \\sum_{m=0}^{\\infty} \\sum_{n=0}^{\\infty} \\frac{r^{2m+1}s^{2n+1}}{(2m+1)(2n+1)} \\delta_{m,n} \\\\\n&\\quad = \\pi \\chi_{2}(rs).\n\\end{align*}", "meta": {"post_id": 555882, "input_score": 52, "output_score": 37, "post_title": "Integral $\\int_0^1\\frac{\\arctan^2x}{\\sqrt{1-x^2}}\\mathrm dx$"}}
{"input": "How can I prove the following identity?\n$$\\int_0^1\\frac{x^2-2\\,x+2\\ln(1+x)}{x^3\\,\\sqrt{1-x^2}}\\mathrm dx=\\frac{\\pi^2}8-\\frac12$$", "output": "First observe that\n$$x^2-2 x+2 \\log{(1+x)} = 2 \\sum_{k=3}^{\\infty} (-1)^{k+1} \\frac{x^k}{k}$$\nThe integral is then equal to\n$$2 \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{k+3} \\int_0^1 dx \\frac{x^k}{\\sqrt{1-x^2}}$$\nNow, we will need separate treatments for the even and odd terms (1):\n$$\\int_0^1 dx \\frac{x^k}{\\sqrt{1-x^2}} = \\begin{cases} \\frac{\\displaystyle 1}{\\displaystyle 2^{2 k}} \\displaystyle \\binom{2 k}{k} \\frac{\\pi}{2} & k \\: \\text{even}\\\\ \\frac{\\displaystyle 2^{2 k-1}}{\\displaystyle k \\binom{2 k}{k}} &  k \\: \\text{odd} \\end{cases} $$\nThat is, the integral is now equal to the difference between two sums:\n$$ \\pi  \\sum_{k=0}^{\\infty} \\frac{1}{2 k+3} \\frac{1}{2^{2 k}} \\binom{2 k}{k} - \\frac12 \\sum_{k=1}^{\\infty} \\frac{1}{ k+1} \\frac{\\displaystyle 2^{2 k}}{\\displaystyle k \\binom{2 k}{k}}$$\nWe now evaluate each sum in turn.  For the first, let\n$$f(x) = \\sum_{k=0}^{\\infty} \\frac{1}{2 k+3} \\frac{1}{2^{2 k}} \\binom{2 k}{k} x^{2 k+3} $$\nThen\n$$f'(x) = x^2 \\sum_{k=0}^{\\infty} \\frac{1}{2^{2 k}} \\binom{2 k}{k} x^{2 k} = \\frac{x^2}{\\sqrt{1-x^2}}$$\nwhich means that, enforcing the condition that $f(0)=0$ (2),\n$$f(x) = \\int dx \\frac{x^2}{\\sqrt{1-x^2}} = \\frac{1}{2} \\arcsin(x)-\\frac{1}{2} x \\sqrt{1-x^2}$$\nThe sum in question is equal to $f(1) = \\pi/4$.  For the second sum, define\n$$g(x) = \\sum_{k=1}^{\\infty} \\frac{1}{k( k+1)} \\frac{\\displaystyle 2^{2 k}}{\\displaystyle \\binom{2 k}{k}} x^{k+1}$$\nThen (see this answer for a reference)\n$$g''(x) = \\frac{1}{x} \\sum_{k=1}^{\\infty} \\frac{(4 x)^k}{\\displaystyle \\binom{2 k}{k}} = \\frac{\\displaystyle 1+\\frac{ \\arcsin\\left(\\sqrt{x}\\right)}{\\sqrt{x(1-x)}}}{1-x}$$\nIntegrating twice and enforcing the condition that $g(0)=0$ and $g'(0)=0$, we find that (3)\n$$g(x) = x+\\arcsin\\left(\\sqrt{x}\\right)^2-2 \\sqrt{x(1-x)} \\arcsin\\left(\\sqrt{x}\\right) $$\nThe second sum is then \n$$g(1) = 1+\\frac{\\pi^2}{4}$$\nThe value of the integral we seek is then equal to \n$$\\pi f(1) - \\frac12 g(1) = \\pi \\frac{\\pi}{4} - \\frac12 \\left ( 1+ \\frac{\\pi^2}{4} \\right ) = \\frac{\\pi^2}{8} - \\frac12$$\nas was to be shown.\nADDENDUM\nI think I should fill in some gaps of the above proof.  I will go through each intermediate result in turn so that the solution is more self-contained.  The integrals I evaluate here are not as difficult as they appear, although there is one subtlety that should be pointed out.\nEquation (1)\n$$\\int_0^1 dx \\frac{x^k}{\\sqrt{1-x^2}}$$\na) $k$ even, i.e., $k=2 m$, $m \\in \\{0,1,2,\\ldots\\}$\nSub $x=\\sin{t}$ to see that this integral is equal to\n$$I_m = \\int_0^{\\pi/2} dt \\, \\sin^{2 m}{t} $$\nIntegrate by parts to see that\n$$\\begin{align}I_m &= -\\underbrace{\\left [ \\cos{t} \\sin^{2 m-1}{t} \\right ]_0^{\\pi/2}}_{\\text{this}=0} + (2 m-1) \\underbrace{\\int_0^{\\pi/2} dt \\, \\cos^2{t} \\sin^{2 m-2}{t}}_{\\cos^2{t}=1-\\sin^2{t}}\\\\ &= (2 m-1) I_{m-1} - (2 m-1) I_m\\end{align}$$\nThus,\n$$I_m = \\frac{2 m-1}{2 m} I_{m-1} = \\frac{(2 m-1)(2 m-3)\\cdots (3)(1)}{(2 m)(2 m-2)\\cdots (2)} I_0$$\nwhere $I_0 = \\int_0^{\\pi/2} dt = \\pi/2$.  We may rearrange the above result by multiplying the numerator by the denominator, and we have for even values of $k$:\n$$I_m = \\frac{1}{2^{2 m}} \\binom{2 m}{m} \\frac{\\pi}{2} $$\nb) $k$ odd, i.e., $k=2 m+1$, $m \\in \\{0,1,2,\\ldots\\}$\nWe perform identical manipulations as above, but now we get that\n$$I_m = \\frac{(2 m)(2 m-2)\\cdots (2)}{(2 m+1)(2 m-1)\\cdots (3)} I_1 $$\nwhere $I_1 = \\int_0^{\\pi/2} dt \\, \\sin{t} = 1$.  Using similar manipulations as above (except we multiply the denominator by the numerator), we have\n$$I_m = \\frac{1}{2 m+1} \\frac{2^{2 m}}{\\displaystyle \\binom{2 m}{m}} $$\nYou may note, however, that this is not the result I displayed in the proof.  Good reason: this form would complicate the series approach to evaluating the sum.  To this effect, let's map $m \\mapsto m-1$ and consider $m \\in \\{1,2,3,\\ldots\\}$.  Then\n$$I_m = \\frac{2^{2 m-2}}{2 m-1} \\frac{[(m-1)!]^2}{(2 m-2)!} = \\frac{2^{2 m-1}}{\\displaystyle m \\binom{2 m}{m}} $$ \nas asserted.\nEquation (2)\n$$\\underbrace{\\int dx \\frac{x^2}{\\sqrt{1-x^2}}}_{x=\\sin{t}} = \\int dt \\, \\sin^2{t} = \\frac{t}{2} - \\frac12 \\sin{t} \\cos{t}$$\nform which the posted result follows.\nEquation (3)\nHere we have 2 integrations.  First, \n$$g'(x) = \\underbrace{\\int dx \\frac{1+\\frac{\\arcsin{\\sqrt{x}}}{\\sqrt{x (1-x)}}}{1-x}}_{x=u^2} = \\underbrace{2 \\int du \\, \\frac{u + \\frac{\\arcsin{u}}{\\sqrt{1-u^2}}}{1-u^2}}_{u=\\sin{t}} = 2 \\int dt \\, \\tan{t} + 2 \\int dt \\, t \\sec^2{t} $$\nDo the second integral by parts:\n$$2 \\int dt \\, t \\sec^2{t} = 2 t \\tan{t} - 2 \\int dt \\, \\tan{t}$$\nThus we have a fortuitous cancellation, and using $t=\\arcsin{\\sqrt{x}}$, and enforcing $g'(0)=0$, we have\n$$g'(x) = 2 \\sqrt{\\frac{x}{1-x}}\\arcsin{\\sqrt{x}}$$\nSo, second, we must integrate this result to get $g(x)$.  We use similar substitutions as above (i.e., $x=u^2$, $u=\\sin{t}$):\n$$g(x) = 4 \\int du \\, \\frac{u^2}{\\sqrt{1-u^2}} \\arcsin{u} = 4 \\int dt \\, t \\, \\sin^2{t}$$\nNow, integrate by parts:\n$$4 \\int dt \\, t \\, \\sin^2{t} = 2 t (t - \\sin{t} \\cos{t}) - 2 \\int dt \\, (t - \\sin{t} \\cos{t}) = t^2 - 2 t \\sin{t} \\cos{t} + \\sin^2{t} +C $$\nNow, use $t = \\arcsin{\\sqrt{x}}$ and the fact that $g(0)=0$ and get\n$$g(x) = \\arcsin{\\left ( \\sqrt{x}\\right )}^2 - 2 \\sqrt{x (1-x)} \\arcsin{\\left ( \\sqrt{x}\\right )} + x$$\nas posted above.", "meta": {"post_id": 556280, "input_score": 35, "output_score": 51, "post_title": "Prove $\\int_0^1\\frac{x^2-2\\,x+2\\ln(1+x)}{x^3\\,\\sqrt{1-x^2}}\\mathrm dx=\\frac{\\pi^2}8-\\frac12$"}}
{"input": "Is it possible to evaluate this integral in a closed form?\n$$I=\\int_0^{\\pi/2}\\arctan^2\\left(\\frac{6\\sin x}{3+\\cos 2x}\\right)\\mathrm dx$$", "output": "I will refer to the following result from my previous answer:\n\\begin{align*}\nI(r, s)\n&= \\int_{0}^{\\frac{\\pi}{2}} \\arctan (r \\sin\\theta) \\arctan (s \\sin\\theta) \\, d\\theta \\\\\n&= \\pi \\chi_{2} \\left( \\frac{\\sqrt{1+r^{2}} - 1}{r} \\times \\frac{\\sqrt{1+s^{2}} - 1}{s} \\right),\n\\end{align*}\nwhere $\\chi_{2}$ is the Legendre chi function. Using the addition formula for the arctangent, it follows that\n$$ \\arctan\\left(\\frac{6\\sin x}{3 + \\cos 2x} \\right) = \\arctan \\left( \\frac{\\frac{3}{2}\\sin x}{1 - \\frac{1}{2}\\sin^{2} x} \\right) = \\arctan (\\sin x) + \\arctan ( \\tfrac{1}{2}\\sin x). $$ So it follows that\n$$ \\int_{0}^{\\frac{\\pi}{2}} \\arctan^{2}\\left(\\frac{6\\sin x}{3+\\cos 2x}\\right) \\, dx = I(1,1) + 2I(1,\\tfrac{1}{2}) + I(\\tfrac{1}{2},\\tfrac{1}{2}), $$\nwhich reduces to a combination of Legendre chi functions\n$$ \\pi \\left\\{ \\chi_{2}(3 - 2\\sqrt{2}) + \\chi_{2}(9 - 4\\sqrt{5}) + 2\\chi_{2}\\left( (\\sqrt{2} - 1)(\\sqrt{5} - 2) \\right) \\right\\}.  $$", "meta": {"post_id": 564816, "input_score": 51, "output_score": 37, "post_title": "Integral $\\int_0^{\\pi/2}\\arctan^2\\left(\\frac{6\\sin x}{3+\\cos 2x}\\right)\\mathrm dx$"}}
{"input": "This is somewhat similar to my previous question: Closed form for $\\int_0^1\\frac{x^{5/6}}{(1-x)^{1/6}\\,(1+2\\,x)^{4/3}}\\log\\left(\\frac{1+2x}{x\\,(1-x)}\\right)\\,dx$\nIs it possible to find a closed form for this integral?\n$$Q=\\int_0^1\\sqrt{\\frac{2-x}{(1-x)\\,x}}\\,\\log\\left(\\frac{(2-x)\\,x}{1-x}\\right)dx$$", "output": "First, we transform the integral into a more computable form by using some substitutions.\n$$\\begin{align*}\\displaystyle Q &= \\int_0^1\\sqrt{\\frac{2-x}{(1-x)\\,x}}\\,\\log\\left(\\frac{(2-x)\\,x}{1-x}\\right)dx\\\\ &=\\int_0^1 \\sqrt{\\frac{1+u}{u(1-u)}}\\log \\left( \\frac{(1+u)(1-u)}{u}\\right)du \\quad \\color{blue}{\\text{where }u=1-x}\\\\\n&= \\int_0^1 \\frac{1+u}{\\sqrt{u(1-u^2)}}\\log \\left( \\frac{1-u^2}{u}\\right)du \\\\\n&= \\frac{1}{2}\\int_0^1 \\frac{1+\\sqrt{t}}{t^{\\frac{3}{4}}\\sqrt{1-t}}\\log \\left(\\frac{1-t}{\\sqrt{t}} \\right)dt \\quad \\color{blue}{\\text{where }t=u^2} \\\\\n&= \\frac{1}{2}\\int_0^1 \\frac{\\log(1-t)}{t^{\\frac{3}{4}}\\sqrt{1-t}}dt-\\frac{1}{4}\\int_0^1 \\frac{\\log(t)}{t^{\\frac{3}{4}}\\sqrt{1-t}}dt \\\\\n&\\quad +\\frac{1}{2}\\int_0^1 \\frac{\\log(1-t)}{t^{\\frac{1}{4}}\\sqrt{1-t}}dt-\\frac{1}{4}\\int_0^1 \\frac{\\log(t)}{t^{\\frac{1}{4}}\\sqrt{1-t}}dt \\tag{1}\n\\end{align*}$$\nThese four integrals can be evaluated by calculating derivatives of beta function in terms of digamma function. For e.g.\n$$\n\\begin{align*}\n\\int_0^1 \\frac{\\log(1-t)}{t^{\\frac{3}{4}}\\sqrt{1-t}}dt  &= \\frac{d}{dz}\\left\\{ \\int_0^1 t^{-\\frac{3}{4}}(1-t)^{z-1} \\; dt\\right\\}_{z=\\frac{1}{2}}\\\\&= \\frac{d}{dz}\\left\\{ \\frac{\\Gamma \\left( \\frac{1}{4}\\right)\\Gamma(z)}{\\Gamma \\left( \\frac{1}{4}+z\\right)} \\right\\}_{z=\\frac{1}{2}}\\\\ &= \\frac{\\Gamma \\left( \\frac{1}{4}\\right)\\sqrt{\\pi}}{\\Gamma \\left( \\frac{3}{4}\\right)}\\left\\{\\psi_0 \\left(\\frac{1}{2} \\right) -\\psi_0 \\left(\\frac{3}{4} \\right)\\right\\}\\\\ &= \\pi^{3/2}\\frac{\\sqrt{2}}{\\Gamma \\left( \\frac{3}{4}\\right)^2}\\left\\{\\log 2-\\frac{\\pi}{2} \\right\\} \\tag{2} \\\\\n\\end{align*}\n$$\nTo get the last expression, I used the special values\n$$\n\\begin{align*}\n\\psi_0 \\left(\\frac{3}{4}\\right) &= -\\gamma +\\frac{\\pi}{2}-3\\log 2 \\\\\n\\psi_0 \\left(\\frac{1}{2}\\right) &= -\\gamma -2\\log 2\n\\end{align*}\n$$\nUsing the same technique, the other three integrals can be evaluated:\n$$\n\\begin{align*}\n\\int_0^1 \\frac{\\log(t)}{t^{\\frac{3}{4}}\\sqrt{1-t}}dt&= -\\pi^{5/2}\\frac{\\sqrt2}{\\Gamma \\left( \\frac{3}{4}\\right)^2}\\tag{3} \\\\\n\\int_0^1 \\frac{\\log(t)}{t^{\\frac{1}{4}}\\sqrt{1-t}}dt&=\\frac{(4\\pi-16)\\Gamma \\left( \\frac{3}{4}\\right)^2}{\\sqrt{2\\pi}} \\tag{4} \\\\\n\\int_0^1 \\frac{\\log(1-t)}{t^{\\frac{1}{4}}\\sqrt{1-t}}dt&=\\frac{2(-8+\\pi+2\\log 2)\\Gamma \\left( \\frac{3}{4}\\right)^2}{\\sqrt{2\\pi}} \\tag{5}\n\\end{align*}\n$$\nSubstituting the results of equations $(2),(3),(4)$ and $(5)$ in $(1)$ gives\n$$Q=\\frac{\\Gamma\\left(\\frac34\\right)^{-2}\\pi^2\\log2-\\Gamma\\left(\\frac34\\right)^2(4-2\\log2)}{\\sqrt{2\\,\\pi}}$$", "meta": {"post_id": 566513, "input_score": 31, "output_score": 46, "post_title": "Closed form for $\\int_0^1\\sqrt{\\frac{2-x}{(1-x)\\,x}}\\,\\log\\left(\\frac{(2-x)\\,x}{1-x}\\right)dx$"}}
{"input": "Looking around there are three candidates for \"foundations of mathematics\": \n\nset theory\ncategory theory\ntype theory\n\nThere is a seminal paper relating these three topics:\nFrom Sets to Types to Categories to Sets by Steve Awodey\nBut at this forum (MSE) and its companion (MO) the tag [type theory] is seriously underrepresented. As of today (2013/11/13) (questions by tag):\nMSE\n\nset theory: 1,866\ncategory theory: 1,658\ntype theory: 39\n\nMO\n\nset theory: 1,437\ncategory theory: 1,920\ntype theory: 40\n\nUpdate (2017/05/17): \nMSE\n\nset theory: 4,435\ncategory theory: 6,137\ntype theory: 224\n\nUpdate (2019/07/15): \nMSE\n\nset theory: 6,267\ncategory theory: 9,009\ntype theory: 371\n\nUpdate (2020/05/05): \nMSE\n\nset theory: 7,038\ncategory theory: 10,255\ntype theory: 441\n\nWhat does this mean? Is type theory a hoax?  For example, I stumbled over this MSE comment (by a learned member):\n\n[...] a lot of people [in the type theory community] didn't know\n  what they really talk about (in comparison to, say, classical\n  analysis, where the definitions are very concrete and clear). I'm sure\n  that that's not 100% true on the actual people, but that impression\n  did stick with me. [...]\n\nI'd like to learn from the MSE- and MO-community (resp. their experts): \n\nWhy is it worth spending time on type theory?", "output": "Type theory is to set theory what computable functions are to usual functions. It's a constructive setting for doing mathematics, so it allows to deal carefully with what can or can't be computed/decided (see intensionality vs. extensionality, or the different notions of reduction and conversion in $\\lambda$-calculus). Furthermore, just like category theory, it gives a great insight on how certain mathematical objects are nothing but particular cases of a general construction, in a very abstract and powerful way. Look up the propositions-as-sets and the proofs-as-programs paradigms to see what I mean (but there's a lot more than that).\nNow, as always there are some cons. I'm thinking of two reasons why type theory hasn't had much success among the general mathematicians:\nFirst, type theory doesn't allow abuse of language. For example, in type theory it is usually the case that if $A$ is a set, then a subset of $A$ is not a set. It is a completely different kind of entity: it will probably be a propositional function, i.e. something which maps elements of $A$ to propositions. Another example: if $n$ is a natural number, then $n$ is not an integer (but something like $\\mathsf{int}(n)$ will be). Distinctions like these make some mathematicians uncomfortable, but they prove helpful in dealing with certain things which are maybe more interesting to computer scientists (and of course logicians).\nSecond, there is no \"canonical\" type theory. Most of the mathematics done in set theory is actually based on $\\mathsf{ZFC}$ (or some extension of it). But the fact that there are many different kinds of type theories makes communication between people harder.\nAnyway, there have been many attempts to actually start developing some mathematics in type theories, and some of them have been quite successful: see for instance the work by G. Gonthier with Coq, a proof assistant based on a type theory called the Calculus of Inductive Constructions.", "meta": {"post_id": 567265, "input_score": 60, "output_score": 38, "post_title": "Why is it worth spending time on type theory?"}}
{"input": "This question is inspired by a previous question. It was shown that, for all function $f \\in \\mathcal{C} ([0, 1])$, \n$$ \\lim_{n \\to + \\infty} \\sum_{k=0}^{n} f \\left( \\frac{k}{n+1} \\right) - \\sum_{k=0}^{n-1} f \\left( \\frac{k}{n} \\right) = \\int_0^1 f (x) \\ dx.$$\nA stronger statement would be that there exists some constant $a(f)$ such that:\n$$\\sum_{k=0}^{n-1} f \\left( \\frac{k}{n} \\right) = n \\int_0^1 f (x) \\ dx + a(f) + o(1),$$\nor, in other words, that there is an asymptotic development at order $1$ of the Riemann sums:\n$$\\frac{1}{n} \\sum_{k=0}^{n-1} f \\left( \\frac{k}{n} \\right) = \\int_0^1 f (x) \\ dx + \\frac{a(f)}{n} + o(n^{-1}).$$\nGiven $f$, can we always find such a constant $a(f)$? If this is false, can we find a counter-example? If this is true, can $a(f)$ be written explicitely?\nI have had a quick look at the litterature, but most asymptotics for the Riemann sums involve different meshes, which depend on the function $f$.", "output": "On the subspace $\\mathcal{C}^1([0,1])$ of continuously differentiable functions, we have\n$$\\lim_{n\\to\\infty} \\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right) - n\\int_0^1 f(x)\\,dx = \\frac{f(0) - f(1)}{2}.$$\nWe can see that by computing\n$$\\begin{align}\n&\\Biggl\\lvert\\frac12\\left(f\\left(\\frac{k}{n}\\right) + f\\left(\\frac{k+1}{n}\\right)\\right) - n\\int_{k/n}^{(k+1)/n} f(x)\\,dx\\Biggr\\rvert\\\\\n&\\qquad = \\frac{n}{2}\\left\\lvert \\int_{k/n}^{(k+1)/n} \\left(f\\left(\\frac{k+1}{n}\\right)-f(x)\\right) - \\left(f(x) - f\\left(\\frac{k}{n}\\right)\\right)\\,dx\\right\\rvert\\\\\n&\\qquad = \\frac{n}{2}\\left\\lvert\\int_{k/n}^{(k+1)/n}\\int_x^{(k+1)/n} f'(t)\\,dt - \\int_{k/n}^x f'(t)\\,dt\\,dx\\right\\rvert\\\\\n&\\qquad = \\frac{n}{2}\\left\\lvert\\int_{k/n}^{(k+1)/n}\\left(t-\\frac{k}{n}\\right)f'(t) - \\left(\\frac{k+1}{n}-t\\right)f'(t)\\,dt\\right\\rvert\\\\\n&\\qquad = n\\left\\lvert\\int_{k/n}^{(k+1)/n} \\left(t-\\frac{k+\\frac12}{n}\\right)f'(t)\\,dt\\right\\rvert\\\\\n&\\qquad = n\\left\\lvert\\int_{k/n}^{(k+1)/n} \\left(t-\\frac{k+\\frac12}{n}\\right)\\left(f'(t)- f'\\left(\\frac{k+\\frac12}{n}\\right)\\right)\\,dt\\right\\rvert\\\\\n&\\qquad \\leqslant n \\cdot\\omega_{f'}\\left(\\frac{1}{2n}\\right) \\int_{k/n}^{(k+1)/n} \\left\\lvert t-\\frac{k+\\frac12}{n}\\right\\rvert\\,dt\\\\\n&\\qquad = \\frac{1}{4n}\\cdot\\omega_{f'}\\left(\\frac{1}{2n}\\right),\n\\end{align}$$\nwhere\n$$\\omega_{f'}(\\delta) = \\sup \\left\\lbrace \\lvert f'(s) - f'(t)\\rvert : s,t\\in [0,1], \\lvert s-t\\rvert \\leqslant \\delta\\right\\rbrace$$\nis a modulus of continuity of $f'$. Summing up we obtain\n$$\\left\\lvert \\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right) - \\frac{f(0)-f(1)}{2} - n\\int_0^1 f(x)\\,dx\\right\\rvert \\leqslant \\frac14 \\cdot\\omega_{f'}\\left(\\frac{1}{2n}\\right),$$\nand the continuity of $f'$ means $\\lim\\limits_{\\delta\\searrow 0} \\omega_{f'}(\\delta) = 0$.\nBut there is no map $\\alpha \\colon \\mathcal{C}([0,1]) \\to \\mathbb{C}$ such that for every $f \\in \\mathcal{C}([0,1])$ we have\n$$\\lim_{n\\to\\infty} \\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right) - n\\int_0^1 f(x)\\,dx = \\alpha(f),$$\nor equivalently\n$$\\frac{1}{n}\\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right) = \\int_0^1 f(x)\\,dx + \\frac{\\alpha(f)}{n} + o\\left(\\frac1n\\right).$$\nFor if there were, since $\\mathcal{C}([0,1])$ is a Banach space under the supremum norm, the Banach-Steinhaus theorem (uniform boundedness principle) would assert that the family\n$$T_n \\colon f \\mapsto \\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right) - n\\int_0^1 f(x)\\,dx$$\nis equicontinuous, or norm-bounded.\nHowever, it is easy to see that\n$$\\lVert T_n\\rVert = 2n.$$\nThus the set of $f \\in \\mathcal{C}([0,1])$ such that\n$$\\lim_{n\\to\\infty} \\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right) - n\\int_0^1 f(x)\\,dx$$\nexists is meagre (it is, however, strictly larger than $\\mathcal{C}^1([0,1])$).\nNote that the previous question only considered continuously differentiable functions, i.e. $\\mathcal{C}^1([0,1])$. The Banach-Steinhaus theorem shows that\n$$\\lim_{n\\to\\infty} \\sum_{k=0}^n f\\left(\\frac{k}{n+1}\\right) - \\underbrace{\\sum_{k=0}^{n-1} f\\left(\\frac{k}{n}\\right)}_{S_n(f)}$$\ndoes not exist for all $f\\in\\mathcal{C}([0,1])$, since $\\lVert S_{n+1} - S_n\\rVert = 2n-1$ is not bounded.", "meta": {"post_id": 569750, "input_score": 27, "output_score": 37, "post_title": "Speed of convergence of Riemann sums"}}
{"input": "Let $X$ an hypersurface in $\\mathbb{P}^{n}$ of degree $d$. I would like to prove that the Hilbert polynomial of $X$ is\n$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad p(n)=\n\\begin{pmatrix}\nn+r \\\\\nn\n\\end{pmatrix}\n-\n\\begin{pmatrix}\nn+d-r \\\\\nn\n\\end{pmatrix}\n$\nIn the book \"Geometry of Algebraic Curves Vol. II\", Arbarello, Cornalba, Griffith, p. 7, I read that this result can be proved by taking the cohomology of the following exact sequence\n$\n\\qquad \\qquad \\qquad \\qquad \\qquad\n0 \\longrightarrow \\mathcal{O}_{\\mathbb{P}^{r}}(n-d) \\longrightarrow \\mathcal{O}_{\\mathbb{P}^{r}}(n) \\longrightarrow \\mathcal{O}_{\\mathbb{P}^{r}}(d) \\longrightarrow 0\n$\nNevertheless, I am not able to compute $p(n)$.", "output": "1) Non-cohomological approach:\nFirst, the Hilbert function $h_X:\\mathbb{Z} \\to \\mathbb{Z}$ is defined as \n$$h_X: d \\mapsto \\dim_kS(X)^{(d)}$$\nfor a projective subscheme $X \\subset \\mathbb{P}^n,$ where $S(X)$ is the homogeneous coordinate ring of $X$ (which is a graded ring) and $S(X)^{(d)}$ is the homogeneous degree $d$ part of it, which is a finite dimensional vector space over $k.$ Also $h_X(d)=0$ for $d<0.$\nExercise: Show that $h_X(d)=\\binom{n+d}{d}$ for $X=\\mathbb{P}^n$ and note that $h_X(d)$ is a polynomial in $d$ of degree $n.$ \nIt turns out that for $d \\gg 0$ the Hilbert function $h_X(d)$ can be viewed as a polynomial in $d$ called the Hilbert polynomial $p_X(d).$ (Harder exercise!) (Of course, $d \\gg 0$ means relatively big enough.) So finding the Hilbert function helps us a lot in order for finding the Hilbert polynomial.\nNow, let $X$ be a hypersurface in $\\mathbb{P}^n$ given by a degree $r$ homogeneous polynomial $f$ i.e. $X=\\text{Proj}k[x_0,\\cdots,x_n]/(f).$ You can easily show that \n$$\\dim_kS(X)^{(d)}=\\dim_k k[x_0,\\cdots,x_n]^{(d)}-\\dim_k k[x_0,\\cdots,x_n]^{(d-r)}$$\nTherefore, $h_X(d)=\\binom{n+d}{d}-\\binom{n+d-r}{d-r}$ which is already a polynomial in $d$ so has to be equal to $p_X(d).$\n2) Cohomological approach:\nThe Hilbert polynomial can also be defined as \n$$p_X(d)=\\chi(\\mathcal{O}_X(d))=\\sum_{i=0}^n \\dim_k (-1)^iH^i(X,\\mathcal{O}_X(d))$$ \nwhen $d \\gg 0$ for a pojective subscheme $X$ of $\\mathbb{P}^n.$\nAs before, assume $X$ is a hypersurface given by a degree $r$ homogeneous polynomial $f$ in $\\mathbb{P}^n.$\nThe correct setting is to first consider the following SES\n$$0 \\longrightarrow \\mathcal{O}_{\\mathbb{P}^n}(-r) \\stackrel{.f}{\\longrightarrow} \\mathcal{O}_{\\mathbb{P}^n} \\longrightarrow \\mathcal{O}_X \\longrightarrow 0$$\nthen twist it by $\\mathcal{O}(d)$ to get\n$$0 \\longrightarrow \\mathcal{O}_{\\mathbb{P}^n}(d-r) \\longrightarrow \\mathcal{O}_{\\mathbb{P}^n}(d) \\longrightarrow \\mathcal{O}_X(d) \\longrightarrow 0$$\nNow, since Euler characteristic $\\chi$ is additive on SES, we'd have \n$$\\chi(\\mathcal{O}_X(d))=\\chi(\\mathcal{O}_{\\mathbb{P}^n}(d))-\\chi(\\mathcal{O}_{\\mathbb{P}^n}(d-r)).$$\nUsing the famous knowledge of sheaf cohomology of $\\mathcal{O}_{\\mathbb{P}^n}(d)$ you should be able to get the same polynomial for $d \\gg 0$ as before.", "meta": {"post_id": 573817, "input_score": 10, "output_score": 36, "post_title": "Hilbert polynomial of an hypersurface in projective space"}}
{"input": "A function $f : \\mathbb{C} \\rightarrow \\mathbb{C}$ is said to be holomorphic in an open set $A \\subset \\mathbb{C}$ if it is differentiable at each point of the set $A$.\nThe function $f : \\mathbb{C} \\rightarrow \\mathbb{C}$ is said to be analytic if it has power series representation.\nWe can prove that the two concepts are same for a single variable complex functions. So why these two different terms? Is there any difference between these two concepts in general, please give example.\nThank you for your help.", "output": "So why these two different terms?\n\nBecause the history of mathematical terms is long and complicated. At least we stopped talking about monogenic functions and regular functions, which are two more terms for the same concept (as far as complex analysis is concerned). Quoting HOMT site:\n\nIn modern analysis the term ANALYTIC FUNCTION is used in two ways: (of a complex function) having a complex derivative at every point of its domain, and in consequence possessing derivatives of all orders and agreeing with its Taylor series locally; (of a real function) possessing derivatives of all orders and agreeing with its Taylor series locally. \n\nSince the first usage is so popular (due to the ubiquity of power series in complex analysis, where they exist for every differentiable function), one will often say real-analytic when referring to the usage of the second kind.  \nAlso from HOMT, an explanation of what analytic  meant in the less rigorous age of analysis:\n\n[In  Lagrange's] Th\u00e9orie des Fonctions Analytiques (1797) [...] an analytic function simply signified a function of the kind treated in analysis. The connection between the usage of Lagrange and modern usage is explained by Judith V. Grabiner in her The Origins of Cauchy\u2019s Rigorous Calculus: \"For Lagrange, all the applications of calculus ... rested on those properties of functions which could be learned by studying their Taylor series developments ... Weierstrass later exploited this idea in his theory of functions of a complex variable, retaining Lagrange\u2019s term \"analytic function\" to designate, for Weierstrass, a function of a complex variable with a convergent Taylor series.\"  \n\nAs for \"holomorphic\": in complex analysis we often encounter both Taylor series and Laurent series. For the latter, it matters very much whether the number of negative powers is finite or infinite.   To enunciate these distinctions, the words holomorphic and meromorphic were introduced.  Meromorphic allows  poles (i.e., finitely many negative powers in the Laurent series), while holomorphic does not. From a certain viewpoint (the Riemann sphere), meromorphic functions are no worse than holomorphic ones; while at other times, the presence of poles changes the situation.", "meta": {"post_id": 573984, "input_score": 70, "output_score": 53, "post_title": "Difference between Analytic and Holomorphic function"}}
{"input": "So I want to show that $\\mathbb{Q}(\\sqrt{2+\\sqrt{2}})$ is Galois over $\\mathbb{Q}$ and determine its Galois group. \nMy thoughts are as follows:\nDefine $\\alpha := \\sqrt{2+\\sqrt{2}}$. Then it is easily shown that $\\alpha$ satisfies $\\alpha^4-4\\alpha^2+2=0$.\nDefine $f(x) := x^4-4x^2+2$. Then $f$ is irreducible over $\\mathbb{Q}$ by Eisenstein with $p=2$.\nSo we have that $f$ is the irreducible polynomial for $\\alpha$ over $\\mathbb{Q}$. \nFurther $|\\mathbb{Q}(\\alpha):\\mathbb{Q}|=4$.\nFor $\\mathbb{Q}({\\alpha})$ to be Galois, it must contain all roots of $f$. \nDefine $K=\\mathbb{Q}(\\alpha)$ for convenience.\nDefine $\\alpha := \\alpha_1$.\nSince $f$ has only even powers, we know that $-\\alpha := \\alpha_2$ is a root, and therefore contained in $K$ since $K$ is a field.\nWe note that the other two roots are $\\alpha_3=\\sqrt{2-\\sqrt{2}}$ and $\\alpha_4=-\\sqrt{2-\\sqrt{2}}$.\nSo in order to show $K$ is Galois, it must be shown that $\\alpha_3$ and $\\alpha_4$ lie in $K$.\nNow $\\alpha_1^2=2+\\sqrt2$ and so $\\sqrt2 \\in K$. Thus $-\\sqrt2 \\in K$ since $K$ is a field.\nCan somebody explain why $\\alpha_3$ and $\\alpha_4$ lie in $K$?\nNext we are to determine the Galois group of $K$.\nAssuming $K$ is Galois, since it has degree $4$ over $\\mathbb{Q}$ (shown earlier), we know that its Galois group has size $4$. There are only two groups of size $4$, namely $V_4$ and $C_4$, the Klein four group and the cyclic group of order $4$.\nHow do we determine which of these choice is in fact the Galois Group?", "output": "$\\alpha^2-2=\\sqrt 2\\in K$ by closure of multiplication and addition.\n$$\\frac{\\sqrt 2}{\\sqrt{2+\\sqrt{2}}}=\\frac{\\sqrt 2 \\cdot\\sqrt{2-\\sqrt 2}}{\\sqrt{2+\\sqrt{2}}\\sqrt{2-\\sqrt 2}}=\\frac{\\sqrt 2\\cdot\\sqrt{2-\\sqrt 2}}{\\sqrt{4-2}}=\\sqrt{2-\\sqrt 2}$$\nSince $K$ is a field, it has multiplicative inverses and is closed under multiplication, so $\\sqrt{2-\\sqrt 2}\\in K$.\nWe can determine the nature of $\\mathrm{Gal}(K/\\mathbb{Q})$ by the order of each element. If $f$ is a field automorphism of $K$ and $f(\\sqrt{2+\\sqrt 2})=\\sqrt{2-\\sqrt 2}$, then $f(\\sqrt 2)=f(\\alpha^2-2)=f(\\alpha)^2-2=-\\sqrt 2$. Therefore $$f(f(\\alpha))=f\\left(\\sqrt{2-\\sqrt 2}\\right)=f\\left(\\frac{\\sqrt{2}}{\\sqrt{2+\\sqrt 2}}\\right)=\\frac{f(\\sqrt 2)}{f(\\sqrt{2+\\sqrt 2})}=\\frac{-\\sqrt{2}}{\\sqrt{2-\\sqrt{2}}}=-\\sqrt{2+\\sqrt 2}$$\nTherefore $\\mathrm{ord}(f)> 2$ and must divide $4=|\\mathrm{Gal}(F/\\mathbb{Q})|$, so $\\mathrm{ord}(f)=4$.  It follows that the Galois group is cyclic and abelian.", "meta": {"post_id": 575171, "input_score": 33, "output_score": 43, "post_title": "Galois Group of $\\sqrt{2+\\sqrt{2}}$ over $\\mathbb{Q}$"}}
{"input": "Let $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ and $g : \\mathbb{R} \\rightarrow \\mathbb{R}$ are two uniform continuous functions. Which of the following options are correct and why?\n\n$f(g(x))$ is uniformly continuous.\n$f(g(x))$ is continuous but not uniformly continuous.\n$f(g(x))$ is continuous and bounded.\n\nMy attempt:\nEvery uniformly continuous function maps a Cauchy sequence to a Cauchy sequence. (Here I have a doubt, as the converse may not be true). So if $\\{x_n\\}$ be a Cauchy sequence, $\\{f(x_n)\\}$ and $\\{g(f(x_n))\\}$ both will be Cauchy sequence. So $g(f(x))$ will be uniformly continuous, i.e. 1 is true.\nComposite function of two continuous functions will be continuous. As 1 is true, 2 is false.\n$f(x) = x $ is uniformly continuous. $g(x) = \\log(x)$ is uniformly continuous in $[1,\\infty)$. So $g(f(x)) = \\log(x)$ is uniformly continuous in $[1, \\infty)$, but not in $\\mathbb{R}$.\nThank you for your help.", "output": "Option $(1):$ True:\nChoose $\\epsilon>0.$ \nThen $\\exists~\\delta_1>0$ such that $|x_1-x_2|<\\delta_1$$\\implies|f(x_1)-f(x_2)|<\\epsilon.$ \nFor above $\\delta_1>0~\\exists~\\delta>0$ such that $|x_1-x_2|<\\delta$$\\implies|g(x_1)-g(x_2)|<\\delta_1.$ \nConsequently, $|x_1-x_2|<\\delta$$\\implies|(f\\circ g)(x_1)-(f\\circ g)(x_2)|<\\epsilon.$\nOptions $(2),~(3):$ Not necessarily true: \nTake $f(x)=g(x)=x$ on $\\mathbb R.$ \nThen $f,g$ are uniformly continuous on $\\mathbb R.$ \nNote $(f\\circ g)(x)=x$ is unbounded and uniformly continuous on $\\mathbb R.$", "meta": {"post_id": 575805, "input_score": 25, "output_score": 44, "post_title": "composition of two uniformly continuous functions."}}
{"input": "I need to evaluate this integral to a high precision:\n$$\\large I=\\int_0^\\infty{_1F_2}\\left(\\begin{array}{c}\\tfrac12\\\\1,\\tfrac32\\end{array}\\middle|-x\\right)\\frac{dx}{1+4\\,x}$$\nSymbolic integration in Mathematica cannot handle this integral. When I try to evaluate it numerically, it converges extremely slowly and the result is very unstable, so I am not even sure how many correct digits I got. It looks kinda $I\\stackrel?\\approx0.6212...$\nSo, my only hope is to find a closed form for $I$ in terms of functions for which fast and robust numerical algorithms exist. Could you please help me to find it?", "output": "OK, I have an analytical result:\n$$\\frac{\\pi}{4} \\left [K_0(1) \\mathbf{L}_{-1}(1) + K_1(1) \\mathbf{L}_{0}(1)\\right ] \\approx 0.621255$$\nwhere $K_0$ and $K_1$ are modified Bessel functions of the second kind, and $\\mathbf{L}_{0}$ and $\\mathbf{L}_{-1}$ are modified Struve functions of the first kind.\nThis may be obtained by recognizing that (+)\n$${_1F_2}\\left(\\begin{array}{c}\\tfrac12\\\\1,\\tfrac32\\end{array}\\middle|-x\\right) = \\int_0^1 du \\, J_0\\left ( 2 u \\sqrt{x}\\right ) $$\nThe integral is then, upon reversing order,\n$$\\int_0^1 du \\, \\int_0^{\\infty} dx \\frac{J_0\\left ( 2 u \\sqrt{x}\\right )}{1+4 x}$$\nThe following will need a derivation (++):\n$$\\int_0^{\\infty} dx \\frac{J_0\\left ( 2 u \\sqrt{x}\\right )}{1+4 x} = \\frac12 K_0(u)$$\nThe stated result is then\n$$\\frac12 \\int_0^1 du \\, K_0(u) = \\frac{\\pi}{4} \\left [K_0(1) \\mathbf{L}_{-1}(1) + K_1(1) \\mathbf{L}_{0}(1)\\right ] $$\nwhich may be found in the DLMF.\nDerivation of (+)\nNote that the coefficient of $x^n$ in ${_1F_2}\\left(\\begin{array}{c}\\tfrac12\\\\1,\\tfrac32\\end{array}\\middle|-x\\right)$, by definition, is\n$$a_n = \\frac{\\frac12 \\left (\\frac12 + 1 \\right ) \\left (\\frac12 + 2 \\right ) \\cdots \\left (\\frac12 + n-1 \\right )}{n! \\frac{3}{2} \\left (\\frac{3}{2} + 1 \\right ) \\left (\\frac{3}{2} + 2 \\right ) \\cdots \\left (\\frac{3}{2} + n-1 \\right )} \\frac{(-1)^n}{n!}$$\nwhich, after simplification, is\n$$a_n = \\frac{(-1)^n}{(2 n+1) (n!)^2} $$\nThen, note that\n$$J_0(2 u \\sqrt{x}) = \\sum_{n=0}^{\\infty} (-1)^n \\frac{u^{2 n}}{(n!)^2} x^n$$\nThen\n$$\\int_0^1 du \\, J_0(2 u \\sqrt{x}) = \\sum_{n=0}^{\\infty} \\frac{(-1)^n}{(2 n+1)(n!)^2} x^n$$\nand one may see that the coefficients of the respective power series are equal.\nDerivation of (++)\nSub $x=r^2$ to get\n$$\\int_0^{\\infty} dr \\, r \\frac{J_0(2 u r)}{1+4 r^2}$$\nNow write\n$$J_0(2 u r) = \\frac{1}{2 \\pi} \\int_0^{2 \\pi} d\\theta \\, e^{i 2 u r \\cos{\\theta}}$$\nso that we now have as the integral\n$$\\frac{1}{2 \\pi} \\int_0^{2 \\pi} d\\theta \\, \\int_0^{\\infty} dr \\, r \\frac{e^{i 2 u r \\cos{\\theta}}}{1+4 r^2}$$\nNote that we may simply convert back to rectangular coordinates to get\n$$\\frac{1}{2 \\pi} \\int_{-\\infty}^{\\infty} dx \\, e^{i 2 u x} \\, \\int_{-\\infty}^{\\infty} \\frac{dy}{1+4 x^2+4 y^2} $$\nThe inner integral is simple, so we are back to a single integral:\n$$\\frac18 \\int_{-\\infty}^{\\infty} dx \\frac{e^{i 2 u x}}{\\sqrt{1+4 x^2}} $$\nBy subbing $x=\\frac12 \\sinh{t}$ and using the definition\n$$K_0(u) = \\int_0^{\\infty} dt \\, \\cos{(u \\, \\sinh{t} )}$$\nwe obtain the stated result.", "meta": {"post_id": 576903, "input_score": 31, "output_score": 40, "post_title": "Integral $\\int_0^\\infty{_1F_2}\\left(\\begin{array}{c}\\tfrac12\\\\1,\\tfrac32\\end{array}\\middle|-x\\right)\\frac{dx}{1+4\\,x}$"}}
{"input": "Question:\nlet $a_{i}>1,i=1,2,3,\\cdots,n$,and such $a_{i}\\neq a_{j}$,for any $i\\neq j$\ndefine the matrix \n\n$$A=\\left(\\dfrac{1}{\\ln{(a_{i}+a_{j})}}\\right)_{n\\times n}$$\n\nshow that:\n$$\\det(A)\\neq 0$$\nMy try: I know this matrix $A$ is similar this Cauchy determinants\uff1a http://en.wikipedia.org/wiki/Cauchy_matrix\nand \n\n$$\\det(A)=\\begin{vmatrix}\n\\dfrac{1}{\\ln{(a_{1}+a_{1})}}&\\dfrac{1}{\\ln{(a_{1}+a_{2})}}&\\cdots&\\dfrac{1}{\\ln{(a_{1}+a_{n})}}\\\\\n\\dfrac{1}{\\ln{(a_{2}+a_{1})}}&\\dfrac{1}{\\ln{(a_{2}+a_{2})}}&\\cdots&\\dfrac{1}{\\ln{(a_{2}+a_{n})}}\\\\\n\\cdots&\\cdots&\\cdots&\\cdots\\\\\n\\dfrac{1}{\\ln{(a_{n}+a_{1})}}&\\dfrac{1}{\\ln{(a_{n}+a_{2})}}&\\cdots&\\dfrac{1}{\\ln{(a_{n}+a_{n})}}\n\\end{vmatrix}$$\n\nbut I can't,Thank you.and this problem is my frend ask me.\nthis is he ask me  is second problem .and I think  this problem is interesting.\nNow  this problem is up $21$. that's mean this problem is hard.I hope someone can solve it.Good luck!Thank you", "output": "For any $s > 0$, let $A(s) \\in M_{n\\times n}(\\mathbb{R})$ be the matrix with entries \n$$A(s)_{ij} = \\frac{1}{(a_i + a_j)^s}$$\nFor any non-zero $u \\in \\mathbb{R}^n$ with components $u_i, i = 1\\ldots n$, we have\n$$u^T\\!A(s)\\,u = \\sum_{1\\le i,j \\le n} \\frac{u_i u_j}{(a_i+a_j)^s} = \\sum_{i\\le i,j \\le n} \\frac{u_i u_j}{\\Gamma(s)}\\int_0^\\infty t^{s-1} e^{-(a_i+a_j)t} dt\\\\\n= \\frac{1}{\\Gamma(s)}\\int_0^\\infty t^{s-1} \\left(\\sum_{i=1}^n u_i e^{-a_i t} \\right)^2 dt > 0\n$$\nbecause $u_i$ not all zero implies as a function of $t$, $\\displaystyle \\sum_{i=1}^n u_i e^{-a_i t}$ not identically zero$\\color{blue}{^{[1]}}$.\nNotice for $x > 1$, $\\frac{1}{\\log x}$ can be expressed as an absolute convergent integral:\n$$\\frac{1}{\\log x} = \\int_0^{\\infty} \\frac{1}{x^s} ds$$\nAs a result, we have\n$$u^T A u = \\sum_{1 \\le i, j \\le n} \\frac{u_i u_j}{\\log (a_i+a_j)}\n= \\sum_{1 \\le i, j \\le n } \\int_0^\\infty \\frac{u_i u_j}{(a_i+a_j)^s} ds\n= \\int_0^\\infty u^T\\!A(s)\\,u\\;ds > 0$$\nThis implies $A$ is positive definite and hence invertible.\nNotes\n\n$\\color{blue}{[1]}$ To justify $\\displaystyle f(t) = \\sum_{i=1}^n u_i e^{-a_i t}$ not identically zero, we need to use the fact $a_i$ are all distinct. If $f(t)$ vanishes on\n$t = 1, \\ldots, n$, then we can construct a Vandermonde matrix with entries $e^{-a_i j}, 1 \\le i, j \\le n$ and use it to\nconclude all $u_i = 0$.", "meta": {"post_id": 577197, "input_score": 31, "output_score": 35, "post_title": "How prove this matrix $\\det (A)=\\left(\\frac{1}{\\ln{(a_{i}+a_{j})}}\\right)_{n\\times n}\\neq 0$"}}
{"input": "I'm reading the book (by A. Kostrikin) on linear algebra and I feel like I'm really missing something about this idea.\nI understand the formal proofs of: a) isomorphism between vector space $V$ and its dual space $V^*$ b) natural isomorphism between $V$ and $V^{**}$.\nWhat I don't understand is the idea behind it:\n1) Why is it so important for isomorphism to be \"natural\"? Does it make it  better, more usable is some way, or?..\n2) It is said later in the book, that established natural isomorphism between $V$ and $V^{**}$ makes them \"totally equal\" (my translation of Russian \"\u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u043e \u0440\u0430\u0432\u043d\u043e\u043f\u0440\u0430\u0432\u043d\u044b\u0435\"). But wait, isn't it enough to establish any kind of isomorphism between two sets to make them \"equal\"? What is it in natural isomorphism that makes the sets more equal than with any other isomorphism?\n3) Is there a way to strictly define natural isomorphism (at least for finite vector spaces) without using category theory?\nThank you!", "output": "3) Of course you can define natural isomorphisms without using the language of category theory, but category theory was (in part) invented in order to efficiently express this notion, so there is little reason to (apart from trying to improve understanding).\n2a) Philosophically, what makes an isomorphism between two objects natural is that constructing the isomorphism does not require more information than constructing the objects.  \nFor example, in order to construct $V^*$ or $V^{**}$, it is enough to know that $V$ is a vector space. What I mean is that in order to build the two sets $V^*=\\{f\\colon V\\to\\mathbb F\\mid f\\text{ is linear}\\}$, $V^{**}=\\{f\\colon V^*\\to\\mathbb F\\mid f\\text{ is linear}\\}$, and to give them the structure of vector spaces, you need know nothing more than what a vector space is (hence what \"linear\" is), and that $V$ is a vector space. \nMost likely in your book, to construct an injective linear map $\\phi\\colon V\\to V^*$, you used additional information about the vector space $V$: probably a choice of basis. For $\\phi$ to also be surjective, and hence an isomorphism, you need even more information: that the basis was finite*. \nOn the other hand, in order to construct the linear map $\\psi_V\\colon V\\to V^{**}$, however, you don't need any extra information other than that $V$ is a vector space. You simply define $\\psi_V(v)\\in V^{**}$ by specifying how the $\\psi(v)$ acts on elements $f\\in V^*$: you declare $\\psi_V(v)(f)=f(v)$. (To know that it is an isomorphism, you still need the extra information that $V$ is finite-dimensional, otherwise $\\psi_V$ is just injective).\n2b) Mathematically, the fact that an isomorphism is natural, i.e. does not depend on structural information beyond that contained in the objects, is captured by defining an isomorphism to be natural if its construction is preserved by structure-preserving (e.g. linear) maps.  This is most easily expressed using the language of category theory (since the language of category theory was invented to express this).\nConcretely, you can see that the isomorphism between $V$ and $V^*$ is not natural since the isomorphism based on $\\{e_1,\\dots,e_n\\}$ likely defines the dual basis $e_i^*(\\sum a_je_j)=a_i$. But all this is doing is defining an inner-product (non-degenerate bilinear form if the base field $\\mathbb F\\neq\\mathbb R$) $\\left<\\cdot,\\cdot\\right>$ on $V$ such that $\\left<e_i,e_j\\right>=\\delta_{ij}=\\begin{cases}0&i\\neq j\\\\1&i=j\\end{cases}$. The basis (when $\\mathbb F=\\mathbb R$) allows us to identify $V$ as our usual $n$-dimensional space with coordinates, the inner product becomes the dot product, and the isomorphism to $V^*$ sends a vector $\\vec v$ to the functional that projects orthogonally onto $\\vec v$. Then the only linear transformations of $V$ to itself that preserve the isomorphism to $V^*$ are the orthonormal transformations, ones that preserve the length and (unsigned) angle between vectors. All other transformations of $V$ to itself break the isomorphism, and hence we can conclude the isomorphism is not natural.\n1) The use of natural transformations is that they ensure that the maps you are constructing reflect genuine properties of the mathematical object, rather than being an artifact of the specific way in which you present the object, and hence a consequence of properties that the object \"in-and-of-itself\" does not have. \n*More generally, the extra information for $\\phi$ that you need is that of a bilinear form, which the basis allows you to define, and for $\\phi^{-1}$ you need the bilinear form to be non-degenerate, which is what finiteness of the basis allows you to build. However, there are other ways to build non-degenerate bilinear forms, e.g. $L^2$ inner products. \n\nEDIT: in response to comments, note that it makes no sense to talk about natural isomorphisms between \"unnatural constructions\". I said that philosophically, a natural construction is one that depends on no extra information. Mathematically, it is a functor: in addition to telling us how to construct a new object $F(V)$ out of an old object $V$, a natural construction also tells us how the construction behaves if we are given a structure-preserving map $f\\colon V\\to W$, by giving a structure-preserving map $F(f)$ between $F(V)$ and $F(W)$. \nThere are two types of natural constructions: covariant and contravariant, depending on whether $F(f\\circ g)=F(f)\\circ F(g)$ or $F(f\\circ g)=F(g)\\circ F(f)$. In particular, one shows the construction of $V^*$ is (contravariantly) natural by defining/constructing, for any $f\\colon V\\to W$, an $f^*\\colon W^*\\to V^*$ given by $f^*\\colon g\\mapsto g\\circ f$ for any $g\\colon W\\to\\mathbb F$, and showing that $(f_1\\circ f_2)^*(g)=g\\circ f_1\\circ f_2=(f_2^*\\circ f_1^*)(g)$.\nNow, a natural transformation between $I$ (the identity construction $I(V)=V$ and $I(f)=f$ are isomorphisms $\\phi_V\\colon I(V)\\to F(V)$ such that:\n$\n\\require{AMScd}\n\\begin{CD}\nI(V) @>{\\phi_V}>> F(V)\\\\\n@VfVV @VVF(f)V \\\\\nI(W) @>{\\phi_W}>> I(W)\n\\end{CD}$ for covariant $F$ or\n$\\begin{CD}\nI(V) @>{\\phi_V}>> F(V)\\\\\n@VfVV @AAF(f)A \\\\\nI(W) @>{\\phi_W}>> I(W)\n\\end{CD}$ for contravariant $F$.\nNow it is easy to see that there is no natural isomorphism between $V$ and $V^*$. For suppose that there were, so that we have \n$\\begin{CD}\nV @>{\\phi_V}>> V^*\\\\\n@VfVV @AAf^*A \\\\\nW @>{\\phi_W}>> W^*\n\\end{CD}$\nfor every $f\\colon V\\to W$. Then clearly, $\\phi_V^{-1}\\circ f^*\\circ\\phi_W\\circ f$ would be the identity, so $f^{-1}\\colon W\\to V$ would be given by $\\phi_V^{-1}\\circ f^*\\circ\\phi_W$. But not every linear map is an isomorphism (in particular, the $f(v)=0$ is a linear map that is not an isomorphism for any choice of $V$ and $W$), so contradiction.", "meta": {"post_id": 579739, "input_score": 37, "output_score": 37, "post_title": "Why it is important for isomorphism between vector space and its double dual space to be natural?"}}
{"input": "I've looked at this and it doesn't help because I don't know anything about SVD. Can someone dumb it down for me please?", "output": "Eigenvalues and eigenvectors of a matrix, say $A$, help us find subspaces which are invariant under $A$ (when $A$ is seen as a linear transformation). If $A$ is non-square, then $A:\\mathbb{R}^m\\rightarrow \\mathbb{R}^n$, where $m\\neq n$. Hence $Av=\\lambda v$ makes no sense, since $Av\\notin\\mathbb{R}^m$.", "meta": {"post_id": 583938, "input_score": 50, "output_score": 35, "post_title": "Do non-square matrices have eigenvalues?"}}
{"input": "I have a formula, which I have no idea how to solve, because I don't know that double vertical-line sign: $\\|{\\rm Ax} \\|$?\n$${\\rm x} \\ne 0 \\in \\Bbb R^n, \\quad 0 < m \\le \\frac {\\| {\\rm Ax} \\|} {\\| {\\rm x} \\|} \\le M, \\quad cond(A) \\le \\frac M m .$$\nWhat does it mean? How should I solve this?", "output": "Double bars (or sometimes even single bars) tend to denote a norm in Mathematics. Most likely, the double bars here are denoting the Euclidean norm. This is just the length of the vector. So for example, the vector (I shall write it horizontally for compactness) $(1,2,3)$ has length\n$$\n\\|(1,2,3) \\|=\\sqrt{1^2+2^2+3^2}=\\sqrt{14}\n$$\nand the vector\n$$\n\\|(3,-1,2) \\|=\\sqrt{3^2+(-1)^2+2^2}=\\sqrt{14}\n$$\nNotice that $A\\mathbf{x}$ is just a vector, so $\\|A\\mathbf{x}\\|$ is just the length of the vector. $\\|\\mathbf{x}\\|$ is just the length of $\\mathbf{x}$. So here you are looking for scaling of $\\mathbf{x}$ under transformation by $A$ to be between $m$ and $M$. (Look at $\\frac{\\|A\\mathbf{x}\\|}{\\|\\mathbf{x}\\|}$ and think about what it means 'pictorially' to see what I am talking about).", "meta": {"post_id": 586245, "input_score": 51, "output_score": 63, "post_title": "What does double vertical-line means in linear algebra?"}}
{"input": "This may be a trivial question yet I was unable to find an answer:\n$$\\left \\| A \\right \\| _2=\\sqrt{\\lambda_{\\text{max}}(A^{^*}A)}=\\sigma_{\\text{max}}(A)$$\nwhere the spectral norm $\\left \\| A \\right \\| _2$ of a complex matrix $A$ is defined as $$\\text{max} \\left\\{ \\|Ax\\|_2 : \\|x\\| = 1 \\right\\}$$\nHow does one prove the first and the second equality?", "output": "First of all,\n$$\\begin{align*}\\sup_{\\|x\\|_2 =1}\\|Ax\\|_2 & = \\sup_{\\|x\\|_2 =1}\\|U\\Sigma V^Tx\\|_2 = \\sup_{\\|x\\|_2 =1}\\|\\Sigma V^Tx\\|_2\\end{align*}$$\nsince $U$ is unitary, that is, $\\|Ux_0\\|_2^2 = x_0^TU^TUx_0 = x_0^Tx_0 = \\|x_0\\|_2^2$, for some vector $x_0$.\nThen let $y = V^Tx$. By the same argument above, $\\|y\\|_2 = \\|V^Tx\\|_2 = \\|x\\|_2 = 1$ since $V$ is unitary.\n$$\\sup_{\\|x\\|_2 =1}\\|\\Sigma V^Tx\\|_2 = \\sup_{\\|y\\|_2 =1}\\|\\Sigma y\\|_2$$\nSince $\\Sigma = \\mbox{diag}(\\sigma_1, \\cdots, \\sigma_n)$, where $\\sigma_1$ is the largest singular value. The max for the above, $\\sigma_1$, is attained when $y = (1,\\cdots,0)^T$. You can find the max by, for example, solving the above using a Lagrange Multiplier.", "meta": {"post_id": 586663, "input_score": 59, "output_score": 44, "post_title": "Why does the spectral norm equal the largest singular value?"}}
{"input": "I'm studying summation. Everything I know so far is that:\n$\\sum_{i=1}^n\\ i = \\frac{n(n+1)}{2}\\ $\n$\\sum_{i=1}^{n}\\ i^2 = \\frac{n(n+1)(2n+1)}{6}\\ $\n$\\sum_{i=1}^{n}\\ i^3 = \\frac{n^2(n+1)^2}{4}\\ $\nUnfortunately, I can't find neither on my book nor on the internet what the result of:\n$\\sum_{i=1}^n\\log i$.\n$\\sum_{i=1}^n\\ln i$.\nis.\nCan you help me out?", "output": "By using the fact that $$\\log a + \\log b = \\log ab $$ then\n$$ \\sum^n \\log i = \\log (n!) $$\n$$ \\sum^n \\ln i = \\ln (n!) $$", "meta": {"post_id": 589027, "input_score": 19, "output_score": 41, "post_title": "What's the formula to solve summation of logarithms?"}}
{"input": "The mean value theorem for holomorphic functions states that if $f$ is analytic in $D$ and $a \\in D$, then $f(a)$ equals the integral around any circle centered at $a$ divided by $2\\pi$.  But if $f$ is analytic, then the line integral around any closed curve is 0, so $f(a) = 0$.  Why does the MVT not result in all holomorphic functions being identically zero?  There must be something I'm missing here.", "output": "The difference is that for the mean value property, we consider the integral with the measure/form $d\\varphi$, and for the integral theorem, the measure/form is $dz$.\nThe mean value property is actually the Cauchy integral formula for the centre of a disk:\n$$\\begin{align}\nf(a) &= \\frac{1}{2\\pi i} \\int_{\\lvert z-a\\rvert = r} \\frac{f(z)}{z-a}\\,dz\\\\\n&= \\frac{1}{2\\pi i} \\int_0^{2\\pi} \\frac{f(a+re^{i\\varphi})}{(a+re^{i\\varphi})-a}\\, d(a+re^{i\\varphi})\\\\\n&= \\frac{1}{2\\pi i} \\int_0^{2\\pi} \\frac{f(a+re^{i\\varphi})}{re^{i\\varphi}}rie^{i\\varphi}\\,d\\varphi\\\\\n&= \\frac{1}{2\\pi} \\int_0^{2\\pi} f(a+re^{i\\varphi})\\,d\\varphi.\n\\end{align}$$\nFor a parametrised circle with centre $a$, we have $d\\varphi = \\dfrac{dz}{i(z-a)}$, so the integral of $d\\varphi$ over a circle does not vanish, while the integral of $dz$ does.", "meta": {"post_id": 595395, "input_score": 33, "output_score": 50, "post_title": "Mean value theorem for holomorphic functions"}}
{"input": "1) Switching two rows or columns causes the determinant to switch sign\n2) Adding a multiple of one row to another causes the determinant to remain the same\n3) Multiplying a row as a constant results in the determinant scaling by that constant.\nUsing the geometric definition of the determinant as the area spanned by the columns of the matrix, could someone give me a geometric interpretation of the above theorems? Thanks.", "output": "Row operations can be thought of as acting on the ENTIRE space by reflection, shearing, or dilation.\nFor 2, have you heard of Cavalieri's principle? It says that shearing things while holding each cross section steady maintains volume.\nFor 3, you are just stretching or shrinking along each axis.\nFor 1, you are just reflecting in the plane $x_i=x_j$.\nEdit: Every row operation is the effect of multiplying on the left by an elementary matrix. We can think of this elementary matrix as a map from $R^n$ to $R^n$, which changes every vector in $R^n$ including the column vectors. Thus, each row operation corresponds to a way of changing the whole space.\nThe row operation in 1 interchanges two rows. This corresponds to interchanging two coordinates in the space. It is not obvious, but it has been shown that interchanging two coordinates is the same thing as reflecting the entire space around the subset where the two coordinates are equal. This does not change volume.\nThe row operation in 3 corresponds to stretching one coordinate by the multiple given, which multiplies volume by the same amount.\nThe operation in 2 can be thought of as follows: say that you add a multiple of the second row to the first. Imagine the space sliced into 'pancakes', one for each value of the second coordinate. The map doesn't interchange pancakes, it just slides each pancake 'horizontally'. This doesn't change the volume of anything.", "meta": {"post_id": 598219, "input_score": 61, "output_score": 49, "post_title": "Effect of elementary row operations on determinant?"}}
{"input": "Let $f:\\Bbb R\\backslash \\{1 \\} \\to \\Bbb R$ be defined by $f(x)= \\frac{1}{(1-x)}$. Use the $\\epsilon$-$\\delta$ definition to prove that $f$ is a continuous function. \nI do not need answers for it. I want your help to twist the questions a little bit with the goal to get another set of questions with more difficulty or similar difficulty and requires different tricks to solve the questions. Please provide the answers and explanation too.", "output": "The general case for polynomials isn't hard to understand, just cumbersome to write. I'll do the sketch of a particular example here, with the full explanation.\n\nFor all $a \\in \\Bbb R$: $$\\lim_{x \\to a} x^3 + 3x^2 - 5x + 1 = a^3 + 3a^2 - 5a + 1 $$\n\nSketch: we want to find $\\epsilon$ such that the difference between the polynomial and the limit is less than $\\epsilon$. If indeed the limit is right, $|x - a|$ always will be a factor of the difference. Let's find out here what we must impose on $\\delta$ to get what we need. We have: $$\\begin{align} |x^3 + 3x^2 - 5x + 1 - (a^3 + 3a^2 - 5a + 1)| &= |x^3 - a^3 + 3(x^2 - a^2) -5(x - a)| \\\\ &= |(x - a)(x^2 + ax + a^2) + 3(x-a)(x+a) - 5(x-a)| \\\\ &= |(x^2 + ax + a^2) + 3(x+a) - 5||x - a| \\end{align}$$\nNow, stop and think. I need to bound the stuff that multiplies $|x - a|$. What inequalities do I know? Well, there's always the triangle inequality, so, why not? $$ \\leq \\left(|(x^2 + ax+ a^2)| + |3(x + a)| + |-5| \\right)|x - a|$$\nSeems a little better. But why not do it again? $$\\leq \\left( |x^2| + |a||x| + a^2 + 3|x| + 3|a| + 5 \\right)|x-a|$$\nNow, there is an issue. I just can't have $\\delta$ depending on $x$. It is varying! But, if I could bound $|x|$ somehow, my problem would be done! Yet, there is another inequality that is useful, and everyone should have it at hand: $$|x| - |a| \\leq |x - a| < \\delta $$\nwhich readily implies that $|x| < \\delta + |a|$. Well, certainly I could suppose that $\\delta$ is, let's say, less than $1$. Why is that? Given $\\epsilon > 0$, I must find $\\delta = \\delta(\\epsilon, a) > 0$ such that yadda yadda yadda (put the definition of the limit here). My point is: every other $\\tilde{\\delta} < \\delta$ will also work! So, suppose $\\delta < 1$. This gives $|x| < 1 + |a|$. Going back to our chain of inequalities, we obtain: $$\\begin{align} &\\leq \\left( (1 + |a|)^2 + |a|(1 + |a|) + a^2 + 3(1+|a|) + 3|a| + 5 \\right) |x - a| \\\\ &= \\left(8 + 9|a| + 3a^2 \\right) |x - a| \\\\ &\\leq (8 + 9|a| + 3a^2) \\cdot \\delta \\end{align}$$\nNow, no matter how ugly it is, $8 + 9|a| + 3a^2$ is just a fixed number! Notice that all of those expression will be quite simple, once given a value for $a$. Now, what should  $\\delta$ be, if I wanted to give one more step? $$(8+ 9|a| + 3a^2) \\cdot \\delta < \\epsilon$$\nCertainly $\\delta < \\frac{\\epsilon }{8 + 9|a| + 3a^2}$ will do the job! However, we must heed some care here. What else have we supposed on our way to get here? That $\\delta < 1$ also, no? If I want everything to hold, I must choose: $$\\delta <\\min \\bigg\\{1, \\frac{\\epsilon}{8 + 9|a| + 3a^2} \\bigg\\} $$\nAnd we should never forget, a more formal proof would begin with:\n\nLet $\\epsilon > 0$, and $x \\in \\Bbb R$ such that $|x - a| < \\delta $. Choose $\\delta \\leq \\min \\big\\{ 1, \\frac {\\epsilon}{8 + 9|a| + 3a^2} \\big\\}$. Then we have $$| \\cdots - \\cdots| \\leq \\cdots \\leq \\cdots \\leq \\cdots \\leq \\epsilon $$\n\nHere, these $\\cdots$ are just the inequalities I did on the sketch. \n\nAn observation that I deem important: what if we had more than one factor $|x - a|$? Should we bound all of them with $\\delta $? My answer to that is a definite no. We certainly don't want to work with $\\sqrt{\\epsilon}, \\sqrt[5]{\\epsilon}$, and stuff like that. You bound only one of the factors $|x - a|$ with $\\delta$, and the other ones you deal with using the triangle inequality + $|x| < 1 + |a|$ combo. Example:\n\nProve that $\\lim_{x \\to 1} x^2 - 2x + 1 = 0$ using the $\\epsilon - \\delta$ definition. \n\nSketch: we have $|x - 1| < \\delta$, and we want $|x^2 - 2x + 1| < \\epsilon$. Suppose $\\delta < 1 $. This way, $|x| < \\delta + 1 < 2$. Finally: $$\\begin{align} |x^2 - 2x + 1| &= |(x-1)^2| \\\\ &= |x - 1||x - 1| \\\\ &\\leq (|x| + 1) \\cdot \\delta \\\\ &\\leq 3 \\delta \\end{align}$$\nSo $\\delta <\\min \\big\\{ 1, \\frac{\\epsilon}{3} \\big\\}$ is the solution to this problem. \n\nThis strategy works beautifully for polynomials in several variables. The only issue is that the calculations become insane, the probability of making an arithmetic mistake is just too damn high, which makes almost inviable to put this in action. Fortunately we have theorems for continuity saving our lives here. The idea would be the same, limiting all the variables one by one, as in: $$|x_1| < \\delta + |a_1| \\\\ |x_2| < \\delta + |a_2| \\\\ {}{}{}\\vdots \\\\ |x_n| < \\delta + |a_n| $$\n\nJust to end my answer here, I'll leave my opinion... these kinds of exercises, although easily generalised, are something every student should do at least once in his/her life, to get used to this type of inequality manipulation. I think this question should have received more attention, and if I find/remember other limits that I feel comfortable enough to explain, maybe I'll come back and update this answer.\nCheers.", "meta": {"post_id": 598796, "input_score": 24, "output_score": 35, "post_title": "Creating a question that use the $\\epsilon$-$\\delta$ definition to prove that $f$ is a continuous function"}}
{"input": "I have a question about something I'm wondering about. I've read somewhere that\nL'Hopitals rule can also be applied to complex functions, when they are analytic.\nSo if have for instance:\n$$\n\\lim_{z \\rightarrow 0} \\frac{\\log(1+z)}{z} \\stackrel{?}{=} \\lim_{z \\rightarrow 0} \\frac{1}{(1+z)} = 1\n$$\nNow i'm wondering if this is correct? Also if we take $|z|<1$, is it then correct?\nThanks,", "output": "L'Hopital's rule is a local statement: it concerns  the behavior of functions near a particular point. The global issues (multivaluedness, branch cuts) are irrelevant. For example, if you consider $\\lim_{z\\to 0}$, then it's automatic that only small values of $z$ are in play.  Saying \"take $|z|<1$\" is redundant.\nGenerally, you have a point $a\\in\\mathbb C$ and some neighborhood of $a$ in which   $f,g$ are holomorphic. If $f(a)=g(a)=0$, then \n$$\\lim_{z\\to a}\\frac{f(z)}{z-a}=f'(a),\\qquad \\lim_{z\\to a}\\frac{g(z)}{z-a}=g'(a) \\tag{1}$$\nhence \n$$\\lim_{z\\to a}\\frac{f(z)}{g(z)}= \\lim_{z\\to a}\\frac{f(z)/(z-a)}{g(z)/(z-a)} =\\frac{f'(a)}{ g'(a)}$$\nNote that the above is a  simple special case of  the L'Hopital's rule, because we have (1). It's basically just the definition of derivative.", "meta": {"post_id": 602650, "input_score": 52, "output_score": 74, "post_title": "Is L'Hopitals rule applicable to complex functions?"}}
{"input": "What does it mean for a function to be well-defined?\nI encountered this term in an exercise asking to check if a linear transformation is well-defined.", "output": "All functions are well-defined; but when we define a function, we don't always know (without doing some work) that our definition really does give us a function. We say the function (or, more precisely, the specification of the function) is 'well-defined' if it does.\nThat is, $f : A \\to B$ is well-defined if for each $a \\in A$ there is a unique $b \\in B$ with $f(a)=b$.\nThis often comes up when defining functions in terms of representatives of equivalence classes, or in terms of how an element of the domain is written. For example, the 'function' $f : \\mathbb{Z} \\to \\mathbb{Z}$ defined by\n$$f(n) = \\text{the first digit of the decimal expansion of}\\ n\\ \\text{after the decimal point}$$\nis not a well-defined function: we get $f(1)=0$ and $f(0.999\\dots)=9$, even though $0.999\\dots = 1$. We could turn it into a well-defined function by saying that the chosen decimal expansion must not have recurring $9$s.", "meta": {"post_id": 606917, "input_score": 45, "output_score": 44, "post_title": "\"Well defined\" function - What does it mean?"}}
{"input": "I learned that every compact set is closed and bounded; and also that an open set is usually not compact. \nHow to show that a concrete open set, for example the interval $(0,1)$, is not compact? I tried to show that $(0,1)$ has no finite sub cover.", "output": "I think the following may be a source of confusion: the statement \"$(0,1)$ has no finite sub cover\" doesn't make any sense.  You first have to choose a cover of $(0,1)$ by open sets.  Then this may or may not have a finite sub-cover. \nIf $(0,1)$ were compact, any such cover would (by the definition of compact in terms of open covers) have to have a finite subcover.  In fact, $(0,1)$ is not compact, and so what this means is that we can find some cover of $(0,1)$ by open sets which does not have a finite subcover.\nOmnomnomnom gives an example of such a cover in their answer, and there are lots of others; here's one: the cover $\\{U_2, \\ldots, U_n , \\ldots\\}$ where $U_n = (1/n, 1-1/n).$\nHere is a cover which is finite, and hence does have a finite subcover (namely, itself): $\\{(0,1)\\}$.\nHere is another: $\\{(0,1/2), (1/3,1)\\}$.\nHere is an infinite cover which admits a finite subcover $\\{(0,1), (0,1/2), \n\\ldots, (0,1/n) , \\ldots \\}$.\nHopefully these examples help to clarify what the definition of compactness in terms of finite subcovers is about.", "meta": {"post_id": 609217, "input_score": 16, "output_score": 39, "post_title": "Why is an open interval not a compact set?"}}
{"input": "First of all, it is not a duplicate of those questions a la \"best textbooks for differential geometry\". What I want to ask is which papers would you recommend reading while taking (self learning) differential geometry courses? I could list (and find advises on) such papers in classical statistics, or Bayesian statistics for example. But I cannot find any info about which papers would someone recommend for differential geometry. The scope I assume would be some sufficiently general topic, not too narrow, so that material would be usefull for those eager to learn differential geometry more in depth.", "output": "From time to time, I teach a graduate seminar at the University of Washington called \"Research in Geometric Analysis.\"  It's aimed at second-year and third-year math PhD students who have already taken at least a year's worth of graduate courses in differential geometry. I give them this list, which is my subjective list of \"greatest hits\" in differential geometry over the past 50 years or so (current as of 2005, the last time I taught the course). \nYou'll be surprised how difficult it is to read mathematics research papers.  My students -- even the best prepared ones -- typically tell me that seminar is the hardest course they've ever taken. As you probably already know, mathematics is more cumulative than any other subject, so to understand any research paper demands an understanding and appreciation of lots of background and context.  And most math papers aren't very easy to follow even for experts, since they're written when the ideas are still new and haven't had time to be cleaned up and made user-friendly.  \nIf you're really serious about familiarizing yourself with the research literature, my suggestion would be to pick one of the \"clusters\" of three or four related papers on this list that interests you, and start skimming them.  You'll quickly run into stuff you don't understand, so you'll have to look at the references in the bibliography and go read them.  Then the same thing will happen.  Keep working backwards until you start getting a sense of the mathematical context in which the paper was written, and then start reading the original paper again.  Through a series of \"successive approximations\" like this, you'll eventually start understanding what's going on.  \nI don't mean to be discouraging, but this process is very hard, even for students who are enrolled in graduate programs and get lots of mentoring and support. So if you can do it successfully by self-study, my hat's off to you!", "meta": {"post_id": 610838, "input_score": 24, "output_score": 45, "post_title": "Reading list of differential geometry **papers**"}}
{"input": "I have a classmate learning algebra.He ask me how to compute the de Rham cohomology of the punctured plane  $\\mathbb{R}^2\\setminus\\{0\\}$ by an elementary way,without homotopy type,without Mayer-Vietoris,just by Calculus. I have tried and failed.Is it possible to compute the de Rham cohomology just by Calculus?", "output": "Let $M = \\mathbb{R}^2 \\setminus \\{0\\}$. We have a global coordinates $(x, y)$ on $M$. We want to compute the cohomology of the complex\n$$0 \\to \\Omega^0(M)\\ \\stackrel{d}{\\to}\\ \\Omega^1(M)\\ \\stackrel{d}{\\to}\\ \\Omega^2(M)\\ \\to 0$$\nwhere $\\Omega^k(M)$ is the space of smooth $k$-forms on $M$. So we want to compute $H^k(M) = Z^k(M)/B^k(M)$ where $Z^k(M) = \\{\\alpha \\in \\Omega^k(M),\\, d\\alpha = 0\\}$ (closed $k$-forms) and $B^k(M) = \\{d\\beta,\\, \\beta \\in \\Omega^{k-1}(M)\\}$ (exact $k$-forms). In this situation,\n\nElements of $\\Omega^0(M)$ are just smooth functions $f(x,y)$ on $M$\nElements of  $\\Omega^1(M)$ can be written $u(x,y) dx + v(x,y) dy$ (where $u$ and $v$ are smooth functions on $M$)\nElements of $\\Omega^2(M)$ can be written $g(x,y) dx \\wedge dy$ (where $g$ is a smooth function on $M$)\n\n\nCompute $H^0(M)$:\n$B^0(M) = \\{0\\}$ and $Z^0(M)$ consists of functions $f(x,y)$ such that $df = 0$. Since $M$ is connected, this implies that $f$ is constant. It follows that $H^0(M)$ is isomorphic to $\\mathbb{R}$.\n\nCompute $H^1(M)$:\nThis is the where the all the fun happens :)\nLet $\\alpha \\in Z^1(M)$, this means that $\\alpha = u(x,y) dx + v(x,y) dy$ with $\\frac{\\partial v}{\\partial x} - \\frac{\\partial u }{\\partial y} = 0$.\n\nLemma 1: Let $R$ be a closed rectangle in $\\mathbb{R}^2$ which does not contain the origin. Then $\\int_{\\partial R} \\alpha = 0$.\n\nProof: This is just Green's theorem (or Stokes' theorem) (in its most simple setting, where it's not hard to show directly).\n\nLemma 2: Let $R$ and $R'$ be closed rectangles in $\\mathbb{R}^2$ whose interiors contain the origin. Then $\\int_{\\partial R} \\alpha = \\int_{\\partial R'} \\alpha$.\n\nProof: This is a consequence of Lemma 1. It might be a bit tedious to write down (several cases need to be addressed, according to the configuration of the two rectangles), but it's fairly easy. You need to cut and rearrange integrals along a bunch of rectangles so that the two initial integrals agree up to integrals along rectangles who do not contain the origin.\nLet's denote by $\\lambda(\\alpha)$ the common value of all integrals $\\int_{\\partial R} \\alpha$ when $R$ is a closed rectangle whose interior contains the origin.\n\nLemma 3: If $\\alpha$ is exact iff $\\lambda(\\alpha) = 0$.\n\nProof: \"$\\Rightarrow$\" is trivial1, let's prove the converse. Fix permanently some point $m_0 \\in M$, whichever you like best. For any $m\\in M$, consider a rectangle $R$ in $\\mathbb{R}^2$ whose boundary contains $m_0$ and $m$ but avoids the origin. Let $\\gamma$ be one of the two paths joining $m_0$ and $m$ along $\\partial R$. Let $f(m) = \\int_\\gamma \\alpha$. Since $\\lambda(\\alpha) = 0$, this definition does not depend on the choice of the rectangle or the path. In other words $f: M \\rightarrow \\mathbb{R}$ is well defined. Let's see that $df = \\alpha$. Check that $\\frac{f(x+h, y) - f(x,y)}{h} = \\frac{1}{h}\\int_x^{x+h} u(t,y) dt$, so that taking the limit when $h\\rightarrow 0$ yields $\\frac{\\partial f}{\\partial x} = u$. Same for $v$.\n\nFinally let's consider the $1$-form $d\\theta = \\frac{-ydx + xdy}{x^2 + y^2}$. NB: Be well aware that $d\\theta$ is a misleading (but standard) notation: it is not an exact form.\n\nLemma 4: $d\\theta$ is a closed $1$-form and $\\lambda(d\\theta) = 2\\pi$.\n\nProof: This is a direct computation.\nNow're done:\n\nProposition: $H^1(M)$ is the one-dimensional vector space spanned by $[d\\theta]$.\n\nwhere $[d\\theta]$ denotes the class of $d\\theta$ in $H^1(M)$. Note that $[d\\theta] \\neq 0$: see Lemma 4 and 3.\nProof: Let $\\alpha$ be a closed $1$-form. Consider $\\beta = \\alpha - \\frac{\\lambda(\\alpha)}{2 \\pi} d\\theta$. We have $\\lambda(\\beta) = 0$ so $\\beta$ is exact by Lemma 3. This proves that $[\\alpha] = \\frac{\\lambda(\\alpha)}{2 \\pi} [d\\theta]$.\n\nCompute $H^2(M)$:\nLet's show that $H^2(M) = 0$, in other words every closed $2$-form on $M$ is exact. This solution is taken from Ted Shifrin in the comments below.\nHere's the idea: in polar coordinates, a $2$-form $\\omega$ can be written $\\omega = f(r,\\theta) dr \\wedge d\\theta$. Then $\\eta = (\\int_1^r f(\\rho, \\theta) d\\rho)\\, d\\theta$ is a primitive of $\\omega$.\nAlthough it's not very insightful, this can be checked by a direct computation without refering to a change of variables. \nLet $\\omega(x,y) = g(x,y) dx \\wedge dy$. Define $$h(x,y) = \\int_1^{\\sqrt{x^2+y^2}} t\\, g\\left(\\frac{tx}{\\sqrt{x^2+y^2}}, \\frac{ty}{\\sqrt{x^2+y^2}}\\right) \\,dt$$\nCheck that $h(x,y) d\\theta$ is a primitive of $\\omega$.\nNB: Any course / book / notes on de Rham cohomology will show that if $M$ is a connected compact orientable manifold, $H^n(M) \\approx \\mathbb{R}$. However, I don't think I've read anywhere that when $M$ is not compact, $H^n(M) = 0$. I wonder if there is an \"elementary\" proof.\n\n1 I'll explain this by request of OP. It is a general fact that if $\\gamma : [a, b] \\rightarrow M$ is a ${\\cal C}^1$ path and $\\alpha$ is a smooth exact one-form i.e. $\\alpha = df$ where $f$ is a smooth function, then $\\int_\\gamma \\alpha = f(\\gamma(b)) - f(\\gamma(a))$. This is because $\\int_\\gamma \\alpha = \\int_a^b \\alpha(\\gamma(t))(\\gamma'(t))\\,dt$ (by definition) and here $\\alpha(\\gamma(t))(\\gamma'(t)) = df(\\gamma(t))(\\gamma'(t)) = f'(\\gamma(t)) \\gamma'(t) = \\frac{d}{dt}\\left(f(\\gamma(t))\\right)$. \nThis fact extends to piecewise ${\\cal C}^1$ maps by cutting the integral. In particular, is $\\gamma$ is a closed piecewise ${\\cal C}^1$ path and $\\alpha$ is exact, then $\\int_\\gamma \\alpha = 0$.\nIn fact, it is useful (e.g. for Cauchy theory in complex analysis) to know that\n\nA one-form is closed iff its integral along any homotopically trivial loop is zero (any boundary of a rectangle contained in the open set is enough).\nA one-form is exact iff its integral along any loop is zero.\n\nNote that the \"smooth\" condition can be weakened.", "meta": {"post_id": 612837, "input_score": 34, "output_score": 56, "post_title": "how to compute the de Rham cohomology of the punctured plane just by Calculus?"}}
{"input": "I am reading baby Rudin and it says all ordered fields with supremum property are isomorphic to $\\mathbb R$. Since all ordered  finite fields would have supremum property that must mean none exist. Could someone please show me a proof of this?\nThank you very much, Regards.", "output": "HINT: Suppose that $(F,0,1,+,\\cdot,<)$ is an ordered field which is finite of characteristic $p$. Then $0<1<1+1<\\ldots$, conclude a contradiction.", "meta": {"post_id": 616828, "input_score": 17, "output_score": 36, "post_title": "A finite field cannot be an ordered field."}}
{"input": "I have seen multiple definitions for what a measurable set is (all of which come together to form a sigma algebra). I was wondering if they are all equivalent and if not what situation would one be used over another?\n\nDefinition 1\n\n\nLet $(X, \\Sigma)$ be a measurable space, then any set $S \\in \\Sigma$ is a measurable set.\n\nMeasurable Space: The pair $(X, \\Sigma)$ where $X$ is a set and $\\Sigma$ is a $\\sigma$-algebra on $X$\n\n\n\nDefinition 2\n\n\nGiven a space $X$ let there exist an outer measure $\\mu : 2^{X} \\to [0, \\infty]$ (where $2^{X} = \\mathcal{P} \\left( X \\right) = $ all the subsets of $X$) then a set $S$ is measurable iff for every $A \\in 2^{X}$\n$$\n\\mu(A) = \\mu(A \\cap S) + \\mu(A \\cap S^{c}) = \\mu(A \\cap S) + \\mu(A \\setminus S)\n$$\n\n\nDefinition 3 (I'm drawing this one from memory from baby rudin, and it's defined on $\\mathbb{R}$)\n\n\nBegin by defining a (outer$_1$) measure $\\mu$. Next put $\\mathcal{M}_f$ to be the set of countable unions of intervals. Then a set $S$ is measurable iff\n$$\n\\exists \\{ S_n \\}_{n=0}^{\\infty} \\, s.t. \\mu(S_n) \\to \\mu(S) \\text{ as } n \\to \\infty\n$$\n\nNow I know that the sets described in definition 2 form a sigma algebra on $X$, and likewise I know the sets $S$ in definition 3 form a sigma algebra on $\\mathbb{R}$, but definition 1 seems to imply that any possible sigma algebra can be used.\nIn definition 3 I cannot remember if he defines this using the outer measure (I believe he does).\nThe only conclusion I have been able to draw from this is definition 2 and 3 must be equivalent, but we call these sets $\\boldsymbol\\mu$-measurable (a specific measure is defined). However in definition 1 we call any set like that just measurable, and that given any sigma algebra $\\Sigma \\, \\exists \\mu$ an outer measure s.t. the sigma algebra formed by definition 2 with this outer measure is $\\Sigma$. Is this correctly put?", "output": "There is no universal and objective definition of what is a measurable subset of a general space $X$.\nThe general concept of a measurable subset has its origins in the problem of measure in Euclidean space:\n\nProblem of measure: Given an object $A\\subset\\mathbb R^n$,\n  how does one assign a measure $m(A)\\in[0,\\infty]$ to $A$? (In the case $n=1,2$ and $3$,\n  the measure $m(A)$ is traditionally referred to as the length,\n  the area, and the volume of $A$, respectively).\n\nWhen the objects considered are very simple,\nthis question is very easy to answer.\nFor example, given a line segment $A=[a,b]\\subset\\mathbb R$,\nthe measure of $A$ should obviously be of $m(A)=b-a$.\nGiven a $n$-dimensional rectangle\n$$A=[a_1,b_1]\\times\\cdots\\times[a_n,b_n]\\subset\\mathbb R^n,$$\nthe answer is equally obvious:\n$$m(A)=\\prod_{i=1}^n(b_i-a_i).$$\nThen,\none can easily extend the measure of rectangle to slightly more general sets,\nsuch as disjoint unions of rectangles\n$$A=R_1\\dot\\cup\\cdots\\dot\\cup R_k$$\nby assigning\n$$m(A)=\\sum_{i=1}^km(R_i)$$\n(Indeed,\nfor a theory of measure to make any kind of geometrical sense,\nthe measure of a union of disjoint parts should be the sum of the measures of the constituent parts.)\nThe real problem comes when trying to measure more complicated subsets of $\\mathbb R^n$.\n\nA classical solution to the measure problem consists in attempting to approximate the measure of a complicated set using simple sets.\nMore precisely,\nsuppose we have a class of simple sets $S$ which we know how to measure (these would contain rectangles and finite unions of rectangle for example).\nThen,\ngiven some arbitrary set $A$,\nwe can define an inner measure $m_I(A)$ and an outer measure $m_O(A)$ of $A$ by letting\n$$m_I(A)=\\sup\\{m(E):E\\subset A,~E\\in S\\}\\text{ and }m_O(A)=\\inf\\{m(E):E\\supset A,~E\\in S\\}.$$\n(Note that\nthe inner and outer measures of sets in $S$ are clearly the same as the measure we have already assigned to them.)\nIn this framework,\none calls a set $A\\subset\\mathbb R^n$ measurable if $m_O(A)=m_I(A)$,\nin which case we assign $m(A)=m_O(A)=m_I(A)$.\nIn other words,\nwe call a set measurable if our theory of measure is capable of giving a sensible answer to \"what is the measure of $A$?\"\n\nThe solution of the measure problem I discussed in the previous paragraph gave rise to the Jordan theory of measure,\nas well as the more modern Lebesgue measure.\nThe concepts of outer measure and general measures you have written down in your questions are answers to generalizations of the problem of measure to arbitrary spaces $X$,\nwith the intent of extending the theory of Lebesgue integration to those spaces.\nThe exact axioms (i.e., the definitions of a $\\sigma$-algebra and measure)\nare in place in order to ensure that we obtain a theory of integration that is similar to the theory of Lebesgue measure/integration,\nthat is,\nwith similar theorems such as\ncountable subadditivity,\ncountinuity from above/below,\nmonotone/dominated convergence, etc.\n\nGoing into more detail would require a lot of explanation. If you'd like to know more, I personally recommend reading sections 1.1 - 1.4 of An introduction to measure theory by Terence Tao.\nI only really started to understand measure theory when I read it.", "meta": {"post_id": 618480, "input_score": 37, "output_score": 38, "post_title": "What is the definition of a measurable set?"}}
{"input": "How can an equation for the following curve be derived?\n \n$$r=(1+0.9 \\cos(8 \\theta)) (1+0.1 \\cos(24 \\theta)) (0.9+0.1 \\cos(200 \\theta)) (1+\\sin(\\theta))$$\n(From WolframAlpha)", "output": "You may do the same as other answer did in details by Maple:\n[> with(plots):\n  animate(polarplot, [(1+.9*cos(A*t))*(1+.1*cos(A*t))*(.9+0.5e-1*cos(A*t))*(1+sin(t)), t = -Pi .. Pi, thickness = 2], A = 0 .. 15);", "meta": {"post_id": 618786, "input_score": 35, "output_score": 46, "post_title": "Cannabis Equation"}}
{"input": "What is the remainder when $$1! + 2! + 3! +\\cdots+ 1000!$$ is divided by $12$.\nI have tried to find the answer using the Binomial Theorem but that doesn't help.\nHow will we do this?\nPlease help.", "output": "If $n\\ge 4$, then $4!=24$ divides $n!$ $-$ in particular $12$ divides $n!$ when $\\ge 4$. \nThus\n$$\n1!+2!+\\cdots+1000!=1!+2!+3! \\!\\!\\!\\!\\pmod{12}=9\\!\\!\\!\\!\\pmod{12}.\n$$", "meta": {"post_id": 618992, "input_score": 14, "output_score": 47, "post_title": "What is the remainder when $1! + 2! + 3! +\\cdots+ 1000!$ is divided by $12$?"}}
{"input": "What is the mean and variance of Squared Gaussian: $Y=X^2$ where: $X\\sim\\mathcal{N}(0,\\sigma^2)$?\nIt is interesting to note that Gaussian R.V here is zero-mean and non-central Chi-square Distribution doesn't work.\nThanks.", "output": "We can avoid using the fact that $X^2\\sim\\sigma^2\\chi_1^2$, where $\\chi_1^2$ is the chi-squared distribution with $1$ degree of freedom, and calculate the expected value and the variance just using the definition. We have that\n$$\n\\operatorname E X^2=\\operatorname{Var}X=\\sigma^2\n$$\nsince $\\operatorname EX=0$ (see here).\nAlso,\n$$\n\\operatorname{Var}X^2=\\operatorname EX^4-(\\operatorname EX^2)^2.\n$$\nThe fourth moment $\\operatorname EX^4$ is equal to $3\\sigma^4$ (see here). Hence,\n$$\n\\operatorname{Var}X^2=3\\sigma^4-\\sigma^4=2\\sigma^4.\n$$", "meta": {"post_id": 620045, "input_score": 39, "output_score": 41, "post_title": "Mean and variance of Squared Gaussian: $Y=X^2$ where: $X\\sim\\mathcal{N}(0,\\sigma^2)$?"}}
{"input": "It is well-known that the function\n$$f(x) = \\begin{cases} e^{-1/x^2}, \\mbox{if } x \\ne 0 \\\\ 0, \\mbox{if } x = 0\\end{cases}$$\nis smooth everywhere, yet not analytic at $x = 0$. In particular, its Taylor series exists there, but it equals $0 + 0x + 0x^2 + 0x^3 + ... = 0$, so while it has radius of convergence $\\infty$, it is not equal to $f$ even in a tiny neighborhood of $0$.\nThere is also a function\n$$f(x) = \\sum_{n=0}^{\\infty} e^{-\\sqrt{2^n}} \\cos(2^n x)$$\nwhich is smooth everywhere (that is, $C^{\\infty}$) yet analytic nowhere. In particular, the Taylor series at every point has radius of convergence $0$. In fact, \"most\" smooth functions are not analytic.\nBut this gets me wondering. Could there exist some function which is smooth everywhere, analytic nowhere, yet its Taylor series at any point has nonzero radius of convergence, and so converges to something, but that something is not the function, not even in a tiny neighborhood about the point of expansion? If yes, what is an example of such a function? If no, what is the proof that such a thing is impossible? And also, if no, what sort of restrictions exist on the convergence of the T.s.? At how many/what distribution of points can it converge to something which is not the function? I note that if we multiply together the two functions just given above, we have another smooth-everywhere, analytic-nowhere function, but this time at $0$ we have a convergent Taylor series (the same zero series as before -- just use the generalized Leibniz rule) which doesn't converge to the function in even a tiny neighborhood of $0$.\nEDIT (Dec 31, 2013): With some Googling I came across a post to mathoverflow:\nhttps://mathoverflow.net/a/81465\n\nThe Taylor series of the Fabius function at any dyadic rational actually has infinite radius of convergence (only finitely many terms are nonzero) but does not represent the function on any interval.\n\nSo it seems it is possible to have a function whose Taylor series converges to \"the wrong thing\" at a dense set of expansion points. But it still doesn't answer the question of whether that is possible for all expansion points on the entire real line.", "output": "No, this is not possible. Dave L. Renfro wrote an excellent historical Essay on nowhere analytic $C^\\infty$ functions in two parts (with numerous references). See here: 1 (dated May 9, 2002 6:18 PM), and 2 (dated May 19, 2002 8:29 PM).\nAs indicated in part 1, in\n\nZygmunt Zahorski. Sur l'ensemble des points singuliers d'une fonction d'une variable r\u00e9elle admettant les d\u00e9riv\u00e9es de tous les ordres, Fund. Math., 34, (1947), 183\u2013245. MR0025545 (10,23c); and Suppl\u00e9ment au m\u00e9moire \"Sur l'ensemble des points singuliers d'une fonction d'une variable r\u00e9elle admettant les d\u00e9riv\u00e9es de tous les orders\", Fund. Math., 36, (1949), 319\u2013320. MR0035329 (11,718a),\n\nZahorski suggested in 1947 the following classification of points where a function $f$ is $C^\\infty$ but not analytic:\n\nA point $a$ is a C-point (for Cauchy) iff the formal Taylor series about $a$ associated to $f$ converges in a neighborhood of $a$, but the resulting analytic function does not coincide with $f$ in any neighborhood of $a$.\nThe point $a$ is a P-point (for Pringsheim) iff the formal Taylor series of $f$ about $a$ has radius of convergence $0$.\n\n\nTheorem (Zahorski). Let $C,P$ be sets of real numbers. The following are equivalent:\n\n\n\n$C$ and $P$ are the sets of C-points and P-points, respectively, of some $C^\\infty$\nfunction $f:\\mathbb R\\to\\mathbb R$.\nThe following 4 conditions hold:\n\n\n\n$C$ is a first category $F_\\sigma$ set.\n$P$ is a $G_\\delta$ set.\n$C\\cap P=\\emptyset$.\n$C\\cup P$ is closed in $\\mathbb R$.\n\nAs a corollary, note that if $f:\\mathbb R\\to\\mathbb R$ is smooth, and its set of P-points is empty then, since no interval is first category (by the Baire category theorem), in every interval there must be points where $f$ is analytic.\nTwo other key references you may want to consult (also mentioned in Renfro's essay) are\n\nGerald Gustave Bilodeau. The origin and early development of nonanalytic infinitely differentiable functions, Arch. Hist. Exact Sci., 27 (2), (1982), 115\u2013135. MR0677684 (84g:26017),\n\nand\n\nHelmut R. Salzmann, and Karl Longin Zeller. Singularit\u00e4ten unendlich oft differenzierbarer Funktionen, Math. Z., 62 (1), (1955), 354\u2013367. MR0071479 (17,134b).\n\n(The latter contains a simplified proof of Zahorski's result.)\n(Coincidentally, last term I had the opportunity to cover some of the results in this area in my analysis class. See also MathOverflow, for a version of this question, and the related question of whether the set of P-points can be $\\mathbb R$.)", "meta": {"post_id": 620290, "input_score": 73, "output_score": 53, "post_title": "Is it possible for a function to be smooth everywhere, analytic nowhere, yet Taylor series at any point converges in a nonzero radius?"}}
{"input": "I've read in several places that one motivation for category theory was to be able to give precise meaning to statements like, \"finite dimensional vector spaces are canonically isomorphic to their double duals; they are isomorphic to their duals as well, but not canonically.\"\nI've finally sat down to work through this, and -\nOkay, yes, it is easy to see that the \"canonical isomorphism\" from $V$ to $V^{**}$ is a functor that has a natural isomorphism (in the sense of category theory) to the identity functor.\nAlso, I see that there is no way that the functor $V\\mapsto V^*$ could have a natural isomorphism to the identity functor, because it is contravariant whereas the identity functor is covariant. My question amounts to:\n\nIs contravariance the whole problem?\n\nTo elaborate: \nI was initially disappointed by the realization that the definition of natural isomorphism doesn't apply to a pair of functors one of which is covariant and the other contravariant, because I was hoping that the lack of a canonical isomorphism $V\\rightarrow V^*$ would feel more like a theorem as opposed to an artifact of the inapplicability of a definition.\nThen I tried to create a definition of a natural transformation from a covariant functor $F:\\mathscr{A}\\rightarrow\\mathscr{B}$ to a contravariant functor $G:\\mathscr{A}\\rightarrow\\mathscr{B}$. It seems to me that this definition should be that all objects $A\\in\\mathscr{A}$ get a morphism $m_A:F(A)\\rightarrow G(A)$ such that for all morphisms $f:A\\rightarrow A'$ of $\\mathscr{A}$, the following diagram (in $\\mathscr{B}$) commutes:\n$$\\require{AMScd}\\begin{CD}\nF(A) @>m_A>> G(A)\\\\\n@VF(f)VV @AAG(f)A\\\\\nF(A') @>>m_{A'}> G(A')\n\\end{CD}$$\nThis is much more stringent a demand on the $m_A$ than the typical definition of a natural transformation.  Indeed, it is asking that $m_A=G(f)\\circ m_{A'}\\circ F(f)$, regardless of how $f$ or $A'$ may vary. Taking $\\mathscr{A}=\\mathscr{B}=\\text{f.d.Vec}_k$, $F$ the identity functor and $G$ the dualizing functor, it is clear that this definition can never be satisfied unless $m_V$ is the zero map for all $V\\in\\text{f.d.Vec}_k$ (because take $f$ to be the zero map). In particular, it cannot be satisfied if $m_V$ is required to be an isomorphism.\n\nIs this the right way to understand (categorically) why there is no natural isomorphism $V\\rightarrow V^*$?\n\nAs an aside, are there any interesting cases of some kind of analog (the above definition or another) of natural transformations from covariant to contravariant functors?\nNote: I have read a number of math.SE answers regarding why $V^*$ is not naturally isomorphic to $V$. None that I have found are addressed to what I'm asking here, which is about how categories make the question and answer precise. (This one was closest.) Hence my question here.", "output": "Congratulations, you have reinvented the notion of a dinatural transformation (see for instance MacLane's Categories for the working mathematician, section IX.4). And your proof, that every dinatural transformation from the identity functor to the dualization functor is zero, is correct. And I agree that this is one (and perhaps the only) way to make precise that a f.d. vector space is not canonically isomorphic to its dual. By the way, for euclidean vector spaces, there is a canonical isomorphism, given by $V \\mapsto V^*, v \\mapsto \\langle v,- \\rangle$.\n1st Edit: In the comments, Mariano has suggested to restrict to isomorphisms as morphisms. This comes down to: If $n \\in \\mathbb{N}$, is there some $M \\in \\mathrm{GL}_n(K)$, such that for all $A \\in \\mathrm{GL}_n(K)$ we have $M = A^T \\cdot M \\cdot A$? By taking $A$ to be some diagonal matrix we immediately see that this is only possible for the trivial case $n=0$ or when $K=\\mathbb{F}_2$.\n2nd Edit: Let us look more closely at the case $K=\\mathbb{F}_2$. For $n=1$ we can take $M=(1)$. As mentioned by ACL (in Mariano's link in the comments), for $n=2$ we can take $M=\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$.\nThus, for $2$-dimensional $\\mathbb{F}_2$-vector spaces $V$ there is a canonical isomorphism $V \\cong V^*$ which is natural with respect to isomorphisms. It is induced by the unique(!) alternating $2$-form on $V$.\nFor $n=3$ this is not possible: By taking $\\small A=\\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$ it follows that $M_{11}=M_{13}=0$, and by taking $\\small A=\\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$ it follows $M_{12}=0$, so that $M$ is not invertible. A similar reasoning works for all $n \\geq 3$.", "meta": {"post_id": 622589, "input_score": 99, "output_score": 50, "post_title": "In categorical terms, why is there no canonical isomorphism from a finite dimensional vector space to its dual?"}}
{"input": "The residue at infinity is given by:\n$$\\underset{z_0=\\infty}{\\operatorname{Res}}f(z)=\\frac{1}{2\\pi i}\\int_{C_0} f(z)dz$$\nWhere $f$ is an analytic function except at finite number of singular points and $C_0$ is a closed countour so all singular points lie inside it.\nIt can be proven that the residue at infinity can be computed calculating the residue at zero.\n$$\\underset{z_0=\\infty}{\\operatorname{Res}}f(z)=\\underset{z_0=0}{\\operatorname{Res}}\\frac{-1}{z^2}f\\left(\\frac{1}{z}\\right)$$\nThe proof is just to expand $-\\frac{1}{z^2}f\\left(\\frac{1}{z}\\right)$ as a Laurent series and to see that the $1/z$ is the integral mentioned.\nI can see that we change $f(z)$ to $f(1/z)$ so the variable tends to infinity.\nBut, is there any intutive reason of why we introduce the $-1/z^2$ factor?", "output": "The thing is that functions do not have residues, but rather differentials have residues. This is something which can be quite confusing in a first complex analysis class. The \"residue of a function\" is not invariant under a change of local parameter, but the residue of a differential is. For this reason, what is usually called the \"residue at $0$ of $f(z)$\" is actually the residue at $0$ of $f(z)dz$.\nWhen you change the coordinate from $z$ to $w=1/z$, the differential $dz$ is transformed into $-dw/w^2$, which explains the change of sign and the extra factor. Thus, \n$$f(z)dz = \\frac{-1}{w^2} f(1/w) dw.$$\nThe \"residue of $f$ at $\\infty$\" is the residue at $0$ of $\\frac{-1}{w^2} f(1/w) dw$.", "meta": {"post_id": 629495, "input_score": 21, "output_score": 37, "post_title": "Intuition behind the residue at infinity"}}
{"input": "Let $V$ be a vector space with infinite dimensions. A Hamel basis for $V$ is an ordered set of linearly independent vectors $\\{ v_i \\ | \\ i \\in I\\}$ such that any $v \\in V$ can be expressed as a finite linear combination of the $v_i$'s; so $\\{ v_i \\ | \\ i \\in I\\}$ spans $V$ algebraically: this is the obvious extension of the finite-dimensional notion. Moreover, by Zorn Lemma, such a basis always exists.\nIf we endow $V$ with a topology, then we say that an ordered set of linearly independent vectors $\\{ v_i \\ | \\ i \\in I\\}$ is a Schauder basis if its span is dense in $V$ with respect to the chosen topology. This amounts to say that any $v \\in V$ can be expressed as an infinite linear combination of the $v_i$'s, i.e. as a series.\nAs far as I understand, if a $v$ can be expressed as finite linear combination of some set $\\{ v_i \\ | \\ i \\in I\\}$, then it lies in its span; in other words, if $\\{ v_i \\ | \\ i \\in I\\}$ is a Hamel basis, then it spans the whole $V$, and so it is a Schauder basis with respect to any topology on $V$.\nHowever Per Enflo has constructed a Banach space without Schauder basis (ref. wiki). So I guess I should conclude that my reasoning is wrong, but I can't see what's the problem.\nAny help appreciated, thanks in advance!\n\nUPDATE: (coming from the huge amount of answers and comments)\nForgetting for a moment the concerns about cardinality and sticking to span-properties, it has turned out that we have two different notions of linear independence: one involving finite linear combinations (Hamel-span, Hamel-independence, in the terminology introduced by rschwieb below), and one allowing infinite linear combinations (Schauder-stuff). So the point is that the vectors in a Hamel basis are Hamel independent (by def) but need not be Schauder-independent in general. As far as I understand, this is the fundamental reason why a Hamel basis is not automatically a Schauder basis.", "output": "People keep mentioning the restriction on the size of a Schauder basis, but I think it's more important to emphasize that these bases are bases with respect to different spans.\nFor an ordinary vector space, only finite linear combinations are defined, and you can't hope for anything more. (Let's call these Hamel combinations.) In this context, you can talk about minimal sets whose Hamel combinations generate a vector space.\nWhen your vector space has a good enough topology, you can define countable linear combinations (which we'll call Schauder combinations) and talk about sets whose Schauder combinations generate the vector space.\nIf you take a Schauder basis, you can still use it as a Hamel basis and look at its collection of Hamel combinations, and you should see its Schauder-span will normally be strictly larger than its Hamel-span.\nThis also raises the question of linear independence: when there are two types of span, you now have two types of linear independence conditions. In principle, Schauder-independence is stronger because it implies Hamel-independence of a set of basis elements.\nFinally, let me swing back around to the question of the cardinality of the basis. \nI don't actually think (/know) that it's absolutely necessary to have infinitely many elements in a Schauder basis. In the case where you allow finite Schauder bases, you don't actually need infinite linear combinations, and the Schauder and Hamel bases coincide. But definitely there is a difference in the infinite dimensional cases. In that sense, using the modifier \"Schauder\" actually becomes useful, so maybe that is why some people are convinced Schauder bases might be infinite.\nAnd now about the limit on Schauder bases only being countable. Certainly given any space where countable sums converge, you can take a set of whatever cardinality and still consider its Schauder span (just like you could also consider its Hamel span). I know that the case of a separable space is especially useful and popular, and necessitates a countable basis, so that is probably why people tend to think of Schauder bases as countable. But I had thought uncountable Schauder bases were also used for inseparable Hilbert spaces.", "meta": {"post_id": 630142, "input_score": 111, "output_score": 65, "post_title": "What is the difference between a Hamel basis and a Schauder basis?"}}
{"input": "Can $18$ consecutive positive integers be separated into two groups, such that their product is equal?  We cannot leave out any number and neither we can take any number more than once. \nMy work:\nWhen the smallest number is not  $17$ or its multiple, there cannot exist any such arrangement as $17$ is a prime.  \nWhen the smallest number is a multiple of $17$ but not of $13$ or $11$, then no such arrangement exists.  \nBut what happens, when the smallest number is a multiple of $ 17 $ and $13$ or $11$ or both?\nPlease help!", "output": "If $18$ consecutive positive integers could be separated into two groups with equal products, then the product of all $18$ integers would be a perfect square. However, the product of two or more consecutive positive integers can never be a perfect square, according to a famous theorem of P. Erd\u0151s, Note on products of consecutive integers, J. London Math. Soc. 14 (1939), 194-198.", "meta": {"post_id": 638646, "input_score": 70, "output_score": 51, "post_title": "Can $18$ consecutive integers be separated into two groups,such that their product is equal?"}}
{"input": "$1 + 3 = 4$ (or $2$ squared)\n$1+3+5 = 9$ (or $3$ squared)\n$1+3+5+7 = 16$ (or $4$ squared)\n$1+3+5+7+9 = 25$ (or $5$ squared)\n$1+3+5+7+9+11 = 36$ (or $6$ squared)\nyou can go on like this as far as you want, and as long as you continue to add odd numbers in order like that, your answer is always going to be a perfect square.\nBut how to prove it?", "output": "Consider a square. It can be divided as the following:\n\nEach red segment contains odd number of circles.", "meta": {"post_id": 639068, "input_score": 14, "output_score": 70, "post_title": "Sum of odd numbers always gives a perfect square."}}
{"input": "I have a sequence $T_1,T_2,\\ldots$ of independent exponential random variables with paramter $\\lambda$. I take the sum $S=\\sum_{i=1}^n T_i$ and now I would like to calculate the probability density function.\nWell, I know that $P(T_i>t)=e^{-\\lambda t}$ and therefore $f_{T_i}(t)=\\lambda e^{-\\lambda t}$ so I need to find $P(T_1+\\cdots+T_n>t)$ and take the derivative. But I cannot expand the probability term, you have any ideas?", "output": "The usual way to do this is to consider the moment generating function, noting that if $S = \\sum_{i=1}^n X_i$ is the sum of IID random variables $X_i$, each with MGF $M_X(t)$, then the MGF of $S$ is $M_S(t) = (M_X(t))^n$.  Applied to the exponential distribution, we can get the gamma distribution as a result.\nIf you don't go the MGF route, then you can prove it by induction, using the simple case of the sum of the sum of a gamma random variable and an exponential random variable with the same rate parameter.  Let's actually do this.  Suppose $Y \\sim {\\rm Gamma}(a,b)$ and $X \\sim {\\rm Exponential}(b)$ are independent, so that $$f_Y(y) = \\frac{b^a y^{a-1} e^{-by}}{\\Gamma(a)} \\mathbb 1(y > 0), \\quad f_X(x) = be^{-bx} \\mathbb 1(x > 0), \\quad a, b > 0.$$  Then, we notice that if $a = 1$, $Y$ would also be exponential (i.e., the exponential distribution is a special case of the Gamma with $a = 1$).  Now consider $Z = X+Y$.  The PDF is $$\\begin{align*} f_Z(z) &= \\int_{y=0}^z f_Y(y) f_X(z-y) \\, dy \\\\ &= \\int_{y=0}^z \\frac{b^{a+1} y^{a-1} e^{-by} e^{-b(z-y)}}{\\Gamma(a)} \\, dy \\\\ &= \\frac{b^{a+1} e^{-bz}}{\\Gamma(a)} \\int_{y=0}^z y^{a-1} \\, dy \\\\ &= \\frac{b^{a+1} e^{-bz}}{\\Gamma(a)} \\cdot \\frac{z^a}{a} = \\frac{b^{a+1} z^a e^{-bz}}{\\Gamma(a+1)}. \\end{align*}$$  But this is just a gamma PDF with new shape parameter $a^* = a+1$.  So, it is easy to see by induction that the sum of $n$ IID exponential variables with common rate parameter $\\lambda$ is gamma with shape parameter $a = n$, and rate parameter $b = \\lambda$.", "meta": {"post_id": 655302, "input_score": 23, "output_score": 38, "post_title": "Gamma Distribution out of sum of exponential random variables"}}
{"input": "I am currently studying Graph Theory and want to know the difference in between Path , Cycle and Circuit. \nI know the difference between Path and the cycle but What is the Circuit actually mean.", "output": "All of these are sequences of vertices and edges. They have the following properties : \n\nWalk\u00a0\u00a0\u00a0\u00a0: Vertices may repeat. Edges may repeat (Closed or Open)\nTrail\u00a0\u00a0\u00a0\u00a0\u00a0: Vertices may repeat. Edges cannot repeat (Open)\nCircuit : Vertices may repeat. Edges cannot repeat (Closed)\nPath\u00a0\u00a0\u00a0\u00a0\u00a0: Vertices cannot repeat. Edges cannot repeat (Open)\nCycle\u00a0\u00a0\u00a0\u00a0: Vertices cannot repeat. Edges cannot repeat (Closed)\n\nNOTE : For closed sequences start and end vertices are the only ones that can repeat.", "meta": {"post_id": 655589, "input_score": 53, "output_score": 104, "post_title": "What is difference between cycle, path and circuit in Graph Theory"}}
{"input": "This question is asked on Physics SE and MathOverflow by somebody else. I don't think it belongs there, but rather here (for reasons given there in my comments there; edit: now self-removed).\n\nImagine the beginning of a game of pool, you have 16 balls, 15 of them in a triangle <| and 1 of them being the cue ball off to the left of that triangle. Imagine that the rack (the 15 balls in a triangle) has every ball equally spaced apart and all balls touching all other appropriate balls. All balls are perfectly round. Now, imagine that the cue ball was hit along a friction free surface on the center axis for this triangle O-------<| and hits the far left ball of the rack dead center on this axis. How would the rack react? I would imagine this would be an extension of newtons cradle and only the 5 balls on the far end would move at all. But in what way would they move? Thanks", "output": "This is it.\u00a0  The perfectly centered billiards break.\u00a0  Behold.\n\nSetup\nThis break was computed in Mathematica using a numerical differential equations model.  Here are a few details of the model:\n\nAll balls are assumed to be perfectly elastic and almost perfectly rigid.\nEach ball has a mass of 1 unit and a radius of 1 unit.\nThe cue ball has a initial speed of 10 units/sec.\nThe force between two balls is given by the formula\n$$\nF \\;=\\; \\begin{cases}0 & \\text{if }d \\geq 2, \\\\ 10^{11}(2-d)^{3/2} & \\text{if }d<2,\\end{cases}\n$$\nwhere $d$ is the distance between the centers of the balls.  Note that the balls overlap if and only if $d < 2$.  The power of $3/2$ was suggested by Yoav Kallus on Math Overflow, because it follows Hertz's theory of non-adhesive elastic contact.\n\nThe initial speed of the cue ball is immaterial -- slowing down the cue ball is the same as slowing down time. The force constant $10^{11}$ has no real effect as long as it's large enough, although it does change the speed at which the initial collision takes place.\nThe Collision\nFor this model, the entire collision takes place in the first 0.2 milliseconds, and none of the balls overlap by more than 0.025% of their radius during the collision.  (These figures are model dependent -- real billiard balls may collide faster or slower than this.)\nThe following animation shows the forces between the balls during the collision, with the force proportional to the area of each yellow circle.  Note that the balls themselves hardly move at all during the collision, although they do accelerate quite a bit.\n\nThe Trajectories\nThe following picture shows the trajectories of the billiard balls after the collision.\n\nAfter the collision, some of the balls are travelling considerably faster than others.  The following table shows the magnitude and direction of the velocity of each ball, where $0^\\circ$ indicates straight up.\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{ball} & \\text{cue} & 1 & 2,3 & 4,6 & 5 & 7,10 & 8,9 & 11,15 & 12,14 & 13 \\\\\n\\hline\n\\text{angle} & 0^\\circ & 0^\\circ & 40.1^\\circ & 43.9^\\circ & 0^\\circ & 82.1^\\circ & 161.8^\\circ & 150^\\circ & 178.2^\\circ & 180^\\circ \\\\\n\\hline\n\\text{speed} & 1.79 & 1.20 & 1.57 & 1.42 & 0.12 & 1.31 & 0.25 & 5.60 & 2.57 & 2.63 \\\\\n\\hline\n\\end{array}\n$$\nFor comparison, remember that the initial speed of the cue ball was 10 units/sec.  Thus, balls 11 and 15 (the back corner balls) shoot out at more than half the speed of the original cue ball, whereas ball 5 slowly rolls upwards at less than 2% of the speed of the original cue ball.\nBy the way, if you add up the sum of the squares of the speeds of the balls, you get 100, since kinetic energy is conserved.\nLinear and Quadratic Responses\nThe results of this model are dependent on the power of $3/2$ in the force law -- other force laws give other breaks.  For example, we could try making the force a linear function of the overlap distance (in analogy with springs and Hooke's law), or we could try making the force proportional to the  square of the overlap distance.  The results are noticeably different\n \nStiff Response\nGlenn the Udderboat points out that \"stiff\" balls might be best approximated by a force response involving a higher power of the distance (although this isn't the usual definition of \"stiffness\").  Unfortunately, the calculation time in Mathematica becomes longer when the power is increased, presumably because it needs to use a smaller time step to be sufficiently accurate.\nHere is a simulation involving a reasonably \"stiff\" force law\n$$\nF \\;=\\; \\begin{cases}0 & \\text{if }d \\geq 2, \\\\ 10^{54}(2-d)^{10} & \\text{if }d<2.\\end{cases}\n$$\n\nAs you can see, the result is very similar to my first answer below.  This seems like good evidence that the behavior discussed in my first answer is indeed the limiting behavior in the case where the stiffness goes to infinity.\nAs you might expect, most of the energy in this case is transferred very quickly at the beginning of the collision.  Almost all of the energy has moves to the back corner balls in the first 0.02 milliseconds.  Here is an animation of the forces:\n\nAfter that, the corner balls and the cue ball shoot out, and the remaining balls continue to collide gently for the next millisecond or so.\nWhile the simplicity of this behavior is appealing, I would guess that \"real\" billard balls do not have such a stiff force response.  Of the models listed here, the intial Hertz-based model is probably the most accurate.  Qualitatively, it certainly seems the closest to an \"actual\" break.\nNote: I have now posted the Mathematica code on my web page.", "meta": {"post_id": 658871, "input_score": 32, "output_score": 91, "post_title": "Perfectly centered break of a perfectly aligned pool ball rack"}}
{"input": "If I have a non-singular matrix $\\bf A$, how can I prove the following?\n$$ \\det \\left( {\\bf A}^{-1} \\right) = \\frac{1}{\\det({\\bf A})} $$\nI know that ${\\bf A} {\\bf A}^{-1} = {\\bf I}$, but I am not sure what to do with that knowledge.", "output": "first of all we know that\n$$\\det(A \\cdot  B)=\\det(A)\\times\\det(B)$$\nalso we know that\n$$A\\times A^{-1}=I$$\nwe know that\n$$\\det(A \\cdot A^{-1})=\\det(I)$$\nor\n$$\\det(A)\\times\\det(A^{-1})=\\det(I)$$\nCan you continue from this? Ask yourself: what is $\\det(I)$?) Take the example of the $3\\times 3$ identity matrix:\n$$ I =  \\begin{pmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$$\n$$ \\det(I) = 1$$\nSo,\n$$\\det(A)\\times\\det(A^{-1})=1$$\nor\n$$\\boxed{\\det(A^{-1})=\\frac{1}{\\det(A)}}$$", "meta": {"post_id": 661633, "input_score": 28, "output_score": 34, "post_title": "Prove that $ \\det \\left( A^{-1} \\right) = \\frac{1}{\\det(A)} $"}}
{"input": "Prove that a closed ball in a metric space is a closed set\n\nMy attempt: Suppose $D(x_0, r)$ is a closed ball. We show that $X \\setminus D $ is open. In other words, we need to find an open ball contained in $X \\setminus D$. \nPick $$t \\in X-D \\implies d(t,x_0) > r \\implies d(t,x_0) - r > 0 $$ Let $B(y, r_1)$ be an open ball, and pick $z \\in B(y,r_1)$. Then, we must have $d(y,z) < r_1 $. We need to choose $r_1$ so that $d(z,x_0) > r$. Notice by the triangle inequality\n$$ d(x_0,t) \\leq d(x_0,z) + d(z,t) \\implies d(z,x_0) \\geq d(x_0,t) - d(z,t) > d(x_0,t) - r_1.$$\nNotice, if we pick $r_1 = d(t,x_0)-r$ then we are done.\nIs this correct?", "output": "Your proof consists of some correct steps done in the wrong order, which makes it something other than a valid proof. It looks more like scratchwork done in preparation for a proof.  I  rewrite it below, with some of the more important additions in bold. I will also change $t$ to $y$ throughout; when you wrote \"$y$\" you probably meant the same thing as \"$t$\".\n\nSuppose $D(x_0, r)$ is a closed ball. We show that $X\\setminus D(x_0,r) $ is open. In other words, for every point $y\\in X\\setminus D(x_0,r)$ we need to find an open ball contained in $X \\setminus D$ with center $y$.\nSince $y \\in X\\setminus D(x_0,r)$, it follows that $d(y,x_0) > r$, so $d(y,x_0) - r > 0 $. Let $r_1 = d(y,x_0)-r$.\nI claim that the open ball $B(y, r_1)$ is contained in $X\\setminus D(x_0,r)$. To prove this, consider any $z \\in B(y,r_1)$.  Notice by the triangle inequality\n$$ d(x_0,y) \\leq d(x_0,z) + d(z,y) \\implies d(z,x_0) \\geq d(x_0,y) - d(z,y) > d(x_0,y) - r_1 =r.$$\nThis shows $z\\in X\\setminus D(x_0,r)$, which completes the proof.", "meta": {"post_id": 661759, "input_score": 22, "output_score": 39, "post_title": "A closed ball in a metric space is a closed set"}}
{"input": "If $X$ and $Y$ are independent random variables, are the statements below true\n$$E(e^{X+Y} ) = E(e^X)\\times E(e^Y)$$\nand \n$$E(X^2\\times Y^2) = E(X^2)\\times E(Y^2),$$\nwhere $E(\\cdot)$ = expectation?", "output": "Yes, because $E(PQ)=E(P)E(Q)$ when the random variables $P$ and $Q$ are independent. In each case you can simply define new random variables that are functions of the first.\n\n$P=e^X, Q=e^Y \\implies PQ=e^X e^Y= e^{X+Y}$\n$P=X^2, Q=Y^2$\n\n\nIf $X$ and $Y$ are independent , how do you knew that $X^2$ and $Y^2$ are independent?\n\nThis can be looked at in two ways.\nFirst of all, and this is what I was relying on above, you can appeal to our everyday understanding of how the world works. E.g.  let $X$ and $Y$ be the results of rolling two dice. $X$ and $Y$ are independent. Now, let's say we square each result. We clearly haven't introduced any dependency by doing this, so $P=X^2$ and $Q=Y^2$ are independent.\nSecondly, you can look more deeply at the underlying mathematics. The general result is JohnK's answer and a specific instance of that is justified in korrok's answer.\nExpectation of two random variables $X$, $Y$ is defined as the sum of the products of the values of those random variables times their joint probabilities. For continuous random variables this is\n$$\\mathrm{E}(XY)=\\int\\int xy \\; f_{XY}(x,y) \\;\\mathrm{d}x\\mathrm{d}y $$\nwhere the integrals are over the range that $X$ and $Y$ can take, and $f_{XY}$ is the joint probability density of $X$ and $Y$.\nIn the more general case of $\\alpha(x)$ as some function of $x$ and $\\beta(y)$ as some function of $y$ (you had $\\alpha = \\beta = x\\mapsto x^2$), the expectation of their product is defined similarly.\n$$\\mathrm{E}(\\alpha(X)\\beta(Y))=\\int\\int \\alpha(x)\\beta(y) \\; f_{XY}(x,y) \\;\\mathrm{d}x\\mathrm{d}y $$\nBecause $X$ and $Y$ are independent, you can factorize the probability density  $f_{XY}$ into the product of the probability density $f_X(x)$ for $X$ and the probability density $f_Y(y)$ for $Y$, i.e.: $f_{XY} = f_X(x)f_Y(y)$. So\n$$\\mathrm{E}(\\alpha(X)\\beta(Y))=\\int\\int \\alpha(x)\\beta(y) \\; f_X(x)f_Y(y)\\;\\mathrm{d}x\\mathrm{d}y $$\nRearranging the integrand we see that the integrand is the product of terms that only depend on $x$ and terms that only depend on $y$ so the integral itself can be split into two. Each of those two integrals is the definition of an expectation.\n$$\\begin{align}\n\\mathrm{E}(\\alpha(X)\\beta(Y))\n&= \\int\\int \\alpha(x)f_X(x) \\; \\beta(y)f_Y(y)\\;\\mathrm{d}x\\mathrm{d}y \\\\\n&= \\int \\alpha(x)f_X(x) \\;\\mathrm{d}x \\int\\beta(y)f_Y(y)\\;\\mathrm{d}y \\\\\n&= \\mathrm{E}( \\alpha(X))\\mathrm{E}(\\beta(Y))\n\\end{align}$$", "meta": {"post_id": 667911, "input_score": 23, "output_score": 34, "post_title": "Expected value of the product of functions of two independent random variables"}}
{"input": "Show that if $n \\geq 0$ and $x>0$, then \n$$ e^x > 1 + x + \\frac{x^2}{2!} + \\dots + \\frac{x^n}{n!}.$$\nNot sure where to get started with this induction proof.", "output": "Base case: $e^x > 1$ for $x > 0$.\nInduction: suppose we are given $k$ such that for all $x > 0$\n$$\ne^x > 1 + x + \\frac{x^2}{2!} + \\cdots + \\frac{x^k}{k!}\n$$\nChange variables to $t$ and integrate both sides:\n$$\n\\int_0^x e^t \\; dt > \\int_0^x \\left( 1 + t + \\frac{t^2}{2!} + \\cdots + \\frac{t^k}{k!} \\right) \\; dt\n$$\n$$\ne^x - 1 > x + \\frac{x^2}{2!} + \\cdots + \\frac{x^{k+1}}{(k+1)!}\n$$\n$$\ne^x > 1 + x + \\frac{x^2}{2!} + \\cdots + \\frac{x^{k+1}}{(k+1)!}\n$$\nNote that if $f > g$ on $(a,b)$ and both are integrable, then $\\int_a^b f > \\int_a^b g$ (not just $\\ge$).", "meta": {"post_id": 667998, "input_score": 6, "output_score": 47, "post_title": "Show that $e^x > 1 + x + x^2/2! + \\cdots + x^n/n!$ for $n \\geq 0$, $x > 0$ by induction"}}
{"input": "If there are 5 points on the surface of a sphere, then there is a closed half sphere, containing at least 4 of them.\nIt's in a pigeonhole list of problems. But, I think I have to use rotations in more than 1 dimension.\nRegards", "output": "Pick two distinct points out of your 5 (if all 5 are identical then they clearly all lie in a single hemisphere).  These two points define at least one great circle (if they're antipodal, they define infinitely many); pick a great circle they define.  This circle then cuts the sphere into two hemispheres.  Now pigeonhole the other three points between these two hemispheres.", "meta": {"post_id": 675646, "input_score": 40, "output_score": 74, "post_title": "if there are 5 points on a sphere then 4 of them belong to a half-sphere."}}
{"input": "I am looking for books or papers which tell me something about representation theory of finite groups over $\\mathbb{Q}$ (or finite extensions thereof which are not splitting fields of the group algebra).\nTo be more precise, I'd like to learn of theorems which, for example, provide me with the following information, given a finite group $G$: how many irreducible representations with coefficients of $\\mathbb{Q}$ are there, how can I compute their characters and can this information somehow be obtained from the \"usual\" character table of the group.\nI have some background in ordinary representation theory over $\\mathbb{C}$.", "output": "This is a well-established theory, which is very nicely presented in the second volume of the two-volume work of Curtis and Reiner. Here is the gist of it:\nSince a rational representation is also a complex representation, you still have character theory to help you. In particular, a rational representation is still uniquely determined by its character, which, of course, only take values in $\\mathbb{Q}$.\nSo suppose that you wanted to do the converse: start with the knowledge of all the complex representations (including the full character table), and construct all the irreducible rational ones. The absolute Galois group of $\\mathbb{Q}$ acts on the set of complex representations by acting on each entry in each matrix, and so also acts on the set of characters. If $\\chi$ is the character of an irreducible rational representation, then it must be invariant under the Galois action. In particular, if $\\phi$ is an irreducible complex character sitting inside $\\chi$, then every Galois conjugate $\\phi^\\sigma$ also has to sit in $\\chi$ with the same multiplicity. So the first step is to take an irreducible complex character $\\phi$ and to \"rationalise\" it by $\\chi = \\sum_{\\sigma\\in \\text{Gal}}\\phi^\\sigma$, with the sum running over the distinct Galois conjugates of $\\phi$.\nSo now you have a $\\mathbb{Q}$-valued character, but it does not mean that the corresponding representation can be realised over $\\mathbb{Q}$ (as an example, think of the standard representation of the quaternion group $Q_8$). However, there is a unique minimal integer $m(\\chi)$ such that $m(\\chi)\\chi$ can be realised over $\\mathbb{Q}$, and this representation is in fact irreducible over $\\mathbb{Q}$. This $m(\\chi)$ is called the Schur index of $\\chi$, and is also nicely treated in Curtis and Reiner, but also in Isaacs for example. It is now easy to see that all irreducible rational representations arise in this way. If you are interested in general number fields, then you only have to average over the Galois conjugates over that field, but you may still have a Schur index flying around.\nThe answer to your question about the number of irreducible rational representations is really neat: it is equal to the number of conjugacy classes of cyclic subgroups of $G$ (as opposed to conj classes of elements, like in the complex case). I seem to remember that this is proven, among other places, in Serre's book on representation theory. This is one of the ways of stating Artin's induction theorem.", "meta": {"post_id": 676303, "input_score": 18, "output_score": 40, "post_title": "Representation theory over $\\mathbb{Q}$"}}
{"input": "Today, at my linear algebra exam, there was this question that I couldn't solve.\n\n\nProve that\n$$\\det \\begin{bmatrix} \nn^{2} & (n+1)^{2} &(n+2)^{2} \\\\ \n(n+1)^{2} &(n+2)^{2}  & (n+3)^{2}\\\\ \n(n+2)^{2} & (n+3)^{2} & (n+4)^{2}\n\\end{bmatrix} = -8$$\n\n\nClearly, calculating the determinant, with the matrix as it is, wasn't the right way. The calculations went on and on. But I couldn't think of any other way to solve it.\nIs there any way to simplify $A$, so as to calculate the determinant?", "output": "Here is a proof that is decidedly not from the book. The determinant is obviously a polynomial in n of degree at most 6. Therefore, to prove it is constant, you need only plug in 7 values. In fact, -4, -3, ..., 0 are easy to calculate, so you only have to drudge through 1 and 2 to do it this way !", "meta": {"post_id": 676586, "input_score": 37, "output_score": 62, "post_title": "How to find the determinant of this $3 \\times 3$ Hankel matrix?"}}
{"input": "Given a cover $\\{U_i\\}$ of a space $X$ and for each $U_i$ a sheaf $\\mathcal{F}_i$ and isomorphisms $\\phi_{ij}:\\mathcal{F_j}|_{U_i \\cap U_j} \\rightarrow \\mathcal{F_i}|_{U_i \\cap U_j}$ satisfying the cocycle condition $\\phi_{ij}\\phi_{jk}\\phi_{ki} = id$, I want to show that there is a sheaf $\\mathcal{F}$ on $X$ whose restriction to each $U_i$ is isomorphic to $\\mathcal{F_i}$. I understand that $\\mathcal{F}(U)$ should consist of tuples $(s_i)$ of sections from the various sheaves, then I need to define restriction maps, prove the presheaf axioms and then the sheaf axioms. I have at least a vague idea of how the proof should go. Unfortunately, on my way to proving the result I get utterly lost in an unholy mess of details, so that even once I \"finished\" my proof I was nowhere near certain my proof was correct. I've searched around for proofs to examine, but any proof I can find is either lacking many details or required as an exercise.\nHere are my questions:\n1) Can someone show me a proof of this result? I am interested to see both a direct proof checking all the details and also a more intuitive proof, possibly appealing to results about gluing morphisms or whatnot.\n2) Should the tuples $(s_i)$ be leaving in the product or the disjoint union of the $\\mathcal{F_i}(U\\cap U_i)$? I thought it would be product, but I saw union on Google Books in Introduction to Singularities and Deformations by Greuel, Lossen, and Shustin.\n3) Where does the cocycle condition come into play?\n4) How can I prevent all these messy sheaves from deterring me from the beautiful subject of algebraic geometry?\nThanks in advance.", "output": "You should really be able to do this directly. The calculations are not messy at all, in my opinion. Also, they are straight forward. It is also a good idea to simplify the notation and to use words more than formulas. This way you really understand what is going on.\nYou define $F(U)$ to be the set of all families $s=(s_i)$ of sections $s_i \\in F_i(U \\cap U_i)$ which are compatible in the sense that $\\phi_{ij}(s_j)=s_i$ for all $i,j$. I have simplified the notation here: Of course we restrict $s_i$ and $s_j$ to $U \\cap U_i \\cap U_j$, and of course we apply $\\phi_{ij}$ at this open subset.\nThe restriction maps of $F$ are induced by the ones for $F_i$. They are well-defined because the asserted compatibility is preserved by restriction, which in turn works since $\\phi_{ij}$ commutes with restriction maps. After having checked this, it is obvious that $F$ becomes a presheaf, using the presheaf properties of the $F_i$.\nNow as for the first sheaf condition, let $s=(s_i)$ be as above, and $U = \\cup_p W_p$ be an open cover. If $s$ is trivial on each $W_p$, this means that $s_i \\in F_i(U \\cap U_i)$ is trivial on each $W_p \\cap U_i$. But since these cover $U \\cap U_i$, it follows $s_i=0$, for all $i$, hence $s=0$. (For sheaves of sets, you can adjust this argument easily.)\nFor the second sheaf condition, let $s^p \\in F(W_p)$ be a family of compatible sections (compatiblity means that $s^p$ and $s^q$ agree on $W_p \\cap W_q$). This means that for every $i$ we have a family of compatible sections $s^p_i \\in F_i(W_p \\cap U_i)$ with respect to the cover $\\{W_p \\cap U_i\\}$ of $W \\cap U_i$. Since $F_i$ is a sheaf, these glue to a section $s_i \\in F_i(W \\cap U_i)$. We have $\\phi_{ij}(s_j)=s_i$ in $F_i(U \\cap U_i \\cap U_j)$, since this is true when restricted to each $W_p \\cap U_i \\cap U_j$, since $s^p \\in F(W_p)$. Hence, $s:=(s_i) \\in F(U)$ and $s$ restricts to $s^p$ on $W_p$ by construction.\nThus, $F$ is a sheaf.\nThe cocycle condition is not needed to make this construction work. We don't even need that the $\\phi_{ij}$ are isomorphisms! This is especially clear in the category-theoretic construction of $F$, see for example Zhen Lin's answer here.\nBut there is a reason why one usually demands this condition: We would like to have that the projection $F|_{U_i} \\to F_i$ mapping a section $s$ to $s_i$ is an isomorphism. We simply construct an inverse by mapping $s_i$ to $s$ defined by $s_j = \\phi_{ij}^{-1}(s_i)$ (here we need that $\\phi_{ij}$ is an isomorphism). This is consistent when $\\phi_{ii}=\\mathrm{id}$ (which would follow from the cocycle condition). By construction $\\phi_{ij}(s_j)=s_i$, but in order to be a section of $F$, we also need $\\phi_{kj}(s_j)=s_k$ for all $k$, i.e. $\\phi_{kj} = \\phi_{ki} \\circ  \\phi_{ij}$, which is precisely the cocycle condition. One then checks that this describes a map $F_i \\to F|_{U_i}$ which is inverse to the projection.\nThere is even an a priori motivation for the cocycle condition. Given a gluing datum of sheaves $(F_i,\\phi_{ij})$, we want to find a sheaf $F$ with isomorphisms $F|_{U_i} \\cong F_i$, but in such a way that the induced isomorphisms $F_j|_{U_i \\cap U_j} \\cong F|_{U_i \\cap U_j} \\cong F_i|_{U_i \\cap U_j}$ really equal $\\phi_{ij}$. But these isomorphisms obviously satisfy the cocycle condition: If we compose (let me again simplify the notation) $F_k \\to F \\to F_j$ with $F_j \\to F \\to F_i$, then $F \\to F_j \\to F$ cancels to the identity, so that we get $F_k \\to F \\to F_i$. In other words, in the following diagram, the outer triangle commutes because all three inner triangles commute:", "meta": {"post_id": 681477, "input_score": 31, "output_score": 44, "post_title": "Using the cocycle condition to glue sheaves"}}
{"input": "Let $X$ be a scheme. It is known that the category $\\mbox{Vec}_r(X)$ of vector bundles of rank $r$ on $X$ and the category $\\mbox{Loc}_r(X)$ of locally free sheaves of rank $r$ on $X$ are equivalent (The equivalence is given by the functor $F\\colon \\mbox{Loc}_r(X)\\rightarrow \\mbox{Vec}_r(X)$ such that $F(\\mathcal{E})=\\textbf{Spec}(\\mbox{Sym}(\\check{\\mathcal{E}}))$, where $\\mbox{Sym}(\\mathcal{F})$ is the symmetric algebra of $\\mathcal{F}$ and $\\textbf{Spec}(.)$ is as defined in Hartshorne p.128.\nMy confusion is the following. In many books it is written that a locally free subsheaf $\\mathcal{E}'$ of a locally free sheaf $\\mathcal{E}$ does not always correspond to a sub-vector bundle of the vector bundle $F(\\mathcal{E})$. However, since the categories are equivalent, the monomorphism $\\iota\\colon\\mathcal{E}'\\rightarrow\\mathcal{E}$ maps to a monomorphism $F(\\iota)\\colon F(\\mathcal{E}')\\rightarrow F(\\mathcal{E})$ in $\\mbox{Vec}_r(X)$. So, I would expect to get a sub-vector bundle of the vector bundle $F(\\mathcal{E})$. \nA monomorphism in a category is not always an injective map in the usual sense. Is it maybe the point that $F(\\iota)$ might not be an injective map, although it is a monomorphism?\nPS: While I was trying to sort out this confusion, I looked in Hartshorne and was even more confused, because he does not define what a morphism of vector bundles is, although he defines what an isomorphism is (again on page 128). So my second question is what is the definition of a morphism of vector bundles in algebraic geometry?", "output": "Let me tell the whole story of vector bundles because in many books this subject is not developed properly or only in an ad-hoc manner.\nDefinition. Let $X$ be a scheme. A pre-vector bundle on $X$ is an $X$-scheme $V$ such that for every $X$-scheme $T$ the set of $T$-valued points $V(T)=\\hom_X(T,V)$ carries the structure of a module over $\\mathcal{O}_T(T)$. This structure belongs to the data. For $X$-morphisms $T \\to T'$ we require that $V(T') \\to V(T)$ is linear over $\\mathcal{O}_{T'}(T') \\to \\mathcal{O}_T(T)$. There is an obvious notion of a morphism of pre-vector bundles. Thus, we obtain a category. Every morphism $X' \\to X$ induces a pullback functor $V \\mapsto V|_{X'}$ (the underlying scheme is $V \\times_X X'$) from pre-vector bundles on $X$ to pre-vector bundles on $X'$.\nAn example is $\\mathbb{A}^n_X$ with the usual module structure on $\\mathbb{A}^n_X(T) = \\mathcal{O}_T(T)^n$. Any pre-vector bundle isomorphic to $\\mathbb{A}^n_X$ for some $n$ is called trivial. A vector bundle over $X$ is a pre-vector $V$ bundle over $X$ which is locally trivial, i.e. there is an open covering $\\{X_i \\to X\\}_i$ such that $V|_{X_i}$ is trivial for each $i$. We obtain a category of vector bundles.\nIf $E$ is a quasi-coherent module on $X$, then $\\mathbb{V}(E):=\\mathrm{Spec}(\\mathrm{Sym}(\\check{E}))$ is an affine $X$-scheme with $\\mathbb{V}(E)(T) = \\hom_{\\mathcal{O}_T}(p^* \\check{E},\\mathcal{O}_T)$ for $p : T \\to X$. This set has a canonical $\\mathcal{O}_T(T)$-module structure, so that $\\mathbb{V}(E)$ becomes a pre-vector bundle. Observe that $\\mathbb{V}(\\mathcal{O}_X^n)=\\mathbb{A}^n_X$ is the trivial vector bundle. Hence, if $E$ is locally free, then $\\mathbb{V}(E)$ is a vector bundle. Clearly this defines a functor $\\mathbb{V}(-)$ from the category of locally free sheaves to the category of vector bundles over $X$.\nTheorem. The functor $\\mathbb{V}(-)$ is an equivalence of categories.\nProof. First, $\\mathbb{V}(-)$ is fully faithful: Let $E,F$ be locally free sheaves on $X$. We claim that $\\hom(E,F) \\to \\hom(\\mathbb{V}(E),\\mathbb{V}(F))$ is a bijection. Actually we have a morphism of sheaves $\\underline{\\hom}(E,F) \\to \\underline{\\hom}(\\mathbb{V}(E),\\mathbb{V}(F))$ on $X$ and we claim that this is an isomorphism. This way we can work locally and thereby assume that $E,F$ are trivial, say $E=\\mathcal{O}_X^n$ and $F=\\mathcal{O}_X^m$.\nThen $\\underline{\\hom}(E,F)$ becomes a sheaf of matrices $\\mathcal{O}_X^{m \\times n}$. The sheaf of $X$-morphisms $\\mathbb{A}^n_X \\to \\mathbb{A}^m_X$ is $\\mathcal{O}_X[t_1,\\dotsc,t_n]^m$. One checks (!) that a morphism $\\mathbb{A}^n_X \\to \\mathbb{A}^1_X$ corresponding to a polynomial in $\\mathcal{O}_X[t_1,\\dotsc,t_n]$ is a morphism of vector bundles iff this polynomial is linear. This shows $\\underline{\\hom}(\\mathbb{V}(E),\\mathbb{V}(F)) \\cong (\\mathcal{O}_X^m)^n$, which finishes the proof.\nSecondly, $\\mathbb{V}(-)$ is essentially surjective: Let $V$ be a vector bundle on $X$. Choose a cover $\\{X_i \\to X\\}$ and a  trivialization $\\phi_i : V|_{X_i} \\cong \\mathbb{V}(\\mathcal{O}_{X_i}^n)$ (here $n$ may depend on $i$). By fully faithfulness, the resulting isomorphisms $\\phi_{ij} := \\phi_i |_{X_i \\cap X_j} \\phi_j^{-1}  |_{X_i \\cap X_j}$ are induced by isomorphisms $\\psi_{ij} : \\mathcal{O}_{X_i \\cap X_j}^n \\cong \\mathcal{O}_{X_i \\cap X_j}^n$. Since $\\phi_{ij}$ satisfy the cocycle condition, the same is true for the $\\psi_{ij}$. Hence, we have a gluing data $(\\mathcal{O}_{X_i}^n,\\psi_{ij})$ and obtain a locally free sheaf $E$ such that $\\mathbb{V}(E) \\cong V$ by construction. $\\square$\nAs already mentioned, every equivalence of categories preserves (and reflects) monomorphisms. In particular, a homomorphism of free sheaves of modules $\\mathcal{O}_X^n \\to \\mathcal{O}_X^m$ is a monomorphism (in the category of locally free sheaves) if and only if the induced morphism of trivial vector bundles $\\mathbb{A}^n_X \\to \\mathbb{A}^m_X$ is a monomorphism. However, monomorphisms of locally free sheaves should not be confused with monomorphisms in the larger category of all sheaves of modules, and monomorphisms of vector bundles should not be confused with monomorphisms of schemes, let alone morphisms of schemes whose underlying map of sets is injective, or injective on the fibers - which seems to be the usual ad-hoc definition of a subbundle.\nFor example, consider a regular global section $s$ of $\\mathcal{O}_X$. Then $s : \\mathcal{O}_X \\to \\mathcal{O}_X$ is a monomorphism of sheaves, hence also of locally free sheaves. The induced morphism of vector bundles $\\mathbb{A}^1_X \\to \\mathbb{A}^1_X$ corresponds to the polynomial $s * t$. If $s$ is invertible, it is an isomorphism. If not, choose some $x \\in X$ such that $s_x \\in \\mathfrak{m}_x$, i.e. $s(x)=0$ in $\\kappa(x)$, so that on the fiber of $x$ we get the the zero morphism $\\mathbb{A}^1_{\\kappa(x)} \\to \\mathbb{A}^1_{\\kappa(x)}$, whose underlying map of sets is not injective.", "meta": {"post_id": 682328, "input_score": 27, "output_score": 37, "post_title": "Equivalence of categories of vector bundles and locally free sheaves"}}
{"input": "I want to show that for $n>0$, $2^n$ and $2^n + 1$ have the same number of digits.\nWhat I did was I found that the formula for the number of digits of a number $x$ is $\\left \\lfloor{\\log_{10}(x)}\\right \\rfloor + 1$, so basically if I subtract that formula with $x = 2^n$ with the formula with $x = 2^n + 1$, I should get zero.\n$\\left \\lfloor{\\log_{10}(2^n)}\\right \\rfloor + 1 - (\\left \\lfloor{\\log_{10}(2^n + 1)}\\right \\rfloor + 1) = \\left \\lfloor{\\log_{10}(2^n)}\\right \\rfloor  -\\left \\lfloor{\\log_{10}(2^n + 1)}\\right \\rfloor $.\nAt this point, I don't know of a way to simplify this any further to make it equal $0$. I thought about mentioning that $\\log_{10}(x)$ increases slower than $x$ as $x$ increases, which would mean the difference of the floor of the logs of two consecutive numbers may be close to zero, but that doesn't cut it to prove that $2^n, 2^n + 1$ have exactly the same number of digits.\nAre there any special floor or log properties I could use to make this easier? Any help is appreciated.", "output": "Note the only way $2^n+1$ can have one more digit than $2^n$ is if $2^n$ ended in a $9$ (actually ends is $\\cdots 999999$ but that is not important). $2^n$ can never end in a $9$.", "meta": {"post_id": 685618, "input_score": 16, "output_score": 53, "post_title": "Show that these two numbers have the same number of digits"}}
{"input": "I want to show the fundamental group of a topological group is abelian. In fact, the question says the topological group is path connected. I do not know where I should use path-connectedness. I think, it is still true if we do not suppose path-connectedness. Right?\nI do not know homotopy. I have just learned the fundamental group.", "output": "The usual proof is to show that for all loops $\\alpha,\\beta \\colon [0,1] \\to G$ of the topological group $G$, the concatenation $\\alpha \\cdot \\beta$ is homotop to $t \\mapsto \\alpha(t)\\beta(t)$ and to $t \\mapsto \\beta(t)\\alpha(t)$. It requires to exhibit formulae.\nHowerver, my favourite proof of this result is the following one, from Grothendieck (I think) : the fundamental group functor $\\pi_1 \\colon \\mathsf{pcTop} \\to \\mathsf{Grp}$ from the category of path-connected topological spaces to the category of groups respects products (classical lemma), so sends group objects to group objects ; the group objects of $\\mathsf{pcTop}$, which are the path-connected topological groups (by definition), are send to group objects of $\\mathsf{Grp}$, which are the abelian groups (easy exercise).", "meta": {"post_id": 686496, "input_score": 14, "output_score": 47, "post_title": "The fundamental group of a topological group is abelian"}}
{"input": "Prove that: $\\lfloor n^{1/2}\\rfloor+\\cdots+\\lfloor n^{1/n}\\rfloor=\\lfloor \\log_2n\\rfloor +\\cdots+\\lfloor \\log_nn \\rfloor$, for $n > 1,\\, n\\in \\mathbb{N}$\nFor example. For $n=2$, we have\n$\\lfloor 2^{1/2} \\rfloor = \\lfloor 1.414 \\rfloor = 1$ whereas \n$\\lfloor \\log_2(2) \\rfloor = 1$\nwhile for $n=3$, we have\n$$\\lfloor 3^{1/2} \\rfloor + \\lfloor 3^{1/3} \\rfloor = \\lfloor 1.732 \\rfloor + \\lfloor 1.442 \\rfloor = 2= \\lfloor 1.585 \\rfloor + \\lfloor 1 \\rfloor=\\lfloor \\log_2(3) \\rfloor + \\lfloor \\log_3(3) \\rfloor .$$\nI was thinking of using induction.\nSo since $n=2$ is true, now assume for all $n$, this identity is true, we would like to prove that $n+1$ is true. Then\n$$\\lfloor n^{1/2} \\rfloor + \\lfloor n^{1/3} \\rfloor + ... + \\lfloor n^{1/n} \\rfloor + \\lfloor (n+1)^{1/(n+1)} \\rfloor,$$\nwhere $(n+1)^{1/(n+1)} > 1$ for all $n>1$ but it's strictly decreasing above 1 so $\\lfloor (n+1)^{1/(n+1)} \\rfloor = 1$ \n$\\implies \\lfloor n^{1/2} \\rfloor + \\lfloor n^{1/3} \\rfloor +\\cdots+ \\lfloor n^{1/n} \\rfloor + \\lfloor (n+1)^{1/(n+1)} \\rfloor\n= \\lfloor n^{1/2} \\rfloor + \\lfloor n^{1/3} \\rfloor +\\cdots+ \\lfloor n^{1/n} \\rfloor + 1 $\n$= \\lfloor \\log_2(n) \\rfloor + \\lfloor \\log_3(n) \\rfloor + \\cdots+ \\lfloor \\log_n(n) \\rfloor + \\lfloor \\log_{n+1}(n+1) \\rfloor$\nsince, $\\log_{n+1}(n+1) = 1$ for all $n$.\n\nMy question is: How do we know that $(n+1)^{1/(n+1)}$ will never go below $1$? i.e., How can we prove that this function $f(x) = (x+1)^{1/(x+1)}$ is always bounded below by $1$ for $x>1$? (First, When $x=0$, $f(0)=1$, then looking at it's derivative, one can see that it's strictly increasing for $x$ between $(0,1)$ and decreasing for all $x>1$).", "output": "This is a classic exercise and one with a very elegant solution.\nThe idea of the proof is to count the number $N$ of the points (see figure below) with integer coordinates, which lie in the region\n$$\nU=\\big\\{(x,y): 0<x\\le n \\,\\,\\,\\text{and}\\,\\,\\, 1<y\\le n^{1/x}\\big\\},\n$$\nand in particular, the red points,\nin two ways: horizontally and vertically.\nHorizontal counting:\n$$\nN=\\lfloor n^{1/2}\\rfloor+\\lfloor n^{1/3}\\rfloor+\\cdots+\\lfloor n^{1/n}\\rfloor,\n$$\nsince on the horizontal line $\\,y=k\\,$ lie exactly $\\,\\lfloor n^{1/k}\\rfloor\\,$ red points.\nVertical counting:\n$$\nN=\\lfloor \\log_2 n\\rfloor+\\lfloor\\log_3 n\\rfloor+\\cdots+\\lfloor \\log_n n\\rfloor,\n$$\nsince on the vertical line $\\,x=k\\,$ lie exactly $\\,\\lfloor \\log_k n\\rfloor\\,$ red points.\n$$\n{}\n$$\n\nNote that the curve in the figure above is of the function $y=n^{1/x}$.\nThis problem was first asked in a Soviet Mathematics Olympiad in 1982 (\u0412\u0441\u0435\u0441\u043e\u044e\u0437\u043d\u044b\u0439 \u041c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u041e\u043b\u0438\u043c\u043f\u0438\u0430\u0434.)", "meta": {"post_id": 690335, "input_score": 16, "output_score": 36, "post_title": "Prove that: $\\lfloor n^{1/2}\\rfloor+\\cdots+\\lfloor n^{1/n}\\rfloor=\\lfloor \\log_2n\\rfloor +\\cdots+\\lfloor \\log_nn \\rfloor$, for $n > 1$"}}
{"input": "Is there some sort of intuition or a good ilustrative example for random variables being $\\sigma$-algebra measurable? I understand the definition, but when looking at martingales, the meaning of random variables being measurable eludes me. So my question is mainly aimed on the case of martingales where sequence of random variables is adapted to some filtration.\nIn Interpretation of sigma algebra, the asker asks (among many others) a similar question, but I don't think it contains an actual answer to this question.", "output": "Maybe this can help you to understand the concept of conditional expectation, behind your question.\nSuppose you have a probability space $(\\Omega, \\mathcal P (\\Omega), \\mathbb{P})$, where $\\mathcal P (\\Omega)$ denotes the set of all possible subsets of $\\Omega$ (evidently, a $\\sigma$-algebra),  and $\\mathbb{P}$ is a probability measure (in this case, a function from $\\mathcal P (\\Omega)$ to [0,1]).\nSuppose you have a random variable (measurable function) $X:(\\Omega, \\mathcal P (\\Omega)) \\to (\\mathbb{R}, \\mathcal B (\\mathbb R ))$, where $\\mathcal B (\\mathbb R )$ is the usual Borel $\\sigma$-algebra.\nTake as a sub-$\\sigma$-algebra the trivial one, $\\mathcal F = \\{\\emptyset, \\Omega\\}$. Suppose we only know the conditional expectation $\\mathbb E(X | \\mathcal F)$, but not $X$ itself. How much do we know about X? Well,  $Y = \\mathbb E(X | \\mathcal F)$ is a random variable, $\\mathcal F$/ $\\mathcal B (\\mathbb R )$- measurable. From Y, we can only determine ONE thing (think about this!):\n$$\\mathbb E(Y) = \\mathbb E(\\mathbb E(X | \\mathcal F)) = \\mathbb E X.$$\nSo, what is $\\mathbb{E}(X | \\mathcal F)$? It is the most simplified knowledge that we can have; we arrive at this if we determine the expectation of the random variable but know nothing about its values in particular events (in $\\mathcal P (\\Omega)$).\n(In fact, $Y$ is constant... otherwise, it would not be measurable.) \nSuppose now that we enlarge this $\\sigma$-algebra, say to $\\mathcal F' = \\{\\emptyset, A, A^c, \\Omega\\}$, for some non-trivial set $A$. Again, suppose that we only know $\\mathbb{E}(X | \\mathcal F')$, not X. Then, we can determine three things about the variable:\n$$\\mathbb E(X 1_A), \\, \\mathbb E(X 1_{A^c}) \\text{ and } \\mathbb E (X).$$\nConclusion: a bigger $\\sigma$ algebra implies more knowledge about the random variable X (we are interested in that one)!\nCheck that in the extreme case, when $\\mathcal F'' =\\mathcal P (\\Omega)$, the knowledge of $\\mathbb E (X|\\mathcal F'')$ allows us to determine all the expected values $\\mathbb E(X 1_{\\{X=x\\}})= x\\mathbb P (X=x)$, because the events $\\{X=x\\}$ are contained in $\\mathcal F''$ (like every other subset). If $X$ only take a finite number of different values (for instance, when $\\Omega$ is finite), these expectations are enough to determine  the probability of all the events $\\{X=x\\}$. (When $X$ is continuous, the above reasoning is not very useful, for the subsets $\\{X=x\\}$ have probability zero and the expectations above are zero too. Anyway, by the general properties of the conditional expectation, $\\mathbb E(X|\\mathcal F'') = X$, because $X$ is $F''$-measurable. In this sense, we can say that the variable is recovered from its conditional expectation.)", "meta": {"post_id": 690531, "input_score": 45, "output_score": 38, "post_title": "Intuition for random variable being $\\sigma$-algebra measurable?"}}
{"input": "Say we have an $n\\times m$ matrix $X$. What are the specific properties that $X$ must have so that $A=X^TX$ invertible?\nI know that when the rows and columns are independent, then matrix $A$ (which is square) would be invertible and would have a non-zero determinant. However, what confuses me is, what sort of conditions must we have on each row of $X$ such that $A$ would be invertible. \nIt would be very nice to have a solution of the form:\n\nwhen $n > m$ then $X$ must have...\nwhen $n < m$ then $X$ must have...\nwhen $n = m$ then $X$ must have...\n\nI think in the 3rd case we just need $X$ to be invertible but I was unsure of the other two cases.", "output": "Precisely when the rank of $X$ is $m$ (which forces $n\\geq m$). \nThe key observation is that for $v\\in\\mathbb R^m$, $Xv=0$ if and only if $X^TXv=0$. For the non-trivial implication, if $X^TXv=0$, then $v^TX^TXv=0$, that is $(Xv)^TXv=0$, which implies that $Xv=0$.\nIf the rank of $X$ is $m$, this means that $X$ is one-to-one when acting on $\\mathbb R^m$. So by the observation, $X^TX$ is one-to-one, which makes it invertible (as it is square). \nConversely, if the rank of $X$ is less than $m$, there exists $v\\in\\mathbb R^m$ with $Xv=0$. Then $X^TXv=0$, and $X^TX$ cannot be invertible.", "meta": {"post_id": 691812, "input_score": 38, "output_score": 35, "post_title": "Proof of when is $A=X^TX$ invertible?"}}
{"input": "Let $\\phi : R \\to S$ be a homomorphism. If $N$ is an ideal of $S$, then $\\phi ^{-1} (N)$ is an ideal of $R$.", "output": "it is the kernel of $R \\rightarrow S \\rightarrow S/N$", "meta": {"post_id": 693618, "input_score": 13, "output_score": 39, "post_title": "Prove: The pre-image of an ideal is an ideal."}}
{"input": "I am working on a physics task, and reduced it to the following equation for $y$:\n$$\\frac{1}{4y^3}-\\frac{2}{(y^2+b^2)^{\\frac{3}{2}}}=0$$\nI handed it to Mathematica, and it gave me two real solutions, $$y_{1,2} = \\pm\\frac{b}{\\sqrt{3}},$$\nalong with some complex ones.\nMy question is, how can I see this? I mean, how can I solve such an equation by hand? I tried moving stuff around a bit, but all I found was a sextic equation, which I really didn't feel like approaching. Is there any easier way? Does it lower the hassle, if only real solutions are to be found?", "output": "First, get rid of the denominators,\n$$\\begin{align}\n\\frac{1}{4y^3} - \\frac{2}{(y^2+b^2)^{3/2}} &= 0\\\\\n\\iff \\frac{1}{4y^3} &= \\frac{2}{(y^2+b^2)^{3/2}}\\\\\n\\iff (y^2+b^2)^{3/2} &= 8y^3.\n\\end{align}$$\nThen, raise it to the $2/3$-th power to simplify it, introducing a third root of unity,\n$$\\begin{align}\ny^2 + b^2 &= 4\\rho y^2\\\\\n\\iff b^2 &= (4\\rho-1)y^2\\\\\n\\iff y &= \\pm \\frac{b}{\\sqrt{4\\rho-1}}.\n\\end{align}$$\nChoosing $\\rho = 1$ as the third root of unity yields the two real solutions. $\\rho = e^{\\pm 2\\pi i/3}$ yields non-real solutions.", "meta": {"post_id": 694335, "input_score": 11, "output_score": 55, "post_title": "Is it possible to solve this equation by hand?"}}
{"input": "As you know, we use the \"Secretary Problem\" to choose the single best candidate. Now I would like to know can we use this rule to find the worst candidate, too? If yes, how to accomplish this?", "output": "Yes. \nThe algorithm simply finds the participant who is optimal according to some property, but does not care what that property means (as long as it defines an ordering relation on the candidates). Changing that property from being good to being bad (or whatever else) is just a definition without algorithmic relevance.", "meta": {"post_id": 697501, "input_score": 14, "output_score": 45, "post_title": "Can I use the \"Secretary Problem\" to find the worst candidate, too?"}}
{"input": "I'm struggling with this integral\n$$I=\\int_0^1\\frac{1-x^2+\\left(1+x^2\\right)\\ln x}{\\left(x+x^2\\right)\\ln^3x}dx.\\tag1$$\nMathematica could not evaluate it in a closed form. Its numeric value is approximately $I\\approx0.7804287418294087023386965471512328112...$$^\\text{[more]}$, but I could not find a plausible closed form for this number using inverse symbolic calculators available online. \n\nUpdate: Based on Raymond Manzoni's comment below, there is actually a conjectural closed form, numerically matching up to at least $10^3$ decimal digits:\n$$I\\stackrel?=6\\ln A-\\frac{\\ln4}3-\\frac14,\\tag2$$\nwhere $A$ is the Glaisher-Kinkelin constant:\n$$A=\\exp\\left(\\frac1{12}-\\zeta'(-1)\\right).\\tag3$$\nCould you suggest a proof of the conjecture $(2)$?", "output": "We start from the first Binet's formula:\n$$\\ln\\Gamma(z)=\\left(z-\\tfrac12\\right)\\ln z-z+\\frac{\\ln(2\\pi)}2+\\int_0^\\infty\\left(\\frac12-\\frac1t+\\frac1{e^t-1}\\right)\\frac{e^{-t\\,z}}t dt.\\tag1$$\nChange variable $t=-2\\ln x$:\n$$\\ln\\Gamma(z)=\\left(z-\\tfrac12\\right)\\ln z-z+\\frac{\\ln(2\\pi)}2+\\frac12\\int_0^1x^{2z-1}\\frac{1-x^2+(1+x^2)\\ln x}{(x^2-1)\\ln^2x}dx.\\tag2$$\nIntegrate on interval $0<z<\\frac12$:\n$$\\psi^{(-2)}\\left(\\tfrac12\\right)=\\frac1{16}+\\frac{\\ln8}8+\\frac{\\ln\\pi}4+\\frac14\\int_0^1\\frac{1-x^2+(1+x^2)\\ln x}{(x+x^2)\\ln^3x}dx.\\tag3$$\nUse formula $(5)$ from here connecting Barnes G-Function and negapolygamma, and let $z=\\frac12$:\n$$\\ln G\\left(\\tfrac12\\right)=\\frac{\\ln(2\\pi)}4+\\frac18-\\frac{\\ln\\pi}4-\\psi^{(-2)}\\left(\\tfrac12\\right).\\tag4$$\nUse formula $(19)$ from the same page to get a closed form for $\\ln G\\left(\\tfrac12\\right)$ in terms of the Glaisher-Kinkelin constant $A$:\n$$\\ln G\\left(\\tfrac12\\right)=-\\frac{3\\ln A}2-\\frac{\\ln\\pi}4+\\frac18+\\frac{\\ln2}{24}.\\tag5$$\nComparing $(4)$ and $(5)$ we can get:\n$$\\psi^{(-2)}\\left(\\tfrac12\\right)=\\frac{3\\ln A}2+\\frac{\\ln\\pi}4+\\frac{5\\ln2}{24}\\tag6.$$\nFinally, from $(3)$ and $(6)$ it follows:\n$$\\int_0^1\\frac{1-x^2+(1+x^2)\\ln x}{(x+x^2)\\ln^3x}dx=6\\ln A-\\frac{\\ln4}3-\\frac14,\\tag7$$\nthat proves the conjecture.", "meta": {"post_id": 705969, "input_score": 34, "output_score": 39, "post_title": "Integral $\\int_0^1\\frac{1-x^2+\\left(1+x^2\\right)\\ln x}{\\left(x+x^2\\right)\\ln^3x}dx$"}}
{"input": "How can I prove that\n$$\n\\lim_{n\\rightarrow\\infty}\\frac{\\left\\lfloor x^{n+1} \\right\\rfloor}{\\left\\lfloor x^n \\right\\rfloor}=x,\n$$\nwhenever $x>1$. Here $\\left\\lfloor \\cdot\\right\\rfloor$ denotes the floor function,\nor the integer part function. \nThe integer part $\\lfloor z\\rfloor$ of $z$ is the largest integer, which does not exceed $z$.\nThanks for your answer.", "output": "Since $y-1< \\lfloor y\\rfloor\\le y$, for every $y\\in\\mathbb R$, then\n$$\n\\frac{x^{n+1}-1}{x^{n}}<\\frac{\\lfloor x^{n+1}\\rfloor}{\\lfloor x^n\\rfloor}< \\frac{x^{n+1}}{x^n-1},\n$$\nand hence\n$$\nx-\\frac{1}{x^n}<\\frac{\\lfloor x^{n+1}\\rfloor}{\\lfloor x^n\\rfloor}<x+\\frac{x}{x^n-1},\n$$\nor\n$$\n-\\frac{1}{x^n}<\\frac{\\lfloor x^{n+1}\\rfloor}{\\lfloor x^n\\rfloor}-x<\\frac{x}{x^n-1}.\n$$\nSince both\n$\n-\\frac{1}{x^n},\\,\\frac{x}{x^n-1}\\to 0, \\quad\\text{as}\\quad n\\to\\infty,\n$\nthen $\\frac{\\lfloor x^{n+1}\\rfloor}{\\lfloor x^n\\rfloor}\\to x$.", "meta": {"post_id": 707294, "input_score": 8, "output_score": 34, "post_title": "If $\\,x>1$, then $\\lim\\limits_{n\\rightarrow\\infty}\\frac{\\left\\lfloor x^{n+1} \\right\\rfloor}{\\left\\lfloor x^n \\right\\rfloor}=x$."}}
{"input": "Given Point A and Point B in 2D space, how can I find the angle Point B is from Point A? 0\u00b0 can be any direction; it doesn't matter. For example, Point A is at (0, 10) and Point B is at (10, 20). The angle is 45\u00b0 in this example (assuming 0\u00b0 is up).", "output": "I believe that the accepted answer does not correctly solve the problem for many people visiting this question. I am doing 2D simulations and the answer above does not solve my problem. Imagine the following circle:\n$\\hskip1.8in$\nAssume that the middle of the circle is point A. Point B is at some angle from A according to the angles of the circle (so 0\u00b0) is right.\n$\\hskip2in$The atan2 function is what you need!\n$$ atan2(y, x) $$\n$\\hskip3.2in$ Where\n$$ y = y_B - y_A $$\n$$ x = x_B - x_A $$\nRead more about it here.", "meta": {"post_id": 707673, "input_score": 23, "output_score": 37, "post_title": "Find angle in degrees from one point to another in 2D space?"}}
{"input": "I have seen descriptions of the \"line with two origins\" using quotient spaces. My professor has defined it in an alternate way. However, I can not wrap my head around how the following descriptions forms a line with two origins. \nConsider $X=\\mathbb{R} \\setminus \\{0\\} \\cup \\{p,q\\}$, that is $X$ is the union of the reals minus $0$, and two points. Consider sets of the type\n$U_a = (-a,0) \\cup \\{p\\} \\cup (0,a)$\n$V_a=(-a,0) \\cup \\{q\\} \\cup (0,a)$\nwhere $a >0$. And let\n$\\mathcal{B}=\\{U_a\\}_{a>0} \\cup \\{V_a\\}_{a >0} \\cup \\{ \\text{all open intervals of} \\hspace{2mm} \\mathbb{R} \\hspace{2mm} \\text{not containing the origin} \\}$\nThen $\\tau=\\{\\bigcup_{\\alpha} B_{\\alpha} \\big | B_{\\alpha} \\in \\mathcal{B} \\}$.\nHow is this a line with two origins?", "output": "Let's make the construction a little simpler.\nWe start with a line, $\\Bbb R$.\n\nThen we removed the origin $0$, so $\\Bbb R\\setminus\\{0\\}$.  Let's call that $\\def\\rstar{\\mathbb R^\\star}\\rstar$.\n\nThen we add a new point $p$\u2014a new point, not a real number\u2014and we have $\\Bbb \\rstar\\cup\\{p\\}$.\nAnd the way we add back this point is special.  Where before we had some open set $U$ that contained $0$, we now have an open set $U\\setminus\\{0\\}\\cup\\{p\\}$.  This set is exactly $U$, but with $0$ replaced by $p$. Where before we had an open set $V$ not containing $0$, we keep $V$ unchanged; $V$ is still open.\n\nSo the open sets are just like the ones we had before, except that $0$ has been replaced by $p$.  Topologically, the point $p$ behaves just like $0$ did before.  Where before we had $0$ in some open set $U$, we now have $p$ in some analogous open set $U\\setminus\\{0\\}\\cup\\{p\\}$. $p$ is a perfect replacement for the origin $0$ that we deleted.  Really it's just $0$, but with another name.\nThis new space, $\\Bbb R\\setminus\\{0\\}\\cup\\{p\\}$, is exactly $\\Bbb R$, except that $0$ has been removed and replaced  by $p$.  It's easy to show that this space is topologically identical to $\\Bbb R$.  The homeomorphism is particularly simple: it is the identity function, except that it takes $0$ to $p$, because $\\Bbb R\\setminus\\{0\\}\\cup\\{p\\}$ has  has $p$ instead of $0$.\nGot that?\n\nNow we add another new point $q$, in exactly the same way we added $p$: if $U$ was open before, and $0\\in U$, then $U\\setminus\\{0\\}\\cup\\{q\\}$ is open now.  This set is exactly $U$, but with $0$ replaced by $q$.\n\nSo now we have something like $\\Bbb R$, except it has this extra point $q$.  But $q$ has all the same properties that $p$ has!  And in particular, just as $p$ was a perfect replacement for the origin $0$ that we deleted, $q$ is also a perfect replacement for the origin $0$ that we deleted.\nAnd just as $\\rstar\\cup\\{p\\}$ was homeomorphic to $\\Bbb R$, so is  $\\rstar\\cup\\{q\\}$ homeomorphic to $\\Bbb R$.\nBut this new space is $\\rstar\\cup\\{p,q\\}$ and has both $p$ and $q$. We can delete either one of them and get a space identical to $\\Bbb R$.\nWe deleted the origin $0$ and replaced it with $p$ and with $q$, so we now have something like $\\Bbb R$, except that instead of one origin it has two, $p$ and $q$.\n\nSo it is called the line with two origins.", "meta": {"post_id": 709777, "input_score": 10, "output_score": 36, "post_title": "The Line with two origins"}}
{"input": "so if given a covariance matrix I can find the eigenvalues and move forward from there... but I seem to have trouble with the step before if I am given a data set and am told to create the covariance matrix. Looking at the notes I see the formula:\n$$cov(x) = \\frac{1}{n-1}\\sum(x_i - \\bar{x})(y_i -\\bar{y})  \n$$\nI'm not too sure what to do with this formula and was hoping you can tell me how.\nData Set:\n\n    X1 | X2\n    ---|---\n    3  | 7\n    2  | 4", "output": "The variance-covariance matrix has the following structure:\n$$\\begin{bmatrix}\nvar(x) & cov(x,y) \\\\\ncov(x,y) & var(y)\n\\end{bmatrix}$$\nwhere $var(x) = \\frac{1}{n-1} \\sum (x_i - \\bar{x})^2 $ and $cov(x,y) = \\frac{1}{n-1} \\sum (x_i - \\bar{x})(y_i - \\bar{y}) $.\nfor your data, \n$\\bar{x} = \\frac{(3 + 2)}{2} = \\frac{5}{2}$\n$\\bar{y} = \\frac{(7 + 4)}{2} = \\frac{11}{2}$\n$var(x) = (3 - \\frac{5}{2})^2 + (2 - \\frac{5}{2})^2$\n$var(y) = (7 - \\frac{11}{2})^2 + (4 - \\frac{11}{2})^2$\n$cov(x,y) = (3 - \\frac{5}{2})(7 - \\frac{11}{2}) + (2 - \\frac{5}{2})(4 - \\frac{11}{2})$\nso, all you need to do is calculate these values and put them in the right places in the matrix. Does that make sense?", "meta": {"post_id": 710214, "input_score": 24, "output_score": 50, "post_title": "How to construct a covariance matrix from a 2x2 data set"}}
{"input": "Hartshorne defines a closed immersion as a morphism $f:Y\\longrightarrow X$ of schemes such that \na) $f$ induces a homeomorphism of $sp(Y)$ onto a closed subset of $sp(X)$, and furthermore \nb) the induced map $f^\\#:\\mathcal{O}_X\\longrightarrow f_* \\mathcal{O}_Y$ of sheaves on $X$ is surjective.\nMy doubt is that why is the condition a) necessary?\nAtleast for affine schemes, if $f:(\\textrm{Spec}\\ B,\\mathcal{O}_{\\textrm{Spec}\\ B})\\longrightarrow (\\textrm{Spec}\\ A,\\mathcal{O}_{\\textrm{Spec}\\ A})$ is a morphism such that condition (b) holds, then we have the stalk level maps are all surjective. But the stalk level maps are all localization maps which will mean that $f^\\#_{\\mathcal{O}_{\\textrm{Spec}\\ A}}:A\\longrightarrow B$ is itself surjective. But that itself will mean that $f$ is a closed immersion right?\nThe definition made in Hartshorne must be because this does not carry through for general schemes. But I am not able to think of any example. Any help in this respect will be appreciated.", "output": "I figured I would make an answer out the comments to the first answer addressing a point which confused me when I first learned this stuff. A morphism $f:X\\to Y$ (I have to write it in this direction or else I'll confuse myself) is said to be a closed immersion if $f$ induces a homeomorphism of $X$ onto a closed subset of $Y$, and $f^\\sharp:\\mathscr{O}_Y\\to f_*(\\mathscr{O}_X)$ is surjective.\nIn some references I've seen it is casually remarked that the second condition is equivalent to surjectivity of the map $f_x^\\sharp:\\mathscr{O}_{Y,f(x)}\\to\\mathscr{O}_{X,x}$ for all $x\\in X$. But is this really trivial? No! This map, which might reasonably be called the stalk of the morphism $f$ at $x$, is not literally the same as the stalk of $f^\\sharp$ at $f(x)$. Indeed, that is a map $f_{f(x)}^\\sharp:\\mathscr{O}_{Y,f(x)}\\to(f_*\\mathscr{O}_X)_{f(x)}$. In general, there is always a natural map $\\varphi_x:(f_*\\mathscr{O}_X)_{f(x)}\\to\\mathscr{O}_{X,x}$, but it isn't in general an isomorphism. The map $f_x^\\sharp$ is equal to $\\varphi_x\\circ f_{f(x)}^\\sharp$. So while it is standard that the map $f^\\sharp$ of sheaves (on $Y$!) is surjective if and only if $f_y^\\sharp:\\mathscr{O}_{Y,y}\\to (f_*\\mathscr{O}_X)_y$ is surjective for all $y\\in Y$, this does not obviously say anything about surjectivity of the maps $f_x^\\sharp:\\mathscr{O}_{Y,f(x)}\\to\\mathscr{O}_{X,x}$ for $x\\in X$. If however $f$ is a homeomorphism onto a closed subset $f(X)\\subseteq Y$, then the stalks of $f_*\\mathscr{O}_X$ at points of $Y$ are easy to compute: they are zero at points outside of $f(X)$, and at a point $f(x)\\in f(X)$, we have that the natural map $(f_*\\mathscr{O}_X)_{f(x)}\\to\\mathscr{O}_{X,x}$ is an isomorphism. So in that case, surjectivity of each $f_x^\\sharp$, $x\\in X$, actually will imply surjectivity of $f_y^\\sharp$, $y\\in Y$, and hence of $f^\\sharp$.\nWithout the condition that $f$ is a closed topological immersion on the underlying topological spaces, it is not going to be true that $f^\\sharp$ is surjective if and only if $f_x^\\sharp$ is surjective for all $x\\in X$. To make this clearer, let's assume $X=\\mathrm{Spec}(B)$ and $Y=\\mathrm{Spec}(A)$, so $f=\\mathrm{Spec}(\\alpha)$ for $\\alpha:A\\to B$ a ring homomorphism. The stalk map of $f$ at $x=\\mathfrak{q}\\in\\mathrm{Spec}(B)$ is the ring map $A_\\mathfrak{p}\\to B_\\mathfrak{q}$, where $\\mathfrak{q}=\\alpha^{-1}(\\mathfrak{p})$. In general, surjectivity of this map for all $\\mathfrak{q}\\in\\mathrm{Spec}(B)$ does not imply surjectivity of $\\alpha$ itself.\nI think the simplest example that will illustrate this is when $B=A_g$ is a principal localization of $A$. Then in fact the stalk map in the previous paragraph is an isomorphism for every prime ideal of $A_g$ (the set of which are in natural bijection with the set of primes of $A$ not containing $g$, i.e. $D(g)$). But the localization map $A\\to A_g$ (i.e. the map on global sections of $f$) is not usually surjective. Note that in this case $f$ is a homeomorphism onto the open subset $D(g)$ of $\\mathrm{Spec}(A)$, but $D(g)$ is not generally closed in $A$.\nI think maybe this illustrates why the first condition is important, and why, if one wants to think about surjectivity of $f^\\sharp$ in terms of the stalks of $f$, $f_x^\\sharp$, for $x\\in X$, the topological condition is needed, and logically \u201cprecedes\u201d the condition on $f^\\sharp$.\nLastly, I should note that the maps which I have been calling the \u201cstalks of $f$,\u201d $f_x^\\sharp$, for $x\\in X$, are in fact the stalks of the map of sheaves on $X$ (in the usual sense) $f^\\flat:f^{-1}\\mathscr{O}_Y\\to \\mathscr{O}_X$ corresponding to $f^\\sharp$ under the adjunction between $f^{-1}$ and $f_*$. So surjectivity of all $f_x^\\sharp$, $x\\in X$, is logically equivalent to surjectivity of $f^\\flat$. There is no reason to believe that $f^\\flat$ is surjective if and only if $f^\\sharp$ is, or even that there is an implication in either direction in general.", "meta": {"post_id": 712332, "input_score": 18, "output_score": 47, "post_title": "Closed immersion definition"}}
{"input": "Let $\\operatorname{Ei}x$ denote the exponential integral:\n$$\\operatorname{Ei}x=-\\int_{-x}^\\infty\\frac{e^{-t}}tdt.\\tag1$$\nIt's not difficult to find that\n$$\\int\\operatorname{Ei}x\\,dx=x\\,\\operatorname{Ei}x-e^x,\\tag2$$\n$$\\int\\operatorname{Ei}^2x\\,dx=x\\,\\operatorname{Ei}^2x-2\\,e^x\\operatorname{Ei}x+2\\,\\operatorname{Ei}(2x)\\tag3$$\nand\n$$\\int_{-\\infty}^0\\operatorname{Ei}x\\,dx=-1,\\tag4$$\n$$\\int_{-\\infty}^0\\operatorname{Ei}^2x\\,dx=\\ln4.\\tag5$$\n\nIs it possible generalize these results for higher powers of $\\operatorname{Ei}x$?\nIn particular, are there closed forms for\n$$\\int\\operatorname{Ei}^3x\\,dx\\tag6$$\nor\n$$\\int_{-\\infty}^0\\operatorname{Ei}^3x\\,dx\\ ?\\tag7$$", "output": "$$\\begin{align}\\int_0^\\infty\\operatorname{Ei}^3(-x)\\,dx&=-3\\operatorname{Li}_2\\left(\\frac14\\right)-6\\ln^22.\\\\\\\\\\int_0^\\infty\\operatorname{Ei}^4(-x)\\,dx&=24\\operatorname{Li}_3\\left(\\frac14\\right)-48\\operatorname{Li}_2\\left(\\frac13\\right)\\ln2-13\\,\\zeta(3)\\\\&-32\\ln^32+48\\ln^22\\,\\ln3-24\\ln2\\,\\ln^23+6\\pi^2\\ln2.\\end{align}$$", "meta": {"post_id": 714628, "input_score": 27, "output_score": 38, "post_title": "Closed form for $\\int_{-\\infty}^0\\operatorname{Ei}^3x\\,dx$"}}
{"input": "I have been reading Terry Tao's notes on Real Analysis and there's a part he just says, but does not really explain, so I am wondering if someone here would. The notes are http://terrytao.wordpress.com/2010/10/02/245a-notes-4-modes-of-convergence/ and my particular question is from Section 4, Corollary 3. It goes as follows,\nLet $f_n \\rightarrow f$ in $L^1$ then there exists a sub sequence $(f_{n_j}) \\subset (f_n)$ such that $f_{n_j} \\rightarrow f$ pointwise a.e. Moreover $(f_{n_j})$ converges almost uniformly to $f$.\nThe proof he gives is simply that since $\\|f_n-f\\|_1 \\rightarrow 0$ as $n \\rightarrow \\infty$ we can pick a sub sequence such that $\\|f_{n_j}-f\\|_1<2^{-j}$ which is enough to show pointwise a.e and almost uniform convergence. But what allows you to pick such a sub sequence is it maybe some Cauchy property or is it some weird construction? Then how do you go from that to pointwise a.e and even almost uniform convergence. I am assuming that for almost uniform, you do something similar to Egorov's theorem without the assumption the domain of $f$ has finite measure. Also I am aware that if you get almost uniform, you immediately have pointwise a.e, but I'd like to see how to get to both. Thank you.", "output": "Choose $N_{k}$ such that $N_{1} \\le N_{2} \\le N_{3} \\le \\cdots$ and such that $\\|f_{m}-f_{n}\\| < 1/2^{k}$ whenever $m, n \\ge N_{k}$. This is possible because $\\{ f_{n}\\}_{n=1}^{\\infty}$ is a Cauchy sequence. Then $\\{ f_{N_{k}}\\}_{k=1}^{\\infty}$ is a subsequence such that $\\|f_{N_{l}}-f_{N_{m}}\\| < 1/2^{k}$ whenever $l,m \\ge k$. Then\n$$\n           f_{N_{m}}=f_{N_{1}}+\\sum_{l=1}^{m}f_{N_{l+1}}-f_{N_{l}}\n$$\nconverges pointwise a.e. absolutely because\n$$\n         g_{m}=|f_{N_{1}}| + \\sum_{l=1}^{m}|f_{N_{l+1}}-f_{N_{l}}|\n$$\nconverges pointwise a.e. to an extended real function $0 \\le g \\le \\infty$ such that, by the monotone convergence theorem,\n$$\n         \\int g\\,d\\mu = \\int |f_{N_{1}}|d\\mu+\\sum_{l=1}^{\\infty}\\int |f_{N_{l+1}}-f_{N_{l}}|d\\mu = \\|f_{N_{1}}\\|+\\sum_{l=1}^{\\infty}\\|f_{N_{l+1}}-f_{N_{l}}\\| < \\infty.\n$$\nSo $g < \\infty$ a.e., which means that $\\lim_{l}f_{N_{l}}$ converges pointwise a.e.. to an $L^{1}$ function.", "meta": {"post_id": 714744, "input_score": 26, "output_score": 38, "post_title": "$L^1$ convergence gives a pointwise convergent subsequence"}}
{"input": "If $n\\times n$ matrix $A$ has eigenvalues $1,-1$ and $n\\times n$ matrix $B$ also has eigenvalues $1,-1$, can I then say something about eigenvalues of $AB$ and $BA$?", "output": "If you are talking about the relationship between the eigenvalues of $AB$ and $BA$, there is a nice and well-known result, first discovered by J. J. Sylvester: $AB$ and $BA$ have identical spectra. (More generally, for any possibly rectangular matrices $A$ and $B$ with appropriate sizes, $AB$ and $BA$ share the same multi-set of nonzero eigenvalues.)\nHowever, if you are talking about the relationship between the eigenvalues of $A,B$ and $AB$, there is only one thing that one can say:\n\nthe product of the eigenvalues of $AB$ is equal to the product of all eigenvalues in $A$ and $B$; this is because $\\det(AB)=\\det(A)\\det(B)$.\n\nNothing further can be said without additional information. To illustrate, suppose $n=2$ and the spectra of both $A$ and $B$ are $\\{1,-1\\}$. The determinantal constraint in the bullet point above dictates that the spectrum of $AB$ must be $\\{\\lambda,\\frac1{\\lambda}\\}$ for some nonzero $\\lambda$. Now, is $\\{\\lambda,\\frac1{\\lambda}\\}$ really a possible spectrum of $AB$ for every $\\lambda\\ne0$? The answer is yes. Let $2t=\\lambda+\\frac1{\\lambda}$ and\n$$\nA=\\pmatrix{1&0\\\\ 0&-1},\\ B=\\pmatrix{t&1\\\\ 1-t^2&-t},\n\\ AB=\\pmatrix{t&1\\\\ t^2-1&t}.\n$$\nYou may verify that $\\operatorname{trace}(B)=0,\\,\\det(B)=-1,\\,\\operatorname{trace}(AB)=2t,\\,\\det(AB)=1$ and hence the characteristic polynomials of $B$ and $AB$ are respectively $x^2-1$ and $x^2-2tx+1=(x-\\lambda)(x-\\frac1{\\lambda})$. Hence the spectrum of $B$ is indeed $\\{1,-1\\}$ and the spectrum of $AB$ is $\\{\\lambda,\\frac1{\\lambda}\\}$. In other words, apart from the determinantal constraint mentioned in the bullet point above, the eigenvalues of $AB$ can be pretty much anything.", "meta": {"post_id": 716990, "input_score": 41, "output_score": 55, "post_title": "If $A,B$ have eigenvalues $\\pm1$, what can I say about the eigenvalues of $AB,BA$?"}}
{"input": "My question concern the definition, geometric meaning, and usage of the normal bundle in algebraic geometry.\nLet $X$ be a nonsingular variety over an algebraically closed field $k$, and $Y\\subseteq X$ a nonsingular closed subvariety. Let $\\mathcal{I}$ be the sheaf of ideals defined by the closed embedding $i: Y \\hookrightarrow X$, and consider the sheaf $\\mathcal{C}_{Y/X}=: \\mathcal{I}/\\mathcal{I}^{2}$. We define the normal sheaf of $Y$ in $X$ to be $\\mathcal{N}_{Y/X}=:\\mathsf{Hom}(\\mathcal{C}_{Y/X},\\mathcal{O}_{Y})$.\nThis definition can be found on Hartshorne's \"Algebraic Geometry\". Recently I came across some concrete examples of normal bundles that I cannot understand.\n$\\mathbf{(1)}$ Let $C$ be a nonsingular curve of degree $2$ and genus $0$ in $\\mathbb{P}^{3}_{k}$. with $k$ an algebraically closed field. It can be proven that for any such curve, there exists a quadric $Q$ in $\\mathbb{P}^{3}_{k}$ such that $C$ lies on $Q$. Then $\\mathcal{N}_{C/Q}=\\mathcal{O}_{C}(1)$, and $\\mathcal{N}_{S}|_{C}=\\mathcal{O}_{C}(2)$.\n$\\mathbf{(2)}$ Let $X$ a nonsingular irreducible cubic surface in $\\mathbb{P}^{3}_{k}$, with $k$ an algebraically closed field. Let $H$ the hyperplane section of $X$. It can be proven that there exists a nonsingular irreducible curve $C\\in |4H+2L|$ of degree $14$ and genus $24$, for a line $L$ on $X$. Then $\\mathcal{N}_{C/X}=\\mathcal{O}_{C}(C)$, and $\\mathcal{N}_{X}=\\mathcal{O}_{X}(3)$.\nThe geometrical meaning of the twisting sheaf, and the Picard group construction are very clear to me. I don't understand how can normal sheaves be computed in the examples presented above. However, rather than focusing on those two cases, I would like to understand the nature of the normal sheaf - i.e. what does it say about the varieties? Is there a more agile definition? How do normal sheaves relate to twisting sheaves? How can they be computed? I am mostly interested in the case of regular projective schemes of low dimension, over an algebraically closed field. Simple examples are also welcome.", "output": "First of all, a warning (to myself first, because I use to get confused). The sheaf $I/I^2$ is a sheaf on $X$. This is not the conormal sheaf. Of course, it is supported on $Y$, but the conormal sheaf is the sheaf $i^\\ast(I/I^2)$, and this is really on $Y$. That said, I will write $I/I^2$ instead of $i^\\ast(I/I^2)$.\n\nExample. Let us compute the normal bundle of a plane conic $C\\subset\\mathbb P^2$.\nThe ideal of the conic is $I=\\mathscr O_{\\mathbb P^2}(-2)$, so $$I/I^2=I|_C=\\mathscr O_{\\mathbb P^2}(-2)|_C=\\mathscr O_{C}(-2)\\,\\Rightarrow\\,\\mathcal N_{C/\\mathbb P^2}=\\mathscr O_C(2).$$\nNote that this line bundle has actually a $5$-dimensional space of sections, as $$\\mathscr O_C(2)\\cong \\mathscr O_{\\mathbb P^1}(4).$$\nThis \"$5$\" is the dimension of the Hilbert scheme of plane conics, which is the smooth space $\\mathbb P^5$. Actually, the vector space $H^0(C,\\mathcal N_{C/\\mathbb P^2})$ computes the tangent space of this $\\mathbb P^5$ at $[C]$, and the latter is the space of deformations of the conic in the plane.\n\nThis is why I think it is useful to have in mind the following association:\n$$\\textrm{(Sections of the) Normal bundle }\\mathcal N_{Y/X}\\,\\,\\longleftrightarrow\\,\\,\\textrm{Deformations of } Y\\textrm{ inside }X.$$\nI will try to explain why, before coming to your examples.\nIn the case that interests you, namely that with $Y$ and $X$ both smooth, there is an exact sequence (called the conormal exact sequence):\n$$0\\to I/I^2\\to \\Omega_X|_Y\\to \\Omega_Y\\to 0.$$ Taking the dual, we find\n$$0 \\to T_Y\\to T_X|_Y\\to \\mathcal N_{Y/X}\\to 0.$$ \nThe normal bundle appears as cokernel of the map which identifies a tangent vector on $Y$ with a tangent vector on $X$, restricted to $Y$: those in the cokernel are tangent vectors restricted from $X$, up to those coming from $Y$. I cannot draw pictures here, but the idea is that a section of the normal bundle should be the datum of a family of  vectors, orthogonal (normal!) to the tangent spaces, and these normal vectors draw for you a \"nearby $Y$\" inside $X$, i.e. a deformation of $Y$ inside $X$. The zeros of a section should then be the points $y\\in Y$ where the vector was the zero vector: that particular $y$ did not contribute to the deformation.\nFormally, if $H$ is the Hilbert scheme of $X$, you have $$H^0(Y,\\mathcal N_{Y/X})=T_{[Y]}H=\\{\\textrm{Deformations of }Y\\textrm{ in }X\\}.$$\n\nNow for your examples.\n\nThe fact that $\\mathcal N_{S/\\mathbb P^3}|_C=\\mathscr O_C(2)$ is exactly as for the conic: $\\mathcal N_{S/\\mathbb P^3}=\\mathscr O_S(2)$. The fact that $\\mathcal N_{C/Q}=\\mathscr O_C(1)$ is because any such curve is obtained as a hperplane section of $Q$, so the line bundle $\\mathcal N_{C/Q}$ has to have degree $1$ on $C$.\nThe fact that $\\mathcal N_{X/\\mathbb P^3}=\\mathscr O_X(3)$ is again identical to the case of the conic, at the beginning. As for $\\mathcal N_{C/X}=\\mathscr O_C(C)$, it is something more general which happens (so you can forget about this special situation): if $Y\\subset X$ is a smooth divisor, its ideal is $I=\\mathscr O_X(-Y)$, so $$\\mathcal N_{Y/X}=(I/I^2)^\\vee=(\\mathscr O_X(-Y)|_Y)^\\vee=(\\mathscr O_Y(-Y))^\\vee=\\mathscr O_Y(Y).$$", "meta": {"post_id": 718192, "input_score": 24, "output_score": 44, "post_title": "Geometric interpretation and computation of the Normal bundle"}}
{"input": "Why is the identity element so important in this construction. I looked up some books and notes but still do not see why. How could the construction started from tangent space of a element other than identity possibly fail to get a Lie algebra so that people can only get it from the tangent space of identity?", "output": "If $G$ is a Lie group and $g \\in G$ then the map $L_g\\colon G \\to G$ defined by $x \\mapsto gx$ (so, left multiplication by $g$) is an isomorphism (a topological isomorphism, it's not a group homomorphism).  It's derivative gives an isomorphism between the tangent space $T_1G$ of $G$ at the identity and the tangent space $T_gG$ of $G$ at $g$.\nSo to answer your question, it's not special.  The tangent spaces at all points of $G$ are isomorphic.  So we just pick one to work with and the identity is the only element that every group is guaranteed to have, so we pick the identity.", "meta": {"post_id": 720469, "input_score": 26, "output_score": 39, "post_title": "Lie algebra: why does it have to be the tangent space at the IDENTITY of a Lie group?"}}
{"input": "It is said that $$\\bigcup_{n\\geq 1}\\left(\\frac 1n, 1+\\frac1n\\right)$$ is not compact.\nWhy?\nIs it because it is not closed? \nOr am I missing something more?\nMany thanks.", "output": "Here are four ways to see that $(0,1]$ is not compact.\n\nThe open cover you gave for $(0,1]$ (namely $\\{(1/n,1+1/n)\\,:n\\in\\mathbb N\\}$ does not have any finite subset which covers $(0,1]$ (in other words, does not have a finite subcover). I think this is the reason you were looking for, as user44441 said.\nA subset of $\\mathbb R^n$ is compact if and only if it is closed and bounded. $(0,1]$ is not closed (although it is bounded).\nExpanding on LAcarguy's comment, in a metric space ($\\mathbb R$ is a metric space) a subset is compact if and only if it is sequentially compact: every sequence of the subset has a convergent subsequence. The sequence $1,1/2,1/3,\\dots$ is contained in $S$ but each of its subsequences converges to $0$ and $0\\notin(0,1]$.\nIf $(0,1]$ were compact, it would be true that every continuous function $f: (0,1]\\to\\mathbb R$ attains a maximum and a minimum. But the function $f(x)=1/x$ defined on $(0,1]$ is continuous and unbounded.", "meta": {"post_id": 722506, "input_score": 8, "output_score": 36, "post_title": "Why isn't $(0,1]$ compact?"}}
{"input": "Let $A$ be a ring (which might or might not be commutative), and let $M,N$ and $K$ be three bi-modules over $A$.\nThere are two hom-tensor adjunctions. One says that\n\n$Hom_A(M\\otimes_A N, K) \\cong Hom_A(M,Hom_A(N,K))$.\n\nThe other says that\n\n$Hom_A(M\\otimes_A N, K) \\cong Hom_A(N,Hom_A(M,K))$.\n\nAre these isomorphisms of bimodules?\nIf so, does this mean that the two bimodules $Hom_A(N,Hom_A(M,K))$ and $Hom_A(M,Hom_A(N,K))$ are isomorphic?", "output": "Be careful. It's cleanest to describe the tensor-hom adjunction with three different rings instead of one, to make it as hard as possible to accidentally write down the wrong thing, so let $A, B, C$ be three different rings, let $_A M_B$ be an $(A, B)$-bimodule, let $_B N_C$ be a $(B, C)$-bimodule, and let $_A K_C$ be an $(A, C)$-bimodule. Then\n$$\\text{Hom}_C(M \\otimes_B N, K) \\cong \\text{Hom}_B(M, \\text{Hom}_C(N, K))$$\nas $(A, A)$-bimodules, and\n$$\\text{Hom}_A(M \\otimes_B N, K) \\cong \\text{Hom}_B(N, \\text{Hom}_A(M, K))$$\nas $(C, C)$-bimodules.\nSpecializing to the case that $A = B = C$ shows that your notation is sloppy (to be fair, so is mine): when you write $\\text{Hom}_A$ you haven't been careful about whether this means left $A$-module or right $A$-module homomorphisms, and it has different meanings in the different parts of your adjunctions unless $A$ is commutative and $M, N, K$ are plain $A$-modules, in which case there's no need to make left/right distinctions.\n(Specifically, $\\text{Hom}_A$ means left the second, fifth, and sixth times you used it, but right the first, third, and fourth times.)", "meta": {"post_id": 723368, "input_score": 15, "output_score": 38, "post_title": "Hom-tensor adjunctions"}}
{"input": "Does anybody know where I could find the solutions to the exercises from the book Lee, Introduction to Smooth Manifolds?\nI searched on the Internet and found only selected solutions but not all of them and not from the author.", "output": "Here's what I wrote in the preface to the second edition of Introduction to Smooth Manifolds:\n\nI have deliberately not provided written solutions to any of the problems, either\n  in the back of the book or on the Internet. In my experience, if written solutions\n  to problems are available, even the most conscientious students find it very hard\n  to resist the temptation to look at the solutions as soon as they get stuck. But it is\n  exactly at that stage of being stuck that students learn most effectively, by struggling\n  to get unstuck and eventually finding a path through the thicket. Reading someone\n  else\u2019s solution too early can give one a comforting, but ultimately misleading, sense\n  of understanding. If you really feel you have run out of ideas, talk with an instructor,\n  a fellow student, or one of the online mathematical discussion communities such as\n  math.stackexchange.com. Even if someone else gives you a suggestion that turns out\n  to be the key to getting unstuck, you will still learn much more from absorbing the\n  suggestion and working out the details on your own than you would from reading\n  someone else\u2019s polished proof.\n\nSo if you have questions about specific problems, by all means ask them here.  But posting a complete list of solutions will not be doing anyone a favor. Many instructors assign those problems as homework, and if complete solution sets become readily available, it makes the problems (and therefore the book) far less useful.\nIt's interesting to note that when I've written chapters with everything proved and few or no problems at the end, readers invariably ask me to provide some problems for them to work on.  If you want problems with solutions already written down, they're already there -- the theorems and examples in the book!  Just look at the statement of a theorem or the claims made in an example, close the book and try to prove the theorem on your own, and then go back and compare your work to the proof in the book.  (And if you find a better proof that the one I wrote, please let me know about it!)", "meta": {"post_id": 750163, "input_score": 25, "output_score": 226, "post_title": "Lee, Introduction to Smooth Manifolds Solutions"}}
{"input": "Would I be right to think that $$\\int dx \\,\\,\\,\\frac{\\partial}{\\partial x} f(x,y)=f(x,y)$$\nOr are there pathological cases?", "output": "If $x$ and $y$ are independent variables (and thus the $y$ is held constant during integration), then it is true that\n$$\n\\int \\frac{\\partial f}{\\partial x} dx = f(x,y) + C(y)\n$$\nwhere $C(y)$ is equivalent to the integration constant for the univariate case. As such, up to the \"constant\", you are right.\nIf $y=y(x)$, then it is not that simple. For instance, if $f(x,y)=x^2-xy+y^2$ and you integrate along the line $y=2x$, then you are actually integrating\n$$\n\\int \\frac{\\partial f}{\\partial x} dx = \\int (2x-y) dx = \\int 0 dx = 0\n$$", "meta": {"post_id": 754742, "input_score": 30, "output_score": 44, "post_title": "Integrating a Partial Derivative"}}
{"input": "Ordinary trigonometric functions are defined independently of exponential function, and then shown to be related to it by Euler's formula. \nCan one define hyperbolic cosine so that the formula\n$$\\cosh{x}=\\dfrac{e^x+e^{-x}}{2}$$\nbecomes something to be proven?", "output": "The more-geometrically-minded of us take $\\cosh u$ and $\\sinh u$ to be defined via the \"unit hyperbola\", $x^2 - y^2 = 1$, in a manner directly analogous to $\\cos\\theta$ and $\\sin\\theta$. Specifically, given $P$ a point on the hyperbola with vertex $V$, and defining $u$ as twice(?!) the area of the hyperbolic sector $OVP$, then $\\cosh u$ and $\\sinh u$ are, respectively the $x$- and $y$-coordinates of $P$.\n\nJust as in circular trig, we can assign measures $u$ (in \"hyperbolic radians\") to angles ---from the flat angle (when $u=0$) to half a right angle (when $u=\\infty$)--- and associate those measures with the lengths of the corresponding $\\cosh$ and $\\sinh$ segments. And, just as in circular trig (prior to the advent of imaginary numbers), we might be forgiven for suspecting that the correspondences $u \\leftrightarrow \\cosh u$ and $u \\leftrightarrow \\sinh u$ are \"non-arithmetical\", which is to say: that no arithmetical formula converts angle measures to their associated trig values.\nHowever, it turns out that the correspondences are not non-arithmetical; to find the appropriate arithmetical conversion formula, all we need is a bit of calculus ...\n\nEdit. (Two years later!) Check the edit history for an inelegant argument that I now streamline with the help of this trigonograph, in which lengths from the unit hyperbola have been scaled by $\\sqrt{2}$ (and, thus, areas by $2$):\n\nBecause the hyperbola is rectangular, we have that $|OX|\\cdot|XY|$ is a constant (here, $1$), which guarantees that the regions labeled $v$ have the same area (namely, $1/2$), and therefore that the regions labeled $u$ have the same area (namely, $u$). Now, the bit of calculus I promised, to evaluate $u$ as the area under the reciprocal curve:\n$$u = \\int_1^{|OX|}\\frac{1}{t}dt = \\ln|OX| \\quad\\to\\quad |OX| = e^{u} \\quad\\to\\quad |XY| = \\frac{1}{e^u}$$\nWith that, we clearly have\n$$2\\,\\sinh u \\;=\\; e^{u}- e^{-u} \\qquad\\qquad 2\\,\\cosh u \\;=\\; e^{u} + e^{-u}$$\nas desired. Easy-peasy!\nEnd of edit.\n\nThat hyperbolic radians are defined via doubling the area of a hyperbolic sector may seem at odds with the common definition of circular radians in terms of arc-length, but it's hard to argue with success, given the elegance of the formulas above. Even so, the hyperbolic twice-the-sector-area definition can be seen as directly analogous to the circular case, since circular radians are also definable as \"twice-the-sector-area\": In the unit circle, the sector with angle measure $\\pi/2$ radians has area $\\pi/4$ (it's a quarter-circle), the sector with angle measure $\\pi$ radians has area $\\pi/2$ (it's a half-circle), and the \"sector\" with angle measure $2\\pi$ radians has area $\\pi$ (it's the full circle); in these, and all other, cases, the angle measure is twice the sector area.", "meta": {"post_id": 757091, "input_score": 24, "output_score": 46, "post_title": "Alternative definition of hyperbolic cosine without relying on exponential function"}}
{"input": "I'm trying to find an example of a non-separable subspace of a separable space.\nWhat kind of examples are there?", "output": "If you don't care about separation axioms (e.g. Hausdorff, etc.) then you can take the following example:\n$\\Bbb R$ with the topology defined as $U$ is open if and only if $0\\in U$ or $U=\\varnothing$. Then $\\{0\\}$ is dense in this topology so the space is separable. \nBut $\\Bbb R\\setminus\\{0\\}$ is discrete (since given $x\\in\\Bbb R\\setminus\\{0\\}$ the set $\\{x,0\\}$ is open, so $\\{x\\}$ is relatively open). And uncountable discrete spaces cannot be separable.", "meta": {"post_id": 758424, "input_score": 23, "output_score": 35, "post_title": "Give an example of a non-separable subspace of a separable space"}}
{"input": "I have heard from various sources that the typical arithmetic operations (addition, subtraction, multiplication, division, rational exponentiation) are not sufficient to express in general the roots of a quintic polynomial. This is due to something along the lines of their inability to \"express the necessary symmetries.\"\nWould it be possible to introduce new arithmetic operations that allow do \"express the necessary symmetries\" and therefore allow a quintic formula to be written?", "output": "You can invent your own notation: say $r(a,b,c,d,e;n)$ for the $n^\\mathrm{th}$ smallest root of the quintic with coefficients $a, b, c, d, e$. You may object that this is kind of a cop-out, and you'd be right, but it's also basically what we do with quadratics. We get excited when we find out that we can \"solve\" $x^2 = 2$ by writing $x = \\sqrt 2$, but all we've really done is say \"the solution to $x^2 = 2$ is a number that, when squared, gives $2$\". Which is not so insightful.\nWhat's interesting about $\\sqrt a$ is that once we can \"solve\" equations like $x^2 - a = 0$ we find that we don't need anything new to solve $ax^2 + bx + c = 0$ \u2013 we can transform all such equations into the simpler form. Moreover, we can develop methods of reasoning with these solutions, so that e.g. $\\sqrt{x}\\sqrt{y} = \\sqrt{xy}$ and so forth (but note that there isn't really a great deal we can say about $\\sqrt 2 + \\sqrt 3$, for example). We don't so much say \"these are the solutions of this equation\" but \"this is a relationship between the solutions of this equation and this other one\", and that is something that takes genuine mathematical work and insight.\nAnyway, it sounds like Julian's answer is pretty much what you wanted, but I thought I'd provide some additional colour.", "meta": {"post_id": 760608, "input_score": 18, "output_score": 34, "post_title": "Can we introduce new operations that make quintics solvable?"}}
{"input": "EDIT:  Due to the solution below, I edited the answer of the post.  Thanks!!!!\nHi I am trying to calculate the infinite double sum \n$$\nS:=\\sum_{j,k=1}^\\infty \\frac{H_j(H_{k+1}-1)}{jk(k+1)(j+k)}=-4\\zeta(2)-2\\zeta(3)+4\\zeta(2)\\zeta(3)+2\\zeta(5),\\quad H_n:=\\sum_{k=1}^n\\frac{1}{k}\\ \\ \\ (\\text{Harmonic Numbers})\n$$\nThank you.\nI am not sure what to do, possibly write the sum as an integral and try working with the integral instead of the sum?  I was trying to figure out if we could write it as an integral representation in terms of logarithm functions.  But this will just give you this sum as the answer.  So I do not know how to calculate the zeta functions from the sum.  Note the Riemann Zeta function is given by \n$$\n\\zeta(s)=\\sum_{n=1}^\\infty \\frac{1}{n^s},\\quad \\zeta(2)=\\frac{\\pi^2}{6}.\n$$", "output": "You can find a rigorous and short proof of this sum in a paper from Panholzer and Prodinger titled Computer-free evaluation of an infinite double sum via Euler sums dated from 2005\n\n\n[Added 2014-04-27] Note: There is a  typo in the paper from Panholzer and Prodinger.\nThe correct identity is\n$$S:=\\sum_{j,k=1}^{\\infty}\\frac{H_j(H_{k+1}-1)}{jk(k+1)(j+k)}=-{\\color{red}{4}}\\zeta(2)-2\\zeta(3)+4\\zeta(2)\\zeta(3)+2\\zeta(5)$$\n\n\nRationale: When going through the details of the proof from Panholzer and Prodinger I also did some auxiliary calculations which were not worked out in the paper. While doing so, one intermediate result in the paper, namely\n\\begin{align*}\nS&=-2\\zeta(2)\n+\\frac{1}{2}\\sum_{k\\geq1}\\frac{H_kH_k^{(2)}}{k^2}\n+\\frac{1}{2}\\sum_{k\\geq1}\\frac{H_k^3}{k^2}\n-\\frac{1}{2}\\sum_{k\\geq1}\\frac{H_k^2}{k^3}\\\\\n&\\qquad+\\left(\\zeta(2)-1\\right)\\sum_{k\\geq1}\\frac{H_k}{k^2}\n-2\\sum_{k\\geq1}\\frac{1}{k^2}\\tag{1}\n\\end{align*}\nwas somewhat peculiar to me, because the rightmost addend is $-2\\zeta(2)$ and it was not clear, why it wasn't simply added to the first addend in the sum, giving $-4\\zeta(2)$. My detailed calculations were conform with $(1)$. All further arguments and calculations in the paper seemed to be correct.\n\nBut, at the end the resulting value was\n$$S=-4\\zeta(2)-2\\zeta(3)+4\\zeta(2)\\zeta(3)+2\\zeta(5)$$\nThen I checked the references from the paper and I found in Carsten and Schneider the identity correctly stated with $-4\\zeta(2)$. So, I came to the conclusion that there is simply a typo in the paper from Panholzer and Prodinger.\n\n\nI'd like to summarize the major steps of the proof and add parts of my detailed calculations, to make the arguments above better understandable.\n\nTask: Proof the following identity\n\\begin{align*}\nS&=\\sum_{j,k=1}^{\\infty}\\frac{H_j(H_{k+1}-1)}{jk(k+1)(j+k)}=-4\\zeta(2)-2\\zeta(3)+4\\zeta(2)\\zeta(3)+2\\zeta(5)\n\\end{align*}\n\nThe proof in the paper from Panholzer and Prodinger is done in three steps:\n\nStep 1: Split the sum, apply partial fraction decomposition and rearrange it to get\n\\begin{align*}\nS&=\\sum_{j,k=1}^{\\infty}\\frac{H_j(H_{k+1}-1)}{jk(k+1)(j+k)}\\\\\n&=\\sum_{k\\geq1}\\frac{H_{k+1}-1}{k(k+1)}\\sum_{j\\geq1}\\frac{H_j}{j(j+k)}\\\\\n&=\\dots\\\\\n&=\\sum_{k\\geq1}\\frac{H_{k+1}-1}{k^2(k+1)}\\left(\\zeta(2)+\\frac{H_k^2+H_k^{(2)}}{2}-\\frac{H_k}{k}\\right)\\\\\n\\end{align*}\nStep 2: Apply partial fraction decomposition, index shifting, telescoping and rearrange the sum by consequently replacing all occurrences of $k+1$ with $k$. This results in\n\\begin{align*}\nS&=\\sum_{k\\geq1}\\frac{H_{k+1}-1}{k^2(k+1)}\\left(\\zeta(2)+\\frac{H_k^2+H_k^{(2)}}{2}-\\frac{H_k}{k}\\right)\\\\\n&=\\dots\\\\\n&=-4\\zeta(2)\n+\\frac{1}{2}\\sum_{k\\geq1}\\frac{H_kH_k^{(2)}}{k^2}\n+\\frac{1}{2}\\sum_{k\\geq1}\\frac{H_k^3}{k^2}\n-\\frac{1}{2}\\sum_{k\\geq1}\\frac{H_k^2}{k^3}\n+\\left(\\zeta(2)-1\\right)\\sum_{k\\geq1}\\frac{H_k}{k^2}\\tag{2}\n\\end{align*}\n\nNote: In the paper in formula $(2)$ is the addend $-4\\zeta(2)$ written as $-2\\zeta(2)-2\\sum_{k\\geq1}\\frac{1}{k^2}$\n\nStep 3: By referring to papers from Borwein and Flajolet the following relations hold:\n\\begin{align*}\n\\sum_{k\\geq1}\\frac{H_kH_k^{(2)}}{k^2}&=\\zeta(5)+\\zeta(2)\\zeta(3)\\\\\n\\sum_{k\\geq1}\\frac{H_k^3}{k^2}&=10\\zeta(5)+\\zeta(2)\\zeta(3)\\\\\n\\sum_{k\\geq1}\\frac{H_k^2}{k^3}&=\\frac{7}{2}\\zeta(5)-\\zeta(2)\\zeta(3)\\\\\n\\sum_{k\\geq1}\\frac{H_k}{k^2}&=2\\zeta(3)\n\\end{align*}\nApplying these relations to $(2)$ results finally in\n\\begin{align*}\nS&=\\sum_{j,k=1}^{\\infty}\\frac{H_j(H_{k+1}-1)}{jk(k+1)(j+k)}=-4\\zeta(2)-2\\zeta(3)+4\\zeta(2)\\zeta(3)+2\\zeta(5)\n\\end{align*}\n\n\nAnd now some gory details of my calculations which I did to verify Step 2.\nWe consider\n\\begin{align*}\nS&=\\sum_{k\\geq1}\\frac{H_{k+1}-1}{k^2(k+1)}\\left(\\zeta(2)+\\frac{H_k^2+H_k^{(2)}}{2}-\\frac{H_k}{k}\\right)\\\\\n\\end{align*}\nUsing partial fraction decomposition the first factor of $S$ can be written as\n\\begin{align*}\n\\frac{H_{k+1}-1}{k^2(k+1)}&=\\left(-\\frac{1}{k}+\\frac{1}{k^2}+\\frac{1}{k+1}\\right)H_k\\\\\n&\\qquad+\\left(-\\frac{1}{k}+\\frac{1}{k+1}+\\frac{1}{(k+1)^2}\\right)\n\\end{align*}\nTherefore we have to evaluate:\n\\begin{align*}\nS&=\\sum_{k\\geq1}\\left(\\left(-\\frac{1}{k}+\\frac{1}{k^2}+\\frac{1}{k+1}\\right)H_k+\\left(-\\frac{1}{k}+\\frac{1}{k+1}+\\frac{1}{(k+1)^2}\\right)\\right)\\\\\n&\\qquad\\qquad\\cdot\\left(\\zeta(2)+\\frac{H_k^2+H_k^{(2)}}{2}-\\frac{H_k}{k}\\right)\n\\end{align*}\nWe calculate it by dividing it into smaller parts. Let\n\\begin{align*}\nS_1&=\\sum_{k\\geq1}\\left(-\\frac{1}{k}+\\frac{1}{k^2}+\\frac{1}{k+1}\\right)H_k=\\sum_{k\\geq1}\\frac{1}{k^2}H_k-\\zeta(2)\\\\\nS_2&=\\sum_{k\\geq1}\\left(-\\frac{1}{k}+\\frac{1}{k+1}+\\frac{1}{(k+1)^2}\\right)=\\zeta(2)-2\\\\\nS_3&=\\sum_{k\\geq1}\\left(-\\frac{1}{k}+\\frac{1}{k^2}+\\frac{1}{k+1}\\right)H_k^3\\\\\n&=\\sum_{k\\geq1}\\frac{1}{k^2}H_k^3-3\\sum_{k\\geq1}\\frac{1}{k^2}H_k^2+3\\sum_{k\\geq1}\\frac{1}{k^3}H_k-\\zeta(4)\\\\\nS_4&=\\sum_{k\\geq1}\\left(-\\frac{1}{k}+\\frac{1}{k+1}+\\frac{1}{(k+1)^2}\\right)H_k^2\\\\\n&=\\sum_{k\\geq1}\\frac{1}{k^2}H_k^2-2\\sum_{k\\geq1}\\left(\\frac{1}{k^2}+\\frac{1}{k^3}\\right)H_k+\\zeta(3)+\\zeta(4)\n\\end{align*}\n\\begin{align*}\nS_5&=\\sum_{k\\geq1}\\left(-\\frac{1}{k}+\\frac{1}{k^2}+\\frac{1}{k+1}\\right)H_kH_k^{(2)}\\\\\n&=\\sum_{k\\geq1}\\frac{1}{k^2}H_kH_k^{(2)}-\\sum_{k\\geq1}\\frac{1}{k^3}H_k-\\sum_{k\\geq1}\\frac{1}{k^2}H_k^{(2)}+\\zeta(4)\\\\\nS_6&=\\sum_{k\\geq1}\\left(-\\frac{1}{k}+\\frac{1}{k+1}+\\frac{1}{(k+1)^2}\\right)H_k^{(2)}\\\\\n&=\\sum_{k\\geq1}\\frac{1}{k^2}H_k^{(2)}-\\zeta(3)-\\zeta(4)\\\\\nS_7&=\\sum_{k\\geq1}\\left(-\\frac{1}{k}+\\frac{1}{k^2}+\\frac{1}{k+1}\\right)\\frac{H_k^2}{k}\\\\\n&=\\sum_{k\\geq1}\\left(-\\frac{1}{k^2}+\\frac{1}{k^3}\\right)H_k^2\n+2\\sum_{k\\geq1}\\frac{1}{k^2}H_{k}-\\zeta(3)\\\\\nS_8&=\\sum_{k\\geq1}\\left(-\\frac{1}{k}+\\frac{1}{k+1}+\\frac{1}{(k+1)^2}\\right)\\frac{H_k}{k}\\\\\n&=-2\\sum_{k\\geq1}\\frac{1}{k^2}H_k+2\\zeta(2)+\\zeta(3)\n\\end{align*}\n\nThis results in:\n\\begin{align*}\nS&=\\sum_{k\\geq1}\\frac{H_{k+1}-1}{k^2(k+1)}\\left(\\zeta(2)+\\frac{H_k^2+H_k^{(2)}}{2}-\\frac{H_k}{k}\\right)\\\\\n&=\\left(\\left(-\\frac{1}{k}+\\frac{1}{k^2}+\\frac{1}{k+1}\\right)H_k+\\left(-\\frac{1}{k}+\\frac{1}{k+1}+\\frac{1}{(k+1)^2}\\right)\\right)\\\\\n&\\qquad\\cdot\\left(\\zeta(2)+\\frac{H_k^2+H_k^{(2)}}{2}-\\frac{H_k}{k}\\right)\\\\\n&=\\zeta(2)(S_1+S_2)+\\frac{1}{2}(S_3+S_4+S_5+S_6)-S_7-S_8\\\\\n&=-4\\zeta(2)+\\frac{1}{2}\\sum_{k\\geq1}\\frac{1}{k^2}H_kH_k^{(2)}\n+\\frac{1}{2}\\sum_{k\\geq1}\\frac{1}{k^2}H_k^3\n-\\frac12\\sum_{k\\geq1}\\frac{1}{k^3}H_k^2\n+\\left(\\zeta(2)-1\\right)\\sum_{k\\geq1}\\frac{1}{k^2}H_k\n\\end{align*}\nwhich was my verification for Step 2.", "meta": {"post_id": 761414, "input_score": 24, "output_score": 48, "post_title": "$-4\\zeta(2)-2\\zeta(3)+4\\zeta(2)\\zeta(3)+2\\zeta(5)=S$"}}
{"input": "The question is: Prove that if $\\lambda$ is an eigenvalue of a matrix A with corresponding eigenvector x, then $\\lambda^2$ is an eigenvalue of $A^2$ with corresponding eigenvector x.\nI assume I need to start with the equation $Ax=\\lambda x$ and end up with $A^2 x=\\lambda^2 x$ but between those I am kind of lost. I have manipulated the equations several different ways and just can't seem to end up where I need to be. Help would be greatly appreciated as I believe this will be on a test tomorrow.", "output": "We know $Ax = \\lambda x$.  Then $A \\lambda x = \\lambda(Ax) = \\lambda^2x = A^2x$.  Putting this into a more readable mathematical sentence, we get:\n$$A^2x = A(Ax) = A\\lambda x = \\lambda(Ax) = \\lambda^2x$$\nYou were done and didn't realize it.  :)", "meta": {"post_id": 767835, "input_score": 19, "output_score": 35, "post_title": "Proving Eigenvalue squared is Eigenvalue of $A^2$"}}
{"input": "Does anyone know if this number is algebraic or transcendental, and why?\n$$\\sum\\limits_{n = 1}^\\infty  {10}^{ - n(n + 1)/2}  = 0.1010010001000010000010000001 \\ldots $$", "output": "The number $0.1010010001000010000010000001\\ldots$ is transcendental.\nConsider following three Jacobi theta series defined by\n$$\\begin{align}\n\\theta_2(q) \n&= 2q^{1/4}\\sum_{n\\ge 0} q^{n(n+1)} = 2q^{1/4}\\prod_{n=1}^\\infty (1-q^{4n})(1 + q^{2n})\\\\\n\\theta_3(q)\n&= \\sum_{n\\in\\mathbb{Z}} q^{n^2} = \\prod_{n=1}^\\infty (1-q^{2n})(1+ q^{2n-1})^2\\\\\n\\theta_4(q)\n&= \\theta_3(-q) =\n\\sum_{n\\in\\mathbb{Z}} (-1)^n q^{n^2} = \\prod_{n=1}^\\infty (1-q^{2n})(1- q^{2n-1})^2\\\\\n\\end{align}\n$$\nand for any $m \\in \\mathbb{Z}_{+}$, $k \\in \\{ 2, 3, 4 \\}$, use \n$\\displaystyle D^m\\theta_k(q)$ as a shorthand for \n$\\displaystyle \\left( q\\frac{d}{dq} \\right)^m \\theta_k(q)$.\nBased on Corollary 52 of a survey article Elliptic functions and Transcendence by M. Waldschmidt in 2006,\n\nLet $i, j$ and $k \\in \\{ 2,3,4 \\}$ with $i \\ne j$. Let $q \\in \\mathbb{C}$\n  satisfy $0 < |q| < 1$. Then each of the two fields\n  $$\n\\mathbb{Q}( q, \\theta_i(q), \\theta_j(q), D\\theta_k(q))\n\\quad\\text{ and }\\quad\n\\mathbb{Q}( q, \\theta_k(q), D\\theta_k(q), D^2\\theta_k(q))\n$$\n  has transcendence degree $\\ge 3$ over $\\mathbb{Q}$\n\nWe know for any non-zero algebraic $q$ with $|q| < 1$, the three $\\theta_k(q)$, in particular $\\theta_2(q)$ is transcendental. Since \n$$\\sum_{n=1}^\\infty 10^{-n(n+1)/2} = \\frac{\\sqrt[8]{10}}{2} \\theta_2\\left(\\frac{1}{\\sqrt{10}}\\right) - 1$$\nand using the fact $\\frac{1}{\\sqrt{10}}$ and $\\frac{\\sqrt[8]{10}}{2}$ are both algebraic, we find the number at hand is transcendental.", "meta": {"post_id": 778218, "input_score": 50, "output_score": 74, "post_title": "Is $0.1010010001000010000010000001 \\ldots$ transcendental?"}}
{"input": "If a function's integral can't be written, then how can we find exact values for it over areas? Can we only ever estimate it? Why can't we make new functions to define these strange unwritable anti-derivatives?", "output": "We can, and do all the time! For example, \nThe Gamma function\n$$\n\\Gamma (z) = \\int_0^{\\infty} t^{z-1}\\mathrm{e}^{-t} \\ \\mathrm{d}t.\n$$\nThe Beta function \n$$\n\\mathrm{B}(z,y) = \\int_0^1 t^{z-1}(1-t)^{y-1}\\,\\mathrm{d}t.\n$$\nThe Exponential integral function\n$$\n\\mathrm{E}_1(z) = \\int_z^\\infty \\frac{e^{-t}}{t}\\,  \\mathrm{d}t.\n$$\nThe Error function\n$$\n\\operatorname{erf}(x) = \\frac{2}{\\sqrt{\\pi}}\\int_{0}^x e^{-t^2}\\,\\mathrm dt.$$\nThe Elliptic integral of the second kind\n$$\n E(\\phi,k)=\\int_0^{\\phi} \\sqrt{1-k^2\\sin^2\\theta} \\ \\mathrm{d}\\theta. \n$$\nThe Logarithmic integral function\n$$\n {\\rm Li} (x) = \\int_2^x \\frac{\\mathrm{d}t}{\\ln t},\n$$\nand many, many more very important \"special functions' are defined by definite integrals. If you go back even to elementary functions, you can define the logarithm via the following integral - \nThe Logarithm\n$$\n\\ln (t) = \\int_1^t \\frac{1}{x} \\, dx.\n$$\nAs to your first question, how do we find areas under these curves if we don't have an elementary antiderivative? Well, how do you find the area under the curve $1/t$ from $1$ to $5$? The above integral tells you the value is $\\ln 5$, but what is that value, exactly? We can only approximate it, given the best methods we have! \nThe same is true of all the above functions. At some special values they have exact values, given perhaps by integers, rational or irrational numbers, or a combination of common mathematical constants such as $\\pi,\\mathrm{e},\\gamma,$ Catalan's constant, etc. (another interesting question is - why are these constants special enough to have names? Because they come up all the time! The same is true for the above functions) .\nBut for almost all values we must approximate the value of the function by computing the definite integral numerically (Trapezoid rule, Simpson's rule, more advanced techniques), or using some other representation of the integral such as an infinite sum etc.", "meta": {"post_id": 780170, "input_score": 11, "output_score": 34, "post_title": "How can a function have an antiderivative that can't be written?"}}
{"input": "I have a very limited understanding of groups and symmetry gained mostly from online videos (for eg. this one), so forgive me if this sounds ignorant.\nParticular parts of Penrose tilings exhibit local 5-fold rotational symmetry (Penrose stars, Penrose suns, etc.), but since the tiling as a whole is non-periodic, how do you describe the overall symmetry of a Penrose tiling?  The symmetries of Moorish tilings in El Alhambra in Granada, Spain can be clearly defined, drawn up in a Cayley table etc.  How do group theorists classify the symmetries of Penrose tilings (I don't think they fit into a Cayley table)?\nIn general I am curious as to what the significance of Penrose tiles is.  They're beautiful and intriguing at face value, high structure and order mixed with chaos and so on, but what is the real mathematical significance?", "output": "So, you can literally write a book about this stuff (exhibit a and b) and so I will try my best to keep it short and sweet (as much as possible at least). I'll also not mention anything about the physical interpretation of aperiodic patterns and tilings which mathematical and solid state physicists are interested in - namely quasicrystals.\nYou mentioned that periodic tilings can be characterised by a group action of the space that they tile under which the tiling is invariant - let's stick to euclidean $2$-space, $\\mathbb{R}^2$ for this post. You can do this in a few ways, you could either look at just the subgroup of the translation group on $\\mathbb{R}^2$ under which the tiling $\\mathcal{T}$ is invariant, in which case a cocompact periodic tiling has invariant subgroup isomorphic to $\\mathbb{Z}^2$, or you can consider the subgroup of the full euclidean group on $\\mathbb{R}^2$ under which $\\mathcal{T}$ is invariant, including rotations - this is more subtle and will depend on the specific tiling but we understand this case very well too.\nThe Tiling Space of $\\mathcal{T}$\nNow, let's also consider the dual topological theory to this setup. Whenever we have a group $G$ acting on a manifold $M$, we can consider the quotient space $M/G$ and so in the case of the translation invariant subgroup for a periodic tiling, we have the space $\\mathbb{R}^2/\\mathbb{Z}^2$ which is a $2$-torus $T=S^1\\times S^1$ (in general, for $n$-dimensional periodic tilings we get the $n$-torus $(S^1)^n$). We'll call this space the tiling space of our tiling $\\mathcal{T}$ and denote it by $\\Omega_{\\mathcal{T}}$ for an arbitrary tiling.\nIf we want to consider rotations as well, we need to be slightly more clever so that we don't lose too much information and instead consider the orbifold (stacky) quotient of the symmetry group. I won't spend any more time on the rotational version as it can get messy fairly quickly and would require its own new question in my opinion.\nNow, how can we view the torus $T$ as an object which gives us information about our tiling? Well it's quite simple really; a point in the torus corresponds to a placement of the origin of $\\mathbb{R}^2$ somewhere into our tiling. That is, if we choose some periodic tiling $\\mathcal{T}$, including a prescribed origin, then if we move along one of the group elements in $G$ we should end up back to where we started and so locally, no matter how far away from the origin you look, everything about the tiling looks the same. We've effectively placed a metric on the set of all tilings of the plane by those tiles in $\\mathcal{T}$ which says \"if a translation by a vector $x$ can get me from the tiling $\\mathcal{T}$ to the tiling $\\mathcal{T}'$ then we say $d(\\mathcal{T},\\mathcal{T}')=|x|$, unless of course there is a vector of shorter length. So really we should say $$d(\\mathcal{T},\\mathcal{T}')=\\inf\\{|x|:\\mathcal{T}+x=\\mathcal{T}\\}$$ and in fact this gives a well defined metric on the set of all tilings of the plane which contain only those tiles appearing in $\\mathcal{T}$ and moreover this space is, by construction, homeomorphic to the tiling space $\\Omega_\\mathcal{T}$. This can therefore be taken as our definition of the tiling space.\nWhat if $\\mathcal{T}$ is aperiodic?\nWell, this is the question you originally asked and it is a good question - it's the reason we spent so long defining the tiling space in terms of effectively local information of the tiling - remember for a periodic tiling the origin only needs to know whereabouts in a tile it is in order to determine what point that tiling represents in the tiling space. We're now set up to handle aperiodic tilings with only a small modification of the above metric.\nFirst, let's suppose that $\\mathcal{T}$ is a tiling of the plane which is aperiodic, so it has no translations that fix it. We can say this in notation as $\\forall x\\in\\mathbb{R}^2\\setminus\\{0\\},\\:\\mathcal{T}+x\\neq\\mathcal{T}$. Let's consider the following set of tilings. We'll say that $\\mathcal{T}'$ is locally isomorphic to $\\mathcal{T}$ if for all $R>0$, the set of tiles within $R$ distance of the origin in $\\mathcal{T}'$, which we will denote by $B_R(\\mathcal{T}')$, can be found somewhere in the tiling $\\mathcal{T}$. In notation $\\mathcal{T}'$ is locally isomorphic to $\\mathcal{T}$ if $\\forall R>0, \\:\\exists x\\in\\mathbb{R}, \\:B_R(\\mathcal{T}')=B_R(\\mathcal{T}+x)$. We'll call the set of all tilings locally isomorphic to $\\mathcal{T}$ the tiling space of $\\mathcal{T}$ and write it as $\\Omega_\\mathcal{T}$. Notice I haven't told you the topology of the space yet.\nNow, from the motivating material before, we want to put some metric on $\\Omega_{\\mathcal{T}}$ which in some way satisfies our notion of two tilings being close if they look very similar to each other around the origin out to a large radius. I'll put the metric here but note that it's not pretty - there are equivalent ways to define this metric and you can even forgo defining the metric altogether and just define the topology but none of them are particularly pretty - instead, just try to keep the intuition that two tilings are close if, after a little wiggle, they overlap each other around the origin on some very large patch of tiles. For tilings $\\mathcal{T}',\\mathcal{T}''\\in\\Omega_{\\mathcal{T}}$ we define the tiling metric $d\\colon\\Omega_\\mathcal{T}\\times\\Omega_\\mathcal{T}\\to\\mathbb{R}$ by\n$$d(\\mathcal{T}',\\mathcal{T}'')=\\inf\\left(\\{\\sqrt{2}\\}\\cup\\{\\epsilon\\mid\\:\\exists u,v\\: B_{1/\\epsilon}(\\mathcal{T}'+u)=B_{1/\\epsilon}(\\mathcal{T}''+v)\\mbox{ and }|u|,|v|<\\epsilon\\}\\right)$$ where $B_R(\\mathcal{T})$ is defined as before to be the set of tiles within a ball of radius $R$ about the origin. It's a nice exercise to show that $d$ as above really is a metric and a slightly easier exercise to then show that if $\\mathcal{T}$ is periodic, $d$ is equivalent to the previous metric defined in the periodic case. In this sense, the above metric is at least a proper generalisation of the periodic case.\nMost of the hard work is done now. We have our tiling space $\\Omega_\\mathcal{T}$ and it seems to represent our intuition of what it means for two tilings to be similar to a 'high degree of accuracy' - there are hundreds of papers by the way on trying to understand the topology of these spaces - they are extremely complicated spaces. For aperiodic tilings which are locally finite (so there are only finitely many patches of tiles of a given radius appearing in $\\mathcal{T}$) they are connected but not path connected spaces. They are also compact.\nThey are a bit like a solenoid as shown below, except they are not homogeneous spaces. Given a regularity condition known as repetitivity which I won't write here (a Penrose tiling is both locally finite and reptitive) a tiling whose tiles are polygons meeting edge to edge has a tiling space which fibers over the $2$-torus $T$ with fiber homeomorphic to a Cantor set. You can think of moving around the torus as translating the tiling by small amounts, and then hopping a small discrete distance from one point in the Cantor fiber to another corresponds to making a different choice of placing a tile far away from the origin, but everything else closer in than that tile is the same.\n\nWhat about the algebra?\nI will leave this section fairly vague as this post is already getting very long. As the astute reader may have guessed, our usual bag of tricks for studying these spaces won't really work - we have an uncountable collection of disjoint path connected components, each component of which is weakly contractible, so singular (co)homology and homotopy groups are all pretty useless in this setting. What we need is an invariant which can somehow see the connected components as opposed to the path connected components - for reasons I won't go in to, it turns out that the right tool is \u010cech cohomology.\nFor CW complexes the \u010cech cohomology is isomorphic to the singular cohomology so as a bonus, for periodic tilings, we can recover our original group $\\mathbb{Z}^2$ as the first \u010cech cohomology group! For aperiodic tilings the \u010cech cohomology is a pretty good invariant, it gives a fair amount of information about the tiling space, which can in turn be translated into information about the tiling. We're still trying to find a full understanding of what information is really embedded in the \u010cech cohomology. Probably the biggest bonus of \u010cech cohomology is that we can actually calculate it for a large class of spaces including the Penrose tiling using various sophisticated techniques. The \u010cech cohomology of the Penrose tiling space $\\Omega_P$ is given by\n$$\\check{H}^0(\\Omega_P) \\cong \\mathbb{Z}\\\\ \\check{H}^1(\\Omega_P) \\cong \\mathbb{Z}^5\\\\ \\check{H}^2(\\Omega_P) \\cong \\mathbb{Z}^8.$$\nThere are a few other invariants we can associate to tiling spaces including its $K$-theory, the tiling groupoid, the $\\lim^1$ invariant, various notions of dynamical equivalence and shape equivalence. For the reader who wishes to read more of what has been outlined here, I recommend the book by Lorenzo Sadun$^{[1]}$ linked above, and the seminal article of Anderson and Putnam (1998).$^{[2]}$\n\n$[1]$ Lorenzo Sadun (2008) Topology of Tiling Spaces. American Mathematical Society, ISBN-13: 978-0-08218-4727-5. ISBN-10: 0-8218-4727-9\n$[2]$  Jared E. Anderson and Ian F. Putnam (1998). Topological invariants for substitution tilings and their associated $C^\\ast$-algebras. Ergodic Theory and Dynamical Systems, 18, pp 509-537.", "meta": {"post_id": 783118, "input_score": 21, "output_score": 44, "post_title": "What is the mathematical significance of Penrose tiles?"}}
{"input": "How do you prove that:\n$$\n\\begin{pmatrix}\n1 & 1\\\\\n1 & 0\n\\end{pmatrix}^n\n= \n\\begin{pmatrix}\nF_{n+1} & F_n\\\\\nF_{n} & F_{n-1}\n\\end{pmatrix}$$", "output": "Let\n$$A=\\begin{pmatrix}\n1 & 1 \\\\ 1 & 0 \n\\end{pmatrix}$$\nAnd the Fibonacci numbers, defined by\n$$\\begin{eqnarray}\nF_0&=&0\\\\\nF_1&=&1\\\\\nF_{n+1}&=&F_n+F_{n-1}\n\\end{eqnarray}$$\nThen, by induction,\n$$A^1=\\begin{pmatrix}\n1 & 1 \\\\ 1 & 0\n\\end{pmatrix} =\n\\begin{pmatrix}\nF_2 & F_1 \\\\ F_1 & F_0\n\\end{pmatrix}$$\nAnd if for $n$ the formula is true, then\n$$A^{n+1}=A\\,A^n=\\begin{pmatrix}\n1 & 1 \\\\ 1 & 0\n\\end{pmatrix}\\begin{pmatrix}\nF_{n+1} & F_{n} \\\\ F_{n} & F_{n-1}\n\\end{pmatrix}=\\begin{pmatrix}\nF_{n+1}+F_n & F_{n}+F_{n-1} \\\\  F_{n+1} & F_{n}\n\\end{pmatrix}=\\begin{pmatrix}\nF_{n+2} & F_{n+1} \\\\ F_{n+1} & F_{n}\n\\end{pmatrix}$$\nSo, the induction step is true, and by induction, the formula is true for all $n>0$.", "meta": {"post_id": 784710, "input_score": 22, "output_score": 38, "post_title": "How to prove Fibonacci sequence with matrices?"}}
{"input": "Are there infinitely many Fibonacci numbers that are also powers of 2? If not, which is the largest?", "output": "Fibonacci numbers have just about the greatest divisibility rule you could expect. Fibonacci numbers share common divisors exactly when their corresponding indices share common divisors, $\\gcd(F[m],F[n])$ = $F_{\\gcd(m,n)}$. \nThis result means that the Fibonacci index of any power of $2$ greater than $8$ must be divisible by $6$ as $F_6 = 8$ and this means that the index of power of $2$ Fibonacci number greater that $8$ must be a power of 6 and therefore must be divisible by $F_{36}$. \nHowever $F_{36}$ is also divisible by $F_{9}$ since $9$ divides $36$ and given that $F_9 = 34, F_{36}$ is therefore divisible by $34$ and cannot be a power of $2$. \nSince any candidate powers of $2$ greater than $8$ must be divisible by $34$ there can be no Fibonacci numbers greater than $8$ which are powers of $2$.", "meta": {"post_id": 795763, "input_score": 28, "output_score": 38, "post_title": "Fibonacci numbers that are powers of 2"}}
{"input": "I'm studying for a probability exam and came across this question. I watched the video solution to it but I don't really understand it. I was hoping someone could explain this problem to me. Are there different ways to go about this?", "output": "Hint:\nThe probability that an equal number of tails and heads appear is $\\large{{2k \\choose k} \\frac{1}{2^{2k}}}$\nThe two remaining outcomes (that there are more heads than tails or more tails than heads) are equally likely.", "meta": {"post_id": 803628, "input_score": 6, "output_score": 37, "post_title": "A fair coin is flipped 2k times. What is the probability that it comes up tails more often than it comes up heads?"}}
{"input": "I originally started off by listing all the primes: $p<200$ then trying to calculate the prime factorisation of each (which I realise is a silly thing to do)\nI believe there must be a simpler way to find the smallest and largest prime factors of $\\dfrac{200!}{180!}$.\nIf I list the prime factorisation of $180$ and $200$ does that help me in any way?\nI have calculated a similar thing before but with similar numbers and I'm not really sure how to deal with these larger numbers?\nThank you for any help", "output": "Note $\\dfrac{200!}{180!}=200(199)\\cdots(181)$. The smallest prime factor is easy to calculate - it's just $2$, since $2$ is the smallest prime and the product contains even factors. The largest prime factor is a little more interesting. You have to find the largest prime factor out of any of the elements in the product. However, noting that $199$ is prime, $200$ has no prime factors greater than $199$, and $199$ is greater than all other elements in the product - never mind their prime factors - gives us the result that the largest prime factor is $199$.", "meta": {"post_id": 804499, "input_score": 15, "output_score": 41, "post_title": "Finding smallest and largest prime factor of $\\frac{200!}{180!}$"}}
{"input": "With axiom of choice it is possible to construct an inner product on $C(\\mathbb R)$.\nMy question is, is it possible to explicitly construct an inner product on $C(\\mathbb R)$? I.e. to give a closed formula to calculate the inner product?\nI know it is straight-forward to write down a scalar product using a Hamel basis. This is not the answer I am looking for.\nThis question came to me, when a student asked me in the lecture today 'whether there are vector spaces without inner products'. So I tried to find scalar produces for function spaces. I think I managed to write one down for $L^1((0,1))$. But I failed to construct one for $C(\\mathbb R)$.", "output": "(Update. Added a self-contained proof that under $\\mathsf{ZF}+\\mathsf{DC}+\\mathsf{BP}$ there is no norm.)\nMart\u00edn-Blas P\u00e9rez Pinilla is on the right track.  You can't even put a norm on $C(\\mathbb{R})$ without using the axiom of choice in an essential way. Dependent choice is not enough.\n\nClaim. It is consistent with $\\mathsf{ZF}+\\mathsf{DC}$ that there does not exist any norm on $C(\\mathbb{R})$.\n\nRecall that a subset $E$ of a topological space is said to have the Baire property if it can be written as a symmetric difference $E = U \\triangle M$ where $U$ is open and $M$ is meager (a countable union of nowhere dense sets).\nA celebrated theorem of Shelah says that consistent with $\\mathsf{ZF}+\\mathsf{DC}$ is the statement $\\mathsf{BP}$: \"Every subset of $\\mathbb{R}$ has the Baire property.\" From $\\mathsf{BP}$ it follows that in fact every subset of any Polish space has the Baire property.\nLet $\\tau$ be the usual topology on $C(\\mathbb{R})$ (uniform convergence on compact sets).  It is induced by a translation-invariant metric:\n$$d(f,g) := \\sum_{n=1}^\\infty 2^{-n} \\min\\left(1, \\sup_{[-n,n]} |f-g|\\right) $$\nThe metric $d$ is complete (this comes from the fact that a uniform limit of continuous functions is continuous).  And it's not hard to see that $\\tau$ is separable (the polynomials with rational coefficients are $\\tau$-dense, by the Weierstrass approximation theorem).  So $(C(\\mathbb{R}),\\tau)$ is a Polish space.\nSuppose now that $\\|\\cdot\\|$ is a norm on $C(\\mathbb{R})$.  Let $B$ be the closed unit $\\|\\cdot\\|$-ball.  We will show that $B$ does not have the Baire property with respect to $\\tau$.  Specifically, let $U$ be any $\\tau$-open set; we will show that $B \\triangle U$ is $\\tau$-nonmeager.\nSuppose first that $U = \\emptyset$ so that $B \\triangle U = B$.  Since $\\bigcup_{n=1}^\\infty nB = C(\\mathbb{R})$, by the Baire category theorem $B$ is $\\tau$-nonmeager.\nNow suppose that $U$ is nonempty.  We will show $U \\setminus B$ is $\\tau$-nonmeager.  Let us begin by showing $U \\setminus B$ is nonempty.  Let $f \\in U$ and let $g \\in C(\\mathbb{R})$ be your favorite nonzero continuous function which is supported in $[0,1]$.  Let $g_n(x) = g(x-n)$ be translates of $g$.  For each $n$, let $a_n$ be a real number sufficiently large that $\\|f + a_n g_n\\| > 1$, so that $f + a_n g_n \\notin B$.  Then $a_n g_n \\to 0$ uniformly on compact sets (i.e. in the $\\tau$ topology), so for some $N$ we have $h := f + a_N g_N \\in U$.  Thus $h \\in U \\setminus B$.  (To say this another way, every nonempty $\\tau$-open set is unbounded, but $B$ is bounded, so $U$ cannot be a subset of $B$.)\nNow for any $u \\in C(\\mathbb{R})$, we have $h + \\frac{1}{n} u \\to h$ in both the $\\tau$ and $\\|\\cdot\\|$ topologies.  So for sufficiently large $n$, we have $h + \\frac{1}{n} u \\in U$ and $h + \\frac{1}{n} u \\in B^c$ (since $B$ is $\\|\\cdot\\|$-closed).  That is, $u \\in n((U \\setminus B)-h)$.  Since $u$ was arbitrary we have shown $\\bigcup_{n=1}^\\infty n((U \\setminus B)-h) = C(\\mathbb{R})$.  By the Baire category theorem, $U \\setminus B$ is $\\tau$-nonmeager, hence so is $B \\triangle U$.\nSo we have shown that if $C(\\mathbb{R})$ has a norm, then it has a set that lacks the Baire property with respect to $\\tau$.  Under $\\mathsf{ZF}+\\mathsf{DC}+\\mathsf{BP}$ there is no such set and hence no norm.\n(Credit where credit is due: This proof is loosely based on the idea of the proof of the Garnir-Wright closed graph theorem from Theorem 27.45 of Eric Schechter's Handbook of Analysis and its Foundations.)\nIncidentally, the only property of the vector space $C(\\mathbb{R})$ we used is that it admits a Polish topology in which every nonempty open set is unbounded.  So the same argument would apply to other vector spaces with this property, such as $\\mathbb{R}^{\\mathbb{N}}$, $C^\\infty(\\mathbb{R}^d)$, etc.", "meta": {"post_id": 814754, "input_score": 25, "output_score": 35, "post_title": "Inner product on $C(\\mathbb R)$"}}
{"input": "I was just a little bit curious about the general statement of this theorem. Honestly, I am not at all interested in fully understanding this, so it is not that I am too lazy to read plenty of books about it, but I would like to know a little bit more, what this means. \nTherefore I would like to go with an example: \nLet $(Tf)(x):= \\frac{df}{dx}(x)+\\sin(x)f(x)$ be a differential operator on $[0,L]$ for some $L \\>>0$. \nApparently, the first question would be: Is this operator Fredholm? I do understand what \nit means for an operator to be Fredholm and I understand the definition of the Fredholm index, but I don't see whether this one actually is such an example of a Fredholm operator. In case that this is true. Where does topology come into play? I know the definition of an index for a path, but this topological index seems to be different. Maybe this example is not that good, as we are not studying something on any abstract manifolds, but still, could anybody elaborate on this?", "output": "Fredholm theory, originally developed for studying (systems of) differential equations, had been around for several decades when Gel'fand et al., working in the '50s, noticed that the Fredholm index was homotopy invariant, i.e., if $F_t$, $a \\leq t \\leq b$ is a continuous path of Fredholm operators, then $\\operatorname{Index}(F_t)$ is constant in $t$, and so began wondering if the Fredholm index of at least some Fredholm operators could be computed in terms of honest-to-goodness topological invariants. What Atiyah and Singer then showed was that if $D : E \\to F$ is an elliptic (pseudo)differential operator between smooth vector bundles $E$ and $F$ over a compact orientable manifold $X$, then\n$$\n \\operatorname{Index}(D) = \\operatorname{Index}_{\\text{top}}(D),\n$$\nwhere\n\n$\\operatorname{Index}(D)$ is the Fredholm index of $D$, a purely analytic datum,\n$\\operatorname{Index}_{\\text{top}}(D)$ is the topological index of $D$, the pairing of a certain cohomology class on $X$, obtained from $D$, with the fundamental homology class $[X]$ of $X$.\n\nLet me now give the two most basic examples of Atiyah--Singer in action:\n\n(Example of odd-dimensional Atiyah--Singer) Let $\\gamma : S^1 \\to \\mathbb{C} \\setminus \\{0\\}$ be a continuous closed path in the plane that doesn't pass through the origin. Recall that the Hardy space on the circle $S^1$ is the closed subspace\n$$\n H^2(S^1) = \\left\\{ f = \\sum_{n=-\\infty}^\\infty a_n e^{i n x} \\in L^2(S^1) \\text{ such that } \\forall n < 0, \\; a_n = 0 \\right\\}\n$$\nof $L^2(S^1)$, and let $P$ be the orthogonal projection from $L^2(S^1)$ onto $H^2(S^1)$. Let $M_\\gamma : L^2(S^1) \\to L^2(S^1)$ be the multiplication operator $(M_\\gamma f)(x) = \\gamma(x)f(x)$, and define the Toeplitz operator $T_\\gamma : H^2(S^1) \\to H^2(S^1)$ by $T_\\gamma = P \\circ M_\\gamma \\circ P$. Then Atiyah-Singer, in this special case, reduces to the Toeplitz index theorem, i.e.,\n$$\n \\operatorname{Index}(T_\\gamma) = -\\text{winding number of $\\gamma$},\n$$\nwhich is certainly a topological invariant of interest.\n(Example of even-dimensional Atiyah--Singer) Let $X$ be a Riemann surface, and view $d+d^\\ast$ as an operator $(X \\times \\mathbb{C}) \\oplus \\wedge^2 T^\\ast_{\\mathbb{C}} X \\to T^\\ast_{\\mathbb{C}} X$, i.e., as a map from even-degree forms to odd-degree forms. Then, on the one hand,\n$$\n \\operatorname{Index}(d+d^\\ast) = \\chi(X),\n$$\nwhere $\\chi(X)$ is the Euler characteristic of $X$, whilst on the other hand,\n$$\n \\operatorname{Index}_{\\text{top}}(d+d^\\ast) = \\frac{1}{2\\pi}\\int_X K dA,\n$$\nwhere $K$ is the Gaussian curvature of $X$. Thus, Atiyah--Singer for the elliptic differential operator $d+d^\\ast$ boils down to the Gauss--Bonnet theorem, i.e.,\n$$\n \\int_X K dA = 2\\pi\\chi(X).\n$$\n\nNow, I'm afraid your example of $T := \\tfrac{d}{dx} + \\sin(x)$ isn't really going to tell you all that much. On the one hand, the closed interval $[0,L]$ is a manifold with boundary, and on the other, you need to impose boundary conditions anyway to actually have a well-defined operator, so that you'd might as well take $L = 2k\\pi$ for some $k \\in \\mathbb{N}$ and impose periodic boundary conditions. Then $T$ will define an elliptic differential operator on $L^2(S^1)$ with the same principal symbol $\\sigma(T)$ as $D := \\tfrac{d}{dx}$, since\n$$\n \\sigma(D)(df) := i[D,f] = if^\\prime = i[T,f] =: \\sigma(T)(df)\n$$\nfor all $f \\in C^\\infty(S^1)$, and hence will have the same topological index.\nHowever, you can directly show that $\\ker(D) = \\mathbb{C}$ and that $\\ker(D^\\ast) = \\ker(-D) = \\mathbb{C}$, so that\n$$\n \\operatorname{Index}(T) = \\operatorname{Index}_{\\mathrm{top}}(T) = \\operatorname{Index}_{\\mathrm{top}}(D) = \\operatorname{Index}(D) = 0.\n$$\nAlternatively, since $\\sin(x)$ is smooth, the multiplication by $\\sin(x)$ defines a bounded operator on the Sobolev spaces $W^{s,2}(S^1)$ for each $s \\geq 0$, and hence $T_t := \\tfrac{d}{dx} + t\\sin(x)$ defines a continuous one-parameter family of elliptic first-order differential operators on $L^2(S^1)$, so that by heat-theoretic methods (cf. Roe, Elliptic operators, topology and asymptotic methods 2nd ed., pp. 144-145), the analytic index $\\operatorname{Index}(T_t)$ is constant in $t$, and hence\n$$\n \\operatorname{Index}(T) = \\operatorname{Index}(T_1) = \\operatorname{Index}(T_0) = \\operatorname{Index}(D).\n$$\nAs it turns out, it's a general fact that the index of an elliptic differential operator on a closed odd-dimensional manifold necessarily vanishes---observe that Toeplitz operators are pseudodifferential, not differential, operators.\nNow, what's the point of Atiyah--Singer? The yoga is similar to that of, say, Stokes's theorem, where one side of the equation is sometimes easier to deal with, and the other side sometimes easier instead.\n\nOn the one hand, Fredholm indices, which were originally devised for studying systems of differential equations, can be very difficult to compute, but the right-hand-side of Atiyah--Singer is sometimes computable instead.\nOn the other hand, you might want to know if a certain topological quantity, which is, a priori, only rational, is actually an integer; if you can realise it as the right-hand-side of Atiyah--Singer for some suitable (pseudo)differential operator, then you know that it is an integer, since the left-hand-side is necessarily an integer.", "meta": {"post_id": 815451, "input_score": 23, "output_score": 39, "post_title": "What is the Atiyah-Singer index theorem about?"}}
{"input": "I am trying to compute the limit of this sequence:\n$$\\lim\\limits_{n \\to \\infty} \\dfrac{(-1)^nn^2}{n!} \\sum\\limits_{k=2}^{n}\\binom{n}{k}(-1)^kk^{n-1}\\ln k$$\nI can compute without the $\\ln k$ in the expression. \n$\\displaystyle\\sum\\limits_{k=0}^{n}\\binom{n}{k}(-1)^kk^{n-1}$ is the $n$-th order finite difference of $x^{n-1}$ at $0$, thus it vanishes. Else one could come to the same conclusion with a combinatorial interpretation (with I.E. Principle)\nThe $\\ln k$ part is causing me trouble. Any ideas how to deal with it?", "output": "EDITED 2. Here is another approach: The starting point is the following identity\n$$ \\int_{0}^{\\infty} \\frac{(1 - e^{-x})^{n}}{x^{m}} \\, dx = \\frac{(-1)^{m}}{(m-1)!} \\sum_{k=1}^{n} \\binom{n}{k} (-1)^{k} k^{m-1} \\log k. \\tag{1} $$\nwhich I proved here. Plugging $m = n$ to $\\text{(1)}$, we have \n$$ A_{n} := \\frac{(-1)^{n}n^{2}}{n!} \\sum_{k=2}^{n} \\binom{n}{k}(-1)^{k} k^{n-1} \\log k = \\int_{0}^{\\infty} n \\left( \\frac{1 - e^{-x}}{x} \\right)^{n} \\, dx. $$\nNow let $f(x) : [0, \\infty) \\to (0, 1]$ be defined by $f(x) = (1 - e^{-x})/x$. This function is monotone decreasing, and let us denote its inverse by $g = f^{-1}$. Then it follows from the substitution $x = g(y)$ that\n$$ A_{n} = \\int_{0}^{1} n y^{n} \\left| g'(y) \\right| \\, dy. $$\nBy observing that $y \\mapsto ny^{n}$ behaves as an approximation to the identity of the unit mass $\\delta_{1}$, it follows that\n$$ \\lim_{n\\to\\infty} A_{n} = \\left| g'(1) \\right| = \\frac{1}{\\left| f'(0) \\right|} = 2. $$\n\nEDITED. I finally came out with a complete solution:\nTo avoid taking account of the cancellation between large quantities, we find a more tractable representation. Let\n$$ A_{n} = \\frac{(-1)^{n}n^{2}}{n!} \\sum_{k=2}^{n} \\binom{n}{k}(-1)^{k} k^{n-1} \\log k $$\nand introduce\n$$ f_{n}(x) = \\frac{n^{2} x^{n-2} \\log x}{(x-1)\\cdots(x-n)}. $$\nThen we have\n$$ \\operatorname{Res}\\limits_{z=k} \\, f_{n}(z) = \\frac{(-1)^{n-k} n^{2} k^{n-1} \\log k}{k!(n-k)!} $$\nand hence for any simple closed contour $C \\subset \\Bbb{C} \\setminus (-\\infty, 0]$ winding $\\{1, \\cdots ,n\\}$ in counter-clockwise direction it follows that\n$$ \\int_{C} f_{n}(z) \\, dz = 2\\pi i A_{n}. $$\nBut since $f_{n}(z) = O(|z|^{-2}\\log|z|)$ as $|z| \\to \\infty$, we can modify contour so that $C$ winds $(-\\infty, 0]$, and a simple calculation shows that\n$$ A_{n} = \\int_{0}^{\\infty} \\frac{n^{2} x^{n-2}}{(x+1)\\cdots(x+n)} \\, dx. \\tag{1} $$\nUsing the substitution $x \\mapsto n^{2}/x$, it follows from $\\text{(1)}$ that\n$$ A_{n} = \\int_{0}^{\\infty} \\prod_{k=1}^{n} \\left( 1 + \\frac{kx}{n^{2}} \\right)^{-1} \\, dx. \\tag{2} $$\nNow notice that when $n \\geq 2$, expanding the denominator and discarding higher order term, we have\n\\begin{align*}\n\\prod_{k=1}^{n} \\left( 1 + \\frac{kx}{n^{2}} \\right)\n&\\geq 1 + \\sum_{1\\leq k \\leq n} \\frac{kx}{n^{2}} + \\sum_{1 \\leq l < k \\leq n} \\frac{kl x^{2}}{n^{4}} \\\\\n&= 1 + \\frac{n+1}{2n} x + \\frac{(3n+2)(n^{2}-1)}{24n^{3}} x^{2} \\\\\n&\\geq 1 + \\frac{x}{2} + \\frac{x^{2}}{8}.\n\\end{align*}\nThis means that the integrands of $\\text{(2)}$ are bounded by an integrable function $1/(x^{2}/8 + x/2 + 1)$ and thus we can apply the dominated convergence theorem, once we prove that the integrand converges pointwise. But it is immediate that for any fixed $x$,\n$$ \\log \\prod_{k=1}^{n} \\left( 1 + \\frac{kx}{n^{2}} \\right)\n= \\sum_{k=1}^{n} \\log \\left( 1 + \\frac{kx}{n^{2}} \\right)\n= \\frac{x}{2} + \\mathcal{O}\\left(\\frac{1}{n}\\right)\n\\longrightarrow \\frac{x}{2} $$\nand therefore\n$$ \\lim_{n\\to\\infty} A_{n} = \\int_{0}^{\\infty} e^{-x/2} \\, dx = 2. $$", "meta": {"post_id": 828630, "input_score": 29, "output_score": 41, "post_title": "Finding the limit of a sequence with an undesirable $\\ln k$"}}
{"input": "I was reading a textbook and saw an alternative formulation of nowhere dense. I am not sure how to prove this alternate formulation below:\nThe Normal Nowhere Dense Statement:\nLet $X$ be a metric space. A subset $A \u2286 X$ is called nowhere dense in $X$ if the interior of\nthe closure of $A$ is empty, i.e. $(\\overline{A})^{\\circ} = \u2205$. Otherwise put, $A$ is nowhere dense i\ufb00 it is contained in a closed set with empty interior. \nAlternate Formulation:\n\"Passing to complements, we can say equivalently that $A$ is nowhere dense i\ufb00 its complement contains a dense open set.\"\nDoes anyone know how I can prove this? It seems rather painfully straightforward but I am not sure how to show it exactly. Thank you!", "output": "First, you should know that, for any $B\\subseteq X$, $X\\setminus\\overline{B}=(X\\setminus B)^\\circ$ and that $X\\setminus B^\\circ=\\overline{X\\setminus B}$. Now\n\\begin{align*}\nA\\text{ nowhere dense }&\\iff\\left(\\overline{A}\\right)^\\circ=\\varnothing\\\\\n&\\iff X\\setminus(\\overline{A})^\\circ=X\\\\\n&\\iff\\overline{X\\setminus \\overline{A}}=X\\\\\n&\\iff\\overline{(X\\setminus A)^\\circ}=X\\\\\n&\\iff (X\\setminus A)^\\circ\\text{ is dense in }X\\\\\n&\\iff(X\\setminus A)\\text{ contains a dense open subset}.\n\\end{align*}\nThe last equivalence may not be so obvious if you're not very used to metric spaces. See below, if necessary:\n\nIf $(X\\setminus A)^\\circ$ is dense in $X$, then $(X\\setminus A)^\\circ$ is a dense open subset of $X\\setminus A$.Conversely, if $(X\\setminus A)$ contains a dense open subset $D$, then $D\\subseteq (X\\setminus A)^\\circ$, so $(X\\setminus A)^\\circ$ is dense as well.", "meta": {"post_id": 829752, "input_score": 35, "output_score": 45, "post_title": "How to show the that a set $A$ nowhere dense is equivalent to the complement of $A$ containing a dense open set?"}}
{"input": "The least common multiple of  $1,2,\\dotsc,n$ is $[1,2,\\dotsc,n]$, then\n$$\\lim_{n\\to\\infty}\\sqrt[n]{[1,2,\\dotsc,n]}=e$$\n\nwe can show this by prime number theorem, but I don't know how to  start\nI had learnt  that it seems we can find the proposition in G.H. Hardy's number theory book, but I could not find it.\nI am really grateful for any help", "output": "Let's look how the least common multiple evolves.\nIf $n > 1$ is a prime power, $n = p^k$ ($k \\geqslant 1$), then no number $< n$ is divisible by $p^k$, but $p^{k-1} < n$, so $[1,2,\\dotsc,n-1] = p^{k-1}\\cdot m$, where $p\\nmid m$. Then $[1,2,\\dotsc,n] = p^k\\cdot m$, since on the one hand, we see that $p^k\\cdot m$ is a common multiple of $1,2,\\dotsc,n$, and on the other hand, every common multiple of $1,2,\\dotsc,n$ must be a multiple of $p^k$ as well as of $m$.\nIf $n > 1$ is not a prime power, it is divisible by at least two distinct primes, say $p$ is one of them. Let $k$ be the exponent of $p$ in the factorisation of $n$, and $m = n/p^k$. Then $ 1 < p^k < n$ and $1 < m < n$, so $p^k\\mid [1,2,\\dotsc,n-1]$ and $m\\mid [1,2,\\dotsc,n-1]$, and since the two are coprime, also $n = p^k\\cdot m \\mid [1,2,\\dotsc,n-1]$, which means that then $[1,2,\\dotsc,n] = [1,2,\\dotsc,n-1]$.\nTaking logarithms, we see that for $n > 1$\n$$\\begin{align}\n\\Lambda (n) &= \\log [1,2,\\dotsc,n] - \\log [1,2,\\dotsc,n-1]\\\\\n&= \\begin{cases} \\log p &, n = p^k\\\\ \\;\\: 0 &, \\text{otherwise}.\\end{cases}\n\\end{align}$$\n$\\Lambda$ is the von Mangoldt function, and we see that\n$$\\log [1,2,\\dotsc,n] = \\sum_{k\\leqslant n} \\Lambda(k) = \\psi(n),$$\nwhere $\\psi$ is known as the second Chebyshev function.\nWith these observations, it is clear that\n$$\\lim_{n\\to\\infty} \\sqrt[n]{[1,2,\\dotsc,n]} = e\\tag{1}$$\nis equivalent to\n$$\\lim_{n\\to\\infty} \\frac{\\psi(n)}{n} = 1.\\tag{2}$$\nIt is well-known and easy to see that $(2)$ is equivalent to the Prime Number Theorem (without error bounds)\n$$\\lim_{x\\to\\infty} \\frac{\\pi(x)\\log x}{x} = 1.\\tag{3}$$\nTo see the equivalence, we also introduce the first Chebyshev function,\n$$\\vartheta(x) = \\sum_{p\\leqslant x} \\log p,$$\nwhere the sum extends over the primes not exceeding $x$. We have\n$$\\vartheta(x) \\leqslant \\psi(x) = \\sum_{n\\leqslant x}\\Lambda(n) = \\sum_{p\\leqslant x}\\left\\lfloor \\frac{\\log x}{\\log p}\\right\\rfloor\\log p \\leqslant \\sum_{p\\leqslant x} \\log x = \\pi(x)\\log x,$$\nwhich shows - the existence of the limits assumed -\n$$\\lim_{x\\to\\infty} \\frac{\\vartheta(x)}{x} \\leqslant \\lim_{x\\to\\infty} \\frac{\\psi(x)}{x} \\leqslant \\lim_{x\\to\\infty} \\frac{\\pi(x)\\log x}{x}.$$\nFor $n \\geqslant 3$, we can split the sum at $y = \\frac{x}{(\\log x)^2}$ and obtain\n$$\\pi(x) \\leqslant \\pi(y) + \\sum_{y < p \\leqslant x} 1 \\leqslant \\pi(y) + \\frac{1}{\\log y}\\sum_{y < p < x}\\log p \\leqslant y + \\frac{\\vartheta(x)}{\\log y},$$\nwhence\n$$\\frac{\\pi(x)\\log x}{x} \\leqslant \\frac{y\\log x}{x} + \\frac{\\log x}{\\log y}\\frac{\\vartheta(x)}{x} = \\frac{1}{\\log x} + \\frac{1}{1 - 2\\frac{\\log \\log x}{\\log x}}\\frac{\\vartheta(x)}{x}.$$\nSince $\\frac{1}{\\log x}\\to 0$ and $\\frac{\\log\\log x}{\\log x} \\to 0$ for $x\\to \\infty$, it follows that (once again assuming the existence of the limits)\n$$\\lim_{x\\to\\infty} \\frac{\\pi(x)\\log x}{x} \\leqslant \\lim_{x\\to\\infty} \\frac{\\vartheta(x)}{x},$$\nand the proof of the equivalence of $(1)$ and $(3)$ is complete.", "meta": {"post_id": 834220, "input_score": 35, "output_score": 42, "post_title": "least common multiple $\\lim\\sqrt[n]{[1,2,\\dotsc,n]}=e$"}}
{"input": "I am confused about the significance of the powers on equations. For example, in $ax = b$, intuitively $b$ is a value $x$ multiplied $a$ times. In $ax + b = c$, $c$ is a value $x$ multiplied $a$ times added to by $b$. In $ax^2 + bx + c = d$, $d$ is a value $x$ multiplied by itself and by another value $a$ added to by the $x$ multiplied by a value $b$ and that added to by another value $c$. This and so on.\nEven in the simple equation $x^2 = y$, $x$ is intuitively $\\sqrt{y}$ and so on for arbitrary powers and roots. For the equation $x + x = y$, $x$ is placed doubly and $y$ is equal to that doubled x, so $x$ would be $y$ halved. But when a particular case like $x^2 + x = y$ arises and so on with all orders and polynomials of those orders (\"higher order equations\"), it cannot be solved so easily and the whole process begins to seem much more foreign and contrived.\nWhy am I no longer able to intuitively solve for the unknown in such an equation when an exponent greater than one is used? Why doesn't the seamless reversing of operations to solve extend to higher order equations?", "output": "I think the difference you are picking up on has to do with the structure of the equation itself.  In some equations, the unknown $x$ is acted on serially through a sequence of nested operations; solving the equation amounts to \"unwinding\" those operations one by one.  For example, $\\frac{(3x+5)^2-19}{2}=10$ can be thought of as \"take $x$, triple it, add $5$, square the result, subtract $19$, and divide by $2$; the result is $10$.  You can diagram this as a series of functions like so:\n\nSo to solve it, you reverse those steps:  Start with $10$, multiply it by $2$, add $19$, take the square root, subtract $5$, divide by $3$.\nBut in other equations -- even fairly simple ones -- have a different kind of structure.  For example, $x^2+5x=10$ looks like this:\n\nNotice that this structure resists any attempt to solve it by \"unwinding\", precisely because of the fork in the diagram.  The $x$ flows through more than one path, which makes it impossible to trace the result backwards to its source.\nSome equations are presented in a form that at first appears to have multiple-paths (e.g. $3x + 5x = 16$) but we can rearrange them to a single-path structure (for example, via the distributive property / combining like terms).  But higher-degree polynomial equations typically have terms that cannot be combined, and this is what makes them resistant to the kind of intuitive solution you are asking about.\nUpdate: In the comments below, the OP asks:\n\nThis makes sense, but then how are general solutions for higher order equations like the quadratic formula derived algebraically/symbolically? I am familiar with the geometric proof by completing the square, but do higher order equations suddenly require the vantage of geometry to solve? Or is there a pure/direct algebraic derivation that can be thought to extend from the basis of \"unwinding\" or the like?\n\nAlthough completing the square is historically geometric in origin, it can be understood in a purely algebraic way as a method of restructuring a function so that it has a \"serial\" structure, enabling it to be solved via unwinding.  Let's take the example of $x^2+5x=10$, already diagrammed above.  In completing the square, you first add $(\\frac{5}{2})^2=\\frac{25}{4}$ to both sides of the equation, obtaining $x^2 + 5x + \\frac{25}{4} = \\frac{65}{4}$.  Then you recognize the left-hand side as a perfect square trinomial, so the equation can be written $(x + \\frac{5}{2})^2 = \\frac{65}{4}$.  This equation, if represented diagrammatically, would have a simple serial structure:  Start with $x$, add $\\frac{5}{2}$, square it, and end up with $\\frac{65}{4}$.  It can then be solved by unwinding: Start with $\\frac{65}{4}$, take the square root(s) (keeping in mind that there are two square roots, one positive and one negative), and subtract $\\frac{5}{2}$.\nOf course, this is not the only method that can be used to tackle quadratics.  Consider the slightly different example of $x^2 + 5x = 24$.  If we rearrange this as $x^2 + 5x - 24 = 0$ and factor the LHS, we get $(x-3)(x+8)=0$.  This equation can be diagrammed as follows:\n\nAt first glance this looks to be no better than the original equation; it has a fork in it, and seems resistant to unwinding.  But!  There is this property of real numbers, the \"zero product property\", which says that if two numbers multiply to be zero, then one of them must be zero.  And that allows you to split the diagram into two separate diagrammatic cases:\n\nAnd each of those can be tackled via a very simple unwinding method.\nIn short, most of the techniques that are taught (at least at the high school level) for solving polynomial equations can be understood as \"methods for re-structuring equations so that unwinding techniques can be employed\".  (I'm not claiming that they are taught in those terms, or that they should be, but merely that they can be thought of that way.)\nUnfortunately this only gets you so far.  Once you get to 5th degree polynomials, it is a famous result that there may be solutions that cannot be expressed by a combination of \"simple\" operations (see here).  That means, among other things, that there is no way to restructure a general 5th degree polynomial so as to enable a solution via unwinding techniques.", "meta": {"post_id": 836362, "input_score": 27, "output_score": 46, "post_title": "Why are higher-degree polynomial equations more difficult to solve?"}}
{"input": "It always baffled me why $L^p$-spaces are the spaces of choice in almost any area (sometimes with some added regularity (Sobolev/Besov/...)). I understand that the exponent allows for convenient algebraic manipulations, but is there more behind it than mathematical convenience? \nWhat bugs me about $L^p$-spaces is that they don't build a scale (of inclusions) but still only allow for one parameter, so by making a choice of exponent you make a choice about two (to my current knowledge) unrelated properties of your function, a) its behavior at singularities (which get milder with high exponent) and b) its tail behavior (which gets less nice with high exponent). How can it still be a good idea to ask \"does this operator map $L^p$ to $L^p$\" rather than \"what does this operator do with singularities and what does it do with tails\"? Of course answers to the latter will be harder to formulate and prove, but is that all?", "output": "This is indeed a very good and natural question, as one usually learns that there is a whole spectrum of $L^p$-spaces but then in practice only $L^2$ (and, to a less extent, $L^1$ and $L^\\infty$) seems to pop up. Why should we care about $L^{\\frac{3}{2}}$? Of course this question has many possible answers, and I find that a convincing one comes from the context of nonlinear analysis.  \n\nThe basic observation is the fact that $$\\lVert u^k\\rVert_{L^p}=\\lVert u\\rVert_{L^{kp}}^k.$$ So, when dealing with nonlinear problems, we can expect that we will have to play some trick with the index $p$. We won't be able to stay in the comfortable $L^2$-space all the time. \n\nFor an example of this let us consider the following PDE: \n$$\\tag{1}\n-\\Delta u (x)= u^2(x), \\qquad x\\in \\mathbb{R}^3.\n$$\nThe associated linear inhomogeneous problem \n$$\\tag{2}\n-\\Delta u= h\n$$\ncan be solved very satisfactorily in the functional setting of $L^2$-space via the Fourier transform: assuming that everything lies in $L^2(\\mathbb{R}^3)$, we can Fourier transform termwise in (2) and write $\\hat{u}(\\xi)=\\lvert\\xi\\rvert^{-2}\\hat{h}$, which can then be anti-transformed back to \n$$u(x)= \\left(\\lvert 4\\pi y\\rvert^{-1} \\ast h\\right) (x)\\stackrel{\\text{def}}{=}(-\\Delta)^{-1} h.$$\n(Note that $\\lvert 4\\pi y\\rvert^{-1}$ is exactly the fundamental solution of the Laplace operator). Setting $h=u^2$, we can now reformulate the nonlinear equation (1) as follows: \n$$\\tag{3}\nu=(-\\Delta)^{-1}\\left( u^2\\right),$$\nwhich is now an equation of fixed-point type. We want to approach it via the contraction mapping principle, by showing that the mapping \n$$\\Phi(u)=(-\\Delta)^{-1}\\left( u^2\\right)$$ \nis contractive on some complete metric space to be specified later. To do so we need some estimates on $\\Phi$ and those can be provided by the Hardy-Littlewood-Sobolev inequality, which in our case ($\\alpha=2,\\ n=3$) reads \n$$\\lVert (-\\Delta)^{-1} f\\rVert_{L^q(\\mathbb{R}^3)} \\le C \\lVert f\\rVert_{L^p(\\mathbb{R}^3)}, \\qquad 2+\\frac{3}{q}=\\frac{3}{p}.\n$$\n(The condition on $p$ and $q$ can be recovered via the scaling argument, by observing that both sides of this inequality are homogeneous with respect to the scaling $f(x)\\mapsto f(\\lambda x)$, and therefore the degrees of homogeneity must match). With $f=u^2$ this inequality reads \n$$\\tag{4}\n\\lVert \\Phi(u)\\rVert_{L^q(\\mathbb{R}^3)}\\le C \\lVert u\\rVert_{L^{2p}(\\mathbb{R}^3)}^2.$$\nIt is now clear that our hands are tied: the only way to get something meaningful is to have $q=2p$, which means that $q=\\frac{3}{2}$. Thus the right functional setting for this problem is $L^\\frac{3}{2}(\\mathbb{R}^3)$-space. \nIndeed, if we let $B_R\\subset L^{\\frac{3}{2}}(\\mathbb{R}^3)$ denote the closed ball of radius $R$, we see from (4) that $\\Phi(B_R)\\subset B_R$ if $R< 1/C$. Then, again by (4), we see that \n$$\n\\begin{split}\n\\lVert \\Phi(u)-\\Phi(v)\\rVert_{L^{\\frac{3}{2}}(\\mathbb{R}^3)}&\\le C \\lVert u^2-v^2\\rVert_{L^{\\frac{3}{4}}(\\mathbb{R}^3)} \\\\ \n&=C\\lVert (u+v)(u-v)\\rVert_{L^{\\frac{3}{4}}(\\mathbb{R}^3)} \\\\\n&\\le C \\lVert u+v\\rVert_{L^{\\frac{3}{2}}(\\mathbb{R}^3)}\\lVert u-v\\rVert_{L^{\\frac{3}{2}}(\\mathbb{R}^3)} \\\\\n&\\le 2RC\\lVert u-v\\rVert_{L^{\\frac{3}{2}}(\\mathbb{R}^3)}.\n\\end{split}\n$$\nThis means that the map $\\Phi\\colon B_R\\to B_R$ is contractive if $R<\\frac{1}{2 C}$. \nAs a final remark, let us observe that we have actually proven two facts:\n\nthe existence of a unique solution to (1) in small neighborhoods of the origin in $L^\\frac{3}{2}(\\mathbb{R}^3)$-space;\nthe fact that the sequence $$\\begin{cases} u_{n+1}=(-\\Delta)^{-1}\\left( u_n^2 \\right) \\\\ \\lVert u_0\\rVert_{L^{\\frac{3}{2}}(\\mathbb{R}^3)} \\le R \\end{cases}$$ converges in the $L^{\\frac{3}{2}}(\\mathbb{R}^3)$ topology to the solution to (1), no matter which initial condition $u_0$ we choose (provided that $R$ is sufficiently small).\n\nFact 2. justifies also the necessity to deal with convergence issues in $L^p$-spaces with $p\\ne 2$.", "meta": {"post_id": 843108, "input_score": 35, "output_score": 37, "post_title": "Why are $L^p$-spaces so ubiquitous?"}}
{"input": "I was hoping someone had an opinion on how to learn higher-mathematics (specific fields that could be of use to me) outside of a classroom setting.\nI graduated with an M.S. in Computer science about a decade ago, standard curriculum that I believe is still somewhat taught (Calc, Multivariate Calc, Dif Eq, Linear Algebra, Discrete Math, etc.). I work as a software engineer (they give us a title of Computer Scientist for some reason) for an Contract R&D (gov stuff).\nI have found my math skills withering over the years, probably for lack of use of particular fields. For the past couple of years, I am constantly reading research papers (computer science related) for background when developing a new algorithm. What I notice is that I will often get stuck on some mathematical notation or methodology that I am unfamiliar with, when trying to understand the paper. I have been attributing this to my withering math skills, and having to do with fields I never studied in school (or deeply enough).\nI try to go back and review what I need to understand the paper, but this leads to a seeming unending link of I need to know this before I can understand that, etc.. With sometimes unsatisfying results.\nI was wondering what people have experienced as the best way to learn higher math (advanced calculus, advanced prob and stats, tensor calculus, advanced linear algebra, etc.) as well as refreshing what they were taught in school MANY years ago.\nI have tried looking course work on MITs website, to see what graduate math students are being taught. I procure those books and notes, and try to go through the class syllabus myself. But I guess its the lack of rigor, that is failing me the most (school imposed a strict rigor), so I end up just glossing over things when I should be trying to deeply understand thee material (trying to get at the meat of what I am trying to understand, for the task at hand). But over-all this seems ultimately flawed and I only come out with partial understanding.\nI want to try to follow a method that would eventually get my math skills on par with a computer science PhD graduate level of understanding of the involved math (say with a focus on computer vision, AI, ML, and computer graphics). What I have been doing over the years is not working for me.\nAny suggestions?", "output": "I\u2019m an econ major, and I have been self-studying pure mathematics along the way for 3 years, from analysis and algebra years ago, to functional analysis, differential geometry, algebraic topology, and algebraic geometry now. I have never take any math courses beyond calculus, linear algebra and probability that are required for econ major. To assure you, serf-learning is not hard at all. It is the most pleasant thing I enjoy in college. But you do need someone to guide you and provide you the relevant information, point to you what books you should read. After your have more and more mathematical maturity, you become more and more independent and are able to find the resources on your own. My aim here is to share my experience and provide guidance for all who want to self study mathematics. After one have had  a firm mathematical background, he or she can go on to study more relevant fields.\nGeneral Advice\n\nMath learning is a long term process. There is no shortcut. You need to take serious efforts, learn as much as possible, and build a firm basis of knowledge. Do not expect just picking relevant fields (like optimization), studying it, and ignore others. Otherwise you will likely encounter lots of gaps, then forget what you\u2019ve learned, and then turn to those old materials over and over agin without any new understanding. Time is a must-have investment for success in math learning.\nThe deeper you go in pure math, the more you will understand those\nelementary concepts in calculus, linear algebra, and other math\ntools used by scientists. For example, if you do not study topology,\nthen it is likely that you will be confused with many convergence\ntheorems you meet everywhere, and you are likely to forget them. And\nwithout topology, you can\u2019t have a true understanding of calculus.\nOnce you have had the theoretical depth, it is often a trivial\nmatter to remember what is really going on.\n\nGeneral Guidance\nThere are two crucial resources for self learning:\n\nYou need good books, and you should spend serious time studying them\nby yourself;\nUse Internet to ask questions and find lecture notes.\n\nAnd after absorbing the knowledge on the books for a while, after you have some mathematical maturity, you should gradually become an active learner\uff1a\nFormulate and ask your own questions, prove theorems listed on the textbooks on your own, using your own notations, or write math blogs to explore your own ideas.\nSo the path is: Read good books to acquire knowledge and maturity $\\Longrightarrow$ At the same time, use Internet to find lecture notes, graphics and videos that can give you less formal and intuitive explanations $\\Longrightarrow$ Have your own ideas and your own understandings on what you learned, and at the same time are more skilled at tackling math problems.\n\nSpecific Guidance\nAfter Calculus and some basic notion in linear algebra, a self-learner may begin his or her first journey on rigorous mathematics. The first book I recommend is\nS. Axler, Linear Algebra Done Right\nThis is the book that lead me to the fantastic world of pure mathematics. I remember how surprised I was when I saw the beautiful proofs and the powerful abstractions for the first time, and how I enjoy reading it day and night in my fresh year (I even read it the night before an econ course final exam :)  It focus on the theory of linear algebra instead of the determinant approach and calculations of matrices. The ideas and the proofs in the book can quickly increase one\u2019s mathematical maturity. And by the way, linear algebra is used extensively in many areas of mathematics, like differential geometry and functional analysis, so a good understanding of it is very important.\nAt the same time of reading Axler\u2019s Linear Algebra Done Right, you may also read\nT. Apostol,  Mathematical Analysis\nPersonally I self-studied Rudin\u2019s Principles of Mathematical Analysis in my fresh year, and read the Apostol afterward. But looking back, I would recommend a self-learner to swallow something easier first. The most important lesson I have learned for self-studying is one should not go too fast.\nAfter that, you have abstract algebra, real analysis, ODE, complex analysis and topology waiting for you to learn. You may start learning topology using\nJ. Munkres, Topology\nright after you have finished mathematical analysis. This book is very suitable for self-learning, and it can also greatly increase one\u2019s mathematical maturity.\nFor abstract algebra I recommend:\nM. Artin, Algebra\nwhich is pretty well-known. It also contains materials on linear algebra that are missing in Axler\u2019s book. I have also read Dummit and Foote, Abstract Algebra but I do not recommend it as a first encounter, since although its materials is detailed, it contains far less motivations, which can cause pain on a self-learner.\nFor complex analysis I recommend\nR. Ash, Complex Variables\nSelf-learners, please do not read the famous Complex Analysis by Ahlfors for a first-time learning in complex analysis. It\u2019s completely useless to you. I also recommend postponing the reading of Rudin\u2019s other two books (real and complex analysis, functional analysis) till much later where you have had enough background and motivations.\nA nice book on ODE is\nW. Adkins and M. Davidson. Ordinary Differential Equations\nFor real analysis, I strongly recommend a graduate level book:\nJ Yeh, Real Analysis: Theory of Measure and Integration\nwhich is super-detailed, zero-gap. A real gem. But warning: while the book is extremely helpful, you should not indulge yourself in the comfortable proofs and go through the material all way long without thinking. Always ask: what theory I have learned, what proof methods I have mastered, and can I remember and reproduce the whole machinery on my own?\nAfter all of these, you may begin exploring more advanced subjects. Here I list several books on each subject\nFunctional Analysis\nE. Kreyszig, Introductory Functional Analysis With Applications\nC. Aliprantis and K. Border, Infinite Dimensional Analysis: A Hitchhiker\u2019s Guide\nP. Lax, Functional Analysis\nJ. Conway, A Course in Functional Analysis\nR. Megginson, An Introduction to Banach Space Theory\nProbability Theory\nR. Ash, Probability and Measure Theory\nJ. Rosenthal, A First Look at Rigorous Probability Theory\nS. Resnick, A probability path\nD. Williams, Probability with martingales\nR. Dudley, Real analysis and probability\nK. Chung, A Course in Probability Theory\nP. Billingsley, Probability and measure\nE. \u00c7inlar, Probability and Stochastics\nP. Billingsley, Convergence of Probability Measures\nDifferential Geometry\nJ. Lee, Introduction to Smooth Manifolds\nM. Spivak, A Comprehensive Introduction to Differential Geometry\nAlgebraic Topology\nK. J\u00e4nich, Topology\nR. Bott and L. Tu, Differential Forms in Algebraic Topology\nAlgebraic Geometry\nM. Ried, Undergraduate Algebraic Geometry\nK. Smith etc, An Invitation to Algebraic Geometry\nD. Mumford, The Red Book of Varieties and Schemes\nThe Stacks Project\nOther Subjects, and More Books for References:\nG. Hardy and E. Wright, An Introduction to the Theory of Numbers\nR. Diestel, Graph Theory (free!)\nC. Adams, The Knot Book\nL. Evans,\u00a0Partial Differential Equations\nJ. Munkres, Analysis on Manifolds\nW. Rudin, Real and Complex Analysis\nW. Rudin, Functional Analysis\nD. Cohn, Measure Theory\nE. Weiss and G. Stein, Introduction to Fourier Analysis on Euclidean Space\nPrinceton Lectures in Analysis by E. Stein and R. Shakarchi\nOnline resources are also very important\nSee for example the excellent videos made by Jos Leys:\nhttp://www.josleys.com/galleries.php?catid=13\nOther websites:\nVirtual Math Museum\nPaul Nylander\u2019s Personal Website\nTerence Tao\u2019s Blog\nThe Scientific Graphics Project\nSavoir Sans Frontieres\nAnd some excellent notes:\nBrad Osgood \u2013 The Fourier Transform and its Applications\nPaul Garrett \u2013 Abstract Algebra\nAndreas Gathmann \u2013 Algebraic Geometry\n$\\hspace{3cm}$\nGood luck to all self learners!!!", "meta": {"post_id": 843697, "input_score": 41, "output_score": 41, "post_title": "Learning higher-mathematics on your own"}}
{"input": "Please translate Hilbert's paper Ein Beitrag zur Theorie des Legendre'schen Polynoms. I cannot find an existing translation for it. Thanks:)", "output": "Acta mathematica 18, printed April 3rd, 1894\nA CONTRIBUTION TO THE THEORY OF THE LEGENDRE POLYNOMIAL\nBY \nDAVID HILBERT\nK\u00f6nigsberg, Prussia\nThis communication at hand is about the question of the smallest value different from $0$ which the integral\n$$\nI = \\int\\limits_\\alpha^\\beta \\left[ f(x) \\right]^2 \\, dx \\quad (\\beta \\gt \\alpha)\n$$\nis able to attain, if one chooses for $f(x)$ a polynomial (lit.: \"whole rational function\") of degree $n - 1$ with integer coefficients and if one understands $\\alpha$ and $\\beta$ as given constants.\nIf one sets\n$$\nf(x) = a_1 x^{n - 1} + a_2 x^{n - 2} + \\cdots + a_n\n$$\nthe integral turns into a positive quadratic form of the $n$ variables $a_1$, $a_2, \\ldots, a_n$:\n$$\nI = \\sum^{i,k} \\alpha_{ik} a_i a_k, \\quad (i, k = 1, 2, \\ldots, n)\n$$\nwhose coefficients are given by the formula\n$$\n\\alpha_{ik} = \n\\int\\limits_\\alpha^\\beta x^{2n-i-k} dx =\n\\frac{\\beta^{2n-i-k+1}-\\alpha^{2n-i-k+1}}{2n-i-k+1} \n$$\nTo get an upper bound for the minimum of this quadratic form $I$ it is needed to calculate its discriminant\n$$\nD_{\\alpha \\beta} =\n\\left|\n\\begin{matrix} \n\\alpha_{11} & \\alpha_{12} & \\cdots & \\alpha_{1n} \\\\\n\\alpha_{21} & \\alpha_{22} & \\cdots & \\alpha_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\alpha_{n1} & \\alpha_{n2} & \\cdots & \\alpha_{nn}\n\\end{matrix}\n\\right| .\n$$\n(end of page 1)\nIf we replace in this determinant each element by its integral expression and thereby use for all elements of the first horizontal row the integration variable $x = x_1$, for all elements of the $2^{\\tiny\\mbox{nd}}, \\ldots, n^{\\tiny\\mbox{th}}$ horizontal row regarding the integration variables $x = x_2, \\ldots, x_n$, the discriminant of the quadratic form $I$ is represented as $n$-fold integral of the following form:\n$$\nD_{\\alpha \\beta} = \n\\int\\limits_\\alpha^\\beta \\ldots \\int\\limits_\\alpha^\\beta\nx_1^{n-1} x_2^{n-2} \\ldots x_n^0\n\\prod^{i,k}(x_i - x_k) dx_1 \\ldots dx_n .\n$$\nThe interchange of the $n$ integration variables $x_1,\\ldots,x_n$ and\nthe addition of the resulting equations gives\n$$\nD_{\\alpha \\beta} = \n\\frac{1}{\\left|\\underline{n}\\right.}\n\\int\\limits_\\alpha^\\beta\n\\ldots\n\\int\\limits_\\alpha^\\beta\n\\prod^{i,k} (x_i - x_k)^2 dx_1 \\ldots dx_n\n$$\nand if we -- using\n$$\nx_i = \\frac{1}{2}(\\beta - \\alpha) y_i + \\frac{1}{2}(\\beta + \\alpha)\n$$\nintroduce the new integration variables $y_1,\\ldots,y_n$, \nwe obtain the formula\n$$\nD_{\\alpha \\beta} =\n\\left(\n\\frac{\\beta - \\alpha}{2}\n\\right)^{n^2} D, \n$$\nwhere for abbreviation $D = D_{-1, +1}$ has been set.\nFor example it follows for $\\alpha = 0$, $\\beta = 1$\n$$\nD = 2^n\n\\left|\n\\begin{matrix} \n1 & 0 & \\frac{1}{3} & 0 & \\frac{1}{5} & \\cdots \\\\\n0 & \\frac{1}{3} & 0 & \\frac{1}{5} & 0 & \\cdots \\\\\n\\frac{1}{3} & 0 & \\frac{1}{5} & 0 & \\frac{1}{7} & \\cdots \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n\\end{matrix}\n\\right|\n= 2^{n^2}\n\\left|\n\\begin{matrix} \n1 & \\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} & \n\\cdots & \\frac{1}{n} \\\\\n\\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} &\n\\cdots & \\frac{1}{n + 1} \\\\ \n\\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} & \n\\cdots & \\frac{1}{n + 2} \\\\ \n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{n} & \\frac{1}{n+1} & \\frac{1}{n+2} & \\frac{1}{n+3} &\n\\cdots & \\frac{1}{2n - 1} \n\\end{matrix}\n\\right| .\n$$\n(end of page 2)\nTo calculate $D$ we develop the polynomial (lit.: \"whole rational function\") $f(x)$ into a progressing series of Legendre polynomials $X_m$.\nBecause of\n$$\nx^m = c_m X_m + c_m' X_{m-1} + \\cdots + c_m^{(m)} X_0,\n$$\nwhere\n$$\nc_m = \\frac{\\left| \\underline{m} \\right.}{1 \\cdot 3 \\cdot 5 \\cdots (2m-1)},\n$$\nwe get\n$$\n\\begin{align}\nf(x) &=\na_1 (c_{n-1} X_{n-1} + c_{n-1}' X_{n-2} + \\cdots) +\na_2 (c_{n-2} X_{n-2} + c_{n-2}' X_{n-3} + \\cdots) + \\cdots \\\\\n&= c_{n-1} a_1 X_{n-1} +\n(c_{n-1}' a_1 + c_{n-2} a_2) X_{n-2} +\n(c_{n-1}'' a_1 + c_{n-2}' a_2 + c_{n-3} a_3 X_{n-3}) + \\cdots\n\\end{align}\n$$\nand with the help of the formulas\n$$\n\\int\\limits_{-1}^{+1} X_m^2 dx = \\frac{2}{2m+1}, \\quad\n\\int\\limits_{-1}^{+1} X_m X_k dx = 0 \\quad\n(m \\ne k)\n$$\nit therefore follows\n$$\n\\left[ I \\right]_{\\alpha = -1 \\atop \\beta = +1} =\n\\int\\limits_{-1}^{+1} f^2(x) \\, dx =\n\\frac{2}{2n-1} b_1^2 +\n\\frac{2}{2n-3} b_2^2 +\n\\frac{2}{2n-5} b_3^2 + \\cdots ,\n$$\nwhere\n$$\n\\begin{align}\nb_1 &= c_{n-1} a_1, \\\\\nb_2 &= c_{n-1}' a_1 + c_{n-2} a_2, \\\\\nb_3 &= c_{n-1}'' a_1 + c_{n-2}' a_2 + c_{n-3} a_3, \\\\\n. \\,\\,\\, & . \\,\\,\\, . \\,\\,\\, . \\,\\,\\, . \\,\\,\\, . \\,\\,\\, . \\,\\,\\, . \n\\,\\,\\, . \\,\\,\\, . \\,\\,\\, . \\,\\,\\, . \\,\\,\\, . \\,\\,\\, . \\,\\,\\, .\n\\end{align}\n$$\nhas been set. Because of this representation as sum of squares of linear forms one wins for the disciminant $D$ the value\n$$\n\\begin{align}\nD &=\n\\frac{2^n}{1 \\cdot 3 \\cdot 5 \\ldots (2n-1)}\n(c_0 c_1 \\cdots c_{n-1})^2 \\\\\n&= 2^{n^2} \n\\frac{\\left\\{\n1^{n-1} 2^{n-2} \\ldots (n-2)^2 (n-1)^1 \n\\right\\}^4}{1^{2n-1} 2^{2n-2} \\ldots (2n-2)^2 (2n-1)^1}\n\\end{align}\n$$\nand herein is the right side exactly identical with\n$\\frac{2^{n^2}}{\\Delta}$ where $\\Delta$ means that value, which I, in my treatise \"About the Discriminant of the in the Finite terminating Hypergeometric Series\" (\"\u00dcber die Discriminante der im Endlichen abbrechenden hypergeometrischen Reihe\") [1],\n[1] Crelle's Journal, Vol. 103, p. 342.\n(end of page 3)\ngot for the discriminant of a $n$-th degree Legendre polynomial under a  certain linear transformation. \nUnder consideration of the formula for $D$ stated above, from this follows the result\n\nThe discriminant of the quadratic form $\\int\\limits_0^1 f^2(x)\\,dx$ has the value \n  $$\n\\left|\n\\begin{matrix} \n1 & \\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} & \n\\cdots & \\frac{1}{n} \\\\\n\\frac{1}{2} & \\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} &\n\\cdots & \\frac{1}{n + 1} \\\\ \n\\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} & \n\\cdots & \\frac{1}{n + 2} \\\\ \n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{1}{n} & \\frac{1}{n+1} & \\frac{1}{n+2} & \\frac{1}{n+3} &\n\\cdots & \\frac{1}{2n - 1} \n\\end{matrix}\n\\right| \n=\n\\frac{\\left\\{\n1^{n-1} 2^{n-2} \\ldots (n-2)^2 (n-1)^1 \n\\right\\}^4}{1^{2n-1} 2^{2n-2} \\ldots (2n-2)^2 (2n-1)^1}\n$$\n  and is equal to the reciprocal value of the disciminant of \n  the $n^{\\tiny\\mbox{th}}$ degree equation\n  $$\n\\xi^n + \n\\binom{n}{1}^2 \\xi^{n-1} +\n\\binom{n}{2}^2 \\xi^{n-2} + \\ldots + 1 = 0, \n$$\n  whose left side, by a linear transformation of the variable $\\xi$, can be turned into the Legendre polynomial $X_n$.\n\nWe now return to the original posed question.\nApplication of the Stirling forumla, if $N$ is a positive number,\ndelivers the equation\n$$\nN l 1 + (N-1) l 2 + \\ldots + \n2 l (N-1) + 1 l N =\n\\frac{1}{2}N^2 l N - \\frac{3}{4} N^2 (1 + \\varepsilon_N),\n$$\nwhere $\\varepsilon_N$ means a with growing $N$ vanishing number.\nWith the help of this formula one easily finds\n$$\nl D = (1 + \\varepsilon_n') l (2^{-n^2}),\n$$\n(end of page 4)\nwhere $\\varepsilon_n'$ vanishes with growing $n$, i.e.\n$$\nD = \\eta_n 2^{-n^2}\n$$\nand further\n$$\nD_{\\alpha \\beta} = \n\\eta_n \\left(\n\\frac{\\beta - \\alpha}{4}\n\\right)^{n^2},\n$$\nwhere $\\eta_n$ approaches the unit with growing $n$.\nNow it is, according to a theorem by H. Minkowski [2], always possible,\nfor a definite quadratic form, to choose the $n$ variables such that\nthe value of the quadratic form turns out less than the $n$-fold\nof the $n^{\\tiny \\mbox{th}}$ root of its discriminant.\nIf therefore the positive difference $\\beta - \\alpha$ is assumed\nless than $4$, it follows that it is always possible\nto determine a polynomial (lit.: \"whole rational function\")\n$f(x)$ with integer coefficients for which the value of\nthe integral $I = \\int\\limits_\\alpha^\\beta f^2(x)\\,dx$ turns\nout smaller than \n$n \\eta_n' \\left( \\frac{|\\beta - \\alpha|}{4} \\right)^n$.\nBut because $\\eta_n'=\\sqrt[n]{\\eta_n}$ approaches the unit too\nwith growing $n$, we get the result\n\nThe integral $\\int\\limits_\\alpha^\\beta f^2(x) \\, dx$ can\n  attain an arbitrary small positive value, if one chooses\n  the polynomial (lit.: \"whole integer function\") $f(x)$\n  properly, under the condition that the interval of\n  integration $\\alpha$ to $\\beta$ is smaller than $4$. \n\nK\u00f6nigsberg in Prussia, March 13th, 1893.\n[2] Crelle's Journal, Vol. 107, p. 291. \n(end of page 5)\n(Special thanks to Donald E. Knuth and the MathJax team)", "meta": {"post_id": 845985, "input_score": 5, "output_score": 92, "post_title": "Translate a German paper by Hilbert"}}
{"input": "Every proof I found online made the same implications.\nTake one for example:\nhttps://artofproblemsolving.com/wiki/index.php?title=Nilradical\nI'm quoting the relevant part, which confuses me:  \n\"To show the converse, it suffices to show that for any non-nilpotent element $a$, there is some prime ideal that does not contain $a$.\nSo suppose that $a$ is an element of $A$ that is not nilpotent. Let $S$ be the set of ideals of $A$ that do not contain any element of the form $a^n$. Since $(0) \\in S$, $S$ is not empty; then by Zorn's Lemma, $S$ has a maximal element $\\mathfrak{m}$.\nIt suffices to show that $\\mathfrak{m}$ is a prime ideal. Indeed, suppose otherwise; then there exist elements $x,y \\notin \\mathfrak{m}$ for which $xy \\in \\mathfrak{m}$. Then the set of elements $z$ for which $xz \\in \\mathfrak{m}$ is evidently an ideal of $A$ that properly contains $\\mathfrak{m}$; it therefore contains $a^n$, for some integer $n$. By similar reasoning, the set of elements $z$ for which $a^n$ $z \\in \\mathfrak{m}$ is an ideal that properly contains $\\mathfrak{m}$, so this set contains $a^m$, for some integer $m$. Then $a^{n+m} \\in \\mathfrak{m}$, a contradiction.\nTherefore $\\mathfrak{m}$ is a prime ideal that does not contain $a$.\"  \nAnd then they conclude that if $a$ is not in the maximal element in $A$, and it is indeed prime, then $a$ is not in the intersection of all prime ideals,\nbut who said there isn't some other prime ideal $P$ that is not in $A$?", "output": "Instead of commenting on this proof, which is too complicated in my opinion, here is a fast proof.\nAssume that $a$ is not nilpotent. Then the localization $A_a$ is non-zero (otherwise $1/1=0$ in $A_a$, which would mean $a^n \\cdot 1 =0$ for some $n$). Hence, it has a prime ideal $\\mathfrak{p}$. The preimage in $A$ is a prime which doesn't contain $a$ (otherwise $\\mathfrak{p}$ would contain a unit). Hence, $a$ is not contained in the intersection of all prime ideals of $A$.", "meta": {"post_id": 859390, "input_score": 15, "output_score": 39, "post_title": "The nil-radical is an intersection of all prime ideals proof"}}
{"input": "$\\phi(\\pi(\\phi^\\pi)) = 1$\nI saw it on an expired flier for a lecture at the university. I don't know what $\\phi$ is, so I tried asking Wolfram Alpha to solve $x \\pi x^\\pi = 1$ and it gave me a bunch of results with $i$, and I don't know what that is either.", "output": "It's a joke based on the use of the $\\phi$ function (Euler's totient function), the $\\pi$ function (the prime counting function), the constant $\\phi$ (the golden ratio), and the constant $\\pi$. Note $\\phi^\\pi\\approx 4.5$, so there are two primes less than $\\phi^\\pi$ (they are $2$ and $3$), so $\\pi(\\phi^\\pi)=2$. There is only one positive integer less than or equal to $2$ which is also relatively prime to $2$ (this number is $1$), so $\\phi(2)=1$. Hence we have\n$$\\phi(\\pi(\\phi^\\pi))=\\phi(2)=1$$", "meta": {"post_id": 861618, "input_score": 13, "output_score": 35, "post_title": "Is the equation $\\phi(\\pi(\\phi^\\pi)) = 1$ true? And if so, how?"}}
{"input": "Is it possible to use row and column operations \"at the same time\" on a matrix $A$? So, for example, first subtracting $row_1$ from $row_2$, and then choosing to multiply $column_3$ by a constant $c$? Or do you have to \"stick to one method\" when reducing a matrix? If so, can somebody explain why?\nEdit: so let me put this more clearly. Suppose you have $$\\left( \\begin{array}{ccc}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23}  \\\\\na_{31} & a_{32} & a_{33} \\end{array} \\right)$$ \nFirst I multiply row 2 by $\\frac{a_{11}}{a_{21}}$, row 3 by $\\frac{a_{11}}{a_{31}}$, then subtract row 1 from row 2 and row 3 so you end up with a matrix of a new general form:\n$$\\left( \\begin{array}{ccc}\na_{11} & a_{12} & a_{13} \\\\\n0 & b_{22} & b_{23}  \\\\\n0 & b_{32} & b_{33} \\end{array} \\right)$$ \nwith $b_{ij}$ the new coefficients which were attained by our row operations. My question is: could you then say, subtract column 1 from columns 2 and 3 in order to get:\n$$\\left( \\begin{array}{ccc}\na_{11} & 0 & 0 \\\\\n0 & b_{22} & b_{23}  \\\\\n0 & b_{32} & b_{33} \\end{array} \\right)?$$ \nEdit2: How come that, according to the answers, this is incorrect, even though this is the exact same thing Serge Lang does in his book \"Introduction to Linear Algebra\"? What's happening here? Are you telling me Serge Lang doesn't know how to reduce a matrix? Reminder: this is NOT a linear system we're solving for, and this is NOT an augmented matrix. It's just a matrix $A$.\nEdit3: here is some more context.", "output": "What \"reducing a matrix\" means\nOne issue here is what is meant by \"reducing a matrix\". The only purpose I can see for reducing a matrix is to decide if the matrix has some specific property. Whatever property you are interested in has to remain unchanged under the operations you do or you have not succeeded in your purpose.\nIf you investigate Serge Lang's book closely, you will find that he had a specific purpose for reducing the matrix,(to find the rank, discussed below) which allows him to use column operations to achieve his aim.\nIt is important to note that for most people, the phrase \"reducing a matrix\" refers specifically to finding the Reduced Row Echelon Form (also known as RREF). As the name implies, RREF is defined using the rows of the matrix:\n 1. The leftmost nonzero entry in any row is a 1 (called a \"leading 1\").\n 2. Leading 1's on lower rows are further to the right than leading 1's on higher rows.\n 3. Other entries in a column with a leading 1 are zero.\n 4. All rows consisting entirely of zeros are at the bottom of the matrix.\nSince you are defining this matrix in terms of rows, you must do row operations to achieve it. In fact, if you do use row operations, then given a particular starting matrix (which may or may not be square, by the way), there is precisely one RREF that it will reduce to. If you do column operations you will not arrive at this same matrix.\nRelationships to solutions of equations\nThe point others have made about finding solutions is valid. Every matrix can be used to represent the coefficients of a system of linear equations, regardless of whether you want it to or not. Many of the important properties of matrices are strongly related to this system of linear equations, and so it is a good idea to have methods that preserve these properties. \nFor example, the set of solutions to $A\\mathbf{x} = \\mathbf{0}$ is the same as the set of solutions to the (homogeneous) linear equations, and the nonzero rows of the RREF form a basis for the space perpendicular to this space of solutions. Neither of these properties are preserved by column operations, but they are preserved by row operations.\nMost importantly, just to reiterate: the solution to your homogeneous linear equations is definitely not preserved by column operations, and this is an important consideration regardless of whether you want your matrix to represent linear equations or not.\nNote that there are things that column operations do preserve, such as the range of the linear transformation defined by your matrix. However a mixture of row and column operations will not preserve this, nor most of the properties you care about.\nThe inverse of a square matrix\nOne property that a mixture of row and column operations does preserve is invertibility. You can see this from the idea of elementary matrices.\nDoing elementary row operations corresponds to multiplying on the left by an elementary matrix. For example, the row operation of \"new R2 = R2 - 3R1\" is produced on a 3 by n matrix when you multiply on the left by $\\begin{pmatrix} 1 & 0 & 0 \\\\ -3 & 1 & 0\\\\ 0 & 0 & 1 \\end{pmatrix}$. Column operations, on the other hand, are produced when you multiply by a matrix on the right hand side. The column operation of \"new C2 = C2 - 3C1\" is produced on an m by 3 matrix when you multiply on the right by $\\begin{pmatrix} 1 & -3 & 0 \\\\ 0 & 1 &  0 \\\\ 0 & 0 & 1\\end{pmatrix}$.\nThe process of finding the inverse of a square matrix by augmenting an identity and doing matching row operations on both works because of these elementary matrices. If you perform row operations on $A$ to get $I$ and the elementary matrices corresponding to these operations are $E_1$, ..., $E_k$, then we have $E_k ... E_1 A = I$ and so the inverse of $A$ must be $E_k ... E_1$. But this is equal to $E_k ... E_1 I$, which is what you get when you do all those row operations on the identity!\nThis would work perfectly well if you did column operations, however a mixture of the two is a bit harder to understand. Suppose you did row operations with matrices $E_1$, ..., $E_k$ and column operations with matrices $F_1$, ..., $F_h$ and you produced the identity as a result. Then\n$$\n\\begin{align}\nE_k \\dots E_1 A F_1 \\dots F_h &= I\\\\\nA &= (E_1)^{-1} \\dots (E_k)^{-1} I (F_h)^{-1} \\dots (F_1)^{-1}\\\\\nA^{-1} &= F_1 \\dots F_h I E_k \\dots E_1\n\\end{align}\n$$\nThis is not the result of doing the matching column operations and row operations on $I$ because the matrices are on the wrong side!\nSo if you can row-and-or-column reduce a matrix to the identity, then this proves that the matrix is invertible, but it is tricky to tell what the inverse actually is unless you do only row or only column operations.\nFinally note that row and column operations simultaneously will allow you to calculate the determinant of a square matrix easily, and so there is a certain advantage to doing this. In particular a row/column operation of the type \"new Ri = Ri + k Rj\" or \"new Ci = Ci + k Cj\" will not change the determinant, so if you restrict yourself to those operations, you can get your matrix into a form where it is clear what the determinant is more quickly than restricting yourself to just one. However, it is worth stressing that this relates to my original comment about the purpose you are trying to achieve -- this is only really an advantage in the situation where you are finding determinants. \nThe rank of a matrix\nFinally, this row-and-or-column reduction also does preserve the rank of your matrix. If you do this kind of reduction on any matrix, what you are guaranteed to produce eventually if you try is a matrix with some 1's down its main diagonal (but not necessarily all the way down) and zeros everywhere else. Something like these:\n$$\n\\begin{pmatrix} 1 & 0 & 0 & 0 & 0\\\\ 0 & 1 & 0 & 0 & 0\\\\ 0 & 0 & 1 & 0& 0\\end{pmatrix} \\quad \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0& 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}\n$$\nThe number of 1's on that diagonal is the rank of your matrix. That is, it is the dimension of the column space and row space, and the dimension of the range of the linear transformation defined by your matrix, and n minus the dimension of the solution space to your system of equations. So the two matrices above have rank 3 and 2 respectively.\nHowever, there is nothing in the final reduced form that tells you what the actual solutions to the equations are or what the bases for the spaces are. These can all be changed in ways that are not easy to follow if you do both row and column operations. If you did just column operations, then the leading-1 columns would be a basis for the range of the linear transformation, but with both row and column operations, these columns are all standard basis vectors and won't necessarily be correct. If you did just row operations, then the non-leading-1 columns would appear as the first several coordinates of the vectors in a basis for the null space of the matrix, but with both row and column operations these columns are all zero. \nSummary\n\nMost people mean Reduced Row Echelon Form when they say \"reduced matrix\", which is defined by doing only row operations.\nRow operations preserve many useful properties of matrices and tell you certain information about matrices.\nColumn operations preserve some useful properties of matrices and tell you certain information about matrices.\nMixtures of row and column operations preserve only a small number of properties. In particular, a mixture will preserve the rank and the invertibility of a matrix (but will not provide bases for associated subspaces or the actual inverse itself).", "meta": {"post_id": 863575, "input_score": 27, "output_score": 37, "post_title": "Can you use row and column operations interchangeably?"}}
{"input": "Solving $a x^2 + bx +c=0$ for $x$ gives\n$$x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a} \\text{, for } a \\ne 0$$\nBut for $a = 0$ we get \n$$x=-\\frac{c}{b}$$\nHow to implement a numerically stable algorithm for computing $x$ from $a,b,c$ that allows $a$ to be close to zero or zero?", "output": "The previous answers correctly identify that there are two quadratic formulas, and that each has a different range of numeric stability; however, they miss the other subtle instabilities.\nTo recap: the standard quadratic formula,\n\\begin{align*}\n  x=\\frac{-b\\pm\\sqrt{b^2-4\\,a\\,c}}{2\\,a},\n\\end{align*}\nis not unique. A second formula can be generated by multiplying the\nnumerator and denominator by the \"conjugate\" of the numerator, i.e.,\n\\begin{align*}\n  x&=\\frac{-b\\pm\\sqrt{b^2-4\\,a\\,c}}{2\\,a}\\times\\frac{-b\\mp\\sqrt{b^2-4\\,a\\,c}}{-b\\mp\\sqrt{b^2-4\\,a\\,c}}= \\frac{2\\,c}{-b\\mp\\sqrt{b^2-4\\,a\\,c}},\n\\end{align*}\nwhere $\\mp=-(\\pm)$. These two expressions are analytically equivalent; however, each is numerically unstable for certain values of the\ncoefficients. \nThe instability discussed by the other posts is if the term, $4\\,a\\,c$, is small compared to $b^2$ and the sign of the radical term and $b$ are\nthe same, then catastropic cancellation occurs. \nFortunately, the two quadratic formulas have opposite signs on the radical term for\nthe same roots, thus it is possible to avoid catastrophic cancellation\nby selecting the stable form. I.e.,\n\\begin{align*}\nx_1 &= \\frac{-b-{\\rm sign}(b)\\sqrt{b^2-4\\,a\\,c}}{2a}& x_2 = \\frac{c}{a\\,x_1}\n\\end{align*}\nFor completeness, there are at least three other numerical\ninstabilities in this formula:\n\nWhen $a=0$: thus the equation is linear and has at most one root given by $-c/b$. It should be noted that if $b=0$ as well, the equation is actually $c=0$ and leaves $x$ undefined/unconstrained!\nWhen $a\\neq0$ and $c=0$: thus one root is at $x=0$. In this case, the second form of the quadratic equation will yield a NaN ($0/0$) for the second root. It is best therefore to determine the second root by factoring out the root at zero to give $-b/a$.\nOverflow: the floating point representation might over/under-flow during calculation and this is most critical for the $b$ coefficient.  Defining $\\mathcal{R}_{max}$ as the largest representable floating point number ($\\approx1.8\\times10^{308}$ for double precision) then $b^2$ will overflow to infinity at $|b|>\\sqrt{\\mathcal{R}_{max}}$ according to the IEEE 754 specification (which is \"only\" $\\approx1.304\\times10^{154}$). You might not define this as an instability, more an out of range error, but I think the reduced upper range of $b$ is surprising and certainly caught me out!\n??: I'm pretty sure I've missed some more, floating point arithmetic can be very subtle.\n\nI have written a compile-time algebra library and you can take a look at my implementation of the quadratic formula. Below it is a \"more-stable\" implementation of the cubic formula, shamelessly stolen from D. Herbison-Evans. For higher-order polynomials I rely on bisection techniques based around Budan's theorem which isn't nearly as difficult to make stable!", "meta": {"post_id": 866331, "input_score": 22, "output_score": 35, "post_title": "Numerically stable algorithm for solving the quadratic equation when $a$ is very small or $0$"}}
{"input": "Can someone explain in a simple way why there are so few known exact Ramsey numbers? I guess it's because there are no efficient algorithms for this task, but are there so many combinations to test?\nAnd an additional question: How are the bounds determined? Why do the bounds, that are known, have those values? Why not i.e. try to take a lower number for the upper bound for some 2-coloring?", "output": "Unfortunately, the original proof that Ramsey numbers exist (as in, are finite) was non-constructive, so mathematicians have been fighting an uphill battle from the beginning. Consider the diagonal Ramsey numbers $R(s, s)$.  The smallest of these which remains unknown is $R(5, 5)$, so let's say we suspect $R(5, 5) = 43$\u2014the current lower bound.  We would need to confirm that the relevant property holds for every possible $2$-coloring on $K_{43}$.  Since this graph has $\\displaystyle \\binom{43}{2} = 903$ edges, there are $2^{903}$ possible colorings.  This is a $272$ digit number!  For comparison, this number is many orders of magnitude larger than the total number of protons, neutrons, and electrons in the entire observable universe.  Of course, it's possible we can whittle this number down a bit with some ingenuity (for instance, some colorings are equivalent up to symmetry), but it would still be unimaginably gargantuan.  Brute force is out of the question, even for the most powerful supercomputers.  \n\nBounds: \nThe work on lower bounds for diagonal Ramsey numbers improves upon the original lower bound Paul Erd\u0151s found using his probabilistic method$^\\dagger$.  With this method, it's possible to show (rather easily) that $\\displaystyle R(s, s) \\geq \\lfloor 2^{\\frac{s}{2}} \\rfloor$.  Of course, the  general lower bound has since been improved, but it still has an exponential growth factor of $\\sqrt{2}$.    \nA relatively decent upper bound for diagonal Ramsey numbers can be proven using the same approach as in the proof that $R(s, t) < \\infty$.  That is, we can show $\\displaystyle R(s, t) \\leq \\binom{s+t-2}{s-1}$.  When $s=t$, we get $\\displaystyle R(s, s) \\leq \\binom{2s-2}{s-1}$, which grows exponentially with a growth factor of $4$.  The current upper bound has been improved a bit, but still has the same exponential growth factor.\n\n$^\\dagger$ I'd highly recommend checking out this proof; it's one of my favorites.  A downright ingenious application of probability theory to this area of math.  Find it here.", "meta": {"post_id": 867470, "input_score": 27, "output_score": 41, "post_title": "Why are there only a few known Ramsey numbers?"}}
{"input": "Let $A$ and $B$ be symmetric matrices.\nProve:\n\n$AB=BA$\n$AB$ is a symmetric matrix\n\nAs for 1. due to the axiom $(AB)^T=B^T A^T$ so $AB=BA$\nAs for 2. I did not find any axiom that can support the claim, but from test I found that it is true for symmetric matrices when the entries on the diagonal are equal.", "output": "Both claims are false and almost any $A$ and $B$ are counterexamples. For a specific example, you can see $$\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 & 2 \\\\ 2 & 3 \\end{pmatrix} = \\begin{pmatrix} 3 & 5 \\\\ 3 & 5 \\end{pmatrix}$$ while $$\\begin{pmatrix} 1 & 2 \\\\ 2 & 3 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 3 & 3 \\\\ 5 & 5 \\end{pmatrix}.$$", "meta": {"post_id": 874469, "input_score": 17, "output_score": 51, "post_title": "Symmetric matrix multiplication"}}
{"input": "I discovered the following conjecture by evaluating the integral numerically and then using some inverse symbolic calculation methods to find a possible closed form:\n$$\\int_0^\\infty\\frac{\\ln x}{\\sqrt{x\\vphantom{1}}\\ \\sqrt{x+1}\\ \\sqrt{2x+1}}dx\\stackrel{\\color{#808080}?}=\\frac{\\pi^{3/2}\\,\\ln2}{2^{3/2}\\,\\Gamma^2\\left(\\tfrac34\\right)}.\\tag1$$\nThe equality holds numerically with a precision of at least $1000$ decimal digits. But so far I was not able to find a proof of it.\nBecause the integral can be represented as a derivative of a hypergeometic function with respect to its parameter, the conjecture can be rewritten as\n$$\\frac{d}{da}{_2F_1}\\left(a,\\ \\tfrac12;\\ 1;\\ \\tfrac12\\right)\\Bigg|_{a=\\frac12}\\stackrel{\\color{#808080}?}=\\frac{\\sqrt\\pi\\,\\ln2}{2\\,\\Gamma^2\\left(\\tfrac34\\right)}\\tag2$$\nor, using a series expansion of the hypergeometric function, as\n$${\\large\\sum}_{n=0}^\\infty\\frac{H_{n-\\frac12}\\ \\Gamma^2\\left(n+\\tfrac12\\right)}{2^n\\ \\Gamma^2\\left(n+1\\right)}\\stackrel{\\color{#808080}?}=-\\frac{3\\,\\pi^{3/2}\\,\\ln2}{2\\,\\Gamma^2\\left(\\tfrac34\\right)}\\tag3,$$\nwhere $H_q$ is the generalized harmonic number, $H_q=\\gamma+\\psi_0\\left(q+1\\right).$\nCould you suggest any ideas how to prove this?", "output": "$$I:=\\int_{0}^{\\infty}\\frac{\\ln{(x)}}{\\sqrt{x}\\,\\sqrt{x+1}\\,\\sqrt{2x+1}}\\mathrm{d}x.$$\n\nAfter first multiplying and dividing the integrand by 2, substitute $x=\\frac{t}{2}$:\n$$I=\\int_{0}^{\\infty}\\frac{2\\ln{(x)}}{\\sqrt{2x}\\,\\sqrt{2x+2}\\,\\sqrt{2x+1}}\\mathrm{d}x=\\int_{0}^{\\infty}\\frac{\\ln{\\left(\\frac{t}{2}\\right)}}{\\sqrt{t}\\,\\sqrt{t+2}\\,\\sqrt{t+1}}\\mathrm{d}t.$$\nNext, substituting $t=\\frac{1}{u}$ yields:\n$$\\begin{align}\nI\n&=-\\int_{0}^{\\infty}\\frac{\\ln{(2u)}}{\\sqrt{u}\\sqrt{u+1}\\sqrt{2u+1}}\\mathrm{d}u\\\\\n&=-\\int_{0}^{\\infty}\\frac{\\ln{(2)}}{\\sqrt{u}\\sqrt{u+1}\\sqrt{2u+1}}\\mathrm{d}u-\\int_{0}^{\\infty}\\frac{\\ln{(u)}}{\\sqrt{u}\\sqrt{u+1}\\sqrt{2u+1}}\\mathrm{d}u\\\\\n&=-\\int_{0}^{\\infty}\\frac{\\ln{(2)}}{\\sqrt{u}\\sqrt{u+1}\\sqrt{2u+1}}\\mathrm{d}u-I\\\\\n\\implies I&=-\\frac{\\ln{(2)}}{2}\\int_{0}^{\\infty}\\frac{\\mathrm{d}x}{\\sqrt{x}\\sqrt{x+1}\\sqrt{2x+1}}.\n\\end{align}$$\nMaking the sequence of substitutions $x=\\frac{u-1}{2}$, then $u=\\frac{1}{t}$, and finally $t=\\sqrt{w}$, puts this integral into the form of a beta function:\n$$\\begin{align}\n\\int_{0}^{\\infty}\\frac{\\mathrm{d}x}{\\sqrt{x}\\sqrt{x+1}\\sqrt{2x+1}}\n&=\\int_{1}^{\\infty}\\frac{\\mathrm{d}u}{\\sqrt{u-1}\\sqrt{u+1}\\sqrt{u}}\\\\\n&=\\int_{1}^{\\infty}\\frac{\\mathrm{d}u}{\\sqrt{u^2-1}\\sqrt{u}}\\\\\n&=\\int_{1}^{0}\\frac{t^{3/2}}{\\sqrt{1-t^2}}\\frac{(-1)}{t^2}\\mathrm{d}t\\\\\n&=\\int_{0}^{1}\\frac{\\mathrm{d}t}{\\sqrt{t}\\,\\sqrt{1-t^2}}\\\\\n&=\\frac12\\int_{0}^{1}\\frac{\\mathrm{d}w}{w^{3/4}\\,\\sqrt{1-w}}\\\\\n&=\\frac12\\operatorname{B}{\\left(\\frac14,\\frac12\\right)}\\\\\n&=\\frac12\\frac{\\Gamma{\\left(\\frac12\\right)}\\Gamma{\\left(\\frac14\\right)}}{\\Gamma{\\left(\\frac34\\right)}}\\\\\n&=\\frac{\\pi^{3/2}}{2^{1/2}\\Gamma^2{\\left(\\frac34\\right)}}\n\\end{align}$$\nHence,\n$$I=-\\frac{\\ln{(2)}}{2}\\frac{\\pi^{3/2}}{2^{1/2}\\Gamma^2{\\left(\\frac34\\right)}}=-\\frac{\\pi^{3/2}\\,\\ln{(2)}}{2^{3/2}\\,\\Gamma^2{\\left(\\frac34\\right)}}.~~~\\blacksquare$$\n\nPossible Alternative: You could also derive the answer from the complete elliptic integral of the first kind instead of from the beta function by making the substitution $t=z^2$ instead of $t=\\sqrt{w}$.\n$$\\begin{align}\n\\int_{0}^{\\infty}\\frac{\\mathrm{d}x}{\\sqrt{x}\\sqrt{x+1}\\sqrt{2x+1}}\n&=\\int_{1}^{\\infty}\\frac{\\mathrm{d}u}{\\sqrt{u-1}\\sqrt{u+1}\\sqrt{u}}\\\\\n&=\\int_{1}^{\\infty}\\frac{\\mathrm{d}u}{\\sqrt{u^2-1}\\sqrt{u}}\\\\\n&=\\int_{1}^{0}\\frac{t^{3/2}}{\\sqrt{1-t^2}}\\frac{(-1)}{t^2}\\mathrm{d}t\\\\\n&=\\int_{0}^{1}\\frac{\\mathrm{d}t}{\\sqrt{t}\\,\\sqrt{1-t^2}}\\\\\n&=2\\int_{0}^{1}\\frac{\\mathrm{d}z}{\\sqrt{1-z^4}}\\\\\n&=2\\,K{(-1)}\\\\\n&=\\frac{\\Gamma^2{\\left(\\frac14\\right)}}{2\\sqrt{2\\pi}}\\\\\n&=\\frac{\\pi^{3/2}}{2^{1/2}\\Gamma^2{\\left(\\frac34\\right)}}.\n\\end{align}$$", "meta": {"post_id": 877460, "input_score": 40, "output_score": 50, "post_title": "Prove ${\\large\\int}_0^\\infty\\frac{\\ln x}{\\sqrt{x}\\ \\sqrt{x+1}\\ \\sqrt{2x+1}}dx\\stackrel?=\\frac{\\pi^{3/2}\\,\\ln2}{2^{3/2}\\Gamma^2\\left(\\tfrac34\\right)}$"}}
{"input": "I discovered the following conjectured identity numerically (it holds with at least $1000$ digits of precision). How can I prove it?\n$${\\large\\int}_0^\\infty\\left({_2F_1}\\left(\\frac16,\\frac12;\\frac13;-x\\right)\\right)^{12}dx\\stackrel{\\color{#808080}?}=\\frac{80663}{153090}$$\n\nUpdate: It looks like this hypergeometric function assumes algebraic values at algebraic points (it's only a guess because I have only approximations to those algebraic numbers). Looking at those values, I was able to further conjecture that the hypergeometric function for $x<0$ is actually the following elementary function:\n$$\n{_2F_1}\\left(\\frac16,\\frac12;\\frac13;x\\right)\\stackrel{\\color{#808080}?}=\n\\\\\n\\frac1{\\sqrt[4]2\\sqrt3}\\cdot\\sqrt{\\frac{\\alpha}{1-x}+\\frac{1}{\\alpha}\n\\sqrt{\\frac{4\\left(\\alpha\\sqrt{2}+2\\right)+x\\left(\\sqrt[3]{4\\beta}-2\\left(\\alpha\\sqrt{2}+4\\right)\\right)+2\\sqrt[3]{2\\beta^2}}{1-x}}}~,\n$$\nwhere\n$$\\alpha=\\sqrt{2-2x+\\sqrt[3]{2\\beta^2}}~,\\qquad\\beta=x(x-1)~.$$", "output": "Consider the hypergeometric equation with parameters $(a,b,c)=\\left(\\frac16,\\frac12,\\frac13\\right)$, and build from its two canonical solutions near $z=0$ the vector\n$$\\vec{y}(z)=\\left(\\begin{array}{c}\ny_1 \\\\ y_2\n\\end{array}\\right)=\\left(\\begin{array}{c}\n_2F_1(a,b;c;z) \\\\ z^{1-c}{}_2F_1(a-c+1,b-c+1;2-c;z)\n\\end{array}\\right).\\tag{1}$$\nThis is a single-valued vector function on $\\mathbb{C}\\backslash\\{(-\\infty,0]\\cup[1,\\infty)\\}$. Its analytic continuation along a closed loop $\\gamma$ gives rise to monodromy representation of $\\pi_1(\\mathbb{C}\\backslash\\{0,1\\})$:\n$$ \\gamma\\mapsto M_{[\\gamma]},\\qquad y(\\gamma z)=M_{[\\gamma]}y(z).$$\nThe monodromy group $G\\subset GL(2,\\mathbb{C})$ of the hypergeometric equation is generated by two matrices corresponding to simple loops around $0$ and $1$. In the case we are interested in these matrices are explicitly given by\n$$M_0=\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & e^{-2\\pi i /3}\\end{array}\\right),\\qquad\nM_1=C\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & e^{2\\pi i /3}\\end{array}\\right)C^{-1},\\tag{2}$$\nwhere the connection matrix $C=\\left(\\begin{array}{cc} 1 & 2^{\\frac43} \\\\ -2^{\\frac83} & 8\\end{array}\\right)$. If $G$ is finite, then $\\vec{y}(z)$ has a finite number of branches, and moreover (Schwarz 1872), is algebraic. \nIt is not difficult to check that the monodromy group $G$ generated by $M_0$, $M_1$ from (2) is indeed finite. In particular, note that\n$$M_0^3=M_1^3=I,\\qquad M_1^2=-M_0M_1M_0, $$ $$M_1M_0M_1=-M_0^2,\\qquad M_1M_0^2M_1=M_0^2M_1M_0^2.$$ \nIt turns out that $G$ has order $24$ and is isomorphic to the binary tetrahedral group:\n$$G\\cong 2T=\\langle s,t\\,|\\,(st)^2=s^3=t^3\\rangle, $$\nwhere the generators can be identified as $s=M_0M_1M_0M_1M_0$, $t=M_1M_0M_1M_0M_1$.\nCorollary: The hypergeometric functions in (1) are algebraic.\n\nAlgebraic solutions of the hypergeometric equations are classified by the so-called Schwarz table, and have been studied by many mathematicians, see e.g. the bibliography in this paper by R.Vidunas. Their explicit construction is somewhat involved but relatively straightforward - at least when the corresponding algebraic curve has genus $0$ (the genus can be determined independently from the Riemann-Hurwitz formula). \nIn our case the task simplifies even more as our parameter values can be obtained from the genus $0$ tetrahedral formula (2.4) of the above mentioned paper by a combination of a linear trasformation (sending $\\frac56$ to $\\frac43-\\frac56=\\frac12$) and differentiation (transforming $\\frac43$ into $\\frac13$). The result is\n$$_2F_1\\left(\\frac16,\\frac12;\\frac13;-\\frac{r(r+2)^3}{(r+1)(1-r)^3}\\right)=\\frac{\\sqrt{1-r^2}}{2r+1}.$$\nCorollary: The antiderivative $\\displaystyle\\int \\mathcal{R}\\left(x,y(x)\\right)dx$, where $y(x)={}_2F_1\\left(\\frac16,\\frac12;\\frac13;-x\\right)$ and $\\mathcal{R}(x,y)$ is rational in both arguments, can be expressed in terms of elementary functions.\n\nExample:\nThe transformation $r\\mapsto x(r)=\\frac{r(r+2)^3}{(r+1)(1-r)^3}$ bijectively maps $(0,1)$ to $(0,\\infty)$, and therefore the initial integral becomes\n\\begin{align}\\mathcal{I}&=\\int_0^1 \\left(\\frac{\\sqrt{1-r^2}}{2r+1}\\right)^{12}\\left(\\frac{r(r+2)^3}{(r+1)(1-r)^3}\\right)'dr=\\\\&=\n2\\int_0^1 \\frac{(1+r)^4(1-r)^2(r+2)^2}{(2r+1)^{10}} dr=\\\\&=\\frac{80\\,663}{153\\,090}.\n\\end{align}", "meta": {"post_id": 878406, "input_score": 27, "output_score": 39, "post_title": "Prove ${\\large\\int}_0^\\infty\\left({_2F_1}\\left(\\frac16,\\frac12;\\frac13;-x\\right)\\right)^{12}dx\\stackrel{\\color{#808080}?}=\\frac{80663}{153090}$"}}
{"input": "Denote by $\\Sigma_d(t)$ the sum of digits in the decimal representation of the number $t$.\nProve / disprove:\n$$\\forall n\\in \\mathbb N:\\ \\ \\Sigma_d (n!) | n!$$", "output": "It's not true.  The first counterexample is for $ n = 432 $.  The sum of the digits in $ 432! $ is 3897, which you can see using Wolfram Alpha.  But the prime factorisation of 3897  is $ 3^2 \\times 433 $, so $ 432! $ cannot be divisible by its sum of digits.\nThe list of counterexamples is sequence A066419 in the OEIS.", "meta": {"post_id": 879445, "input_score": 23, "output_score": 43, "post_title": "Is every factorial divisible by its sum of digits?"}}
{"input": "Applying the Copson's inequality, I found:\n$$S=\\displaystyle\\sum_{k=1}^{\\infty }\\left(\\Psi^{(1)}(k)\\right)^2\\lt\\dfrac{2}{3}\\pi^2$$ where\n$\\Psi^{(1)}(k)$ is the polygamma function.\nIs it known any sharper bound for the sum $S$?\nThanks.", "output": "First, we have\n$$\n\\begin{align}\n\\psi'(n)\n&=\\sum_{k=0}^\\infty\\frac1{(k+n)^2}\\\\\n&=\\sum_{k=n}^\\infty\\frac1{k^2}\\tag{1}\n\\end{align}\n$$\nThen\n$$\n\\begin{align}\n\\sum_{n=1}^\\infty\\psi'(n)^2\n&=\\sum_{n=1}^\\infty\\sum_{j=n}^\\infty\\frac1{j^2}\\sum_{k=n}^\\infty\\frac1{k^2}\\tag{2}\\\\\n&=\\sum_{n=1}^\\infty\\left(\\sum_{j=n}^\\infty\\frac1{j^4}+2\\sum_{j=n}^\\infty\\sum_{m=1}^\\infty\\frac1{j^2}\\frac1{(j+m)^2}\\right)\\tag{3}\\\\\n&=\\sum_{j=1}^\\infty\\sum_{n=1}^j\\frac1{j^4}+2\\sum_{j=1}^\\infty\\sum_{m=1}^\\infty\\sum_{n=1}^j\\frac1{j^2}\\frac1{(j+m)^2}\\tag{4}\\\\\n&=\\sum_{j=1}^\\infty\\frac1{j^3}+2\\sum_{j=1}^\\infty\\sum_{m=1}^\\infty\\frac1{j(j+m)^2}\\tag{5}\\\\\n&=\\zeta(3)+2\\sum_{n=1}^\\infty\\frac{H_{n-1}}{n^2}\\tag{6}\\\\\n&=\\zeta(3)-2\\zeta(3)+2\\sum_{n=1}^\\infty\\frac{H_n}{n^2}\\tag{7}\\\\[9pt]\n&=\\zeta(3)-2\\zeta(3)+4\\zeta(3)\\tag{8}\\\\[18pt]\n&=3\\zeta(3)\\tag{9}\n\\end{align}\n$$\nExplanation:\n$(2)$: use $(1)$\n$(3)$: first sum covers $j=k$ the other $j\\lt k$ and $j\\gt k$\n$(4)$: change order of summation\n$(5)$: sum in $n$\n$(6)$: $n=j+m$ and $j=n-m$ and sum in $m$\n$(7)$: $H_{n-1}=H_n-\\frac1n$\n$(8)$: equation $(14)$ of this answer with $q=2$\n$(9)$: add", "meta": {"post_id": 882621, "input_score": 29, "output_score": 34, "post_title": "Polygamma function series: $\\sum_{k=1}^{\\infty }\\left(\\Psi^{(1)}(k)\\right)^2$"}}
{"input": "Does anyone know the exact proof of this limit result?\n$$\\lim_{n\\to\\infty} \\left(1+\\frac{x}{n}\\right)^n = e^x$$", "output": "$$e^{\\ln{(1 + \\frac{x}{n})^n} }=e^{n \\ln(1+\\frac{x}{n})}$$\n$$\\lim_{n \\to +\\infty} (1 + \\frac{x}{n})^n\n=\\lim_{n \\to +\\infty} e^{n \\ln(1+\\frac{x}{n})} \\\\ \n=e^{\\lim_{n \\to +\\infty} n \\ln(1+\\frac{x}{n})}\n=e^{\\lim_{n \\to +\\infty}\\frac{ \\ln(1+\\frac{x}{n})}{\\frac{1}{n}}}$$ \nApply L'Hopital's Rule:\n$$=e^{\\lim_{n \\to +\\infty}\\frac{(\\frac{-x}{n^2})\\frac{1}{1+\\frac{x}{n}}}{-\\frac{1}{n^2}}}\n=e^{\\lim_{n \\to +\\infty}\\frac{x}{1+\\frac{x}{n}}}\n=e^x$$\nTherefore, $$(1+\\frac{x}{n})^n \\to e^x$$", "meta": {"post_id": 882741, "input_score": 49, "output_score": 52, "post_title": "Limit of $(1+ x/n)^n$ when $n$ tends to infinity"}}
{"input": "I discovered the following conjectured identity numerically while studying a family of related integrals.\nLet's set\n$$\nR^{+}:=  \\frac{2}{\\pi}\\int_{0}^{\\pi/2}\\sqrt[\\normalsize{8}]{x^2 + \\ln^2\\!\\cos x}\n\\sqrt{ \\frac{1}{2}+\\frac{1}{2}\\sqrt{\n\\frac{1}{2}+ \\frac{1}{2}  \\sqrt{ \\frac{\\ln^{2}\\!\\cos x}{\nx^2 + \\ln^2\\! \\cos x}}}}\\,\\mathrm{d}x, \\tag1\n$$\n$$\nR^{-}:=  \\frac{2}{\\pi}\\int_{0}^{\\pi/2}\\frac{1}{\\sqrt[\\normalsize{8}]{x^2 + \\ln^2\\!\\cos x}}\n\\sqrt{ \\frac{1}{2}+\\frac{1}{2}\\sqrt{\n\\frac{1}{2}+ \\frac{1}{2} \\sqrt{ \\frac{\\ln^{2}\\!\\cos x}{\nx^2 + \\ln^2\\! \\cos x}}}}\\,\\mathrm{d}x. \\tag2\n$$\nWe may numerically observe with at least 500 digits of precision that\n$$ \n\\begin{align}\n& R^{+}R^{-} \\stackrel{?}{=}1 \\tag 3 \\\\\\\\\n& R^{+} \\stackrel{?}{=} \\sqrt[\\normalsize{4}]{\\ln 2} \\tag4 \\\\\\\\\n& R^{-} \\stackrel{?}{=}\\frac{1}{\\sqrt[\\normalsize{4}]{\\ln 2}}. \\tag5\n\\end{align}\n$$\nHow can we prove it?\nA version of this has been sent to Eric Weisstein, these integrals are on Mathworld as Ramanujan log-trigonometric integrals.", "output": "Here is an approach.\nTheorem. \nLet $s$ be a real number such that $-1<s<1$. Then\n\n\\begin{equation}{\\Large\\int_{0}^{\\!\\Large \\frac{\\pi}{2}}} \\frac{\\cos \\left(\\! s \\arctan \\left(-\\frac{x}{\\ln \\cos x}\\right)\\right)}{(x^2+\\ln^2\\! \\cos x)^{\\Large\\frac{s}{2}}}\\, \\mathrm{d}x = \\frac{\\pi}{2}\\frac{1}{\\ln^{\\Large s}\\!2} \\tag1\\end{equation}\n\nProof. First assume  that $0<s<1.$ Then we may write\n$$\n\\begin{align}\n\\int_{0}^{\\!\\Large \\frac{\\pi}{2}} \\frac{\\cos \\left(\\! s \\arctan \\left(-\\frac{x}{\\ln \\cos x}\\right)\\right)}{(x^2+\\ln^2\\! \\cos x)^{s/2}} \\mathrm{d}x\n& = \\frac{1}{\\Gamma(s)}\\int_{0}^{\\!\\Large \\frac{\\pi}{2}}\\!\\!\\int_{0}^{+\\infty} u^{s-1}  \\cos (u x) \\:e^{u \\ln \\cos x}\\:\\mathrm{d}u \\:\\mathrm{d}x \\tag2\\\\\\\\\n& = \\frac{1}{\\Gamma(s)}\\int_{0}^{+\\infty}u^{s-1}\\!\\!\\int_{0}^{\\!\\Large  \\frac{\\pi}{2}}  \\cos^u\\! x \\cos (u x)\\:\\mathrm{d}x \\:\\mathrm{d}u \\tag3\\\\\\\\\n& = \\frac{1}{\\Gamma(s)}\\int_{0}^{+\\infty}u^{s-1} \\frac{\\pi}{2^{u+1}}\\:\\mathrm{d}u \\tag4\\\\\\\\\n& = \\frac{\\pi}{2} \\frac{1}{\\Gamma(s)}\\int_{0}^{+\\infty}u^{s-1} e^{-u\\ln 2}\\:\\mathrm{d}u \\tag5\\\\\\\\\n& = \\frac{\\pi}{2}\\frac{1}{\\Gamma(s)}\\frac{\\Gamma(s)}{\\ln^{s}\\!2} \\tag6\\\\\\\\\n& = \\frac{\\pi}{2}\\frac{1}{\\ln^{s}\\!2} \\tag7\n\\end{align}\n$$\nwhere we have used Fubini's theorem and the classic results (here and there)\n$$\n\\begin{align}\n& \\int_{0}^{+\\infty} u^{s-1}  \\cos (a u) \\:e^{-b u}\\:\\mathrm{d}u = \\Gamma (s)\\frac{\\cos \\left(\\! s \\arctan \\left(\\frac{a}{b}\\right)\\right)}{(a^2+b^2)^{s/2}}, \\, \\left(\\Re(s)>0, b>0, a>0 \\right) \\tag8 \\\\\n& \\int_{0}^{\\!\\Large  \\frac{\\pi}{2}}  \\cos^u\\! x \\cos (u x)\\:\\mathrm{d}x = \\frac{\\pi}{2^{u+1}}, \\quad u>-1. \\tag9\n\\end{align}\n$$\nWe may extend identity $(7)$ by analytic continuation to obtain $(1)$.\nExample 1. We have\n\n$$\n\\int_{0}^{\\pi/2}\\displaystyle\n\\sqrt{\\sqrt{x^2 + \\ln^2\\!\\cos x}-\\ln\\! \\cos x}\\,\\,\\mathrm{d}x = \\frac{\\pi}{2}\\sqrt{2\\ln 2} \\tag{10}\n$$\n\nand\n\n$$\n\\int_{0}^{\\pi/2}\\displaystyle\n\\frac{1}{\\sqrt{x^2 + \\ln^2\\!\\cos x}} \\sqrt{\\sqrt{x^2 + \\ln^2\\!\\cos x}-\\ln\\! \\cos x}\\,\\,\\mathrm{d}x = \\frac{\\pi}{\\sqrt{2\\ln 2}} \\tag{11}\n$$\n\nProof. Let $0<x<\\frac{\\pi}{2}$ and set $t:=\\arctan \\left(-\\frac{x}{\\ln \\cos x}\\right)$.  Observe that $0 < t < \\frac{\\pi}{2}$ and\n$$\n\\cos t=\\cos \\left(\\!\\arctan \\left(-\\frac{x}{\\ln \\cos x}\\right)\\right)= -\\frac{\\ln \\cos x}{\\sqrt{x^2 + \\ln^2\\!\\cos x}},\n$$\n$$\n\\cos \\left(\\frac{t}{2}\\right) = \\sqrt{\n\\frac{1}{2}+ \\frac{1}{2} \\cos t}\n$$\n then put successively $\\displaystyle s=-\\frac{1}{2}$, $\\displaystyle s=\\frac{1}{2}$ in $(1)$ to obtain $(10)$ and $(11)$.\nExample 2.\n\n$$\n\\frac{2}{\\pi}\\int_{0}^{\\pi/2}\\sqrt[\\normalsize{8}]{x^2 + \\ln^2\\!\\cos x}\n\\sqrt{ \\frac{1}{2}+\\frac{1}{2}\\sqrt{\n\\frac{1}{2}+ \\frac{1}{2}  \\sqrt{ \\frac{\\ln^{2}\\!\\cos x}{\nx^2 + \\ln^2\\! \\cos x}}}}\\,\\mathrm{d}x = \\sqrt[\\normalsize{4}]{\\ln 2} \n\\tag{12} $$\n\nand\n\n$$\n\\frac{2}{\\pi}\\int_{0}^{\\pi/2}\\frac{1}{\\sqrt[\\normalsize{8}]{x^2 + \\ln^2\\!\\cos x}}\n\\sqrt{ \\frac{1}{2}+\\frac{1}{2}\\sqrt{\n\\frac{1}{2}+ \\frac{1}{2} \\sqrt{ \\frac{\\ln^{2}\\!\\cos x}{\nx^2 + \\ln^2\\! \\cos x}}}}\\,\\mathrm{d}x=\\frac{1}{\\sqrt[\\normalsize{4}]{\\ln 2}}\n\\tag{13} $$\n\nProof. Let $0<x<\\frac{\\pi}{2}$ and set $t:=\\arctan \\left(-\\frac{x}{\\ln \\cos x}\\right)$.  Observe that $0 < t < \\frac{\\pi}{2}$ and\n$$\n\\cos t=\\cos \\left(\\!\\arctan \\left(-\\frac{x}{\\ln \\cos x}\\right)\\right)=\\sqrt{ \\frac{\\ln^{2}\\!\\cos x}{x^2 + \\ln^2\\! \\cos x}},\n$$\n$$\n\\cos \\left(\\frac{t}{4}\\right) = \\sqrt{\\frac{1}{2}+\\frac{1}{2}\\sqrt{\n\\frac{1}{2}+ \\frac{1}{2} \\cos t}},\n$$  then put successively $\\displaystyle s=-\\frac{1}{4}$, $\\displaystyle s=\\frac{1}{4}$ in $(1)$ to obtain $(12)$ and $(13)$.\nExample n. \nSet\n\n$$\nR_n^{+}:=\\frac{2}{\\pi}\\int_{0}^{\\pi/2}\\sqrt[\\Large {2^n}]{x^2 + \\ln^2\\!\\cos x}\n\\sqrt{\\frac{1}{2}+\\frac{1}{2}\\sqrt{\\frac{1}{2}+\\cdots+\\frac{1}{2}\\sqrt{\n\\frac{1}{2}+ \\frac{1}{2}\\sqrt{ \\frac{\\ln^{2}\\!\\cos x}{\nx^2 + \\ln^2\\! \\cos x}}}}}\\,\\mathrm{d}x \n$$\n\nand\n\n$$\nR_n^{-}:=\\frac{2}{\\pi}\\int_{0}^{\\pi/2}\\frac{1}{\\sqrt[\\Large {2^n}]{x^2 + \\ln^2\\!\\cos x}}\n\\sqrt{\\frac{1}{2}+\\frac{1}{2}\\sqrt{\\frac{1}{2}+\\cdots+\\frac{1}{2}\\sqrt{\n\\frac{1}{2}+ \\frac{1}{2}\\sqrt{ \\frac{\\ln^{2}\\!\\cos x}{\nx^2 + \\ln^2\\! \\cos x}}}}}\\,\\mathrm{d}x.  \n$$\n\nThen\n\n$$\nR_n^{+}= \\sqrt[\\Large {2^n}]{\\ln 2} \n\\tag{14}\n$$\n\nand\n\n$$\nR_n^{-}= \\frac{1}{\\sqrt[\\Large {2^n}]{\\ln 2}}\n\\tag{15}.\n$$\n\nProof. Let $0<x<\\frac{\\pi}{2}$ and set $t:=\\arctan \\left(-\\frac{x}{\\ln \\cos x}\\right)$.  Observe that $0 < t < \\frac{\\pi}{2}$ and\n$$\n\\cos t=\\sqrt{ \\frac{\\ln^{2}\\!\\cos x}{x^2 + \\ln^2\\! \\cos x}},\n$$\n$$\n\\cos \\left(\\frac{t}{2^n}\\right) = \\sqrt{\\frac{1}{2}+\\frac{1}{2}\\sqrt{\\frac{1}{2}+\\frac{1}{2}\\sqrt{\\frac{1}{2}+\\cdots+\\frac{1}{2}\\sqrt{\n\\frac{1}{2}+ \\frac{1}{2} \\cos t}}}},\n$$  then put successively $\\displaystyle s=-\\frac{1}{2^n}$, $\\displaystyle s=\\frac{1}{2^n}$, $n\\geq 1,$ in $(1)$ to obtain $(14)$ and $(15)$.", "meta": {"post_id": 884021, "input_score": 83, "output_score": 43, "post_title": "Ramanujan log-trigonometric integrals"}}
{"input": "I recently began experimenting with gnuplot and I quickly made an interesting discovery. I plotted all of the prime numbers beneath 1 million in polar coordinates such that for every prime $p$, $(r,\\theta) = (p,p)$. I was not expecting anything in particular, I was simply trying it out. The results are fascinating.\nWhen looking at the primes beneath 30000, a spiral pattern can be seen:\n\nFor comparison, here is the same graph with the multiples of 3 and 7 superimposed on it. Primes are in yellow, multiples of 3 and 7 in green and red respectively.\n\nWhat is really interesting to me, though, is the behavior when the range is increased. Multiples of a given number appear to spiral out in the same pattern into infinity, but the primes begin to form rays in groups of 3 or 4. See below:\n\n\nCompared to multiples of 3 and 7 again:\n\nNow, I must admit that I am very much a novice mathematician with little experience beyond trigonometry. I am just going into Calculus and Discrete Mathematics this upcoming fall.\nI know that there is something called the Prime Number theorem - are these patterns related to it? Are these rays the same phenomenon as the diagonal lines found in Ulam Spirals?\nEDIT:\nIn response to Greg Martin's explanation, I decided to add a couple more graphs. To see why they are relevant, read his answer.\n$(r,\\theta)=(n,n), n \\in  \\mathbb{N}$", "output": "$3$Blue$1$Brown has given a beautiful explanation to the spiral and ray patterns. Please watch his video titled \"Why do prime numbers make these spirals?\"", "meta": {"post_id": 885879, "input_score": 250, "output_score": 47, "post_title": "Meaning of Rays in Polar Plot of Prime Numbers"}}
{"input": "I did some numerical approximation of $$\\sum_{n=-\\infty}^\\infty \\exp(-(x+n)^2)$$ and found that this function is \"almost\" constant ($\\approx 1.772$). Why does the sum fluctuate little? Is there a closed form for this sum?\nAdded: since $f(x) = \\sum_{n=-\\infty}^\\infty \\exp(-(x+n)^2)$ has period $1$ and is even, can we give an upper bound of $\\sup\\{ f(x)/f(y) : (x,y)\\in [0,0.5]^2\\}$?", "output": "Recall the general case of the Poisson sum formula:\n$$\\sum_{-\\infty}^\\infty f(x+n)\n=\\sum_{k=-\\infty}^\\infty e^{2\\pi i k x} \n\\int_{-\\infty}^{\\infty} e^{-2\\pi i k y}f(y)\\,dy$$ \nThen \n$\\displaystyle\\int_{-\\infty}^{\\infty} e^{-2\\pi i k y}e^{-y^2}\\,dy$ is a Gaussian integral, and (skipping the tedious step of completing the square) evaluates to \n$\\sqrt{\\pi} e^{ -k^2 \\pi^2}$. So\n$$\\sum_{-\\infty}^\\infty e^{-(x+n)^2}\n= \\sqrt{\\pi}\\sum_{k=-\\infty}^\\infty e^{2\\pi i k x} e^{-k^2 \\pi^2}=\\sqrt{\\pi}+2\\sqrt{\\pi}\\sum_{k=1}^\\infty e^{-k^2 \\pi^2}\\cos 2\\pi kx $$\nObserve that this means that the function has an average value of $\\sqrt\\pi$, and that we have a tower of corrections which are each exponentially smaller than the last. To a very good approximation, then, we can take the function to be $$\\sqrt\\pi+2\\sqrt\\pi e^{-\\pi^2} \\cos 2\\pi x$$ with a variation around the value $\\sqrt\\pi\\approx1.77245$ of merely $\\pm2\\sqrt\\pi e^{-\\pi^2}\\approx\\pm0.0002$. So the function is very flat.\nThis is also consistent with Dmoreno's observation that the sum is a Jacobi theta function, since the argument above amounts to a proof that $\\vartheta_3(x;\\tau)$ has its known Fourier expansion\n$$\\vartheta_3(z,q)=\\sum_{-\\infty}^\\infty q^{n^2}e^{2 i n z}=1+2\\sum_{n=1}^\\infty q^{n^2} \\cos 2n z$$\nfor the case $q=e^{-\\pi^2},$ $z=\\pi x$.\nAdded:\nWe can generalize the above calculation to obtain the sum \n$$\\sum\\limits_{-\\infty}^\\infty \\exp\\left[-\\left(\\dfrac{x+n}{a}\\right)^2\\right]\n=\\sqrt{\\pi}|a|\\vartheta_3(\\pi x,e^{-\\pi^2 a^2})=\\sqrt{\\pi}|a|\\left[1+2\\sum_{k=1}^\\infty e^{-\\pi^2 a^2 k^2} \\cos(2\\pi k x)\\right]$$\nObserve that if we pick $a$ to be small, then $e^{-a^2 \\pi^2}$ (formally the elliptic nome $q$) need not be small. In that case the approximation given earlier breaks down. The cases $a=\\pi^{-1}$ and $a=(2\\pi)^{-1}$ give striking examples, as evident in these WolframAlpha plots [1] [2]: the sums definitely aren't flat!\nTo explain this in less formal terms, note that the sum consists of an infinite set of shifted Gaussians $\\{e^{-(x+n)/a^2}\\}$. If $a$ is small, then each Gaussian is narrowly peaked and does not overlap much with its neighbors; consequently, the sum of all of them together leads to a 'comb' of narrow peaks. But if $a$ is not small--and $a=1$ is not--then each Gaussian overlaps strongly with its neighbors, and so the resulting 'comb' due to the sum has its gaps mostly filled in (i.e. nearly flat.)", "meta": {"post_id": 891974, "input_score": 15, "output_score": 34, "post_title": "Why is $\\sum_{n=-\\infty}^{\\infty}\\exp(-(x+n)^2)$ \"almost\" constant?"}}
{"input": "Let $A$ be a square and singular matrix of order $n$.\nIs $\\operatorname{adj}(A)$ necessarily singular? What would be the rank of $\\operatorname{adj}(A)$?", "output": "No, the adjugate of a singular matrix can be non-singular. But it happens only for the $1\\times 1$ zero matrix.\nHere is a complete classification, referring to this answer and this one. One always has $$\\def\\adj{\\operatorname{adj}}A \\cdot \\adj(A) = \\det(A) I_n.$$\n\nIf $A$ has rank$~n$, then it is invertible, and so is $\\det(A)$, and $\\adj(A)=\\det(A)A^{-1}$ is invertible too, and has rank$~n$.\nIf $A$ has rank$~n-1$ then at least one $(n-1)\\times(n-1)$ minor is nonzero, and so $\\adj(A)\\neq0$. On the other hand by the given relation the image of $\\adj(A)$ is contained in the kernel of $A$ which has dimension$~1$ by rank-nullity; it follows that $\\adj(A)$ has rank$~1$ in this case.\nIf $A$ has rank${}<n-1$ then all $(n-1)\\times(n-1)$ minors are equal to zero, and so $\\adj(A)$ has rank$~0$.\n\nThe cases where $\\adj(A)$ has rank$~n$ are the first case for any$~n$, and the second case for $n=1$. (And, I would be inclined to say, the last case for $n=0$; but that of course cannot happen at all.) So the only case where $A$ is singular but $\\adj(A)$ is not, is the case $A=(0)$ (with $n=1$).", "meta": {"post_id": 892039, "input_score": 14, "output_score": 36, "post_title": "Is adjoint of singular matrix singular? What would be its rank?"}}
{"input": "Let $A$ be a positive-definite real matrix in the sense that $x^T A x > 0$ for every nonzero real vector $x$. I don't require $A$ to be symmetric.\nDoes it follow that $\\mathrm{det}(A) > 0$?", "output": "Here is en eigenvalue-less proof that if $x^T A x > 0$ for each nonzero real vector $x$, then $\\det A > 0$.\nConsider the function $f(t) = \\det \\left(t \\cdot I + (1-t) \\cdot A\\right)$ defined on the segment $[0, 1]$. Clearly, $f(0) = \\det A$ and $f(1) = 1$. Note that $f$ is continuous. If we manage to prove that $f(t) \\neq 0$ for every $t \\in [0, 1]$, then it will imply that $f(0)$ and $f(1)$ have the same sign (by the intermediate value theorem), and the proof will be complete.\nSo, it remains to show that $f(t) \\neq 0$ whenever $t \\in [0, 1]$. But this is easy. If $t \\in [0, 1]$ and $x$ is a nonzero real vector, then\n$$\nx^T (tI + (1-t)A) x = t \\cdot x^T x + (1-t) \\cdot x^T A x > 0,\n$$\nwhich implies that $tI + (1-t)A$ is not singular, which means that its determinant is nonzero, hence $f(t) \\neq 0$. Done.\nPS: The proof is essentially topological. We have shown that there is a path from $A$ to $I$ in the space of all invertible matrices, which implies that $\\det A$ and $\\det I$ can be connected by a path in $\\mathbb{R} \\setminus 0$, which means that $\\det A > 0$. One could use the same techniqe to prove other similar facts. For instance, this comes to mind: if $S^2 = \\{(x, y, z) \\mid x^2 + y^2 + z^2 = 1\\}$ is the unit sphere, and $f: S^2 \\to S^2$ is a continuous map such that $(v, f(v)) > 0$ for every $v \\in S^2$, then $f$ has degree $1$.", "meta": {"post_id": 892729, "input_score": 35, "output_score": 50, "post_title": "Does a positive definite matrix have positive determinant?"}}
{"input": "The question is quite simple actually. I am trying to understand the differences between the angle bracket $\\left<X,Y\\right>$ of two processes with jumps $X,Y$, and the sharp bracket of $[X,Y]$.\nI am aware that they are equivalent in the continuous case, but not for jump processes. The literature does not seem to be too explicit on this, so I was hoping that somebody can provide some additional information.\nThank you", "output": "Let $(X_t,\\mathcal{F}_t)_{t \\geq 0}$ be an (c\u00e0dl\u00e0g) $L^2$-martingale, i.e. a martingale which satisfies\n$$\\sup_{t < \\infty} \\mathbb{E}(X_t^2)<\\infty.$$\nThen it follows from the Doob-Meyer decomposition that there exists a unique increasing previsible process $(A_t)_{t \\geq 0}$ such that $A_0=0$ and\n$$(X_t^2-A_t,\\mathcal{F}_t)_{t \\geq 0} \\,\\, \\text{is a martingale}. \\tag{1}$$\nThe process $A_t:= \\langle X \\rangle_t$ is called angle bracket or previsible quadratic variation. If $(X_t)_t$ and $(Y_t)_t$ are two martingales, then the covariation is defined via polarization, i.e.\n$$\\langle X,Y \\rangle_t := \\frac{1}{4} \\big( \\langle X+Y \\rangle_t - \\langle X-Y \\rangle_t \\big).  \\tag{2}$$\nThis definition implies that $(X_t Y_t - \\langle X,Y \\rangle_t,\\mathcal{F}_t)_{t \\geq 0}$ is a martingale. In particular, the notion of predictable quadratic variation is restricted to martingales whereas the sharp bracket is defined for semimartingales: For a semimartingale $(X_t)_{t \\geq 0}$, we set\n$$[X]_t := X_t^2-X_0^2 - 2 \\int_0^t X_{s-} \\, dX_s. \\tag{3}$$\nThe covariation $[X,Y]_t$ is again defined via polarization. Another characterization of the sharp bracket is the following:\n$$[X,Y]_t = \\text{ucp}-\\lim_{k \\to \\infty} \\sum_{j=0}^{n-1} (X_{t_{j+1}^k \\wedge t}-X_{t_j^k \\wedge t}) \\cdot (Y_{t_{j+1}^k \\wedge t}-Y_{t_j^k \\wedge t})$$\nwhere the partitions $\\pi_k = \\{0=t_0^k < \\ldots < t_n^k<\\infty\\}$ satisfy $|\\pi_k| \\to 0$ (Here ucp denotes the uniform [with respect to $t$] limit in probability.) See e.g. Protter [2] for a proof.\nIf $(X_t)_{t \\geq 0}$ is a martingale, then\n$$X_t^2 - [X]_t = 2 \\int_0^t X_{s-} \\, dX_s$$\nis a martingale, i.e. $(1)$ is satisfied. It is important to note that this does not imply $[X]_t = \\langle X \\rangle_t$; in fact, the process $[X]_t$ is in general not previsible. One (important) exception are martingales with continuous sample paths. In fact, for continuous martingales $(X_t)_t$ it holds that $[X]=\\langle X \\rangle$. On the other hand, any $L^2$-martingale $(X_t)_{t \\geq 0}$ admits a decomposition of the form\n$$X_t = X_t^c+X_t^d$$\nwhere $(X_t^c)_{t \\geq 0}$ is a continuous martingale and $(X_t^d)_{t \\geq 0}$ a pure-jump martingale. One can show that\n$$[X]_t = [X^c]_t + [X^d]_t = \\langle X^c \\rangle_t+ \\sum_{s \\leq t} (\\Delta X_s)^2. \\tag{4}$$\nThere are several books which introduce both notions of quadratic variation, but there are only few containing a proof of the (very important) equality $(4)$. One of them is the monograph 1 by Jacod and Shiryaev.\nLet me finish this answer with some basic examples:\nExample 1: Brownian motion\nIf $(X_t)_{t \\geq 0}$ is a Brownian motion, then $(X_t)_{t \\geq 0}$ is a martingale and it is not difficult to see that $\\langle X \\rangle_t = t$. Since Brownian motion is continuous, we get $[X]_t = t$.\nExample 2: Subordinate Brownian motion\nLet $(B_t)_{t \\geq 0}$ be a Brownian motion and $f:[0,\\infty) \\to [0,\\infty)$ an increasing c\u00e0dl\u00e0g function. Then $X_t := B_{f(t)}$ is called a (particular case of a) subordinate Brownian motion. Using $(1)$ and the independence and stationarity of the increments, it is not difficult to see that $\\langle X \\rangle_t = f(t)$. A direct (lenghty) calculation reveals that $$[X]_t = f(t) - \\sum_{s \\leq t} \\Delta f(s) + \\sum_{s \\leq t} |\\Delta X_s|^2.$$ If we choose $f(t) = t$ we recover Example 1.\nExample 3: Poisson process\nLet $(X_t)_{t \\geq 0}$ be a compensated Poisson process with intensity $\\lambda>0$. Using the independence and stationarity of the increments, we see that $\\langle X \\rangle_t = \\lambda t$. In particular, $\\langle X \\rangle_t$ is deterministic. On the other hand, it follows from $(4)$ that $$[X]_t = \\sum_{s \\leq t} |\\Delta X_s|^2.$$ This is one of the easiest examples showing that both notions of quadratic variation are (in general) totally different.\nExample 4: Square integrable martingales\nLet $(M_t)_{t \\geq 0}$ be an $L^2$-martingale and $f$ a \"nice\" function such that the stochastic integral $X_t := \\int_0^t f(s) \\, dM_s$ is well-defined. Then $$\\langle X \\rangle_t = \\int_0^t f(s)^2 \\, d\\langle M \\rangle_s.$$ A similar result can be obtained for stochastic integrals with respect to Poisson random measures.\nLiterature:\n\n(1) Limit Theorems for Stochastic Processes - J. Jacod, A. Shiryaev [Angle Bracket, Sharp Bracket]. This book contains all mentioned results (and proofs thereof).\n(2) Stochastic Integration and Differential Equations - P. Protter [Sharp Bracket]\n(3) Continuous Martingales and Brownian Motion - D. Revuz, M. Yor [Angle Bracket]\n(4) Stochastic Differential Equations and Diffusion Processes - N. Ikeda, S.Watanabe [Angle Bracket]\n(5) Brownian Motion and Stochastic Calculus - I. Karatzas, S. Shreve [Angle Bracket]", "meta": {"post_id": 902886, "input_score": 28, "output_score": 66, "post_title": "Angle bracket and sharp bracket for discontinuous processes"}}
{"input": "The dual of a norm $\\|\\cdot \\|$ is defined as:\n$$\\|z\\|_* = \\sup \\{ z^Tx \\text{ } | \\text{ } \\|x\\| \\le 1\\}$$\nCould anybody give me an intuition of this concept? I know the definition, I am using it to solve problems, but in reality I still lack intuitive understanding of it.", "output": "Here's the way I like to think about it.  I'll start with the finite dimensional space $\\Bbb{R}^n$ because it looks like that's where you are, but I'll give an analogy for infinite dimensional spaces as well.\nThe quantity $z^Tx$ represents a linear functional on $\\Bbb{R}^n$, that is a linear function which eats a vector and spits out a real number:\n$$\nf_z(x):\\Bbb{R}^n\\rightarrow\\Bbb{R}\\quad \\text{such that }\\quad f_z(\\alpha x+\\beta y)=\\alpha f_z(x)+\\beta f_z(y)\\quad \\forall \\alpha,\\beta\\in\\Bbb{R},x,y\\in\\Bbb{R}^n\n$$\nBecause of the Riesz Representation Theorem, we know that any linear function $f:\\Bbb{R}^n\\rightarrow\\Bbb{R}$ will take the form $f=f_z$ for some $z\\in\\Bbb{R}^n$, i.e. $f(x) = z^Tx$.\nThe question is now this: given a linear function(al) $f_z(\\cdot)$, how \"big\" is it?  Well, to measure the size of vectors, we look at norms, so the idea is simple: how big is the number $f_z(x)=z^Tx$ relative to the size (norm) of $x$?  This is exactly the number \n$$\n\\frac{z^Tx}{\\|x\\|}\n$$  We then say that the norm of $z$ is the largest this quantity can possibly be: \n$$\n\\|z\\|_* = \\sup_{x\\neq 0} \\frac{z^Tx}{\\|x\\|}\n$$  In a way, this is a kind of \"stretch factor\", but the stretching is measured with respect to $\\|x\\|$, which is the way we're measuring the size of $x$.  With a simple one-line proof, you can show that my way of defining $\\|z\\|_*$ is the same as yours.\nThis idea extends to infinite dimensional normed spaces such as $L^p$ as well - every normed space has a \"dual\" space of (continuous/bounded) linear functionals, i.e. mappings which eat vectors (which might actually be functions) and spit out numbers.  Each of these functionals has an associated \"size\", and that size is given by the dual norm: \n$$\n\\|f\\|_* = \\sup_{x\\neq 0}\\frac{f(x)}{\\|x\\|}\n$$ \nTo really complete the picture - and to expand on a couple of comments - it helps to also think about the dual norm as a special case of an operator norm.  The idea behind a general operator norm is pretty much the same as what I described above, but for a more general linear operator $A:X\\rightarrow Y$ where $X$ and $Y$ are any normed linear spaces.  In the case of linear functionals, $X$ is a vector space like $\\Bbb{R}^n$ or $L^p$ etc, and $Y$ is simply the 'base field', $\\Bbb{R}$ (or more generally $\\Bbb{C}$).  The idea is that $A$ eats vectors and spits out other vectors, and to measure the \"size\" of $A$ we might look again at the ratio of the size of $Ax$ (measured with the $Y$ norm) to the size of $x$ (measured with the $X$ norm):\n$$\n\\frac{\\|Ax\\|_Y}{\\|x\\|_X}\n$$  The largest of these values over nonzero $x\\in X$ is a good value for the size of $A$, because it tells us a sort of worst-case stretch factor: \n$$\n\\|A\\|=\\sup_{x\\neq 0}\\frac{\\|Ax\\|_Y}{\\|x\\|_X}\n$$\nThis is very similar to the idea of a singular value - in fact, if we use the Euclidean norm $\\|\\cdot\\|_2$, the operator norm of a matrix is its largest singular value!", "meta": {"post_id": 903484, "input_score": 62, "output_score": 91, "post_title": "Dual norm intuition"}}
{"input": "What is the sum of the areas of the grey circles? I have not made any progress so far.", "output": "This is an alternate approach to derive the area of the circles\nusing Descartes four circle theorem.\nWOLOG, assume $R = 1$. Let us call\n\nthe outer circle (with radius $r_a = 1$) as $C_a$.\nthe inner green circle (with radius $r_b = \\frac12$) as $C_b$.\nthe $1^{st}$ gray circle (the largest one at bottom) as $C_0$.\nthe $2^{nd}$ gray circle (the one above $C_b$ and $C_0$) as $C_1$.\nin general, for any $k > 0$, we will call the gray circle above $C_b$ and $C_{k-1}$ as $C_k$.\nWe can reflect the figure in question vertically. For $k < 0$, we will let $C_k$ be the mirror image of the circle  $C_{-(1+k)}$.\n\nThe key is for any $k\\in\\mathbb{Z}$, the four circles $C_a, C_b, C_k$ and $C_{k\\pm 1}$ are kissing each other. Let $r_k$ be the radius of $C_k$. We can apply Descartes four circle theorem to obtain:\n$$\\left(\\frac{1}{r_{k\\pm 1}} + \\frac{1}{r_k} + \\frac{1}{r_b} - \\frac{1}{r_a} \\right)^2\n= 2 \\left(\\frac{1}{r_{k\\pm 1}^2} + \\frac{1}{r_k^2} + \\frac{1}{r_b^2} + \\frac{1}{r_a^2}\\right)\\tag{*1}$$\nSince $r_a = 1$ and $r_b = \\frac12$, this implies $\\displaystyle\\;\\frac{1}{r_{k \\pm 1}}\\;$ are the two roots of quadratic equation\n$$\\left(\\rho + \\frac{1}{r_k} + 2 - 1 \\right)^2\n= 2 \\left(\\rho^2 + \\frac{1}{r_k^2} + 2^2 + 1^2 \\right)\n$$\nand hence\n$$\\frac{1}{r_{k+1}} + \\frac{1}{r_{k-1}} = 2\\left(\\frac{1}{r_k} + 1\\right)\n\\quad\\iff\\quad\n\\frac{1}{r_{k+1}} + \\frac{1}{r_{k-1}} - \\frac{2}{r_k} = 2$$\nThe RHS is an inhomogeneous linear recurrence relation on $\\displaystyle\\;\\frac{1}{r_k}\\;$ \nwith constant term. Since the characteristic polynomial $(\\lambda-1)^2$ has a double root at $1$, we know its solution must have the form $\\displaystyle\\;\\frac{1}{r_k} = k^2 + \\lambda k + \\mu\\;$.  By symmetry, \n$$r_k = r_{-(1+k)}\\quad\\implies\\quad \\lambda = 1 \\quad\\implies\\quad\n\\frac{1}{r_k} = k(k+1) + \\mu$$\nTo fix $\\mu$, apply $(*1)$ to the case $C_a, C_b, C_0$ and $C_{-1}$, we get\n$$\\left(2\\mu + 1\\right)^2 = 2\\left(2\\mu^2 + 5\\right)\n\\quad\\implies\\quad \\mu = \\frac{9}{4}\n$$\nFrom this, we get \n$$r_k = \\frac{4}{4k^2 + 4k + 9}\n\\quad\\implies\\quad\n\\text{Area} = \\sum_{k=0}^\\infty \\frac{16\\pi}{(4k^2+4k+9)^2}$$\nreproducing what's in J.J's answer. \nUpdate\nTo evaluate the sum, we will use following infinite product expansion of $\\cosh(\\pi x)$,\n$$\\cosh\\pi x = \\prod_{k=0}^\\infty \\left( 1 + \\frac{x^2}{(k+\\frac12)^2}\\right)\n$$\nTaking logarithm, differentiate w.r.t $x$ and divide by $2x$, we get\n$$\\sum_{k=0}^\\infty \\frac{1}{(k+\\frac12)^2 + x^2} = \\frac{\\pi}{2x} \\tanh\\pi x$$\nDifferentiate w.r.t $x$ and divide by $-2x$ once more, we get\n$$\\sum_{k=0}^\\infty \\frac{1}{((k+\\frac12)^2 + x^2)^2} \n= \\frac{\\pi}{4x^3}\\tanh(\\pi x) - \\frac{\\pi^2}{4x^2\\cosh^2(\\pi x)}\n$$\nWith this, we find\n$$\\text{Area} \n=  \\sum_{k=0}^\\infty \\frac{\\pi}{((k+\\frac12)^2 + 2)^2}\n=  \\frac{\\pi^2}{16}\\left[\\sqrt{2}\\tanh(\\pi\\sqrt{2}) - \\frac{2\\pi}{\\cosh^2(\\pi\\sqrt{2})}\\right]\n$$\nOnce again, this agrees with J.J's result.", "meta": {"post_id": 903834, "input_score": 58, "output_score": 34, "post_title": "Geometry problem involving infinite number of circles"}}
{"input": "Prove without using induction that the following formula:$$\\sum_{k=0}^n (-1)^k\\binom{n}{k}=0$$ is valid for every $n\\ge1$.\n\nProgress\nFor each odd $n$ we can use the identity:$$\\binom{n}{k}=\\binom{n}{n-k}$$ In fact all terms equidistant from the end points are opposite. My question is: if $n$ is even how can we prove it?", "output": "Set $a=1,b=-1$ in the Binomial Expansion formula for non-negative intger $n$ $$(a+b)^n=\\sum_{k=0}^n\\binom nk a^{n-k}b^k$$", "meta": {"post_id": 907742, "input_score": 8, "output_score": 37, "post_title": "Alternating sum of binomial coefficients is equal to zero"}}
{"input": "This thought jumped out of me during my calculus teaching seminar. \nIt is well known that the classical L'Hospital rule claims that for the $\\frac{0}{0}$ indeterminate case, we have:\n$$\n\\lim_{x\\rightarrow A}\\frac{f(x)}{g(x)}=\\lim_{x\\rightarrow A}\\frac{f'(x)}{g'(x)}\n$$\nwhere the later could take any value including $\\infty$. Here we assume that right hand side limit exist. \nHowever, to apply it one often has to take the derivative of $f'(x)$ again at $A$, and in principle one assumes by repeatedly applying this rule we can resolve the problem by plug in the value into the function's derivative at $A$. My question is, what if the student ask if it is possible for $\\lim_{x\\rightarrow A} f(x),\\lim_{x\\rightarrow A} f'(x)\\cdots \\lim^{n}_{x\\rightarrow A}f^{n}(x)$ be all zero for any $n$, so the rule 'fails'. How should we answer the question properly?\nFor example, consider the well-known non-analytic smooth function:\n$$f(x)=\n\\begin{cases}\ne^{-1/x}& x> 0\\\\\n0& x\\le 0\n\\end{cases}\n$$\nIt is a trivial exercise to verify that $f^{n}(0)=0$ for any $n\\in \\mathbb{N}$. Now using L'Hospital rule we compute (as if we are a low level student)\n$$\n1=\\lim_{x\\rightarrow 0^{+}}\\frac{f(x)}{f(x)}=\\lim_{x\\rightarrow 0^{+}}\\frac{f'(x)}{f'(x)}=\\lim_{x\\rightarrow 0^{+}}\\frac{f''(x)}{f''(x)}\\cdots =\\frac{0}{0}=?\n$$\nas the chain does not stop if the student applies the rule faithfully and blindly. This is a silly example, but in general for non-analytical functions I think this kind of thing could happen. And there should be more non-analytical functions than analytical functions. Is there a way for us to resolve this at introductory calculus level, so that the student know what to do, without introducing `confusing concepts' like $\\epsilon-\\delta$ language, Cauchy mean value theorem, Taylor series, and infinitesimals?", "output": "Even for analytical functions, this kind of thing can happen. \nConsider $\\displaystyle\\lim_{x \\to \\infty}\\dfrac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$. \nBlindly applying L'Hopital's Rule repeatedly gives: \n$\\displaystyle\\lim_{x \\to \\infty}\\dfrac{e^{x}-e^{-x}}{e^{x}+e^{-x}} = \\lim_{x \\to \\infty}\\dfrac{e^{x}+e^{-x}}{e^{x}-e^{-x}} = \\lim_{x \\to \\infty}\\dfrac{e^{x}-e^{-x}}{e^{x}+e^{-x}} = \\cdots$. \nBut if we divide the numerator and denominator by $e^x$ we get: \n$\\displaystyle\\lim_{x \\to \\infty}\\dfrac{e^{x}-e^{-x}}{e^{x}+e^{-x}} = \\lim_{x \\to \\infty}\\dfrac{1-e^{-2x}}{1+e^{-2x}} = \\dfrac{1+0}{1+0} = 1$.\n\nAlso, consider $\\displaystyle\\lim_{x \\to 0^+}x \\ln x = \\lim_{x \\to 0^+}\\dfrac{\\ln x}{1/x}$. \nBlindly applying L'Hopital's Rule repeatedly gives: \n$\\displaystyle\\lim_{x \\to 0^+}x \\ln x = \\lim_{x \\to 0^+}\\dfrac{\\ln x}{1/x} = \\lim_{x \\to 0^+}\\dfrac{1/x}{-1/x^2} = \\lim_{x \\to 0^+}\\dfrac{-1/x^2}{2/x^3} = \\lim_{x \\to 0^+}\\dfrac{2/x^3}{-6/x^4} = \\cdots$.\nBut it we stop after applying L'Hopital's Rule once and simplify stuff, we get: \n$\\displaystyle\\lim_{x \\to 0^+}x \\ln x = \\lim_{x \\to 0^+}\\dfrac{\\ln x}{1/x} = \\lim_{x \\to 0^+}\\dfrac{1/x}{-1/x^2} = \\lim_{x \\to 0^+} -x = 0$. \n\nIn both of these problems, the solution was to use basic algebra instead of just L'Hopital's Rule. The techniques taught in introductory calculus will not solve every limit problem in the world, but they will solve the problems encountered in introductory calculus. The important thing for students is to know many techniques and learn to figure out which ones will work for a given problem. Many students learn L'Hopital's Rule and then forget how to use every other tool. This is why after teaching L'Hopital's Rule, you should throw in a few examples where L'Hopital's Rule fails. This way, they think of L'Hopital's Rule as just another tool instead of magic.", "meta": {"post_id": 912650, "input_score": 70, "output_score": 64, "post_title": "When does L' Hopital's rule fail?"}}
{"input": "While trying to find the list of axioms of ZF on the Web and in literature I noticed that the lists I had found varied quite a bit. Some included the axiom of empty set, while others didn't. \nThat is perfectly understandable - the statement of the axiom is provable from the axiom schema of specification. Some lists also contained the axiom of pairing, while others didn't - I've heard here on MSE that the statement of this axiom is also provable. \nI was wondering: are there other axioms of ZF statements of which are also provable that I don't know of? What is the true commonly accepted list of ZF axioms which doesn't contain any redundant axioms included just for emphasis?", "output": "Here is my preferred list of axioms, they are written in the language of $\\in$, and $=$ is a logical symbol.\n\nExtensionality. $\\forall x\\forall y(x=y\\leftrightarrow\\forall z(z\\in x\\leftrightarrow z\\in y))$. Two sets are equal if and only if they have the same elements.\nUnion. $\\forall x\\exists y\\forall u(u\\in y\\leftrightarrow\\exists v(v\\in x\\land u\\in v))$. If $x$ is a set, then $\\bigcup x$ is a set.\nRegularity. $\\forall x(\\exists y(y\\in x)\\rightarrow\\exists y(y\\in x\\land\\forall z(z\\in x\\rightarrow z\\notin y)))$. The $\\in$ relation is well-founded.\nPower set. $\\forall x\\exists y\\forall z(z\\in y\\leftrightarrow\\forall u(u\\in z\\rightarrow u\\in x))$. If $x$ is a set, then $\\mathcal P(x)$ is a set.\nReplacement schema. If $\\varphi(x,y,p_1,\\ldots,p_n)$ is a formula in the language of set theory, then: $$\\forall p_1\\ldots\\forall p_n\\\\ \\forall u(\\forall x(x\\in u\\rightarrow(\\exists y\\varphi(x,y,p_1,\\ldots,p_n)\\rightarrow\\exists y(\\varphi(x,y,p_1,\\ldots,p_n)\\land\\forall z(\\varphi(x,z,p_1,\\ldots,p_n)\\rightarrow z=y)))\\rightarrow\\exists v\\forall y(y\\in v\\leftrightarrow\\exists x(x\\in u\\land\\varphi(x,y,p_1,\\ldots,p_n))).$$ For every fixed parameters, $p_1,\\ldots,p_n$, and for every set $u$, if for every $x\\in u$ there is at most one $y$ such that $\\varphi(x,y,p_1,\\ldots,p_n)$, namely the formula, with the fixed parameters, define a partial function on $u$, then there is some $v$ which is exactly the range of this function.\nInfinity. $$\\exists x(\\exists y(y\\in x\\land\\forall z(z\\notin y))\\land\\forall u(u\\in x\\rightarrow\\exists v(v\\in x\\land\\forall w(w\\in v\\leftrightarrow w\\in u\\lor w=u))))\\text{.}$$ There exist a set $x$ which has the empty set as an element, and whenever $y\\in x$, then $y\\cup\\{y\\}\\in x$ as well.\n\nI wrote those purely in the language of $\\in$, as you can see, to avoid any claims that I need to use $\\subseteq$ or $\\mathcal P$ or $\\bigcup$. I will now allow myself these addition to the language.\nFrom these axioms we can easily:\n\nProve there is an empty set: it is the element of the set guaranteed to exist in the infinity axiom.\nProve the pairing axiom: By the power set axiom, $\\mathcal P(\\varnothing)$ exists, and its power set $\\{\\varnothing,\\{\\varnothing\\}\\}$ exists too. Now consider the formula $\\varphi(x,y,a,b,c,d)$ whose content is $$(x=a\\land y=c)\\lor(x=b\\land y=d).$$\nGiven two sets, $u,v$ consider the replacement axiom for $\\varphi$ with the parameters: $\\varphi(x,y,\\varnothing,\\mathcal P(\\varnothing),u,v)$, and the domain $\\mathcal{P(P(\\varnothing))}$. Then there is a set who is the range of the function $\\varphi$ defines here, which is exactly $\\{u,v\\}$.\nSpecification schema: Suppose that $\\varphi(x,p_1,\\ldots,p_n)$ is a formula in the language of set theory, and $A$ is a set which exists. Define $\\psi(x,y,p_1,\\ldots,p_n)$ to be $\\varphi(x,p_1,\\ldots,p_n)\\land x=y$. Easily we can prove that given any element of $A$ there is at most one element satisfying $\\psi(x,y,p_1,\\ldots,p_n)$ (with the fixed parameters). And therefore the range of the function defined is $\\{x\\in A\\mid\\varphi(x,p_1,\\ldots,p_n)\\}$ as wanted.\n\nAnd so on and so forth. The choice of axiomatization usually doesn't matter. But it does matter when one has to verify the axioms by hand for one reason or another, then it might be fortuitous to add explicit axioms or it might be better to keep it minimal. Depending on the situation.\nIt is also an important question what axioms you keep, or add, when you consider weakening of $\\sf ZF$. You can remove replacement, but add specification, or perhaps specification for a particular class of formulas; or you can remove extensionality and then the choice whether to use Replacement or Collection schemas really prove a big different; and so on.", "meta": {"post_id": 916072, "input_score": 31, "output_score": 34, "post_title": "What axioms does ZF have, exactly?"}}
{"input": "Pythagoras says that $\\cos^2 \\theta + \\mathrm{sin}^2\\theta = 1$ for all real $\\theta$.\n(Vague) Question. Is this the only relationship between the functions $\\cos$ and $\\sin$?\nMore precisely:\nLet $\\langle \\cos,\\sin\\rangle$ denote the intersection of all subalgebras of the $\\mathbb{R}$-algebra of all functions $\\mathbb{R} \\rightarrow \\mathbb{R}$ containing $\\{\\cos,\\sin\\}$. (By default, all my algebras are unital, associative and commutative.) Let $A$ denote the $\\mathbb{R}$-algebra presented by the generators $\\{c,s\\}$ and the relation $c^2+s^2=1$. There is a unique $\\mathbb{R}$-algebra homomorphism $\\varphi : A \\rightarrow \\langle \\cos,\\sin\\rangle$ given as follows. $$\\varphi(c) = \\cos, \\,\\,\\varphi(s) = \\sin$$\nWe know that $\\varphi$ is surjective.\n\nQuestion. Is $\\varphi$ injective?\n\nSo consider $f \\in A$. Then $f = \\sum_{i,j : \\mathbb{N}}a_{ij}s^ic^i$ for certain choices of $a_{ij} : \\mathbb{R}$. Now suppose $\\varphi(f)=0$. We want to show that $f=0$. Ideas, anyone?", "output": "If I understand well, you are asking if the $\\mathbb R$-algebra generated by $\\sin$ and $\\cos$, that is, $\\mathbb R[\\sin,\\, \\cos]$ is isomorphic to $\\mathbb R[X,Y]/(X^2+Y^2-1)$. \nConsider the surjective morphism $\\varphi:\\mathbb R[X,Y]\\to\\mathbb R[\\sin,\\,\\cos]$ defined by $\\varphi(X)=\\sin$, $\\varphi(Y)=\\cos$. Then $(X^2+Y^2-1)\\subseteq\\ker\\varphi$. Conversely, let $f\\in\\ker\\varphi$. We can write $f=(X^2+Y^2-1)g+r$ where $\\deg_Xr\\le 1$, so $r=a(Y)+b(Y)X$. Moreover, $a(\\cos)+b(\\cos)\\sin=0$. This means that $a(\\cos x)+b(\\cos x)\\sin x=0$ for all $x\\in\\mathbb R$. Changing $x$ by $-x$ we get $a(\\cos x)-b(\\cos x)\\sin x=0$ for all $x\\in\\mathbb R$, so $a(\\cos x)=0$ for all $x\\in\\mathbb R$, and $b(\\cos x)=0$ for all $x\\in\\mathbb R$, $x\\ne k\\pi$. Since $a,b$ are polynomials this is enough to conclude $a=b=0$, and therefore $r=0$. We thus proved that $\\ker\\varphi=(X^2+Y^2-1)$.", "meta": {"post_id": 937517, "input_score": 54, "output_score": 60, "post_title": "Is Pythagoras the only relation to hold between $\\cos$ and $\\sin$?"}}
{"input": "How can the equation of a circle be determined from the equations of a sphere and a plane which intersect to form the circle?  At a minimum, how can the radius and center of the circle be determined?\nFor example, given the plane equation $$x=\\sqrt{3}*z$$ and the sphere given by $$x^2+y^2+z^2=4$$\nWhat is the equation of the circle that results from their intersection?  I have used Grapher to visualize the sphere and plane, and know that the two shapes do intersect:\n\nHowever, substituting $$x=\\sqrt{3}*z$$ into $$x^2+y^2+z^2=4$$ yields the elliptical cylinder $$4x^2+y^2=4$$while substituting $$z=x/\\sqrt{3}$$ into $$x^2+y^2+z^2=4$$ yields $$4x^2/3+y^2=4$$ Once again the equation of an elliptical cylinder, but in an orthogonal plane.\nWhy does this substitution not successfully determine the equation of the circle of intersection, and how is it possible to solve for the equation, center, and radius of that circle?", "output": "$\\newcommand{\\Vec}[1]{\\mathbf{#1}}$Generalities: Let $S$ be the sphere in $\\mathbf{R}^{3}$ with center $\\Vec{c}_{0} = (x_{0}, y_{0}, z_{0})$ and radius $R > 0$, and let $P$ be the plane with equation $Ax + By + Cz = D$, so that $\\Vec{n} = (A, B, C)$ is a normal vector of $P$.\nIf $\\Vec{p}_{0}$ is an arbitrary point on $P$, the signed distance from the center of the sphere $\\Vec{c}_{0}$ to the plane $P$ is\n$$\n\\rho = \\frac{(\\Vec{c}_{0} - \\Vec{p}_{0}) \\cdot \\Vec{n}}{\\|\\Vec{n}\\|}\n  = \\frac{Ax_{0} + By_{0} + Cz_{0} - D}{\\sqrt{A^{2} + B^{2} + C^{2}}}.\n$$\nThe intersection $S \\cap P$ is a circle if and only if $-R < \\rho < R$, and in that case, the circle has radius $r = \\sqrt{R^{2} - \\rho^{2}}$ and center\n$$\n\\Vec{c}\n  = \\Vec{c}_{0} + \\rho\\, \\frac{\\Vec{n}}{\\|\\Vec{n}\\|}\n  = (x_{0}, y_{0}, z_{0}) + \\rho\\, \\frac{(A, B, C)}{\\sqrt{A^{2} + B^{2} + C^{2}}}.\n$$\n\n\nNow consider the specific example\n$$\nS = \\{(x, y, z) : x^{2} + y^{2} + z^{2} = 4\\},\\qquad\nP = \\{(x, y, z) : x - z\\sqrt{3} = 0\\}.\n$$\nThe center of $S$ is the origin, which lies on $P$, so the intersection is a circle of radius $2$, the same radius as $S$.\nWhen you substitute $x = z\\sqrt{3}$ or $z = x/\\sqrt{3}$ into the equation of $S$, you obtain the equation of a cylinder with elliptical cross section (as noted in the OP). However, you must also retain the equation of $P$ in your system. That is, each of the following pairs of equations defines the same circle in space:\n\\begin{align*}\nx - z\\sqrt{3} &= 0,       & x - z\\sqrt{3} &= 0,             & x - z\\sqrt{3} &= 0, \\\\\nx^{2} + y^{2} + z^{2} &= 4; & \\tfrac{4}{3} x^{2} + y^{2} &= 4; & y^{2} + 4z^{2} &= 4.\n\\end{align*}\nThese may not \"look like\" circles at first glance, but that's because the circle is not parallel to a coordinate plane; instead, it casts elliptical \"shadows\" in the $(x, y)$- and $(y, z)$-planes.\nNote that a circle in space doesn't have a single equation in the sense you're asking.", "meta": {"post_id": 943383, "input_score": 34, "output_score": 52, "post_title": "Determine Circle of Intersection of Plane and Sphere"}}
{"input": "I am trying to wrap my head around back-propagation in a neural network with a Softmax classifier, which uses the Softmax function:\n\\begin{equation}\np_j = \\frac{e^{o_j}}{\\sum_k e^{o_k}}\n\\end{equation}\nThis is used in a loss function of the form\n\\begin{equation}L = -\\sum_j y_j \\log p_j,\\end{equation}\nwhere $o$ is a vector. I need the derivative of $L$ with respect to $o$. Now if my derivatives are right,\n\\begin{equation}\n\\frac{\\partial p_j}{\\partial o_i} = p_i(1 - p_i),\\quad i = j\n\\end{equation}\nand\n\\begin{equation}\n\\frac{\\partial p_j}{\\partial o_i} = -p_i p_j,\\quad i \\neq j.\n\\end{equation}\nUsing this result we obtain\n\\begin{eqnarray}\n\\frac{\\partial L}{\\partial o_i} &=& - \\left (y_i (1 - p_i) + \\sum_{k\\neq i}-p_k y_k \\right )\\\\\n&=&p_i y_i - y_i + \\sum_{k\\neq i} p_k y_k\\\\\n&=& \\left (\\sum_i p_i y_i \\right ) - y_i\n\\end{eqnarray}\nAccording to slides I'm using, however, the result should be\n\\begin{equation}\n\\frac{\\partial L}{\\partial o_i} = p_i - y_i.\n\\end{equation}\nCan someone please tell me where I'm going wrong?", "output": "Your derivatives $\\large \\frac{\\partial p_j}{\\partial o_i}$ are indeed correct, however there is an error when you differentiate the loss function $L$ with respect to $o_i$. \nWe have the following (where I have highlighted in $\\color{red}{red}$ where you have gone wrong)\n$$\\frac{\\partial L}{\\partial o_i}=-\\sum_ky_k\\frac{\\partial \\log p_k}{\\partial o_i}=-\\sum_ky_k\\frac{1}{p_k}\\frac{\\partial p_k}{\\partial o_i}\\\\=-y_i(1-p_i)-\\sum_{k\\neq i}y_k\\frac{1}{p_k}({\\color{red}{-p_kp_i}})\\\\=-y_i(1-p_i)+\\sum_{k\\neq i}y_k({\\color{red}{p_i}})\\\\=-y_i+\\color{blue}{y_ip_i+\\sum_{k\\neq i}y_k({p_i})}\\\\=\\color{blue}{p_i\\left(\\sum_ky_k\\right)}-y_i=p_i-y_i$$ given that $\\sum_ky_k=1$ from the slides (as $y$ is a vector with only one non-zero element, which is $1$).", "meta": {"post_id": 945871, "input_score": 197, "output_score": 173, "post_title": "Derivative of Softmax loss function"}}
{"input": "Can you show me a continuous function $f \\colon \\mathbb{R}^n\\to\\mathbb{R}^m$ that satisfies $f(a+b)=f(a)+f(b)$ but is not linear?\n\nWe have that \n$$f(0)=f(0+0)=2f(0)\\implies f(0)=0\\\\\nf(x-x)=f(0)=f(x)+f(-x)=0\\implies f(-x)=-f(x)\\\\\nf(nx)=f(x+x+\\dots+x)=f(x)+\\dots+f(x)=nf(x)\\quad \\forall n \\in \\mathbb{N}$$\nBut $$\nf(-nx)=-f(nx)=-nf(x)\n$$\nSo:\n$$\nf(ax)=af(x) \\quad \\forall a \\in \\mathbb{Z}\n$$", "output": "Nobody can, because a continuous function $f \\colon \\mathbb{R}^n\\to\\mathbb{R}^m$ satisfying\n$$\nf(x+y)=f(x)+f(y)\n$$\nfor all $x,y\\in\\mathbb{R}^n$ is linear.\nThe proof is quite easy.\n\n$f(ax)=af(x)$ for all $a\\in\\mathbb{Z}$ and $x\\in\\mathbb{R}^n$\n$f(\\frac{a}{b}x)=\\frac{a}{b}f(x)$, for all $\\frac{a}{b}\\in\\mathbb{Q}$ and all $x\\in\\mathbb{R}^n$\n$f(rx)=rf(x)$, for all $r\\in\\mathbb{R}$ and all $x\\in\\mathbb{R}^n$.\n\nFor the last step, if $r\\in\\mathbb{R}$, consider a sequence $q_k$ in $\\mathbb{Q}$ converging to $r$. Then $q_kx$ is a sequence in $\\mathbb{R}^n$ converging to $rx$ and $q_kf(x)$ is a sequence in $\\mathbb{R}^m$ converging to $rf(x)$. By continuity of $f$,\n$$\nf(rx)=f(\\lim_{k\\to\\infty}q_kx)=\n\\lim_{k\\to\\infty}f(q_kx)=\n\\lim_{k\\to\\infty}q_kf(x)=\nrf(x)\n$$", "meta": {"post_id": 957274, "input_score": 27, "output_score": 58, "post_title": "$f(a+b)=f(a)+f(b)$ but $f$ is not linear"}}
{"input": "This is a Putnam Problem that I have been trying to solve (on and off) for two years, but I have failed. I am in Calculus BC. This problem comes from the book \"Calculus Eighth Edition by Larson, Hostetler, and Edwards\". This problem is at the end of the first section of the chapter 8 exercises. Here's the problem:\nEvaluate $$\\int_2^4 \\frac{\\sqrt{\\ln(9-x)}\\,dx}{\\sqrt{\\ln(9-x)} + \\sqrt{\\ln(x+3)}}.$$\nPlease. Any help is very much appreciated. So are solutions. Thank you!\nEdit: I like the solution given, but I was interested to see if there is any other way of doing the problem? I'm excited to see the results.", "output": "Let\n$$ \\mathcal{I}=\\int_{2}^{4}\\dfrac{\\sqrt{\\ln(9-x)}}{\\sqrt{\\ln(9-x)}+\\sqrt{\\ln(3+x)}}\\,\\mathrm{d}x $$\nNow, use that\n$$ \\int_{a}^{b}f(x)\\,\\mathrm{d}x\\overset{(1)}{=}\\int_{a}^{b}f(a+b-x)\\,\\mathrm{d}x $$\nThen,\n$$ \\mathcal{I}=\\int_{2}^{4}\\dfrac{\\sqrt{\\ln(3+x)}}{\\sqrt{\\ln(3+x)}+\\sqrt{\\ln(9-x)}}\\,\\mathrm{d}x $$\nAdd up these two integrals to get\n$$ 2\\mathcal{I}=\\int_{2}^{4}\\dfrac{\\sqrt{\\ln(9-x)}+\\sqrt{\\ln(3+x)}}{\\sqrt{\\ln(9-x)}+\\sqrt{\\ln(3+x)}}\\,\\mathrm{d}x $$\nThus,\n$$ \\mathcal{I}=1 $$\nIn order to prove $(1)$, write the integral using another variable, say, $t$:\n$$ \\int_{a}^{b}f(a+b-x)\\,\\mathrm{d}x=\\int_{a}^{b}f(a+b-t)\\,\\mathrm{d}t $$\nIn the latter one, set $x=a+b-t$ and $\\mathrm{d}t=-\\mathrm{d}x$ and change the limits of integration to obtain\n$$ \\begin{aligned}\n\\int_{a}^{b}f(a+b-t)\\,\\mathrm{d}t&=-\\int_{b}^{a}f(x)\\,\\mathrm{d}x\\\\\n&=\\int_{a}^{b}f(x)\\,\\mathrm{d}x.\n\\end{aligned} $$", "meta": {"post_id": 957510, "input_score": 18, "output_score": 36, "post_title": "A Putnam Integral $\\int_2^4 \\frac{\\sqrt{\\ln(9-x)}\\,dx}{\\sqrt{\\ln(9-x)} + \\sqrt{\\ln(x+3)}}.$"}}
{"input": "Let $X$ be a metric space. Prove if $X$ is compact, then $X$ is\n  separable.\n\n\nX separable $\\iff X$ contains a countable dense subset.  \n$E \\subset X $ dense in $X \\iff \\overline{E} = X$.\n$X$ compact $\\iff$ every open cover of $X$ admits a finite subcover.\n\none might also use that $X$ compact in a metric space implies closed and bounded. \nProof\nWe want to show $\\exists E \\subset X. X = \\overline{E} = E \\cup E'$  where $E'$ denotes the set of limit points. If $X$ is compact then a subset $E$ of $X$ would be compact, so that subset is closed and bounded. Since $E$ is closed, $E \\supseteq E'$. Now we need to show that $E$ is a countable dense subset... But I don't know where to go from here. I have the following:\nHint: cover $X$ with neighborhoods of radius $\\frac{1}{n}$ - so for every positive integer $n$, there are finitely many neighborhoods of radius $\\frac{1}{n}$ whose union covers $X$. So maybe it would be better to work with the open cover definition of compact.", "output": "One approach is to prove that if $X$ is a compact metric space then $X$ is totally bounded. This means that for every $\\varepsilon > 0$ there is a finite number, say $n(\\varepsilon)$, of points, call them $x_1,\\dots,x_{n(\\varepsilon)}$, such that the balls $B_\\varepsilon(x_1),\\dots,B_\\varepsilon(x_{n(\\varepsilon)})$ cover $X$. This is actually quite simple to prove, because if $X$ is a compact metric space, then given $\\varepsilon > 0$, the cover $\\{ B_{\\varepsilon}(x) : x \\in X \\}$ has a finite subcover of the desired form.\nFrom there, cover $X$ with finitely many balls of radius $1$; extract the center of each. Now every point is within $1$ of a point in your (finite) set. Cover $X$ with finitely many balls of radius $1/2$; extract the center from each. Now every point is within $1/2$ of a point in your (still finite) set. Repeat for each $1/m$ for $m \\in \\mathbb{N}$ and take the countable union. A countable union of finite sets is countable, so you have your countable dense subset.", "meta": {"post_id": 974233, "input_score": 42, "output_score": 59, "post_title": "Prove if $X$ is a compact metric space, then $X$ is separable."}}
{"input": "I'm trying to mentally summarize the names of the operands for basic operations. I've got this so far:\n\nAddition: Augend + Addend = Sum.\nSubtraction: Minuend - Subtrahend = Difference. \nMultiplication: Multiplicand \u00d7 Multiplier = Product. Generally, operands are called factors.\nDivision: Dividend \u00f7 Divisor = Quotient.\nModulation: Dividend % Divisor = Remainder.\nExponentiation: Base ^ Exponent = ___.\nFinding roots: Degree \u221a Radicand = Root.\n\nMy questions:\n\nI've heard addend used generally for addition operands. Is that correct formal usage?\nDo subtraction and division lack general names for their operands because they are not commutative? Or am I just ignorant of them?\nIs the base the same as a mantissa?\nIs there a formal name for the result of exponentiation?\nIs there a formal name for the operation of finding the nth root?\nAm I missing anything else?", "output": "Found this table on Wikipedia. It has all the formal names for those operations plus logarithm.\nhttps://en.wikipedia.org/wiki/Template:Calculation_results\nAddition\n${\\left.{\\begin{matrix}{\\text{summand}}+{\\text{summand}}\\\\{\\text{addend (broad sense)}}+{\\text{addend (broad sense)}}\\\\{\\text{augend}}+{\\text{addend (strict sense)}}\\end{matrix}}\\right\\}}=sum$\nSubtraction\n${\\text{minuend}}-{\\text{subtrahend}}=difference$\nMultiplication\n$\\left.{\\begin{matrix}{\\text{factor}}\\times {\\text{factor}}\\\\{\\text{multiplier}}\\times {\\text{multiplicand}}\\end{matrix}}\\right\\}=product$\nDivision\n${\\left.{\\begin{matrix}{\\frac {{\\text{dividend}}}{{\\text{divisor}}}}\\\\{\\text{ }}\\\\{\\frac {{\\text{numerator}}}{{\\text{denominator}}}}\\end{matrix}}\\right\\}}={{\\begin{matrix}fraction\\\\quotient\\\\ratio\\end{matrix}}}$\nModulo\n${\\text{dividend}}{\\bmod {\\text{divisor}}}=remainder$\nExponentiation\n${\\text{base}}^{\\text{exponent}}=power$\nnth root\n${\\sqrt[{\\text{degree}}]{{\\text{radicand}}}}=root$\nLogarithm\n$\\log _{\\text{base}}({\\text{antilogarithm}})=logarithm$", "meta": {"post_id": 975541, "input_score": 76, "output_score": 44, "post_title": "What are the formal names of operands and results for basic operations?"}}
{"input": "My brother's friend gave me the following wicked integral with a beautiful result\n\n\\begin{equation}\n{\\Large\\int_0^\\infty} \\frac{dx}{\\sqrt{x} \\bigg[x^2+\\left(1+2\\sqrt{2}\\right)x+1\\bigg] \\bigg[1-x+x^2-x^3+\\cdots+x^{50}\\bigg]}={\\large\\left(\\sqrt{2}-1\\right)\\pi}\n\\end{equation}\n\nHe claimed the above integral can be generalised to the following form\n\\begin{equation}\n{\\Large\\int_0^\\infty} \\frac{dx}{\\sqrt{x} \\bigg[x^2+ax+1\\bigg] \\bigg[1-x+x^2-x^3+\\cdots+(-x)^{n}\\bigg]}=\\ldots\n\\end{equation}\nThis is a challenging problem. How to prove it and what is the closed-form of the general integral?", "output": "Indeed let\n$$\nI(n,a)=\\int_0^\\infty\\frac{dx}{\\sqrt{x}(1+ax+x^2)(\\sum_{k=0}^n(-x)^k)}\n$$\nThe change of variables $x\\leftarrow 1/x$ yields\n$$\nI(n,a)=\\int_0^\\infty\\frac{(-1)^nx^{n+1}dx}{ \\sqrt{x}(1+ax+x^2)(\\sum_{k=0}^n(-x)^k)}\n$$\nThus\n$$\n2I(n,a)=\\int_0^\\infty\\frac{1+x}{\\sqrt{x}(1+ax+x^2)}dx=\n2\\int_0^\\infty\\frac{1+t^2}{ 1+at^2+t^4}dt\n$$\nOr equivalently, setting $u=t-1/t$,\n$$\nI(n,a)=\n\\int_{-\\infty}^\\infty\\frac{du}{ 2+a+u^2} =\\frac{\\pi}{\\sqrt{2+a}}.\n$$", "meta": {"post_id": 978560, "input_score": 57, "output_score": 56, "post_title": "Evaluating $\\int_0^\\infty \\frac{dx}{\\sqrt{x}[x^2+(1+2\\sqrt{2})x+1][1-x+x^2-x^3+...+x^{50}]}$"}}
{"input": "Is there a function $\\,f:\\mathbb{R}\\rightarrow\\mathbb{R},\\,$ which has a limit at every $x\\in\\mathbb R$ and is everywhere discontinuous?", "output": "Answer. No.\nInstead, the following is true: If a function $f:\\mathbb R\\to\\mathbb R$ has a limit for every $x\\in\\mathbb R$, then $f$ is discontinuous in a set of points which is at most countable.\nMore specifically, we have the following facts:\nFact A. If $g(x)=\\lim_{y\\to x}f(y)$, then $g$ is continuous everywhere.\nFact B. The set $A=\\{x: f(x)\\ne g(x)\\}$ is countable.\nFact C. The function $\\,f\\,$ is continuous at $\\,x=x_0\\,$ if and only if $\\,f(x_0)=g(x_0)$, and hence $f$ is discontinuous in at most countably many points. \nFor Fact A, let $x\\in\\mathbb R$ and $\\varepsilon>0$, then there exists a $\\delta>0$, such that\n$$\n0<\\lvert y-x\\rvert<\\delta\\quad\\Longrightarrow\\quad  g(x)-\\varepsilon<f(y)<g(x)+\\varepsilon,\n$$\nbut the above inequality implies that for every $z$, with $|z-x|<\\delta$,\n$$\ng(x)-\\varepsilon \\le g(z)=\\lim_{y\\to z}f(y) \\le g(x)+\\varepsilon,\n$$\nand hence $g$ is continuous.\nFor Fact B, define for $\\varepsilon>0$ the set\n$$A_\\varepsilon=\\{x: \\lvert\\,f(x)- g(x)\\rvert>\\varepsilon\\}.$$ This set cannot have a limit point, for otherwise, $f$ would not have a limit there. Thus $A_\\varepsilon$ is at most countable. Next observe that\n$$\nA=\\bigcup_{n\\in\\mathbb N}A_{1/n},\n$$\nand hence $A$, the set of discontinuities of $f$, is at most countable.\nFact C is straight-forward.", "meta": {"post_id": 980022, "input_score": 34, "output_score": 35, "post_title": "Is there a function having a limit at every point while being nowhere continuous?"}}
{"input": "In a course on differential manifolds and Lie groups, the following theorem was stated, though never proven:\n\nLet $M$ and $N$ be smooth manifolds, and suppose $G$ is a Lie group acting on $M$. If the group action is free and proper, then $M/G$ has a manifold structure so that the quotient map $\\pi:M\\to M/G$ is smooth. Additionally, if $f:M/G\\to N$ and $f\\circ\\pi$ is smooth, then $f$ is smooth.\n\nThe definition of a proper group action was given as follows: given any compact set $K \\subset M$, the action of $G$ on $M$ is proper if and only if the set $\\{g\\in G:gK\\cap K\\neq\\emptyset\\}$ has compact closure in $G$.\nI am confused about two things:\n\n1) The definition of proper group action differs from the one I got from Wikipedia and the books I have, where the acting group is assumed to be discrete. Moreover, I found it highly contrived due to a lack of understanding on my part. What exactly is this definition helping us to see? The need for the closure to be compact, for instance, is lost on me.\n2) Removing the context of manifolds and Lie groups, if one were to look at arbitrary group actions, then how would the proper action be defined? What kind of topology on G makes sense?\n\nI apologise for any naivet\u00e9; I am new to group actions, even in the algebraic case. Thank you for your patience!", "output": "If $G$ is a topological group acting on a topological space $M$, the usual definition is that the action is proper if the map $G\\times M\\to M\\times M$ defined by $(g,x)\\mapsto (g\\cdot x,x)$ is a proper map, which means that the preimage of every compact set is compact. This definition works both in the topological category and in the smooth category. \nFor sufficiently nice spaces, there are other characterizations that are often useful. For example:\n\nIf $M$ is Hausdorff, then properness is equivalent to the condition you described.\nIf $M$ and $G$ are Hausdorff and first-countable, then properness is equivalent to the following condition: If $(x_i)$ is a sequence in $M$ and $(g_i)$ is a sequence in $G$ such that both $(x_i)$ and $(g_i\\cdot x_i)$ converge, then a subsequence of $(g_i)$ converges.\n\nActions of compact groups (on Hausdorff spaces) are always proper, so properness really has meaning only for actions of noncompact groups. In this case, properness has the intuitive meaning that \"most\" of $G$ (i.e., all but a compact subset) moves compact sets of $M$ far away from themselves.  \nYou can find more information on group actions in the topological category in my Introduction to Topological Manifolds (2nd ed.), and in the smooth category in Introduction to Smooth Manifolds (2nd ed.), among other places.", "meta": {"post_id": 987038, "input_score": 20, "output_score": 44, "post_title": "Clarification of notion of proper group action."}}
{"input": "Suppose $L$ is a line bundle and $V$ is bundle of rank $r$ on a surface (compact complex manifold of dim 2). Recall the formula for $c_1$ and $c_2$:\n\n$c_1(V\\otimes L)=c_1(V)+rc_1(L)$\n$c_2(V\\otimes L)=c_2(V)+(r-1)c_1(V).c_1(L)+{r \\choose 2}c_1(L)^2 $\n\nFriedman book\nFriedman, Robert, Algebraic surfaces and holomorphic vector bundles, Universitext. New York, NY: Springer. ix, 328 p. (1998). ZBL0902.14029.\nsays there are similar formulas for $\\operatorname{Sym}^kV$, $\\bigwedge^kV$, $\\operatorname{Hom}(V,W)$, $V\\otimes W$. Could you please illustrate that by some examples? Thank you very much.", "output": "As Asal Beag Dubh says in the comments, the key point is to use the splitting principle to reduce the computations to the case of line bundles. Everything becomes more or less an exercise in symmetric function theory. Here are the four basic examples:\nThe dual bundle \nIf $L$ is a line bundle with Chern class $c_1(L)$, then the dual line bundle $L^{\\ast}$ is isomorphic to the inverse line bundle and hence has Chern class $c_1(L^{\\ast}) = - c_1(L)$. If $V \\cong \\bigoplus_i L_i$ then $V^{\\ast} \\cong \\bigoplus_i L_i^{\\ast}$, so we have\n$$c(V^{\\ast}) = \\prod_i (1 - c_1(L_i))$$\nfrom which it follows that\n$$c_i(V^{\\ast}) = (-1)^i c_i(V).$$\nThe tensor bundle\nIf $L, L'$ are line bundles with Chern classes $c_1(L), c_1(L')$, then the tensor product $L \\otimes L'$ has Chern class $c_1(L \\otimes L') = c_1(L) + c_1(L')$. If $V \\cong \\bigoplus_i L_i$ and $V' \\cong \\bigoplus_j L_j'$, then\n$$V \\otimes V' \\cong \\bigoplus_{i, j} L_i \\otimes L_j'$$\nso we have\n$$c(V \\otimes V') = \\prod_{i, j} (1 + c_1(L_i) + c_1(L_j')).$$\nExtracting more explicit formulas from this is a tedious exercise. An alternative here is to use the Chern character, which is multiplicative with respect to tensor product by design:\n$$\\text{ch}(V \\otimes V') = \\text{ch}(V)\\text{ch}(V').$$\nFor example, this gives\n$$c_1(V \\otimes V') = c_1(V) \\dim V' + c_1(V') \\dim V.$$\nYou can get corresponding formulas for the hom bundle using the isomorphism $V^{\\ast} \\otimes W \\cong \\text{Hom}(V, W)$. \nThe symmetric powers\nIt's cleanest to do all of the symmetric powers at once. The key is the isomorphism\n$$S(V \\oplus W) \\cong S(V) \\otimes S(W)$$\nwhere $S(V) \\cong \\bigoplus_i S^i(V)$ is the symmetric algebra. This is an isomorphism of graded vector bundles, and remembering the grading is important in what comes next. If $V \\cong \\bigoplus_i L_i$, it follows that\n$$S(V) \\cong \\bigotimes_i S(L_i)$$\nand hence that the graded Chern character of $S(V)$, as a graded vector bundle, can be computed as\n$$\\text{ch}(S(V)) = \\sum_k t^k \\text{ch}(S^k(V)) = \\prod_i \\text{ch}(S(L_i)) = \\prod_i \\frac{1}{1 - t e^{c_1(L_i)}}$$\nwhere $t$ is a formal variable. Again, extracting more explicit formulas from this is a tedious exercise. \nThe exterior powers\nAs for the symmetric powers, we again have\n$$\\Lambda(V \\oplus W) \\cong \\Lambda(V) \\otimes \\Lambda(W)$$\nwhere $\\Lambda(V) \\cong \\bigoplus_i \\Lambda^i(V)$ is the exterior algebra. The discussion is exactly the same as for the symmetric algebra except that the last Chern character computation is a bit different, and we get\n$$\\text{ch}(\\Lambda(V)) = \\sum_k t^k \\text{ch}(\\Lambda^k(V)) = \\prod_i \\text{ch}(\\Lambda(L_i)) = \\prod_i (1 + t e^{c_1(L_i)}).$$\nOnce again, extracting more explicit formulas from this is a tedious exercise. To get you started on $c_2(\\Lambda^2(V))$, by looking at the coefficient of $t^2$ we get\n$$\\text{ch}(\\Lambda^2(V)) = \\sum_{i < j} e^{c_1(L_i) + c_1(L_j)}.$$\nThe first term of this expansion gives you the dimension of $\\Lambda^2(V)$, which you already know. The second term gives you the first Chern class, which is\n$$c_1(\\Lambda^2(V)) = (\\dim V - 1) c_1(V).$$\nThe third term gives you the third term in the Chern character of $\\Lambda^2(V)$, which you need to correct a little by $c_1^2$ to get $c_2$.", "meta": {"post_id": 989147, "input_score": 19, "output_score": 35, "post_title": "Quick question: Chern classes of Sym, Wedge, Hom, and Tensor"}}
{"input": "They say that the right and left limits do not approach the same value hence it does not satisfy the definition of derivative.  But what does it mean verbally in terms of rate of change?", "output": "There are 2 ways to understand your question, one is asking for a proof, the other is asking for intuitive reasoning, I will answer both here.\n\nProof that $|x|$ is not differentiable at $x=0$\nWe are trying to find\n$$\\lim_{h\\to0}\\frac{f(0+h)-f(0)}{h}\\quad\\text{where }f(x)=|x|$$\n$$\\lim_{h\\to0}\\frac{|0+h|-|0|}{h}$$\n$$=\\lim_{h\\to0}\\frac{|h|}{h}$$\nLet's find the right limit\n$$=\\lim_{h\\to0^+}\\frac{|h|}{h}$$\nSince $h>0$\n$$=\\lim_{h\\to0^+}\\frac{h}{h}=1$$\nLet's find the left limit\n$$=\\lim_{h\\to0^-}\\frac{|h|}{h}$$\nSince $h<0$\n$$=\\lim_{h\\to0^-}\\frac{-h}{h}=-1$$\nIf the left and right limits are not the same, the limit does not exist.\n\nIntuitive reasoning\nWhat does the derivative represent? The slope of the tangent line. Looking at different values of the absolute value function in some plots:\n\nNote that the tangent line is below the actual line for the absolute value function.\nThe problem with the derivative at $x=0$ is that it changes abrubtly, and derivatives don't like that. Compare to the same plot but with $x^2$\n\nThe difference is clear, the tangent line smoothly changes when approaching $x=0$ instead of the abrubt change from one line to another. This is why the derivative does not exist at $x=0$ for $|x|$.\nAnother way to think about derivatives is as \u201cthe slope of the line you get, when you zoom in really far\u201d. If you zoom in on $x^2$ it'll look a lot like a straight line, but if you zoom in on $|x|$, it will never look like a straight line. If a function is differentiable, it will look like a straight line when you zoom in far enough.", "meta": {"post_id": 991475, "input_score": 60, "output_score": 136, "post_title": "Why is the absolute value function not differentiable at $x=0$?"}}
{"input": "I've been studying differential geometry using Do Carmo's book. There's the notion of exponential map, but I don't understand why it is called \"exponential\" map. How does it has something to do with our common notion of exponentiation? \nI read from the book The road to reality (by R. Penrose) that it is related to taking exponentiation when making (finite) Lie group elements from Lie algebra elements. It seems like using Taylor's theorem on a manifold so we have, for example, there was the following equation explaining why it is the case. \n$f(t) = f(0) + f'(0)t + \\frac{1}{2!}f''(0) t^2+\\cdots = (1+t\\frac{d}{dx}+\\frac{1}{2!}t^2\\frac{d^2}{dx^2}+\\cdots)f(x)|_{x=0} = e^{t\\frac{d}{dx}}f(x)|_{x=0}$. \nThe differential operator can be thought of as a vector field on a manifold, and it is how Lie algebra elements (which are vectors, on a group manifold (Lie group), in a tangent space at the identity element). If I understood correctly, the truth is that this is exactly the exponential map that sends a vector on a tangent space into the manifold in such a way that it becomes the end point of a geodesic (determined by the vector) having the same length. \nWhy is the above Taylor expansion valid on a manifold? Why is the exponential map the same as taking exponentiation?", "output": "The idea of an exponential is the continuous compounding of small actions. Suppose you start with an object $p$, perform an action on it $v$, and then add the result back to the original object. What happens if you instead take half as much action but do it twice? What about if you take one tenth the action but do it ten times? The exponential function tries to capture this idea:\n$$\\exp (\\text{action}) = \\lim_{n \\rightarrow \\infty} \\left(\\text{identity} + \\frac{\\text{action}}{n}\\right)^n.$$\nOn a differentiable manifold there is no addition, but we can consider this action as pushing a point a short distance in the direction of the tangent vector,\n$$``\\left(\\text{identity} + \\frac{\\text{v}}{n}\\right)\"p := \\text{push }p\\text{ by} \\frac{1}{n} \\text{ units of distance in the }v \\text{ direction}.$$\nDoing this over and over, we have \n$``\\left(\\text{identity} + \\frac{\\text{v}}{n}\\right)^n\"p$ means push $p$ by $\\frac{1}{n}$ units of distance in the $v$ direction, then push it again in the same direction you already pushed it, and keep doing so until you have pushed it $n$ times. \nSo long as $\\frac{1}{n}$ is small enough that pushing points and vectors in a tangent direction makes sense, what we end out doing is pushing the point $p$ a total of $1$ unit of distance along the geodesic generated by $v$.", "meta": {"post_id": 999515, "input_score": 37, "output_score": 67, "post_title": "Meaning of Exponential map"}}
{"input": "Is an irrational number times a rational number always irrational?\nIf the rational number is zero, then the result will be rational. So can we conclude that in general, we can't decide, and it depends on the rational number?", "output": "Any nonzero rational number times an irrational number is irrational. Let $r$ be nonzero and rational and $x$ be irrational. If $rx=q$ and $q$ is rational, then $x=q/r$, which is rational. This is a contradiction.", "meta": {"post_id": 1009570, "input_score": 10, "output_score": 54, "post_title": "Is irrational times rational always irrational?"}}
{"input": "As the define goes:\n\nA subgroup $N$ of a group $G$ is called a normal subgroup if it is invariant under conjugation; that is, for each element $n$ in $N$ and each $g$ in $G$, the element $gng^{\u22121}$ is still in $N$.\n\nMy Question is: Can anyone give me an intuitive explanation or an example of this concept? Why it is very important in algebra?", "output": "Let's take a look at the group of rotations of cube. It has a subgroup of rotations around vertical axis. This subgroup (let's call it $A1$) has 4 elements: rotate the cube for 0, 90, 180 or 270 degrees.\nThere is another subgroup: rotation around one of horizontal axes. Let's call it $A2$.\nSubgroups $A1$ and $A2$ are obviously different. But still they look so very much alike! If there was someone else looking at our cube from different angle he could even fail to understand my descriptions of $A1$ and $A2$ \"correctly\" and confuse $A1$ with $A2$.\nThis is because $A1$ and $A2$ are conjugated. The $g x g^{\u22121}$ actually means \"look at $x$ from another point of view\", and $g$ defines this \"point of view\".\nSubgroup is normal if it is very \"symmetric\". No matter from which point you look at the whole group $G$, the subgroup $N$ remains at place.\nUPDATE: example of a normal subgroup.\nLet's take the same cube. Now lets allow only rotations for 180 degrees around $x$, $y$ and $z$ axis. And any combination of such rotations. This will be group $B$. $B$ is a subgroup of the group of all rotations of cube. It is a proper subgroup of the original group (each face of our cube either remains in place or is moved to the opposite position by our rotations, so not all the elements of original group are included into this subgroup).\nThe definition of $B$ \"does not depend of frame of reference\", so I am sure this is a normal subgroup. Well, I understand that \"does not depend of frame of reference\" is not an accurate description, but this whole question is about intuition.\nBy the way, looks like group $B$ consists of only 4 elements: after any combination of described rotations only 1 of 2 faces can become the front, and only 1 of 2 faces can become the top. So \"and any combination of such rotations\" in my definition of group $B$ can be substituted with \"and identity element $e$\".", "meta": {"post_id": 1014535, "input_score": 31, "output_score": 37, "post_title": "Is there any intuitive understanding of normal subgroup?"}}
{"input": "A group of ten people sits down, uniformly at random, around a table. Ken\n  and John are part of this group. Determine the probability that Ken and John sit\n  next to each other.\n\nThere are $10!$ ways to arrange the seating for everyone, there are 10 possible ways for John and Ken to sit together.\n$$\\operatorname{Pr}(J\\ \\&\\ K ) = \\frac{10}{10!} = \\frac{1}{9!}$$\nAm I correct?", "output": "Two of nine people sit next to John.  The probability that Ken is one of these two is $\\frac29$.", "meta": {"post_id": 1016151, "input_score": 19, "output_score": 57, "post_title": "Probability that Ken and John set next to each other"}}
{"input": "I am looking for a good book on abstract algebra (and if possible linear algebra).\nObviously as most of these texts are fairly expensive I want to know for sure which one is best for me. Could someone here give me a rough overview of the strengths and weaknesses of Dummit and Foote's \"abstract algebra\" compared with, for instance, Fraleigh's \"A first course in abstract algebra\" and maybe give some advice as to which is best for my current level.\nI'm not yet an undergraduate, but I've read the book \"An introduction to abstract algebra\" by W. Nicholson, as well as having done many of the exercises. The book seems to cover a lot of the introductory stuff for groups, rings and fields, as well as coverage of other material such as the sylow theorems and some Galois Theory. I want to move onto a book which is more advanced, though preferably one that I can successfully self study and which maybe contains the introductory stuff so I can review it (I don't $\\textit{own}$ my textbook, I have to give it back soon).\nI am also reading some introductory analysis, but any textbook which does not reference too much analysis without explanation would be good.\nIf linear algebra is not contained in the book, could one also direct me to a suitable text on that, please?\nThank you", "output": "Here are some of my suggestions.\n\nMake sure you are familiar with the material of Nicholson's book before you reading Foote's.\nIn my experience, it is not enough to read only once in abstract algebra.\nI suggest you study Fraleigh's book. \nYou need to clarify the difference between a ring with unity and a ring without unity. Nicholson defines a ring as having a unity. This assumption creates some confusion for me when I read Hungerford's Algebra after I reading Nicholson's book.\nThere are many advantages in Fraleigh's book. \n(a) Its exercises are in order from easy to difficult. \n(b) Fraleigh teaches readers many concepts in learning algebra.\nFor example, he says that: \"If you do not understand what the statement of a theorem means, \nit will probably be meaningless for you to read the proof (2/e p.xi).\"\nAnother example appears when he was teaching Lagrange's Theorem.\nHe says: \"Never underestimate results that count something. \nHe mentions this sentence many times throughout this book.\"\n(c) He compares theorems in group theory and ring theory.\n(d) He emphasizes the three most important theorems in basic ring theory (p.248). \n(e) He gives an excellent explanation of field extension.\nEspecially $\\Bbb{Q}(x)\\cong \\Bbb{Q}(\\pi)$(p.270).\nThe advantages of Foote and Dummit's book.\n(a) They give the relationship between field, E.D., P.I.D., U.F.D. and I.D. by an inculsion chain (3/e p.292).\n(b) They compare the notion in module and vector space by a table (p.408).\n(c) They give an excellent explanation of representation theory.\n(They show the similarity between $FG$-module and $F[x]$-module.\nThe disadvantages of Foote and Dummit's book.\n(a) They usually give their assumptions in the beginning of each section.\nThis convention often makes me wonder because when they state some theorems or exercises,\nthey omit the assumption. \n(b) They only give the algorithm of how to find the canonical rational form of a matrix.\nYou need to refer to Goodman's Algebra and Weintraub's Algebra to understand why the algorithm works.\nI recommend you read Hungerford's Algebra as an advanced text book.\n(a) It has the same level as Foote and Dummit's.\nHe clarifies many concepts which I had previously misunderstood. \nFor example, \nthe form of an ideal varies from ring to ring (p.123).\n(b) If there is a theorem which states the $P\\Rightarrow Q$, \nthen he always give an example why the reversion doesn't hold.\n(c) He discuss ring without unity. I think this is important for me in the advanced ring theory. See ch. IX. The structure of rings.\nIn summary, if you want to be familiar with abstract algebra, \nyou don't need to compare these books. Because in my opinion, \nyou should read all of them (even it is still not enough).\nFor linear algebra, I recommend Friedberg's book.\nYou can treat it as an easier version of Hoffman's.\nIf you want to learn linear algebra by more geometric interpretation or intuitive aspect, \nthen Anton's book is a good choice.", "meta": {"post_id": 1017434, "input_score": 32, "output_score": 42, "post_title": "How does Dummit and Foote's abstract algebra text compare to others?"}}
{"input": "I am currently reading through JC Lagarias' \"The $3x+1$ Problem and its Generalizations\" and have come across some notation reading :\n$$\\sup_{K \\ge 0} T^{(K)}(N)$$\nNow I assume that this means \"suppose that $K$ is greater than or equal to $0$\", however I want to understand any equations I am potentially going to use in a project and thus don't want to rely on assumptions. This may seem a very simple question but I appreciate any help.", "output": "Sup (\"supremum\") means, basically, the largest. So this:\n$$\\sup_{k\\ge0}T^{(k)}(N)$$\nrefers to the largest value $T^{(k)}(N)$ could get to as $k$ varies.\nIt's technically a bit different than the maximum\u2014it's the smallest number that is greater-than-or-equal to every number in the set.\nSo, for example, the interval $[0,1)$ has no maximum value, but $1$ is the supremum of the interval, because it's greater-than-or-equal-to everything in that interval, and because it's the smallest number with that property. (Note that $(0,1),(0,1],[0,1),[0,1]$ all have the same supremums.)", "meta": {"post_id": 1018350, "input_score": 51, "output_score": 89, "post_title": "\"sup\" in an equation"}}
{"input": "Theorem 1.2 of Bennett and Skinner (Canad. J. Math., 2004) asserts that the Diophantine equation $x^{p} - 4y^{p} = z^{2}$ is unsolvable for every prime $p \\geq 7.$\nThe following is a possible proof (from an arXiv author) that Fermat's Last Theorem is a consequence of this theorem, i.e., proof that if there exist integers $x, y, z > 0$ such that $(x, y) = 1$ and $x^{p} + y^{p} = z^{p},$ then there exist integers $a, b, c > 0$ such that $(a, b)=1$ and $a^{p} - 4b^{p} = c^{2}$.\nTake a prime\n$p \\geq 7.$\nWe will prove Fermat's Last Theorem in the form:\nTake integers $x, y, z > 0$.\nIf\n$(x, y) = 1,$\nthen\n$x^{p} + y^{p} \\neq z^{p}$.\nWe argue by contradiction.\nBy the equation\n$x^{p} + y^{p} = z^{p}$\nthere is a rational\n$0 < r < 1$\nsuch that\n$$x^{p} = rz^{p}\\ \\ \\mbox{and}\\ \\ y^{p} = (1-r)z^{p},$$\nso that\n$$r^{2} - r + \\dfrac{(xy)^{p}}{z^{2p}} = 0,$$\nand hence\n$$r = \\dfrac{1 + \\sqrt{1 - 4(xy)^{p}/z^{2p}}}{2}\\ \\ \\mbox{or}\\ \\  \\dfrac{1 - \\sqrt{1 - 4(xy)^{p}/z^{2p}}}{2}.$$\nTherefore,\nthe difference\n$1 - 4(xy)^{p}/z^{2p} \\geq 0$\nis to be a perfect square.\nBut since\n$$1 - \\dfrac{4(xy)^{p}}{z^{2p}} = \\dfrac{z^{2p} - 4(xy)^{p}}{z^{2p}},$$\nsince\n$z^{2p}$\nis a perfect square,\nand since if $z^{2p} = 4(xy)^{p}$\nthen from the equation\n$x^{p} + y^{p} = z^{p}$\nwe have\n$x = y$\nthat leads to a contradiction,\nso there is an integer\n$c > 0$\nsuch that\n$$z^{2p} - 4(xy)^{p} = c^{2}.$$\nOn choosing\n$$a := z^{2}\\ \\ \\mbox{and}\\ \\ b := xy$$\nwe have\n$$a, b > 0\\ \\ \\mbox{and}\\ \\ a^{p} - 4b^{p} = c^{2}.$$\nMoreover,\nbecause\n$(x, y) = 1$\nand\n$x^{p} + y^{p} = z^{p},$\nwe have\n$(x, y) = (y, z) = (x, z) = 1,$\nwhence\n$$(a, b) = 1.$$\nBut the existence of such\n$a, b, c$\ncontradicts Theorem 1.2 of Bennett and Skinner [1].", "output": "Your proof looks OK to me. But don't rush to publish $-$ it seems to me that Theorem 1.2 (available at this link) depends on the Taniyama\u2013Shimura\u2013Weil conjecture (or the modularity theorem, as it should now be called), just like Wiles' proof of Fermat's Last Theorem did.\nSo your proof is $-$ roughly speaking $-$ using a consequence of Fermat's Last Theorem to prove Fermat's Last Theorem.", "meta": {"post_id": 1035839, "input_score": 33, "output_score": 36, "post_title": "Checking a Proof of a Theorem"}}
{"input": "I started learning algebraic number theory, but it seems like all the sources I had are too abstract, giving me difficulty understanding the concept and tripping me up frequently. For today it is ramification (for a prime ideal).\n\nCan someone explain to me, with concrete example, the motivation and the intuition behind concept such as \"ramification index\", \"ramified/unramified\", \"totally ramified\", and \"tamely/wildly ramified\"?\nAlso, I saw something about ramified character in the context of Hecke's thesis, not sure if it have anything to do with this, if it does, can someone point out the relevance too?\n\nThank you very much.", "output": "Ok, this is certainly a non-trivially complicated question. \nI chose to give some sort of geometric intuition. I'm not totally sure this is what you were after, but hopefully it's of some use to you!\n\nGeometric prelude\nSo, the best place to start thinking about ramification, is in terms of maps of Riemann surfaces. While this may seem unrelated at first, bear through it, and I promise (hopefully!) it will make sense at the end.\nSo, a Riemann surface is something like a one-dimensional manifold, but instead of our space 'locally looking like an open subset of $\\mathbb{R}$' it locally looks like an open subset of $\\mathbb{C}$. Moreover, whereas 'locally looking like' in the context of real manifolds is meant in the smooth (i.e. $C^\\infty$) sense, in the case of Riemann surfaces, it's meant in the holomorphic (i.e. analytic) sense. \nGood examples of Riemann surfaces are things like the sphere $S^2$ for which at each point $p\\in S^2$ we can think of the sphere as locally looking like $\\mathbb{C}$. Moreover, as we change these charts (local ways of looking like $\\mathbb{C}$), the coordinates they give vary in a holomorphic way. Another example would be the torus $\\mathbb{C}/\\Lambda$ (for $\\Lambda\\subseteq\\mathbb{C}$ a lattice). \nNow, if we have two Riemann surfaces $X$ and $Y$, we can look at holomorphic maps $f:X\\to Y$. Given two points $x\\in X$ and $y\\in Y$ we can choose charts around $x$ and $y$, so homeomorphisms $U\\to X$ (whose image contains $x$), and $V\\to Y$ (whose image contains $y$), where $U,V\\subseteq \\mathbb{C}$ are open. We can then use these to think (locally around $x$/$y$) of $f$ as being just a holomorphic map $f:U\\to V$.\nNow, it's a common fact from the theory of one complex variable (sometimes called the 'local normal form theorem'), that says that there exists a unique integer $n$ such that for sufficiently small neighborhoods $W$ of $x$, the mapping $f:W\\to V$ looks like (i.e. up to pre/post composition with a biholomorphism is) the mapping $z\\mapsto z^n$ from the disc $D(0,1)$ to $\\mathbb{C}$;\nNow, let's think about what the $n^\\text{th}$-power map $D(0,1)\\to\\mathbb{C}$ looks like. Well, if we write an element of $D(0,1)$ as $z=r e^{i\\theta}$, for $r<1$ we see that the $n^\\text{th}$-power map send this to $r^n e^{i n\\theta}$. So, it shrinks the length of the vector $z$, and also multiples its angle by $n$. In particular, for $r\\ne 0$, we see that a $w= r e^{i\\theta n}\\in D(0,1)$ has exactly $n$ preimage points: $r^{\\frac{1}{n}}e^{i \\omega}$, where $\\displaystyle \\omega=\\frac{2\\pi \\theta}{m}$, for $m=1,\\ldots,n$. \nHowever, look what happens when $r=0$ (i.e. when $z=0$)--the $n^{\\text{th}}$-power map does nothing. If you think about the previous paragraph as saying that the $n^{\\text{th}}$-power map causes the disc to 'cover itself $n$ times', and each covering is a 'sheet', then all of these sheets meet at precisely one place--the fixed point $0$. So, if $n>1$, then somehow $0$ is this degenerate point which stops the $n^{\\text{th}}$-power map from being precisely a $n$-sheeted covering of itself. It ties all the sheets together, and since it's in multiple sheets, 'counts' for more than just a point, it counts for how many sheets it lies in.\nNow, let's bring this back to the original context of Riemann surfaces. This local analysis tells us that for our map of Riemann surfaces $f:X\\to Y$, and our points $x\\in X$ and $y\\in Y$, the mapping $f$ locally (around the points) looks something like some $n$-sheeted covering, here $n$ is the same as the one given to us by the local normal form theorem. But, as we said above, it's not precisely an $n$-sheeted covering (if $n>1$) since the point $x$ (which corresponds to $0$ in the disc) doesn't get covered $n$-times, it just stays stationery--it lies in $n$-sheets (opposed to the other points around it, which lie in $1$-sheet). Let's call this integer $e_x$--it's how many 'sheets' that $x$ lies inside of.\nNow, why is this integer something of importance to us. Well, first and foremost, it allows us to give a very simple answer to an (otherwise complicated) question:\n\nHow many points are in $f^{-1}(y)$, for $y\\in Y$?\n\nIn other words, how many points are in the preimage of a point in $Y$?\nNow, taken literally, this question is hard. The answer could really be anything, and greatly depends on which point $y\\in Y$ that we take. But, if we're clever, and tweak the question every so slightly, we get a much more satisfying answer.\nFor example, let's consider the $n^\\text{th}$-power map $g:D(0,1)\\to D(0,1)$ again. If we take an arbitrary point $y\\in D(0,1)$, we can ask how many points are in the preimage. Well, the answer breaks into two case:\n$$\\# g^{-1}(y)=\\begin{cases}1 & \\mbox{if}\\quad y=0\\\\ n & \\mbox{if}\\quad y\\ne 0\\end{cases}$$\nBut, if we use our intuition that we should count the point $0\\in D(0,1)$ not just as itself, but for the $n$ sheets it's contained in, we do get a constant answer of $n$.\nSaid differently, instead of asking for $\\#f^{-1}(y)$, let's instead ask for \n$$\\sum_{x\\in f^{-1}(y)}e_x$$\nso, we're not just asking for how many points map to $y$, but to weight these points by how many sheets there in. This makes the answer a constant number. This is a theorem you have to prove, but it's not hard. This map $X\\to \\mathbb{Z}$ sending $x$ to this sum can be shown to be locally constant (think about the local normal form theorem!), and so if $X$ is connected, must, in fact, be constant. \nSo, it clearly is nicer, and fits better the 'global properties', to not count total preimages, but these weighted preimages based on the 'multiplicities' $e_x$ of points. \nThus, the natural thing then is to study the points with multiplicity greater than $1$: $x$ such that $e_x>1$. These are the points where multiple sheets come together.\nAn important thing to notice is thatese points are  the points where $f:X\\to Y$ is NOT an isomorphism on a neighbhorhood of $x$ (think about the local normal form theorem again!). Translating this into a form which will be more useful later, they are the points where $f$ doesn't send charts (around $x$) to charts (around $y$).\nThese points, these 'bad' but significant points, are called the ramification points of $f:X\\to Y$.\n\nRelation to number theory\nSomewhat surprisingly, this is the motivating factor, and intuition, for the notion of ramified primes in algebraic number theory. \nJust to make sure we're on the same page, let's briefly recall the setup of what ramified vs. unramified primes look like. For the sake of convenience, I'm going to assume that you're working over number fields.\nSo, we have an extension of number fields $L/K$. We then have associated to this an extension of Dedekind domains $\\mathcal{O}_L/\\mathcal{O}_K$. For any non-zero prime $\\mathfrak{p}$ of $\\mathcal{O}_K$, we know that $\\mathcal{p}\\mathcal{O}_L$ (by integrality), and so we can factor this ideal as:\n$$\\mathcal{p}\\mathcal{O}_L=\\mathfrak{P}_1^{e_1}\\cdots\\mathfrak{P}_m^{e_m}$$\nWe say that $\\mathfrak{P}_i$ is ramified if $e_i>1$.\nNow, how are we going to relate this to the geometric setting of maps between Riemann surfaces? Well, the go between is the subject called algebraic geometry, which allows us to think about the set of prime ideals of $\\mathcal{O}_L$, and the set of prime ideals of $\\mathcal{O}_K$, as geometric objects unto themselves. \nIt isn't important if you are familiar with algebraic geometry, as long as you're willing to take (on faith) two things. To each ring $R$ (such as $\\mathcal{O}_K$ or $\\mathcal{O}_L$) a geometric object $\\text{Spec}(R)$, and to each ring map $f:R\\to S$, an associated map of geometric objects (notice the switch in direction!) $\\text{Spec}(S)\\to\\text{Spec}(R)$--on points, this map just takes a prime $\\mathfrak{p}$ of $S$ to the prime $f^{-1}(\\mathfrak{p})$ of $R$.\nSo, now to the inclusion of number rings $\\mathcal{O}_K\\hookrightarrow\\mathcal{O}_L$, we get an induced map of geometric spaces $\\text{Spec}(\\mathcal{O}_L)\\to \\text{Spec}(\\mathcal{O}_K$. So, now we're in a setup somewhat similar to the case of Riemann surfaces--a map between geometric objects. So, let's do what we did there.\nFix a point $\\mathfrak{p}$ of $\\text{Spec}(\\mathcal{O}_K)$, and let's ask about the set of points in the preimage of $\\mathfrak{p}$ under the map $\\text{Spec}(\\mathcal{O}_L)$. If you think about this for a minute, you'll see that the set of preimage points is EXACTLY the set $\\{\\mathfrak{P}_i\\}$ of primes which divide $\\mathfrak{p}\\mathcal{O}_L$. \nSo, we can start asking about how many points are in the preimage of some random $\\mathfrak{p}$. Just as before, the answer is hard to get at. There is no satisfactory/uniform way to answer this question as stated. And, moreover, just as before, this is because the 'true answer' comes from not counting literal points in the preimage, but counting them weighted by how many 'sheets' they lie in. As you may have already deduced, these points which are weighted greater than $1$ (the amount contributed just by them being a point), are precisely the primes over $\\mathfrak{p}$ that ramify!\nHow can we connect this notion of ramified prime to the one above though--why are they intuitively connected? Well, let's first do what we did above, and zoom in on the point $\\mathfrak{p}$ and one of its preimage points $\\mathfrak{P}_i$. In algebraic geometry, this corresponds (see here for some intuition) to localizing our rings at the respective primes. Thus, we're looking at the mapping $(\\mathcal{O}_K)_\\mathfrak{p}\\to (\\mathcal{O}_L)_{\\mathfrak{P}_i}$.\nNow, before, once we shrunk the neighborhoods around the points of our Riemann surfaces small enough, we took charts, and then asked about questions relative to these charts. Now, the objects $\\text{Spec}(\\mathcal{O}_K)$ and $\\text{Spec}(\\mathcal{O}_L)$ are '$1$-dimensional' (if you know commutative algebra, this is just the fact that the rings $\\mathcal{O}_L$ and $\\mathcal{O}_K$ are one-dimensional). So, charts should be like 'one map', which locally dictates the behavior of the space. For us, a chart of $\\text{Spec}(\\mathcal{O}_K)$ at $\\mathfrak{p}$ will correspond to a uniformizer $\\pi$ of $(\\mathcal{O}_K)_\\mathfrak{p}$, and similarly, a chart of $\\text{Spec}(\\mathcal{O}_L)$ at $\\mathfrak{P}_i$ will be a uniformizer $\\varpi$ of $(\\mathcal{O}_L)_{\\mathfrak{P}_i}$ (while this may not mean anything to you, this is more than just an analogy--for smooth curves over $\\mathbb{C}$, uniformizers literally become the charts of the analytifications).\nSo, we said above that what it mean for a point $x$ to be ramified, was that the map $f$ did NOT map charts at $x$ to charts at $y$. The same is true here, the chart $\\pi$ at $\\mathfrak{p}$ gets mapped to the object $\\pi\\in(\\mathcal{O}_L)_{\\mathfrak{P}_i}$. This element will be a chart (i.e. a uniformizer) if and only if $\\mathfrak{p}$ is unramified! In fact, up to units, $\\pi$ is $\\varpi^{e_i}$, where $e_i$ is the same $e_i$ as above! \nIn fact, if you think about local rings of Riemann surfaces, that notion of ramification point is precisely the same as uniformizers not going to uniformizers. Also, the amount of sheets $e_x$ is precisely the 'power' of a uniformizer at $y$ that a uniformizer at $x$ is sent to. These situations are precisely analogous.\nThus, we should picture somehow that $\\text{Spec}(\\mathcal{O}_L)\\to\\text{Spec}(\\mathcal{O}_K)$ is like a multi-sheeted covering, and, just as in the case of Riemann surfaces, the points of ramification are precisely the points where the sheets come together.\nIf the analogies are to hold, we'd like to also have an analogy, as alluded to above, of the fact that with our notion of ramification points, and keeping track of the number of sheets, that we can uniformly describe amount of points in the preimage. Not only do we have such a theorem, but it's one you are probably very familiar with:\n$$\\sum_{\\mathfrak{P}\\mid \\mathfrak{p}}\\text{ }e_\\mathfrak{P} f_\\mathfrak{P}=[L:K]$$\nIndeed, we already noted that $\\{\\mathfrak{P}:\\mathfrak{P}\\mid \\mathfrak{p}\\}$ is precisely the preimage of $\\mathfrak{p}$ under the map $\\text{Spec}(\\mathcal{O}_L)\\to\\text{Spec}(\\mathcal{O}_K)$, and that the $e_\\mathfrak{P}$ were the analogy of the multiplicity (or number of sheets) that showed up in the Riemann surface case. The only thing which is unexplained are the $f_\\mathfrak{P}$, and those are a holdover of 'hidden points'. That's a whole nother story. See my answer here for a taste.\n\nI should probably mention the things I didn't talk about. Totally ramified just means that there is one-preimage point where $[L:K]$ sheets come together. Tamely and wildly ramified is a more technical condition, which has less of a geometric flair (or, at least, not one as easily yielded by basic words in algebraic geometry).\nAs for the characters in Hecke's thesis, I would need you to be more specific about what type of characters they are. Are they grossencharacters, or characters of the (absolute) Galois group?", "meta": {"post_id": 1043073, "input_score": 13, "output_score": 34, "post_title": "Motivation and examples for ramification"}}
{"input": "If both the Poisson and Binomial distribution are discrete, then why do we need two different distributions?", "output": "The binomial distribution counts discrete occurrences among discrete trials.\nThe poisson distribution counts discrete occurrences among a continuous domain.\nIdeally speaking, the poisson should only be used when success could occur at any point in a domain.  Such as, for example, cars on a road over a period of time, or random knots in a string over a length, etc.  We are talking about infinitely many infinitesimally small trials, each having at most one success.\nIn practice, though, the poisson can be used to approximate the binomial under certain conditions, but it is only a rough approximation. Such as using the Normal curve in place of a Binomial under the right conditions.", "meta": {"post_id": 1050184, "input_score": 50, "output_score": 35, "post_title": "Difference between Poisson and Binomial distributions."}}
{"input": "Is there any continuous function defined on $(0, 1]$ whose range is $(0, 1)$?", "output": "**Answer without words! see the figure **", "meta": {"post_id": 1052470, "input_score": 21, "output_score": 44, "post_title": "Continuous function from $(0, 1]$ onto $(0, 1)$?"}}
{"input": "Suppose $f,g$ are two Riemann  Integrable functions .Is it true that $f\\circ g$ is also Riemann  Integrable?\nTrying this for a long time but not getting the answer", "output": "A function on a bounded interval is Riemann-integrable iff it is bounded and almost everywhere continuous. So the functions\n$$\nf(x) = \\begin{cases} 1 & \\text{ for }x \\ne 0 \\\\ 0 & \\text{ for } x = 0  \\end{cases}\n\\quad \\text{ and } \\quad\ng(x) = \\begin{cases} 1/q & \\text{ for }x=p/q \\\\ 0 & \\text{ for } x \\notin \\mathbb{Q}  \\end{cases}\n$$\nare Riemann-integrable over any bounded interval, since $f$ is continuous everywhere except at $0$, and $g$ is continuous at every irrational $x$. (In the definition $x=p/q$ is the unique representation of rational $x$ with $p$ and $q$ relatively prime integers and $q>0$.)\nThe composition of these functions is\n$$\nf(g(x)) = \\begin{cases} 1 & \\text{ for }x \\in \\mathbb{Q} \\\\ 0 & \\text{ for } x \\notin \\mathbb{Q}  \\end{cases}\n$$\nwhich is nowhere continuous, so not Riemann-integrable over any interval.", "meta": {"post_id": 1060834, "input_score": 32, "output_score": 82, "post_title": "Composition of two Riemann integrable functions"}}
{"input": "I'm a student just starting calculus in college, and my math skills are pretty stale.\nSo... how come finding limits using change of variable works?\nFor example:\n$$\\lim_{x \\to 1}\\frac{x\\cos(x-1) -1}{x-1}$$\nA way to solve this is to invent \"out of thin air\" $t = x-1$, and then the limit above is equal to:\n$$\\lim_{t \\to 0}\\frac{(t + 1)\\cos(t) - 1}{t}$$\nHow come this works?\nA limit is not an algebraic equation. what about domains of definition? \nWe are actually finding a different limit of a different function in a different place, how come they are equal (in general)?\n\njust to clarify I'm not asking about this specific example. I'm asking in general, when can you do this to find limits? when not? and why?", "output": "There is a very general result which guarantees such substitutions. Let $\\lim\\limits_{x \\to a}f(x) = L$ exist and let $\\lim\\limits_{t \\to b}g(t) = a$ exist and also assume that $g(t) \\neq a$ when $t$ is in a certain neighborhood of $b$ then $\\lim_{t \\to b}f(g(t)) = L$.\nPlease understand that the theorem is valid only under the conditions given in the above result and one of the first conditions is that $\\lim_{x \\to a}f(x)$ exists. If we don't know in advance whether the limit of $f(x)$ exists then how do we make a substitution $x = g(t)$ (in this question we put $x = t + 1$)?\nTo answer this we need to understand that the substitution $x = g(t)$ ($x = t + 1$) used here is invertible so that we have an inverse substitution $t = h(x)$ ($t = x - 1$) with $x = g(h(x)), t = h(g(t))$ which will allow us to infer the existence of limit $\\lim_{x \\to a}f(x)$ on the basis of existence of limit $\\lim_{t \\to b}f(g(t))$ via the theorem given in the beginning of this post.\nAnother condition which is very very important is to ensure that $g(t) \\neq a$ when $t$ is near $b$. Clearly this holds in the substitution used in the current question when $x = t + 1$ and $a = 1, b = 0$.\nIf we think deeply we will find that if $g(t)$ is invertible in the neighborhood of $t = b$ then it will automatically ensure that $g(t) \\neq a$ in a certain neighborhood of $b$. So in practice we use the following :\n\nTheorem: If $x = g(t)$ is an invertible function with inverse $t = h(x)$ in the deleted neighborhood of $t = b$ and $\\lim\\limits_{t \\to b}g(t) = a, \\lim\\limits_{x \\to a}h(x) = b$ then either both the limits $\\lim\\limits_{x \\to a}f(x)$ and $\\lim\\limits_{t \\to b}f(g(t))$ exist and are equal or both of them don't exist.\n\nNote that there is no condition on $f$ for the above theorem.", "meta": {"post_id": 1069642, "input_score": 39, "output_score": 37, "post_title": "Finding a limit using change of variable- how come it works?"}}
{"input": "Suppose $\\mathbb{A}^1$ and $\\mathbb{P}^1$ are affine space and projective space respectively. I'm not sure if it matters, but I don't mind if we assume that we're working over algebraically closed fields.\nI'm curious, is it possible to find a surjective, regular mapping $\\mathbb{A}^1\\to\\mathbb{P}^1$?", "output": "Yes it is possible to find a surjective, regular mapping $\\mathbb{A}^1\\to\\mathbb{P}^1$!    \nBy algebra\n$\\mathbb A^1\\to \\mathbb P^1:z\\mapsto[z:z^2+1] $\nBy geometry (better !)\nTake a ramified $2$-covering $f:\\mathbb P^1\\to \\mathbb P^1$, consider a non-critical point $a\\in \\mathbb P^1$ (easy: there are only two critical points for $f$ !) and the required surjective morphism is the restriction $$\\text{res}(f):\\mathbb P^1\\setminus \\{a\\}=\\mathbb A^1\\to \\mathbb P^1$$ By concrete geometry (best !)\nConsider the projection  $p$ of a smooth conic  $C\\subset \\mathbb P^2$ from a point $Q$ outside $C$ onto a line $L\\subset \\mathbb P^2$,  take a point $a\\in C$ such that the tangent line $\\Theta_aC\\subset \\mathbb P^2$ at $a$ does not pass through $Q$ (easy: there are only two such undesirable points!) and restrict the projection $p$ to the complement of $a$ to obtain the required surjective morphism $$\\text{res}(p):C\\setminus \\{a\\}=\\mathbb A^1 \\to L=\\mathbb P^1$$Confused? Make a drawing and just LOOK!", "meta": {"post_id": 1070860, "input_score": 37, "output_score": 53, "post_title": "Does there exist a regular map $\\mathbb{A}^1\\to\\mathbb{P}^1$ which is surjective?"}}
{"input": "I found that Kullback-Leibler loss, log-loss or cross-entropy is the same loss function. Is the logistic-loss function used in logistic regression equivalent to the cross-entropy function? If yes, can anybody explain how they are related?\nThanks", "output": "The relationship between Cross-entropy, logistic loss and K-L divergence is quite natural and immersed in the definition itself.\nCross-entropy is defined as:\n\\begin{equation}\nH(p, q) = \\operatorname{E}_p[-\\log q] = H(p) + D_{\\mathrm{KL}}(p \\| q)=-\\sum_x p(x)\\log q(x)\n\\end{equation}\nWhere, $p$ and $q$ are two distributions and using the definition of K-L divergence. $H(p)$ is the entropy of p. \nNow if $p \\in \\{y,1-y\\}$ and $q \\in \\{\\hat{y}, 1-\\hat{y}\\}$, we can re-write cross-entropy as:\n\\begin{equation}\nH(p, q) = -\\sum_x p_x \\log q_x =-y\\log \\hat{y}-(1-y)\\log (1-\\hat{y})\n\\end{equation}\nwhich is nothing but logistic loss.\nFurther, log loss is also related to logistic loss and cross-entropy as follows:\nExpected Log loss is defined as follows:\n\\begin{equation}\nE[-\\log q]\n\\end{equation}\nNote the above loss function used in logistic regression where q is a sigmoid function.\nExcess risk for the above loss function is defined as follows:\n\\begin{equation}\nE[\\log p - \\log q ]=E[\\log\\frac{p}{q}]=D_{KL}(p||q)\n\\end{equation}\nNotice that the K-L divergence is nothing but the excess risk of the log loss and K-L differs from Cross-entropy by a constant factor (see the first definition).\nOne important thing to remember is that we usually minimize the log loss instead of the cross-entropy in logistic regression which is not perfectly OK but it is in practice.", "meta": {"post_id": 1074276, "input_score": 44, "output_score": 50, "post_title": "How is logistic loss and cross-entropy related?"}}
{"input": "I was planning to write some article for the Mathematics magazine of our college and it occurred to me that it will be a good idea to write about the impact and importance of Set Theory. \nI plan writing an article that precisely explains why Set Theory is important in Mathematics. I plan to divide the article into two main sections. The first section would explain the Importance of Set Theory in Mathematics and the second section would explain the Impact of Set Theory in Mathematics. Though I realize that the second section logically belongs to the first but I plan to give a separate section just to emphasize the point. \nThe problem that I am facing now is lack of material. To accomplish my goal I need a great deal of material to read and so I want the help of my fellow Stack Exchange users. To be precise, my question is the following,\n\nWhat is the importance and impact of Set Theory in Mathematics?\n\nOr,\n\nWhy do we need to learn Set Theory?\n\n\nNote:-\nIf this question seems to be too broad to answer then please notify me about the problem in the comment. I will try my best to reword it to meet the community standard.\nEdit:- Along with all the answers that is provided below see this paper.", "output": "Why study set theory?\n\u00a0\u00a0\u00a0\u00a0 We like to think that mathematics developed from the need of our ancients to count things. I have four sheep, you have sixteen camels, my tribe has ten dozens of men, you have six hundred wives... etc. etc. But if you look closely, counting how many things you have of a certain type, first required you have be able and collect them into one collection. The \"collection of all sheep I have\", or the \"collection of men in my tribe\", and so on.\nSets came to solve a similar problem. Sets are collections of mathematical objects which themselves are mathematical objects. \nThis, of course, doesn't mean that we should learn set theory just for that purpose alone. The applications of set theory are not immediate for finite collections, or rather sufficiently small collections. We don't need to think about pairs or sets with five elements as particular objects. Whatever we want to do with them we can pretty much do by hand.\n\u00a0\u00a0\u00a0\u00a0 Sets come into play when you want to talk about infinite sets. Infinite sets collect infinitely many objects into one collection. The set of natural numbers, the set of finite sets of sets of sets of natural numbers, the set of sets of sets of sets of sets of sets of irrational numbers, etc. Once you establish that mathematical objects can be collected into other mathematical objects you can start analyzing their structure.\nBut here comes the problem. Infinite sets defy our intuition, which comes from finite sets. The many paradoxes of infinity which include Galileo's paradox, Hilbert's Grand Hotel, and so on, are all paradoxes that come to portray the nature of infinity as counterintuitive to our physical intuition.\n\u00a0\u00a0\u00a0\u00a0 Studying set theory, even naively, is the technical spine of how to handle infinite sets. Since modern mathematics is concerned with many infinite sets, larger and smaller, it is a good idea to learn about infinite sets if one wishes to understand mathematical objects better.\nAnd one can study, naively, a lot of set theory, especially under the tutelage of a good teacher that will actually teach axiomatic set theory in a naive guise. And this sort of learning can, and perhaps should, include discussions about the axiom of choice, about ordinals, and about cardinals. As Ittay said, and I'm agreeing ordinals and cardinals are two ways of counting, which extend beyond our intuitive understand that counting is done via the natural numbers, and allow us to count infinite objects.\nIf one couples these ideas with the basics of first-order logic, predicate calculus, and basic first-order logic, one understands how set theory can be used as a basis for modern mathematics. Which again, allows us to better see into some parts of mathematics.\n\u00a0\u00a0\u00a0\u00a0 Axiomatic set theory, on the other hand, is a mathematical field like any other. It has certain type of typical problems, and set theorists work in their typical or atypical ways to solve them, or at least understand them better. Axiomatic set theory does, however, handle the fine-grained problems that come from infinity better.\nWhy do I mean by that? A lot of the infinite sets in modern mathematics are countable or have size continuum. Rarely we run into larger sets (e.g. the set of all Lebesgue measurable sets is larger), but even then we rarely care about that. But now that we understand infinite sets better, we can ask questions like \"Given an abelian group with such and such properties, is it necessarily free [abelian]?\" usually we can prove these sort of theorems for countable objects, in this case countable groups, but not beyond that. \nSometimes we are interested in topology, which allows us to extend our control from countable objects to things that can be approximated \"in a good way\" with countable objects (like separable spaces). But even then we can ask questions which involved an arbitrary objects, and not necessarily one which has 'nice properties'.\nIt turns out that our lack of intuition for infinite sets is reflected in the lack of \"naively provable structure\" of infinite sets. We cannot even provably determine how many distinct cardinalities lie between the cardinality of $\\Bbb N$ and $\\Bbb R$. It might be none, or it might be one or two or many more. Here axiomatic set theory comes into play.\n\u00a0\u00a0\u00a0\u00a0 Axiomatic set theory deals with the additional axioms that we can require the set theoretic universe to have, and how they affect the structure of infinite sets. And this is the importance of set theory to mathematical research, as well. It deals with solving the existence or what sort of assumptions we need to prove, or disprove, the existence of certain objects.\nThese objects, while seemingly arbitrary, can have a great influence and strong effects on the structure of \"mathematically interesting sets\". For example, we know that every Borel set is Lebesgue measurable. But the continuous image of a Borel set need not be Borel. Is it Lebesgue measurable? It turns out that yes, but if we close the Borel sets under complements and continuous functions, will the resulting sets be Lebesgue measurable? Will they satisfy some sort of \"continuum hypothesis\"? Will they have the Baire property? And other questions, which are all quite natural, originated all sort of strange set theoretical objects and axioms which assert their existence.\nAnd if you ask me, that is why we should learn set theory, and what its importance is. It allows us to better understand infinite objects, and the assumptions needed to better control their behavior.", "meta": {"post_id": 1075320, "input_score": 25, "output_score": 45, "post_title": "Why do we need to learn Set Theory?"}}
{"input": "A very simple question.\nWe all know that there are no solutions to $x^3 + y^3 = z^3$ for integer $x$, $y$ and $z$, $xyz\\neq 0$, but are rational $x$, $y$ and $z$ possible?  Thanks.", "output": "No, because if $x=\\frac{a}{b}$, $y=\\frac{c}{d}$, and $z=\\frac{e}{f}$ were rational solutions, so that\n$$\\left(\\frac{a}{b}\\right)^3+\\left(\\frac{c}{d}\\right)^3=\\left(\\frac{e}{f}\\right)^3,$$\nthen there would be an integer solution\n$$(adf)^3+(bcf)^3=(bde)^3$$\n(edit: this is just writing out the details of Did's answer)", "meta": {"post_id": 1075400, "input_score": 25, "output_score": 39, "post_title": "Fermat's Last Theorem (Case n = 3) Question"}}
{"input": "I'm planning on self-studying linear algebra, and trying to decide on a book.  I'm thinking of using Hoffman and Kunze.\n What sort of experience is required to handle Hoffman and Kunze?\nSo far, I've read most of Axler's Linear Algebra Done Right. (It was for a class in high school, so we just worked through it and got as far as we got.) I feel like I understand it pretty well, and I really liked it, but I've read that it has a rather unusual approach and I would like to try something different.\nI've read that Hoffman and Kunze is good, but that it is heavy on the algebra.  I'm not sure how do calibrate that, though.  Does it mean \"Don't use it for linear algebra for engineers\" or \"You should have a year of algebra, but if you have that, it's not a big deal\".  (I guess it's somewhere in between.)\nI specifically like that it includes a strong emphasis on matrices, which are pointedly ignored in Axler, without devolving into being just a manual for computation.\nThis is my impression of the book from having read around (mostly here), but if something of it is wrong, please correct me.  I have very little experience to provide comparison and normalize the different recommendations I've read.", "output": "Hoffman & Kunze is to linear algebra what baby Rudin is to analysis.  If you plan to major in mathematics or physics, you should read it, study it, and do as many exercises from it as possible.  The amount of abstract algebra in H&K is minimal, and all the definitions/background is provided so that the text is self contained.  So if you were fine with Axler, you'll have no issue (though the prose in H&K is significantly dryer and more demanding than the conversational Axler, and may take some getting used to).\nThe reason why Axler's approach is \"unusual\" is that he doesn't use determinants in his presentation of eigenvalues, etc.  This is nice in theory and provides alternative proofs which can be illuminating, but ultimately the approach limits one's ability to perform concrete calculations.  Also, determinants cannot be banished forever in linear algebra, so at some point you need to learn about them, and the chapter in H&K covering them is excellent (though probably the most technically difficult chapter in the text).  Other than that, it's all basically standard fare and the level of difficulty is comparable to H&K, though H&K covers quite a bit more than Axler.", "meta": {"post_id": 1079266, "input_score": 20, "output_score": 41, "post_title": "Is Hoffman-Kunze a good book to read next?"}}
{"input": "The hypothetical relation is $z = \\mathrm{xor}\\left(x,y\\right)$ where xor is any bitwise operator such as AND, OR, NAND, etc. I see that these operations may be defined for integers trivially using binary-decimal conversion.\nIn the same way, can't we perform bitwise arithmetic on real numbers? For example, the following is $\\mathrm{xor}\\left(1.5, 2.75\\right)$:\n    01.01\nxor 10.11\n---------\n=   11.10\n\nThe answer is 3.5.\nWhat do the 3D plots of the binary bitwise operators look like, and what are some interesting mathematical properties? (e.g. gradient)\nBy the way, if you plot any of these using Sage, can I see the code? I couldn't get bitwise operations to work this way.", "output": "Basically, it looks like this:\n\n(Image rendered in POV-Ray by the author, using a recursively constructed mesh, some area lights and lots of anti-aliasing.)\nIn the picture, the blue square on the $x$-$y$ plane represents the unit square $[0,1]^2$, and the yellow shape is the graph $z = x \\oplus y$ over this square, where $\\oplus$ denotes bitwise $\\rm xor$.\nNote that this graph is discontinuous at a dense subset of the plane.  In the 3D rendering above, no attempt has been made to accurately portray the precise value of $x \\oplus y$ at the points of discontinuity, and indeed, it is not generally uniquely defined.  That is because the discontinuities occur at points where $x$ or $y$ is a dyadic fraction, and therefore has two possible binary expansions (e.g. $\\frac12 = 0.100000\\dots_2 = 0.011111\\dots_2$).\nAs can be seen from the picture, the graph is self-similar, in the sense that the full graph over $[0,1]^2$ consists of four scaled-down and translated copies of itself.  Indeed, this self-similarity is evident from the properties of the $\\oplus$ operation, namely that:\n\n$\\displaystyle \\frac x2 \\oplus \\frac y2 = \\frac{x \\oplus y}2$, and\n$\\displaystyle x \\oplus \\left(y \\oplus \\frac12\\right) = \\left(x \\oplus \\frac12\\right) \\oplus y = (x \\oplus y) \\oplus \\frac12$.\n\nThe first property implies that the graph of $x \\oplus y$ over the bottom left quarter $[0,1/2]^2$ of the square $[0,1]^2$ is a scaled-down copy of the full graph, while the second property implies that the graphs of $x \\oplus y$ in the other quarters are identical to the first quarter, except that the lower right and upper left ones are translated up by $\\frac12$.\nThe resulting fractal shape is also known as the Tetrix or the Sierpinski tetrahedron, and is a 3D analogue of the 2-dimensional Sierpinski triangle, which is also closely linked with the $\\rm xor$ operation \u2014 one way to construct approximations of the Sierpinski triangle is to compute  $2^n$ rows of Pascal's triangle using integer addition modulo $2$, which is equivalent to logical $\\rm xor$.\nIt may be surprising to observe that this fully 3-dimensional fractal shape is indeed (at least approximately, ignoring the pesky multivaluedness issues at the discontinuities) the graph of a function in the $x$-$y$ plane.  Yet, when viewed from above, each of the four sub-tetrahedra indeed precisely covers one quarter of the full unit square (and each of the 16 sub-sub-tetrahedra covers one quarter of a quarter, and so on...).", "meta": {"post_id": 1080223, "input_score": 29, "output_score": 44, "post_title": "What do bitwise operators look like in 3d?"}}
{"input": "I am self-studying analysis and ran across this: \n$\\mathbb R \\setminus \\mathbb N$ is an open subset of $\\mathbb R$\nMy best guess for interpretation was this:\nthe set $\\mathbb R \\setminus \\mathbb N$ is an open subset of $\\mathbb R$. \nwhich doesn't mean much to me. Can anyone clear this up a bit? I know that the 'divided by' symbol is usually a slash in the opposite direction. And I am unsure how I would divide the reals by the naturals anyway.", "output": "It\u2019s set theoretic complement and in this case it denotes the set of all reals which are not natural:\n$$\u211d \\setminus \u2115 = \\{x \u2208 \u211d;~x \\notin \u2115\\}$$", "meta": {"post_id": 1080987, "input_score": 30, "output_score": 47, "post_title": "Meaning of the backslash operator on sets"}}
{"input": "Inspired the various** algebraic X'mas greetings sent to me over the festive period, I thought I would try to devise one of my own. \n$$\\Large \\color{red}{\\sum_{i=a-1}^{r-1}}\\color{green}{\\sum_{j=s-1}^{r-1}}\\color{orange}{\\binom {e-x}{m-x}}\\color{red}{\\binom ex}\\color{orange}{ \\binom i{a-1}}\\color{green}{\\binom j{s-1}}\\color{red}{\\binom y{\\prod_{k=1}^{2014}k}}\\\\\n$$\nThe colours are purely ornamental!\n** Actually there were only two versions: one was an equation with a $\\ln$ function and the other required knowledge of Newton's second law; both of these have popped up in various places on web as well.", "output": "$$\\large\\begin{align}\n& \\color{red}{\\sum_{i=a-1}^{r-1}}\\color{green}{\\sum_{j=s-1}^{r-1}}\n\\color{orange}{\\binom {e-x}{m-x}}\\color{red}{\\binom ex}\\color{orange}{ \\binom i{a-1}}\n\\color{green}{\\binom j{s-1}}\\color{red}{\\binom y{\\prod_{k=1}^{2014}k}}\\\\\n&=\\color{orange}{\\binom {e-x}{m-x}}\\color{red}{\\binom ex}\\color{red}{\\binom y{\\prod_{k=1}^{2014}k}}\\color{red}{\\sum_{i=a-1}^{r-1}}\n\\color{orange}{ \\binom i{a-1}}\\color{green}{\\sum_{j=s-1}^{r-1}}\\color{green}{\\binom j{s-1}}\\\\\n&=\\color{red}{\\binom ex}\\color{orange}{\\binom {e-x}{m-x}}\n\\color{red}{\\binom y{\\prod_{k=1}^{2014}k}}\\color{orange}{ \\binom ra}\\color{green}{\\binom rs}\\\\\n&=\\color{red}{\\binom em}\\color{orange}{\\binom mx}\\color{red}{\\binom y{2014!}}\n\\color{orange}{ \\binom ra}\\color{green}{\\binom rs}\\\\\n&=\\color{orange}{\\binom mx}\\color{red}{\\binom em}\\color{orange}{ \\binom ra}\n\\color{green}{\\binom rs}\\color{red}{\\binom y{2014!}}\n\\end{align}$$\nMerry Xmas, everyone!!!", "meta": {"post_id": 1081270, "input_score": 33, "output_score": 46, "post_title": "Xmas Maths 2014"}}
{"input": "Could anyone help me to find the mistake in the following problem? Based on the formula of the sum of a geometric series:\n\\begin{equation}\n1 + x + x^{2} + \\cdots + x^{n} + \\cdots = \\frac{1}{1 - x}\n\\end{equation}\n\\begin{equation}\n1 + \\frac{1}{x} + \\frac{1}{x^{2}} + \\cdots + \\frac{1}{x^{n}} + \\cdots = \\frac{1}{1 - 1/x} = \\frac{x}{x-1}\n\\end{equation}\nAdding both equations\n\\begin{equation}\n2 + x + \\frac{1}{x} + x^{2} + \\frac{1}{x^{2}} + \\cdots + x^{n} + \\frac{1}{x^{n}} + \\cdots = \\frac{1}{1 - x} + \\frac{x}{x-1} = \\frac{1-x}{1-x} = 1\n\\end{equation}\nSo,\n\\begin{equation}\n2 + x + \\frac{1}{x} + x^{2} + \\frac{1}{x^{2}} + \\cdots + x^{n} + \\frac{1}{x^{n}} + \\cdots = 1\n\\end{equation}\nAnd the left side is always bigger than $2$ for $x>0$. \nWhat is wrong?? Thanks in advance", "output": "The first series only applies when $|x| < 1$ whereas the second series only applies when $\\left|\\frac{1}{x}\\right| < 1$ (i.e. $|x| > 1$). By adding them, you are assuming that they both apply simultaneously, but they don't (for any $x$).", "meta": {"post_id": 1081892, "input_score": 22, "output_score": 38, "post_title": "What is wrong with the sum of these two series?"}}
{"input": "Let us define a properly discontinuous action of a group $G$ on a topological space $X$ as an action such that every $x \\in X$ has a neighborhood $U$ such that $gU \\cap U \\neq \\emptyset$ implies $g = e$. I would like to prove that this property is equivalent to, having given $G$ the discrete topology and in the $X$ locally compact Hausdorff case, the map $G \\times X \\rightarrow X \\times X$ given by $(g, x) \\mapsto (x, gx)$ being proper (i.e. closed and preimage of compact sets is compact) plus the action being free.\nI have managed to prove one direction, that is, if the action is proper and free with $G$ having the discrete topology then it is properly discontinuous. I'm having trouble though with the other direction. Here is an attempt: let's denote by $\\rho : G \\times X \\rightarrow X \\times X$ the map $\\rho(g, x) = (x, gx)$. Suppose $K \\subset X \\times X$ is compact. We wish to show $\\rho^{-1}(K)$ is compact. Let $(g_i, x_i)$ be a net in $\\rho^{-1}(K)$. Then $\\rho(g_i, x_i) = (x_i, g_i x_i)$ admits a convergent subnet, so passing to it we may assume $x_i \\rightarrow x$ and $g_i x_i \\rightarrow y$. Essentially we must now find a way to prove $g_i$ converges, but I can't seem to do this. Any hints?", "output": "These properties are not equivalent. Here's a counterexample: Let $X=\\mathbb R^2\\smallsetminus\\{(0,0)\\}$, and define an action of $\\mathbb Z$ on $X$ by $n\\cdot (x,y) = (2^n x, 2^{-n} y)$. This is properly discontinuous by your definition, but it's not a proper action. The subset $K \\times K \\subseteq X\\times X$ is compact, where $K = \\{(x,y): \\max(|x|,|y|)=1\\}$,   but $\\rho^{-1}(K\\times K)$ contains the sequence $(n, (2^{-n},1))$, which has no convergent subsequence. \nI think one reason for your confusion is that different authors give different definitions of \"properly discontinuous.\" Topologists concerned primarily with actions that determine covering maps often give the definition you gave:\n\n(i) Every $x \\in X$ has a neighborhood $U$ such that $gU \\cap U \\neq \\emptyset$ implies $g = e$.\n\nThis is necessary and sufficient for the quotient map $X\\to X/G$ to be a covering map. However, in order for the action to be proper (and thus for the quotient space to be Hausdorff), an additional condition is needed:\n\n(ii) If $x,x'\\in X$ are not in the same $G$-orbit, then there exist neighborhoods $U$ of $x$ and $U'$ of $x'$ such that $gU\\cap U' = \\emptyset$ for all $g\\in G$.\n\nWhen $X$ is a locally compact Hausdorff space and $G$ is a discrete group acting freely on $X$, the action is proper if and only if both conditions (i) and (ii) are satisfied. Differential geometers, who are typically concerned with forming quotient spaces that are manifolds, are more apt to define \"properly discontinuous\" to mean both (i) and (ii) are satisfied. \nBecause of this ambiguity (and because the term \"properly discontinuous\" leads to oxymoronic phrases such as \"a continuous properly discontinuous action\"), Allen Hatcher in his Algebraic Topology coined the term covering space action for an action satisfying condition (i). I've adopted that terminology, and I use free and proper action for an action satisfying (i) and (ii) (at least for locally compact Hausdorff spaces). I sincerely hope the term properly discontinuous will eventually die out. \nYou can find more about these issues in the second editions of my books Introduction to Topological Manifolds (Chapter 12) and Introduction to Smooth Manifolds (Chapter 21).", "meta": {"post_id": 1082834, "input_score": 68, "output_score": 119, "post_title": "Properly discontinuous action: equivalent definitions"}}
{"input": "Prove that a function $f:\\mathbb{R}\\to\\mathbb{R}$ which satisfies\n$$f\\left({\\frac{x+y}3}\\right)=\\frac{f(x)+f(y)}2$$\nis a constant function.\nThis is my solution: constant function have derivative $0$ for any number, so I need to prove that $f'$ is always $0$. I first calculated $\\frac{d}{dx}$ and then $\\frac{d}{dy}$:\n$$f'\\left({\\frac{x+y}3}\\right)\\frac13=\\frac{f'(x)}2$$\n$$f'\\left({\\frac{x+y}3}\\right)\\frac13=\\frac{f'(y)}2$$\nFrom this I can see that $\\frac{f'(x)}2=\\frac{f'(y)}2$. Multiplying by $2$ and integrating I got:\n$$f(x)=f(y)+C$$\nfor some constant $C\\in\\mathbb{R}$. By definition of $f$ it is true for any $x,y\\in\\mathbb{R}$, so I can write\n$$f(y)=f(x)+C$$\nAdding this two equation and simplifying I got\n$$C=0$$\nso $f(x)=f(y)$ for all $x,y\\in\\mathbb{R}$. Is my solution mathematically correct. Is this complete proof, or I missed something?", "output": "It is not even necessary to assume that $f$ is continuous.\n\nBy letting $y = 2x$, we see that $f(x) = f(2x)$\nLetting $y = -4x$, we get $f(-x) = \\frac{f(x) + f(-4x)}{2}$.  However, from (1), $f(-4x) = f(-2x) = f(-x)$, so this simplifies to $f(-x) = f(x)$\nFinally, let $y = -x$ and simplifying gives $2f(0) = f(x) + f(-x)$.  Substituting in from (2), this becomes $f(0) = f(x)$.  Since this holds for all $x \\in \\mathbb{R}$, we conclude that $f$ must be constant.", "meta": {"post_id": 1093227, "input_score": 21, "output_score": 53, "post_title": "A function that satisfies $f\\left({\\frac{x+y}3}\\right)=\\frac{f(x)+f(y)}2$ must be a constant"}}
{"input": "In my course on linear PDEs, the professor used $H^{1/2}$ without defining it, and I have been looking on google trying to find a definition, but the only related thing I found was $H^{-1/2}$ as being the dual space to $H^{1/2}$ which does not really help. Plugging in the one half in the defintion of the standard Sobolev spaces $H^m$ does not make any sense. Could someone quickly help me out there?\nThank you.", "output": "$\\newcommand{\\tr}{\\operatorname{tr}}$\nThere are multiple definitions of $H^{1/2}(\\partial \u03a9)$ which are equivalent if the boundary is regular enough (Lipschitz continuous). The technically simplest, and how it usually appears in lectures on weak solutions for partial differential equations, is as the range of the trace operator $\\tr \\colon H^1(\u03a9) \\to L^2(\\partial \u03a9)$:\n$$ \\begin{align}H^{1/2}(\\partial \u03a9) &:= \\tr(H^1(\\Omega)) := \\{ v \\in L^2(\\partial \u03a9) \\;|\\; \\exists u \\in H^1(\u03a9) : \\tr(u) =v \\},\\\\\n\\| v \\|_{H^{1/2}(\\partial \u03a9)} &:= \\inf \\{ \\| \\tilde u \\|_{H^1(\u03a9)} \\;|\\; \\tilde u \\in H^1(\u03a9) \\land \\tr(\\tilde u) = v \\}.\\end{align}$$\nThe definition of the norm arises as follows. By the First isomorphism theorem for Banach Spaces, the trace operator induces an isomorphism\n$$ \\begin{align}\\widehat{\\tr}\\colon H^1(\u03a9) / \\operatorname{ker} \\tr &\\to \\tr(H^1(\\Omega)), \\\\\n[u] &\\mapsto \\tr(u)\\end{align} $$\nwhere $\\operatorname{ker} \\tr$ is the kernel of the trace operator, $[u] \\in H^1(\u03a9) / \\operatorname{ker} \\tr$ denotes an equivalence class with representative $u \\in H^1(\u03a9)$ and the norm on the quotient space is given by\n$$ \\| [u] \\|_{H^1(\u03a9) / \\operatorname{ker} \\tr} := \\inf \\{ \\| \\tilde u \\|_{H^1(\u03a9)} \\;|\\; \\tilde u \\in [u] \\}. $$\nThis is a general construct for the quotient norm on Banach spaces. As a side remark, there holds $\\operatorname{ker} \\tr = H^1_0(\u03a9)$ (the latter space is defined as completion of $C^\\infty_0(\\Omega)$ in $H^1(\\Omega)$). One can then define a norm on $H^{1/2}(\\partial \u03a9)$ using $\\widehat{\\tr}$:\n$$ \\| v \\|_{H^{1/2}(\\partial \u03a9)} := \\| \\widehat{\\tr}^{-1}(v) \\|_{H^1(\u03a9) / \\operatorname{ker} \\tr} = \\inf \\{ \\| \\tilde u \\|_{H^1(\u03a9)} \\;|\\; \\tilde u \\in \\widehat{\\tr}^{-1}(v) \\}$$\nusing that $\\tilde u \\in \\widehat{\\tr}^{-1}(v)$ if and only if $\\tilde u \\in H^1(\\Omega)$ and $\\tr(\\tilde u) = v$ one arrives at the expression of the norm given in the beginning.\nThis definition of $H^{1/2}(\\partial \u03a9)$ is not very useful if one wishes to check whether a specific function $v \\in L^2(\\partial \u03a9)$ is in $H^{1/2}(\\partial \u03a9)$ and it does not explain the name $H^{1/2}(\\partial \u03a9)$ (which came later historically).\nThe other definition of $H^{1/2}(\\partial \u03a9)$ I present here is quite technical in detail as $\\partial \u03a9$ is a $(n-1)$-dimensional manifold. In case $\\partial \u03a9$ is a plane you have $\\partial \u03a9 \\cong \\mathbb R^{n-1}$ and you end up having to define $H^{1/2}(\u03a9')$ for $\u03a9' \\subset \\mathbb R^{n-1}$. For a general Lipschitz boundary you can \"straighten\" your boundary locally to look like a plane (this is a general technique while working with manifolds) and in the end you ask for a transformation of your boundary function to be in $H^{1/2}(\u03a9')$. (See [1] for details.)\nAll in all, you end up having to define $H^{1/2}(\u03a9')$. There are multiple ways for doing that, one using the H\u00f6lder-like seminorms as mentioned by Thom\u00e1s, one using the Fourier coefficients (see Fractional Sobolev Spaces on  Wikipedia) and one using interpolation between $L^2(\u03a9')$ and $H^1(\u03a9')$ (see [1] again).\nFor understanding the actual behavior of functions in $H^{1/2}(\u03a9')$ the definition using the H\u00f6lder-like norm (Sobolev-Slobodeckij norm) is probably the best:\n$$H^{1/2}(\u03a9') = \\left\\{ v \u2208 L^2(\u03a9') \\;\\middle|\\; \\| v \\|_{L^2(\u03a9')} + \\int_{\u03a9'}\\int_{\u03a9'}\\frac{|v(x)-v(y)|^2}{|x-y|^{n+1}} dx \\, dy < \\infty \\right\\}$$\nNote that the additional integral term is somewhat like a H\u00f6lder condition. I like to think of $H^1(\u03a9) \\subset H^{1/2}(\u03a9) \\subset L^2(\u03a9)$ as something analogous to $C^1(\u03a9) \\subset C^{1/2}(\u03a9) \\subset C^0(\u03a9)$ in terms of regularity. That this is really analogous can be made precise using interpolation theory, which allows one to define spaces $H^s(\u03a9)$ for any $0 < s < 1$ \"in-between\" $L^2(\\Omega)$ and $H^1(\u03a9)$, where the trace space appears as special case for $s = 1/2$.\nThe only source claiming the equivalence of the norms I know of is [2], but my Italian is not sufficient to follow the argument.\n[1] Lions, J. L., & Magenes, E. (1972). Non-Homogeneous Boundary Value Problems and Applications.\n[2] Gagliardo, E. (1957). Caratterizzazioni delle tracce sulla frontiera relative ad alcune classi di funzioni in n variabili. Rendiconti Del Seminario Matematico Della Universit\u00e0 Di Padova, 27, 284\u2013305.", "meta": {"post_id": 1095246, "input_score": 27, "output_score": 35, "post_title": "The Sobolev Space $H^{1/2}$"}}
{"input": "If $G$ is a group of even order, prove it has an element $a \\neq e$ satisfying $a^2 = e$.\nMy proof:\nLet $|G| = 2n$. Since $G$ is finite, there exists, $a \\in G$ such that $a^p = e$ and by Lagrange's Theorem, p divides 2n. By Euclid's lemma, since p does not divide 2, p divides n. Let $n = pk$. Hence, $(a^n)^2 = (a^{pk})^2 = ((a^p)^k)^2 = (e^k)^2 = e$. Therefore, $a^n$ is an element that satisfy the condition.\nIs my solution OK?\nFor this problem, I am just wondering how I can solve this problem without using Lagrange's Theorem, as this problem is an exercise before the Lagrange's Theorem was taught.", "output": "The following is perhaps one of most simple proofs:\nPair up if possible each element of $\\;G\\;$ with its inverse, and observe that\n$$g^2\\neq e\\iff g\\neq g^{-1}\\iff \\;\\text{there exists the pair}\\;\\;(g, g^{-1})$$\nNow, there is one element that has no pairing: the unit $\\;e\\;$ (since indeed $\\;e=e^{-1}\\iff e^2=e$), so since the number of elements of $\\;G\\;$ is even there must be at least one element more, say $\\;e\\neq a\\in G\\;$ ,  without a pairing, and thus $\\;a=a^{-1}\\iff a^2=e\\;$", "meta": {"post_id": 1111868, "input_score": 27, "output_score": 84, "post_title": "If $G$ is a group of even order, prove it has an element $a\\neq e$ satisfying $a^2=e$."}}
{"input": "The intuition behind homology may be summarized in a sentence: to find objects without boundary which are not the boundary of an object. This has geometric meaning and explains the algebraic boundary operator $\\partial$ - quotient of vector spaces procedure.\nOn the other hand, the definition of de Rham cohomology comes always unprovided of such intuitive approach. My question is: how may be intuitively understood de Rham cohomology?\n(Please see this related question)", "output": "EDIT: An extended version of this answer and further discussion may be found here\n\nThis is a way to explain the intuition behind de Rham cohomology:\nCohomolgy comes up as a dual answer to homology. Homology identifies the shape of an object finding \u2018holes\u2019. More concretely, it looks for objects without boundary which are not the boundary of an object (and therefore the definition $H_k(M)=\\text{ker}\\partial_n/\\text{im}\\partial_{n+1}$).\n\nCohomology works in a completely different fashion. Instead of looking for subspaces detecting holes, cohomology assigns a real value to each object in our space. For example, in $\\mathbb{R}^2$ we may assign to each curve (oriented, with startpoint and endpoint) the value of the $x$-projection. When the curve moves rightwards, gains projection, whereas loses projection when moving leftwards; that\u2019s good if we want our assignment additive and differentiable: if our curve is divided into several pieces, then it is the same to calculate the value of the whole curve or to add the values of the small ones.\n\nThe previous example is actually rather simple; it does not matter the whole curve, only the $x$-coordinates of the startpoint and endpoint, from which we calculate the difference. Indeed a closed curve has always zero value. Let\u2019s consider a less obvious example. In $\\mathbb{R}^2$ we have a vector field, $f(x,y)=(y,0)$. We may perform the following assignment: each curve $\\gamma$ has value the circulation integral $\\int_{\\gamma}f$. As the picture shows, the value does not depend only on the startpoint and endpoint, because the curves that go up and then go down have a positive circulation, but the curves that go down and then go up have a negative circulation. Moreover, a closed curve has nonzero circulation (in general); in the picture, the small closed curve has slightly negative circulation, because on top it goes in the opposite direction to the field and on bottom it is in the field direction, but the field is stronger in the top.\n\nA third example: in $\\mathbb{R}^2\\smallsetminus (0,0)$ we consider the assignment swept out central angle. This example resembles the first one. The important data is the start angle and the end angle. And that\u2019s why a little closed curve has zero swept angle. But pay attention! In the second picture there is a curve that encloses the origin and that sweeps an angle of $2\\pi$, contrary to what we thought about closed curves. This phenomenon is only possible if there are holes in the topological space: we have given zero value to small closed curves, those which are the boundary of a little disc, but other values are allowed for big closed curves, those which perhaps are not the boundary of anything. It is as if we had given values to different homological objects: 0 to the curves not enclosing the origin, $2\\pi$ to those which circle the origin once, $4\\pi$ to those which circle the origin twice, and so on.\n\nAs stated, we want our assignment additive. Therefore we only need to know the value we would assign to, say, little curves, little surface pieces or little volumes. This is done by means of a differential form. Differential forms a local valuation in each point and each direction. \nDifferential forms language is well suited for describing these phenomena. The previous three examples are described by three 1-forms in $\\mathbb{R}^2$: $\\alpha_1=\\mathrm{d}x$, $\\alpha_2=y\\mathrm{d}x$ and $\\alpha_3=\\frac{-y}{x^2+y^2}\\mathrm{d}x+\\frac{x}{x^2+y^2}\\mathrm{d}y$ (please note that the last one is not defined at the origin). De Rham cohomology studies these differential forms and a so called exterior derivative $\\mathrm{d}$. In the first and third cases, $\\mathrm{d}\\alpha_1=\\mathrm{d}\\alpha_3=0$ and that\u2019s why small closed curves have zero value; we say that $\\alpha_1$ and $\\alpha_3$ are closed forms. $\\mathrm{d}\\alpha_2\\neq 0$, so $\\alpha_2$ is not a closed form. On the other hand, $\\alpha_1$ is exact: $\\alpha_1=\\mathrm{d}(x)$, so $x$ is the function to be evaluated in the startpoint and the endpoint, and that\u2019s the reason why large closed curves have value zero, because they have the same initial and final point. $\\alpha_3$ is not exact; we would be delighted to say that $\\alpha_3=\\mathrm{d}(angle)$, but there is not such $angle$ function defined in all $\\mathbb{R}^2\\smallsetminus (0,0)$, we always fall into $2\\pi$ steps. \nSo for our cohomological search of holes, we must find closed forms which are not exact.", "meta": {"post_id": 1112419, "input_score": 57, "output_score": 79, "post_title": "Intuitive Approach to de Rham Cohomology"}}
{"input": "When we say $f \\in C^1$, we mean that $f$ is continuously differentiable. Isn't the continuity a redundant word? I mean, we have a theorem that says if $f$ is differentiable then it is continuous. So why in most of the textbooks they always mention them two?\nSo these are all equivalent:\n\n$f \\in C^1$\n$f$ is continuously differentiable\n$f'$ exists", "output": "No, they are not equivalent.\nA function is said to be differentiable at a point if the limit which defines the derivate exists at that point. However, the function you get as an expression for the derivative itself may not be continuous at that point. A good example of such a function is $$f(x) = \\begin{cases}x^2(\\sin(\\frac{1}{x^2})) &\\quad x \\neq 0 \\\\ 0 &\\quad x = 0 \\end{cases}$$ which has a finite derivative at $x=0,$ but the derivative is essentially discontinuous at $x=0.$\nA continuously differentiable function $f(x)$ is a function whose derivative function $f'(x)$ is also continuous at the point in question.\nIn common language, you move the secant to form a tangent and it may give you a real tangent at that point, but if you see the tangents around it, they will not seem to be approaching this tangent in any sense. Might sound counter intuitive, but it is possible. Such a function is not a continuously differentiable.", "meta": {"post_id": 1117323, "input_score": 40, "output_score": 39, "post_title": "The definition of continuously differentiable functions"}}
{"input": "I read through similar questions, but I couldn't find an answer to this:\nHow do you determine the symmetric matrix A if you know:\n$\\lambda_1 = 1, \\  eigenvector_1 = \\pmatrix{1& 0&-1}^T;$\n$\\lambda_2 = -2, \\ eigenvector_2 = \\pmatrix{1& 1& 1}^T;$\n$\\lambda_3 = 2,  \\ eigenvector_3 = \\pmatrix{-1& 2& -1}^T;$\nI tried to solve it as an equation system for each line, but it didn't work somehow.\nI tried to find the inverse of the eigenvectors, but it brought a wrong matrix.\nDo you know how to solve it?\nThanks!", "output": "Writing the matrix down in the basis defined by the eigenvalues is trivial.  It's just\n$$\nM=\\left(\n\\begin{array}{ccc}\n  1 & 0 & 0 \\\\\n  0 & -2 & 0 \\\\\n  0 & 0 & 2\n\\end{array}\n\\right).\n$$\nNow, all we need is the change of basis matrix to change to the standard coordinate basis, namely:\n$$\nS = \\left(\n\\begin{array}{ccc}\n 1 & 1 & -1 \\\\\n 0 & 1 & 2 \\\\\n -1 & 1 & -1 \\\\\n\\end{array}\n\\right).\n$$\nThis is just the matrix whose columns are the eigenvectors.  We can change to the standard coordinate bases by computing $SMS^{-1}$.  We get\n$$\nSMS^{-1} = \\frac{1}{6}\\left(\n\\begin{array}{ccc}\n 1 & -8 & -5 \\\\\n -8 & 4 & -8 \\\\\n -5 & -8 & 1 \\\\\n\\end{array}\n\\right).\n$$\nYou can check that this matrix has the desired eigensystem.  For example,\n$$\n\\frac{1}{6}\\left(\n\\begin{array}{ccc}\n 1 & -8 & -5 \\\\\n -8 & 4 & -8 \\\\\n -5 & -8 & 1 \\\\\n\\end{array}\n\\right)\n\\left(\n  \\begin{array}{c}\n    -1 \\\\ 2 \\\\ -1\n  \\end{array}\n\\right)\n=\n\\left(\n  \\begin{array}{c}\n    -2 \\\\ 4 \\\\ -2\n  \\end{array}\n\\right).\n$$", "meta": {"post_id": 1119668, "input_score": 28, "output_score": 43, "post_title": "Determine a matrix knowing its eigenvalues and eigenvectors"}}
{"input": "I am trying to get an explanation in words, or math, of what the $d\\mu$ means in an integration statement. Such as:\n$$\\int f \\ d\\mu$$ \nHow does the measure change our old \"calculus\" notion of integration? What is going on here that is different?", "output": "I actually struggled with this concept in grad school since I was studying applied math and was sort of thrust into higher level theory without building it up rigorously like I assume would be done in a pure math program.\nIf we are integrating over a space $X$, I sometimes prefer the notation $\\int_X f(x)\\mu(dx)$. I like to think of it as splitting the space we are integrating over into infinitesimal pieces, but we have to take the measure of those infinitesimal pieces as they may not all be identical under $\\mu$. I'm used to working with probability measures, and at least in that case, you can often think of it similar to the way Lebesgue and Riemann integration are developed.\nCreate a disjoint partition $X=\\cup_{k=1}^N A_k$, and define the sum which will approximate the integral using appropriately chosen sample points $x_k\\in A_k$.\n$$\\int_X f(x)\\mu(dx) \\approx \\sum_{k=1}^N f(x_k) \\mu(A_k).$$\nIdeally, taking the limit as $N\\rightarrow\\infty$ (carefully refining the partition and choosing appropriate sample points as $N$ increases) will make the sum converge to the integral. What we are doing here is effectively approximating the function $f$ with a simple function which is constant on a finite collection of measurable sets.", "meta": {"post_id": 1123045, "input_score": 29, "output_score": 39, "post_title": "Integration with respect to a measure"}}
{"input": "Versal Property \nLocal Deformation Space\nMini-versal deformation space\n\nI came across these words while studying these papers a) Desingularization of moduli varities for vector bundles on curves, Int. Symp on Algebraic Geometry by C. S. Seshadri and b) Cohomology of certain moduli spaces of vector bundles  Proc. Indian Acad. Sci. by V. Balaji \nSo, I Googled and I ended up trying to understand Deformation Theory. \nI have tried reading few lecture notes, for example:\n1)  Notes on Deformation Theory   by Nitin Nitsure\n2) Deformation Theory by M. Doubek, M. Markl and P. Zima \n3) A glimpse on Deformation theory by Brian Osserman \n4) Robin Hartshorne's book on Deformation Theory\nNothing helped me to understand what is deformation theory actually. \nI am finding it difficult to understand why everyone suddenly starts talking about artinian local algebras. All the lectures seems to be very abstract to me. \nMay be I am missing some points for understanding. \nI would appreciate if someone writes an answer either stating 1) Why to study deformation theory? 2) What is deformation theory? or Someone can point out any another nice reference to study Deformation Theory. \nI understand what is meant by Moduli Space. Some of the above mentioned notes say that deformation theory is somehow related to Moduli Theory. But I have no clue how.", "output": "What follows is an attempt to motivate this beautiful and difficult (in my opinion) subject. It is just an attempt, I cannot promise it will be useful.\nSuppose you have a family of curves over $\\mathbb A^1=\\textrm{Spec }\\mathbb C[t]$, like for instance the family $$\\pi:\\textrm{Spec }\\mathbb C[x,y,t]/(xy-t)\\to \\mathbb A^1$$ given by $t\\mapsto t$. As it is explained very well in Hartshorne's book, deformation theory is: \n$\\textbf{the infinitesimal study of a family in a neighborhood of one of its members.}$\nFor instance, the member corresponding to $t=0$ is very special in the above family, as it is the only singular fiber of $\\pi$: the smooth hyperbolae degenerate, or rather, deform to a singular conic, the union of two lines at a point (draw a picture!). In a \"neighborhood\" of this member of the family, all other curves are smooth conics, so when we stare at this unique, very special singular conic, the natural question arises:\n$$\\textrm{How could that happen?}$$\nThe curiosity towards the answer to such a question could be one motivation for deformation theory.\nNow you can already see the relation to moduli: we just finished talking about a \"family of curves\"...\nNow let me tell you something very naive. Let $x$ be a (closed) point on a variety $X$. What does it mean to deform $x$ in $X$? Well, pretend you are a point on a sphere, then to \"deform yourself\" you have to look around you in all possible directions and see what surrounds you - but you need to do this infinitesimally, first because you are a point, and second because deformation theory is the infinitesimal study of geometric objects. So it turns out that to deform yourself means to choose a tangent direction on the sphere. More generally,\n$$\\{\\textrm{Deformations of }x\\textrm{ in } X\\}=T_xX=\\hom_{k(x)}(\\textrm{Spec }k[t]/t^2,X).$$\nLet $D=\\textrm{Spec }k[t]/t^2$. In general you have this:\nDefinition. Let $i:Y\\hookrightarrow X$ be a closed subscheme. A first order deformation of $Y$ in $X$ (also called a deformation of $i$) is flat morphism $f:\\mathfrak X\\to D$, where $\\mathfrak X$ is a closed subscheme of $X\\times D$, $Y$ is the fiber over the closed point of $D$, and $f$ is induced by the projection $X\\times D\\to D$.\nI'll tell you later what nice group describes these objects!\nExample. Suppose $X$ is a variety such that $H^1(X,\\mathscr O_X)=0$. This cohomology group is the tangent space of any point $[L]\\in \\textrm{Pic }X$. So if it vanishes, it means that line bundles on $X$ do not deform.\nExample. Let $\\mathbb P^5_\\mathbb C$ be moduli space of plane conics. Let's pick an explicit conic $C\\subset\\mathbb P^2_\\mathbb C$, and let us try to compute its tangent space as a point $p=[C]$ in the moduli space $\\mathbb P^5$. So we find:\n\\begin{align}\nT_{[C]}\\mathbb P^5&=\\hom_{\\mathbb C(p)}(\\textrm{Spec }k[t]/t^2,\\mathbb P^5) \\notag\\\\\n&=\\hom_\\mathbb C(m_p/m_p^2,\\mathbb C)\\notag\\\\\n&=H^0(C,N_{C/\\mathbb P^2}).\n\\end{align}\nIs it really $5$-dimensional? Since\n$$N_{C/\\mathbb P^2}=\\mathscr O_{\\mathbb P^2}(C)|_C=\\mathscr O_C(2)=\\mathscr O_{\\mathbb P^1}(4),$$ yes, it is $5$-dimensional, as expected. \nMore than finding the expected dimension for the tangent space, it is interesting to observe that, once you define what a first order deformation of $C$ in $\\mathbb P^2$ is (as I did above), it turns out that such objects are parameterized by the cohomology group $H^0(C,N_{C/\\mathbb P^2})$. So the upshot is: the deformations of the closed embedding $C\\subset \\mathbb P^2$ are exactly the deformations of $[C]$ as a moduli point in $\\mathbb P^5$. There we found another strong link with moduli!\nMore generally: The first order deformations of a closed subscheme $i:Y\\hookrightarrow X$ are parameterized by $H^0(Y,N_{Y/X})$, which is also the tangent space of $[Y]$ as a point in the Hilbert scheme of $X$.\n\nI just realized that my answer is much longer than I thought it was in my mind, so let me finish justifying the ubiquity of local Artinian $k$-algebras: their category is equivalent (under the functor $\\textrm{Spec}$) to the category of fat points over $k$.\nWhy on earth should we care about fat points? (A fat point over $k$ is just a $k$-scheme $F$ such that the structural morphism $F_{\\textrm{red}}\\to \\textrm{Spec }k$ is an isomorphism: they are $0$-dimensional schemes having one closed point with some ugly but useful non-reduced structure). Now, $D=\\textrm{Spec }k[t]/t^2$ is one such, but it is very special, because it describes the unique scheme structure one can put on a double point. First order deformations are those parameterized by this $D$, and they are flat morphisms (say) over $D$ such that over the closed point there lies the object you want to deform. Considering families over a fatter point, e.g. over $\\textrm{Spec }k[t]/t^3$ is the study of higher order deformations. These are very different from the first order one, e.g. you may not have any deformation at all over a certain algebra $A$, whereas over $D$ you always have the trivial deformation (the one corresponding to the element of the cohomology group which is concerned). If you have one, you may want to know if you can extend it further, and this leads to study small extensions of local Artin $k$-algebras.\n\nGood references are online notes by Ravi Vakil, and Sernesi's book Deformations of algebraic schemes.", "meta": {"post_id": 1123669, "input_score": 27, "output_score": 35, "post_title": "Studying Deformation Theory of Schemes"}}
{"input": "$A  \\underset{\\mathbb{C}}{\\sim} B \\overset{\\text{def}}{\\iff} A=C^{-1}BC, \\space C\\in M_{n}(\\mathbb{C})$ and similarly for $\\underset{\\mathbb{R}}{\\sim}$.\n\nI want to prove that $ A \\underset{\\mathbb{C}}{\\sim} B$ for $A,B \\in M_{n}(\\mathbb{R})$ therefore $A \\underset{\\mathbb{R}}{\\sim} B$.\n\nMy idea is that elementary divisors of $A,B$ over $\\mathbb{C}$ are the same, and if $(x-z)^k$ is elementary divisor than $(x-\\overline{z})^k$ is also elementary divisor $\\implies$ $A,B$ have same elementary divisors over $\\mathbb{R}$. But i think it's not clear.", "output": "If $ A \\underset{\\mathbb{C}}{\\sim} B$, there is a matrix $C \\in GL_n(\\mathbb{C})$ such that $A=C^{-1}BC$.\nSo $CA=BC$.\n$C=P+iQ$ with $P,Q \\in M_n(\\mathbb{R})$.\nIf $A \\in M_n(\\mathbb{R})$ and $B \\in M_n(\\mathbb{R})$, we have $CA=BC \\implies (P+iQ)A=B(P+iQ)\\implies PA=BP$ and $QA=BQ$.\nThe polynomial $\\det (P+XQ)$ is not null, because $\\det(P+iQ)=\\det C \\neq 0$.\nSo, there is a value $\\lambda \\in \\mathbb{R}$ such hat $\\det(P+\\lambda Q) \\neq 0$.\nLet $D=P+\\lambda Q$, $DA=BD$ because $PA=BP$ and $QA=BQ$.\nSo, $A=D^{-1}BD$ and $A \\underset{\\mathbb{R}}{\\sim} B$", "meta": {"post_id": 1129628, "input_score": 25, "output_score": 38, "post_title": "Similarity of real matrices over $\\mathbb{C}$"}}
{"input": "I am trying to prove that $\\mathbb{R}$ with the lower limit topology is not second-countable.\nTo do this, I'm trying to form an uncountable union $A$ of disjoint, half-open intervals of the form $[a, b)$, $a < b$. Is this possible? I think this would imply the $A$ is open but no countable union of basis elements could coincide with $A$ therefore making the real numbers with the lower limit topology not second-countable.\nI think there must exist something like $A$ described above but I am having trouble visualizing it and coming up with a formula to represent it.\nMaybe there is some other way to show it is not second-countable.", "output": "Suppose $\\mathcal B$ is a base for the \"lower limit\" topology on $\\mathbb R$, better known as the Sorgenfrey line. By the definition of a base for a topology, for any open set $U$ and any point $x\\in U$ there is a basic open set $B\\in\\mathcal B$ such that $x\\in B\\subseteq U$. Hence, for any point $x\\in\\mathbb R$, since $[x,\\infty)$ is an open set containing $x$, we can choose a set $B_x\\in\\mathcal B$ with $\\min B_x=x$. Since the sets $B_x(x\\in\\mathbb R)$ are distinct, this shows that $|\\mathcal B|\\ge|\\mathbb R|\\gt\\aleph_0$.", "meta": {"post_id": 1135993, "input_score": 32, "output_score": 53, "post_title": "$\\mathbb{R}$ with the lower limit topology is not second-countable"}}
{"input": "I came across this simple proof of Fermat's last theorem. Some think it's legit. Some argued that the author's assumptions are flawed. It's rather lengthy but the first part goes like this:\nLet $x,y$ be $2$ positive non-zero coprime integers and $n$ an integer greater than $2$. According to the binomial theorem:$$(x+y)^n=\\sum_{k=0}^{n}\\binom{n}{k}x^{n-k}{y^k}$$\nthen,$$(x+y)^n-x^n=nx^{n-1}y+\\sum_{k=2}^{n-1}\\binom{n}{k}x^{n-k}{y^k}+y^{n}$$\n$$(x+y)^n-x^n=y(nx^{n-1}+\\sum_{k=2}^{n-1}\\binom{n}{k}x^{n-k}y^{k-1}+y^{n-1})$$\n$$y(nx^{n-1}+\\sum_{k=2}^{n-1}\\binom{n}{k}x^{n-k}y^{k-1}+y^{n-1})=z^n$$\nIn the first case, he assumed that the 2 factors are coprime when $\\gcd(y,n)=1$ . Then he wrote: \n$$y=q^n$$\n$$ nx^{n-1}+\\sum_{k=2}^{n-1}\\binom{n}{k}x^{n-k}y^{k-1}+y^{n-1}=p^n$$\nBy replacing $y$ by $q^n$,\n\\begin{equation} nx^{n-1}+\\sum_{k=2}^{n-1}\\binom{n}{k}x^{n-k}q^{n(k-1)}+q^{n(n-1)}=p^n (*)\n\\end{equation}\nfrom this bivariate polynomial,he fixed alternatively $x$ and $y=q^n$ and by applying the rational root theorem, he obtained  $$q^{n(n-1)}-p^n=nxt   $$ and\n$$  nx^{n-1}-p^n=q^ns $$ \n($s,t$ non-zero integers)\nby equating $p^x$: $$ q^{n(n-1)}-sq^n=nx(t-x^{n-2})$$\nThen, he uses one of the trivial solutions of Fermat's equations. He wrote, when $x+y=1$,if $x=0$ then $y=1$ and vice versa. \nTherefore, he wrote: \n$x=0$ iff $q^{n(n-1)}=sq^n$, he obtains: $$q=1$$ or $$s=q^{n-2}$$ \nBy substituting $s$ by $q^{n-2}$ in $nx^{n-1}-p^n=q^ns$, he obtains: $$nx^{n-1}-p^n=q^{n(n-1)}$$\nThen, he replace that expression in equation (*) and pointed out that:$$\\sum_{k=2}^{n-1}\\binom{n}{k}x^{n-k}q^{n(k-1)}=0$$. Since $x,y=q^n$ are positive integers for all $n>2$, a sum of positive numbers can not be equal to zero. Which leads to a contradiction.\nWhat do you think?", "output": "There is a \"trick\", due to Marc Krasner, which prevents you from wasting time in examining \"elementary\" arithmetic proofs of Fermat's Last Theorem. \"Elementary\" means precisely that the proof uses only addition and multiplication (operations in a ring), and perhaps also the existence and unicity of decomposition into prime factors (so the ring in question is factorial). I suppose this is the case here, although not all details are given. Then, without checking anything, you can be assured that the reasoning is certainly wrong. This is because all such \"elementary\" arguments can be repeated word for word in the ring $Z_p$ of p-adic integers, which is factorial (and a lot more !), but in which FLT is false, because in the field $Q_p$ of p-adic numbers, the equation $x^p + y^p = 1$ always has non trivial solutions (if you take $y$ to be a high power of $p$, then p-adic analysis tells you that $1 - y^p$ has a p-th root in $Q_p$).", "meta": {"post_id": 1139251, "input_score": 10, "output_score": 92, "post_title": "Fermat's Last Theorem simple proof"}}
{"input": "How  evaluate this integral?\n $$I=\\int_0^{\\pi/2}\\frac{x^2\\log^2{(\\sin{x})}}{\\sin^2x}\\,dx$$\n Note: $$\\int_0^{\\pi/2}\\frac{x^2\\log{(\\sin x)}}{\\sin^2x}dx=\\pi\\ln{2}-\\frac{\\pi}{2}\\ln^22-\\frac{\\pi^3}{12}.$$", "output": "Let's start out with the auxiliary result\n\\begin{equation*}\n\\int_0^{\\pi/2}\\frac{x^2\\log{(\\sin(x))}}{\\sin^2(x)}dx=\\pi\\ln{(2)}-\\frac{\\pi}{2}\\ln^2(2)-\\frac{\\pi^3}{12}.\n\\end{equation*}\nBy the integration by parts all reduces to \n$$\\int_0^{\\pi/2} \\cot(x) (x^2 \\cot(x)+2 x\\log(\\sin(x))) \\ dx=\\int_0^{\\pi/2} x^2 \\cot^2(x) \\ dx+2\\int_0^{\\pi/2} x\\cot(x) \\log(\\sin(x))) \\ dx.$$\nFor the first integral in the right-hand side we apply the integration by parts that yields\n\\begin{equation*}\n\\begin{aligned}\n\\int_0^{\\pi/2} x^2 \\cot^2(x) \\ dx &=-\\frac{\\pi^3}{8}+\\int_0^{\\pi/2} 2 x (x+\\cot (x)) \\ dx \\\\\n                                                        &=-\\frac{\\pi^3}{24}+2\\int_0^{\\pi/2}x \\cot (x) \\ dx \\\\\n                                                        &=-\\frac{\\pi^3}{24}-2\\int_0^{\\pi/2} \\log(\\sin(x)) \\ dx \\\\\n                                                        &=-\\frac{\\pi^3}{24}-\\int_0^{\\pi} \\log(\\sin(x)) \\ dx \\\\                                                        \n\\end{aligned}\n\\end{equation*}\nwhere in the penultimate equality we used again the integration by parts, and then the symmetry.\nThen, \n\\begin{equation*}\n\\begin{aligned}\n\\int_0^{\\pi} \\log(\\sin(x)) \\ dx &=\\int_0^{\\pi} \\log(2\\sin(x/2)\\cos(x/2)) \\ dx \\\\\n                                                &=\\pi \\log(2)+ \\int_0^{\\pi} \\log(\\sin(x/2)) \\ dx+ \\int_0^{\\pi} \\log(\\cos(x/2)) \\ dx.                                    \n\\end{aligned}\n\\end{equation*}\nLetting $x/2=y$ in both integrals in the right-hand side, we obtain that \n\\begin{equation*}\n\\begin{aligned}\n\\int_0^{\\pi} \\log(\\sin(x)) \\ dx &=\\pi \\log(2) + 2\\int_0^{\\pi/2} \\log(\\sin(x)) \\ dx+2\\int_0^{\\pi/2} \\log(\\cos(x)) \\ dx \\\\      \n                                                &=\\pi \\log(2) + 4\\int_0^{\\pi/2} \\log(\\sin(x)) \\ dx \\\\ \n                                                &=\\pi \\log(2) + 2\\int_0^{\\pi} \\log(\\sin(x)) \\ dx \\\\                               \n\\end{aligned}\n\\end{equation*}\nwhence we get that\n$$\\int_0^{\\pi} \\log(\\sin(x)) \\ dx =-\\pi\\log(2).$$\nThen, \n$$\\int_0^{\\pi/2} x^2 \\cot^2(x) \\ dx=\\pi\\log(2)-\\frac{\\pi^3}{24}.$$\nOn the other hand, the integration by parts yields that\n\\begin{equation*}\n\\begin{aligned} \n2\\int_0^{\\pi/2}x\\cot(x) \\log(\\sin(x)) \\ dx &=-2\\int_0^{\\pi/2}  (\\log ^2(\\sin (x))+ x \\cot (x) \\log (\\sin (x))) \\ dx \\\\\n                                                                    &=-2\\int_0^{\\pi/2}  \\log ^2(\\sin (x)) \\ dx -2\\int_0^{\\pi/2} x \\cot (x) \\log (\\sin (x)) \\ dx\n\\end{aligned}\n\\end{equation*}\nwhence we have that \n$$\\int_0^{\\pi/2}x\\cot(x) \\log(\\sin(x)) \\ dx=-\\frac{1}{2}\\int_0^{\\pi/2}  \\log ^2(\\sin (x)) \\ dx.$$\nAccording to the trigonometric form of the beta function, we have that \n$$\\int_0^{\\pi/2} \\sin^a(x)\\cos^b(x) \\ dx=\\frac{1}{2}B \\left(\\frac{1}{2}(a+1),\\frac{1}{2}(b+1)\\right).$$\nDifferentiating $2$ times with respect to $a$ and then letting $a\\to 0$ and $b\\to 0$, we obtain that\n\\begin{equation*}\n\\begin{aligned} \n\\int_0^{\\pi/2}  \\log ^2(\\sin (x)) \\ dx &=\\frac{1}{2} \\lim_{b \\to 0} \\lim_{a \\to 0} \\frac{\\partial^2}{\\partial a^2}\\left(B \\left(\\frac{1}{2}(a+1),\\frac{1}{2}(b+1)\\right)\\right) \\\\\n                                                           &=\\frac{1}{24} \\left(\\pi ^3+12 \\pi  \\log ^2(2)\\right).\n\\end{aligned}\n\\end{equation*}\nThus, \n$$\\int_0^{\\pi/2}x\\cot(x) \\log(\\sin(x)) \\ dx=-\\frac{1}{48} \\left(\\pi ^3+12 \\pi  \\log ^2(2)\\right).$$\nand finally our auxiliary result is proved.\n$$\\int_0^{\\pi/2}\\frac{x^2\\log{(\\sin(x))}}{\\sin^2(x)}dx=\\pi\\ln{(2)}-\\frac{\\pi}{2}\\ln^2(2)-\\frac{\\pi^3}{12}.$$\nNow, we prove the main result, \n\\begin{equation*}\n\\int_0^{\\pi/2}\\frac{x^2\\log^2{(\\sin(x))}}{\\sin^2(x)}dx=\\left(\\frac{\\pi ^3 }{6} +2 \\pi \\right)  \\log (2)+\\frac{1}{3} \\pi  \\log ^3(2)+\\frac{1}{8}\\pi  \\zeta (3)-\\frac{\\pi ^3}{6}-\\pi  \\log ^2(2).\n\\end{equation*}\nApplying the integration by parts, we get \n$$2\\int_0^{\\pi/2} x^2 \\cot ^2(x) \\log (\\sin (x)) \\ dx+2 \\int_0^{\\pi/2} x \\cot (x) \\log ^2(\\sin (x)) \\ dx$$\nFor the integral in the left side we make use of the integration by parts that yields \n$$2\\int_0^{\\pi/2} x^2 \\cot ^2(x) \\log (\\sin (x)) \\ dx$$\n$$=2 \\int_0^{\\pi/2} x^3 \\cot (x) \\ dx + 2 \\int_0^{\\pi/2} x^2 \\cot ^2(x) \\ dx + 4\\int_0^{\\pi/2} x^2 \\log (\\sin (x)) \\ dx+4  \\int_0^{\\pi/2} x \\cot (x) \\log (\\sin (x)) \\ dx$$\n$$=\\frac{2}{3} \\int_0^{\\pi/2} x^3 \\cot (x) \\ dx + 2 \\int_0^{\\pi/2} x^2 \\cot ^2(x) \\ dx +4  \\int_0^{\\pi/2} x \\cot (x) \\log (\\sin (x)) \\ dx$$\nand since the last $2$ integrals are already compute (see the auxiliary result), we obtain \n$$\\frac{2}{3} \\int_0^{\\pi/2} x^3 \\cot (x) \\ dx + 2 \\int_0^{\\pi/2} x^2 \\cot ^2(x) \\ dx +4  \\int_0^{\\pi/2} x \\cot (x) \\log (\\sin (x)) \\ dx$$\n$$=\\frac{2}{3} \\int_0^{\\pi/2} x^3 \\cot (x) \\ dx+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}$$\nand integrating by parts again, we get \n$$-2\\int_0^{\\pi/2} x^2 \\log(\\sin(x)) \\ dx+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}.$$\nUsing that $\\displaystyle \\log(\\sin(x))=-\\log(2)-\\sum_{n=1}^{\\infty} \\frac{\\cos(2 n x)}{n}$, we obtain\n$$-2\\int_0^{\\pi/2} x^2\\left(-\\log(2)-\\sum_{n=1}^{\\infty} \\frac{\\cos(2 n x)}{n}\\right)\\ dx+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}$$\n$$=2\\log(2)\\int_0^{\\pi/2} x^2\\ dx+2\\int_0^{\\pi/2} x^2 \\sum_{n=1}^{\\infty} \\frac{\\cos(2 n x)}{n} \\ dx+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}$$\n$$=2\\int_0^{\\pi/2} x^2 \\sum_{n=1}^{\\infty} \\frac{\\cos(2 n x)}{n} \\ dx+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}+\\frac{1}{12} \\pi ^3 \\log (2)$$\n$$=2  \\sum_{n=1}^{\\infty} \\int_0^{\\pi/2} x^2 \\frac{\\cos(2 n x)}{n} \\ dx+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}+\\frac{1}{12} \\pi ^3 \\log (2)$$\n$$=-\\frac{3}{8} \\pi \\zeta(3)+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}+\\frac{1}{12} \\pi ^3 \\log (2).$$\nTherefore, we have that \n$$2\\int_0^{\\pi/2} x^2 \\cot ^2(x) \\log (\\sin (x)) \\ dx=-\\frac{3}{8} \\pi \\zeta(3)+2\\pi\\log(2)-\\pi\\log^2(2)-\\frac{\\pi^3}{6}+\\frac{1}{12} \\pi ^3 \\log (2).$$\nFor the remaining integral, we use the integration by parts again that yields \n$$2 \\int_0^{\\pi/2} x \\cot (x) \\log ^2(\\sin (x)) \\ dx=-2\\int_0^{\\pi/2} \\log ^3(\\sin (x)) \\ dx-4 \\int_0^{\\pi/2} x \\cot (x) \\log ^2(\\sin (x)) \\ dx$$\nand thus\n$$2 \\int_0^{\\pi/2} x \\cot (x) \\log ^2(\\sin (x)) \\ dx=-\\frac{2}{3}\\int_0^{\\pi/2} \\log ^3(\\sin (x)) \\ dx.$$\nAccording to the trigonometric form of the beta function, we know that\n$$\\int_0^{\\pi/2} \\sin^a(x)\\cos^b(x) \\ dx=\\frac{1}{2}B \\left(\\frac{1}{2}(a+1),\\frac{1}{2}(b+1)\\right).$$\nDifferentiating $3$ times with respect to $a$ and then letting $a\\to 0$ and $b\\to 0$, we obtain that\n\\begin{equation*}\n\\begin{aligned} \n\\int_0^{\\pi/2}  \\log ^3(\\sin (x)) \\ dx &=\\frac{1}{2} \\lim_{b \\to 0} \\lim_{a \\to 0} \\frac{\\partial^3}{\\partial a^3}\\left(B \\left(\\frac{1}{2}(a+1),\\frac{1}{2}(b+1)\\right)\\right) \\\\\n                                                           &=-\\frac{3 \\pi }{4}\\zeta (3)-\\frac{1}{2} \\pi  \\log ^3(2)-\\frac{1}{8} \\pi ^3 \\log (2).\n\\end{aligned}\n\\end{equation*}\nThus, \n$$2 \\int_0^{\\pi/2} x \\cot (x) \\log ^2(\\sin (x)) \\ dx=\\frac{1}{2}\\pi  \\zeta (3)+\\frac{1}{3} \\pi  \\log ^3(2)+\\frac{1}{12} \\pi ^3 \\log (2).$$\nHence, \n$$\\int_0^{\\pi/2}\\frac{x^2\\log^2{(\\sin(x))}}{\\sin^2(x)}dx=\\left(\\frac{\\pi ^3 }{6} +2 \\pi \\right)  \\log (2)+\\frac{1}{3} \\pi  \\log ^3(2)+\\frac{1}{8}\\pi  \\zeta (3)-\\frac{\\pi ^3}{6}-\\pi  \\log ^2(2).$$\nQ.E.D.", "meta": {"post_id": 1142705, "input_score": 28, "output_score": 38, "post_title": "Evaluate $\\int_0^{\\pi/2}\\frac{x^2\\log^2{(\\sin{x})}}{\\sin^2x}dx$"}}
{"input": "Well, studying sheaf cohomology, I've faced the notion of dualizing sheaf on a projective scheme over a field $k$. Recall that a dualizing sheaf on $X$ (according to Hartshorne) is a coherent sheaf $\\omega_X^\\circ$, such that the composition\n$\nHom(\\mathscr{F},\\omega_X^\\circ)\\times H^n(X,\\mathscr{F})\\to H^n(X,\\omega_X^\\circ)\\overset{t}{\\longrightarrow}k\n$\nof the natural pairing with the trace homomorphism $t$ induces an isomorphism\n$\nHom(\\mathscr{F},\\omega_X^\\circ)\\cong H^n(X,\\mathscr{F})^*.\n$\nAlthough I formally understand the definition (and the proof of its existence), it seems quite mysterious to me (trace homomorphism, in particular). As I can guess, we want to define some analog of canonical sheaf for singular schemes.\nAnyway, what is a motivation for introducing such a definition? Are there simple examples (maybe explicit calculations) when we have to deal with the dualizing sheaf instead of the canonical one?\nP.S. By the way, I know that in complex-analytical settings there is a way to define the canonical sheaf on a normal variety starting from defining it on a nonsingular part. Maybe there is some connection between these two approaches?", "output": "The question asked for concrete examples and calculations, something I will leave for others to supply. Instead I aspire to share some insight into Serre duality, and by extension the definition of a dualizing sheaf. I hope that provides useful as well.\nThe lesson summed up in one sentence is:\nit helps to look at things in the derived setting. The rest of this post is going to try to explain and justify this assertion. I am trying to make this answer as self-contained as possible, so that no prior knowledge of derived categories is required  to make it readable. This has contributed to its significant length, however.\nDerived categories of sheaves\nIf you are not familiar with the formalism of derived categories, just imagine that you are working with chain complexes and the morphisms are just chain maps, tweaked so that all weak equivalences (maps which induce isomorphisms on cohomology) are isomorphisms. Informally, if cohomology can't tell two morphisms apart, the derived category identifies them as well.\nFor a nice-enough scheme $X$ over a nice-enough field $k$ (I am trying to avoid getting too technical here), let $D(X)$ denote the derived category of coherent sheaves on $X.$ As mentioned above, the objects of $D(X)$ are just complexes of coherent sheaves. There is an obvious embedding $\\operatorname{Coh}(X)\\to D(X)$ that sends every coherent sheaf to the complex with $\\mathscr F$ at the $0$-th spot and $0$ everywhere else. This embedding is fully faithful, which is to say that for each pair of coherent sheaves on $X$ we have\n$$\n\\operatorname{Hom}_{D(X)}(\\mathscr F,\\mathscr G) = \\operatorname{Hom}_{\\mathscr O}(\\mathscr F,\\mathscr G)\n$$\n(since the entire story takes place over a fixed scheme $X,$ I will write $\\mathscr O$ in place of $\\mathscr O_X$ and generally omit the subscript $X$ when no confusion is likely to arise from this omission). We will henceforth identify sheaves with their images in $D(X),$ which is to say with complexes concentrated in $0$-th degree.\nVarious functors between categories of sheaves induce functors on the respective derived categories. A right-exact (left-exact) functor $F\\colon \\operatorname{Coh}(X)\\to\\operatorname{Coh}(Y)$ extends to its left (right) derived functor $RF\\colon D(X)\\to D(Y)$ ($LF\\colon D(X)\\to D(Y)$). Taking the cohomologies of these produce the right derived functors $R^iF$ and $L^iF$ you might be more faimiliar with. The two derived functors we will be most interested in are the  derived tensor product denoted $\\mathscr F\\overset L\\otimes_{\\mathscr O}\\mathscr G,$ the left derived functor of $\\mathscr F\\mapsto \\mathscr F\\otimes_\\mathscr O \\mathscr G,$ and the derived Hom denoted $R\\mathscr{Hom}_{\\mathscr O}(\\mathscr F,-).$\nOne last thing to note is that the Hom-sets of the category $D(X)$ carry not only a $k$-vector space structure, as do Homs of $\\operatorname{Coh}(X),$ but actually the structure of cochain complexes of vector spaces over $k.$\nSerre duality\nNow let's look at Serre duality with derived eyes. It obtains the particulary natural form: \n$$\n\\operatorname{Hom}_{D(X)}(\\mathscr O, \\mathscr F) \\cong \\operatorname{Hom}_{D(X)}(\\mathscr F,\\omega)^*.\\qquad (1)\n$$\nIf you find this formula a bit scary (in the \"What does it all mean?!\" way), don't worry. I will try to write it out a bit more explicitly in a moment. For now, just appreciate what a nice formally-pleasing formula it is: it allows us to relate morphisms into $\\mathscr F$ to (duals of) morphisms from $\\mathscr F$ into a canonically determined (only dependent on $X,$ not on $\\mathscr F$) object $\\omega\\in \\operatorname{ob}D(X).$ This $\\omega$ is what is called the dualizing complex.\nIn the rest of this post we will rewind the definitions to see what this means. Note in particular that both sides are chain complexes (of vector spaces), so we can take their cohomology groups and compare them. Since we are considering everything in derived categories, two complexes being isomorphic is the same as all their cohomology groups agreeing. (Technically that's not quite true. There furtheremore has to exist a morphism of chain complexes from one to the other which induces those isomorphisms on cohomology. But let's ignore this issue for the moment.)\nLHS of (1)\nThere is an obvious identification $\\Gamma(X,\\mathscr F) =\\operatorname{Hom}_{\\mathscr O}(\\mathscr O,\\mathscr F)$ for any sheaf $\\mathscr F$ of $\\mathscr O$-modules, obtained just by considering what it means to specify a global section. \nOn the other hand, if $V_\\bullet$ is a chain complex of the form $0\\to V_0 \\to V_1\\to\\ldots,$ then clearly $H^0(V_\\bullet)=V_0.$ Therefore taking the $0$-th cohomology of the left side of (1) we get\n$$\nH^0\\left(\\operatorname{Hom}_{D(X)}(\\mathscr O,\\mathscr F)\\right) \\cong \\Gamma(X,\\mathscr F)\n$$\nand by the same logic taking higher cohomology produces\n$$\nH^i\\left(\\operatorname{Hom}_{D(X)}(\\mathscr O,\\mathscr F)\\right) \\cong H^i(X,\\mathscr F).\n$$\nRHS of (1)\nTensoring sheaves of $\\mathscr O$-modules with the sheaf $\\mathscr O$ does nothing, and so that continues to be true in the derived category. Therefore the (dual of the) right hand side of (1) can be equivalently written as\n$$\n\\operatorname{Hom}_{D(X)}(\\mathscr F,\\omega)\\cong \\operatorname{Hom}_{D(X)}(\\mathscr F\\overset{L}{\\otimes}\\mathscr O,\\omega).\\qquad (2)\n$$\nNow recall that functorial things that are true in $\\operatorname{Coh}(X)$ are also true in $D(X)$ once you replace all the functors with their respective derived functors. So the adjunction $-\\otimes_{\\mathscr O}\\mathscr F \\dashv \\mathscr{Hom}_{\\mathscr O}(\\mathscr F,-)$\nand the well-know isomorphism $\\mathscr{Hom}_{\\mathscr O}(\\mathscr F,\\mathscr O)\\otimes_{\\mathscr O}\\mathscr G\\cong \\mathscr{Hom}_{\\mathscr O}(\\mathscr F,\\mathscr G)$ (which holds at least when $\\mathscr G$ is locally free; luckily we will only use it for $\\omega$ and if $X$ is smooth then there will be no problems) extend to the adjuntion and isomorphism\n$$\n-\\overset{L}\\otimes_{\\mathscr O}\\mathscr F \\dashv R\\mathscr{Hom}_{\\mathscr O}(\\mathscr F,-),\\qquad\\quad R\\mathscr{Hom}_{\\mathscr O}(\\mathscr F,\\mathscr O)\\overset L\\otimes_{\\mathscr O}\\mathscr G\\cong R\\mathscr{Hom}_{\\mathscr O}(\\mathscr F,\\mathscr G).\n$$\nWe can use these to continue the chain of isomorphisms (2) as\n$$\n\\cong\\operatorname{Hom}_{D(X)}(\\mathscr O, R\\mathscr{Hom}_{\\mathscr O}(\\mathscr F,\\mathscr O)\\overset L\\otimes_{\\mathscr O}\\omega) \\cong\\operatorname{Hom}_{D(X)}(\\mathscr O, \\mathscr F^{\\vee}\\overset L\\otimes_{\\mathscr O}\\omega),\n$$\nwhere $\\mathscr F^{\\vee}$ is the standard notation for the dual of $\\mathscr F$ though note that we are talking in the derived context here, so we are taking the right-derived functor of $\\mathscr{Hom}_{\\mathscr O}$.\nAnyway, just as we did with the LHS, we can now take the $i$-th cohomology of the (dual of) RHS to obtain, by the same reasoning as for LHS,\n$$\nH^i\\big(\\operatorname{Hom}_{D(X)}(\\mathscr F, \\mathscr O\\overset L\\otimes_\\mathscr O \\omega)\\big)= H^i\\big(X, \\mathscr F^\\vee\\overset L\\otimes_{\\mathscr O}\\omega\\big).\\qquad (3)\n$$\nThose with a keen eye will notice that $\\mathscr F$ can't be an arbitrary coherent sheaf, but must instead be locally free for this step to have been legal. So let's assume that too.\nWe should at this point also take into account that in the statement (1) of Serre duality above, the right side appears as a dual. The dual of a chain complex of vector spaces $V_{\\bullet}$ of course replaces each $V_i$ with its dual $V^*_i,$ but that isn't all that is deos. Recall that the fucntor $V\\mapsto V^*$ is contravariant, therefore it reverses the orders of the arrows. So the dual of the cochain complex\n$$\n0\\to V_0\\to V_1\\to V_2\\to V_3\\to \\ldots,\n$$\nwhich, for this purpose may be written more instructively as\n$$\n\\ldots\\to 0\\to 0\\to V_0\\to V_1\\to V_2\\to\\ldots,\n$$\nis the cochain complex\n$$\n\\ldots\\to V_2^*\\to V_1^*\\to V_0^*\\to 0\\to 0\\to\\ldots.\n$$\nThat is to say, if $V_i$ appears on the $i-$th spot in the complex $V_\\bullet,$ then $V_i^*$ appears on the $(-i)$-th spot in the dual complex $X_\\bullet^*.$\nFrom this it is easy to see that $H^i(V_\\bullet^*)=H^{-i}(V_\\bullet)^*,$\nso it follows from (3) that the $i$-th cohomology of the right side of (1) is really\n$$\nH^i\\big(\\operatorname{Hom}_{D(X)}(\\mathscr F, \\mathscr O\\overset L\\otimes_\\mathscr O \\omega)^*\\big)= H^{-i}\\big(X, \\mathscr F^\\vee\\overset L\\otimes_{\\mathscr O}\\omega\\big).\n$$\nSerre duality, pt. 2\nComparing both sides, we now get Serre duality as a natural bijection\n$$\nH^i(X,\\mathscr F) \\cong H^{-i}\\big(X,\\mathscr F^{\\vee}\\overset L\\otimes_{\\mathscr O} \\omega\\big)^*\\qquad (4)\n$$\nfor all integers $i$ (which, when $\\mathscr F$ is just a sheaf instead of a complex of sheaves, is interesting only for $i\\ge 0$).\nSo far everything made sense without any restrictions on $X.$ However if we do restrict our attention when $X$ is smooth and of dimension $n,$ we can identify the dualizing complex as $\\omega = \\Omega^n[n],$ where $\\Omega^n = \\bigwedge^n\\Omega_{X/k}$ is the sheaf of $n$-forms. ''What is that $[n]$ though?'' you might ask.\nSuspension operator\nThe symbol $[i]\\colon D(X)\\to D(X)$ dentoes the \"shift by $i$\" operator, generally known as $i$-fold suspension. It takes a complex and shifts it for $i$ places to the left, explicitly\n$\n(\\mathscr F_{\\bullet}[i])_j =\\mathscr F_{i+j}\n$\nfor any complex of coherent sheaves $\\mathscr F_\\bullet$ on $X.$ What is great about this operator is that for any coherent sheaf $\\mathscr F$ on $X$ we have\n$$\nH^0\\left(\\operatorname{Hom}_{D(X)}(\\mathscr O, \\mathscr F[i])\\right) \\cong H^i(X,\\mathscr F),\n$$\nextending the identification $\\Gamma(X,\\mathscr F) =H^0\\left(\\operatorname{Hom}_{D(X)}(\\mathscr O,\\mathscr F)\\right)$ mentioned previously.\nAn important case to consider is that of point, i.e. when $X=\\operatorname{Spec}(k).$ Then $D(X)$ consists objectwise just of complexes of finite dimensional vector spaces over $k$ and for any such complex $V_\\bullet$ we get\n$$\nH^j(V_\\bullet[i])= H^{i+j}(V_\\bullet).\n$$\nFurthermore it is apparent that suspension commutes with Hom, which is to say that\n$$\n\\operatorname{Hom}_{D(X)}(\\mathscr F, \\mathscr G[i]) = \\operatorname{Hom}_{D(X)}(\\mathscr F,\\mathscr G)[i].\n$$\nSerre duality, pt. 3\nWith what we just learned about suspension and identification of the dualizing complex on a smooth $n$-dimensional $X$ as $\\Omega^n[n],$ formula (4) can be rewritten finally into the classical form of Serre duality\n$$\nH^i(X,\\mathscr F)\\cong H^{-i}\\big(X,\\mathscr F^\\vee\\overset L\\otimes_{\\mathscr O}\\Omega^n[n]\\big)^* = H^{n-i}(X,\\mathscr F^\\vee\\otimes_\\mathscr O \\Omega^n)^*.\\qquad (5)\n$$\n(Really, formula (1) says a bit more than formula (5), for having all cohomology groups isomorphic is a neccessary condition for two objects of the derived category to coincide, but it is not sufficient. That said, most proofs of even the classical form of Serre duality proceed by defining a trace map which can be seen as inducing the isomorphism of formula (1) and which yields formula (5) by passing to cohomology.)\nCatharsis\nYou might at this point be wondering what purpose the journey into derived categories served to only restate the original theorem. I would argue the point is, at least for me, that if Serre duality in the form (5) looks a little artificial or mysterious, it is because it is being viewed it in a catgory in which it does not naturally live. If only one is prepared to look at it in the context of the derived category, it assumes the very simple and natural form (1).\nNow the dualizing complex is defined just as something which makes this splendid theorem work in more general contexts. Of course it is in general not a sheaf but a proper complex, but that is not that big of a problem. After all, the language of derived categories was defined in a big part in order to provide a natural context for Serre duality and its generalizations.\nTrace morphism\nThe original question also asked why one would define the trace morphism as it is defined. Let us look at formulation (1) of Serre duality again. It is a duality theorem, asserting an isomorphism between (chain complex of) vector spaces and duals, and as any such isomorphism, it comes from a perfect pairing\n$$\n\\operatorname{Hom}_{D(X)}(\\mathscr O,\\mathscr F)\\otimes_k\\operatorname{Hom}_{D(X)}(\\mathscr F,\\omega)\\to k.\\qquad (6)\n$$\nThis is the same thing as the trace morphism Hartshorne defines, once you take $n$-th cohomology of it. But again, the only reason cohomology appears there is because one is really talking about things in the derived category.\nThis pairing is obtained (as can be deduced already form the proof in Hartshorne's book) essentially by composing the two morphisms. The parallel with trace in the context of vector spaces is at this point probably apparent.\nBut in case it is not, allow me to make it explicit.\nTake $X$ to be a point, which of course is understood to be $\\operatorname{Spec}(k).$ Then sheaves of $\\mathscr O$-modules are just vector spaces over $k$ and they are coherent when they are finite dimensional. The structure sheaf $\\mathscr O$ corresponds to $k,$ as does the dualizing sheaf $\\omega.$ For a vector space $V$ that corresponds to a coherent sheaf $\\mathscr F,$ we then have\n$$\n\\operatorname{Hom}_{D(X)}(\\mathscr O,\\mathscr F) = \\operatorname{Hom}_k(k, V)\\cong V,\\qquad \\operatorname{Hom}_{D(X)}(\\mathscr F,\\omega)=\\operatorname{Hom}_k(V,k) = V^*.\n$$\nand so Serre duality amounts to specifying a natural isomorphism $V\\cong V^{**}$ (the usual one, of course). This too arises form a perfect pairing $V\\otimes_k V^*\\to k$ given by sending $v\\otimes \\varphi\\mapsto \\varphi(v).$ Under the standard isomorphism $V\\otimes_k V^*\\cong \\operatorname{End}_k(V),$ this clearly corresponds to the trace of an endomorphism.", "meta": {"post_id": 1143459, "input_score": 29, "output_score": 60, "post_title": "The idea behind the notion of dualizing sheaf"}}
{"input": "I'm trying to find a closed form for this integral:\n$$I=\\int_0^\\infty\\arctan\\left(\\frac{2\\pi}{x-\\ln\\,x+\\ln\\left(\\frac\\pi2\\right)}\\right)\\frac{dx}{x+1}$$\nIts approximate numeric value is\n$$I\\approx3.3805825284453469793953592216276992165696856825906055108192183...$$\nAny help is appreciated. Thanks!", "output": "Computing a Related Contour Integral:\nDefine\n$$f(z)=\\frac{i}{2}\\frac{z-1}{1+az}\\left(\\frac{1}{z-\\ln{z}+\\ln\\left(\\frac{\\pi}{2}\\right)}+\\frac{1}{z-\\ln{z}+2\\pi i+\\ln\\left(\\frac{\\pi}{2}\\right)}\\right)$$\nand let $\\gamma$ denote a keyhole contour deformed around $[0,\\infty]$. Restricting the argument between $0$ and $2\\pi$, it is not hard to see that $f(z)$ has poles at $z=-\\dfrac{1}{a}$, $z=-W_{-1}\\left(-\\dfrac{\\pi}{2}\\right)=\\dfrac{\\pi i}{2}$, and $z=-W_0\\left(-\\dfrac{\\pi}{2}\\right)=-\\dfrac{\\pi i}{2}$. The residues at these poles are\n\\begin{align}\n\\operatorname*{Res}_{z=\\frac{\\pi i}{2}}f(z)\n&=\\frac{i}{2}\\frac{\\frac{\\pi i}{2}-1}{\\frac{\\pi i}{2}a+1}\\frac{1}{1-\\frac{2}{\\pi i}}\\\\\n\\operatorname*{Res}_{z=-\\frac{\\pi i}{2}}f(z)\n&=\\frac{i}{2}\\frac{\\frac{\\pi i}{2}+1}{\\frac{\\pi i}{2}a-1}\\frac{1}{1+\\frac{2}{\\pi i}}\\\\\n\\operatorname*{Res}_{z=-\\frac{1}{a}}f(z)\n&=-\\frac{i}{2}u'(a)\\left(\\frac{1}{u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)-\\pi i}+\\frac{1}{u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)+\\pi i}\\right)\\\\\n\\end{align}\nwhere $u(a)=\\ln{a}-\\dfrac{1}{a}$. By the residue theorem,\n\\begin{align}\n\\oint_{\\gamma}f(z)\\ dz\n&=2\\pi i\\sum_{z_k\\in\\left\\{-a^{-1}, \\pm\\pi i/2\\right\\}}\\operatorname*{Res}_{z=z_k}f(z)\\\\\n&=\\pi\\left(\\frac{2\\left(u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)}{\\left(u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+\\pi^2}u'(a)-\\frac{2\\pi^2a}{\\pi^2a^2+4}\\right)\n\\end{align}\n\nParameterisation of the Contour Integral:\nWe take the argument of $z$ to be $0$ above the branch cut, and $2\\pi$ below the branch cut. Also, the contribution from the big arc is clearly $2\\pi i\\times\\dfrac{i}{2}\\times\\dfrac{1}{a}\\times (1+1)=-\\dfrac{2\\pi}{a}$. Taking all of these points into consideration, we eventually arrive at\n\\begin{align}\n\\oint_\\gamma f(z)\\ dz+\\frac{2\\pi}{a}\n&=\\small\\frac{i}{2}\\int^\\infty_0\\frac{x-1}{1+ax}\\left(-\\frac{1}{x-\\ln|x|-2\\pi i+\\ln\\left(\\frac{\\pi}{2}\\right)}+\\frac{1}{x-\\ln|x|+2\\pi i+\\ln\\left(\\frac{\\pi}{2}\\right)+\\pi^2}\\right)\\ dx\\\\\n&=2\\pi\\int^\\infty_0\\frac{x-1}{\\left(x-\\ln{x}+\\ln\\left(\\frac{\\pi}\n{2}\\right)\\right)^2+4\\pi^2}\\frac{dx}{1+ax}\\\\\n\\end{align}\n\nObtaining the Closed Form:\nIntegrating with respect to $a$, we obtain\n\\begin{align}\n\\small\\int^\\infty_0\\frac{2\\pi\\left(1-\\frac{1}{x}\\right)\\ln(1+ax)}{\\left(x-\\ln{x}+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+4\\pi^2}\\ dx\n&\\small=\\ \\pi\\int\\left(\\frac{2\\left(u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)}{\\left(u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+\\pi^2}u'(a)-\\frac{2\\pi^2a}{\\pi^2a^2+4}+\\frac{2}{a}\\right)\\ da\\\\\n&=\\small\\pi\\left(\\ln\\left(\\left(u(a)+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+\\pi^2\\right)-\\ln\\left(\\pi^2a^2+4\\right)+\\ln{a^2}\\right)+\\text{const.}\\\\\n&=\\small\\pi\\ln\\left(\\frac{\\left(\\ln{a}-\\frac{1}{a}+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+\\pi^2}{\\pi^2+\\frac{4}{a^2}}\\right)+\\text{const.}\n\\end{align}\nLetting $a\\to 0$, we find that the constant term is $\\pi\\ln{4}$. Plugging in $a=1$ and integrating by parts, we finally arrive at the closed form.\n\\begin{align}\n\\int^\\infty_0\\frac{2\\pi\\left(1-\\frac{1}{x}\\right)\\ln(1+x)}{\\left(x-\\ln{x}+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+4\\pi^2}\\ dx\n&=\\int^\\infty_0\\arctan\\left(\\frac{2\\pi}{x-\\ln{x}+\\ln\\left(\\frac{\\pi}{2}\\right)}\\right)\\frac{dx}{1+x}\\\\\n&=\\left.\\pi\\ln\\left(\\frac{\\left(\\ln{a}-\\frac{1}{a}+\\ln\\left(\\frac{\\pi}{2}\\right)\\right)^2+\\pi^2}{\\frac{\\pi^2}{4}+\\frac{1}{a^2}}\\right)\\right|_{a=1}\\\\\n&=\\color{red}{\\pi\\ln\\left(\\frac{\\ln^2\\left(\\frac{\\pi}{2}\\right)-2\\ln\\left(\\frac{\\pi}{2}\\right)+1+\\pi^2}{\\frac{\\pi^2}{4}+1}\\right)}\n\\end{align}", "meta": {"post_id": 1150822, "input_score": 39, "output_score": 54, "post_title": "Closed form for $\\int_0^\\infty\\arctan\\Bigl(\\frac{2\\pi}{x-\\ln\\,x+\\ln(\\frac\\pi2)}\\Bigr)\\frac{dx}{x+1}$"}}
{"input": "Evaluate the integral:\n$$\\displaystyle \\int_{0}^{\\frac{\\pi}{4}}\\tan^{-1}\\left(\\frac{\\sqrt{2}\\cos3 \\phi}{\\left(2\\cos 2 \\phi+ 3\\right)\\sqrt{\\cos 2 \\phi}}\\right)d\\phi$$ \nI have no clue on how to attack it.\nThe only thing I noticed is that there exists a symmetry around $\\pi/8$, meaning that from $\\pi/8$ to $\\pi/4$ is the negative of zero to $\\pi/4$. But, there exists a root of the integrand at $\\pi/6$ and the limit of the integrand at $\\pi/4$ is $-\\infty$.\nConjecture: The integral is $0$ for the reason of symmetry I mentioned above.\nHowever I cannot prove that. I would appreciate your help.", "output": "By replacing $\\phi$ with $\\arctan(t)$, then using integration by parts, we have:\n$$ I = \\int_{0}^{1}\\frac{1}{1+t^2}\\,\\arctan\\left(\\frac{\\sqrt{2}(1-3t^2)}{(5+t^2)\\sqrt{1-t^2}}\\right)\\,dt =\\frac{\\pi^2}{8}-\\int_{0}^{1}\\frac{3\\sqrt{2}\\, t \\arctan(t)}{(3-t^2)\\sqrt{1-t^2}}\\,dt.$$\nNow comes the magic. Since:\n$$\\int \\frac{3\\sqrt{2}\\,t}{(3-t^2)\\sqrt{1-t^2}}\\,dt  = -3\\arctan\\sqrt{\\frac{1-t^2}{2}}\\tag{1}$$\nintegrating by parts once again we get:\n$$ I = \\frac{\\pi^2}{8}-3\\int_{0}^{1}\\frac{1}{1+t^2}\\arctan\\sqrt{\\frac{1-t^2}{2}}\\,dt \\tag{2}$$\nhence we just need to prove that:\n$$ \\int_{0}^{1}\\frac{dt}{1+t^2}\\,\\arctan\\sqrt{\\frac{1-t^2}{2}}=\\int_{0}^{\\frac{1}{\\sqrt{2}}}\\frac{\\arctan\\sqrt{1-2t^2}}{1+t^2}\\,dt=\\color{red}{\\frac{\\pi^2}{24}}\\tag{3}$$\nand this is not difficult since both\n$$\\int_{0}^{1}\\frac{dt}{1+t^2}(1-t^2)^{\\frac{2m+1}{2}},\\qquad \\int_{0}^{\\frac{1}{\\sqrt{2}}}\\frac{(1-2t^2)^{\\frac{2m+1}{2}}}{1+t^2}\\,dt $$\ncan be computed through the residue theorem or other techniques. For instance:\n$$\\int_{0}^{1}\\frac{(1-t)^{\\frac{2m+1}{2}}}{t^{\\frac{1}{2}}(1+t)}\\,dt = \\sum_{n\\geq 0}(-1)^n \\int_{0}^{1}(1-t)^{\\frac{2m+1}{2}} t^{n-\\frac{1}{2}}\\,dt=\\sum_{n\\geq 0}(-1)^n\\frac{\\Gamma\\left(m+\\frac{3}{2}\\right)\\Gamma\\left(n+\\frac{1}{2}\\right)}{\\Gamma(m+n+2)}$$\nor just:\n$$\\int_{0}^{1}\\frac{\\sqrt{\\frac{1-t^2}{2}}}{(1+t^2)\\left(1+\\frac{1-t^2}{2}u^2\\right)}\\,dt = \\frac{\\pi}{2(1+u^2)}\\left(1-\\frac{1}{\\sqrt{2+u^2}}\\right)\\tag{4}$$\nfrom which:\n$$\\int_{0}^{1}\\frac{dt}{1+t^2}\\,\\arctan\\sqrt{\\frac{1-t^2}{2}}=\\frac{\\pi}{2}\\int_{0}^{1}\\frac{du}{1+u^2}\\left(1-\\frac{1}{\\sqrt{2+u^2}}\\right) =\\color{red}{\\frac{\\pi^2}{24}} $$\nas wanted, since:\n$$ \\int \\frac{du}{(1+u^2)\\sqrt{2+u^2}}=\\arctan\\frac{u}{\\sqrt{2+u^2}}.$$", "meta": {"post_id": 1151817, "input_score": 28, "output_score": 35, "post_title": "Integrate $ \\int_{0}^{\\frac{\\pi}{4}}\\tan^{-1}\\left(\\frac{\\sqrt{2}\\cos3 \\phi}{\\left(2\\cos 2 \\phi+ 3\\right)\\sqrt{\\cos 2 \\phi}}\\right)d\\phi$"}}
{"input": "The entropy of a uniform distribution is $ ln(b-a)$. With $a=0$ and $b=1$ this reduces to zero. How come there is no uncertainty?", "output": "Continuous entropy doesn't have quite the same meaning as discrete entropy. For example, we could also take $a = 0$ and $b = 1/2$, giving entropy $-\\ln(2) < 0$, where as in the discrete case entropy is always non-negative. Note that a lot of the difference comes from the fact that a probability density function (pdf) can be greater than one, on a set of measure (size) less than 1, though, so that the integral is 1.\nCheck out the WolframAlpha entry on it: Differential Entropy. Also, here is the Wikipedia entry on it: Differential Entropy.\nCompare this with the discrete distribution: Suppose we have $P(X = x_n) = 1/N$ where X takes the values $\\{ x_1, ..., x_N \\}$. This gives entropy\n$$H(X) = -\\sum_{n=1}^N P(X=X_n) \\log_2 P(X = X_n) = -\\sum_{n=1}^N {1 \\over N} \\log_2 {1 \\over N} = N \\cdot {1 \\over N} \\log_2 N = \\log_2 N.$$\nNote that this is actually the maximal value for the entropy - this can be shown using Gibbs' inequality, or just by finding the maximum of the function $f(x) = -x \\ln x$ (eg by differentiating and solving $f'(x) = 0$), and observing that\n$$\\log_2 x = {\\ln x \\over \\ln 2}.$$\nHope this helps! If it does, remember to upvote! ;)", "meta": {"post_id": 1156404, "input_score": 25, "output_score": 42, "post_title": "Entropy of a uniform distribution"}}
{"input": "I know that the following implications are true:\n$$\\text{Almost sure convergence} \\Rightarrow \\text{ Convergence in probability } \\Leftarrow \\text{ Convergence in }L^p $$\n$$\\Downarrow$$\n$$\\text{Convergence in distribution}$$\nI am looking for some (preferably easy) counterexamples for the converses of these implications.", "output": "Convergence in probability does not imply convergence almost surely: Consider the sequence of random variables $(X_n)_{n \\in \\mathbb{N}}$ on the probability space $((0,1],\\mathcal{B}((0,1]))$ (endowed with Lebesgue measure $\\lambda$) defined by $$\\begin{align*} X_1(\\omega) &:= 1_{\\big(\\frac{1}{2},1 \\big]}(\\omega) \\\\ X_2(\\omega) &:= 1_{\\big(0, \\frac{1}{2}\\big]}(\\omega) \\\\ X_3(\\omega) &:= 1_{\\big(\\frac{3}{4},1 \\big]}(\\omega) \\\\ X_4(\\omega) &:= 1_{\\big(\\frac{1}{2},\\frac{3}{4} \\big]}(\\omega)\\\\ &\\vdots \\end{align*}$$ Then $X_n$ does not convergence almost surely (since for any $\\omega \\in (0,1]$ and $N \\in \\mathbb{N}$ there exist $m,n \\geq N$ such that $X_n(\\omega)=1$ and $X_m(\\omega)=0$). On the other hand, since $$\\mathbb{P}(|X_n|>0) \\to 0  \\qquad \\text{as} \\, \\,  n \\to \\infty,$$ it follows easily that $X_n$ converges in probability to $0$.\nConvergence in distribution does not imply convergence in probability: Take any two random variables $X$ and $Y$ such that $X \\neq Y$ almost surely but $X=Y$ in distribution. Then the sequence $$X_n := X, \\qquad n \\in \\mathbb{N}$$ converges in distribution to $Y$. On the other hand, we have $$\\mathbb{P}(|X_n-Y|>\\epsilon) = \\mathbb{P}(|X-Y|>\\epsilon) >0$$ for $\\epsilon>0$ sufficiently small, i.e. $X_n$ does not converge in probability to $Y$.\nConvergence in probability does not imply convergence in $L^p$ I: Consider the probability space $((0,1],\\mathcal{B}((0,1]),\\lambda|_{(0,1]})$ and define $$X_n(\\omega) := \\frac{1}{\\omega} 1_{\\big(0, \\frac{1}{n}\\big]}(\\omega).$$ It is not difficult to see that $X_n \\to 0$ almost surely; hence in particular $X_n \\to 0$ in probability. As $X_n \\notin L^1$, convergence in $L^1$ does not hold. Note that $L^1$-convergence fails because the random variables are not integrable.\nConvergence in probability does not imply convergence in $L^p$ II: Consider the probability space $((0,1],\\mathcal{B}((0,1]),\\lambda|_{(0,1]})$ and define $$X_n(\\omega) := n 1_{\\big(0, \\frac{1}{n}\\big]}(\\omega).$$ Then $$\\mathbb{P}(|X_n|>\\epsilon) = \\frac{1}{n} \\to 0 \\qquad \\text{as} \\, \\, n \\to \\infty$$ for any $\\epsilon \\in (0,1)$. This shows that $X_n \\to 0$ in probability. Since $$\\mathbb{E}X_n = n \\cdot \\frac{1}{n} = 1$$ the sequence does not converge to $0$ in $L^1$. Note that $L^1$-convergence fails although the random variables are integrable. (Just as a side remark: This example shows that convergence in probability does also not imply convergence in $L^p_{\\text{loc}}$.)", "meta": {"post_id": 1170559, "input_score": 25, "output_score": 41, "post_title": "Convergence types in probability theory : Counterexamples"}}
{"input": "I'm doing a little project on the $\\zeta$ function, and I am at a complete loss of what it is actually doing.  I understand it is way over my head, but when I am plugging say $\\zeta(1 + i)$ into WolframAlpha, what is it even calculating? Wikipedia and .edu sites don't seem to have an answer, which is making me think there is no single answer.\nThanks, smart people of MathStack!\nEDIT:\nWhy is everyone voting to close this? I understand it's similar to a different question, but this question might provide someone with some different intuition.  Either way - this helped me a ton! Thanks @Mixed_Math.", "output": "The Riemann zeta function $\\zeta(s)$ is a sum of reciprocals of powers of natural numbers,\n$$\\zeta(s) = \\sum_{n \\geq 1} \\frac{1}{n^s}.$$\nAs written, this makes sense for complex numbers $s$ so long as $\\text{Re } s > 1$. For these numbers, there is little more to be said.\nBut you've asked about an interesting number: $\\zeta(1 + i)$, and $\\text{Re }(1 + i) \\not > 1$. What's happening there is a bit subtle, and a bit abusive in terms of notation.\nIt turns out there is another function (let's call it $Z(s)$) which makes sense for all complex numbers $s$ except for $s = 1$, and which exactly agrees with $\\zeta(s)$ when $\\text{Re } s > 1$. If you're familiar with some calculus or complex analysis, then you should also know that the function $Z(s)$ is also complex differentiable everywhere except for $s = 1$. This is a very special property that distinguishes $Z(s)$. The theory of complex analysis (in particular, the theory of \"analytic continuation\") gives that there can be at most one function that extends $\\zeta(s)$ to a larger region, like $Z(s)$ does.\nIn this sense, we could realize that $Z(s)$ is uniquely determined by $\\zeta(s)$. As it agrees with $\\zeta(s)$ everywhere $\\zeta(s)$ (initially) makes sense, it might even be reasonable to just use the name $\\zeta(s)$ instead of $Z(s)$. That is, when I write $\\zeta(s)$, what I'm really saying is\n$$\\zeta(s) = \\begin{cases}\n\\zeta(s) & \\text{if Re }s > 1 \\\\\nZ(s) & \\text{otherwise }\n\\end{cases}$$\nIt is this function that W|A computes when you ask it for $\\zeta(1 + i)$.\nAlthough what I've written is true (and important), it doesn't answer one aspect of your question\n\nWhat is it even calculating?\n\nI mentioned there exists this function $Z(s)$, or rather that it is possible to give meaningful values to $\\zeta(s)$ for all $s \\neq 1$. But how? Stated differently, yo're asking what is the analytic continuation of the Riemann zeta function?\nThe continuation is unique, but the steps to get there are not. I'll give a very short, incomplete proof that describes one way to calculate $\\zeta(1+i)$.\nWe start by considering $\\displaystyle h(s) =  \\sum_{n \\geq 1} \\frac{2}{(2n)^s}$. Performing some rearrangements,\n$$\\begin{align}\nh(s) &=  \\sum_{n \\geq 1} \\frac{2}{(2n)^s} \\\\\n&= \\frac{1}{2^{s - 1}} \\sum_{n \\geq 1} \\frac{1}{n^s} \\\\\n&= \\frac{1}{2^{s - 1}} \\zeta(s)\n\\end{align}$$\nLet's subtract this from the regular zeta function. On the one hand,\n$$ \\zeta(s) - h(s) = \\zeta(s)(1 - \\frac{1}{2^{s-1}}).$$\nOn the other hand,\n$$ \\begin{align}\\zeta(s) - h(s) &= \\sum_{n \\geq 1} \\left( \\frac{1}{n^s} - \\frac{2}{(2n)^s} \\right) \\\\\n&= \\sum_{n \\geq 1} \\frac{(-1)^{n+1}}{n^s},\n\\end{align}$$\nand this last series makes sense for $\\text{Re } s > 0$. (If you haven't looked at alternating series before, this might not be obvious. But the idea is that the sign changes cancel out a lot of the growth, so much that it converges for a larger region).\nIn total, this means that\n$$\\zeta(s) = (1 - 2^{s - 1})^{-1} \\sum_{n \\geq 1} \\frac{(-1)^{n+1}}{n^s},$$\nand you can just \"plug in\" $1+i$ here. [Notice that the problem when $s = 1$ is apparent here, as you cannot divide by $0$.] In practice, it's an infinite sum, so you'll take the first very many terms to get the value of $\\zeta(1+i)$ to any precision you want.\nFor completeness, it also turns out that\n$$\\pi^{-s/2} \\zeta(s) \\Gamma(\\tfrac{s}{2}) = \\pi^{(s-1)/2} \\zeta(1-s) \\Gamma(\\tfrac{1-s}{2}),$$\nwhich lets us transform values of  $\\zeta(s)$ for $\\text{Re } s > 0$ into values when $\\text{Re } s < 1$. The $\\Gamma(z)$ function here is called the \"Gamma function\" (it's an integral, a sort of generalization of a factorial) and this equation is called the symmetric functional equation of the zeta function.", "meta": {"post_id": 1180388, "input_score": 14, "output_score": 38, "post_title": "What exactly *is* the Riemann zeta function?"}}
{"input": "I am having trouble seeing the difference between weak and strong induction.\n\nThere are a few examples in which we can see the difference, such as reaching the $k^{th}$ rung of a ladder and proving every integer $>1$ can be written as a product of primes:\n\nTo show every $n\\ge2$ can be written as a product of primes, first we note that $2$ is prime. Now we assume true for all integers $2 \\le m<n$. If $n$ is prime, we're done. If $n$ is not prime, then it is composite and so $n=ab$, where $a$ and $b$ are less than $n$. Since $a$ and $b$ are less than $n$, $ab$ can be written as a product of primes and hence $n$ can be written as a product of primes. QED\n\n\nHowever, it seems sort of like weak induction, only a bit dubious. In weak induction, we show a base case is true, then we assume true for all integers $k-1$, (or $k$), then we attempt to show it is true for $k$, (or $k+1$), which implies true $\\forall n \\in \\mathbb N$.\nWhen we assume true for all integers $k$, isn't that the same as a strong induction hypothesis? That is, we're assuming true for all integers up to some specific one.\n\nAs a simple demonstrative example, how would we show $1+2+\\cdots+n= {n(n+1) \\over 2}$ using strong induction?\n(Learned from Discrete Mathematics by Kenneth Rosen)", "output": "Initial remarks: Good question. I think it deserves a full response (warning: this is going to be a long, but hopefully very clear, answer). First, most students do not really understand why mathematical induction is a valid proof technique. That's part of the problem. Second, weak induction and strong induction are actually logically equivalent; thus, differentiating between these forms of induction may seem a little bit difficult at first. The important thing to do is to understand how weak and strong induction are stated and to clearly understand the differences therein (I disagree with the previous answer that it is \"just a matter of semantics\"; it's not, and I will explain why). Much of what I will have to say is adapted from David Gunderson's wonderful book Handbook of Mathematical Induction, but I have expanded and tweaked a few things where I saw fit. That being said, hopefully you will find the rest of this answer to be informative.\n\nGunderson remark about strong induction: While attempting an inductive proof, in the inductive step one often needs only the truth of $S(n)$ to prove $S(n+1)$; sometimes a little more \"power\" is needed (such as in the proof that any positive integer $n\\geq 2$ is a product of primes--we'll explore why more power is needed in a moment), and often this is made possible by strengthening the inductive hypothesis. \n\nKenneth Rosen remark in Discrete Mathematics and Its Applications Study Guide: Understanding and constructing proofs by mathematical induction are extremely difficult tasks for most students. Do not be discouraged, and do not give up, because, without doubt, this proof technique is the most important one there is in mathematics and computer science. Pay careful attention to the conventions to be observed in writing down a proof by induction. As with all proofs, remember that a proof by mathematical induction is like an essay--it must have a beginning, a middle, and an end; it must consist of complete sentences, logically and aesthetically arranged; and it must convince the reader. Be sure that your basis step (also called the \"base case\") is correct (that you have verified the proposition in question for the smallest value or values of $n$), and be sure that your inductive step is correct and complete (that you have derived the proposition for $k+1$, assuming the inductive hypothesis that proposition is true for $k$--or the slightly strong hypothesis that it is true for all values less than or equal to $k$, when using strong induction. \n\nStatement of weak induction: Let $S(n)$ denote a statement regarding an integer $n$, and let $k\\in\\mathbb{Z}$ be fixed. If\n\n(i) $S(k)$ holds, and\n(ii) for every $m\\geq k, S(m)\\to S(m+1)$,\n\nthen for every $n\\geq k$, the statement $S(n)$ holds.\n\nStatement of strong induction: Let $S(n)$ denote a statement regarding an integer $n$. If \n\n(i) $S(k)$ is true and\n(ii) for every $m\\geq k, [S(k)\\land S(k+1)\\land\\cdots\\land S(m)]\\to S(m+1)$,\n\nthen for every $n\\geq k$, the statement $S(n)$ is true. \n\nProof of strong induction from weak: Assume that for some $k$, the statement $S(k)$ is true and for every $m\\geq k, [S(k)\\land S(k+1)\\land\\cdot\\land S(m)]\\to S(m+1)$. Let $B$ be the set of all $n>m$ for which $S(n)$ is false. If $B\\neq\\varnothing, B\\subset\\mathbb{N}$ and so by well-ordering, $B$ has a least element, say $\\ell$. By the definition of $B$, for every $k\\leq t<\\ell, S(t)$ is true. The premise of the inductive hypothesis is true, and so $S(\\ell)$ is true, contradicting that $\\ell\\in B$. Hence $B=\\varnothing$. $\\blacksquare$\n\nProof of weak induction from strong: Assume that strong induction holds (in particular, for $k=1$). That is, assume that if $S(1)$ is true and for every $m\\geq 1, [S(1)\\land S(2)\\land\\cdots\\land S(m)]\\to S(m+1)$, then for every $n\\geq 1, S(n)$ is true. \nObserve (by truth tables, if desired), that for $m+1$ statements $p_i$,\n$$\n[p_1\\to p_2]\\land[p_2\\to p_3]\\land\\cdots\\land[p_m\\to p_{m+1}]\\Rightarrow[(p_1\\land p_2\\land\\cdots\\land p_m)\\to p_{m+1}],\\tag{$\\dagger$}\n$$\nitself a result provable by induction (see end of answer for such a proof). \nAssume that the hypotheses of weak induction are true, that is, that $S(1)$ is true, and that for arbitrary $t, S(t)\\to S(t+1)$. By repeated application of these recent assumptions, $S(1)\\to S(2), S(2)\\to S(3),\\ldots, S(m)\\to S(m+1)$ each hold. By the above observation, then\n$$\n[S(1)\\land S(2)\\land\\cdots\\land S(m)]\\to S(m+1).\n$$\nThus the hypotheses of strong induction are complete, and so one concludes that for every $n\\geq 1$, the statement $S(n)$ is true, the consequence desired to complete the proof of weak induction. $\\blacksquare$\n\nProving any positive integer $n\\geq 2$ is a product of primes using strong induction: Let $S(n)$ be the statement \"$n$ is a product of primes.\"\nBase step ($n=2$): Since $n=2$ is trivially a product of primes (actually one prime, really), $S(2)$ is true. \nInductive step: Fix some $m\\geq 2$, and assume that for every $t$ satisfying $2\\leq t\\leq m$, the statement $S(t)$ is true. To be shown is that\n$$\nS(m+1) : m+1 \\text{ is a product of primes},\n$$\nis true. If $m+1$ is a prime, then $S(m+1)$ is true. If $m+1$ is not prime, then there exist $r$ and $s$ with $2\\leq r\\leq m$ and $2\\leq s\\leq m$ so that $m+1=rs$. Since $S(r)$ is assumed to be true, $r$ is a product of primes [note: This is where it is imperative that we use strong induction; using weak induction, we cannot assume $S(r)$ is true]; similarly, by $S(s), s$ is a product of primes. Hence $m+1=rs$ is a product of primes, and so $S(m+1)$ holds. Thus, in either case, $S(m+1)$ holds, completing the inductive step.\nBy mathematical induction, for all $n\\geq 2$, the statement $S(n)$ is true. $\\blacksquare$\n\nProof of $1+2+3+\\cdots+n = \\frac{n(n+1)}{2}$ by strong induction: Using strong induction here is completely unnecessary, for you do not need it at all, and it is only likely to confuse people as to why you are using it. It will proceed just like a proof by weak induction, but the assumption at the outset will look different; nonetheless, just to show what I am talking about, I will prove it using strong induction.\nLet $S(n)$ denote the proposition\n$$\nS(n) : 1+2+3+\\cdots+n = \\frac{n(n+1)}{2}.\n$$\nBase step ($n=1$): $S(1)$ is true because $1=\\frac{1(1+1)}{2}$. \nInductive step: Fix some $k\\geq 1$, and assume that for every $t$ satisfying $1\\leq t\\leq k$, the statement $S(t)$ is true. To be shown is that\n$$\nS(k+1) : 1+2+3+\\cdots+k+(k+1)=\\frac{(k+1)(k+2)}{2}\n$$\nfollows. Beginning with the left-hand side of $S(k+1)$,\n\\begin{align}\n\\text{LHS} \n&= 1+2+3+\\cdots+k+(k+1)\\tag{by definition}\\\\[1em]\n&= (1+2+3+\\cdots+k)+(k+1)\\tag{group terms}\\\\[1em]\n&= \\frac{k(k+1)}{2}+(k+1)\\tag{by $S(k)$}\\\\[1em]\n&= (k+1)\\left(\\frac{k}{2}+1\\right)\\tag{factor out $k+1$}\\\\[1em]\n&= (k+1)\\left(\\frac{k+2}{2}\\right)\\tag{common denominator}\\\\[1em]\n&= \\frac{(k+1)(k+2)}{2}\\tag{desired expression}\\\\[1em]\n&= \\text{RHS},\n\\end{align}\nwe obtain the right-hand side of $S(k+1)$. \nBy mathematical induction, for all $n\\geq 1$, the statement $S(n)$ is true. $\\blacksquare$\n$\\color{red}{\\text{Comment:}}$ See how this was really no different than how a proof by weak induction would work? The only thing different is really an unnecessary assumption made at the beginning of the proof. However, in your prime number proof, strong induction is essential; otherwise, we cannot assume $S(r)$ or $S(s)$ to be true. Here, any assumption regarding $t$ where $1\\leq t\\leq k$ is really useless because we don't actually use it anywhere in the proof, whereas we did use the assumptions $S(r)$ and $S(s)$ in the prime number proof, where $1\\leq t\\leq m$, because $r,s < m$. Does it now make sense why it was necessary to use strong induction in the prime number proof? \n\nProof of $(\\dagger)$ by induction: For statements $p_1,\\ldots,p_{m+1}$, we have that\n$$\n[p_1\\to p_2]\\land[p_2\\to p_3]\\land\\cdots\\land[p_m\\to p_{m+1}]\\Rightarrow[(p_1\\land p_2\\land\\cdots\\land p_m)\\to p_{m+1}].\n$$ \nProof. For each $m\\in\\mathbb{Z^+}$, let $S(m)$ be the statement that for $m+1$ statements $p_i$,\n$$\nS(m) : [p_1\\to p_2]\\land[p_2\\to p_3]\\land\\cdots\\land[p_m\\to p_{m+1}]\\Rightarrow[(p_1\\land p_2\\land\\cdots\\land p_m)\\to p_{m+1}].\n$$\nBase step: The statement $S(1)$ says\n$$\n[p_1\\to p_2]\\Rightarrow [(p_1\\land p_2)\\to p_2],\n$$\nwhich is true (since the right side is a tautology). \nInductive step: Fix $k\\geq 1$, and assume that for any statements $q_1,\\ldots,q_{k+1}$, both\n$$\nS(1) : [q_1\\to q_2]\\Rightarrow [(q_1\\land q_2)\\to q_2]\n$$\nand\n$$\nS(k) : [q_1\\to q_2]\\land[q_2\\to q_3]\\land\\cdots\\land[q_k\\to q_{k+1}]\\Rightarrow[(q_1\\land q_2\\land\\cdots\\land q_k)\\to q_{k+1}].\n$$\nhold. It remains to show that for any statements $p_1,p_2,\\ldots,p_k,p_{k+1},p_{k+2}$ that\n$$\nS(k+1) : [p_1\\to p_2]\\land[p_2\\to p_3]\\land\\cdots\\land[p_{k+1}\\to p_{k+2}]\\Rightarrow[(p_1\\land p_2\\land\\cdots\\land p_{k+1})\\to p_{k+2}]\n$$\nfollows. Beginning with the left-hand side of $S(k+1)$,\n\\begin{align}\n\\text{LHS} &\\equiv [p_1\\to p_2]\\land\\cdots\\land[p_{k+1}\\to p_{k+2}]\\land[p_{k+1}\\to p_{k+2}]\\\\[0.5em]\n&\\Downarrow\\qquad \\text{(definition of conjunction)}\\\\[0.5em]\n&[[p_1\\to p_2]\\land[p_2\\to p_3]\\land\\cdots\\land[p_{k+1}\\to p_{k+2}]]\\land[p_{k+1}\\to p_{k+2}]\\\\[0.5em]\n&\\Downarrow\\qquad \\text{(by $S(k)$ with each $q_i = p_i$)}\\\\[0.5em]\n&[(p_1\\land p_2\\land\\cdots\\land p_k)\\to p_{k+1}]\\land[p_{k+1}\\to p_{k+2}]\\\\[0.5em]\n&\\Downarrow\\qquad \\text{(by $S(1)$ with $q_1=p_1\\land\\cdots\\land p_k)$ and $q_2=p_{k+1}$)}\\\\[0.5em]\n&[[(p_1\\land p_2\\land\\cdots\\land p_k)\\land p_{k+1}]\\to p_{k+1}]\\land [p_{k+1}\\to p_{k+2}]\\\\[0.5em]\n&\\Downarrow\\qquad \\text{(by definition of conjunction)}\\\\[0.5em]\n&[(p_1\\land p_2\\land\\cdots\\land p_k\\land p_{k+1}]\\to p_{k+1}]\\land [p_{k+1}\\to p_{k+2}]\\\\[0.5em]\n&\\Downarrow\\qquad \\text{(since $a\\land b\\to b$ with $b=[p_{k+1}\\to p_{k+2}]$)}\\\\[0.5em]\n&[(p_1\\land p_2\\land\\cdots\\land p_k\\land p_{k+1})\\to p_{k+2}]\\land[p_{k+1}\\to p_{k+2}]\\\\[0.5em]\n&\\Downarrow\\qquad \\text{(since $a\\land b\\to a$)}\\\\[0.5em]\n&(p_1\\land p_2\\land\\cdots\\land p_k\\land p_{k+1})\\to p_{k+2}\\\\[0.5em]\n&\\equiv \\text{RHS},\n\\end{align}\nwe obtain the right-hand side of $S(k+1)$, which completes the inductive step.\nBy mathematical induction, for each $n\\geq 1, S(n)$ holds. $\\blacksquare$", "meta": {"post_id": 1184541, "input_score": 57, "output_score": 85, "post_title": "What exactly is the difference between weak and strong induction?"}}
{"input": "Let $f\\colon \\mathbb R^+\\to\\mathbb R$ be a function that satisfies the following conditions:\n$$\\tag1 \\lim_{x\\to 1}f(x)=0 $$\n$$\\tag2f(x_1)+f(x_2)=f(x_1x_2)$$\nShow that $f$ is continuous in its domain.\nI managed to show that $f$ is continuous at $x=1$, but I have no idea how to continue from there. Here's what I've done so far:\nBecause $\\lim_{x\\to 1}f(x)=0$, for every \u03f5>0 there exists a \u03b4>0 so that\n$$0<|x - 1|<\u03b4\u21d2|f(x)-0|<\u03f5$$ \nTo prove continuity at $x=1$ it's enough to show that $f(1)=0$ using the condition 2):\n$$f(1)+f(1)=f(1 \u00b71)$$\n$$f(1)=f(1)-f(1)$$\n$$f(1)=0$$\nSo now we have the definition of continuity at $x=1$:\n$$|x - 1|<\u03b4\u21d2|f(x)-f(1)|<\u03f5$$", "output": "You have shown continuity at $x=1$, i.e., $$\\lim_{x\\to1}f(x)=f(1).$$\nConsequently, for any $x_0\\ne0$\n$$\\lim_{x\\to x_0}f(x)=\\lim_{x\\to x_0}f\\left(\\frac x{x_0}\\cdot x_0\\right)=\\lim_{x\\to x_0}\\left(f\\left(\\frac x{x_0}\\right)+f(x_0)\\right)=f(1)+f(x_0)=f(x_0).$$", "meta": {"post_id": 1198295, "input_score": 15, "output_score": 34, "post_title": "Prove that function is continuous without knowing the function explicitly"}}
{"input": "I'm a student in an elementary linear algebra course. Without bashing on my professor, I must say that s/he is very poor at answering questions, often not addressing the question itself. Throughout the course, there have been multiple questions that have gone unanswered, but I must have the answer to this one question.\n\"Why are some vectors written as row vectors and others as column vectors?\"\nI understand that if I transpose one, it becomes the other. However, I'd like to understand the purpose behind writing a vector in a certain format.\nTaking examples from my lectures, I see that when I'm trying to prove linear independence of a group of vectors, the vectors are written as column vectors in a matrix, and the row reduced form is found.\nOther times, like trying to find the cross product or just solving a matrix, I see the vectors written as row vectors.\nMy professor is very vague on the notations and explanations, and it bugs me as a person who needs to know the reason behind every small thing, why this variation occurs in the format. Any input is greatly appreciated.", "output": "In one sense, you can say that a vector is simply an object with certain\nproperties, and it is neither a row of numbers nor a column of numbers.\nBut in practice, we often want to use a list of $n$ numeric coordinates to describe\nan $n$-dimensional vector, and we call this list of coordinates a vector.\nThe general convention seems to be that the coordinates are listed in the\nformat known as a column vector, which is (or at least, which acts like)\nan $n \\times 1$ matrix.\nThis has the nice property that if $v$ is a vector and $M$ is a matrix\nrepresenting a linear transformation, the product $Mv$, computed by the usual\nrules of matrix multiplication, is another vector (specifically, a column vector)\nrepresenting the image of $v$ under that transformation.\nBut because we write mostly in a horizontal direction, it is not always\nconvenient to list the coordinates of a vector from left to right.\nIf you're careful, you might write\n$$ \\langle x_1, x_2, \\ldots, x_n \\rangle^T $$\nmeaning the transpose of the row vector $\\langle x_1, x_2, \\ldots, x_n \\rangle$;\nthat is, we want the convenience of left-to-right notation but we\nmake it clear that we actually mean a column vector\n(which is what you get when you transpose a row vector).\nIf we're not being careful, however, we might just write\n$\\langle x_1, x_2, \\ldots, x_n \\rangle$\nas our \"vector\" and assume everyone will understand what we mean.\nOccasionally, we actually need the coordinates of a vector in row-vector format,\nin which case we can represent that by transposing a column vector.\nFor example, if $u$ and $v$ are vectors (that is, column vectors), then the\nusual inner product of $u$ and $v$ can be written $u^T v$, evaluated as\nthe product of a $1\\times n$ matrix with an $n \\times 1$ matrix.\nNote that if $u$ is a (column) vector, then $u^T$\nreally is a row vector and can (and should) legitimately be written as\n$\\langle u_1, u_2, \\ldots, u_n \\rangle$.\nThis all works out quite neatly and conveniently when people are careful\nand precise in how they write things.\nAt a deeper and more abstract level, you can formalize these ideas as shown in\nanother answer.\n(My answer here is relatively informal, intended merely to give a sense of why\npeople think of the column vector as \"the\" representation of an abstract vector.)\nWhen people are not careful and precise, it may help to say to yourself sometimes\nthat the transpose of a certain vector representation is intended in a\ncertain context even though the person writing that representation\nneglected to indicate it.", "meta": {"post_id": 1198729, "input_score": 45, "output_score": 36, "post_title": "Row vector vs. Column vector"}}
{"input": "First of all, I am doing some mathematical background information for a software I am creating. \nWhat I want to achieve is the point on an object rotating towards where the mouse is. Like in tank games, where the turret rotated depending on mouseX and mouseY \nIn terms of programming, this can be achieved by using the atan2 function, that returns an angle between two points (I believe).\nWhat I want to do is find the angle between an object, and mouse click.\n\nIs there a special maths formula for this? Because my research on google, brings 'atan2' since I'm a programmer. Since most languages have their own maths library, it is abstraction. I want to know how the formula works, in terms of maths. \nThe question is, what is the formula called for this; to find the angle between two points. So my object can rotate towards the mouseX and mouseY position", "output": "The atan2 function is an extended version of the trigonometric inverse tangent function. In the figure below, $\\tan{\\theta}=y/x$, so $\\tan^{-1}{y/x}=\\theta$. The atan2 function just calculates $\\tan^{-1}{y/x}$ with y and x as separate parameters. The reason it does so is to give a more accurate answer: since the tangent function is periodic, there are multiple values of x and y that would appear to have the same angle (but do not). Also, atan2 provides the correct values when y/x is undefined, such as at $\\pi/2$.\n\nAnother way to find the angle you're looking for is to use vectors. Using the x-axis as one of the vectors and $\\vec{OP}$ as another one, you could use the formula \n$$\\cos{\\theta}=\\frac{u\\cdot v}{||u||\\times||v||}$$\nNote that whichever way you use, you need two lines to measure an angle. You would have to choose a reference line to measure the angle $\\theta$ with; most commonly one would use the x-axis.", "meta": {"post_id": 1201337, "input_score": 20, "output_score": 44, "post_title": "Finding the angle between two points"}}
{"input": "I am looking for applications of the Eckmann\u2013Hilton argument. I found one application in Algebraic Topology where we show that the fundamental group is abelian in case of a topological group. Thank You.", "output": "There are tons of applications! There are also many useful reformulations. The following will be more or less in increasing level of sophistication. But first, a silly picture:\n\nI've written a blog post which expands a lot upon this answer, if you're interested.\n\nAs you mentioned, it can be used to prove that the fundamental group of a topological group is abelian. More generally, the fundamental group of an $H$-space is commutative, for exactly the same reasons.\nIt also shows that the higher homotopy groups of any space are abelian. This is a \"particular case\" of the preceding point, because $\\pi_n(X) \\cong \\pi_1(\\Omega^{n-1}X)$ and $\\Omega^{n-1}X$ (the iterated loop space) is an $H$-space (for $n-1 \\ge 1 \\iff n \\ge 2$).\nA monoid object in the category of monoids is a commutative monoid. This is literally the statement of the Eckmann\u2013Hilton argument if you think about it, but it's a very useful reformulation, because it helps in generalizing it to other contexts. Similarly, a group object in the category of groups is an abelian group.\nIf $X$ is a cogroup object and $G$ is a group object in a category $\\mathsf{C}$, then $\\hom_\\mathsf{C}(X,G)$ is an abelian group. This is a generalization of the first point: $S^1$ is a cogroup object via the pinching map, and $\\pi_1(G) \\cong \\hom_{\\mathsf{Ho}(\\mathsf{Top}_*)}(S^1, G)$ for a pointed space $G$. So if $G$ is a topological group, you immediately get that $\\pi_1(G)$ is commutative.\nThe center $Z(\\mathsf{C}$) of a category $\\mathsf{C}$ is defined to be the monoid of natural transformations $\\operatorname{id}_\\mathsf{C} \\to \\operatorname{id}_\\mathsf{C}$. If $\\mathsf{C}$ is a category with one object, AKA a monoid, it's straightforward to check that this coincides with the notion of center of a monoid: $Z(M) = \\{ x \\in M : xy = yx \\, \\forall y \\}$. And just like the center of a monoid is commutative, the center of a category is commutative. This follows from the Eckmann\u2013Hilton argument: elements of $Z(\\mathsf{C})$ can either be composed horizontally or vertically, and the two are compatible.\nAnother notion of \"center of a category\" exists: when $(\\mathsf{C}, \\otimes, 1)$ is a monoidal category, you can consider the monoid $Z(\\mathsf{C}, \\otimes) = \\operatorname{End}(1)$ of endomorphisms of the unit object $1$. This has two multiplications: the usual composition, and the tensor product (if $f,g : 1 \\to 1$, then $f \\otimes g : 1 \\cong 1 \\otimes 1 \\xrightarrow{f \\otimes g} 1 \\otimes 1 \\cong 1$). With a bit of work you can show that this second law is well-defined, and compatible with composition, thus by the Eckmann\u2013Hilton argument $\\operatorname{End}(1)$ is commutative.\nA generalization of the two notions of the center of a category I talked about is the following. Let $\\mathsf{C}$ be a $2$-category, then the endomorphisms of the identity $\\operatorname{End}(\\operatorname{id}_x)$ of any object $x \\in \\mathsf{C}$ has two multiplications, vertical and horizontal composition. Thus the two laws are the same and it's a commutative monoid.\n\nTo recover the center $Z(\\mathsf{C})$, consider the $2$-category $\\mathsf{Cat}$ of categories and let $\\mathsf{C} \\in \\mathsf{Cat}$, then $Z(\\mathsf{C}) = \\operatorname{End}(\\operatorname{id}_\\mathsf{C})$.\nTo recover the center $Z(\\mathsf{C}, \\otimes)$, consider the monoidal category $(\\mathsf{C}, \\otimes)$ as a $2$-category $\\Sigma \\mathsf{C}$ with a single object $*$ (this is the \"suspension\"), then $Z(\\mathsf{C}, \\otimes) = \\operatorname{End}(\\operatorname{id}_*)$.\n\n(The following is more advanced, and purposefully vague to not get bogged down in technical details.)\n\nA category is a category. (I didn't manage to get Markdown to start the list at 0, but consider this to be the zeroth case.)\nA monoidal category is (more or less) a monoid object in the category of categories.\nA $2$-fold monoid object in the category of categories (ie. a category $\\mathsf{C}$ with two bifunctors $\\otimes_1, \\otimes_2 : \\mathsf{C}^2 \\to \\mathsf{C}$ that are \"compatible\" in the Eckmann\u2013Hilton sense) is a braided monoidal category, not a symmetric monoidal category, as one might expect!\nBut a $3$-fold monoidal category is a symmetric monoidal category, and it stabilizes: a $k$-fold monoidal category is symmetric for $k \\ge 3$.\n\nThis is a higher version of the Eckmann\u2013Hilton argument. In Higher category theory you have similar generalizations, by considering $k$-fold monoidal $n$-categories. You should look at the table 21 in Baez & Dolan's article cited below.\nAn $E_n$-algebra (ie. an algebra over the little disks operad) is, in some sense, a (strongly) homotopy-associative algebra, which is homotopy commutative \"up to level $n$\". One can make an alternative definition in the spirit of Eckmann\u2013Hilton: consider a symmetric monoidal $(\\infty, 1)$-category $\\mathsf{C}$. Then an $E_n$-algebra in $\\mathsf{C}$ can be defined as an object $A$ equipped with $n$ compatible multiplications (cf. Lurie below).\nThe link with braided monoidal categories can also be made here. The nerve of an $n$-fold monoidal category is an $E_n$-algebra (cf. Balteanu\u2013Fiedorowicz\u2013Schw\u00e4nzl\u2013Vogt below).\n\n\nUseful references, and also interesting articles related to what I said at the end:\n\nEckmann\u2013Hilton argument at Wikipedia.\nThese two blog posts by John Baez: number 1, number 2. The second one has a very nice graphical representation of the argument.\nJohn C. Baez and James Dolan. \u201cHigher-dimensional algebra and topological\nquantum field theory\u201d. In: J. Math. Phys. 36.11 (1995), pp.\u00a06073\u20136105. \u026ass\u0274:\u00a00022-2488. \u1d05\u1d0f\u026a:\u00a010.1063/1.531236. arXiv:\u00a0q-alg/9503002 [math.QA]. MR1355899.\nC. Balteanu, Z. Fiedorowicz, R. Schw\u00e4nzl, and R. Vogt. \u201cIterated monoidal categories\u201d. In: Adv. Math. 176.2 (2003),\npp.\u00a0277\u2013349. \u026ass\u0274:\u00a00001-8708. \u1d05\u1d0f\u026a:\u00a010.1016/S0001-8708(03)00065-3. arXiv:\u00a0math/9808082 [math.AT]. MR1982884.\nJacob Lurie. \u201cOn the classification of topological field theories\u201d. In: Current developments in mathematics, 2008. Int. Press, Somerville, MA, 2009, pp.\u00a0129\u2013280. arXiv:\u00a00905.0465 [math.CT]. MR2555928.", "meta": {"post_id": 1203807, "input_score": 15, "output_score": 40, "post_title": "Applications of Eckmann\u2013Hilton argument"}}
{"input": "When it comes to integration on manifolds, I speak two languages. The first is of course the language of differential forms, which is something I am relatively well acquainted with.\nThe second language is what is often used in general relativity books that follow a rather traditionalist viewpoint (eg. no explicit usage of modern differential geometry, just coordinate and index-based tensor-analysis with \"funky coordinates\"), which try to define \"tensor densities\" as indexed objects that pick up some power of the jacobian determinant during coordinate change, in addition to the usual jacobian matrices on the tensor indexes. It is then stated that these \"densities\" can be integrated invariantly.\nThese concepts generally do not come with any rigorous, or even semi-rigorous explanation, just stated and then used without much thought.\nIn time I came to understand \"scalar densities\" as the single independent component of a top-order differential form expressed in some coordinate basis, seeing as this component transforms by picking up the jacobian determinant, which comes directly from the skew-symmetric properties of the differential form, as well as the fact that a top-order form has one independent component only.\nOn the other hand, I have been told by a mathematician whose Lie-groups course I have been attending that densities are perfectly well defined mathematical objects that can be used to integrate on manifolds that aren't orientable.\nI understand the general idea behind densities, but I have no idea how to rigorously define objects like these, neither do I know how any concrete density even \"looks like\".\nI mean, if someone asked me the same thing I am asking here, but with differential forms, I would\n\nExplain to them algebraic exterior forms on finite dimensional vector spaces, exterior products, etc.\n\nShow them that on a tangent space of a differential manifold, any exterior form can be written as a linear combination of wedge products of coordinate differentials, eg. $\\mathrm{d}x^\\mu$.\n\nDefine fields of these objects by either showing what it means for a $p\\mapsto\\omega_p$ assignment ($p\\in M$, $\\omega_p\\in\\Lambda^kT^*_pM$) to vary smoothly or by \"bundle-izing\" exterior products of cotangent spaces over the manifold.\n\nDefine the exterior derivative and show how it works.\n\nShow that an integral of $k$-differential form over a $k$-dimensional submanifold of $M$ is independent of coordinates, basically by defining ($k=n$ for simplicity) $$ \\int_\\mathcal{D}\\omega=\\int_\\mathcal{D}F\\mathrm{d}x^1\\wedge...\\wedge\\mathrm{d}x^n=\\int_{\\Psi(\\mathcal{D})}F(x^1...x^n)dx^1...dx^n, $$ where $\\Psi$ is the coordinate map, and the right side is a standard Riemann-integral or an integral against the standard Lebesgue-measure of $\\mathbb{R}^n$ and $\\omega=F\\mathrm{d}x^1\\wedge...\\wedge\\mathrm{d}x^n$, and then showing that if I change the coordinates, everything in the integral transforms in such way that the value of the integral stays the same.\n\n\nQuestions:\n\nHow do I (mostly) rigorously define and construct a density on a manifold and how does it look like?\n\n(Eg. like, a differential $n$-form on an $n$-dimensional manifold, when expressed in a chart looks like $\\omega=F\\mathrm{d}x^1\\wedge...\\wedge\\mathrm{d}x^n$, where $F$ is a scalar function, plus I'd appreciate a similar explanation to what I outlined above for difforms).\n\nHow are they related to measures?\n\nI mean, as an example, I thought about the following:\nAs far as I am aware, every smooth non-orientable manifold is locally orientable, so I guess if I take a differential form defined in some chart, $F\\mathrm{d}x^1\\wedge...\\wedge\\mathrm{d}x^n$ (so that the basis $n$-form is nonvanishing) and then define a measure as $$ \\mu_x(\\mathcal{D})=\\left|\\int_\\mathcal{D}\\mathrm{d}x^1\\wedge...\\wedge\\mathrm{d}x^n\\right| ,$$\nthen this case the integral $$ \\int_\\mathcal{D}Fd\\mu_x $$ makes sense in the chart in a way that is independent of coordinates, because if I change the coordinates, I first transform the differential form, then construct another measure $\\mu_y$, then extend this via a partition of unity, but this isn't really explicit in the sense that I don't think $$F\\ d\\mu_x $$\nis a well defined object without the integral symbol, and I am not even sure what I'm doing here is even correct.", "output": "The basic concept is that of a density of weight $s$ (or just an $s$-density)  for any $s\\in \\mathbb R$ on an $n$-dimensional real vector space $V$, which is a map $\\mu\\colon V\\times\\dots\\times V\\to \\mathbb R$ that satisfies the following identity for every linear map $A\\colon V\\to V$:\n$$\n\\mu(AX_1,\\dots,AX_n) = |\\det A|^s\\mu(X_1,\\dots,X_n).\n$$\n(If $s<0$, you have to require this just for nonsingular $A$.)\nCompare this to an $n$-form $\\omega$, which satisfies\n$$\n\\omega(AX_1,\\dots,AX_n) = (\\det A)\\omega(X_1,\\dots,X_n).\n$$\nAny $n$-form $\\omega$ determines an $s$-density $|\\omega|^s$ by\n$$\n|\\omega|^s(X_1,\\dots,X_n) = |\\omega(X_1,\\dots,X_n)|^s.\n$$\nOn a smooth $n$-manifold $M$, for each $s$ there is a smooth real line bundle $\\Omega^s M$, whose fiber at each point $p\\in M$ is the $1$-dimensional vector space of densities of weight $s$ on $T_pM$. Any choice of coordinates $(x^i)$ yields a local frame $|dx^1\\wedge \\dots dx^n|^s$ for $\\Omega^s$, so a section of $\\Omega^s M$ (also called a density of weight $s$ on $M$) can be written locally as $f\\,|dx^1\\wedge \\dots dx^n|^s$ for some function $f$. The functions $f$ and $\\widetilde f$ associated with different coordinates transform by $\\widetilde f  = |J|^s f$, where $J$ is the Jacobian determinant of the coordinate transformation: $J=\\det (\\partial \\widetilde x^i/\\partial x^j)$.\nA density of weight $1$ can be integrated over $M$ in a coordinate-independent fashion, since the transformation law for $1$-densities exactly matches the change of variables formula for integrals. Thus $1$-densities are also sometimes called volume densities or just densities.\nA tensor density is just a section of a tensor product bundle of the form $T^{(k,l)}M\\otimes \\Omega^s M$. The transition functions for such a section are the transition functions for an ordinary $(k,l)$-tensor multiplied by $|J|^s$. To distinguish them from tensor densities, ordinary densities are sometimes called scalar densities.\nNote that the counting of weights is sometimes done differently in conformal geometry. A density of conformal weight $r$ is typically defined to be a density of weight $r/n$. This is so that the scaling of of densities matches that of lengths.\nThis is all explained pretty well in this Wikipedia article.\nADDED IN RESPONSE TO B. HUEBER'S COMMENT:\nThere are analogous objects called pseudovectors and pseudotensors that are often used in physics. They can be constructed in a very similar way.\nFor an $n$-dimensional real vector space $V$, define a pseudoscalar on $\\boldsymbol V$ to be a map $\\sigma\\colon V\\times\\cdots\\times V\\to \\mathbb R$ that satisfies the following identity for every invertible linear map $A\\colon V\\to V$:\n$$\n\\sigma(AX_1,\\dots,AX_n) = \\operatorname{sgn}(\\det A) \\sigma(X_1,\\dots,X_n),\n$$\nwhere\n$$\n\\operatorname{sgn}(\\det A) = \\frac{\\det A}{|\\det A|}  \n= \\begin{cases}\n+1 & \\text{if $A$ is orientation-preserving},\\\\\n-1 & \\text{if $A$ is orientation-reversing}.\n\\end{cases}\n$$\nAny nonzero $n$-form $\\omega$ on $V$ determines a nonzero pseudoscalar $\\sigma_\\omega$ by\n$$ \n\\sigma_\\omega(X_1,\\dots,X_n) = \\frac{\\omega(X_1,\\dots,X_n)}{|\\omega(X_1,\\dots,X_n)|},\n$$\nand every pseudoscalar is a multiple of this one.\nOn a smooth $n$-manifold $M$, there is a smooth real line bundle $\\Sigma M$, whose fiber at each point $p\\in M$ is the space of pseudoscalars on $T_pM$; a section of this line bundle is called a pseudoscalar field on $\\boldsymbol M$.  A choice of smooth local coordinates $(x^1,\\dots,x^n)$ determines a local nonvanishing $n$-form $\\omega = dx^1\\wedge\\cdots\\wedge dx^n$ and therefore a nonvanishing pseudoscalar field $\\sigma_\\omega$, which forms a local frame for the bundle $\\Sigma M$; thus every pseudoscalar field can be written locally as $\\sigma = f\\sigma_\\omega$ for some real-valued function $f$.\nIf you switch to a different coordinate chart $(\\widetilde x^1,\\dots,\\widetilde x^n)$, then the corresponding local frame satisfies\n$$\n\\sigma_{\\widetilde\\omega} = \\operatorname{sgn} \\left( \\det \\frac{\\partial \\widetilde x^i}{\\partial x^j}\\right) \\sigma_{\\omega},\n$$\nand therefore $f\\sigma_\\omega = \\widetilde f\\sigma_{\\widetilde \\omega}$, with $\\widetilde f = \\pm f$, the sign depending on whether the two charts determine the same or opposite orientations.\nNow a pseudovector field on $M$ can be defined as a section of the tensor product bundle $TM \\otimes \\Sigma M$, and similarly a pseudotensor field is a section of $T^{(k,l)}M\\otimes \\Sigma M$.\n(Note that this Wikipedia article says you can consider a pseudoscalar field to be a section of the top exterior power bundle $\\Lambda^n M$, but that's only correct if the manifold is already endowed with a metric, or at least with a global nonvanishing $n$-form or density. For the construction I gave, the absolute value of a pseudoscalar field always makes sense because $|\\widetilde f| = |f|$ in terms of the local representations I described above. But the \"absolute value\" of an $n$-form doesn't make sense unless you've chosen a global $n$-form or global density to compare against.)", "meta": {"post_id": 1210148, "input_score": 37, "output_score": 36, "post_title": "Explain densities to me please!"}}
{"input": "Can we give an example of Lebesgue non-measurable function, for which set $\\{x: f(x)=C\\}~\\forall C\\in\\mathbb{R}$ is measurable? Thanks.", "output": "Let $S$ a non-measurable subset of $]0,+\\infty[$. Define $$g(x)=\\begin{cases} x\\text{ if } x\\in S\\\\-x\\text{ if } x\\notin S\\end{cases}$$\n$g^{-1}(y)$ is finite $\\forall y\\in \\mathbb{R}$, but $\\{ g\\geq 0\\}\\setminus\\ ]-\\infty,0]=S$ is not measurable.", "meta": {"post_id": 1237593, "input_score": 19, "output_score": 35, "post_title": "Lebesgue non-measurable function"}}
{"input": "Let $f: \\Bbb C \\to \\Bbb C$ be an entire (analytic on the whole plane) function such that exists $\\omega_1,\\omega_2 \\in \\mathbb{S}^1$, linearly independent over $\\Bbb R$ such that: $$f(z+\\omega_1)=f(z)=f(z+\\omega_2), \\quad \\forall\\,z\\in \\Bbb C.$$Prove that $f$ is constant.\n\nThe intuition seems clear to me, we have the three vertices of a triangle given by $0$, $\\omega_1$ and $\\omega_2$. All points in the plane are one of the vertices of that triangle under a suitable parallel translation. The constant value will be $f(0)$, fine. Throwing values for $z$ there, I have found that $$f(n\\omega_1) = f(\\omega_1) = f(0)=f(\\omega_2) = f(n\\omega_2), \\quad \\forall\\, n \\in \\Bbb Z.$$\nI don't know how to improve the above for, say, rationals (at least). Some another ideas would be:\n\nChecking that $f' \\equiv 0$. I don't have a clue of how to do that.\nWrite $w = a\\omega_1+b\\omega_2$, with $a,b \\in \\Bbb R$, do stuff and conclude that $f(w) = f(0)$. This approach doesn't seem good, because I only have a weak result with integers above.\nFinding that $f$ coincides with $f(0)$ on a set with an accumulation point. This seems also bad: the set on with $f$ coincides with $f(0)$ by which I found above is discrete. \n\nNothing works and this is getting annoying... And I don't see how analyticity comes in there. I'll be very thankful if someone can give me an idea.\n(On a side note.. I know that this title is not informative at all. Feel free to edit if you come up with something better.)", "output": "The values the function take on the plane are the values it takes in the compact parallelogram with vertices on $0,\\omega_1,\\omega_2,\\omega_1+\\omega_2$. Therefore the entire function is bounded, and hence constant by Liouville's theorem.\n\nEvery point $z$ of the plane can be written as $z=x\\omega_1+y\\omega_2$ with $x,y$ reals, since $\\omega_1,\\omega_2$ are independent over the reals. Then $$f(z)=f(\\{x\\}\\omega_1+\\{y\\}\\omega_2),$$ where $0\\leq\\{x\\}<1$ is such that $x-\\{x\\}$ is integer. The point $\\{x\\}\\omega_1+\\{y\\}\\omega_2$ is inside the compact parallelogram with vertices $0,\\omega_1,\\omega_2,\\omega_1+\\omega_2$.", "meta": {"post_id": 1244416, "input_score": 23, "output_score": 38, "post_title": "Proving that a doubly-periodic entire function $f$ is constant."}}
{"input": "Given that $f$ and $g$ are two real functions and both are differentiable, is it true to say that $h=\\max{(f,g)} $ is differentiable too?\nThanks", "output": "No. Consider $f(x)=x$ and $g(x)=-x$. You get $\\max(f(x),g(x))=|x|$.", "meta": {"post_id": 1246224, "input_score": 18, "output_score": 61, "post_title": "Is the max of two differentiable functions differentiable?"}}
{"input": "I read the following statement in a book on Calculus, as part of my mathematics course:\n\nTechnically this separation of $\\frac{dy}{dx}$ is not mathematically valid. However, the resulting integration leads to correct answer.\n\nThe book also contains the following:\n\nTo solve a differential equation by separation of variables:\n\nget all the $x$ values on one side and all the $y$ values on the other side by multiplication and division.\nseparate $\\frac{dy}{dx}$ as if it were a fraction.\nintegrate both sides.\n\n\nNote: This box doesn't refer to a particular problem. It refers to a class of problems of differential equations which can be solved using the Method of Separation of Variables.\nMy high school mathematics teacher told me that this is the most fundamental way to solve differential equations but the textbook says it is not mathematically valid. I am not able to understand why are certain methods being followed without having a mathematical proof. Or am I wrong?", "output": "The problem with this form of separation of variables (I say \"this form\" because \"separation of variables\" can refer to multiple things) is that treating the derivative $dy/dx$ as a ratio is a purely formal algebraic manipulation. There is a way to arrive at the same results in a rigorous fashion, but textbooks often don't address this.\nTo elaborate, separation of variables in ODEs most commonly refers to a method of solving the ODE\n$$\n\\frac{dy}{dx} = g(x)h(y)\n$$\nfor the unknown function $y(x)$. Introductory textbooks often tell you to split the \"fraction\" $dy/dx\"$ and unite common variables, like so:\n$$\n\\frac{dy}{h(y)} = g(x)dx\n$$\nand then integrate both sides, as long as $h(y)\\neq 0$, to obtain\n$$\nH(y(x)) = \\int g(x)~dx + C,\n$$\nwhere $H(y)$ is an antiderivative of $\\frac{1}{h(y)}$.\nUnfortunately \"$dy$\" and \"$dx$\" have no actual mathematical meaning in this context, so all we've done is pull a little algebraic trick without understanding why it works. To resolve this, we rearrange:\n$$\n\\frac{1}{h(y)}\\frac{dy}{dx} = g(x).\n$$\nIntegrating in $x$,\n$$\n\\int \\frac{1}{h(y(x))}\\frac{dy}{dx}(x)~dx = \\int g(x)~dx + C.\n$$\nNow if $H(y)$ is an antiderivative of $1/h(y)$, then by the chain rule\n$$\n\\frac{d}{dx}H(y(x)) = \\frac{1}{h(y(x))}\\frac{dy}{dx}(x)\n$$\nso the left-hand integral is\n$$\n\\int \\frac{1}{h(y(x))}\\frac{dy}{dx}(x)~dx = \\int \\frac{d}{dx}H(y(x))~dx = H(y(x))\n$$\nleading us to our desired conclusion,\n$$\nH(y(x)) = \\int g(x)~dx + C.\n$$\nDoing it this way gives a rigorous justification of the result, but frankly the abuse of notation with the symbolic approach is much easier for most to memorize, so it is often the way it is taught to students. However, I think not explaining why the abuse of notation works confuses many students, both about the method and about their already flimsy understanding of the derivative.", "meta": {"post_id": 1252405, "input_score": 73, "output_score": 135, "post_title": "Is it mathematically valid to separate variables in a differential equation?"}}
{"input": "Could someone please indicate precisely the difference between a scalar and a vector field? I find no matter how many times I try to understand, but I always am confused in the end. So what exactly makes them different?", "output": "A scalar is a number, like 3 or 0.227.  It has a bigness (3 is bigger than 0.227) but not a direction.  Or not much of one; negative numbers go in the opposite direction from positive numbers,  but that's all.  Numbers don't go north or east or northeast.  There is no such thing as a north 3 or an east 3.\nA vector is a special kind of complicated number that has a bigness and a direction.  A vector like $(1,0)$ has bigness 1 and points east.  The vector $(0,1)$ has the same bigness but points north.  The vector $(0,2)$ also points north, but is twice as big as $(0,2)$.  The vector $(1,1)$ points northeast, and has a bigness of $\\sqrt2$, so it's bigger than $(0,1)$ but smaller than $(0,2)$.\nFor directions in three dimensions, we have vectors with three components.  $(1,0,0)$ points east. $(0,1,0)$ points north.  $(0,0,1)$ points straight up.  \nA scalar field means we take some space, say a plane, and measure some scalar value at each point.  Say we have a big flat pan of shallow water sitting on the stove.  If the water is shallow enough we can pretend that it is two-dimensional. Each point in the water has a temperature; the water over the stove flame is hotter than the water at the edges.  But temperatures have no direction.  There's no such thing as a north or an east temperature. The temperature is a scalar field: for each point in the water there is a temperature, which is a scalar, which says how hot the water is at that point.  \nA vector field means we take some space, say a plane, and measure some vector value at each point.  Take the pan of water off the stove and give it a stir.  Some of the water is moving fast, some slow, but this does not tell the whole story, because some of the water is moving north, some is moving east, some is moving northeast or other directions. Movement north and movement west could have the same speed, but the movement is not the same, because it is in different directions.  To understand the water flow you need to know the speed at each point, but also the direction that the water at that point is moving. Speed in a direction is called a \"velocity\", and the velocity of the swirling water at each point is an example of a vector field.\nI think the only other thing to know is that in one dimension, say if you had water on a long narrow pipe instead of a flat dish, vectors and scalars are the same thing, because in one dimension there is only one way to go, forwards.  Or you can go backwards, which is just like going forwards a negative amount.  But there is no north or east or northeast.  So one-dimensional vectors are interchangeable with scalars: all the vector stuff works for scalars, if you pretend that the scalars are one-dimensional vectors.\nIf this isn't clear please leave a comment.", "meta": {"post_id": 1264851, "input_score": 20, "output_score": 38, "post_title": "What is the difference between a scalar and a vector field?"}}
{"input": "$1^2 = 1$,\n$2^2 = 4$,\n$3^2 = 9$,\n$4^2 = 16$,\n$5^2 = 25$, etc... \nLooking at the difference between those square values, we get: 3, 5, 7, 9, etc...\nThe difference from one (integer) square to the next increases by 2 without fault (let's assume).\nWhy is that? Why is there that pattern of increases by 2? What is it due to? What is the source of it?\nI can \"see\" squaring visually as the construction of an actual square and I have drawn subdivided squares within squares to see the pattern unfold, but I just don't understand how to explain that increase by 2; I can't trace it, essentially.", "output": "For a visual intuition.... The inner blue region increments the gray square by one, but to make the next increment you need the extra yellow unit squares to get to the following square.", "meta": {"post_id": 1271618, "input_score": 21, "output_score": 52, "post_title": "Where does the constant increase by 2 of differences between integer square values come from?"}}
{"input": "Is every subgroup of a normal subgroup normal ?\nThat is if $H$ is a normal subgroup of a group $G$ and $K$ is a subgroup of $H$, then $K$ is a normal subgroup of $G$. Is it true ? If not what is the example?  \nProgress\n$a\\in G$ and $k\\in K$. Then $k\\in H$, since $K\\subseteq H$.\nNow, $aka^{-1}=k_1aa^{-1}=k_1\\in K$  [since $H$ is normal in $G$, $ak=k_1a$]  \nThis implies  that $K$ is normal in $H$.\nIs my approach correct ?", "output": "The silly counterexample is this: if $H$ is not normal in $G$, then we have\n$$H \\not\\lhd G\\quad G\\lhd G$$\nIndeed, this need not even be true if $K$ itself is normal in $H$. For example, in $S_4$, we have\n$$C_2 \\lhd V_4\\lhd S_4$$\nbut $C_2\\not\\lhd S_4$.\n(Here, $V_4 = \\{(1), (12)(34),(13)(24),(14)(23)\\}$ and $C_2 =  \\{(1), (12)(34)\\}$) \nThe flaw in your argument is taking $ak = k_1 a$ where $k_1\\in K$. The fact that $a\\in G$ and $H \\lhd G$ only allows you to assume that $k_1 \\in H$.", "meta": {"post_id": 1275420, "input_score": 22, "output_score": 37, "post_title": "Is every subgroup of a normal subgroup normal?"}}
{"input": "What textbooks on higher category theory are there? What books do you recommend? I am looking for self-contained introductions, no research reports. There are lots of informal summaries and arXiv papers, but I am really only asking for textbooks here.\nI know of Lurie's Higher Topos Theory, which \"only\" treats $(\\infty,1)$-categories. I am looking for books which treat $\\infty$-categories in general. Then I know of Leinster's Higher Operads, Higher Categories, which is from 2004. Is it still up to date? Is Leinster's book the best introduction to the subject? What do you think of Higher-Dimensional Categories: an illustrated guide book by Cheng and Lauda, which is also from 2004 and still a draft? Is it too informal when one really wants to work with the concepts?\nBonus question: Meanwhile, is there some \"preferred\" definition of an $\\infty$-category among the dozen definitions which have been studied?", "output": "1.\nFirst of all, make sure to have a few references on category theory available. Good ones include:\n\nBasic Category Theory (Tom Leinster);\nCategory Theory in Context (Emily Riehl);\nThe nLab.\n\n2.\nIt also pays of to learn about the insights leading to $\\infty$-categories before learning about their theory proper. A good reference here is John Baez's An Introduction to $n$-Categories. Another one is Section 1.2 of Hellstr\u00f8m-Finnsen's thesis.\n3.\n$\\infty$-Categories require two fundamental prerequisites: model category theory and simplicial sets.\nSimplicial Sets. Friedman's An elementary illustrated introduction to simplicial sets is a marvelous introduction for beginners. For more in-depth references, there are May's Simplicial Objects in Algebraic Topology, and Simplicial Homotopy Theory by Goerss\u2013Jardine.\nModel Categories. Good references for model category theory include:\n\nSections 2.1\u20132.3 of Higher Categories and Homotopical Algebra (Cisinski);\nIntroduction to Homotopy Theory (nLab);\nCategorical Homotopy Theory (Riehl);\nModel Categories (Hovey);\nPart 4 of More Concise Algebraic Topology (May);\n\n4. ($\\infty$-Categories, finally)\nIt is hard to capture in a precise way the idea of an $\\infty$-category as a set of objects, together with a set of morphisms, a set of $2$-morphisms, and so on. There are two ways of approaching this difficulty, one traditional, the other very recent.\nVia Quasicategories. The traditional one is to use models for $\\infty$-categories. One such model is given by a special kind of simplicial set called a quasicategory. This is the approach developed by Joyal and Lurie. For learning the theory of quasicategories, there are:\n\nA Short Course on $\\infty$-Categories (Groth);\nKerodon (Lurie; a (WIP) textbook);\nHigher Categories and Homotopical Algebra (Cisinski);\nHigher Topos Theory (Lurie; this is best used as a reference, not as a textbook).\n\nVia $\\infty$-Cosmoi. The second one is the model-independent approach of Riehl and Verity (which is currently being developed). Instead of axiomatizing what $\\infty$-categories are via models, Riehl\u2013Verity axiomatize the mathematical object in which $\\infty$-categories live, and call it an $\\infty$-cosmos.\nWhen working with an specific model for $\\infty$-categories, one is often lead to complicated arguments involving its combinatorics. On the other hand, in Riehl\u2013Verity's framework, it is possible to prove statements about $\\infty$-categories in a much simpler, model-independent, way\nRiehl and Verity are currently compiling their work in a textbook, called Elements of $\\infty$-Category Theory.\n\nExtra References\n$\\infty$-Categories\n\nNotes on $\\infty$-categories (Hinich);\nNotes on $\\infty$-categories (Morel, in French);\nHomotopy Theory of Higher Categories: From Segal Categories to $n$-Categories and Beyond (Simpson);\nThe Homotopy Theory of $(\\infty,1)$-Categories (Bergner).\n\nBackground on model categories and simplicial sets\n\nNotes on Homotopical Algebra (Low);\nHomotopy Limit Functors on Model Categories and Homotopical Categories (Dwyer et al.);\nModel Categories and Their Localizations (Hirschhorn);\n\n\nEdit: Here are some extra (extra) references, in the (way far from optimal) format of a directory tree.\n(P.S. Please take the above attempt at a guide with a big grain of salt; there's a lot missing from it!)\n\u251c\u2500\u2500 Complete Segal Spaces\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [BC, 10 Pages] Equivariant Complete Segal Spaces.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Brito, 26 Pages] Segal Objects and the Grothendieck Construction.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rasekh, 17 Pages] A Model for the Higher Category of Higher Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rasekh, 44 Pages] Complete Segal Objects.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rasekh, 478 Pages] A Theory of Elementary Higher Toposes.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rasekh, 61 Pages] Introduction to Complete Segal Spaces.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rasekh, 75 Pages] Yoneda Lemma for Simplicial Spaces.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rasekh, 81 Pages] Cartesian Fibrations and Representability.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Stenzel, 21 Pages] Univalence and Completeness of Segal Objects.pdf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [Stenzel, 26 Pages] Bousfield-Segal Spaces.pdf\n\u251c\u2500\u2500 Elementary \u221e-Topoi\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rasekh, 10 Pages] Yoneda Lemma for Elementary Higher Toposes.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rasekh, 30 Pages] Filter Quotients and Non-Presentable (\u221e,1)-Toposes.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rasekh, 40 Pages] A Theory of Elementary Higher Toposes.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rasekh, 51 Pages] Every Elementary Higher Topos Has a Natural Number Object.pdf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [Rasekh, 84 Pages] An Elementary Approach to Truncations.pdf\n\u251c\u2500\u2500 Enriched (\u221e,1)-Categories\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [AMR, 68 Pages] Factorization Homology of Enriched \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [GH, 100 Pages] Enriched \u221e-Categories via Non-Symmetric \u221e-Operads.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Haugseng, 29 Pages] Bimodules and Natural Transformations for Enriched \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [Haugseng, 52 Pages] Rectification of Enriched Infinity-Categories.pdf\n\u251c\u2500\u2500 General References\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [AL, 26 Pages] Exponentiable Higher Toposes.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Arctaedius, 38 Pages] Grothendieck's Homotopy Hypothesis and the Homotopy Theory of Homotopy Theories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Bergner, 13 Pages] A Survey of (\u221e, 1)-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Bergner, 287 Pages] The Homotopy Theory of (\u221e,1)-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Bergner, 29 Pages] A Survey of Models for (\u221e,n)-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Bergner, 39 Pages] Workshop on the Homotopy Theory of Homotopy Theories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Camarena, 45 Pages] A Whirlwind Tour of the World of (\u221e,1)-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Clough, 35 Pages] An Outline of the Theory of (\u221e,1)-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Dorn, 99 Pages] Basic concepts in homotopy theory.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [HF, 148 Pages] The Homotopy Theory of (\u221e,1)-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Mazel-Gee, 26 Pages] The Zen of \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Porter, 37 Pages] -categories, -groupoids, Segal categories and quasicategories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Porter, 54 Pages] Spaces as \u221e-groupoids.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Porter, 759 Pages] The Crossed Menagerie.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Schommer-Pries, 65 Pages] Dualizability in Low-Dimensional Higher Category Theory.pdf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [Simpson, 653 Pages] Homotopy Theory of Higher Categories.pdf\n\u251c\u2500\u2500 Model (\u221e,1)-Categories\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [LM, 21 Pages] From Fractions to Complete Segal Spaces.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Mazel-Gee, 16 Pages] A User's Guide to Co\u29f8Cartesian Fibrations.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Mazel-Gee, 20 Pages] Quillen Adjunctions Induce Adjunctions of Quasicategories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Mazel-Gee, 26 Pages] The Universality of the Rezk Nerve.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Mazel-Gee, 29 Pages] Model \u221e-Categories II: Quillen Adjunctions.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Mazel-Gee, 34 Pages] Model \u221e-Categories III: The Fundamental Theorem.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Mazel-Gee, 41 Pages] All About the Grothendieck Construction.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Mazel-Gee, 43 Pages] Hammocks and Fractions in Relative \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Mazel-Gee, 528 Slides] Goerss\u2013Hopkins obstruction theory for \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Mazel-Gee, 545 Pages] Goerss\u2013Hopkins obstruction theory via model \u221e-categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Mazel-Gee, 54 Pages] Goerss\u2013Hopkins Obstruction Theory for \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Mazel-Gee, 66 Pages] Model \u221e-Categories I: Some Pleasant Properties of the \u221e-Category of Simplicial Spaces.pdf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [Mazel-Gee, 6 Pages] _\u221e Automorphisms of Motivic Morava E-Theories.pdf\n\u251c\u2500\u2500 Other Models\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Batanin \u221e-Categories\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 [Ara, 168 Pages] Sur les \u221e-groupo\u00efdes de Grothendieck et une variante \u221e-cat\u00e9gorique.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Grothendieck\u2013Maltsiniotis \u221e-Categories\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [AL, 65 Pages] The Folk Model Category Structure on Strict \u03c9-Categories Is Monoidal.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [AM, 123 Pages] A Quillen's Theorem A for Strict \u221e-Categories II: The \u221e-Categorical Proof.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [AM, 227 Pages] Join and Slices for Strict \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [AM, 25 Pages] The Brown\u2013Golasinski Model Structure on Strict \u221e-Groupoids Revisited.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [AM, 42 Pages] Comparison of the n-Categorical Nerves.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [AM, 51 Pages] A Quillen's Theorem A for Strict \u221e-Categories I: The Simplicial Proof.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [AM, 68 Pages] The Homotopy Type of the \u221e-Category Associated to a Simplicial Complex.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [AM, 92 Pages] Towards a Thomason Model Structure on the Category of Strict n-Categories.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [Ara, 22 Pages] On Homotopy Types Modelized by Strict \u221e-Groupoids.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [Ara, 22 Pages] Strict \u221e-Groupoids Are Grothendieck \u221e-Groupoids.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [Ara, 33 Pages] A Quillen Theorem B for Strict \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [Ara, 41 Pages] The Groupoidal Analogue \u0398 to Joyal's Category \u0398 Is a Test Category.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 [Ara, 58 Pages] On the Homotopy Theory of Grothendieck \u221e-Groupoids.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Miscellany\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [CL, 61 Pages] Weak \u221e-Categories via Terminal Coalgebras.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [Harpaz, 69 Pages] Quasi-Unital \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [LM, 68 Pages] Linear Quasi-Categories as Templicial Modules.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 [Nikolaus, 26 Pages] Algebraic Models for Higher Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Relative Categories\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [BK, 19 Pages] n-Relative Categories.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [BK, 27 Pages] Relative Categories: Another Model for the Homotopy Theory of Homotopy Theories.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [BK, 3 Pages] A Thomason-Like Quillen Equivalence Between Quasi-Categories and Relative Categories.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 [BK, 5 Pages] In the Category of Relative Categories the Rezk Equivalences Are Exactly the DK-equivalences.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 [Meier, 21 Pages] Fibration Categories Are Fibrant Relative Categories.pdf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Topological Categories\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 [Amrani, 22 Pages] A Model Structure on the Category of Topological Categories.pdf\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 [Lindberg, 70 Pages] Equivariant Sheaves on Topological Categories.pdf\n\u251c\u2500\u2500 Parametrised (\u221e,1)-Categories\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [BDGNS, 11 Pages] Parametrized Higher Category Theory and Higher Algebra: A General Introduction.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [BDGNS, 23 Pages] Parametrized Higher Category Theory and Higher Algebra: Expos\u00e9 I -- Elements of Parametrized Higher Category Theory.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Nardin, 21 Pages] Parametrized Higher Category Theory and Higher Algebra: Expos\u00e9 IV -- Stability With Respect to an Orbital \u221e-Category.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Shah, 81 Pages] Parametrized Higher Category Theory and Higher Algebra: Expos\u00e9 II - Indexed Homotopy Limits and Colimits.pdf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [Shah, 86 Slides] Parametrized Higher Category Theory.pdf\n\u251c\u2500\u2500 Quasicategories\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [AL, 26 Pages] Exponentiable Higher Toposes.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Beardsley, 78 Pages] Coalgebraic Structure and Intermediate Hopf\u2013Galois Extensions of Thom Spectra in Quasicategories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Berman, 13 Pages] On Lax Limits in \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [BG, 11 Pages] On the Fibrewise Effective Burnside \u221e-Category.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [BM, 30 Pages] Spectral Sequences in (\u221e,1)-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [BS, 18 Pages] Fibrations in \u221e-Category Theory.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [BV, 267 Pages] Homotopy Invariant Algebraic Structures On Topological Spaces.djvu\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Campbell, 3 Pages] A Counterexample in Quasi-Category Theory.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Cisinski, 204 Pages] Alg\u00e8bre Homotopique et Cat\u00e9gories Sup\u00e9rieures.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Cisinski, 446 Pages] Higher Categories and Homotopical Algebra.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Cisisnki, 57 Pages] Cat\u00e9gories Sup\u00e9rieures et Th\u00e9orie des Topos.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [DS, 46 Pages] Mapping Spaces in Quasi-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Fiore, 24 Pages] Quasicategorical Adjunctions.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [GR, 11 Pages] Simplified HTT 4.3.2.15.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [GR, 85 Pages] Some Higher Algebra.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Gregoric, 21 Pages] Gregoric Blitzkrieg.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Groth, 77 Pages] A Short Course on \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Harpaz, 116 Pages] Little Cube Algebras and Factorisation Homology.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Harpaz, 7 Pages] Limits, colimits and adjunctions in \u221e-categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Haugseng, 12 Pages] On (Co)ends in \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Haugseng, 28 Pages] A Fibrational Mate Correspondence for \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Haugseng, 87 Pages] Introduction to \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [HH, 684 Pages] Higher Categories I & II\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 0    [Hebestreit, 13 Pages] A Fairytale.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 10  [Hebestreit, 11 Pages] Fat and Thin Slices.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 11  [Hebestreit, 43 Pages] Cartesian Fibrations.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 12  [Hebestreit, 121 Pages] Straightening and Unstraightening.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 12' [HH, 53 Pages] Straightening and Unstraightening (Heuts's Notes).pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 13  [Hebestreit, 14 Pages] Homotopy Colimits.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 14  [Hebestreit, 10 Pages] Simplicial Model Categories.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 15  [Hebestreit, 35 Pages] Yoneda's Lemma, Adjunctions and (Co)Limits.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 1    [Hebestreit, 37 Pages] Categories.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 2    [Hebestreit, 25 Pages] Simplicial Sets.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 3    [Hebestreit, 10 Pages] Quasicategories.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 4    [Hebestreit, 53 Pages] Simplicial Categories.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 5    [Hebestreit, 38 Pages] Simplicial Homotopy Theory.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 6    [Hebestreit, 57 Pages] Quasicategories and Anima.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 7    [Hebestreit, 67 Pages] Equivalences, Equivalences, Equivalences.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 8    [Hebestreit, 9 Pages] A Fairytale.pdf\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 9    [Hebestreit, 88 Pages] Localisations and Model Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Hinich, 111 Pages] Lectures on \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Joyal, 244 Pages] Notes on Quasi-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Joyal, 350 Pages] The Theory of Quasi-Categories and its Applications II.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Joyal, 479 Pages] The Theory of Quasi-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Lejay, 139 Pages] Alg\u00e8bres \u00e0 Factorisation et Topos Sup\u00e9rieurs Exponentiables.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Lurie, 60 Pages] On \u221e-Topoi.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Lurie, 841 Pages] Kerodon.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Lurie, 949 Pages] Higher Topos Theory.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Lysenko, 208 Pages] Lysenko's comments to Gaitsgory\u2013Rozenblyum.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Morel, 118 Pages] \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Nguyen, 94 Pages] Theorems in Higher Category Theory and Applications.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [NRS, 21 Pages] Adjoint Functor Theorems for \u221e-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Porta, 100 Pages] Derived formal moduli problems.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rezk, 11 Pages] Degenerate Edges of Cartesian Fibrations are Cartesian Edges.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rezk, 175 Pages] Stuff About Quasicategories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rezk, 50 Pages] Toposes and homotopy toposes.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Riehl, 20 Pages] Quasi-Categories as (\u221e,1)-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Riehl, 292 Pages] Categorical homotopy theory.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Riehl, 9 Pages] Associativity Data in an (\u221e,1)-Category.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Rovelli, 38 Pages] Weighted Limits in an (\u221e,1)-Category.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [RS, 151 Pages] Notes on Higher Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [RV, 33 Pages] Completeness Results for Quasi-Categories of Algebras, Homotopy Limits, and Related General Constructions.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Stevenson, 12 Pages] Stability for Inner Fibrations Revisited.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Tanaka, 14 Pages] Functors (Between \u221e-Categories) That Aren't Strictly Unital.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Thanh, 46 Pages] Quasicategories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Wong, 83 Pages] The Grothendieck construction in enriched, internal and \u221e-Category Theory.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Zs\u00e1mboki, 31 Pages] A summary of higher topos theory.pdf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [\u7121, 5 Pages] HTT ToC.pdf\n\u251c\u2500\u2500 Simplicial Categories\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Bergner, 16 Pages] A model category structure on the category of simplicial categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Bergner, 22 Pages] Complete Segal Spaces Arising From Simplicial Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Bergner, 40 Pages] Three models for the homotopy theory of homotopy theories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Cordier, 21 Pages] Sur la notion de diagramme homotopiquement coh\u00e9rent.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [CP, 26 Pages] Vogt's Theorem on Categories of Homotopy Coherent Diagrams.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [CP, 54 Pages] Homotopy Coherent Category Theory.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [DS, 29 Pages] Rigidification of Quasi-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [DS, 64 Pages] Mapping Spaces in Quasi-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Hinich, 23 Pages] Homotopy Coherent Nerve in Deformation Theory.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [HK, 14 Pages] Mapping Spaces in Homotopy Coherent Nerves.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Joyal, 66 Pages] Quasi-Categories vs Simplicial Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Riehl, 16 Pages] On the Structure of Simplicial Categories Associated to Quasi-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Riehl, 26 Pages] Homotopy Coherent Structures.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Riehl, 292 Pages] Categorical Homotopy Theory.pdf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [Riehl, 7 Pages] Understanding the Homotopy Coherent Nerve.pdf\n\u251c\u2500\u2500 Unicity\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Bergner, 16 Pages] Equivalence of Models for Equivariant (\u221e,1)-Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [BS, 47 Pages] On the Unicity of the Homotopy Theory of Higher Categories.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [JT, 49 Pages] Quasi-Categories vs Segal Spaces.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 [Riehl, 58 Pages] Seminar Notes on the Barwick\u2013Schommer\u2013Pries Unicity Theorem.pdf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 [To\u00ebn, 32 Pages] Vers une axiomatisation de la th\u00e9orie des cat\u00e9gories sup\u00e9rieures.pdf\n\u2514\u2500\u2500 Un\u29f8Straightening\n    \u251c\u2500\u2500 [AF, 89 Pages] Fibrations of \u221e-Categories.pdf\n    \u251c\u2500\u2500 [BGN, 19 Pages] Dualizing Cartesian and Cocartesian Fibrations.pdf\n    \u251c\u2500\u2500 [BS, 18 Pages] Fibrations in \u221e-Category Theory.pdf\n    \u251c\u2500\u2500 [Campbell, 31 Slides] A modular proof of the straightening theorem.pdf\n    \u251c\u2500\u2500 [GHN, 42 Pages] Lax Colimits and Free Fibrations in \u221e-Categories.pdf\n    \u251c\u2500\u2500 [GR, 85 Pages] Some Higher Algebra.pdf\n    \u251c\u2500\u2500 [Hebestreit, 121 Pages] Straightening and Unstraightening.pdf\n    \u251c\u2500\u2500 [HH, 53 Pages] Straightening and Unstraightening (Heuts's Notes).pdf\n    \u251c\u2500\u2500 [HM, 17 Pages] Left Fibrations and Homotopy Colimits II.pdf\n    \u251c\u2500\u2500 [HM, 27 Pages] Left Fibrations and Homotopy Colimits.pdf\n    \u251c\u2500\u2500 [Mazel-Gee, 16 Pages] A User's Guide to (Co)Cartesian Fibrations.pdf\n    \u251c\u2500\u2500 [Mazel-Gee, 41 Pages] All About the Grothendieck Construction.pdf\n    \u251c\u2500\u2500 [Noel, 1 Page] Cartesian Model Structure.pdf\n    \u251c\u2500\u2500 [PK, 43 Pages] Straightening and Unstraightening.pdf\n    \u251c\u2500\u2500 [Richardson, 14 Pages] Mapping Spaces and Straightening-Unstraightening.pdf\n    \u251c\u2500\u2500 [Stevenson, 41 Pages] Model Structures for Correspondences and Bifibrations.pdf\n    \u251c\u2500\u2500 [Stevenson, 49 Pages] Covariant Model Structures and Simplicial Localization.pdf\n    \u2514\u2500\u2500 [Wong, 83 Pages] The Grothendieck Constructionin Enriched, Internal and \u221e-Category Theory.pdf", "meta": {"post_id": 1275964, "input_score": 54, "output_score": 42, "post_title": "Textbooks on higher category theory"}}
{"input": "Let be $1\\leq p\\in\\mathbb{R}$, denote:\n  $$\\ell^p(\\mathbb {R})=\\left\\{(x_n)\\subset \\mathbb{R}: (x_n) \\mbox{ is a sequence with } \\displaystyle\\sum_{n=1}^{\\infty}|x_n|^p<\\infty \\right\\}$$ \n  Prove that: \n\n\nThe function: $d_p:\\ell^p(\\mathbb{R})\\times\\ell^p(\\mathbb{R})\\to \\mathbb{R}$ is a metric for $\\ell^p(\\mathbb{R})$  where $d_p(x_n,y_n)= \\left| \\displaystyle\\sum_{n=1}^\\infty |x_n-y_n|^p \\right|^\\frac{1}{p}$ (Only triangular inequality, I work in $\\mathbb{R}$, should I assume Minkowski inequality and its done?)\n$\\ell^p(\\mathbb{R})$ is a complete metric space.\n\nit is right? I mean $p\\in\\mathbb{R}$? I've never work with $\\ell^p$ spaces, this is a question from introduction to topology.", "output": "Let $\\left( x^{(n)}\\right)_{n=1}^{\\infty} \\subset \\ell^p$ be a Cauchy sequence. Since I see you have troubles with your notations of sequence of sequences, this is the notation that I will use for each element $x^{(n)}$ in the sequence:\n$$\nx^{(n)} = \\left( x_j^{(n)}\\right)_{j=1}^{\\infty} = \\left( x_1^{(n)},x_2^{(n)}, \\cdots \\right)\\in \\ell^p\n$$ \nFor $x= \\left( x_j\\right)_{j=1}^{\\infty} , y= \\left( y_j\\right)_{j=1}^{\\infty} \\in \\ell^p$, lets define the $p$-norm $\\|\u00a0\\cdot \\|_p$ as the one who induces $d_p$, that is $\\|x-y\\|_p=d_p(x,y)$. Precisely \n$$\n\\|x-y\\|_p= \\left(\\sum_{j=1}^{\\infty} \\left|x_j-y_j\\right|^p\\right)^{1/p}\n$$\nNow lets continue, take $\\varepsilon>0$, then there exist a $N=N(\\varepsilon) \\in \\mathbb{N}$, such that if $m,n >N$ then\n$$\n\\|x^{(m)}-x^{(n)}\\|_p<\\varepsilon.\n$$\nThus, for any $j \\in \\mathbb{N}$, it follows that\n$$\n\\left|x^{(m)}_j-x^{(n)}_j\\right|^p \\leq \\sum_{j=1}^{\\infty} \\left|x^{(m)}_j-x^{(n)}_j\\right|^p = \\|x^{(m)}-x^{(n)}\\|^p_p<\\varepsilon^p\n$$\nthat is, for any $j \\in \\mathbb{N}$ the sequence $\\left( x^{(n)}_j\\right)_{n=1}^{\\infty} \\subset \\mathbb{R}$ is a Cauchy one. Since $\\mathbb{R}$ is complete, for each $j$ there exist a $x_j \\in \\mathbb{R}$ such that\n $$\n \\lim_{n \\to \\infty} x^{(n)}_j = x_j\n$$\nLets fix $k \\in \\mathbb{N}$, then in a similar way for $m,n >N$\n \\begin{equation}\n \\sum_{j=1}^{k} \\left|x^{(m)}_j-x^{(n)}_j\\right|^p \\leq \\sum_{j=1}^{\\infty} \\left|x^{(m)}_j-x^{(n)}_j\\right|^p = \\|x^{(m)}-x^{(n)}\\|^p_p<\\varepsilon^p \\tag{1}\n\\end{equation}\nLetting $n \\to \\infty$ in (1), we get that for $m>N$ \n\\begin{equation}\n \\sum_{j=1}^{k}\\left|x^{(m)}_j-x_j\\right|^p < \\varepsilon^p \\tag{2}\n\\end{equation}\nThen by the usual triangle inecuality ( Minkowski's inequality for $\\|\\cdot\\|_p$ in $\\mathbb{R}^k$) we get that if $m>N$\n$$\n\\left( \\sum_{j=1}^{k}|x_j|^p \\right)^{1/p} \\leq \\left(  \\sum_{j=1}^{k}\\left|x^{(m)}_j-x_j\\right|^p \\right)^{1/p} + \\left( \\sum_{j=1}^{k} \\left|x^{(m)}_j \\right| \\right)^{1/p} < \\varepsilon + \\left( \\sum_{j=1}^{k} \\left|x^{(m)}_j \\right| \\right)^{1/p}\n$$\nby letting $k \\to \\infty$, we get $\\|x\\|_p\\leq \\varepsilon + \\|x^{(m)}\\|_p$, which is the same as getting that $x=\\left( x_j\\right)_{j=1}^{\\infty} \\in \\ell^p$. Again, letting $k \\to \\infty$ in (2), we obtain that if $m>N$\n$$\n\\|x^{(m)}-x\\|_p^p= \\sum_{j=1}^{\\infty}\\left|x^{(m)}_j-x_j\\right|^p < \\varepsilon^p\n$$\nthus \n$$\n\\lim_{m \\to \\infty} \\|x^{(m)}-x\\|_p= 0 \n$$\nso indeed, $\\left( x^{(m)}\\right)_{m=1}^{\\infty} \\subset \\ell^p$, is a convergent sequence who converges to $x \\in \\ell^p$. We conclude then that $\\ell^p$ is a complete metric space for $1\\leq p < \\infty$.", "meta": {"post_id": 1276470, "input_score": 15, "output_score": 34, "post_title": "Proving $\\ell^p$ is complete"}}
{"input": "I'm interested in integrals of the form\n$$I(a,b)=\\int_0^\\infty\\operatorname{arccot}(x)\\cdot\\operatorname{arccot}(a\\,x)\\cdot\\operatorname{arccot}(b\\,x)\\ dx,\\color{#808080}{\\text{ for }a>0,\\,b>0}\\tag1$$\nIt's known$\\require{action}\\require{enclose}\\texttip{{}^\\dagger}{Gradshteyn & Ryzhik, Table of Integrals, Series, and Products, 7th edition, page 599, (4.511)}$ that\n$$I(a,0)=\\frac{\\pi^2}4\\left[\\ln\\left(1+\\frac1a\\right)+\\frac{\\ln(1+a)}a\\right].\\tag2$$\nMaple and Mathematica are also able to evaluate\n$$I(1,1)=\\frac{3\\pi^2}4\\ln2-\\frac{21}8\\zeta(3).\\tag3$$\n\nIs it possible to find a general closed form for $I(a,1)$? Or, at least, for $I(2,1)$ or $I(3,1)$?", "output": "I finally managed to find a general solution to this problem. Although I put a lot of effort into simplification of the result, it is still not as pretty and symmetric as I would like it to be. I hope to improve it later. Sorry for poor typesetting.\nAssuming $0<a<b<c,$\n$$\\int_0^\\infty\\operatorname{arccot}(ax)\\,\\operatorname{arccot}(bx)\\,\\operatorname{arccot}(cx)\\,dx=\\\\\n\\frac1{24 a b c}\\left(2 a (b+2 c) \\ln ^3(a)-3 \\left(2 b (a+c) \\ln (b)+(4 a c-2 b c) \\ln (c)+\\\\\nb \\left(-2 c \\operatorname{artanh}\\left(\\frac{b}{c}\\right)+(c-a) \\ln\n   (c-a)-(a+c) \\ln (a+c)+a \\ln \\left(c^2-b^2\\right)\\right)\\right) \\ln ^2(a)\\\\\n+3 \\left(b c \\left(\\ln ^2(b)+2 \\left(2 \\ln (a+c)+\\ln\n   \\left(-\\frac{c (b+c)}{(a-b) (a-c) (b-c)}\\right)\\right) \\ln (b)+\\ln ^2(b-a)\\\\\n-3 \\ln ^2(c)+\\ln ^2(c-a)-\\ln ^2(c-b)+\\ln ^2(b+c)-4 \\operatorname{artanh}\\left(\\frac{a}{c}\\right) \\ln (a+b)+\\\\\n2 \\ln (b-a) \\ln \\left(\\frac{c}{c-a}\\right)-6 \\ln (c) \\ln (a+c)+2 \\ln (a+c) \\ln (c-b)\\\\\n+4 \\ln (c)\n   (\\ln (c-a)+\\ln (c-b))-4 \\ln (c) \\ln (b+c)-2 \\ln (c-a) \\ln (b+c)\\right)\\\\\n+a \\left(b \\left(2 \\ln ^2(b)-2 \\ln\n   \\left(\\frac{a^2-c^2}{b^2-c^2}\\right) \\ln (b)+\\ln ^2(a+b)-\\ln ^2(c-a)-\\ln ^2(a+c)\\\\\n-3 \\ln ^2(b+c)-2 \\ln (b-a) \\ln (c-b)+2 \\ln (c-a) (\\ln\n   (b-a)+\\ln (b+c))\\\\\n+2 \\ln (a+b) \\ln \\left(\\frac{b+c}{c^2-a^2}\\right)+2 \\ln (a+c) \\ln \\left(c^2-b^2\\right)\\right)-c \\left(\\ln ^2(b)-2 (\\ln\n   ((b-a) c)-\\ln (a+c)) \\ln (b)+\\ln ^2(b-a)+\\ln ^2(a+b)\\\\\n-3 \\ln ^2(c)+\\ln ^2(a+c)+\\ln ^2(c-b)+2 \\ln ^2(b+c)+2 \\ln (b-a) \\ln\n   \\left(\\frac{c}{c-b}\\right)\\\\\n-2 \\ln (a+b) \\ln (b+c)-2 \\ln (a+c) \\ln (c (b+c))\\right)\\right)\\right) \\ln (a)\\\\\n-2 (a (b-c)+b c) \\ln ^3(b)+a c \\ln\n   ^3(b-a)-b c \\ln ^3(b-a)-3 a c \\ln ^3(c)+5 b c \\ln ^3(c)\\\\\n+2 a b \\ln ^3(c-a)-2 b c \\ln ^3(c-a)+2 a c \\ln ^3(a+c)-2 b c \\ln ^3(a+c)-a b \\ln\n   ^3(c-b)\\\\\n-b c \\ln ^3(c-b)-a b \\ln ^3(b+c)-3 b c \\ln ^3(b+c)-3 b c \\ln (b-a) \\ln ^2(c)\\\\\n-3 a b \\ln ((a-b) (b-c)) \\ln ^2(c-a)+3 b c \\ln ((a-b)\n   (b-c)) \\ln ^2(c-a)\\\\\n+3 b c \\ln \\left(\\left(b^2-a^2\\right) c\\right) \\ln ^2(a+c)-3 a c \\ln (a+b) \\ln ^2(c-b)+3 a c \\ln ((b-a) c) \\ln\n   ^2(c-b)\\\\\n-3 a b (\\ln (b-a)-\\ln ((a+b) (a+c))) \\ln ^2(c-b)+3 b c \\ln (c (a+c)) \\ln ^2(c-b)\\\\\n-3 a c \\ln \\left(\\frac{b-a}{a+b}\\right) \\ln\n   ^2(b+c)+6 a c \\ln (c) \\ln ^2(b+c)-12 b c \\ln (c) \\ln ^2(b+c)\\\\\n+3 a b \\ln (a+c) \\ln ^2(b+c)+3 b c \\left(\\ln \\left(b^2-a^2\\right)+\\ln\n   (a+c)\\right) \\ln ^2(b+c)\\\\\n-3 a c \\ln ^2(b-a) \\ln \\left(1-\\frac{b}{c}\\right)-b c \\pi ^2 \\left(\\ln (b-a)+\\ln\n   \\left(\\frac{b-c}{a-c}\\right)\\right)-5 a c \\pi ^2 \\ln (c)-5 b c \\pi ^2 \\ln (c)\\\\\n-3 b c \\ln ^2(b-a) \\ln \\left(-\\frac{c}{b-c}\\right)-12 b c \\ln\n   ^2(c) \\ln \\left(\\frac{c-a}{a+c}\\right)-12 b c \\ln (a+b) \\ln (c) \\ln \\left(\\frac{c-a}{a+c}\\right)\\\\\n+\\pi ^2 a c (\\ln (b-a)-\\ln (a+c))+3 a b\n   \\pi ^2 \\ln (a+c)+4 b c \\pi ^2 \\ln (a+c)-6 b c \\ln ^2(c) \\ln (c-b)\\\\\n+6 a b \\ln (b-a) \\ln (c-a) \\ln (c-b)-6 b c \\ln (b-a) \\ln (c-a) \\ln\n   (c-b)\\\\\n-6 a b \\ln (a+b) \\ln (a+c) \\ln (c-b)-6 b c \\ln (a+b) \\ln (a+c) \\ln (c-b)\\\\\n-12 b c \\ln (c) \\ln (a+c) \\ln (c-b)-a b \\pi ^2 (\\ln\n   (c-a)+\\ln (c-b))\\\\\n+3 a c \\ln ^2(a+b) \\ln \\left(\\frac{c (c-b)}{b+c}\\right)+3 a c \\ln ^2(c) (\\ln (b-a)-\\ln (b+c))+9 b c \\ln ^2(c) \\ln\n   (b+c)\\\\\n-6 a c \\ln (a+b) \\ln (c) \\ln (b+c)+6 b c \\ln (a+b) \\ln (c) \\ln (b+c)+12 b c \\ln (c) \\ln (c-a) \\ln (b+c)\\\\\n+5 \\pi ^2 a b \\ln (b+c)+6\n   a c \\pi ^2 \\ln (b+c)+3 b c \\pi ^2 \\ln (b+c)-3 a b \\ln ^2(a+c) \\ln \\left(\\frac{b+c}{a+b}\\right)\\\\\n+6 a c \\ln (b-a) \\ln (a+c) \\ln (c (b+c))-6\n   b c \\ln (b-a) \\ln (a+c) \\ln (c (b+c))\\\\\n-3 a c \\ln ^2(a+c) (\\ln (b-a)+\\ln (c (b+c)))-3 b c \\ln ^2(a+b) \\ln \\left(-\\frac{c\n   (b+c)}{b-c}\\right)\\\\\n-3 \\ln ^2(b) \\left(a \\left(c (\\ln (b-a)+\\ln (c)-2 \\ln (a+c)+\\ln (b+c))\\\\\n+b \\ln\n   \\left(\\frac{b^2-c^2}{a^2-c^2}\\right)\\right)-b c \\ln \\left(\\frac{(a-b) c (c-b)}{a^2-c^2}\\right)\\right)-6 a c \\ln (b-a) \\ln (c) \\ln\n   \\left(c^2-b^2\\right)\\\\\n+6 b c \\ln (b-a) \\ln (c) \\ln \\left(c^2-b^2\\right)\\\\\n-\\ln (b) \\left(b c \\left(3 \\left(-\\ln ^2(a+b)+2 (\\ln ((a+c)\n   (b+c))-\\ln (c-a)) \\ln (a+b)\\\\\n+2 \\ln ^2(c)+\\ln ^2(c-a)+\\ln ^2(a+c)-3 \\ln ^2(b+c)-2 \\ln (a+c) \\ln (c-b)\\\\\n+2 \\ln (c-a) \\ln (b+c)+2 \\ln (c)\n   (\\ln ((a+c) (b+c))-2 \\ln (c-a))\\\\\n-2 \\ln (b-a) \\ln \\left(c^2-a^2\\right)+2 \\ln (b-a) \\ln \\left(c^2-b^2\\right)\\right)+\\pi ^2\\right)\\\\\n+a \\left(c\n   \\left(6 (\\ln (b-a)+\\ln (c)-\\ln (a+c)) \\ln \\left(\\frac{a+c}{b+c}\\right)+\\pi ^2\\right)\\\\\n+3 b \\left(\\ln ^2(a+b)+2 \\ln\n   \\left(\\frac{b+c}{c^2-a^2}\\right) \\ln (a+b)-\\ln ^2(c-a)-\\ln ^2(a+c)-3 \\ln ^2(b+c)\\\\\n+2 \\ln (b-a) \\ln \\left(\\frac{a-c}{b-c}\\right)+2 \\ln (c-a)\n   \\ln (b+c)+2 \\ln (a+c) \\ln \\left(c^2-b^2\\right)+2 \\pi ^2\\right)\\right)\\right)\\\\\n+6 \\left(2 \\left(a \\ln \\left(\\frac{a}{c}\\right)+b \\ln\n   \\left(\\frac{c}{b}\\right)\\right) \\operatorname{Li}_2\\left(\\frac{a}{c}\\right) c+2 \\left(a \\ln \\left(\\frac{a}{c}\\right)+b \\ln\n   \\left(\\frac{c}{b}\\right)\\right) \\operatorname{Li}_2\\left(-\\frac{c}{a}\\right) c-2 b \\ln \\left(\\frac{b}{c}\\right) \\operatorname{Li}_2\\left(\\frac{b (a+c)}{a\n   (b-c)}\\right) c+2 b \\ln \\left(\\frac{b}{c}\\right) \\operatorname{Li}_2\\left(\\frac{b (a-c)}{a (b+c)}\\right) c-2 a \\operatorname{Li}_3\\left(\\frac{a}{c}\\right) c+2\n   b \\operatorname{Li}_3\\left(-\\frac{b}{c}\\right) c-2 b \\operatorname{Li}_3\\left(\\frac{b}{c}\\right) c+2 a \\operatorname{Li}_3\\left(-\\frac{c}{a}\\right) c-(a-b)\n   \\operatorname{Li}_3\\left(\\frac{(b-a) c}{b (a+c)}\\right) c-(a-b) \\operatorname{Li}_3\\left(\\frac{a (c-b)}{(a-b) c}\\right) c+(a+b) \\operatorname{Li}_3\\left(\\frac{a\n   (c-b)}{(a+b) c}\\right) c-(a-b) \\operatorname{Li}_3\\left(\\frac{b-a}{b+c}\\right) c+(a+b) \\operatorname{Li}_3\\left(\\frac{a+b}{b+c}\\right) c-(a+b)\n   \\operatorname{Li}_3\\left(\\frac{a (b+c)}{(a+b) c}\\right) c+(a-3 b) \\zeta (3) c+2 a b \\ln \\left(\\frac{a}{b}\\right) \\operatorname{Li}_2\\left(\\frac{a}{b}\\right)+2\n   a b \\ln \\left(\\frac{a}{b}\\right) \\operatorname{Li}_2\\left(-\\frac{b}{a}\\right)+\\left(a b \\ln \\left(\\frac{a}{b}\\right)+b c \\ln\n   \\left(\\frac{b}{c}\\right)+a c \\ln \\left(\\frac{c}{a}\\right)\\right) \\operatorname{Li}_2\\left(\\frac{b-a}{b-c}\\right)+\\left(a b \\ln\n   \\left(\\frac{a}{b}\\right)+b c \\ln \\left(\\frac{b}{c}\\right)+a c \\ln \\left(\\frac{c}{a}\\right)\\right)\n   \\operatorname{Li}_2\\left(\\frac{b-c}{a+b}\\right)-\\left(a b \\ln \\left(\\frac{a}{b}\\right)+a c \\ln \\left(\\frac{a}{c}\\right)-b c \\ln\n   \\left(\\frac{b}{c}\\right)\\right) \\operatorname{Li}_2\\left(\\frac{a+b}{b+c}\\right)-\\left(a b \\ln \\left(\\frac{a}{b}\\right)+a c \\ln\n   \\left(\\frac{a}{c}\\right)-b c \\ln \\left(\\frac{b}{c}\\right)\\right) \\operatorname{Li}_2\\left(\\frac{a+c}{b+c}\\right)-2 a b\n   \\operatorname{Li}_3\\left(\\frac{a}{b}\\right)+2 a b \\operatorname{Li}_3\\left(-\\frac{b}{a}\\right)-b (a-c) \\operatorname{Li}_3\\left(\\frac{a-b}{a-c}\\right)-a (b-c)\n   \\operatorname{Li}_3\\left(\\frac{b-a}{b-c}\\right)+a (b-c) \\operatorname{Li}_3\\left(\\frac{a+b}{b-c}\\right)-b (a-c) \\operatorname{Li}_3\\left(\\frac{a (b-c)}{b\n   (a-c)}\\right)+b (a+c) \\operatorname{Li}_3\\left(\\frac{a+b}{a+c}\\right)+b (a+c) \\operatorname{Li}_3\\left(\\frac{b (a+c)}{a (b-c)}\\right)+b (a-c)\n   \\operatorname{Li}_3\\left(\\frac{b (a-c)}{a (b+c)}\\right)-b (a-c) \\operatorname{Li}_3\\left(\\frac{c-a}{b+c}\\right)+b (a+c)\n   \\operatorname{Li}_3\\left(\\frac{a+c}{b+c}\\right)-a (b+c) \\operatorname{Li}_3\\left(\\frac{a (b+c)}{b (a+c)}\\right)\\right)\\right)$$\nHere is the equivalent Mathematica expression. The formula can be proved using differentiation by parameters $a,b,c.$ \nIn fact, the integrand even has a closed-form antiderivative in terms of elementary functions, dilogarithms and trilogarithms, but it is too large to put it here.", "meta": {"post_id": 1279165, "input_score": 69, "output_score": 37, "post_title": "Integrals of the form ${\\large\\int}_0^\\infty\\operatorname{arccot}(x)\\cdot\\operatorname{arccot}(a\\,x)\\cdot\\operatorname{arccot}(b\\,x)\\ dx$"}}
{"input": "Could someone possible explain the differences between each of these; \nSingularities, essential singularities, poles, simple poles.\nI understand the concept and how to use them in order to work out the residue at each point, however, done fully understand what the difference is for each of these\nAs far as i understand a simple pole is a singularity of order $1$?\nthen we have poles of order $n$ which aren't simple?\nnot too sure about essential singularity", "output": "The point $z_{0}$ is an isolated singularity of $f(z)$ if $f(z)$ is analytic in \n$0 \\lt |z-z_{0}| \\lt r$ (a circle of radius r centered at $z_{0}$ with the point $z_{0}$ punched out). If one expands a function $f(z)$ in a Laurent series about the point $z_{0}$, \n$$f(z) = \\sum\\limits_{k=-\\infty}^{\\infty} a^{k} (z-z_{0})^{k}$$\nwe can classify isolated singularties into 3 cases:\n\nIf there are no negative powers of $z-z_{0}$, then $z_{0}$ is a removable singularity and the Laurent series\nis a power series.\n\nExample: $$\\frac{\\sin(z)}{z} = 1 - \\frac{z^{2}}{3!} + \\frac{z^{4}}{5!} - ...$$ has a removable singularity\nat 0.\n\n$f(z)$ has a pole of order m at $z_{0}$ if m is the largest positive integer such that \n$a_{-m} \\ne 0$. A pole of order one is a simple pole. A pole of order two is a double pole, etc.\n\nExample: $$f(z) = \\frac{1}{(z-3i)^{7}}$$ has a pole of order 7 at $z=3i$\n\nIf there are an infinite number of negative powers of $z-z_{0}$, then $z_{0}$ is an essential singularity.\n\nExample: $$\\mathrm{e}^{1/z} = 1 + \\frac{1}{z} + \\frac{1}{2!z^{2}} + ...$$ has an essential singularity\nat 0.", "meta": {"post_id": 1284316, "input_score": 36, "output_score": 49, "post_title": "Singularities, essential singularities, poles, simple poles"}}
{"input": "$V = \\Bbb{R}^3$ and has basis $\\mathcal{B} = \\{\\vec{e_1}-\\vec{e_2},\\vec{e_1}+\\vec{e_2},\\vec{e_3}\\}$\nHow do I find the dual basis? This is not homework, but an example that I am struggling to grasp. This is a simple question, so I would really appreciate if you wouldn't skip any details no matter the triviality as there may be fundamental gaps in my understanding.", "output": "Notice that the definition of a dual basis is that, given $\\beta = \\{v_1, ..., v_n\\}$, its dual is $\\beta^* = \\{f_1, ..., f_n\\}$ such that $f_i(v_j) = \\delta_{ij}$.\nGiven $\\beta = \\{\\vec{e_1}-\\vec{e_2},\\vec{e_1}+\\vec{e_2},\\vec{e_3}\\}$, we want such a basis. Also, since we know that the $f_i$ are linear functionals, we have that $f_i(x_1, x_2, x_3) = ax_1 + bx_2 + cx_3$. As you probably know, if we define the behaviour of $f_i$ in terms of our basis, we completely determine the function. I'll do the first example, the requirements are:\n$$f_1(e_1 - e_2) = a - b = 1$$\n$$f_1(e_1 + e_2) = a + b = 0$$\n$$f_1(e_3) = c = 0$$\nHence, $a = -b$, which implies that $-2b = 1$, hence $b = \\frac{-1}{2}$, and $a = \\frac{1}{2}$, while $c = 0$. Thus:\n$$f_1(x_1, x_2, x_3) = \\frac{1}{2} x_1 - \\frac{1}{2} x_2$$\nWhich, as desired, satisfies all the constraints. Just repeat this process for the other $f_i$s and that will give you the dual basis!", "meta": {"post_id": 1286100, "input_score": 19, "output_score": 39, "post_title": "How do I find a dual basis given the following basis?"}}
{"input": "I am reading this article on Wikipedia, where three sample paths of different OU-processes are plotted. I would like to do the same to learn how this works, but I face troubles implementing it in Matlab.\nI think I have to discretize this equation somehow:\n$ x_t  = x_0 e^{-\\theta t} + \\mu(1-e^{-\\theta t}) + \\int_0^t \\sigma e^{\\theta (s)}\\, \\mathrm{d}W_s. \\, $,\nbut especially the integral equation confuses me a lot.\nI also think I will  have to use $W_t = W_t-W_0 \\sim N(0,t)$ somehow, but don't know how yet...\nCan someone please help me out?\nI am new to stochastic calculus, so please help me understand step by step.", "output": "The Wikipedia article you cite provides everything you need to evaluate the analytical solution of the Ornstein\u2013Uhlenbeck process. However, for a beginner, I agree that it may not be very clear.\n1. Simulating the Ornstein\u2013Uhlenbeck process\nYou should first be familiar with how to simulate this process using the Euler\u2013Maruyama method. The stochastic differential equation (SDE)\n$$\\mathrm{d}x_t = \\theta (\\mu - x_t)\\mathrm{d}t + \\sigma \\mathrm{d}W_t$$\ncan be discretized and approximated via\n$$ X_{n+1} = X_n + \\theta (\\mu - X_n)\\Delta t + \\sigma \\Delta W_n$$\nwhere $\\Delta W_n$ are independent identically distributed Wiener increments, i.e., normal variates with zero mean and variance $\\Delta t$. Thus, $W_{t_{n+1}}-W_{t_n} = \\Delta W_n \\sim N(0,\\Delta t) = \\sqrt{\\Delta t} \\space N(0,1)$. This can be simulated in Matlab very easily using randn to generate standard normal variates:\nth = 1;\nmu = 1.2;\nsig = 0.3;\ndt = 1e-2;\nt = 0:dt:2;             % Time vector\nx = zeros(1,length(t)); % Allocate output vector, set initial condition\nrng(1);                 % Set random seed\nfor i = 1:length(t)-1\n    x(i+1) = x(i)+th*(mu-x(i))*dt+sig*sqrt(dt)*randn;\nend\nfigure;\nplot(t,x);\n\nwhich will result in a plot something like the blue trace in the Wikipedia article (it won't be identical because different random values were used \u2013 see also my comments below this answer). Note that the above is not the most efficient code.\n2. Solution in terms of integral\nThe equation in your question is in terms of a stochastic integral\n$$x_t  = x_0 e^{-\\theta t} + \\mu (1-e^{-\\theta t}) + \\sigma e^{-\\theta t}\\int_0^t e^{\\theta s} \\mathrm{d}W_s$$\nTo obtain a numerical solution in Matlab with this, you'll need need to numerically approximate (discretize) the integral term using an SDE integration scheme like Euler\u2013Maruyama described above:\nth = 1;\nmu = 1.2;\nsig = 0.3;\ndt = 1e-2;\nt = 0:dt:2;             % Time vector\nx0 = 0;                 % Set initial condition\nrng(1);                 % Set random seed\nW = zeros(1,length(t)); % Allocate integrated W vector\nfor i = 1:length(t)-1\n    W(i+1) = W(i)+sqrt(dt)*exp(th*t(i))*randn; \nend\nex = exp(-th*t);\nx = x0*ex+mu*(1-ex)+sig*ex.*W;\nfigure;\nplot(t,x);\n\nor without a for loop using cumsum:\nth = 1;\nmu = 1.2;\nsig = 0.3;\ndt = 1e-2;\nt = 0:dt:2;             % Time vector\nx0 = 0;                 % Set initial condition\nrng(1);                 % Set random seed\nex = exp(-th*t);\nx = x0*ex+mu*(1-ex)+sig*ex.*cumsum(exp(th*t).*[0 sqrt(dt)*randn(1,length(t)-1)]);\nfigure;\nplot(t,x);\n\n3. Analytical solution\nTo compute the full analytical solution of an Ornstein\u2013Uhlenbeck process for a given time series and corresponding Wiener increments, you'll need use a \"scaled time-transformed\" Wiener process:\n$$x_t = x_0 e^{-\\theta t} +\\mu (1-e^{-\\theta t}) + \\frac{\\sigma e^{-\\theta t}}{\\sqrt{2 \\theta}}W_{e^{2 \\theta t}-1}$$\nSee Doob 1942 for further details and a derivation. The $W_{e^{2 \\theta t}-1}$ may appear confusing, but it's just a Wiener process with zero mean and variance $e^{2 \\theta t}-1$. To calculate this in Matlab:\nth = 1;\nmu = 1.2;\nsig = 0.3;\ndt = 1e-2;\nt = 0:dt:2;             % Time vector\nx0 = 0;                 % Set initial condition\nrng(1);                 % Set random seed\nW = zeros(1,length(t)); % Allocate integrated W vector\nfor i = 1:length(t)-1\n    W(i+1) = W(i)+sqrt(exp(2*th*t(i+1))-exp(2*th*t(i)))*randn;\nend\nex = exp(-th*t);\nx = x0*ex+mu*(1-ex)+sig*ex.*W/sqrt(2*th);\nfigure;\nplot(t,x);\n\nThis can be implemented without a for loop using cumsum and diff:\nth = 1;\nmu = 1.2;\nsig = 0.3;\ndt = 1e-2;\nt = 0:dt:2;      % Time vector\nx0 = 0;          % Set initial condition\nrng(1);          % Set random seed\nex = exp(-th*t);\nx = x0*ex+mu*(1-ex)+sig*ex.*cumsum([0 sqrt(diff(exp(2*th*t)-1)).*randn(1,length(t)-1)])/sqrt(2*th);\nfigure;\nplot(t,x);\n\n4. Resources\nYou can also use my own SDETools Matlab toolbox on GitHub for numerically solving SDEs and computing analytical solutions of common stochastic processes. In particular, see the sde_ou function to calculate analytical solutions for the Ornstein\u2013Uhlenbeck process.\nI also recommend reading the following excellent article for further details on SDEs and simulating them in Matlab:\n\nDesmond J. Higham, 2001, An Algorithmic Introduction to Numerical Simulation of Stochastic Differential Equations, SIAM Rev. (Educ. Sect.), 43 525\u201346. http://dx.doi.org/10.1137/S0036144500378302\n\nThe URL to the Matlab files in the paper won't work \u2013 they can be found here now. Note, however, that some of the Matlab syntax (particularly related to random number generation and seeding) is a bit outdated as this was written nearly 15 years ago.", "meta": {"post_id": 1287634, "input_score": 30, "output_score": 51, "post_title": "Implementing Ornstein\u2013Uhlenbeck in Matlab"}}
{"input": "Inspired by an article on Prime Spiral and Hough transform I tried to analyze patterns created by plotting numbers on spiral (Archimedean?).\n$$x = \\cos( angle ) * radius$$\n$$y = \\sin( angle ) * radius$$\nwhere angle and radius is incremented by a constant value\n\nNo surprise so far.\nWhen all non prime numbers are suppressed this ray pattern appears at a specific point (manipulating the increments of the spiral):\n\nIt turned out that the increment of the angle is exactly\n$$\\cfrac{\\pi}{30}$$\nAll prime numbers are on 16 lines (ignoring that the first numbers probably don't match the scheme).\nSetting the increments to\n$$\\cfrac{\\pi}{3}$$\nleads to (white dots are prime numbers):\n\nMirroring them on the x-axis doesn't seem to help to have them on a single line. Because there are some non prime numbers in the lines.\nWhat could I do the force them to a single straight line?\nIf this pattern is a well known property of prime numbers how is it called?\nIn case someone would like to experiment I added the java source at github\nRelated:\nMeaning of Rays in Polar Plot of Prime Numbers", "output": "There are 2 behaviours going on here.\nIn your last picture, it's easy to see that all numbers lie on the 6 rays through the origin. Why? This is because there are $2\\pi$ radians in a circle, and you are incrementing by $\\pi/3$ radians each time (which is 1/6 of the circle). This is why you are getting distinct rays.\nThe other behaviour occurs when you only look at primes. In the second picture (the one consisting of only primes), there is space for 60 rays (since you are incrementing by $\\pi/30$ radians, which is 1/60th of a circle each time). So the new question is, why do only 16 rays appear?\nThe answer is that $\\varphi(60) = 16$, meaning that there are only 16 residue classes for primes to fit in mod 60. Stated differently there are only 16 solutions to $p \\equiv x \\pmod {60}$ in $x$, where $p$ ranges across all the primes. So there are 16 distinguished rays containing primes.\nSimilarly, there are two distinguished rays in the last picture, which is why you can only see primes on 2 of the rays.\nYou might be interested to know that the property of being a ray containing primes will not be origin-symmetric, but mirror-symmetric over the horizontal line $y = 0$. This has to do with how $\\gcd(x,n) = \\gcd(n-x,n)$, and the order in which you are plotting these rays.\nTo answer your final question, you can plot all primes (except 2) on the same line by using $\\pi$ as your increment, or by using $2\\pi$ as your increment. The first is equivalent to saying that all primes except $2$ are odd. The second actually puts all numbers on a single line.", "meta": {"post_id": 1291787, "input_score": 34, "output_score": 41, "post_title": "How to force prime numbers into a line?"}}
{"input": "Suppose $G$ is a finite group and I know for every $k \\leq |G|$ that exactly $n_k$ elements in $G$ have order $k$. Do I know what the group is? Is there a counterexample where two groups $G$ and $H$ have the same number of elements for each order, but $G$ is not isomorphic to $H$? I suspect that there is, but I haven't thought of one.", "output": "Take $G=\\mathbb{Z}/4\\times \\mathbb{Z}/4$, and $H=Q_8\\times \\mathbb{Z}/2$ of order $16$, where $Q_8$ denotes the quaternion group. Both groups have exactly $1$ element of order $1$, $3$ elements of order $2$ and $12$ elements of order $4$.\nEdit: I understood the question as follows: Is there a counterexample where two groups $G$ and $H$ have the same number of elements for each order, but $G$ is not isomorphic to $H$ ? Is it really required, that all elements different from $1$ in $G$ have the same order ?", "meta": {"post_id": 1296833, "input_score": 69, "output_score": 79, "post_title": "If I know the order of every element in a group, do I know the group?"}}
{"input": "There is a probabilistic method to solve it. But I am not familiar with probability. I am trying to compute it by analytic method, such as using L Hospital's rule or Stolz formula, but they are not working.", "output": "Limit using Poisson distribution\nPartial sums of exponential series\nEvaluating $\\lim\\limits_{n\\to\\infty} e^{-n} \\sum\\limits_{k=0}^{n} \\frac{n^k}{k!}$\nCalculate limit with summation index in formula\nCentral value of the partial exponential function\nFinding $\\lim_{n\\to\\infty} e^{-n}\\sum_{k=0}^n \\frac{n^k}{k!}$ if it exists\nCompare $e^n$ and its first $n$ terms sum\nShow that $\\lim\\limits_{n\\rightarrow\\infty} e^{-n}\\sum\\limits_{k=0}^n \\frac{n^k}{k!}=\\frac{1}{2}$\nHow can I compute this limit?\n$ \\lim_{n\\to\\infty} e^{-n}\\sum_{k=1}^n \\frac{n^k}{k!} $\nLimit of Series with Variable Lower Bound\nWeird limit $\\lim \\limits_{n\\mathop\\to\\infty}\\frac{1}{e^n}\\sum \\limits_{k\\mathop=0}^n\\frac{n^k}{k!} $\nIs the sequences$\\{S_n\\}$ convergent?\nSummation of exponential series\nDoes n power of e grow much more faster than its Maclaurin polynomial?\nLimit Challenge\nLimits of sequences connected with real and complex exponential\nThe limit $\\lim_{n\\to \\infty}\\frac{T_n(n)}{e^n}$ where $T_n(x)$ is the Taylor polynomial of $e^x$\nLimit of a series (Gamma distribution)\n\n\nAlso related:\n\nLimit of an expression", "meta": {"post_id": 1297553, "input_score": 6, "output_score": 43, "post_title": "How to compute $\\lim_{n\\rightarrow\\infty}e^{-n}\\left(1+n+\\frac{n^2}{2!}\\cdots+\\frac{n^n}{n!}\\right)$"}}
{"input": "So, $$1,1,2,3,5,8,13,21...$$ Any connection to primes?...it appears not. However, in between the Fibonacci numbers are how much primes? Let's see:\n\n$1$ and $1$: $0$\n$1$ and $2$: $0$\n$2$ and $3$: $0$\n$2$ and $3$: $0$\n$5$ and $8$: $1$\n$8$ and $13$: $1$\n$13$ and $21$: $2$\n$21$ and $34$: $3$\n$34$ and $55$: $5$\n$55$ and $89$: $8$\n$89$ and $144$: $13$\n\nHuh. What could this imply? Let me just close with the same annoying (but wonderful) pattern. $$1,2,3,5,8,13,21...$$", "output": "Eyebrow raising indeed, though the pattern does not continue as you suggest. I get\n$$\n0, 1, 1, 2, 3, 5, 7, 10, 16, 23, 37, 55, 84, 125, 198\n$$\nRemember that the the number of primes has a well known growth rate (https://en.wikipedia.org/wiki/Prime_number_theorem). Since the Fibonacci numbers are relatively spread out, using $n/\\log n$ to approximate the number of primes less than $n$ will cause the number of primes between them to behave like the growth rate of the primes.", "meta": {"post_id": 1303258, "input_score": 51, "output_score": 55, "post_title": "Eyebrow-raising pattern of number of primes between terms of the Fibonacci number sequence?"}}
{"input": "How do I find this sum: $$\\sum_{n=1}^\\infty \\frac{1}{p(n)}$$ where\n$p(n)=\\dfrac{n(3n-1)}{2}$ is the $n$th pentagonal number?\nI know it is a convergent series, but I don't know if the sum can be found in closed form.", "output": "Another way to do is just use basic calculus without using the digamma function: Let\n$$ f(x)=\\sum_{n=1}^\\infty\\frac{2}{n(3n-1)}x^{3n}. $$\nClearly $\\sum_{n=1}^\\infty\\frac{2}{n(3n-1)}=f(1)$. Note\n$$ f'(x)=6\\sum_{n=1}^\\infty\\frac{1}{3n-1}x^{3n-1},f''(x)=6\\sum_{n=1}^\\infty x^{3n-2}=\\frac{6x}{1-x^3}. $$\nSo\n\\begin{eqnarray}\nf(1)&=&\\int_0^1\\int_0^x\\frac{6t}{1-t^3}dtdx\\\\\n&=&\\int_0^1\\int_t^1\\frac{6t}{1-t^3}dxdt\\\\\n&=&\\int_0^1\\frac{6t(1-t)}{1-t^3}dt\\\\\n&=&\\int_0^1\\frac{6t}{1+t+t^2}dt\\\\\n&=&\\int_0^1\\frac{6t}{(t+\\frac{1}{2})^2+(\\frac{\\sqrt3}{2})^2}dt\\\\\n&=&3\\ln3-\\frac{\\pi}{\\sqrt3}.\n\\end{eqnarray}", "meta": {"post_id": 1305546, "input_score": 20, "output_score": 36, "post_title": "Infinite sum of reciprocals of pentagonal numbers"}}
{"input": "The Taylor expansion itself can be derived from mean value theorems which themselves are valid over the entire domain of the function. Then why doesn't the Taylor series converge over the entire domain? I understand the part about the convergence of infinite series and the various tests. But I seem to be missing something very fundamental here..", "output": "It is rather unfortunate that in calc II we teach Taylor series at the same time as we teach Taylor polynomials, all the while not doing a very good job of stressing the distinction between an infinite series and a finite sum. In the process we seem to teach students that Taylor series are a much more powerful tool than they are, and that Taylor polynomials are a much less powerful tool than they are.\nThe main idea is really the finite Taylor polynomial. The Taylor series is just a limit of these polynomials, as the degree tends to infinity. The Taylor polynomial is an approximation to the function based on its value and a certain number of derivatives at a certain point. The remainder formulae tell us about the error in this approximation. In particular they tell us that higher degree polynomials provide better local approximations to a function.\nBut the issue is that \"local\" has a different meaning for different values of the degree $n$. To explain what I mean by that, you can look at the Lagrange remainder, which tells you that the error in an approximation of degree $n$ is\n$$\\frac{f^{(n+1)}(\\xi_n) (x-x_0)^{n+1}}{(n+1)!}$$\nwhere $\\xi_n$ is between $x_0$ and $x$. So the ratio of the error with degree $n$ to the error with degree $n-1$ is*\n$$\\frac{f^{(n+1)}(\\xi_n) (x-x_0)}{f^{(n)}(\\xi_{n-1}) (n+1)}$$\nwhere similarly $\\xi_{n-1}$ is between $x$ and $x_0$. So the error is smaller where this quantity is less than $1$. From this form we can see that if $|x-x_0|<(n+1) \\left | \\frac{f^{(n)}(\\xi_{n-1})}{f^{(n+1)}(\\xi_n)} \\right |$ then the error with degree $n$ is less than that with degree $n-1$. That looks good because we have that growing factor of $n+1$, but what if $\\frac{f^{(n)}(\\xi_{n-1})}{f^{(n+1)}(\\xi_n)}$ goes to zero, maybe even doing so really fast? Then the interval where the error is reduced by adding another term will shrink, potentially contracting down to just the point of expansion as $n$ tends to infinity.\nIn other words, if the derivatives near $x_0$ (not necessarily just at $x_0$, because we have to evaluate the derivatives at these $\\xi$'s) grow way too fast with $n$, then Taylor expansion has no hope of being successful, even when the derivatives needed exist and are continuous.\n*Here I am technically assuming that $f^{(n)}(\\xi_{n-1}) \\neq 0$. This assumption can fail even when $f$ is not a polynomial; consider $f=\\sin,x_0=\\pi/2,n=1$. But this is a \"degenerate\" situation in some sense.", "meta": {"post_id": 1308992, "input_score": 42, "output_score": 58, "post_title": "Why doesn't a Taylor series converge always?"}}
{"input": "Does $$~\\displaystyle{\\int}_0^1\\frac{\\text{arctanh }x}{\\tan\\left(\\dfrac\\pi2~x\\right)}~dx~\\simeq~0.4883854771179872995286585433480\\ldots~$$ possess a closed form expression ?\n\n\nThis recent post, in conjunction with my age-old interest in Gudermannian functions, have inspired me to ask this question. The reason I suspect that such a closed form might possibly exist is because the integration interval is \u201cmeaningful\u201d for both functions used in the integrand. However, none of the various approaches that I can think of seem to be of any help. Perhaps I'm missing something ?", "output": "Here is an approach.   \nWe give a preliminary result.\n\n   A series of squares of logarithms\n\nLet us consider the poly-Hurwitz zeta function initially defined by the series\n$$\n\\begin{align}\n  \\displaystyle \\zeta(s,t\\mid a,b) := \\sum_{n=1}^{+\\infty} \\frac{1}{(n+a)^{s}(n+b)^t}, \n  \\quad \\Re a>-1, \\, \\Re b>-1, \\, \\Re (s+t)>1. \\tag1\n\\end{align} \n$$\nThe function $ \\displaystyle \\zeta(\\cdot,\\cdot \\mid a,b)$ extends  to a meromorphic function on $\\mathbb{C}^2$  with only singularities on the set $\\displaystyle \\left\\{(s,t) \\in \\mathbb{C}^2, \\,\\Re (s+t)=1\\right\\}$. It clearly generalizes the classic Hurwitz zeta function initially defined by the series\n$$\n\\begin{align}\n  \\displaystyle \\zeta(s,a) := \\sum_{n=0}^{+\\infty} \\frac{1}{(n+a)^s}, \n  \\quad \\Re a>0, \\, \\Re s>1. \\tag2\n\\end{align} \n$$\nWe have the following new result.\n\nTheorem. Let $a, b$ be complex numbers such that $\\Re a>-1$ and $\\Re b>-1$. \nThen\n  $$\n\\begin{align}\n\\sum_{n=1}^{+\\infty}\\log^2\\!\\left(\\! \\frac{n+a}{n+b}\\!\\right)= \\zeta\u2019\u2019(0,a+1)+ \\zeta\u2019\u2019(0,b+1)-2\\zeta^{1,1}(0,0\\mid a,b)\\tag3\n\\end{align} \n$$\n\nwhere $\\log (z)$ denotes the principal value of the logarithm defined for all $z \\neq 0$ by\n$$\n\\log (z) = \\ln |z|+i \\arg z, \\quad -\\pi<\\arg z\\leq \\pi,\n$$\n$ \\displaystyle \\zeta(\\cdot,a)$ and $ \\displaystyle \\zeta(\\cdot,\\cdot \\mid a,b)$ denoting the Hurwitz zeta function and the poly-Hurwitz zeta function respectively and where $$ \\zeta\u2019\u2019(0, a)=\\partial_{s}^2\\left.\\zeta(s,a)\\right|_{s=0},\\qquad \\zeta^{1,1}(0,0\\mid a,b)=\\partial_{st}^2\\left.\\zeta(s,t\\mid a,b)\\right|_{(s,t)=(0,0)}.$$\nProof. \nOn the one hand, one has \n$$\n\\begin{align}\n&\\partial_a \\left(\\zeta''(0,a+1)+\\zeta''(0,b+1)-2\\zeta^{1,1}(0,0\\mid a,b)\\right)\\\\\\\\\n&= \\left.\\partial_s^2 \\left(\\partial_a \\zeta(s,a+1)\\right)\\right|_{s=0}-2\\left.\\partial_{st}^2 \\left(\\partial_a \\zeta(s,t\\mid a,b)\\right)\\right|_{(s,t)=(0,0)}\\\\\\\\\n&= \\left.\\partial_s^2 \\left(-s\\zeta(s+1,a+1)\\right)\\right|_{s=0}-2\\left.\\partial_{st}^2 \\left(-s\\zeta(s+1,t\\mid a,b)\\right)\\right|_{(s,t)=(0,0)}\\\\\\\\ \n&= -\\left.\\left(2\\zeta'(s+1,a+1)+s\\zeta''(s+1,a+1)\\right)\\right|_{s=0}+2\\left.\\partial_s \\!\\left(s\\zeta^{0,1}(s+1,t\\mid a,b)\\right)\\right|_{(s,t)=(0,0)}\\\\\\\\ \n&=2\\gamma_1(a+1)-2\\gamma_1(b,a),\n\\end{align}\n$$  using Theorem $1$ here.\nOn the other hand, one has \n$$\n\\begin{align}\n\\partial_a\\! \\left(\\sum_{n=1}^{+\\infty}\\log^2\\!\\left(\\! \\frac{n+a}{n+b}\\!\\right)\\right)\n\\!=  2\\sum_{n=1}^{+\\infty} \\frac{\\log (n+a)-\\log (n+b)}{n+a}\n=2\\gamma_1(a+1)-2\\gamma_1(b,a),\n\\end{align}\n$$\nusing Theorem $2$ here.\nObserving that \n$$\n\\zeta(s,t\\mid 0,0)=\\zeta(s+t), \\quad \\zeta(s,1)=\\zeta(s),\n$$ where $\\zeta(\\cdot)$ is the Riemann zeta function, then \n$$\n\\zeta\u2019\u2019(0,1)-\\zeta^{1,1}(0,0\\mid 0,0)=0\n$$ and both sides of $(3)$ vanish at $a=b=0$.\nThus $(3)$ holds true. $\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\Box$\n\n   Lucian's integral\n\nWe prove that Lucian's integral is related to the preceding family of logarithmic series.\n\nProposition 1. We have\n  $$\n\\begin{align}\n\\int_0^1 \\frac{\\text{arctanh}\\: x}{\\tan \\left( \\frac{\\pi}2x\\right)}\\:{\\rm d}x=\\frac\\pi4-\\frac1{2\\pi}\\sum_{n=1}^{+\\infty}\\log^2\\!\\left(\\! \\frac{2n-1 }{ 2n+1}\\!\\right).\\tag{4}\n\\end{align} \n$$\n\nProof. Let us proceed on Jack D'Aurizio's route which starts by using the standard expansion \n$$\n\\frac1{\\tan \\left( \\frac{\\pi}2x\\right)}=\\frac{2}{\\pi x}-\\frac{1}{\\pi}\\sum_{n=0}^{\\infty}\\frac{\\zeta(2n+2)}{2^{2n}}x^{2n+1},\\quad 0<x<1,\\tag{5}\n$$ then integrating termwise using\n$$\n\\begin{align}\n&\\int_{0}^{1}x^{2n+1}\\:\\text{arctanh} \\:x \\:{\\rm d}x\\\\\n&=\\frac1{2(n+1)(2n+1)}+\\frac{\\ln2}{2(n+1)}+\\frac1{4(n+1)}\\left(\\gamma+\\psi \\left(n+\\frac12 \\right) \\right)\\tag{6}\n\\end{align}\n$$\nto get\n$$\n\\begin{align}\n\\int_0^1 \\frac{\\text{arctanh}\\: x}{\\tan \\left( \\frac{\\pi}2x\\right)}\\:{\\rm d}x &=\\frac{\\pi }{4}+\\frac{2}{\\pi }(1-\\ln 2)\\ln\\left(\\frac{\\pi }{2}\\right)\\\\\\\\&-\\frac{1}{\\pi }\\sum_{n=0}^{\\infty}\\frac{\\zeta(2n+2)}{(2n+1)2^{2n}}-\\frac{1}{\\pi }\\sum_{n=0}^{\\infty}\\frac{\\zeta(2n+2)\\left(\\psi\\left(n+\\frac12\\right)+\\gamma\\right)}{(n+1)2^{2n+2}}. \\tag7\n\\end{align} \n$$\nWe are left with two non trivial series to evaluate. \nWe prove that each series may be evaluated using the poly-Stieltjes constants.\nOne may write\n$$\n\\require{cancel}\n\\begin{align}\n\\sum_{n=0}^{\\infty}\\frac{\\zeta(2n+2)}{(2n+1)2^{2n}}&=\\sum_{n=0}^{\\infty}\\sum_{k=1}^{\\infty}\\frac1{k^{2n+2}}\\frac1{(2n+1)2^{2n}}\\\\\n&=4\\sum_{k=1}^{\\infty}\\sum_{n=0}^{\\infty}\\frac1{(2n+1)}\\frac1{(2k)^{2n+2}}\\\\\n&=\\sum_{k=1}^{\\infty}\\frac1k\\left(\\log \\left(1 + \\frac1{2k}\\right)-\\log \\left(1 - \\frac1{2k}\\right)\\right)\\\\\n&=\\sum_{k=1}^{\\infty}\\frac1k\\left(\\log \\left(k + \\frac12\\right)-\\log \\left(k - \\frac12\\right)\\right)\\\\\n&=\\gamma_1\\Big({\\small\\frac12,0}\\Big)-\\gamma_1\\Big({\\small-\\frac12,0}\\Big) \\tag{8}\n\\end{align}\n$$ using Theorem $2$ here.\nTo evaluate the last series on the right hand side of $(7)$, one may check with some algebra that, for any complex number $z$ satisfying $|z|<1$, the following identity holds true:\n$$\n\\begin{align}\n&\\sum_{n=0}^{\\infty}\\frac{\\psi\\left(n+\\frac12\\right)+\\gamma}{n+1}z^{2n+2}\\\\\n&=2z\\log\\left(\\frac{1-z}{1+z} \\right)-2\\left(1- \\ln 2 \\right)\\log (1-z^2)+\\frac12\\log^2\\left(\\frac{1-z}{1+z} \\right). \\tag9\n\\end{align}\n$$\nThen\n$$\n\\require{cancel}\n\\begin{align}\n&\\sum_{n=0}^{\\infty}\\frac{\\zeta(2n+2)\\left(\\psi\\left(n+\\frac12\\right)+\\gamma\\right)}{(n+1)2^{2n+2}} \\\\\n&=\\sum_{n=0}^{\\infty}\\sum_{k=1}^{\\infty}\\frac1{k^{2n+2}}\\frac{\\psi\\left(n+\\frac12\\right)+\\gamma}{(n+1)2^{2n+2}}\\\\\n&=\\sum_{k=1}^{\\infty}\\sum_{n=0}^{\\infty}\\frac{\\psi\\left(n+\\frac12\\right)+\\gamma}{n+1}\\frac1{(2k)^{2n+2}}\\\\\n&=\\sum_{k=1}^{\\infty}\\frac1k \\log \\left(\\frac{1-\\frac1{2k}}{1+\\frac1{2k}}\\right)-2\\left(1- \\ln 2 \\right)\\sum_{k=1}^{\\infty}\\log \\left(1 - \\frac1{4k^2}\\right)+\\frac12\\sum_{k=1}^{\\infty}\\log^2 \\left(\\frac{1-\\frac1{2k}}{1+\\frac1{2k}}\\right)\\\\\n&=\\sum_{k=1}^{\\infty}\\frac1k \\log \\left(\\frac{k-\\frac12}{k+\\frac12}\\right)+2\\left(1- \\ln 2 \\right)\\ln\\left(\\frac{\\pi }{2}\\right)+\\frac12\\sum_{k=1}^{\\infty}\\log^2\\!\\left(\\! \\frac{2k-1 }{ 2k+1}\\!\\right)\\\\\n&=\\gamma_1\\Big({\\small-\\frac12,0}\\Big)-\\gamma_1\\Big({\\small\\frac12,0}\\Big)+2\\left(1- \\ln 2 \\right)\\ln\\left(\\frac{\\pi }{2}\\right)+\\frac12\\sum_{k=1}^{\\infty}\\log^2\\!\\left(\\! \\frac{2k-1 }{ 2k+1}\\!\\right).\\tag{10}\n\\end{align}\n$$\nInserting $(10)$ and $(8)$ into $(7)$ gives the announced result $(4)$. $\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\Box$\nWe deduce the following closed form.\n\nProposition 2. We have \n$$\n\\begin{align}\n\\int_0^1 \\frac{\\text{arctanh} x}{\\tan \\left( \\frac{\\pi}2x\\right)}\\:{\\rm d}x=\\frac\\pi4+\\frac2\\pi\\ln^2 2+\\frac1\\pi\\ln 2\\ln \\pi+\\frac1\\pi\\zeta^{1,1}\\Big(0,0 \\:\\Bigr\\rvert {\\small-\\frac12,\\frac12}\\Big). \\tag{11}\n\\end{align} \n$$ \n\nProof. One may observe that\n$$\n\\begin{align}\n\\zeta\\left(s,\\frac12 \\right) & = \\left(2^s-1 \\right)\\zeta(s) \\tag{12}\\\\\n\\zeta\\left(s,\\frac32 \\right) & = \\left(2^s-1 \\right)\\zeta(s)-2^s, \\tag{13}\n\\end{align}\n$$ and recalling that $\\zeta'(0)=-\\frac12 \\ln (2 \\pi)$, one may obtain\n$$\n\\begin{align}\n\\zeta''\\left(0,\\frac12 \\right) & = -\\frac32 \\ln^2 2 - \\ln 2 \\ln \\pi \\tag{14}\\\\\n\\zeta''\\left(0,\\frac32 \\right) & = -\\frac52 \\ln^2 2 - \\ln 2 \\ln \\pi. \\tag{15}\n\\end{align}\n$$\nFrom $(4)$ and $(3)$, we have\n$$\n\\require{cancel}\n\\begin{align}\n\\int_0^1 \\frac{\\text{arctanh}\\: x}{\\tan \\left( \\frac{\\pi}2x\\right)}\\:{\\rm d}x\n&=\\frac\\pi4-\\frac1{2\\pi}\\sum_{n=1}^{+\\infty}\\log^2\\!\\left(\\! \\frac{2n-1 }{ 2n+1}\\!\\right)\\\\\\\\\n&=\\frac\\pi4-\\frac1{2\\pi}\\sum_{n=1}^{+\\infty}\\log^2\\!\\left(\\! \\frac{n-\\frac12 }{ n+\\frac12}\\!\\right)\\\\\\\\\n&=\\frac\\pi4-\\frac1{2\\pi}\\left( \\zeta''\\left(0,-\\frac12+1 \\right)+ \\zeta''\\left(0,\\frac12+1 \\right)-2\\zeta^{1,1}\\Big(0,0 \\:\\Bigr\\rvert {\\small-\\frac12,\\frac12}\\Big)\\right)\\\\\\\\\n&=\\frac\\pi4-\\frac1{2\\pi}\\left( \\zeta''\\left(0,\\frac12 \\right)+ \\zeta''\\left(0,\\frac32 \\right)-2\\zeta^{1,1}\\Big(0,0 \\:\\Bigr\\rvert {\\small-\\frac12,\\frac12}\\Big)\\right),\n\\end{align}\n$$ by appealing to $(14)$ and $(15)$, we get $(11)$. $\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\Box$\nBy combining Proposition $2$ and pisco125's derivation we obtain the following new closed forms.\n\nProposition 3. We have \n$$\n\\begin{align}\n\\int_{0}^{\\infty} {\\ln (1+x^2)\\over {e^{2\\pi x}+1}}\\:{\\rm d}x =&\\:\\frac\\pi4+\\frac1{2\\pi}\\ln^2 2+\\frac1\\pi\\zeta^{1,1}\\Big(0,0 \\:\\Bigr\\rvert {\\small-\\frac12,\\frac12}\\Big) \\tag{16}\n\\\\\\\\\n \\sum_{k=1}^{\\infty} {(-1)^k\\over k} \\text{Ci} (2k\\pi)=&\\:\\frac{\\pi^2}4+\\frac12\\ln^2 2+\\:\\zeta^{1,1}\\Big(0,0 \\:\\Bigr\\rvert {\\small-\\frac12,\\frac12}\\Big) \\tag{17}\n\\end{align} \n$$\n\nwhere $\\text{Ci} (\\cdot)$ is the cosine integral and where  $\\displaystyle \\zeta(\\cdot,\\cdot\\mid a,b)$ is the poly-Hurwitz zeta function.", "meta": {"post_id": 1311929, "input_score": 61, "output_score": 61, "post_title": "Closed Form for $~\\int_0^1\\frac{\\text{arctanh }x}{\\tan\\left(\\frac\\pi2~x\\right)}~dx$"}}
{"input": "We can define trace if $A =\\sum_{i} \\langle e_i, Ae_i\\rangle$ where $e_i$'s are standard column vectors, and $\\langle x, y\\rangle =x^t y$ for suitable column vectors $x, y$. With this set up, I want to prove trace of AB and BA are same, so it's enough to prove that $$\\sum_{i} \\langle e_i, ABe_i\\rangle =\\sum_{i} \\langle e_i, BAe_i\\rangle$$ but how to conclude that?", "output": "by definition $$\\begin{align}trace(AB) &= (AB)_{11}+(AB)_{22}+\\cdots+(AB)_{nn}\\\\\n&=a_{11}b_{11}+a_{12}b_{21}+\\cdots + a_{1k}b_{k1} \\\\\n&+ a_{21}b_{12}+a_{22}b_{22}+\\cdots + a_{2k}b_{k2}\\\\\n&+\\vdots  \\\\\n&+a_{n1}b_{1n}+a_{n2}b_{2n}+\\cdots + a_{nk}b_{kn}\\end{align}$$ if you view the sum according to the columns, then you see that it is the $trace(BA).$  therefore, $$trace(AB) = trace(BA). $$", "meta": {"post_id": 1314142, "input_score": 10, "output_score": 35, "post_title": "Trace of AB = Trace of BA"}}
{"input": "I have a question, to aid my understanding, about the proof that differentiability implies continutity.$\\mathstrut$ \nDifferentiability Definition\nWhen we say a function is differentiable at $x_0$, we mean that the limit:\n$$\u200ef^{\\prime} \u200e(x) = \\lim_{x\\to x_0} \\frac{f(x) - f(x_0)}{x-x_0}$$ exists.\nContinuity Definition\nWhen we say a function is continuous at $x_0$, we mean that:\n$$\\lim_{x\\to x_0} f(x) - f(x_0) = 0$$\nTheorem: Differentiability implies Continuity: If $f$ is a differentiable function at $x_0$, then it is continuous at $x_0$.\nProof:\nLet us suppose that $f$ is differentiable at $x_0$. Then\n$$ \\lim_{x\\to x_0} \\frac{f(x) - f(x_0)}{x-x_0} =  \u200ef^{\\prime} \u200e(x) $$\nand hence\n$$ \\lim_{x\\to x_0} f(x) - f(x_0) = \\lim_{x\\to x_0} \\left[ \\frac{f(x) - f(x_0)}{x-x_0} \\right] \\cdot \\lim_{x\\to x_0} (x-x_0) = 0$$\nWe have therefore shown that, using the definition of continuous, if the function is differentiable at $x_0$, it must also be continuous.\nMy Question\nThe proof seems to execute the following steps:\n\nAssume the function is continuous at $x_0$\nShow that, with little algebra, we can change this into an equivalent question about differentiability at $x_0$. With this little bit of algebra, we can show that if a function is differentiable at $x_0$ it is also continuous.\n\nWhat I am slightly unsure about is the apparent circularity. In my mind it seems to say, if a function is continuous, we can show that if it is also differentiable, then it is continuous. Rather than what I was expecting, namely, if a function is differentiable, we can show it must be continuous.\nHopefully my confusion is clear. Any help will be greatly appreciated.", "output": "Technically, there is an implicit issue of existence of limits which is being swept under the rug in the presentation you have given. The assumption of differentiability at $x_0$ says that the limit\n$$\\lim_{x \\to x_0} \\frac{f(x) - f(x_0)}{x-x_0}$$\nexists as a finite number. The limit $\\lim_{x \\to x_0} x-x_0$ exists and is zero regardless of our assumptions. Then the product rule for limits tells us both that $\\lim_{x \\to x_0} f(x)-f(x_0)$ exists, and that it is the product of the two limits above, which means it must be zero. Because the product rule also tells us that the limit exists, we do not have to assume continuity first.", "meta": {"post_id": 1314630, "input_score": 28, "output_score": 35, "post_title": "Differentiability implies continuity - A question about the proof"}}
{"input": "I am not sure how to solve this, but this is my best guess.  Since we want to know how many 12 bit strings have more 1's than 0's, start with 5 since it is one less than 12/2=6.\nThen we proceed with:\n$${12 \\choose 5} + {12 \\choose 4} + {12 \\choose 3} + {12 \\choose 2}+ {12 \\choose 1}+ {12 \\choose 0}=1586$$\nIs my reasoning correct?  I am not sure if we should use the choose function here, but I believe we do.", "output": "Your expression is correct. Here is another way that exploits the symmetry. \nLet $a$ be the number of strings with more $1$'s than $0$'s, let $b$ be the number with more $0$'s than $1$'s, and let $c$ be the number with equal numbers of $0$'s and $1$'s.\nThen $a=b$, $a+b+c=2^{12}$, and $c=\\binom{12}{6}$. So $2a=2^{12}-\\binom{12}{6}$ and therefore \n$$a=2^{11}-\\frac{1}{2}\\binom{12}{6}.$$", "meta": {"post_id": 1315712, "input_score": 13, "output_score": 34, "post_title": "How many 12-bit strings with more 1\u2019s than 0\u2019s?"}}
{"input": "Consider the map \n$$ f: \\mathbb R^2 \\to \\mathbb R, (x,y) \\mapsto x^3 + y^3 + xy$$\nThis defines a surface in $\\mathbb R^3$. Let's consider some level set $f(x,y) = c$:\n\n(see here page 67)\nI think of these pictures as viewed from above looking down on the x-y-plane. Using these 3 pictures to imagine what the surface should look like lead me to believe that the surface should look like this:\n(apologies, this is the best I could do with the online drawing tool)\n\nTo verify this I then used an online plotter which yielded this:\n\nNow my question is:\n\nHow is it possible that the level lines of the last graph yield the\n  level lines in the first three pictures? I do not see how this is\n  possible.\n\n(the range of the plot I used was $-1 \\le x,y \\le 1$ and $-1.3 \\le z \\le 3$, varying the range does not seem to change the graph)", "output": "Your sketch doesn't even look like the graph of a smooth function - remember that it should just be a deformed plane with nowhere vertical tangents. Here's an animation I whipped up that might help your intuition for this example: \n\nIn general it's helpful to remember how changes in the topology of the level curves correspond to local features of the function: a loop appearing/disappearing corresponds to a local extremum, while a transition across a self-intersection like you see in this example at $c=0$ corresponds to a saddle point.", "meta": {"post_id": 1320876, "input_score": 26, "output_score": 52, "post_title": "What is the flaw in my thinking for the graph of this function?"}}
{"input": "One distinct difference between axioms of topology and sigma algebra is the asymmetry between union and intersection; meaning topology is closed under finite intersections sigma-algebra closed under countable union. It is very clear mathematically but is there a way to think; so that we can define a geometric difference? In other words I want to have an intuitive idea in application of this objects.", "output": "I would like to mention that in An Epsilon of Room, remark 1.1.3, Tao states:\n\nThe notion of a measurable space (X, S) (and of a measurable  function)  is  superficially  similar  to  that  of  a topological space (X, F) (and of a\n  continuous function); the topology F contains \u2205 and X just as the \u03c3-algebra S\n  does,  but is now closed under arbitrary unions and finite intersections, rather than countable unions, countable  intersections,  and  complements.   The  two  categories  are linked to each other by the Borel algebra construction.\n\nLater, in example 1.1.5:\n\ngiven any collection\n  F\n  of sets on\n  X\n  we can define the\n  \u03c3-algebra\n  B\n  [\n  F\n  ]\n  generated  by\n  F\n  , defined to be the intersection of all\n  the\n  \u03c3-algebras containing\n  F\n  , or equivalently the coarsest algebra for\n  which all sets in\n  F\n  are measurable.  (This intersection is non-vacuous,\n  since it will always involve the discrete\n  \u03c3-algebra 2^X).  In particular,\n  the open sets\n  F\n  of a topological space (\n  X,\n  F\n  ) generate a\n  \u03c3-algebra,\n  known as the\n  Borel\n  \u03c3-algebra\n  of that space.", "meta": {"post_id": 1330649, "input_score": 60, "output_score": 45, "post_title": "Difference between topology and sigma-algebra axioms."}}
{"input": "$X$ is a compact metric space, then $C(X)$ is separable,\nwhere $C(X)$ denotes the space of continuous functions on $X$.\nHow to prove it?\nAnd if $X$ is just a compact Hausdorff space, then is $C(X)$ still separable?\nOr if $X$ is just a compact (not necessarily Hausdorff) space, then\nis $C(X)$ still separable?\nPlease help me. Thanks in advance.", "output": "Theorem.  If $X$ is compact Hausdorff then $C(X)$ is separable iff $X$ is metrizable.\n\nThere is a natural embedding $x\\in X\\to \\delta _x\\in \\mathcal{M}(X)$ (more precisely in the unit ball of $\\mathcal{M}(X)$). This is an homeomorphism for the weak*-topology of $\\mathcal{M}(X)$. If $C(X)$ is separable then $(\\mathcal{M}(X), w*)$ have a compact metrizable unit ball. So $X$ is metrizable.\nFor the converse, assume $X$ is a metrizable compact Hausdorff space. Let $d$ be a metric inducing the topology and $(x_n)$ a dense countable subset of $X$. Define $d_n: x\\in X \\to d_n(x):=d(x,x_n)$. It is a continuous function. It is easy to check that the algebra generated by $1$ and $(d_n)_n$ separate the points in $X$ so by Stone Weierstrass theorem this subalgebra is dense in $C(X)$. By considering linear combination with rational coefficient of element of this subalgebra it is easy to see that $C(X)$ is separable.", "meta": {"post_id": 1331321, "input_score": 27, "output_score": 34, "post_title": "$C(X)$ is separable when $X$ is compact?"}}
{"input": "I am trying to understand when we can interchange the order of Integration and Summation. I am increasingly encountering Integrals; some of which are being solved by interchanging the order of Summation and  Integration, and some which cannot (for no given reason) be solved using this.\nDespite looking at a variety of sites, I was unable to understand when we can do so. \n$$$$\nI came up with the following two requirements here on MSE:$$$$\nIf$f_n(x)\\ge 0$ for all $x,n$\n$$\\sum \\int f_n(x) \\, dx = \\int \\sum f_n(x) \\,dx$$\nAlso if $\\sum \\int |f_n| < \\infty$ or $\\int \\sum |f_n| < \\infty$, then \n$$\\int \\sum f_n = \\sum \\int f_n$$\nI would be grateful if somebody could  please explain this to me. Thanks very much in advance.", "output": "The more general question is about interchanging limits and integration. With infinite sums, this is a special case, because by definition $\\sum_{n=1}^\\infty f_n(x) = \\lim_{N \\to \\infty} \\sum_{n=1}^N f_n(x)$. So because one can always interchange finite sums and integration, the only question is about interchanging the limit and the integration.\nWriting what I just said in symbols, we want conditions such that\n$$\\sum_{n=1}^\\infty \\int_X f_n(x) dx = \\int_X \\sum_{n=1}^\\infty f_n(x) dx.$$\nExpanding the definition:\n$$\\lim_{N \\to \\infty} \\sum_{n=1}^N \\int_X f_n(x) dx = \\int_X \\lim_{N \\to \\infty} \\sum_{n=1}^N f_n(x) dx.$$\nNow one interchange is free:\n$$\\lim_{N \\to \\infty} \\sum_{n=1}^N \\int_X f_n(x) dx = \\lim_{N \\to \\infty} \\int_X \\sum_{n=1}^N f_n(x) dx.$$\nThe issue is with the last interchange, which is what most of the rest of this answer is about.\nThe most general result of this type is the Vitali convergence theorem. It says that if $f_n$ is a sequence of measurable functions, $f_n \\to f$ pointwise, $f_n$ is uniformly integrable, and $f_n$ is tight, then $\\int_X f_n(x) dx \\to \\int_X f(x) dx.$ (Here $X$ is the set over which we integrate.) You can look up the formal definitions of \"uniformly integrable\" and \"tight\" yourself. Roughly speaking they mean that you cannot \"compress mass into a point\" and that you can't \"move mass to infinity\". These intuitions are illustrated by the failure of the conclusion of the theorem for the sequences $f_n(x)=\\begin{cases} n & x \\in [0,1/n] \\\\ 0 & \\text{otherwise} \\end{cases}$ on $[0,1]$ and $g_n(x)=\\begin{cases} 1 & x \\in [n,n+1] \\\\ 0 & \\text{otherwise} \\end{cases}$ on the whole line.\nThe Vitali convergence theorem is general but it is not convenient. The result with perhaps the best balance between generality and convenience to check is the dominated convergence theorem. This says that if $f_n \\to f$ pointwise and there is a fixed integrable function $g$ such that $|f_n(x)| \\leq g(x)$ for all $n$ and $x$, then $\\int_X f_n(x) dx \\to \\int_X f(x) dx.$\nOne relatively basic result is the monotone convergence theorem, which says that if $f_n$ is an increasing sequence of nonnegative functions and $f_n \\to f$ pointwise, then $\\int_X f_n(x) dx \\to \\int_X f(x) dx$. In particular this holds whether or not $f$ is actually integrable (if it isn't, then the limit of the integrals is $+\\infty$). This is also applicable to the case when $f_n$ are nonpositive and decrease to $f$ (this is easy to prove, since $\\int_X -g(x) dx = -\\int_X g(x) dx$). This is useful for summation, because if $f_n(x) \\geq 0$ then $g_N(x)=\\sum_{n=1}^N f_n(x)$ is an increasing sequence of nonnegative functions.\nFinally in the special case of interchanging summation and integration, one can apply the abstract version of the Fubini-Tonelli theorem. This is because summation can be identified as integration with respect to the counting measure. As a result, if either\n$$\\sum_{n=1}^\\infty \\int_X |f_n(x)| dx < \\infty$$\nor\n$$\\int_X \\sum_{n=1}^\\infty |f_n(x)| dx < \\infty$$\nthen one may interchange summation and integration. (This requires a hypothesis about $X$; because this holds for the case of $\\mathbb{R}^n$, I won't state it, since this is already a more advanced writeup than you wanted.)", "meta": {"post_id": 1334907, "input_score": 31, "output_score": 48, "post_title": "Reversing the Order of Integration and Summation"}}
{"input": "There are a lot of questions like this all over the site, but I cannot find one that resolved my confusion- what are the formal definitions of direct sums, direct products, and tensor products (in the most general sense), and how are they different?", "output": "I won't even attempt to be the most general with this answer, because I admit, I do not have a damn clue about what perverted algebraic sets admit tensor products, for example, so I will stick with vector spaces, but I am quite sure everything I say about vector spaces works for finitely generated modules over commutative rings as well. And also that the basic concept for all direct sums and tensor products are the same, just the algebraic structures involved are different.\nDirect product\nI'm pretty sure the direct product is the same as Cartesian product. If $X$ and $Y$ are two sets, then $X\\times Y$, the Cartesian product of $X$ and $Y$ is a set made up of all ordered pairs of elements of $X$ and $Y$.\nErgo, if $x\\in X$ and $y\\in Y$, then $(x,y)\\in X\\times Y$.\nDirect sum\nIf $V$ and $W$ are vector spaces over a field $\\mathbb{F}$, then their direct sum $V\\oplus W$ is the vector space whose elements are ordered pairs of elements of $V$ and $W$, equipped with the following linear structure: $$ (x_1,y_1)+(x_2,y_2)=(x_1+x_2,y_1+y_2) \\\\ \\alpha(x,y)=(\\alpha x,\\alpha y), $$ where $x,x_1,x_2\\in V,\\ y,y_1,y_2\\in W$ and $\\alpha\\in\\mathbb{F}$.\nHow is this different from the direct product? Well, the direct product can be made between arbitrary sets, and has nothing to do with algebraic properties, while the direct sum also carries over the linear structure.\nHowever, most of the time, when you take direct product of vector spaces, you assume quietly, or directly state the existence of this linear structure on the product space, and there is one other difference:\nIf you take a Cartesian product of infinite amount of vector spaces, then an element of this product space can have infinite nonzero \"components\", while if you take the direct sum of infinite amount of vector spaces, then an element of that sum will always have finite nonzero \"terms\".\nTensor product\nOh man, this is quite different.\nBasically, if you check the properties of a direct sum, you'll see that it indeed kinda does behave like a sum. Also for example, the map that maps $x\\in V$ and $y\\in W$ to $(x,y)\\in V\\oplus W$, is absolutely not a bilinear map, as evidenced by the fact that $\\alpha(x,y)\\neq(\\alpha x,y)$.\nQuestion: Would it be possible to create a third vector space from $V$ and $W$ in a way, that the \"pair forming map\" is bilinear, and thus this new space behaves kinda like a product?\nAnother, seemingly unrelated question: If we have a multilinear map from a direct sum of vector spaces, then is it possible to somehow represent this map as a linear map, and have this representation be unique?\nAlthough the two questions are seemingly unrelated, if the answer to both questions is yes, then maybe our multilinear map would appear as the composition of the \"pair forming map\" with a unique linear map?\nThese musings lead us to the actual concept of tensor product.\nDefinition:\nLet $V$ and $W$ be finite dimensional vector spaces over the field $\\mathbb{F}$. In addition, let $U$ and $X$ also be finite dimensional vector spaces over $\\mathbb{F}$.\nLet $p:V\\times W\\rightarrow X$ be a multilinear map. The pair $(X,p)$ is the tensor product of $V$ and $W$ if for every multilinear map $A:V\\times W\\rightarrow U$, there exists a unique linear map $A^\\otimes:X\\rightarrow U$ such that $$ A=A^\\otimes\\circ p. $$ --------------------\nWe usually denote $p(x,y)$ as $x\\otimes y$, and $X$ as $V\\otimes W$.\nThis also works for any amount of vector spaces, but I chose to roll with two (same goes for direct sum and direct product too).\nAfter this, it is customary to prove existence and unicity for the tensor product. Unicity in this case means that if there is another tensor product of $V$ and $W$, then the two tensor product spaces are canonically isomorphic, and the $p$ map is given by the composition of the other \"$p$ map\" and the isomorphism.\nExistence is usually given by constructing a tensor product directly. One elegant example is to assume that $U$ is $\\mathbb{F}$ (we can do that without loss of generality, since the space of multilinear maps $V\\times W\\rightarrow U$ is canonically isomorphic to the space of multilinear functionals $V\\times W\\times U^*\\rightarrow\\mathbb{F}$.), and take the tensor product space to be the dual of the space of multilinear functionals over $V\\times W$.\nA more immediately understandable construction would be to take the tensor product space to be the space of multilinear functionals over $V^*\\times W^*$, since the two are canonically isomorhpic.\nAlso as a closing remark, note that unlike the direct sum, which always consists of pairs $(x,y)$, elements of the tensor product space are generated by pairs $x\\otimes y$, but they are not all pairs.", "meta": {"post_id": 1334965, "input_score": 57, "output_score": 76, "post_title": "Direct Sum vs. Direct Product vs. Tensor Product"}}
{"input": "The title is fairly self explanatory: I have been trying to rigorously prove that $y(x)=x^{x^{x^{\\ldots}}}$ is a strictly increasing function over the interval  $[1,e^{\\frac{1}{e}})$ for a while now, primarily by exploring various manipulations using logarithms and polylogarithms but have gotten nowhere. Although it is simple enough to show that $y(\\sqrt{2})>y(1)$ and if $y'(x)>0$ for some $x \\in [1,e^{\\frac{1}{e}})$ then $y'(x)>0$ for all $x \\in [1,e^{\\frac{1}{e}})$ (since either $y$ must be strictly increasing or strictly decreasing), I am not satisfied by the rigor of this argument, although perhaps this is me being too finicky. This lack of progress has led me to explore the possibility that it is only strictly non-decreasing but this loosening of constraints has not helped at all. When it comes to proving that it is a function I've been at a loss as to where I might even begin. Any and all insights are welcome.", "output": "It is easier to prove that the inverse function is strictly increasing. Since the inverse function is just:\n$$ g(x) = \\left(\\frac{1}{x}\\right)^{-\\frac{1}{x}}$$\nwith a change of variable everything boils down to proving that $h(x)=x^x$ is increasing over $\\left[\\frac{1}{e},1\\right]$. That is trivial since:\n$$ h'(x) = h(x)\\cdot\\frac{d}{dx}\\log h(x) = (1+\\log x)\\,h(x) \\geq 0.$$", "meta": {"post_id": 1335096, "input_score": 19, "output_score": 41, "post_title": "Proof of strictly increasing nature of $y(x)=x^{x^{x^{\\ldots}}}$ on $[1,e^{\\frac{1}{e}})$?"}}
{"input": "I'm studying this paper and somewhere in the conclusion part is written:  \n\"Since this rotation of the coherency matrix is carried out based on the ensemble average of polarimetric scattering characteristics in a selected imaging window, we obtain the rotation angle as a result of second-order statistics.\" \nAlso I've seen the term ensemble average in several other papers of this context.  \nNow I want to understand the exact mathematical or statistical definition of ensemble averaging not only in this context but the exact meaning and use of ensemble averaging in statistics and mathematics.  \nI googled the term ensemble average and here in wikipedia we have the definition as \n\"In statistical mechanics, the ensemble average is defined as the mean of a quantity that is a function of the microstate of a system (the ensemble of possible states), according to the distribution of the system on its microstates in this ensemble.\" \nBut I didn't understand this definition because I don't even know what does the microstate of a system or possible states of system mean in mathematics.  \nCould you please give me a simple definition with some examples for ensemble averaging?\nCompare time averaging and ensemble averaging?\nAnd also introduce me some good resources to study more especially resources that can be helpful in image processing too?", "output": "I realize this is a late answer to this post, but it still makes the top two to three results on Google for \"ensemble average\" and an answer has not yet been officially accepted. For posterity, I figured I would try to answer it to the best of my ability in the way that the question has been phrased.\nFirst, it is important to have a broad understanding of what a stochastic process is. It is a fairly simple concept, analogous to a random variable. However, where the value of a random variable can take on certain numbers with various probabilities, the \"values\" of stochastic processes manifest as certain waveforms (again, with various probabilities). As an example in the discrete world, the outcome of a coin flip could be viewed as a random variable - it can take on two values with roughly equal probability. However, if you recorded the outcome of n coin flips (where n could be any whole number, up to infinity), and were to do so many times, you could view this \"set of n coin flips\" as a stochastic process. Results where roughly half are heads and half tails would have relatively high probabilities, while results where almost all are heads or almost all tails would have relatively low probabilities. Obviously, there are also continuous random variables and stochastic processes can be either discrete or continuous for both axes (time/trials vs. values/outcomes of each trial).\nIt is also important to understand expected value. This is even simpler - it's the value that, over a long period of time/many trials, you would expect your random variable to have. It's the mean. The average. Integrate/sum over all time/trials and divide by the amount of time/number of trials.\nNow that these two things are covered, the ensemble average of a stochastic process can be explained in simple language and mathematical terms. In the simplest sense, the ensemble average is analogous to expected value. That is, given a large number of trials, it is the \"average\" waveform that would result from a stochastic process. Note that this means that an ensemble average is a function of the same variable that the stochastic process is. Mathematically, it can be denoted as:\n$$ E[X(t)] = \\mu_X(t) = \\int_{-\\infty}^\\infty x*p_{X(t)}(x)dx $$\nwhere $p_{X(t)}$ is the PDF of $X(t)$.\nYou also mentioned the time average for a stochastic process. This is a very different thing, which itself is actually a random variable! The reason for this is that a time average of a stochastic process is simply the average value of a single outcome of a stochastic process. Note that this means that unlike the ensemble average, the time average is not a function, but a value (a number). It can be described mathematically as:\n$$ \\lim_{T\\to\\infty} \\frac{1}{2T}\\int_{-T}^T X(t)dt $$\nwhere $X(t)$ is the stochastic process in question, evaluated at time $t$.\nTo wrap up: ensemble and time averages are properties of stochastic processes, which are like random variables but take the form of waveforms. Ensemble average is analogous to expected value or mean, in that it represents a sort of \"average\" for the stochastic process. It is a function of the same variable as the stochastic process, and when evaluated at a particular value denotes the average value that the waveforms will have at that same value. Time average is more like a typical average, in that it is the average value of a single outcome of a stochastic process. It is a random variable itself, as it depends upon which outcome it is being evaluated for (and the outcome itself is random).", "meta": {"post_id": 1339012, "input_score": 24, "output_score": 43, "post_title": "what does `ensemble average` mean?"}}
{"input": "I'm having a hard time understanding the intuitive relationship between these three distributions. I thought that poisson is what you get when you sum n number of exponentially distributed variables, but if seems that gamma is the same...Could someone describe the relationship in layman's terms?", "output": "Poisson and exponential distributions are very strongly related but they're fundamentally different because the Poisson is discrete (a count variable) and the exponential is continuous (a waiting time).\nSo how are they related?\nIf the time between a certain type of event is exponentially distributed with rate $\\lambda$, then the number of events in a given time period of length $t$ follows a Poisson distribution with parameter $\\lambda t$.\nFor example, if shooting stars appear in the sky at a rate of $\\lambda$ per unit time, then the time you wait until you see your first shooting star is distributed exponentially with rate $\\lambda$. If you watch the night sky for $t$ units of time, then you could see $0, 1, 2, ...$ shooting stars. The number of shooting stars that you count in this time is a $\\text{Poisson}(\\lambda t)$ random variable.\nBut what if you ask, how long must I wait before I see $n$ shooting stars?\nThe answer is a sum of independent exponentially distributed random variables, and it follows a $\\text{gamma}(\\lambda, n)$ distribution (also sometimes called an Erlang distribution, to distinguish it from the general gamma distribution where $n$ is allowed to be a non-integer).", "meta": {"post_id": 1340158, "input_score": 25, "output_score": 48, "post_title": "What is the relationship between poisson, gamma, and exponential distribution?"}}
{"input": "(Note: I apreciate very much who marked this as a duplicate but I would like an answer for why my proof is wrong)\nThis is my solution, I have no clue why it failed. Let's start:\ndefine\n$$I_n(m) = \\int_{0}^{x} \\frac{t^m}{1 + t + t^2/2 + ... + t^n/n!}\\ dt$$\nso it should be true that\n$$\\sum_{m=0}^{n}\\frac{I_n(m)}{m!} = x$$\nThen I use Pascal inversion:\n$$\\sum_{m=0}^n \\frac{n! I_n(m)}{m!} = n!x$$\n$$\\sum_{m=0}^n {n\\choose m} B_n(m) = n!x$$\nwhere $B_n(m) = (n-m)!I_n(m)$\nby Pascal's formula:\n$$I_n(n) = (-1)^nxn! \\sum_{m=0}^{n} \\frac{(-1)^m}{(n-m)!}$$\nwhat did I do wrong ????", "output": "You may observe that\n$$\n\\left(1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}\\right)'=1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^{n-1}}{(n-1)!}\n$$ giving\n$$\n\\left(1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}\\right)-\\left(1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}\\right)'=\\frac{x^n}{n!}\n$$ and\n$$\n\\begin{align}\n\\int \\frac{x^n}{1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}}dx&=n!\\int\\frac{\\left(1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}\\right)-\\left(1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}\\right)'}{1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}}\\:dx\\\\\\\\\n&=n!\\int dx-n!\\int\\frac{\\left(1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}\\right)'}{\\left(1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}\\right)}\\:dx.\n\\end{align}\n$$ Thus \n\n$$\n\\int \\frac{x^n}{1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}}dx=n!\\:x-n!\\ln \\left| 1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}\\right|+C.\n$$", "meta": {"post_id": 1342752, "input_score": 16, "output_score": 54, "post_title": "Evaluating $\\int{ \\frac{x^n}{1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!}}}dx$ using Pascal inversion"}}
{"input": "In the first introductory chapter of his book Gravitation and cosmology: principles and applications of the general theory of relativity Steven Weinberg discusses the origin of non-euclidean geometries and the \"inner properties\" of surfaces.\nHe mentions that distances between all pairs of 4 points on a flat surface satisfy a particular relation:\n$$\\begin{align}\n0 &= d_{12}^4d_{34}^2 + d_{13}^4d_{24}^2 + d_{14}^4d_{23}^2 + d_{23}^4d_{14}^2 + d_{24}^4d_{13}^2 + d_{34}^4 d_{12}^2\\\\\n&\\phantom{{}=} + d_{12}^2 d_{23}^2 d_{31}^2 + d_{12}^2 d_{24}^2d_{41}^2 + d_{13}^2d_{34}^2d_{41}^2 + d_{23}^2d_{34}^2d_{42}^2\\\\\n&\\phantom{{}=} - d_{12}^2d_{23}^2d_{34}^2- d_{13}^2d_{32}^2d_{24}^2 - d_{12}^2d_{24}^2d_{43}^2 -   d_{14}^2d_{42}^2d_{23}^2\\\\\n&\\phantom{{}=} - d_{13}^2d_{34}^2d_{42}^2 - d_{14}^2d_{43}^2d_{32}^2 - d_{23}^2d_{31}^2d_{14}^2 - d_{21}^2d_{13}^2d_{34}^2\\\\\n&\\phantom{{}=} - d_{24}^2d_{41}^2d_{13}^2 - d_{21}^2d_{14}^2d_{43}^2 - d_{31}^2d_{12}^2d_{24}^2 - d_{32}^2d_{21}^2d_{14}^2\n\\end{align}$$\nand then presents the reader with the map of Tolkien's Middle Earth with distances between four cities indicated:\n\n$d$(Hobbiton, Erebor) = 813 mi\n$d$(Erebor, Dagorlad) = 735 mi\n$d$(Dagorlad, City of Corsairs) = 780 mi\n$d$(City of Corsairs, Hobbiton) = 1112 mi\n$d$(Hobbiton, Dagorlad) = 960 mi\n$d$(Erebor, City of Corsairs) = 1498 mi\n\nSubstituting these numbers into the rhs of the formula I got $588330312698242944 \\ \\rm{mi}^6 \\approx (915.384 \\ \\rm{mi})^6$.\nSo my questions are:\n\nIf this is correct then what is the Middle Earth: surface of a ball or a hyperboloid? Is it possible to find its radius?\n\nHow did Weinberg get this relation? He just writes that it's \"easy to show\".", "output": "Overview\n\nIs the middle-earth flat? NO.\nCan the middle-earth lies on the surface of a ball?\nYES - In fact there are two radii that work.\nHow about the surface of a hyperboloid? NO.\n\n\nPart I - Is middle-earth flat?\nThat complicated expression from Weinberg is proportional to something\ncalled Cayley Menger determinant.\n$$\\Delta_{CM}(d_{ij}) \\stackrel{def}{=} \\det\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1\\\\\n1 & 0 & d_{12}^2 & d_{13}^2 & d_{14}^2\\\\\n1 & d_{12}^2 & 0 & d_{23}^2 & d_{24}^2\\\\\n1 & d_{13}^2 & d_{23}^2 & 0 & d_{34}^2\\\\\n1 & d_{14}^2 & d_{24}^2 & d_{34}^2 & 0\n\\end{bmatrix}$$\nUsing the fact $d_{ij} = d_{ji}$, one can show that Weinberg's expression is simply $-\\frac12 \\Delta_{CM}(d_{ij})$.\nGiven any tetrahedron in $\\mathbb{R}^3$ with vertices $\\vec{x}_1, \\ldots, \\vec{x}_4$. It is known that the volume $V$ of that tetrahedron can be computed\nby following formula.\n$$288 V^2 = \\Delta_{CM}( |\\vec{x}_i - \\vec{x}_j| )\\tag{*1}$$\nConversely, if we are given a set of $6$ positive numbers $d_{ij}, 1 \\le i < j \\le 4$. It can be realized as the edge lengths of a tetrahedron when\n\nthe edge lenghts satisfy triangular inequalities. \nand the corresponding Cayley-Menger determinant $\\Delta_{CM}(d_{ij})$ is non-negative.\n(positive if we want a non-degenerate tetrahedron).\n\nFor a proof of this, please see the paper Edge lengths determining tetrahedrons by Karl Wirth and Andre S. Dreiding.\nBack to the question whether the middle-earth is flat. \nIf it is flat, then we can embed the $4$ cities congruently in $\\mathbb{R}^2$ and hence in $\\mathbb{R}^3$. The corresponding tetrahedron will be degenerate and its volume vanishes. Using $(*1)$,\nwe find the distances between the cities need to satisfy $\\Delta_{CM}( d_{ij} ) = 0$. \nHowever, if we substitute the supplied distances into the defining formula for $\\Delta_{CM}(d_{ij})$, we get a negative number! This means the middle-earth is not only non-flat, we can't realize the supplied distances as Euclidean distances in $\\mathbb{R}^3$.\n\nPart II - Can the middle-earth lies on the surface of a ball?\nThe answer is YES, there are two radii $571.164553{\\rm mi}$ and $693.660559{\\rm mi}$ that work. For these two radii, we can realize the supplied distances on a sphere of that radius.\nBefore we start, let us look at a simplified problem:\n\nGiven any $6$ numbers $\\alpha_{ij} \\in (0,\\pi)$, $0 \\le i < j \\le 3$ satisfying an appropriate set of triangular inequalities. What is the extra condition\n  one need to satisfy in order to have $4$ point $q_0,\\ldots q_3$ on the unit sphere $S^2$ such that the geodesic distance $d(q_i,q_j) = \\alpha_{ij}$ ?\n\nParametrize the unit sphere $S^2$ by polar coordinates\n$$[0,\\pi] \\times [-\\pi,\\pi) \\ni (\\theta,\\phi) \\quad\\mapsto\\quad (\\sin\\theta\\cos\\phi,\\sin\\theta\\sin\\phi,\\cos\\theta ) \\in S^2 \\subset \\mathbb{R}^3$$\nLet $i, j, k$ be any permutation of $1, 2, 3$ such that $j < k$ and define\na bunch of variables:\n$$\n\\begin{cases}\n\\theta_i &= \\alpha_{0i},\\\\\n\\psi_i   &= \\alpha_{jk}\n\\end{cases},\n\\quad\n\\begin{cases}\nb_i &= \\cos\\psi_i\\\\\nc_i &= \\cos\\theta_i,\\\\\ns_i &= \\sin\\theta_i,\\\\\n\\end{cases}\n\\quad\\text{ and }\\quad\ne_i = \\frac{b_i - c_j c_k}{s_j s_k} = \\frac{\\cos\\psi_i - \\cos\\theta_j\\cos\\theta_k}{\\sin\\theta_j\\sin\\theta_k}\n$$\nWe can fulfill the requirement on $\\alpha_{01}, \\alpha_{02}, \\alpha_{03}$ by placing \n$$q_0 \\text{ at } (0,0),\\quad\n  q_1 \\text{ at } (\\theta_1, 0 ),\\quad\n  q_2 \\text{ at } (\\theta_2, \\phi_{12} )\\quad\\text{ and }\\quad\n  q_3 \\text{ at } (\\theta_3, \\phi_{13} )\n$$\nfor some $\\phi_{12}$, $\\phi_{13}$ to be determined.\nTo fulfill the requirement of $\\alpha_{12}$ and $\\alpha_{13}$, we need\n$$\\begin{cases}\nb_3 &= \\cos\\alpha_{12} = \\cos\\theta_1\\cos\\theta_2 + \\sin\\theta_1\\sin\\theta_2\\cos\\phi_{12} = c_1 c_2 + s_1 s_2\\cos\\phi_{12}\\\\\nb_2 &= \\cos\\alpha_{13} = \\cos\\theta_1\\cos\\theta_3 + \\sin\\theta_1\\sin\\theta_3\\cos\\phi_{13} = c_1 c_3 + s_1 s_3\\cos\\phi_{13}\n\\end{cases}\n$$\nThis is equivalent to $\\begin{cases}\n\\cos\\phi_{12} &= e_3\\\\\n\\cos\\phi_{13} &= e_2\\\\\n\\end{cases}\n$ and we can do this by setting $\n\\begin{cases}\n\\phi_{12} &=  +  \\cos^{-1}e_3\\\\\n\\phi_{13} &= \\pm \\cos^{-1}e_2\n\\end{cases}\n$.\nOne may worry whether $\\phi_{12}, \\phi_{13}$ defined in this manner is well defined. It turns out when the appropriate set of triangular inequalities is satisfied, all the $|e_i| \\le 1$. So $\\phi_{12}$ is well defined and up to a sign, so does $\\phi_{13}$.\nTo fix the sign of $\\phi_{13}$ and fulfill the requirement $\\alpha_{23}$, we need\n$$b_1 = \\cos\\alpha_{23} = \\cos\\theta_2\\cos\\theta_3 + \\sin\\theta_2\\sin\\theta_3\\cos(\\phi_{12} - \\phi_{13}) = c_2 c_3 + s_2 s_3\\cos(\\phi_{12} - \\phi_{13})$$\nThis is equivalent to \n$$\\begin{align}\ne_1 \n&= \\cos(\\phi_{12} - \\phi_{13}) \n= \\cos\\phi_{12}\\cos\\phi_{13} + \\sin\\phi_{12}\\sin\\phi_{13}\\\\\n&= e_3 e_2 + \\text{sign}(\\phi_{13})\\sqrt{1-e_3^2}\\sqrt{1-e_2^2}\n\\end{align}\\tag{*2}\n$$\nThis leads to following condition on $\\alpha_{ij}$\n$$(e_1 - e_2 e_3)^2 = (1-e_3^2)(1-e_2^2) \n\\iff\n1 - e_1^2 - e_2^2 - e_3^2 + 2e_1e_2e_3 = 0\\tag{*3}$$\nWorking backwards, it is not hard to verify if $\\alpha_{ij}$ satisfies $(*3)$, we can find a sign of $\\phi_{13}$ to satisfy $(*2)$. \nWhat this means is $(*3)$ is the necessary and sufficient condition we are seeking\nfor placing the $4$ points $q_i$ on unit sphere.\nApply this to our problem of placing the 4 cities on a sphere of radius $R$.\nLet $q_0, q_1, q_2, q_3$ be the locations of\n\"Hobbiton\", \"City of Corsairs\", \"Dagorlad\" and \"Erebor\" respectively.\nWe have\n$$( d_{01}, d_{02}, d_{03}, d_{23}, d_{13}, d_{12} ) = ( 1112, 960, 813, 735, 1498, 780 )$$\nLet $\\alpha_{ij} = \\frac{d_{ij}}{R}$ and compute the value of the expression\n$$1 - e_1^2 - e_2^2 - e_3^2 + 2e_1 e_2 e_3$$\nas a function for $R \\in [\\frac{1498}{\\pi}, \\infty)$. We find this expression vanishes at two $R$. By the discussion above, we can place the 4 cites on two spheres, one for each radii.\nThe corresponding radius and sample location for the cities are:\n$$\n\\begin{cases}\nR   &\\approx 571.164553{\\rm mi}\\\\\nq_0 &= (0^\\circ,0^\\circ)\\\\\nq_1 &\\approx (111.5491^\\circ,0^\\circ),\\\\\nq_2 &\\approx ( 96.3014^\\circ,79.8187^\\circ),\\\\\nq_3 &\\approx ( 81.5553^\\circ, 152.2807^\\circ)\n\\end{cases}\n\\quad\\text{ OR }\\quad\n\\begin{cases}\nR   &\\approx 693.660559{\\rm mi}\\\\\nq_0 &= (0^\\circ,0^\\circ)\\\\\nq_1 &\\approx (91.8503^\\circ,0),\\\\\nq_2 &\\approx ( 79.2952^\\circ,63.5359^\\circ),\\\\\nq_3 &\\approx ( 67.1531^\\circ, 126.1082^\\circ).\n\\end{cases}\n$$\n\nPart III - How about the surface of a hyperboloid?\nThe answer is NO. We cannot realize the supplied distances on a hyperbolic plane,\nno matter what Gaussian curvature it has.\nLet $K = -\\frac{1}{r^2}$ be the Gaussian curvature of the hyperbolic plane.\nLet $q_0, q_1, q_2, q_4$ be any $4$ points on the hyperbolic plane.\nLet $d_{ij}$ be the distance between them and $\\displaystyle\\;\\alpha_{ij} = \\frac{d_{ij}}{r}$.\nWe can compute the angles $\\phi_{jk} = \\angle q_j q_0 q_k$ using Hyperbolic law of cosines\n$$\\cosh\\alpha_{jk}\n= \\cosh\\alpha_{0j}\\cosh\\alpha_{0k} - \\sinh\\alpha_{0j}\\sinh\\alpha_{0k} \\cos(\\phi_{jk})$$\nLet $i, j, k$ be any permutation of $1, 2, 3$ with $j < k$. If we define\n$e_1, e_2, e_3$ by\n$$e_i = \\frac{\\cosh\\alpha_{0j}\\cosh\\alpha_{0k} - \\cosh\\alpha_{jk}}{\\sinh\\alpha_{0j}\\sin\\alpha_{0k}}$$\nwe find $\\cos\\phi_{i} = e_{jk}$. Repeat essentially the same argument as the spherical case, we find $e_1, e_2, e_3$ once again satisfy:\n$$1 - e_1^2 - e_2^2 - e_3^2 + 2e_1e_2e_3 = 0$$\nHowever, if we use the supplied distances and compute the value of LHS as a function of $r$, we find LHS is non-zero for all positive $r$. This implies we cannot realized the distances on a hyperbolic plane, no matter what Gaussian curvature it has.", "meta": {"post_id": 1351403, "input_score": 46, "output_score": 37, "post_title": "Is Tolkien's Middle Earth flat?"}}
{"input": "I am a student who is preparing for IIT exam. I was just practicing calculus and encountered this problem. I tried different substitutions but none of them seemed to work. So what is the primitive function of  $$\\int \\frac{1}{x^{2n} +1} \\, \\mathrm{d}x $$ ?", "output": "We have $f(x)=\\frac{1}{x^n+1}$.  Note that we can write \n$$f(x)=\\prod_{k=1}^n(x-x_k)^{-1} \\tag {1}$$\nwhere $x_k=e^{i(2k-1)\\pi/n}$, $k=1, \\cdots,n$.  \nWe can also express $(1)$ as \n$$f(x)=\\sum_{k=1}^na_k(x-x_k)^{-1} \\tag {2}$$\nwhere $a_k=\\frac{-x_k}{n}$.\nNow, we can write\n$$\\begin{align}\n\\int\\frac{1}{x^n+1}dx=-\\frac1n\\sum_{k=1}^nx_k\\log(x-x_k)+C\n\\end{align}$$\nwhich can be more explicitly written as \n$$\\bbox[5px,border:2px solid #C0A000]{\\int\\frac{1}{x^n+1}dx=-\\frac1n\\sum_{k=1}^n\\left(\\frac12 x_{kr}\\log(x^2-2x_{kr}x+1)-x_{ki}\\arctan\\left(\\frac{x-x_{kr}}{x_{ki}}\\right)\\right)+C'}\n$$\nwhere $x_{kr}$ and $x_{ki}$ are the real and imaginary parts of $x_k$, respectively, and are given by\n$$x_{kr}=\\text{Re}\\left(x_k\\right)=\\cos \\left(\\frac{(2k-1)\\pi}{n}\\right)$$\n$$x_{ki}=\\text{Im}\\left(x_k\\right)=\\sin \\left(\\frac{(2k-1)\\pi}{n}\\right)$$\n\nNOTE 1:\nThe integral of $\\frac{1}{1+x^{2n}}$ is a special case for the development herein.  Simply let $n\\to 2n$.\n\nNOTE 2:\nAs requested, we will derive the form $a_k=-\\frac{x_k}{n}$.  To that end, we use $(2)$ and observe that\n$$\\begin{align}\n\\lim_{x\\to x_\\ell}\\left((x-x_{\\ell})\\sum_{k=1}^{n}a_k(x-x_k)^{-1}\\right)&=\\lim_{x\\to x_\\ell}\\left((x-x_{\\ell})\\frac{1}{1+x^n}\\right) \\tag 3 \n\\end{align}$$\nThe left-hand side of $(3)$ is simply $a_{\\ell}$.  For the right-hand side, straightforward application of L'Hospital's Rule yields \n$$\\begin{align}\n\\lim_{x\\to x_\\ell}\\left(\\frac{(x-x_{\\ell})}{1+x^n}\\right)&=\\frac{1}{nx_{\\ell}^{n-1}}\n\\end{align}$$\nFinally, we note that since $x_{\\ell}^n=-1$, then \n$$\\begin{align}\n\\frac{1}{nx_{\\ell}^{n-1}}&=\\frac{x_{\\ell}}{nx_{\\ell}^n}\\\\\\\\\n&=-\\frac{x_{\\ell}}{n}\n\\end{align}$$\nThus, we have that \n$$\\bbox[5px,border:2px solid #C0A000]{a_{k}=-\\frac{x_k}{n}}$$", "meta": {"post_id": 1354106, "input_score": 14, "output_score": 38, "post_title": "What is the primitive function of $\\int 1/(x^{2n} +1)dx$?"}}
{"input": "The illustration on Wolfram's page claims to present a uniquely colorable, triangle-free graph. However, this seems to be blatantly false: the graph has a symmetry with respect to a reflection through the horizontal axis, and we can use this symmetry to construct a new colouring not isomorphic to the original one.\nAm I missing something obvious here, or is the illustration simply wrong? If it's the latter, what is a simple example of a triangle-free, uniquely 3-colourable graph?", "output": "Yes, Wolfram is wrong in this case. I just checked the archives of the Journal of Combinatorial Theory (where the erratum to the paper in question is published) and the two top vertices are supposed to be connected by an edge.\nI cannot provide a link because it requires a login, and I was able to log in through my university's subscription to the journal.", "meta": {"post_id": 1356407, "input_score": 34, "output_score": 45, "post_title": "Is Wolfram wrong about unique 3-colorability, or am I just confused?"}}
{"input": "I am interested in describing the group of special orthogonal matrices $\\operatorname{SO}(n)$ by a set of parameters, in any dimension. I would also like to obtain an expression of the density of the Haar measure in this set of parameters.\nCould anyone help me on this or indicate a good reference?\nThanks", "output": "The only explicit description of the Haar measure on $SO(n)$ that I'm aware of is inductive and based on hyperspherical coordinates on the unit $(n-1)$-sphere $S^{n-1}$.  The idea is to first perform an arbitrary rotation of the first $n-1$ coordinates, and then perform a rotation that maps $\\textbf{e}_n$ to any possible location on $S^{n-1}$.\nI will describe this parameterization using explicit inductive formulas. For convenience, we will use the following notation.  If $\\textbf{v}\\in\\mathbb{R}^n$ is a vector, let $\\textbf{v}^a\\in\\mathbb{R}^{n+1}$ be the vector obtained by augmenting $\\textbf{v}$ with a zero, i.e.\n$$\n(v_1,\\ldots,v_n)^a \\;=\\; (v_1,\\ldots,v_n,0).\n$$\nSimilarly, if $M$ is an $n\\times n$ matrix, let $M^a$ be the $(n+1)\\times(n+1)$ matrix with the following block diagonal form:\n$$\nM^a \\;=\\; \\begin{bmatrix}M & \\textbf{0} \\\\ \\textbf{0}^T & 1\\end{bmatrix}.\n$$\nWe will also use the notation $\\textbf{e}_1,\\ldots,\\textbf{e}_n$ for the  standard basis vectors in $\\mathbb{R}^n$.\n\nHyperspherical Coordinates\nThese are a coordinate system for specifying a point $\\boldsymbol{\\Sigma}_n(\\theta_1,\\ldots,\\theta_n)$ on the unit $n$-sphere $S^n$ in $\\mathbb{R}^{n+1}$ given $n$ angles $\\theta_1,\\ldots,\\theta_n$.  The first few hyperspherical coordinate systems are given by\n\\begin{align*}\n\\boldsymbol{\\Sigma}_1(\\theta_1) &\\;=\\; (\\sin\\theta_1,\\,\\cos\\theta_1), \\\\[3pt]\n\\boldsymbol{\\Sigma}_2(\\theta_1,\\theta_2) &\\;=\\; (\\sin\\theta_1\\sin\\theta_2,\\,\\cos\\theta_1\\sin\\theta_2,\\,\\cos\\theta_2), \\\\[3pt]\n\\text{and }\\boldsymbol{\\Sigma}_3(\\theta_1,\\theta_2,\\theta_3) &\\;=\\; (\\sin\\theta_1\\sin\\theta_2\\sin\\theta_3,\\,\\cos\\theta_1\\sin\\theta_2\\sin\\theta_3,\\,\\cos\\theta_2\\sin\\theta_3,\\,\\cos\\theta_3).\n\\end{align*}\nand in general the $i$th Cartesian coordinate $\\Sigma_{n,i}$ of $\\boldsymbol{\\Sigma}_n$ is given by the formula\n$$\n\\Sigma_{n,i}(\\theta_1,\\ldots,\\theta_n) \\;=\\; \\begin{cases}\\sin \\theta_1 \\cdots \\sin \\theta_n & \\text{if } i=1, \\\\[3pt] \\cos \\theta_{i-1} \\sin \\theta_i \\cdots \\sin \\theta_n & \\text{if }2\\leq i \\leq n+1.\\end{cases}\n$$\nThe function $\\boldsymbol{\\Sigma}_n$ can also be defined inductively by the formula\n$$\n\\boldsymbol{\\Sigma}_n(\\theta_1,\\ldots,\\theta_n) \\;=\\; (\\sin \\theta_n)\\,\\bigl(\\boldsymbol{\\Sigma}_{n-1}(\\theta_1,\\ldots,\\theta_{n-1})\\bigr)^a \\,+\\, (\\cos \\theta_n)\\,\\textbf{e}_{n+1}.\n$$\nwith base case $\\boldsymbol{\\Sigma}_1$.\nDomain and Volume Form\nIf we let $D_n$ be the subset of the parameter space defined by\n$$\n0\\leq\\theta_1\\leq 2\\pi\n\\qquad\\text{and}\\qquad\n0\\leq \\theta_i\\leq \\pi\\;\\; \\text{ for }2\\leq i \\leq n,\n$$\nthen $\\boldsymbol{\\Sigma}_n$ maps $D_n$ onto $S^n$, and is one-to-one on the interior of $D_n$.  The $n$-dimensional volume form with respect to $\\boldsymbol{\\Sigma}_n$ is\n$$\ndV \\;=\\; \\bigl(\\sin \\theta_2\\bigr)\\bigl(\\sin^2\\theta_3\\bigr) \\cdots \\bigl(\\sin^{n-1} \\theta_n\\bigr)\\,d\\theta_1 \\cdots d\\theta_n,\n$$\nwhich comes from the fact that the partial derivatives of $\\boldsymbol{\\Sigma}_n$ are orthogonal with\n$$\n\\left\\|\\frac{\\partial \\boldsymbol{\\Sigma}_n(\\theta_1,\\ldots,\\theta_n)}{\\partial \\theta_i}\\right\\| \\;=\\; (\\sin\\theta_{i+1})\\cdots(\\sin \\theta_n)\n$$\nfor all $1\\leq i\\leq n$.\n\nAn Orthonormal Basis\nBefore writing down the parameterization of $SO(n)$, we need to extend $\\{\\boldsymbol{\\Sigma}_n(\\theta_1,\\ldots,\\theta_n)\\}$ to an orthonormal basis of $\\mathbb{R}^{n+1}$. The basis is\n$$\n\\bigl\\{\\textbf{U}_{n,1}(\\theta_1,\\ldots,\\theta_n),\\ldots,\\textbf{U}_{n,n}(\\theta_1,\\ldots,\\theta_n),\\boldsymbol{\\Sigma}_n(\\theta_1,\\ldots,\\theta_n)\\bigr\\}\n$$\nwhere\n$$\n\\textbf{U}_{n,i}(\\theta_1,\\ldots,\\theta_n) \\;=\\; \\frac{1}{(\\sin \\theta_{i+1}) \\cdots (\\sin \\theta_n)}\\frac{\\partial \\boldsymbol{\\Sigma}(\\theta_1,\\ldots,\\theta_n)}{\\partial \\theta_i}.\n$$\nThat is, $\\textbf{U}_{n,i}$ is the unit vector tangent to $S^n$ in the direction of increasing $\\theta_i$. For example, in the case of $n=2$ we have $\\boldsymbol{\\Sigma}_2(\\theta_1,\\theta_2) = (\\sin \\theta_1\\sin\\theta_2,\\cos\\theta_1 \\sin\\theta_2,\\cos\\theta_2)$, so\n$$\n\\textbf{U}_{2,1}(\\theta_1,\\theta_2) = (\\cos \\theta_1,-\\sin\\theta_1,0),\n\\qquad\n\\textbf{U}_{2,2}(\\theta_1,\\theta_2) = (\\sin \\theta_1\\cos\\theta_2,\\cos\\theta_1 \\cos\\theta_2,-\\sin\\theta_2).\n$$\nSaying these vectors are orthonormal is the same thing as saying that hyperspherical coordinates are an orthogonal coordinate system.\nThe vectors $\\textbf{U}_{n,i}(\\theta_1,\\ldots,\\theta_n)$ can also be defined inductively by the formula\n$$\n\\textbf{U}_{n,n}(\\theta_1,\\ldots,\\theta_n) \\;=\\; (\\cos \\theta_n)\\,\\bigl(\\boldsymbol{\\Sigma}_{n-1}(\\theta_1,\\ldots,\\theta_{n-1})\\bigr)^a \\,-\\, (\\sin \\theta_n)\\,\\textbf{e}_{n+1}\n$$\nand $\\textbf{U}_{n,i}(\\theta_1,\\ldots,\\theta_n) = \\bigl(\\textbf{U}_{n-1,i}(\\theta_1,\\ldots,\\theta_{n-1})\\bigr)^a$ for $i<n$.\nLet $M_{n+1}(\\theta_1,\\ldots,\\theta_n)$ denote the $(n+1)\\times (n+1)$ matrix whose columns are the vectors of this orthonormal basis:\n$$\nM_{n+1}(\\theta_1,\\ldots,\\theta_n) \\;=\\; \\begin{bmatrix}\\textbf{U}_{n,1}(\\theta_1,\\ldots,\\theta_n) & \\cdots & \\textbf{U}_{n,n}(\\theta_1,\\ldots,\\theta_n) & \\boldsymbol{\\Sigma}_n(\\theta_1,\\ldots,\\theta_n)\\end{bmatrix}.\n$$\nSo $M_n(\\theta_1,\\ldots,\\theta_{n-1})$ is an $n\\times n$ matrix in $SO(n)$ that maps $\\textbf{e}_n$ to an arbitrary point $\\boldsymbol{\\Sigma}_{n-1}(\\theta_1,\\ldots,\\theta_{n-1})$ on the unit $(n-1)$-sphere.\n\nParameterization of $SO(n)$\nOur parameterization for $SO(n)$ will be an inductively defined function $\\Phi_n$, which will take the $\\binom{n}{2}$ angles $\\{\\phi_{ij}\\}_{1\\leq i \\leq j\\leq n-1}$ as input, and output an $n\\times n$ matrix in $SO(n)$.  It is defined inductively by the rule\n$$\n\\Phi_2(\\phi_{11}) \\;=\\; \\begin{bmatrix}\\cos \\phi_{11} & \\sin \\phi_{11} \\\\ -\\sin \\phi_{11} & \\cos \\phi_{11}\\end{bmatrix}.\n$$\nand\n$$\n\\Phi_n\\bigl(\\{\\phi_{ij}\\}_{1\\leq i \\leq j\\leq n-1}\\bigr) \\;=\\; M_n(\\phi_{1,n-1},\\ldots,\\phi_{n-1,n-1})\\, \\bigl(\\Phi_{n-1}(\\{\\phi_{ij}\\}_{1\\leq i \\leq j\\leq n-2})\\bigr)^a\n$$\nwhere the product is a matrix product. Conceptually, the $\\bigl(\\Phi_{n-1}(\\{\\phi_{ij}\\}_{1\\leq i \\leq j\\leq n-2})\\bigr)^a$ factor performs an arbitrary rotation on the first $n-1$ coordinates, and then the $M_n(\\phi_{1,n-1},\\ldots,\\phi_{n-1,n-1})$ performs a specific rotation that maps $\\textbf{e}_n$ to an arbitrary point on $S^{n-1}$.\nAgain, if we let $E_n$ be the subset of parameter space defined by $0\\leq \\phi_{1j}\\leq 2\\pi$ for $1\\leq j \\leq n-1$ and $0\\leq \\phi_{ij}\\leq \\pi$ for $2\\leq i\\leq j \\leq n-1$, then $\\Phi_n$ maps $E_n$ onto $SO(n)$ and $\\Phi_n$ is one-to-one on the interior of $E_n$.\nThe volume form on $SO(n)$ corresponding to Haar measure is\n$$\ndV \\;=\\; \\left(\\prod_{1\\leq i \\leq j \\leq n-1} \\sin^{i-1} \\phi_{ij} \\right) d\\phi_{11} \\cdots d\\phi_{n-1,n-1}.\n$$\nNote that this measure isn't normalized.  Instead, the total volume of $SO(n)$ is the product\n$$\n\\prod_{i=1}^{n-1} \\mathrm{Vol}(S^i),\n$$\nwhere $\\mathrm{Vol}(S^i)$ denotes the $i$-dimensional volume (i.e. surface area) of the unit $i$-sphere in $\\mathbb{R}^{i+1}$.\n\nSome Examples\nFor $n=3$, we are parameterizing $SO(3)$ using $3$ variables $\\phi_{11},\\phi_{12},\\phi_{22}$, where $\\phi_{11},\\phi_{12}\\in[0,2\\pi]$ and $\\phi_{22}\\in[0,\\pi]$.  The parameterization $\\Phi_3(\\phi_{11},\\phi_{12},\\phi_{22})$ is given by the following matrix product\n$$\n\\begin{bmatrix}\\cos\\phi_{12}&\\sin\\phi_{12}\\cos\\phi_{22}&\\sin\\phi_{12} \\sin \\phi_{22} \\\\ -\\sin\\phi_{12}&\\cos\\phi_{12}\\cos\\phi_{22}& \\cos\\phi_{12}\\sin\\phi_{22} \\\\ 0&-\\sin\\phi_{22}& \\cos\\phi_{22}\\end{bmatrix}\n\\begin{bmatrix}\\cos \\phi_{11} & \\sin \\phi_{11} & 0 \\\\ -\\sin \\phi_{11} & \\cos \\phi_{11} & 0 \\\\ 0 & 0 & 1\\end{bmatrix}.\n$$\nThe volume form is\n$$\ndV \\;=\\; \\sin \\phi_{22} \\,d\\phi_{11}\\,d\\phi_{12}\\,d\\phi_{22},\n$$\nand the total volume of $SO(3)$ is $(2\\pi)(4\\pi) = 8\\pi^2$.\nFor $n=4$, we are parameterizing $SO(4)$ with six parameters $\\phi_{11},\\phi_{12},\\phi_{22},\\phi_{13},\\phi_{23},\\phi_{33}$, where $\\phi_{11},\\phi_{12},\\phi_{13}\\in[0,2\\pi]$ and $\\phi_{22},\\phi_{23},\\phi_{33}\\in[0,\\pi]$. The parameterization $\\Phi_4(\\phi_{11},\\phi_{12},\\phi_{22},\\phi_{13},\\phi_{23},\\phi_{33})$ is the product of the matrix\n$$\n\\begin{bmatrix}\n\\cos\\phi_{13} & \\sin\\phi_{13}\\cos\\phi_{23} & \\sin\\phi_{13}\\sin\\phi_{23}\\cos\\phi_{33} & \\sin\\phi_{13}\\sin\\phi_{23}\\sin\\phi_{33} \\\\\n-\\sin\\phi_{13} & \\cos\\phi_{13}\\cos\\phi_{23} & \\cos\\phi_{13}\\sin\\phi_{23}\\cos\\phi_{33} & \\cos\\phi_{13}\\sin\\phi_{23}\\sin\\phi_{33} \\\\\n0 & -\\sin\\phi_{23} & \\cos\\phi_{23}\\cos\\phi_{33} & \\cos\\phi_{23}\\sin\\phi_{33} \\\\\n0 & 0 & -\\sin\\phi_{33} & \\cos\\phi_{33}\n\\end{bmatrix}\n$$\nwith $\\begin{bmatrix}\\Phi_3(\\phi_{11},\\phi_{12},\\phi_{22}) & \\textbf{0} \\\\ \\textbf{0}^T & 1\\end{bmatrix}$.  The volume form is\n$$\ndV \\;=\\; \\bigl(\\sin \\phi_{22}\\bigr) \\bigl(\\sin \\phi_{23}\\bigr) \\bigl(\\sin^2 \\phi_{33}\\bigr)\\,d\\phi_{11}\\,d\\phi_{12}\\,d\\phi_{22}\\,d\\phi_{13}\\,d\\phi_{23}\\,d\\phi_{33},\n$$\nand the total volume of $SO(4)$ is $(2\\pi)(4\\pi)(2\\pi^2) = 16\\pi^4$.", "meta": {"post_id": 1364495, "input_score": 22, "output_score": 36, "post_title": "Haar measure on $\\operatorname{SO}(n)$"}}
{"input": "Given two real positive definite (and therefore, symmetric) matrices $A$ and $B$, are all the eigenvalues of $AB$ real and positive?\n\nWikipedia says $AB$ is positive definite if $A$ and $B$ are positive definite and commute, but I don't need $AB$ to be symmetric.\nBetween the lines of this question the asking user somehow prove that yes, \"the eigenvalues of $AB$  are hence real and strictly positive\" but I couldn't understand if that is confirmed in the answer.", "output": "If we call $B^{1/2}$ the symmetric matrix such that $B^{1/2}B^{1/2}=B$ (i.e. the standard square root of a positive definite matrix) then \n$$\nAB=AB^{1/2}B^{1/2}=B^{-1/2}(B^{1/2}AB^{1/2})B^{1/2},\n$$\nthat is $AB$ is similar to the positive definite matrix $B^{1/2}AB^{1/2}$, sharing all eigenvalues. It makes the eigenvalues of $AB$ be positive.", "meta": {"post_id": 1365079, "input_score": 27, "output_score": 34, "post_title": "The product of two positive definite matrices has real and positive eigenvalues?"}}
{"input": "With regard to this comment I wanted to ask (and provide an answer): what else do we need to assume and check for before we can apply L'Hospital's rule? And why do we have to do this e.g. does it really matter if we don't check them?", "output": "For the sake of the argument I'll use a question from an exam and combine two solutions (\"real solutions\" as in those were handed in by students) that will point out common mistakes when applying L'Hospital's rule. The exam was taken by students in their first semester of university.\n\nQuestion: Determine the following limit (if it exists): $$\\lim\\limits_{x\\to\\infty}\\frac{x-\\sin(x)}{x+\\sin(x)}$$\n\nWrong solution: we have $\\lim\\limits_{x\\to\\infty}(x-\\sin(x))=\\lim\\limits_{x\\to\\infty}(x+\\sin(x))=\\infty$, thus by applying L'Hospital's rule we get: $$\\lim\\limits_{x\\to\\infty} \\frac{x-\\sin(x)}{x+\\sin(x)}=\\lim\\limits_{x\\to\\infty}\\frac{1-\\cos(x)}{1+\\cos(x)}.$$ For this limit we get an indeterminate form as well, so applying L'Hospital's rule again yields $$\\lim\\limits_{x\\to\\infty} \\frac{x-\\sin(x)}{x+\\sin(x)}=\\lim\\limits_{x\\to\\infty}\\frac{1-\\cos(x)}{1+\\cos(x)}=\\lim\\limits_{x\\to\\infty}\\frac{\\sin(x)}{-\\sin(x)}=-1.$$\n\nThe solution $\\lim\\limits_{x\\to\\infty}\\frac{x-\\sin(x)}{x+\\sin(x)}=-1$ and the solution process are completely wrong. First of all one can show, that $$\\frac{x-\\sin(x)}{x+\\sin(x)}\\geq 0$$ for all $x\\in\\mathbb R\\setminus\\{0\\}$; thus the limit (if it exists) can't be negative. For analyzing the mistakes, we first need to look up, what L'Hospital's rule really says:\n\nLet $a,b\\in\\mathbb R\\cup\\{-\\infty,\\infty\\},a<b$. Let $f:(a,b)\\rightarrow\\mathbb R$ and $g:(a,b)\\rightarrow\\mathbb R$ be differentiable with $g'(x)\\neq 0$ for all $x\\in(a,b)$. Then the following holds:\nIf $\\lim\\limits_{x\\uparrow b}f(x)=\\lim\\limits_{x\\uparrow b}g(x)=A\\in\\{0,-\\infty,\\infty\\}$ and $\\lim\\limits_{x\\uparrow b} \\frac{f'(x)}{g'(x)}=L\\in\\mathbb R\\cup\\{-\\infty,\\infty\\}$ exists, then $\\lim\\limits_{x\\uparrow b} \\frac{f(x)}{g(x)}=L$.\nA similar argument holds for $\\lim\\limits_{x\\downarrow a}\\frac{f(x)}{g(x)}$.\n\nThe requirements of $f,g$ being differentiable functions etc. were never checked in the solution. Now let's look at the applications of L'Hospital's rule.\nFirst application: $\\displaystyle\\lim\\limits_{x\\to\\infty} \\frac{x-\\sin(x)}{x+\\sin(x)}=\\lim\\limits_{x\\to\\infty} \\frac{1-\\cos(x)}{1+\\cos(x)}$\nWe check if we can apply L'Hospital's rule. First of all we need to define $$f:(0,\\infty)\\rightarrow\\mathbb R,x\\mapsto x-\\sin(x)~\\text{and}~g:(0,\\infty)\\rightarrow\\mathbb R,x\\mapsto x+\\sin(x).$$ Then $f$ and $g$ are differentiable and $\\displaystyle\\frac{f(x)}{g(x)}$ is well-defined as $g(x)\\neq 0$ for $x\\in(0,\\infty)$.\nWe also have $\\lim\\limits_{x\\to\\infty}(x-\\sin(x))=\\lim\\limits_{x\\to\\infty}(x+\\sin(x))=\\infty$. But we must not apply L'Hospital's rule, as we don't have $g'(x)\\neq 0$ for $x\\in (0,\\infty)$:\nwith $g'(x)=1+\\cos(x)$ we have $$g'(x)=0 \\Leftrightarrow \\cos(x)=-1 \\Leftrightarrow x=\\pi+2\\pi k,k\\in\\mathbb Z.$$ As $\\pi\\in (0,\\infty)$, we don't have $g'(x)\\neq 0$ for all $x\\in(0,\\infty)$. \nOne might now try to shift the lower endpoint of the interval $(0,\\infty)$ up; as we're looking at $x\\to\\infty$, the lower endpoint of the interval doesn't really matter. So let's define $f,g:(4,\\infty)\\rightarrow\\mathbb R$, this way we remove $\\pi$ from the domain of $g$ and $g'$. But we still get $g'(3\\pi)=0$ with $3\\pi\\in (4,\\infty)$. No matter on which interval $(a,\\infty)$ we define $f$ and $g$, we will always find $k\\in\\mathbb Z$ with $\\pi+2\\pi k>a$ and therefore $g'(\\pi+2\\pi k)=0$. Thus the first application of L'Hospital's rule in the solution is wrong.\nSecond application: $\\displaystyle \\lim\\limits_{x\\to\\infty}\\frac{1-\\cos(x)}{1+\\cos(x)}=\\lim\\limits_{x\\to\\infty}\\frac{\\sin(x)}{-\\sin(x)}$\nFirst of all we'd need to define $$f:(a,\\infty)\\rightarrow \\mathbb R,x\\mapsto 1-\\cos(x)~\\text{and}~g:(a,\\infty)\\rightarrow\\mathbb R,x\\mapsto 1+\\cos(x).$$ Then with $g'(x)=-\\sin(x)$ we'd have the same problem as in the first application: $$g'(x)=0\\Leftrightarrow x=\\pi k,k\\in\\mathbb Z.$$ But there is another mistake, that (at least in my experience) often happens when trying to apply L'Hospital's rule.\nThe limit $\\lim\\limits_{x\\to\\infty} \\frac{1-\\cos(x)}{1+\\cos(x)}$ is not of the indeterminate form $\\frac{A}{A}$ with $A\\in\\{0,\\pm\\infty\\}$. For the application of L'Hospital's rule this is a necessary condition. \nYes, the limit $\\lim\\limits_{x\\to\\infty} \\frac{1-\\cos(x)}{1+\\cos(x)}$ is of some indeterminate form, but it doesn't fit L'Hospital's rule. So just because a limit is of an indeterminate form, one can't simply apply L'Hospital's rule.\n\nL'Hospital's rule does need more than just $\\frac{0}{0}$ or $\\frac{\\infty}{\\infty}$ to be applicable and it is important to at least think about the requirements when using it. Often there are other ways to get to the right answer without using L'Hospital's rule which in my opinion should be considered first.\nAs a last point, one right solution to calculate the limit:\nFor $x\\neq 0$ we have: $$\\frac{x-\\sin(x)}{x+\\sin(x)}=\\frac{x\\left(1-\\frac{\\sin(x)}{x}\\right)}{x\\left(1+\\frac{\\sin(x)}{x}\\right)}=\\frac{1-\\frac{\\sin(x)}{x}}{1+\\frac{\\sin(x)}{x}}.$$ Because $|\\sin(x)|\\leq 1$ we have $-\\frac{1}{x}\\leq \\frac{\\sin(x)}{x}\\leq \\frac{1}{x}$. With $\\lim\\limits_{x\\to\\infty} \\frac{1}{x}=0$ and the squeeze theorem we get $\\lim\\limits_{x\\to\\infty} \\frac{\\sin(x)}{x}=0$ and thus we have $$\\lim\\limits \\frac{x-\\sin(x)}{x+\\sin(x)}=\\lim\\limits_{x\\to\\infty} \\frac{1-\\frac{\\sin(x)}{x}}{1+\\frac{\\sin(x)}{x}}=1.$$\nEdit: as asked by S.Panja-1729 in the comments, one way to show $\\lim\\limits_{x\\to\\infty} x+\\sin(x)=\\infty$.\nWith $|\\sin(x)|\\leq 1$ we have $x+\\sin(x)\\geq x-1$. As $\\lim\\limits_{x\\to\\infty} x-1=\\infty$ we can conclude $\\lim\\limits_{x\\to\\infty} x+\\sin(x)=\\infty$. A similar argument holds for $\\lim\\limits_{x\\to\\infty} x-\\sin(x)=\\infty$.\nTo point out another mistake when using L'Hospital's rule (as suggested by Andrew D. Hwang in the comments):\n\nQuestion: Determine the following limit (it it exists): $$\\lim_{x \\to \\infty} \\frac{x - \\frac{1}{2}\\sin(x)}{x + \\frac{1}{2} \\sin(x)}$$\n\nWrong solution: we have $\\lim\\limits_{x\\to\\infty} \\left( x-\\frac{1}{2}\\sin(x) \\right)=\\lim\\limits_{x\\to\\infty} \\left(x+\\frac{1}{2}\\sin(x) \\right)=\\infty$, thus by applying L'Hospital's rule we get\n$$\\lim_{x \\to \\infty} \\frac{x - \\frac{1}{2}\\sin(x)}{x + \\frac{1}{2} \\sin(x)}=\\lim\\limits_{x\\to\\infty}\\frac{1-\\frac{1}{2}\\cos(x)}{1+\\frac{1}{2}\\cos(x)}.$$ As $$\\lim\\limits_{x\\to\\infty}\\frac{1-\\frac{1}{2}\\cos(x)}{1+\\frac{1}{2}\\cos(x)}$$ doesn't exist, we can conclude that $$\\lim_{x \\to \\infty} \\frac{x - \\frac{1}{2}\\sin(x)}{x + \\frac{1}{2} \\sin(x)}$$ doesn't exist.\n\nFor analyzing the mistake, we again check if we can apply L'Hospital's rule.\nLet $$f:(0,\\infty)\\rightarrow \\mathbb R,f(x)=x - \\frac{1}{2}\\sin(x),~g:(0,\\infty)\\rightarrow\\mathbb R,g(x)=x + \\frac{1}{2} \\sin(x).$$ Then $f$ and $g$ are differentiable and $\\displaystyle\\frac{f(x)}{g(x)}$ is well-defined, as $g(x)\\neq 0$ for $x\\in(0,\\infty)$. Futhermore, with $g'(x)=1+\\frac{1}{2}\\sin(x)$ we have $g'(x)\\neq 0$ for $x\\in (0,\\infty)$, thus $\\displaystyle \\frac{f'(x)}{g'(x)}$ is well-defined. But we still can't apply L'Hospital's rule as the last requirement is not fulfilled.\nFor applying L'Hospital's rule it is necessary, that the limit $$\\lim\\limits_{x\\uparrow b} \\frac{f'(x)}{g'(x)}$$ exists either as a real number, meaning that $\\displaystyle\\frac{f'(x)}{g'(x)}$ converges to $L\\in\\mathbb R$ as $x$ approaches $b$, or the limit exists as $\\pm\\infty$, meaning that $\\displaystyle\\frac{f'(x)}{g'(x)}$ diverges (strictly) to $\\infty$ or $-\\infty$ as $x$ approaches $b$. Only in this case can we identify the limit $$\\lim\\limits_{x\\uparrow b}\\frac{f'(x)}{g'(x)}$$ with $$\\lim\\limits_{x\\uparrow b}\\frac{f(x)}{g(x)}.$$\nIn our question we get $$\\lim\\limits_{x\\to\\infty}\\frac{1-\\frac{1}{2}\\cos(x)}{1+\\frac{1}{2}\\cos(x)}$$ which neither converges to $L\\in\\mathbb R$ nor does it (strictly) diverge to $\\infty$ or $-\\infty$. Thus we can't apply L'Hospital's rule.\nTo get the correct answer, we can use the same argument we have used for the first question and get: $$\\lim\\limits_{x\\to\\infty}\\frac{x-\\frac{1}{2}\\sin(x)}{x+\\frac{1}{2}\\sin(x)}=\\lim\\limits_{x\\to\\infty}\\frac {1-\\frac{1}{2}\\cdot \\frac{\\sin(x)}{x}}{1+\\frac{1}{2}\\cdot\\frac{\\sin(x)}{x}}=1.$$\n\nQuestions, comments, corrections etc. are appreciated.", "meta": {"post_id": 1366382, "input_score": 18, "output_score": 38, "post_title": "Why do we need to check for more than $\\frac{\\infty}{\\infty}$ or $\\frac{0}{0}$ when applying L'Hospital?"}}
{"input": "I know that derivative is the slope of the tangent line, and that integral is the area under the curve. \nMy question is that how these two distinct concepts are geometrically related? What is the relation between the slope of the tangent line and the area under the curve? \nIf these are inverse of each other, then there should be a relation between them, I believe.", "output": "The magic word is \"rate of change\". The slope is the rate of change, so the slope is simply \"how much the function grows when you move right\". So if you plot the integral curve (which geometrically can be interpreted as an area under some other curve), then the rate of change is \"how much the area grows when you expand it to the right\", which is exactly the value of the original function.\nSketch:\nOn the left, you have a curve of the original function $f(x)$, and the approximate area under it, shaded as red rectangles. The curve of the integral $\\int f(x)dx$ is obtained by \"adding\" the areas together (right figure, the $y$ coordinate measures the total area of the curve in the left figure). The derivative of the right curve is the slope (dashed lines across the rectangles), and obviously, the slope of a rectangle with unit width is exactly its height, and the height of the rectangle brings you back to the left figure and the value $f(x)$.", "meta": {"post_id": 1368425, "input_score": 36, "output_score": 85, "post_title": "How is the derivative geometrically inverse of integral?"}}
{"input": "I'd like to think that I understand symmetry groups. I know what the elements of a symmetry group are - they are transformations that preserve an object or its relevant features - and I know what the group operation is - composition of transformations. Given a polyhedron or wallpaper tiling or whatever, I could probably start spotting the symmetries, which would entail listing out elements of the symmetry group, and then I could start filling in the multiplication table.\nPenrose attaches a group to impossible figures to capture their inherent ambiguity, and I'd like to grok these groups like I do symmetry groups. Take a prototypical example, the tribar:\n\nHe names the \"ambiguity group\" $G=\\Bbb R^+$ (positive numbers under multiplication) to describe possible distances of points. We can split the figure up into three components, as above, and interpret them as being disconnected from each other in three-space but from our perspective they seem to make a single figure. For convenience, I think we should let $A_{ij}$ denote points on the figures as well as represent their distances from the origin, interchangeably.\nOne can define the relative distances by $d_{ij}=A_{ij}/A_{ji}$. Since $d_{ji}=d_{ij}^{-1}$, there are only three relevant proportions: $d_{12}$, $d_{23}$, and $d_{31}$. According to Penrose, the $d_{ij}$s do not actually depend on our choice of overlapping points $A_{ij}$, but this seems wrong to me: varying the points $A_{ij}$ through the overlap regions will change them linearly and so any ratio $d_{ij}$ will only remain invariant if $d_{ij}=1$ to begin with. But probably this quibble is unimportant.\nOne can scale the distances the components $Q_1,Q_2,Q_3$ are from the origin without affecting our perception of them. (Perhaps consider our \"perception\" of them to be their radial projection onto the unit sphere, or something.) The effect of scaling one of these $Q_i$ by a factor of $\\lambda$ on the $d_{12},d_{23},d_{31}$ is to scale one of them by $\\lambda$, a second by $\\lambda^{-1}$, and leave the third unchanged.\nIf the $Q_1,Q_2,Q_3$ were compatible and could be combined into a single figure, then such a configuration would have $(d_{12},d_{23},d_{31})=(1,1,1)$. If they were compatible but the components were separated by independent scalings $q_1,q_2,q_3$ (respectively) then we'd have\n$$(d_{12},d_{23},d_{31})=\\left(\\frac{q_1}{q_2},\\frac{q_2}{q_3},\\frac{q_3}{q_1}\\right). \\tag{1}$$\nNote that $\\tau=d_{12}d_{23}d_{31}$ is an invariant, in the sense that scaling the components independently does not change the value of $\\tau$. The compatibility situation $(1)$ occurs precisely when $\\tau=1$.\nPenrose defines the group $H$ to be the tuples $(d_{12},d_{23},d_{31})\\in(\\Bbb R^+)^3$ modulo the rescalings by $\\lambda$ and modulo the elements of the form $(1)$. As I understand it, the invariant $(d_{12},d_{23},d_{31})\\mapsto\\tau$ is a bijection $H\\to\\Bbb R^+$. But now here are my questions.\n$\\hskip 1.4in$ \n$\\sf \\color{Fuchsia}{(A)}$ How do we know what the ambiguity group is? The tribar's ambiguity group is $G=\\Bbb R^+$. With the Necker cubes above, Penrose says the ambiguity group is $G=\\Bbb Z_2$. Is the ambiguity group meant to parametrize the possible positions of the individual pieces of the figure? Where does the group operation of $G$ actually come into play?\n$\\sf \\color{Fuchsia}{(B)}$ How do we know what pieces to cut a figure up into? Since the tribar has obvious threefold symmetry, that kind of inspires the choice of three pieces. But it seems that with the congruence relation used to define $H$, we could choose to put any two of those pieces together into one component and fix its position, only letting the last component vary (which would be one degree of freedom, exactly as $H\\cong\\Bbb R^+$ predicts). So we could have cut into two pieces. Or we ould cut into six pieces, or any number of pieces. Will it never matter how many pieces we choose? Why would we break apart the tribar's corners but not break apart the faces of the Necker cubes? What figures would we cut into pieces, and what figures would we do something else to? And with the latter figures, what would we do to find their $H$ group?\n$\\sf \\color{Fuchsia}{(C)}$ What are the group elements and what is the group operation? As I mentioned with symmetry groups, it's intuitive what their elements are and what the operation is. But what about with $H$? It seems the elements are physically realizable configurations consistent with our perception, modulo altering the configuration in a way that wouldn't change our perception. The identity element would be the configurations in which our perception is actually correct and sensible. And the group operation seems to be ... I don't know. Presumably we could use componentwise multiplication of the representative tuples $(d_{12},d_{23},d_{31})$, or equivalently multiplication of the invariants $\\tau$, which would make $H\\to\\Bbb R^+$ a group isomorphism, but how would this operation be meaningful or relevant?\n$\\sf \\color{Fuchsia}{(D)}$ Why is this called a cohomology group? Yes, our $H^1(Q,G)$s are being called cohomology groups. I deliberately put off using that word as long as possible. (And so you've read this far. Suckers.) But in what sense are these groups cohomological? Are there higher cohomology groups $H^n(Q,G)$ and coboundary operators? Is this cohomology dual to some kind of homology of impossible figures? Probably I will be unable to understand answers to this question, as I don't really know what cohomology is in the first place. No time like the present?\nUltimately, I'd like to be able to look at an impossible figure and systematically derive its cohomology group, just like I can derive a figure's symmetry group. Or alternately, create impossible figures with given cohomology group. But perhaps the analogy isn't tenable, as cohomology groups aren't really symmetry groups at all.\nSource: On the Cohomology of Impossible Figures.", "output": "To answer $\\sf \\color{Fuchsia}{(A)}$, one has to understand the meaning of the ambiguity group clearly first - indeed, your interpretation of the group is not entirely correct.\nThe punchline here is that the ambiguity group measures ambiguity, not quite impossibility (which is the job of the cohomology group defined later), of the given figure. For example, consider the tribar, and piece it up into $Q_1, Q_2, Q_3$, as done by Penrose. If you stretch $Q_1$ by a factor of $\\lambda > 0$, shrink $Q_2$ by a factor of $\\lambda$ (that is to say, stretch by $\\lambda^{-1}$), and leave $Q_3$ as it is, the resulting object is really the same tribar, which is what Penrose was trying to convey through the following picture from the paper.\n\nThat is to say, modifying $Q_i$'s in this way gives you a symmetry of $X$. And you have such a symmetry for each $\\lambda > 0$. Of course, collection of such symmetries $G$ is a group: consider one symmetry which stretches $Q_1$ by $\\lambda$, shrinks $Q_2$ by $\\lambda$ and leaves $Q_3$ as is and another one which does the same thing with $\\lambda'$ this time instead. You can of course compose them, and the resulting symmetry stretches $Q_1$ by $\\lambda \\lambda'$, shrinks $Q_2$ by $\\lambda \\lambda'$, and leaves $Q_3$ invariant. This is the ambiguity group. This partially answers $\\sf \\color{Fuchsia}{(C)}$. Note that the origin of the name comes from the fact that the tribar is ambiguous in the sense that you can have different $Q_1, Q_2, Q_3$'s matching up to produce the same tribar. You can never tell the length of a bar in the tribar - it'll depend on the point of view you're seeing it from. And the \"group of symmetries\" $G$ captures all the ambiguity there is. \nIt is now clear that there is a bijection $G \\to \\Bbb R^+$, sending a symmetry to the corresponding $\\lambda > 0$ in $\\Bbb R^+$ (group under multiplication), which is in fact a homomorphism. Thus, $G \\cong \\Bbb R^+$.\nThat said, given the Necker cubes, the ambiguity is in the vertices. Pick any vertex $v$ from the figure. There are two point of views here - $v$ can either point upwards, or point downwards. If $v$ points upwards, then so does all the other vertices, and similar for the other choice. So two point of views, two choices - hence, the ambiguity group is $\\Bbb Z/2\\Bbb Z$. More formally, the symmetry is obtained from reflecting the whole figure (imagined as sitting in $\\Bbb R^3$) along an appropriate hyperplane. Obviously, the symmetry group then is the cyclic group of order 2. \nTo answer $\\sf \\color{Fuchsia}{(D)}$. Yes, this is indeed a variant of \u010cech cohomology. \nFirst, let me explain what cohomology really is. If $X$ is a sufficiently nice topological space, you can always set up a homeomorphism $X \\cong T$ of $X$ with a simplicial complex $T$. Such a homeomorphism is called a triangulation of $X$. Note that there need not be a unique such homeomorphism, so there may be lots of ways to triangulate a given nice topological space.\nDeclare $\\Delta_n(T)$ to be the free abelian group generated by $n$-simplices of $T$. Let $\\Delta^n(T)$ be $\\hom(\\Delta_n(T), \\Bbb Z)$, the dual of $\\Delta_n(T)$. Elements of $\\Delta^n(T)$ are called $n$-cochains. There is a natural map $\\partial : \\Delta^{n-1}(T) \\to \\Delta^n(T)$ taking an $n$-cochain $\\psi : \\Delta_{n-1}(T) \\to \\Bbb Z$ to the $n$-cochain $\\psi \\circ \\text{bd} : \\Delta^n(T) \\to \\Delta^{n-1}(T) \\to \\Bbb Z$ where $\\text{bd}$ sends an $n$-simplex in $T$ to sum of signed faces of that $n$-simplex (signs chosen appropriately to take care of orientation). $\\partial$ is called the coboundary map. One can check that the map $\\partial \\partial : \\Delta^{n-1}(T) \\to \\Delta^n(T) \\to \\Delta^{n+1}(T)$ is the zero map. The sequence of maps \n$$\\cdots \\to \\Delta^{n-1}(T) \\to \\Delta^n(T) \\to \\Delta^{n+1}(T) \\to \\cdots$$\nis then a cochain complex, that is, going two steps in the sequences lands you into $0$. As a consequence of $\\partial \\partial = 0$, we have that $\\text{im} \\partial \\subset \\ker \\partial$. Elements of $\\text{im} \\partial$ are called coboundaries, and elements of $\\ker \\partial$ are called cocycles. As everything is abelian, $\\text{im} \\partial$ is also normal in $\\ker \\partial$. We set $H^n(X; \\Bbb Z) := \\ker \\partial/\\text{im} \\partial$. This is called the simplicial cohomology of $X$, and is indeed independent of the triangulation $T$ we chose. It's a topological invariant of $X$. We'll see something similar happening in Penrose's paper.\nIf $X$ is the given ambiguous figure, let $\\{U_i\\}$ be a good cover of $X$. That is, $X = \\bigcup U_i$ and each of $U_i$, $U_i \\cap U_j$, $U_i \\cap U_j \\cap U_k$ and further intersections, are homeomorphic to balls $B^n$ (a more general condition would be that they are all contractible). Let each intersection $U_{i_1} \\cap \\cdots \\cap U_{i_n}$ be denoted as $U_{i_1 \\cdots i_n}$. Treat these sets as symbols (i.e., formal intersection), and distinguish $U_{ij}$ from $U_{ji}$ even though they are the same set. \n$G$ be the ambiguity group of $X$. Define an $n$-dimensional cochain to be a function $\\varphi^n : \\{U_{i_1 \\cdots i_n}\\}  \\to G$ such that $\\varphi^n(U_{i_1 \\cdots i_k \\cdots i_l \\cdots i_n}) = \\varphi^n(U_{i_1 \\cdots i_l \\cdots i_k \\cdots i_n})^{-1}$ holds for any $k, l$, where $\\{U_{i_1\\cdots i_n}\\}$ is the collection of all possible intersection of $n+1$ many sets from the cover. When $n=0$, it's just an assignment of a an element of $G$ to each $U_i$. \nLet $C_n(X)$ denote the group of all $n$-dimensional cochains, where we multiply two cochains by multiplying their values in $G$. Define the coboundary map $\\partial : C_{n-1}(X) \\to C_{n}(X)$ given by sending $\\varphi^{n-1}$ to the $n$-cochain $\\varphi^n$ assigning to each $U_{i_i \\cdots i_n}$ the number $\\prod_j \\varphi^{n-1}(U_{i_1 \\cdots \\hat{i_j} \\cdots i_n})^{\\varepsilon_j}$ where $\\varepsilon_j = (-1)^n$ if $j$ is even and $(-1)^{n+1}$ if $j$ is odd, and $\\hat{i_j}$ means that term is missing from the indices. So for example, in the tribar example, assigning to each $Q_i$ the number $q_i$ is a $0$-cochain and applying the coboundary map one gets an assignment of each (oriented) intersection $Q_{ij} = Q_i \\cap Q_j$ to the number $q_i/q_j$, a $1$-cochain in our language.\nThe final thing to check is that $\\partial \\partial : C_{n-1}(X) \\to C_{n}(X) \\to C_{n+1}(X)$ is the zero map. This is a rather tedious and technical thing to check, but if the readers try verifying it, they will understand the motivation behind the rather bizarre definition of $\\partial$ above. To sum up, we have that\n$$\\cdots \\to C_{n-1}(X) \\stackrel{\\partial}{\\to} C_n(X) \\stackrel{\\partial}{\\to} C_{n+1}(X) \\to \\cdots$$\nis a cochain complex. We can thus take it's cohomology $H^n(X; G) := \\ker \\partial/\\text{im}\\partial$ (the $G$ is there to indicate that the cochains are $G$-valued. Indeed, we could have put any group to be the coefficient group there, but it seems Penrose finds it natural to use the ambiguity group of $X$). For $n = 1$, this is precisely the $1$st cohomology group of Penrose. \nLet us compute the cohomology group of the tribar. $H^1(X;\\Bbb R^+)$ is the group of $1$-cocycles modulo $1$-coboundaries. As there are no nontrivial $2$-cochains, every $1$-cochain is a cocycle. That is to say, we're computing $1$st cohomology of the two-term cochain $$0 \\to C_0(X) \\stackrel{\\partial}{\\to} C_1(X) \\to 0$$ which is just $C_1(X)/\\text{im} \\partial$. Note that this has a slight similarity with your interpretation (the definition of $H$), but is not quite the same. In any case, I can find a $1$-cocycle in $C_1(X)$ which is not a coboundary using the clever technique devised by Penrose: the $1$-cochain $\\psi^1$ sending $Q_{ij} = Q_i \\cap Q_j$ to $d_{ij} = A_i/A_j$ is independent of the choice of $A_k$'s, and I claim that it is a nontrivial cochain (i.e., is not a boundary). Well, if it was a boundary, then the equality $d_{ij} = \\psi^1(Q_{ij}) = \\psi^0(Q_i)/\\psi^0(Q_j)$ would hold for some $0$-cochain $\\psi^0$. But then we'd have $\\tau(\\psi^1) = d_{12}d_{23}d_{31} = 1$.\nHowever, if we choose the placement of tribar appropriately so that $A_{12}$ and $A_{21}$ has the same distance from origin (i.e., $d_{12} = 1$) and $A_{23}$ is farther away from origin than $A_{32}$ (so that $d_{23} > 1$, then $A_{13}$ would automatically be closer than $A_{31}$ (that is, $d_{31} = d_{13}^{-1} > 1$), which implies $\\tau(\\psi^1)$ is strictly greater than $1$, forcing $\\psi^1$ to be nontrivial. Once we have this nontrivial cochain, we can get $|\\Bbb R^+|$-many nontrivial cochains by multiplying $\\psi^1(Q_{12})$ by $1$, $\\psi^1(Q_{23})$ by $\\lambda$, and $\\psi^1(Q_{31})$ by $1/\\lambda$ for each $\\lambda > 0$. This gives a natural identification with $G$. I think with some work you can show that these are all the nontrivial $1$-cocycles there are, and conclude $H^1(X; \\Bbb R^+) \\cong \\Bbb R^+$. In general, it need not be true that $H^1(Q; G)$ is the same as the ambiguity group $G$, but the $1$-st cohomology group $H^1(Q; G)$ would be a module over the ambiguity group $G$.\nI picked the open cover $\\{U_i\\}$ of $X$ to be good, i.e., all sets and all intersections are contractible. The reason is that by the nerve theorem, $X$ would have the homotopy type of the nerve $\\mathcal{N}$ of $\\{U_i\\}$. \u010cech cohomology just computes cohomology of the nerve $\\mathcal{N}$, but since this is homotopy equivalent to $X$, that is the same as taking cohomology of $X$, so it's independent of whatever good cover we chose. I think something similar must be at work for these ambiguous/impossible figures. This probably answers $\\sf \\color{Fuchsia}{(B)}$, but only Penrose knows what he had in mind.\nOne more thing I haven't addressed yet.\n\nUltimately, I'd like to be able to look at an impossible figure and systematically derive its cohomology group, just like I can derive a figure's symmetry group. Or alternately, create impossible figures with given cohomology group. But perhaps the analogy isn't tenable, as cohomology groups aren't really symmetry groups at all.\n\nThe first part of this answers gives a rough overview of how you could recognize the ambiguity group from a given ambiguous picture, so I presmume I have answered that bit. I do not know what a rigorous definition of the ambiguity group might be, but I presume it's some sort of inherent symmetry group of the chosen cover $\\{U_i\\}$. Also, certainly the analogy is tenable: if you read my answer to $\\sf \\color{Fuchsia}{(A)}$, you'd see that this ambiguity group is indeed some sort of symmetry groups which captures symmetries \"induced from ambiguity\". However, I do not know if you can construct an ambiguous figure such that the ambiguity group $G$ is isomorphic to a given group. Here's some food for thought: Assume $G$ is a finitely presented abelian group. Pick generators $x_1, \\cdots, x_n$ of $G$. $F$ be the free abelian group generated by these. If $Y$ is some ambiguous figure with infinite cycle ambiguity group, then disjoint union of $n$ copies of $Y$ gives you a figure with ambiguity group isomorphic to $F$. Now given relators $r_i = 1$ for $G$, try identifying the copies of $Y$ appropriately to kill the symmetries corresponding to $r_i$. Can this be done?\nPS: I wanted to add an explicit computation of the Penrose cohomology $H^1(X; \\Bbb Z/2)$ of the Necker cube as a summary, but I'd need some pictures for that and I'm too ashamed of my drawing skills to add it. Maybe if OP (and others reading this) are really interested.", "meta": {"post_id": 1372980, "input_score": 62, "output_score": 37, "post_title": "Penrose's remark on impossible figures"}}
{"input": "One time I was bored and played around a bit with integrals and wolfram alpha and tested the following integral: \nhttp://www.wolframalpha.com/input/?i=integral_0%5E1+ceil%28x*sin%281%2Fx%29%29\nNote: The result at my laptop shows: $ \\int_0^1 \\lceil { x\\sin({1 \\over x})} \\rceil  \\,dx  = 1 - \\frac{\\log(4)}{2\\pi} $\nI was a bit surprised seeing this and wondered, if this result makes any sense and if yes, if there were an explanation, where $\\frac{\\log(4)}{2\\pi}$ exactly comes from. The numbers look too neat to be random, so there might be a way to derive those values directly without using the help of a calculator etc. \nAs always: Thanks in advance for any constructive answer/comment.", "output": "The ceiling function gives one iff $\\sin(1/x)> 0$, otherwise zero. So the integral is just a sum of interval lengths where the sin is positive, i.e. \n$$\n\\left(1-\\frac{1}{\\pi}\\right)+\\left(\\frac{1}{2\\pi}-\\frac{1}{3\\pi}\\right)+\\left(\\frac{1}{4\\pi}-\\frac{1}{5\\pi}\\right)+\\ldots=1-\\frac{1}{\\pi}\\left(1-\\frac12+\\frac13-\\frac14+\\ldots\\right)=\\\\\n=1-\\frac{1}{\\pi}\\ln(1+1)=1-\\frac{\\ln(2)}{\\pi}.\n$$", "meta": {"post_id": 1377921, "input_score": 45, "output_score": 55, "post_title": "Why does $ \\int_0^1 \\lceil { x\\sin({1 \\over x})} \\rceil \\,dx = 1 - \\frac{\\log(4)}{2\\pi} $?"}}
{"input": "Let $\\cal C$ be a circle in ${\\mathbb R}^2$ : \n$\\cal C=\\lbrace (x,y)\\in{\\mathbb R}^2 | \n(x-x_0)^2+(y-y_0)^2=r^2\\rbrace$ for some\nconstants $x_0,y_0,r$.\nWhat is the maximal number of points \nthat can be contained in ${\\cal C}\\cap {\\mathbb Z}^2$ ?\nI conjecture it is $4$, attained for the \"trivial\" case\n$x_0=y_0=0,r=1$.", "output": "The unit circle centred on the origin can be parametrised by $$x=\\frac {1-t^2}{1+t^2}; y=\\frac {2t}{1+t^2}$$\nAny rational value of $t$ gives rational values of $x$ and $y$. This can be scaled by a factor $r$ to give a circle of radius $r$.\nChoose $n$ such points, and then choose a radius which clears all the denominators - the resulting circle will have at least $n$ integer points.", "meta": {"post_id": 1379574, "input_score": 38, "output_score": 78, "post_title": "A circle in the plane contains at most four lattice points?"}}
{"input": "I'm studying cycle decomposition in group theory. The exercises on my book keep saying things like:\n\nFind a permutation $\\sigma$ such that $\\sigma (123) \\sigma^{-1} = (456)$\nProve that there is no permutation $\\sigma$ such that $\\sigma (1 2) \\sigma^{-1} = (123)$\nShow that $\\pi(x_1 x_2 \\cdots x_n)\\pi^{-1} = (\\pi(x_1)\\cdots \\pi(x_n))$\n\nWhy this  $\\sigma \\pi \\sigma^{-1}$ appears every time in this book? Is this important for something? Could somebody give me a clear picture of how we arrive at this? Also, would be nice to know how to prove the third one, if somebody could help me.", "output": "Conjugation amounts to changing perspective. Here are three major examples to illustrate this point: notational representations of permutations with respect to labels on objects, matrices representing linear transformations with respect to ordered bases, and loops a la the fundamental group as studied in algebraic topology and homotopy theory.\n$\\sf \\color{DarkOrange}{Permutations}$. As you've seen, $\\pi(x_1\\cdots x_n)\\pi^{-1}=(\\pi(x_1)\\cdots\\pi(x_n))$. Let's see if I can explain what this means. Suppose we have a set $X$ and a permutation $\\sigma$ on it. Furthermore, say we relabel all of the elements of $X$. (The way to encode this is as a bijection $\\pi:X\\to Y$ for some set of labels $Y$.) Then there should be a corresponding permutation $\\sigma'$ of $Y$ which does the same thing to $Y$ that $\\sigma$ does to $X$, just with the elements relabeled. More concretely, if our permutation $\\sigma$ sends $x_1$ to $x_2$ (in $X$) then $\\sigma'$ should send $\\pi(x_1)$ to $\\pi(x_2)$. And thus the resulting effect on cycles (and thus on cycle types of any permutation) are seen: as $\\sigma$ maps $x_1\\mapsto x_2$ and $x_2\\mapsto x_3$ and so on, we must have $\\sigma'$ mapping $\\pi(x_1)\\mapsto\\pi(x_2)$ and $\\pi(x_2)\\mapsto\\pi(x_3)$ and so on.\nConsider the following scenario. A blind woman shuffles an arrangement of $n$ things placed in front of her - and has mentally memorize this shuffle, even if she can't see the labels that are put on the $n$ things (for instance, they could have the numerals $1$ through $n$, or the word \"one\" through whatever, or those same words in a different language, etc.). Us seeing people can describe her shuffle using, say, cycle notation or one-line notation. Whatever the case, if we then relabel the objects and the woman performs her shuffle, then there will be a change in how we represent the permutation. Indeed if we swap the labels, then she shuffles the objects, then we swap the labels back to their originals, the effect is the same as if she had just directly performed the shuffle without any label swapping at all. This means the following diagram commutes: \n$$\\require{AMScd}\\begin{CD}\nX @>\\pi>> Y\\\\@VV{\\sigma}V @VV{\\sigma'}V\\\\ X @>\\pi>> Y\n\\end{CD} \\tag{$\\circ$}$$\nThus, we have $\\sigma'\\circ\\pi=\\pi\\circ\\sigma$. If $Y=X$ (so we use the same labels for the objects the woman shuffles, but we shuffle the labels themselves!) then this is $\\sigma'\\pi=\\pi\\sigma$, i.e. $\\sigma'=\\pi\\sigma\\pi^{-1}$.\nThe diagram $(\\circ)$ is important and ubiquitous in mathematics. It is how we transport the structure of the symmetries of some object to the symmetries of an isomorphic object (often taking place in some category). This pattern repeats itself in other examples. It appears when we wish to define equivariant and intertwining operators. Anytime something happens in one situation, and you want to convert it to another situation, something along these lines is happening.\nOn the other hand, group theory also has conjugation appear a lot simply because it is useful in exploring the structure of a group. Two elements commuting $ab=ba$ is equivalent to $b=aba^{-1}$ (you can use this to compute the probability two elements commute!). If one defines the maps $\\varphi_g(x):=gxg^{-1}$, then for each $g\\in G$ the map $\\varphi_g:G\\to G$ is an automorphism.\nYou need conjugation to define normal subgroups, which are kernels of group homomorphisms and (equivalently) the congruence classes of the identity whose cosets are the congruence classes of all the other elements. One uses normal subgroups to break apart groups into pieces called composition factors (see the Jordan Holder theorem) - indeed in this way finite cyclic groups break apart into cyclic groups of prime order with multiplicities, exactly analogous to natural numbers breaking apart into products of primes with multiplicities. And another structure-theoretic result coming from the arithmetic of a group's order is seen in the three Sylow theorems, whose statements and proofs rely on conjugation.\n$\\sf \\color{DarkOrange}{Matrices}$. Often students are introduced to linear algebra in a concrete geometric way, and often the only vectors introduced are coordinate vectors and linear maps are simply defined by matrices, but there is more abstraction behind the picture than this. Vectors don't need to be coordinate vectors - they just need to be things, things that can be added or multiplied by scalars. One learns somewhat early on that every vector space has a basis. Given a basis $\\{e_1,\\cdots,e_n\\}$ of a vector space $V$ of finite dimension $n$, any vector $v\\in V$ may be written as $v=x_1e_1+\\cdots+x_ne_n$ uniquely, and so we can represent any vector $v$ by a corresponding coordinate vector $(x_1,\\cdots,x_n)$. So in some sense, nothing of value is lost by restricting our attentions to coordinate vectors (or so a physicist might reason, but a set theorist would certainly argue), but often we want to change bases.\nBases are also how linear transformations $T:V\\to W$ get represented as matrices. Say $\\{f_i\\}$ is a basis for $W$ (with $\\dim=m$). Since $T(v)=T(x_1e_1+\\cdots+x_ne_n)=x_1T(e_1)+\\cdots+x_nT(e_n)$, in order to know where $T$ sends every vector $v\\in v$ it suffices to know where $T$ sends the basis vectors, and moreover, each of $T(e_1),\\cdots,T(e_n)$ may be written as\n$$T(e_1)= t_{11}f_1+\\cdots+t_{m1}f_m \\\\ \\vdots \\\\ T(e_n)=t_{1n}f_1+\\cdots+t_{mn}f_m$$\nThus, $T$ is specified by a rectangular array of scalars. One may check that if writing $T(v)$ in coordinates (according to the chosen basis of $W$), the result is the same as if we multiply out\n$$\\begin{bmatrix}t_{11} & \\cdots & t_{1n} \\\\ \\vdots & \\ddots & \\vdots \\\\ t_{m1} & \\cdots & t_{mn}\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}.$$\nOne may further check that if $T:V\\to W$ and $S:W\\to U$ are two linear transformations and $\\circ$ denotes composition, then the matrix of $S\\circ T$ is the matrix product of $S$ and $T$'s matrices (note bases must be chosen for each of $U,V,W$ for this to make sense). This is where matrix multiplication comes from in the first place!\nNow suppose we just have $T:V\\to V$ for simplicity, and $\\cal B$ is a basis for $V$. Denote the corresponding matrix by $[T]_{\\cal B}$. If we have another basis ${\\cal C}$, how does $[T]_{\\cal C}$ relate to $[T]_{\\cal B}$?\n(I suppose at this point I should observe that all of our bases have not just been bases, but ordered bases. Technically this is important for bookkeeping purposes, but sources frequently forget to mention this qualification at all!) Given ${\\cal B}=\\{e_i\\}$ and ${\\cal C}=\\{f_i\\}$, there is a unique transformation $P:V\\to V$ with $P(e_i)=f_i$ for each $i$. Let's abbreviate ${\\cal C}=P{\\cal B}$. The way to figure out what $[T]_{P{\\cal B}}$ is, is to look at how vectors are affected.\nDenote by $[v]_{\\cal B}$ the vector $v$ written as a coordinate vector with respect to ordered basis $\\cal B$. Then we know that $[Tv]_{\\cal B}=[T]_{\\cal B}[v]_{\\cal B}$. Moreover, if $v=x_1e_1+\\cdots+x_ne_n$ then $P(v)=x_1f_1+\\cdots+x_nf_n$ and so we conclude $[Pv]_{P{\\cal B}}=[v]_{\\cal B}$, both being $(x_1,\\cdots,x_n)$. Now from $[v]_{\\cal B}=[Pv]_{P\\cal B}$ and $[v]_{P\\cal B}=[P^{-1}v]_{\\cal B}$ we see that $[P]_{\\cal B}=[P]_{P\\cal B}$, call this $\\Phi^{-1}$. Understandably, $\\Phi$ is called the change of basis matrix, because $\\Phi:[v]_{\\cal B}\\mapsto [v]_{P\\cal B}$ for all $v$. This $\\Phi$ is analogous to our permutation $\\pi$: it changes our perspective on the vector space.\nThen $[Tv]_{P\\cal B}=[P^{-1}Tv]_{\\cal B}=[P^{-1}T]_{\\cal B}[v]_{\\cal B}=[P]_{\\cal B}^{-1}[T]_{\\cal B}[Pv]_{P\\cal B}=\\Phi[T]_{\\cal B}\\Phi^{-1}[v]_{P\\cal B}$. Since this holds for all $v$, from this we may conclude $[T]_{P\\cal B}=\\Phi[T]_{\\cal B}\\Phi^{-1}$. Thus we see in linear algebra, similar (i.e. conjugate) matrices represent the same linear transformation but with respect to different ordered bases. (Okay, technically we saw the converse of this, but seeing that given any matrix $\\Phi$ there are corresponding bases is not too difficult afterwards.)\n$\\sf \\color{DarkOrange}{Loops}$. A path in a topological space $X$ is a continuous map $[0,1]\\to X$. If we reparametrize the path (so in particular, it traces out the same image) or continuously wiggle the path, we do not fundamentally change the path - this inspires the idea of a homotopy class of path. Paths that begin and end at the same point are loops. There is an obvious way of concatenating two paths (trace out the first path first, and the second path second - although both with double their usual \"speed\"), and homotopy equivalence of paths is a congruence relation for this operations so we can define the fundamental group $\\pi_1(X,x)$ to be the homotopy classes of loops from $x$ to itself, with this intuitive notion of composition. See the link for more information.\nIntuitively, one should be able to continuously move $x$ itself around and subsequently move the loops based in $\\pi_1(X,x)$ around with it. Indeed, if $\\gamma:[0,1]\\to X$ is a path from $x$ to $y$, then we given any (homotopy class of loop) $t\\in\\pi_1(X,x)$ we can form a new path that starts at $y$, goes along $\\gamma$ backwards, goes along $t$ from $x$ to itself, and then goes back along $\\gamma$ to $y$. In this way, we input loops $t$ based at $x$ and get loops $t'$ based at $y$. Let's write this up as a diagram:\n$$\\begin{CD}\nx @>\\gamma>> y\\\\@VV{t}V @VV{t'}V\\\\ x @>\\gamma>> y\n\\end{CD} $$\nThis is a bit informal, but you should be able to get the idea. Thus $t'=\\gamma\\circ t\\circ\\gamma^{-1}$ where $\\gamma^{-1}$ has the obvious interpretation: take $\\gamma$ backwards from $y$ to $x$. Then $t=\\gamma^{-1}\\circ t'\\circ\\gamma$, so this process is invertible. Altogether this means conjugation is an isomorphism $\\pi_1(X,x)\\to\\pi_1(X,y)$. If we have that $x=y$ then this amounts to taking a loop $\\gamma$ and moving other loops $t$ around it (keeping the basepoint always on $\\gamma$) to get a new loop $t'$ when we finally come back to the original basepoint.", "meta": {"post_id": 1381652, "input_score": 27, "output_score": 38, "post_title": "Why this $\\sigma \\pi \\sigma^{-1}$ keeps appearing in my group theory book? (cycle decomposition)"}}
{"input": "I am familiar with norms on vectors and functions, but do there exist norms for spaces of matrices i.e. $A$ some $n \\times m$ matrix?\nIf so, that would that imply matrices also form some sort of vector space?", "output": "First of all, yes: the matrices form some sort of vector space.  You can add any two matrices, and you can multiply matrices by a number, and you'll always get another matrix.  In a sense, that's all you need for a set to be a vector space. Matrices have a little bit more structure too: for one, you can multiply two matrices together (which you can't generally do with vectors).  Moreover, matrices are really linear maps.  I'll get back to those in a minute.\nThere are three kinds of matrix norms, each of which is useful in different circumstances.\n\nNorms (\"just\" a norm):\nSometimes a norm is just a norm.  Often, it's useful to think of a matrix as \"a box of numbers\" in the same way that you would think of a vector in $\\Bbb R^n$ as a \"list of numbers\".  A \"matrix norm\" by this definition is any function on the matrices that satisfies the usual rules that define a norm.  In particular, for any matrices $A,B \\in \\Bbb R^{n \\times m}$ and constant $\\alpha$, we need to have\n\n\n$\\|A\\| \\geq 0$, with $\\|A\\| = 0 \\iff A = 0$\n$\\|\\alpha A\\| = |\\alpha|\\|A\\|$\n$\\|A + B\\| \\leq \\|A\\| + \\|B\\|$\n\n\nYou would use these norms any time you would use an ordinary norm.  One reason we would need this kind of norm is to show that a function involving matrices is \"continuous\", or \"differentiable\".  The usual example of this kind of norm is the \"entrywise $p$-norm\", which is given by\n$$\n\\|A\\| = \\left(\\sum_{i=1}^n \\sum_{j=1}^m |a_{ij}|^p\\right)^{1/p}\n$$\nfor $1 \\leq p \\leq \\infty$.\nEvery matrix norm can be thought of in this way, i.e. as a \"general norm\".  However, sometimes we want our matrix norm to have a bit more structure.\n\nSubmultiplicative Norms (AKA \"matrix norms\")\n\nWe say that a matrix norm $\\|\\cdot\\|$ is submultiplicative if, in addition to being a norm, it also satisfies the inequality\n$$\n \\|AB\\| \\leq \\|A\\| \\cdot \\|B\\|\n$$\nFor any square matrices $A,B$ of the same size\n\nA lot of times, your everyday norm just won't cut it.  For those occasions, submultiplicative norms tend to come in handy.  These are useful for dealing with \"polynomials\" on matrices since we have inequalities like\n$$\n\\|f(A)\\| = \\left\\|\\sum_{k}a_kA^k \\right\\| \\leq \\sum_k |a_k|\\|A\\|^k\n$$\nNotably, if the $a_k$ are non-negative, $\\|f(A)\\| \\leq f(\\|A\\|)$, so that we have $\\|e^A\\| \\leq e^{\\|A\\|}$ for instance.\nMultiplicative norms are also very useful for spectral (eigenvalue) analysis.  In fact, we have some theorems involving $\\rho(A)$, the spectral radius of $A$, and any submultiplicative norm:\n\n$\\|A\\| \\geq \\rho(A)$\n$\\rho(A) = \\lim_{k \\to \\infty} \\|A^k\\|^{1/k}$\n$\\rho(A) = \\inf_{\\|\\cdot\\| \\text{ is submult.}} \\|A\\|$\n\nOne \"classic\" example of a submultiplicative norm is the Frobenius norm, AKA the entrywise $2$-norm, AKA the Schatten $2$-norm:\n$$\n\\|A\\|_F = \\sqrt{\\sum_{i = 1}^n\\sum_{j=1}^m |a_{ij}|^2} = \\sqrt{\\sum_{i = 1}^d\\sigma_i^2(A)},\n$$\nwhere d = $\\min\\{m,n\\}$ and $\\sigma_i$ denotes the $i$th singular value of $A$ (in non-increasing order). This is probably the most commonly used of all matrix norms. It is particularly useful since it is the norm derived from the Frobenius inner product (AKA Hilbert-Schmidt inner product). That is, it turns out that taking the \"dot product\" of matrices is a useful thing to do, and the Frobenius norm is the norm that results from this dot product.\nThe Frobenius norm falls into the class of matrix norms that are unitarily invariant, that is, norms $\\|\\cdot\\|$ that satisfy $\\|UAV\\| = \\|A\\|$ for compatible unitary matrices $U,V$. Equivalently, these are the matrix norms that can be expressed as the result of a vector norm applied to the vector of singular values (cf. Bhatia's Matrix Analysis). Some other commonly used unitarily invariant norms are the Schatten norms, which have the formula\n$$\n\\|A\\|_{S,p} = \\left[\\sum_{i=1}^d \\sigma_i^p(A)\\right]^{1/p}\n$$\nfor some value $1 \\leq p \\leq \\infty$, and the Ky-Fan norms\n$$\n\\|A\\|_{KF,k} = \\sum_{i=1}^k \\sigma_i(A)\n$$\nfor some value of $k$ with $1 \\leq k \\leq d$. In both of the above equations, $d = \\min\\{m,n\\}$\nThe entrywise $p$-norms from earlier only happen to be submultiplicative norms when $1 \\leq p \\leq 2$; these are easy to compute, but tend not give tight bounds.\nFinally, we might want our norms to be nicer still.\n\nOperator Norms (AKA \"induced/derived norms\")\n\nSuppose $\\|\\cdot \\|$ is a vector norm on $\\Bbb R^n$. We define the corresponding operator norm on $\\Bbb R^{m \\times n}$ to be given by\n$$\n\\|A\\| = \\sup_{\\|x\\| \\leq 1} \\|Ax\\|\n$$\n\nEvery operator norm is a submultiplicative norm.  However, not every submultiplicative norm is an operator norm.  Besides doing everything that the submultiplicative norms can do, operator norms are useful when you're thinking about how matrices act on vectors.  In particular, with operator norms, we have the inequality\n$$\n\\|Av\\| \\leq \\|A\\|\\cdot \\|v\\|\n$$\nIt follows that for every operator norm, the identity matrix $I$ has the property $\\|I\\| = 1$.  This fact turns out to have some useful consequences (e.g. inequalities involving the norm of a matrix's inverse).\nMost of the norms that we have mentioned are not operator norms.  The operator norm that you see most often is the one derived from the Euclidean norm ($2$-norm) on vectors.  In particular, we have\n$$\n\\|A\\|_2 = \\sup_{\\|x\\| \\leq 1} \\|Ax\\|_2 = \\sigma_1(A)\n$$\nThat is, this norm is equal to the largest singular value of $A$. This norm also happens to coincide with the \"Schatten $\\infty$-norm\" $\\|\\cdot\\|_{S,\\infty}$ and the Ky-Fan norm $\\|\\cdot\\|_{KF,1}$.\nA particularly useful property of this norm is that $\\|A\\|_2 = \\rho(A)$ whenever $A$ happens to be normal (i.e. whenever $A^TA = AA^T$). Because of this property, $\\|\\cdot\\|_2$ is sometimes called the \"spectral norm\".\nTwo other operator norms that are commonly used (especially in the context of numerical linear algebra) are the one derived from the $1$-norm (\"taxicab norm\") and the one derived from the $\\infty$-norm (\"max norm\"). These are straightforward to compute; in particular, we have\n$$\n\\|A\\|_1= \\max_j \\sum_{i=1}^m |A_{ij}|\\\\\n\\|A\\|_{\\infty}= \\max_i \\sum_{j=1}^n |A_{ij}|\n$$", "meta": {"post_id": 1394113, "input_score": 12, "output_score": 36, "post_title": "What are some usual norms for matrices?"}}
{"input": "Is there a direct way to evaluate the following series?\n\n$$\n\\sum_{n=1}^{\\infty}\\ln \\!\\left(1+\\frac1{2n}\\right) \\!\\ln\\!\\left(1+\\frac1{2n+1}\\right)=\\frac12\\ln^2 2. \\tag1\n$$ \n\nI've tried telescoping sums unsuccessfully. The convergence is clear. Given the simplicity of the result, I'm inclined to think it might exist an elegant way to get $(1)$.", "output": "Use:\n$$\\left(\\ln\\left(1+\\frac{1}{2n}\\right)+\\ln\\left(1+\\frac{1}{2n+1}\\right)\\right)^2=\\ln^2\\left(1+\\frac{1}{2n}\\right)+\\ln^2\\left(1+\\frac{1}{2n+1}\\right)+2\\ln\\left(1+\\frac{1}{2n}\\right)\\ln\\left(1+\\frac{1}{2n+1}\\right)$$\n$$\\Rightarrow \\ln\\left(1+\\frac{1}{2n}\\right)\\ln\\left(1+\\frac{1}{2n+1}\\right)=\\frac{1}{2}\\left(\\ln^2\\left(1+\\frac{1}{n}\\right)-\\ln^2\\left(1+\\frac{1}{2n}\\right)-\\ln^2\\left(1+\\frac{1}{2n+1}\\right)\\right)$$\nTherefore, the sum in the given problem is:\n$$\\frac{1}{2}\\left(\\sum_{n=1}^{\\infty}\\ln^2\\left(1+\\frac{1}{n}\\right)-\\sum_{n=1}^{\\infty}\\ln^2\\left(1+\\frac{1}{2n}\\right)-\\sum_{n=1}^{\\infty}\\ln^2\\left(1+\\frac{1}{2n+1}\\right)\\right)$$\nNotice that all the terms of the first sum are cancelled by the other two sums except $n=1$, hence our answer is:\n$$\\boxed{\\dfrac{1}{2}\\ln^22}$$", "meta": {"post_id": 1394623, "input_score": 21, "output_score": 37, "post_title": "Evaluating $\\sum_{n \\geq 1}\\ln \\!\\left(1+\\frac1{2n}\\right) \\!\\ln\\!\\left(1+\\frac1{2n+1}\\right)$"}}
{"input": "To indicate approximate equality, one can use \u2243, \u2245, ~, \u264e, or \u2252.\nI need to indicate an approximate inequality. Specifically, I know A is greater than a quantity of approximately B.\nIs there a way to succinctly express this mathematically?", "output": "LaTeX has the symbols \\lessapprox ($\\lessapprox$) and \\gtrapprox ($\\gtrapprox$).\nIncidentally, a long time ago I came across this awesome Short Math Guide for $\\LaTeX$, a free pdf by the American Mathematical Society. Well-worth keeping it close by.", "meta": {"post_id": 1394952, "input_score": 58, "output_score": 75, "post_title": "Is there a \"greater than about\" symbol?"}}
{"input": "I don't think I ever understood the rationale behind this.\nI get that the dot product $\\mathbf{a} \\cdot  \\mathbf{b} =\\lVert \\mathbf{a}\\rVert \\cdot\\lVert \\mathbf{b}\\rVert \\cos\\theta$ is derived from the cosine rule. (Do correct me if I'm wrong.)\nHowever, I never really understood why $\\mathbf{a} \\times  \\mathbf{b} =\\lVert \\mathbf{a}\\rVert \\cdot\\lVert \\mathbf{b} \\rVert \\sin\\theta$. Is there a proof for this?\nIn addition, how does the vector cross product lead to orthogonal vectors?\nI remember learning that they do and I know how to solve equations using the respective formulas, but I never got why is it so.", "output": "$$\\Large{\\text{Bye_World's Second Treatise}} \\\\ \\large{\\text{On the Products of Vectors}}$$\n\nTable of Contents$\\bullet \\ $Preface$\\bullet \\ $Vectors$\\bullet \\ $The Dot Product$\\bullet \\ $The Cross Product$\\bullet \\ $The Real Skinny on the Cross Product$\\bullet \\ $The Wedge Product$\\bullet \\ $The Relationship between the Cross Product and Wedge Product$\\bullet \\ $A Quick Note on Further Products\n\nPreface\nThis work is my completely insane attempt to try to fix whatever issue the downvoter to this answer had with my less comprehensive former answer.  This answer is undoubtedly one of the longest on math.SE (actually, this is the 7th longest answer on math.SE as of this posting ) and goes far beyond what any reasonable person could want from an answer to the above question.\nEnjoy. \n\nVectors\nOur very first notions of vectors, the first objects we were taught in linear algebra, are tuples and oriented line segments.  When we learned about these we very quickly were told that they are really the \"same\" objects.  This facilitated our computations.  The problem is, they really aren't the same objects.  In my opinion, just because we have a canonical way of associating with each tuple an oriented line segment and with each oriented line segment a tuple (once a basis is chosen for the oriented line segments) doesn't mean that we don't need to define our products on these objects individually.  It does mean that after we do so, we need to confirm that the products are equivalent in their respective spaces.  In this treatise, I will try to give both an algebraic and a geometric definition for each of our products.\nBoth the set of $n$-tuples and the set of oriented line segments in $n$-dimensional Euclidean space (along with the usual operations on those sets) are inner product spaces over the field of real numbers.  That just means that they are vector spaces, with an inner product (called the dot product), and the scalars that we are allowed to multiply these vectors by are real numbers.\nNow let's go over exactly what tuples and oriented line segments are.\nTuples:\nThese are our purely algebraic vectors.  An $n$-tuple is simply an ordered list of $n$ real numbers.  For instance, the space of $4$-tuples, denoted $\\Bbb R^4$, is the set of all objects of the sort $(w,x,y,z)$ where $w,x,y,z \\in \\Bbb R$.  This is a $4$-dimensional vector space over the real numbers.\nThe way that we add and scale tuples are component-wise.  That is $$(x_1, x_2, \\dots, x_n) + (y_1, y_2, \\dots, y_n) = (x_1+y_1, x_2+y_2, \\dots, x_n+y_n) \\\\ \\alpha(x_1, \\dots, x_n) = (\\alpha x_1, \\dots, \\alpha x_n)$$\nOne of the interesting properties of $\\Bbb R^n$ is that it comes equipped with a natural orthonormal basis.  For instance, in $\\Bbb R^4$ that basis is $\\{(1,0,0,0),(0,1,0,0),(0,0,1,0),(0,0,0,1)\\}$.  We will soon be able to see that the norm of any of these vectors is $1$.\nInstead of writing $n$-tuples as ordered lists with parentheses and commas, often it is more convenient to write them as row or column matrices:  $$(1,2,3) \\leftrightarrow \\begin{bmatrix} 1 & 2 & 3\\end{bmatrix} \\leftrightarrow \\begin{bmatrix} 1 \\\\ 2 \\\\ 3\\end{bmatrix}$$\nThe way that we choose to write down the numbers doesn't matter when it comes to scalar multiplication or vector addition, however the benefit of writing tuples as matrices is that performing any linear transformation on a tuple is equivalent to multiplying a row (or column) matrix by a unique $n\\times m$ (or $m\\times n$) matrix.\nOriented Line Segments:\nOriented line segments, also known as translation vectors, are the elements of Euclidean space.  I'm going to call this space $\\Bbb L^n$ ($\\Bbb L$ for \"line segment\").  This is not a standard notation, it's just the one I like.  These objects are characterized by a specific length and a specific direction.\nVector addition in this case is given by the parallelogram rule\n\nand scalar multiplication is done by scaling the length of the line segment, preserving its orientation if scaled by a number $\\gt 0$ or negating its orientation if scaled by a number $\\lt 0$\n\nNotice that I have not said anything here about the location of a vector.  That is because oriented line segments do not have any intrinsic location, they just exist in space.\nThere are some interesting things about these vectors.  For one thing, every vector is specified in part by its norm -- as known as its length.  Thus while in most inner product spaces the inner product induces a norm, in this space the norm exists without ever needing to specify an inner product, though we will define one in a bit -- the dot product.  Other interesting properties that are more fundamental than in less geometric vector spaces are the angles between vectors and the ideas of parallelness and perpendicularity.  All of these things exist even without ever defining that dot product.\n\nThe Dot Product\nThe dot product is the inner product of $\\Bbb R^n$ and $\\Bbb L^n$.\nThe Algebraic Definition:\nThe dot product on $\\Bbb R^n$ is defined as follows: given $v,w \\in \\Bbb R^n$ where $v=(v_1, \\dots, v_n)$ and $w=(w_1, \\dots, w_n)$, the dot product, denoted $v \\cdot w$, is defined by $$v\\cdot w = \\sum_{i=1}^n v_iw_i = v_1w_1 + v_2w_2 + \\cdots + v_nw_n$$\nThe idea here even generalizes pretty well to real-valued functions of a real varible.\nThe Geometric Definition:\nI'm going to define the dot product of two vectors $\\vec v, \\vec w \\in \\Bbb L^n$ in a slightly nonstandard way.  In my opinion, this definition is more intuitive geometrically.\nI define the dot product, $\\vec v \\cdot \\vec w$, by $$\\vec v \\cdot \\vec w = \\operatorname{sproj}_{\\vec w}(\\vec v)\\|\\vec w\\| = \\operatorname{sproj}_{\\vec v}(\\vec w)\\|\\vec v\\|$$\nWhere the scalar projection operation, $\\operatorname{sproj}$, is defined by $$\\operatorname{sproj}_{\\vec w}(\\vec v) = \\begin{cases}\\|\\operatorname{proj}_{\\vec w}\\vec v\\|, & \\text{the angle between $\\vec v$ and $\\vec w$ is $\\le \\frac {\\pi}2$} \\\\ -\\|\\operatorname{proj}_{\\vec w}\\vec v\\|, & \\text{the angle between $\\vec v$ and $\\vec w$ is $\\gt \\frac {\\pi}2$}\\end{cases}$$\nHere $\\|\\vec v\\|$ denotes the length of the vector $\\vec v$ and $\\operatorname{proj}_{\\vec w}\\vec v$ is the projection of the vector $\\vec v$ onto the subspace $\\operatorname{span}(\\vec w)$.  Thus I define the dot product in terms of length and orthogonal projection and not the other way around.  (Note: How I define orthogonal projection for the particular space $\\Bbb L^n$ without making use of the dot product is beyond the scope of this answer.)\n\nWhile I hope that my definition of the dot product on $\\Bbb L^n$ catches on and I start seeing it in all the new linear algebra texts ;), I should probably mention the standard definition.  The standard definition of the dot product on $\\Bbb L^n$ is $$\\vec v \\cdot \\vec w = \\|\\vec v\\|\\|\\vec w\\|\\cos(\\theta)$$  This can be seen to be equivalent to my definition because the signed length of $\\operatorname{proj}_{\\vec w}\\vec v$ is $\\|\\vec v\\|\\cos(\\theta)$.  You can see this in this image\n\nThus, one could certainly make the argument that the definition $\\vec v \\cdot \\vec w = \\|\\vec v\\|\\|\\vec w\\|\\cos(\\theta)$ is just a more compact version of my own definition.  I make that concession, but in my mind a direct association of the dot product and projections is the most intuitive way to define it.\nAlgebraic Properties:\nBoth of these dot products share some important algebraic properties.  These all can be proven from the definitions.  I will simply list them here.  Given any vectors $u,v,w$ in either $\\Bbb R^n$ or $\\Bbb L^n$ and any $k\\in \\Bbb R$:\n$$\\begin{array}{lcr} (1) & u \\cdot v = v\\cdot u & \\left(\\text{commutativity}\\right) \\\\ (2) & u\\cdot(v+w) = u\\cdot v + u\\cdot w & \\left(\\text{distributivity}\\right) \\\\ (3) & k(u\\cdot v) = (ku)\\cdot v = u\\cdot (kv) & \\left(\\begin{array}{c}\\text{interacts well with} \\\\ \\text{scalar multiplication}\\end{array}\\right)\\end{array}$$\nApplications:\nOne of the major mathematical benefits of defining an inner product is that it induces a norm on the vector space.  As mentioned earlier, $\\Bbb L^n$ comes pre-equipped with a norm, but $\\Bbb R^n$ doesn't.  So in $\\Bbb R^n$, the norm of a vector $x$ is defined by $\\|x\\| = \\sqrt{x\\cdot x}$.  I leave it as an exercise for the reader to confirm that the dot product on $\\Bbb R^n$ is positive definite, and thus we don't need to worry about a negative under the radical.\nAnother major application is that the dot product provides a convenient way of defining orthogonality in $\\Bbb R^n$ and testing for orthogonality (perpendicularity) in $\\Bbb L^n$.  We define orthogonality, denoted $v\\ \\bot\\ w$, in $\\Bbb R^n$ by $$v\\ \\bot\\ w \\iff v\\cdot w = 0$$\nThe dot product is useful in physics when you only want to know about the components of a vector in a specific direction.  For instance, work along a straight line is defined as $W = \\vec F \\cdot \\vec r$ in a gravitational field.  $\\vec F$ is just the force due to gravity, where $\\vec F=-mg\\,\\hat e_3$, and $\\vec r$ is the vector which describes the straight line path a particle moves in.  But when calculating work, we really only care about the projection of $\\vec r$ in the direction of the force ($-\\,\\hat e_3$ in this case).  Thus it makes sense that we'd use the dot product to define it.\n\nThe Cross Product\nAlgebraic Definition:\nThe cross product is defined as the unique vector $b\\times c\\in \\Bbb R^3$ such that $$a\\cdot (b\\times c) = \\det(a,b,c),\\quad \\forall a\\in\\Bbb R^3$$ This is an implicit definition.  However, it can be shown to be equivalent to $b\\times c= (b_2c_3-b_3c_2,\\ b_3c_1-b_1c_3,\\ b_1c_2-b_2c_1)$.\nNote: I don't directly define $b\\times c$ as the vector $(b_2c_3-b_3c_2,\\ b_3c_1-b_1c_3,\\ b_1c_2-b_2c_1)$ because $(1)$ it's harder to remember than my definition and $(2)$ my definition immediately tells readers who are familiar with determinants several properties of the cross product, for instance that $v\\ \\bot\\ v\\times w$ for all $v,w\\in \\Bbb R^3$ and $v\\times w = -w\\times v$ for all $v,w\\in\\Bbb R^3$.\nExplicit Formula for the Cross Product:\nUniqueness Lemma: If $v$ and $v'$ are two vectors in $\\Bbb R^3$ such that $a\\cdot v = \\det(a,b,c) = a\\cdot v',\\ \\forall a\\in \\Bbb R^3$, then $v=v'$.\nProof: Subtract $a\\cdot v' = \\det(a,b,c)$ from $a\\cdot v = \\det(a,b,c)$ to get $$a\\cdot v-a\\cdot v' = 0,\\quad \\forall a \\\\ \\implies a\\cdot(v-v')=0,\\quad \\forall a \\\\ \\implies a\\ \\bot\\ (v-v'),\\quad \\forall a$$\nBut the only vector orthogonal to $a$ for all $a$ is the zero vector.  Thus $$v-v'=0 \\\\ \\implies v=v'$$ Therefore if any vector $v$ satisfies the equation $a\\cdot v= \\det(a,b,c),\\ \\forall a$, then it is unique.$\\ \\ \\ \\square$\nThus we only need to prove that $b\\times c = (b_2c_3-b_3c_2,\\ b_3c_1-b_1c_3,\\ b_1c_2-b_2c_1)$ is a vector that satisfies the definition to show that this is the unique cross product defined above.\nLemma: $$(a_1, a_2, a_3)\\cdot (b_2c_3-b_3c_2,\\ b_3c_1-b_1c_3,\\ b_1c_2-b_2c_1) = \\det\\begin{bmatrix} a_1 & b_1 & c_1 \\\\ a_2 & b_2 & c_2 \\\\ a_3 & b_3 & c_3\\end{bmatrix}$$\nProof:  On the LHS we get $$(a_1, a_2, a_3)\\cdot (b_2c_3-b_3c_2,\\ b_3c_1-b_1c_3,\\ b_1c_2-b_2c_1) = a_1(b_2c_3-b_3c_2) + a_2(b_3c_1-b_1c_3) + a_3(b_1c_2-b_2c_1)$$  On the RHS, expanding along the left column, we get $$\\begin{align}\\det\\begin{bmatrix} a_1 & b_1 & c_1 \\\\ a_2 & b_2 & c_2 \\\\ a_3 & b_3 & c_3\\end{bmatrix} &= a_1\\left|\\begin{matrix} b_2 & c_2 \\\\ b_3 & c_3\\end{matrix}\\right| - a_2\\left|\\begin{matrix} b_1 & c_1 \\\\ b_3 & c_3\\end{matrix}\\right| + a_3\\left|\\begin{matrix} b_1 & c_1 \\\\ b_2 & c_2\\end{matrix}\\right| \\\\ &= a_1(b_2c_3-b_3c_2) - a_2(b_1c_3-b_3c_1) + a_3(b_1c_2-b_2c_1) \\\\&= a_1(b_2c_3-b_3c_2) + a_2(b_3c_1-b_1c_3) + a_3(b_1c_2-b_2c_1)\\end{align}$$\nThis proves that $b\\times c= (b_2c_3-b_3c_2,\\ b_3c_1-b_1c_3,\\ b_1c_2-b_2c_1)$ is in fact a vector which satisfies our definition.$\\ \\ \\ \\ \\square$\nTherefore this is the unique cross product that satisfies the definition.\nGeometric Definition:\nTo define a product on $\\Bbb L^3$, we simply need to define the length and orientation of the product in terms of the input vectors.  Here is our definition:\nGiven two vectors $\\vec v, \\vec w\\in \\Bbb L^3$, we define a third vector $\\vec v \\times \\vec w$ as the vector whose length is given by the area of the parallelogram with sides $\\vec v$ and $\\vec w$ and whose direction is orthogonal to both $\\vec v$ and $\\vec w$, as determined by the right-hand rule.\n\nLemma: $$\\|\\vec a\\times \\vec b\\|=\\|\\vec a\\|\\|\\vec b\\|\\sin(\\theta)$$ where $\\theta$ is the angle between vectors $\\vec a, \\vec b \\in \\Bbb L^3$.\nProof: First we remember a fact from geometry: for constant cross sectional areas, the total area is given by \"base times height\".  This is a direct consequence of Cavalieri's principle.  Then from the following image:\n\nwe can see that the base is $\\|\\vec b\\|$ and the height is $\\|\\vec a\\|\\sin(\\theta)$.  Therefore the area of the parallelogram -- and thus the length of the vector $\\vec a \\times \\vec b$ -- is $\\|\\vec a\\|\\|\\vec b\\|\\sin(\\theta)$. $\\ \\ \\ \\ \\square$\nAlgebraic Properties:\nBoth of these cross products share some important algebraic properties.  These all can be proven from the definitions.  I will simply list them here.  Given any vectors $u,v,w$ in either $\\Bbb R^n$ or $\\Bbb L^n$ and any $k\\in \\Bbb R$:\n$$\\begin{array}{lcr} (1) & u \\times v = -v\\times u & \\left(\\text{anticommutativity}\\right) \\\\ (2) & u\\times(v+w) = u\\times v + u\\times w & \\left(\\text{distributivity}\\right) \\\\ (3) & k(u\\times v) = (ku)\\times v = u\\times (kv) & \\left(\\begin{array}{c}\\text{interacts well with} \\\\ \\text{scalar multiplication}\\end{array}\\right) \\\\ (4) & u\\times(v\\times w) + v\\times(w\\times u) + w\\times(u\\times v) = 0 & \\left(\\text{Jacobi identity}\\right)\\end{array}$$\nOne more property, which is actually a consequence of the anticommutativity of the cross product (can you prove it?), is that for any vector $v$, we have $v\\times v=0$.\nOne important property that the cross product doesn't have is associativity.  Consider the triple products $u\\cdot (v\\cdot w)$ and $u\\times (v\\times w)$.  Without much effort we can see that $u\\cdot (v\\cdot w)$ is undefined.  This is because $v\\cdot w$ is a scalar and then the dot product of a vector and a scalar is undefined by our above definition.  However, $u\\times (v\\times w)$ is defined.  Knowing it's defined, our next question should be \"are the parentheses necessary?\"  Yes. In general, $u\\times (v\\times w)$ is not equal to $(u\\times v)\\times w$.\nApplications:\nThe mathematical and physical significance of the cross product, $v\\times w$, is it provides a vector orthogonal to the plane, $\\operatorname{span}(v,w)$.\nFor instance, using another physics example, we can experimentally determine that a charged particle moving through a constant magnetic field will instantaneously feel a force in a direction orthogonal to both the direction it is moving in at that instant and the direction of the magnetic field.  So it shouldn't surprise you that the definition of the magnetic force is $\\vec F_m = q(\\vec v\\times \\vec B)$, which is just the cross product of the velocity vector $\\vec v$ (pointing in the direction the particle is moving) and the magnetic field (pseudo)vector $\\vec B$, scaled by some number $q$.\n\nThe Real Skinny on the Cross Product\nThe cross product is actually a terrible product, though.  Let's list some of the reasons why.\n\nIt's not commutative, but because it's anticommutative that's not that big of a deal.  Anticommutative things are actually pretty useful in mathematics (and physics).\nIt's not associative, but it does obey the Jacobi identity so I guess that's sort of OK.  It's not great, though.\nThe thing that we get from the cross product isn't really a vector.  It's just a pretender.  It's an object that looks really, really similar to a vector, but doesn't quite behave right under reflections.  If you're interested, ask your professor about this.  The name for this type of object is pseudovector. Note: the fact that the cross product of two vectors isn't a vector isn't actually a problem. Afterall, the dot product isn't either. The problem is that there is no standard notation which distinguishes pseudovectors from vectors. So you just have to keep in mind what type of object you're working with.\nBut the biggest problem, the most awful thing about the cross product is that it is only defined in $3$ dimensions.  That's terrible.  Linear algebra works in any finite dimension (infinite dimensional linear algebra is called functional analysis) but we have a product which only works in $3$-dimensions?  This is not a good product.\n\nIt honestly astounds me that we keep using it to this day.  Really the cross product should be replaced by something else: the wedge product.\n\nThe Wedge Product\nLet's talk about the wedge product.  One thing to note in this section is that I will not provide an algebraic definition.  That's not because there isn't one, it simply requires slightly more math than I'm willing to believe OP will understand. And even more importantly, it's not necessary.  As long as we can determine the key algebraic properties of the wedge product, we'll have all we need to work with it.\nGeometric Motivation:\nFirst, let's go back over some of the properties of the vectors in $\\Bbb L^n$.  These elements are oriented line segments.  This means that every vector $\\vec v\\in \\Bbb L^n$\n\nhas a specific length, denoted $\\|\\vec v\\|$ \nis parallel to a unique line through the origin (except the\nzero vector, but zero has weird properties in any set of objects)\npoints in one of the two directions along that line\n\nVectors (line segments) can be scaled by numbers and added together with the parallelogram rule:\n\nWe could make a similar definition for oriented plane segments.  These bivectors would be elements of a space denoted $\\Lambda \\Bbb L^3$.  A bivector $B$ is an object that \n\nhas a specific area, denoted $\\|B\\|$\nis parallel to a unique plane containing the origin (except the zero bivector)\nhas one of two orientations that are a little harder to visualize than with line segments\n\nBivectors (which can be visualized as parallelograms in space) can be scaled by numbers and added together via a generalized version of the parallelogram rule:\n\nI can't find a picture of scalar multiplication of a bivector, but just imagine a parallelogram getting bigger (scaling by a number whose absolute value is $\\gt 1$) or smaller (scaling by a number whose absolute value is $\\lt 1$).\nWe can also imagine higher dimensional objects, like trivectors, etc.  A trivector is just an oriented volume segment (a parallelopiped with an orientation).  Here's a little image to show you how these objects progress:\n\nGeometric Definition:\nThe wedge product is the operation we use to make bivectors out of vectors.  \nGiven two vectors $\\vec v,\\vec w\\in \\Bbb L^n$, we define the bivector $\\vec v \\wedge \\vec w\\in \\Lambda \\Bbb L^n$ as the oriented plane segment whose area(/norm) is equal to the area of the parallelogram with sides $\\vec v$ and $\\vec w$, whose direction is parallel to the plane $\\operatorname{span}(\\vec v, \\vec w)$ (if this is a plane, otherwise $\\vec v \\wedge \\vec w = 0$), and whose orientation is given by the order of the factors.  Here's an image to help you visualize it:\n\nThat's all we need to uniquely define a bivector.  Higher dimensional $n$-vectors are defined analogously.\nAlgebraic Properties:\nThis wedge product has some important algebraic properties.  These all can be proven from the definition.  I will simply list them here.  Given any vectors $u,v,w$ in either $\\Bbb R^n$ or $\\Bbb L^n$ and any $k\\in \\Bbb R$:\n$$\\begin{array}{lcr} (1) & u \\wedge v = -v\\wedge u & \\left(\\text{anticommutativity}\\right) \\\\ (2) & u\\wedge(v\\wedge w) = (u\\wedge v)\\wedge w & \\left(\\text{associativity}\\right) \\\\ (3) & u\\wedge(v+w) = u\\wedge v + u\\wedge w & \\left(\\text{distributivity}\\right) \\\\ (4) & k(u\\wedge v) = (ku)\\wedge v = u\\wedge (kv) & \\left(\\begin{array}{c}\\text{interacts well with} \\\\ \\text{scalar multiplication}\\end{array}\\right)\\end{array}$$\nOne more property, which is actually a consequence of the anticommutativity of the wedge product (can you prove it?), is that for any vector $v$, we have $v\\wedge v=0$.\nAlso note that this product is defined in $\\Bbb L^n$ or $\\Bbb R^n$ for any $n$ (yay!).\nApplications:\nThe major application of the wedge product, and the $n$-vectors it generates, is in representing subspaces of $\\Bbb R^n$ and $\\Bbb L^n$ as elements of $\\Lambda \\Bbb R^n$ and $\\Lambda L^n$, respectively.\nOne of the consequences of this representational property is that we can define the determinant of a linear transformation $f: \\Bbb R^n \\to \\Bbb R^n$ as $$f(v_1) \\wedge f(v_2) \\wedge \\cdots \\wedge f(v_n) = \\det(f)v_1\\wedge v_2 \\wedge \\cdots \\wedge v_n$$ where $v_1, \\dots, v_n$ are $n$ linearly independent vectors in $\\Bbb R^n$.  Intuitively this just says that the linear transformation $f$ scales the (signed) volume of an $n$-dimensional parallelotope by a factor of $\\det(f)$.\nWe can also replace all of the instances of the cross product in physics with the wedge product (or a combination of the wedge product and a generalization of the dot product that is defined on $n$-vectors) -- possibly with some modification -- to get formulas which work not only in $\\Bbb R^3$ but also in $4$-dimensional Euclidean space (useful in advanced classical mechanics) and Minkowski space (useful in special/ general relativity).  I also personally feel that these new formulas are more intuitive than the standard ones -- once one has the necessary mathematics understood (example: magnetic bivector fields make way more sense to me than magnetic (pseudo)vector fields).\n\nThe Relationship Between the Cross Product and the Wedge Product\nI told you that the cross product should be replaced by this wedge product but I haven't really told what the relationship between them is.\nThe exact relationship between them is something called duality (in $\\Bbb R^3$ and $\\Bbb L^3$ only), and it'd take even more math to explain that.  As this is already a crazy long post, I'll instead just show you a couple of ways in which they are related.\nThe first thing I want to point out is that we know the cross product $a\\times b$ has a length equal to the area of the parallelogram with sides $a$ and $b$.  But remember, we defined the area of the wedge product $a\\wedge b$ to also be the area of the parallelogram with sides $a$ and $b$.  Thus $\\|a\\times b\\| = \\|a\\wedge b\\|$.\nBut norm isn't everything.  Let's look at the components.  Remember that the components of the cross product (once you work it all out) is $$a\\times b= (\\color{red}{a_2b_3-a_3b_2})e_1 + (\\color{purple}{a_3b_1-a_1b_3})e_2 +(\\color{blue}{a_1b_2-a_2b_1})e_3$$ where $\\{e_1, e_2, e_3\\}$ is an orthonormal basis for $\\Bbb R^3$ or $\\Bbb L^3$.  Let's work out the components of the wedge product using our above rules: $$\\begin{align} a\\wedge b &= (a_1e_1 + a_2e_2 + a_3e_3)\\wedge (b_1e_1 + b_2e_2 + b_3e_3) \\\\ &= (a_1b_1)e_1\\wedge e_1 + (a_1b_2)e_1\\wedge e_2 + (a_1b_3)e_1\\wedge e_3 + (a_2b_1)e_2\\wedge e_1 + (a_2b_2)e_2\\wedge e_2 + (a_2b_3)e_2\\wedge e_3 + (a_3b_1)e_3\\wedge e_1 + (a_3b_2)e_3\\wedge e_2 + (a_3b_3)e_3\\wedge e_3 \\\\ &= 0 + (a_1b_2)e_1\\wedge e_2 + (-a_1b_3)e_3\\wedge e_1 + (-a_2b_1)e_1\\wedge e_2 + 0 + (a_2b_3)e_2\\wedge e_3 + (a_3b_1)e_3\\wedge e_1 + (-a_3b_2)e_2\\wedge e_3 + 0 \\\\ &= (\\color{red}{a_2b_3-a_3b_2})e_2\\wedge e_3 + (\\color{purple}{a_3b_1-a_1b_3})e_3\\wedge e_1 + (\\color{blue}{a_1b_2-a_2b_1})e_1\\wedge e_2\\end{align}$$\nSo you can see that in $\\Bbb R^3$ and $\\Bbb L^3$, the wedge product and the cross product have exactly the same components.  Thus if you wanted to create the cross product out of the wedge product you'd just need to do the operation $$\\pmatrix{e_2\\wedge e_3 \\\\ e_3\\wedge e_1 \\\\ e_1 \\wedge e_2} \\mapsto \\pmatrix{e_1 \\\\ e_2 \\\\ e_3}$$\n\nA Quick Note on Further Products\nThe dot, cross, and wedge products are not the only products we can define on Euclidean vectors.  Far from it.\nFor a more historically relevant precursor to the modern products, take a look at the Hamilton product of quaternions.\nThe two most important products on Euclidean vectors that I have not covered yet are the geometric product and the tensor product.  Both the geometric and tensor products contain the wedge product as subproducts.  However, these products require mathematical material that goes well beyond what I want to cover in this treatise and thus I will simply provide references.\nFor information on the geometric product, and the algebra (over the field of reals) that it creates, I'd recommend either of the books Linear and Geometric Algebra by Alan Macdonald or Clifford Algebra to Geometric Calculus by David Hestenes and Garret Sobczyk.  Macdonald's book is great if you've never taken a linear algebra course before.  If you have, then you can probably handle the more advanced book by Hestenes and Sobczyk.\nFor information on the tensor product I'd recommend taking a look at the book Introduction to Vectors and Tensors, Volume I by Ray Bowen and C. Wang.  The last couple chapters give a pretty good introduction to tensor algebra and if you decide to get the second volume you'll have a good text on calculus on manifolds as well.", "meta": {"post_id": 1395970, "input_score": 73, "output_score": 189, "post_title": "What is the logic/rationale behind the vector cross product?"}}
{"input": "It's clear that Mersenne primes can't end in $9$, since $2^n$ can't end in $0$, but $2^n$ can end in $4$ and $2^{n}-1$ would end in 3. From the list at http://mathworld.wolfram.com/MersennePrime.html though, there aren't any known Mersenne primes that end in $3$ besides $3$. Is that a coincidence or is it impossible for a Mersenne prime to end in $3$?", "output": "We have $2^n\\equiv 4\\pmod{ 10}$ iff $n\\equiv 2\\pmod 4$, especially $n$ is even. As $n$ itself needs to be prime, the only candidate is with $n=2$ and in that case $2^n-1=3$ gives us the only Mersenne prime ending in $3$.", "meta": {"post_id": 1396012, "input_score": 37, "output_score": 61, "post_title": "Are there any Mersenne primes, besides 3, that end in 3"}}
{"input": "For example is $\\log_{\\sin(x)}(3x)$ a ridiculous equation?\nI couldn't find an example on any page about logarithms that used a function on a base, but it seems that for an equation like $\\sin(x)^{12x}$, the log's base would have to be the sine function. Thank you for the advice!", "output": "Can a logarithm have a function as a base ?\n\nOf course not ! But, then again, $\\sin(x)$ is not a \u201cfunction\u201d ! Rather, it is the value of a function \u2014 in this case, the sine function \u2014 evaluated at point x. These are two different concepts ! Related, to be sure, but different nonetheless.\n\n\nIs $\\log_{\\sin(x)}(3x)$ a ridiculous equation ?\n\nOf course not ! In order for an expression to be a \u201cridiculous equation\u201d, it must be an \u201cequation\u201d first. But I see no equality signs there \u2014 do you ?\n\nNow that I'm done answering the questions you did ask, allow me to answer the one you never actually asked, but probably meant to all along: Yes, the mathematical expression $\\log_{\\sin x}(3x)$ $=\\dfrac{\\log(3x)}{\\log\\sin x}$ makes perfect sense, assuming x lies inside positive intervals for which $\\sin x$ is also positive.", "meta": {"post_id": 1396636, "input_score": 23, "output_score": 42, "post_title": "Can a logarithm have a function as a base?"}}
{"input": "Starting from the number $1$ we write down a sequence of numbers where the next number in the sequence is obtained from the previous one either by doubling it or rearranging its digits (not allowing the first digit of the rearranged number to be $0$) for instance a sequence might begin \n$$1,2,4,8,16,61,122,212,424,\\ldots$$\nIs it possible to make a sequence that ends in $1000000000$ and a sequence that ends in $9876543210$. Please show me how and if there is any working.", "output": "We first note that we can attain $1000$ from $1$ by $$1,2,4,8,16,32,64,128,256,512,125,250,500,1000$$\nTherefore $1000^n$ can be attained by performing the same operations as above with $1$ replaced by $1000^{n-1}$.\nOn the other hand, note that rearranging the digits does not change the remainder when divided by $3$, and $2^n \\neq 0 \\pmod{3}$. Therefore the sequence would not reach any multiple of $3$.", "meta": {"post_id": 1399055, "input_score": 33, "output_score": 71, "post_title": "Is it possible to construct a sequence that ends in 1000000000?"}}
{"input": "I am trying to understand (intuitive explanation will be fine) why determinant is a multilinear function and therefore to learn how elementary row operation affect the determinant.\nI understand that it has something to do with the definition of determinant by permutations, due to permutation being a bijection, in each product of the determinant there is just one entry from each row, but what's next?", "output": "Multilinearity of the determinant follows from Cavalieri's principle applied to n-dimensional parallelipipeds.\n\nThe determinant of a matrix measures the (n-dimensional) volume of the parallelipiped generated by the columns of the matrix:\n\nMultilinearity means that the determinant is a linear function in each column of the input matrix, independently. I.e.:\n\n$$\\det \\left( \\begin{bmatrix}{\\color{purple}\\lambda} \\mathbf{v_1} & \\mathbf{v_2} & \\dots & \\mathbf{v_n}\\end{bmatrix} \\right) = {\\color{purple}\\lambda}\\det \\left(\\begin{bmatrix} \\mathbf{v_1} & \\mathbf{v_2} & \\dots & \\mathbf{v_n} \\end{bmatrix} \\right)$$\n$$\\det \\left( \\begin{bmatrix} \\mathbf{\\color{darkgreen} u} + \\mathbf{\\color{blue} w} & \\mathbf{v_2} & \\dots & \\mathbf{v_n} \\end{bmatrix} \\right) = \\det \\left( \\begin{bmatrix} \\mathbf{\\color{darkgreen} u} & \\mathbf{v_2} & \\dots & \\mathbf{v_n} \\end{bmatrix} \\right) + \\det \\left( \\begin{bmatrix} \\mathbf{\\color{blue} w} & \\mathbf{v_2} & \\dots & \\mathbf{v_n} \\end{bmatrix} \\right),$$\n\nand similar formulas must hold for the second, third, etc.. columns.\n\nThe first property (pulling out of scalars $\\lambda$) is easy to see and already discussed in user2520938's answer. When you linearly scale a parallelipiped in a single direction, you increase its volume by the scaling factor:\n\n\nTo see that the second property holds (multilinearity under addition), translate the two parallelipipeds associated with the right hand side of 2. so that they share a lower dimensional parallelipiped as a common face (the parallelipiped defined by the shared vectors $\\mathbf{v_2},\\dots, \\mathbf{v_n}$). All the slices of this combined object have the same shape, and these slices also have the same shape as the slices of the summed parallelipiped associated with the left hand side of 2. Hence by Cavalieri's principle the parallelipipeds associated with the left and right hand sides of 2. must have the same volume:\n\n\nFor intuition about Cavalieri's principle, just think about a stack of coins. If you take a straight stack of coins and shear it in any pattern, the volume stays the same (image credit for the coin stack to wikipedia):\n\nOf course, the same argument holds when applied to any other column, hence determinant is multilinear in the columns of the input matrix.", "meta": {"post_id": 1403735, "input_score": 26, "output_score": 45, "post_title": "Why is determinant a multilinear function?"}}
{"input": "I've been studying an introductory book on set theory that uses the ZFC set of axioms, and it's been really exciting to construct and define numbers and operations in terms of sets. But now I've seen that there are other alternatives to this approach to construct mathematics like New Foundations (NF), category theory and others. \nMy question is, what was the motivation to look for an alternative for ZFC? Does ZFC have any weakneses that served as motivation? I know there are some concepts that don't exist in ZFC like the universal set or a set that belongs to itself. So I thought that maybe some branches of mathematics needed these concepts in order to develop them further, is that true?", "output": "Well. That depends on whom you might ask this.\n\nSet theory might be inconsistent. In particular $\\sf ZFC$ and its extension by large cardinal axioms. It's a nontrivial thing, to feel safe with these theories, and it takes a lot of practice and time until you understand that $\\sf ZFC$ is self-evident to some extent, and [some] large cardinal axioms are somewhat self-evident as well.\nBut of course, we might be tricked. Crooks have been known to seem honest, until you're left with a big heap of nothing in your hands. That is why Nigerian royalty is going to have a very hard time emailing people around the world.\nIf that happens, we need to ask ourselves where the problem lies. Is it in one of the axioms, or maybe specifically in the existence of infinite sets? Will a slightly weaker set theory (e.g. $\\sf ZC$) work better, or maybe we have to resort to arithmetic theories to fix things?\nOur grasp of things is usually largely inconsistent,1 but we do like to think in \"types\". So working inside different categories when needed, or working with some type theory or another. A lot, and I mean a lot, of people will twitch when you ask them what are the elements of $\\pi$. Or whether or not $\\frac13$ is a subset of $e$.\nOf course, those who have a firm understanding of this know that this is a question of implementation, and this is like asking whether or not the machine code of one implementation of an algorithm is the same or different of another. But people don't think about it this way, although in a way they kinda do.\nInstead, people focus on their math, and they just remember (or more accurately: they don't) that you can formalize this in terms of set inside set theory.\n$\\sf ZFC$ is incomplete. That doesn't sound like a big deal. But it is, in a deep sense. We might half expect a foundational theory to be able and decide all sort of things about the mathematical universe. Enough so most, or any, questions we might have can be answered there. Again, those who study foundations long enough should realize this is not the goal of a foundational theory, but it is somewhat expected.\nAnd some people feel very awkward with this incompleteness. They will also have a problem with arithmetic based foundations, since that is an incomplete foundation also. But it's true that in $\\sf ZFC$ those incompleteness phenomenon are far more gaping, and as soon as you leave the term \"countable\" and enter the \"uncountable\", more or less everything becomes open and independent.\nThis poses an actual problem. Imagine that half the functional analysts would work in one set theory (say $V=L$), and the other half in another (say $\\sf ZFC+PFA$). It would create some strange discrepancies which will eventually tear the field up. And the same goes to everything else, except set theory, whose main occupation is these different axioms.\n\nBut that's just counting three strikes against $\\sf ZFC$. And I cannot, in good conscience finish my post like this. So let's balance things as to why $\\sf ZFC$ is a good foundation.\n\nSet theory, and in particular $\\sf ZFC$ is quite self evident. When I was a starting masters student, I attended a course by one of the generation's greatest mathematician and asked him about the axioms of $\\sf ZFC$ and he said that a good foundational theory is one whose axioms you don't feel you're using. It might be that we're trained to think in \"set theory ways\", but it's also true that the axioms give you exactly the expressive power to talk about everything you care about and not care about one specific implementation over another (looking your way, Replacement axioms!), which is quite great.\nCan you imagine a mathematical universe where the reals constructed with Dedekind cuts and with Cauchy sequences are different? (Those mathematical worlds exists in other foundations, by the way, and at least to me, that is weird.)\nThe previous point leads us exactly to this argument. Humans take an abstract algorithm, implement it in various languages, on various processors, using various data structures and types. But the code and data all turn into electronic signals. So working with high level objects like $\\Bbb N$ and $\\Bbb R$, and function spaces and so on is our algorithms. And we can turn that into electronic signals, or sets and first-order structures in this case.\nWe like sets more than we are willing to admit. One of the problems of second-order logic is that the logic is incomplete.2 And if you have an algorithm for a list of inference rules, to verify a proof, then there will be statements which are true (even valid) that these inference rules cannot prove. So your proof theory is quite insufficient in this aspect. Because at least the valid sentences should be provable.\nSo we can, instead, use a first-order based foundation to fix this. Instead of proving something about $\\Bbb R$, we prove that $\\sf ZFC$ proves that thing about $\\Bbb R$; and this we can verify mechanically. So one option is to resort to some arithmetic foundation. But this causes a different problem. We cannot \"take objects\" anymore. We don't have sets of reals, or rings, or groups. So when you prove something about groups, you can't say \"Let $G$ be an abelian group, then bla bla bla\" anymore. You have to say \"The theory of abelian groups proves that bla bla bla\".\nAnd this is quite a big issue, since we think about mathematics in some material sense. The objects exists somewhere. They are not just axiomatic consequences. And set theory, in particular $\\sf ZFC$ with its implementation agnosticism, provides us with the means to do exactly this.\n\n\nFootnotes.\n(1) Is $\\Bbb N\\subseteq\\Bbb R$? Often the answer is yes, many other times the answer is no. And it really depends on what you want to do, or how you mostly use these two objects. But since we know that either method can be replaced by the other we're not worried about it too much. But it is a concrete question that has convenient answer either way, and we exploit it. And that, in a nutshell, a \"global inconsistency\" in our thinking.\nYou can argue about this, but it's going to be besides the point.\n(2) Incomplete here means in the sense of the completeness theorem. Something true in every model need not be provable. This is a different kind of incompleteness than the one referred to earlier, where $\\sf ZFC$ is incomplete in the sense that it does not prove or refute every statement.", "meta": {"post_id": 1406462, "input_score": 28, "output_score": 42, "post_title": "Motivation for different mathematics foundations"}}
{"input": "When it comes to inner product I have thus far only dealt with vectors, and so the concept is very intuitive because one can easily visualize two vectors and how they get multiplied, and it is clear why the dot product of two vectors is defined the way it is. For $v\u2217u$ it would basically be the length projection of $v$ onto $u$ (the part of $v$ in direction of $u$) multiplied by the length of $u$. So you basically have a measure of how much the vectors move in same direction.\nBut when it comes to functions, what becomes the meaning of the inner product, why is the formula the way it is and what is the intution behind it. Basically for $f(x),g(x)$ you have....\n$\\int_{a}^{b} f(x)g(x) dx$\nas the inner product for two function $f(x),g(x)$ on the interval [a,b].", "output": "\"Only dealt with vectors\", you're dealing with vectors here too! Functions are vectors, and this is an inner product on a vector space!\nReally, the integral is exactly the same thing as with the dot product. For two vectors in $\\Bbb{R}^n$, the dot product is $(x_1,...,x_n)\\cdot (y_1,...,y_n)=x_1y_1+\\cdots+x_ny_n$. \nFor functions, you can think of the dot product being the same thing! You multiply the two components and add them up! The only difference is the measure or \"weight\" of each point. Since $\\Bbb{R}^n$ is discrete, each component has weight $1$, where in these function spaces each component has weight \"$dx$\".\nSo loosely, on $\\Bbb{R}^n$:\n$$(x_1,...,x_n)\\cdot (y_1,...,y_n)=x_1y_1 \\times1+\\cdots+x_ny_n\\times 1$$ \nOn a sequence space:\n$$(x_1,...)\\cdot (y_1,...)=x_1y_1 \\times1+\\cdots$$ \nOn these function spaces (I write $f\\cdot g$ to emphasise that I mean the dot product between the functions $f$ and $g$ and not the product $f(x)g(x)$ between the numbers $f(x)$ and $g(x)$):\n$$f \\cdot g=f(a)g(a)\\times dx+f(a+dx)g(a+dx)\\times dx+\\cdots=\\int_a^b f(x)g(x) dx$$\nThe inner product on function spaces is exactly the regular dot product, just in infinite dimensions and with a different \"weight\".\nEdit, for the issue of orthogonality. The pressing issue here is that inner products define what it means to be orthogonal. So I issue a challenge to you here. Without referencing an inner product (this includes angles, as the inner product DEFINES angles), what does it mean to be orthogonal? You can't answer this question. The entire notion of angle, orthogonality, etc. are summarized in:\n$$(x_1,...,x_n)\\cdot (y_1,...,y_n)=x_1y_1 \\times1+\\cdots+x_ny_n\\times 1$$ \nEven visualizing this is difficult. What do orthogonal vectors look like in 4 dimensions for example? I certainly don't know. Could you try picturing this in infinite dimensions? I can't. \nSo what does it mean to say that the inner product of two functions is $0$? The same thing it always does, that those two functions are orthogonal! Is there a nice visualization of this? Not really. \nFor another example, with this inner product comes the norm $\\|f(x)\\|_2=\\sqrt{\\int_a^b f^2(x) dx}$. Can you draw a unit circle? That is, all functions $f:[a,b]\\to\\mathbb R$ such that $\\|f(x)\\|_2=\\sqrt{\\int_a^b f^2(x) dx}=1$?\nCertainly you can't. Things become less geometric in infinite dimensions.", "meta": {"post_id": 1414389, "input_score": 50, "output_score": 88, "post_title": "What is the geometric meaning of the inner product of two functions?"}}
{"input": "I've seen the statement \"The matrix product of two orthogonal matrices is another orthogonal matrix. \" on Wolfram's website but haven't seen any proof online as to why this is true. By orthogonal matrix, I mean an $n \\times n$ matrix with orthonormal columns. I was working on a problem to show whether $Q^3$ is an orthogonal matrix (where $Q$ is orthogonal matrix), but I think understanding this general case would probably solve that.", "output": "As an alternative to the other fine answers, here's a more geometric viewpoint:\nOrthogonal matrices correspond to linear transformations that preserve the length of vectors (isometries). And the composition of two isometries $F$ and $G$ is obviously also an isometry.\n(Proof: For all vectors $x$, the vector $F(x)$ has the same length as $x$ since $F$ is an isometry, and $G(F(x))$ has the same length as $F(x)$ since $G$ is an isometry; hence $G(F(x))$ has the same length as $x$.)", "meta": {"post_id": 1416726, "input_score": 37, "output_score": 36, "post_title": "Why is the matrix product of 2 orthogonal matrices also an orthogonal matrix?"}}
{"input": "I know that because of the birthday problem, even after 365 friends, you're going to have a lot of doubles and that there's also an infinitesimal chance that even with infinite friends that there's one day left out. But I was curious how many friends you'd need on average to have every day represented (this is ignoring leap day and assuming birthdays are equally distributed). Or to generalize it further, given n unique boxes and you're placing balls in them with an equal 1/n chance for the ball to go into any box, how many balls would you have to place on average before every box had at least one ball?", "output": "This is known as the coupon collector's problem, with the coupons in this case being birthdays.\nIf you keep adding friends one by one until you have one with each birthday, it will take 1 friend until you have the friend with one birthday. After the first friend, each new friend has a probability of $\\frac{364}{365}$ to have a birthday you haven't seen yet, and the expected number of friends to go through until you see a new birthday is the reciprocal of that probability, that is $\\frac{365}{364}$. After this, the expected number of friends until you see the third unique birthday is $\\frac{365}{363}$, and so forth.\nThe key insight here is that once you have seen, say, 42 different birthdays, the expected number of more friends you have to inspect until you see a new birthday does not depend on how many friends it took you to reach 42, so we can calculate the expectation for each next-unseen-birthday wait separately and just sum them.\nThe total expected number of friends until you have seen all birthdays is\n$$ 1+\\frac{365}{364}+\\frac{365}{363}+\\cdots+\\frac{365}{2}+\\frac{365}{1} = 365\\left(\\frac1{365}+\\frac1{364}+\\frac1{363}+\\cdots+\\frac12+1\\right)=365 H_{365} $$\nwhere $H_{365}$ is the 365th harmonic number, which is approximately $\\ln(365)+0.577$ (the added term is the Euler-Mascheroni constant).\nSo you need about $365\\cdot(\\ln(365)+0.577)=2364$ friends.\n\nIf we take leap days into account (but still assume all birthdays except February 29 are uniformly distributed and the probability of being born on Feb 29 is $\\frac{0.25}{365.25}$), things begin to get more complex.\nTo compute the expected time until all other days than February 29 have been seen, the numerators in the first sum above all become $365.25$ instead of $365$, so this expectation is $\\frac{365.25}{365}2364 \\approx 2366$. That's not much of a difference, so the main correction comes from the risk that we won't have seen any February 29 by the time all of the other days have been hit.\nWe can estimate the risk of this happening crudely as $(\\frac{365}{365.25})^{2364}\\approx 0.1982$ -- but this is not at all exact because (among other reasons) the averaging that led to 2364 does not commute with the non-linear $x\\mapsto (365/365.25)^x$ mapping.\nThe exact risk is\n$$ \\frac{365}{365.25} \\times \\frac{364}{364.25} \\times \\frac{363}{363.25} \\times \\cdots \\times \\frac{1}{1.25} = \\frac{\\Gamma(366)\\Gamma(1.25)}{\\Gamma(366.25)} \\approx 0.2073$$\nnamely, each time we wait for a new birthday the risk that the next new birthday will be non-leap.\n(To evaluate the gammas we can either cheat and use Wolfram Alpha, or use the approximation $\\Gamma(a+\\varepsilon)/\\Gamma(a)\\approx (a+\\frac\\varepsilon2-\\frac12)^{\\varepsilon}$ which gives plenty of precision here and allows us to compute the risk as $\\frac{\\Gamma(\\frac14)}{4 \\sqrt[4]{365\\frac58}}$ if only we can find $\\Gamma(\\frac14)$ in a reference work).\nSo the expected number of friends in the leap-day scenario should be\n$$ 2366 + 0.2073 \\frac{365.25}{0.25}\\approx 2669 $$", "meta": {"post_id": 1428097, "input_score": 54, "output_score": 83, "post_title": "On average, how many friends would I need to have to have at least one friend's birthday every day?"}}
{"input": "I know that given a polynomial $p(i)$ of degree $d$, the sum $\\sum_{i=0}^n p(i)$ would have a degree of $d + 1$. So for example\n$$\n\\sum_{i=0}^n \\left(2i^2 + 4\\right) = \\frac{2}{3}n^3+n^2+\\frac{13}{3}n+4.\n$$\nI can't find how to do this the other way around. What I mean by this, is how can you, when given a polynomial, calculate the polynomial which sums to it?\nFor example, if we know that \n$$\n\\sum_{i=0}^{n} p(i) = 2n^3 + 4n^2 + 2,$$\nhow can we find the polynomial $p(i)$?", "output": "To rephrase, I believe the question is this: \nSuppose that polynomials $p$ and $q$ have the property that \n$$\n\\sum_{i=0}^n p(i) = q(n)\n$$\nIf you're given $q$, how can you find $p$? \nFirst, this is a lovely question. I'd never really considered it, because we almost always study instead \"if you know $p$, how do you find $q$?\" \nTo answer though, turns out to be fairly simple. Write the following:\n\\begin{align}\nq(n-1) &= p(0) + p(1) + \\ldots + p(n-1) \\\\\nq(n) &= p(0) + p(1) + \\ldots + p(n-1) + p(n) \\\\\n \\end{align}\nNow subtract the top from the bottom to get\n\\begin{align}\nq(n) - q(n-1) &= p(n) \n \\end{align}\nAs an example, in your case if we knew \n$$\nq(n) = 2n^3 + 4n^2 + 2\n$$\nwe'd find that \n$$\np(n) = q(n) - q(n-1) = 2n^3 + 4n^2 + 2 - [2(n-1)^3 + 4(n-1)^2 + 2],\n$$\nwhich you simplifies to\n$$\np(n) = 6n^2 +2n - 2.\n$$\nLet's do an example: we know that for $p(n) = n$, we have $q(n) = \\frac{n(n+1)}{2}$. So suppose we were given just $q$. We'd compute\n\\begin{align}\nq(n) - q(n-1) \n&= \\frac{n(n+1)}{2} - \\frac{(n-1)(n)}{2} \\\\\n&= \\frac{n^2 + n}{2} - \\frac{n^2 - n}{2} \\\\\n&= \\frac{n^2 + n-(n^2 - n)}{2} \\\\\n&= \\frac{n^2 + n- n^2 + n}{2} \\\\\n&= \\frac{2n}{2} \\\\\n&= n,\n\\end{align}\nso that $p(n) = n$, as expected. \nNote: As written, I've assumed that $p$ and $q$ are both polynomials. But the solution shows that if $q$ is a polynomial, then $p$ must also be a polynomial, which is sort of pleasing. \nPost-comment remarks\nAs @Antonio Vargas points out, though, there's an interesting subtlety: \nI've given a correct answer to my revised question, which was \"If there are polynomials $p$ and $q$ satisfying a certain equality, then how can one find $p$ given $q$.\" \nBut suppose that there is no such polynomial $p$. My answer still computes an expression which $p$, if it existed, would have to match. But since no such $p$ exists, the computed expression has no value. \nOr maybe I should say that it has a limited value: you can take the polynomial $p$ and compute its sum using inductive techniques and see whether you get $q$. If so, that's great; if not, then there wasn't any answer in the first place. \nFortunately, you can also do that \"Does it really work\" check much more simply. You just need to check the the $n = 0$ case: if \n$$\n\\sum_{i = 0}^0 p(i) = q(0)\n$$\nthen all higher sums will work as well. And this check simplifies to just asking: is\n$$\np(0) = q(0)?\n$$\nIn our example, $p(0) = -2$, while $q(0) = +2$, so it doesn't work out.", "meta": {"post_id": 1431042, "input_score": 31, "output_score": 39, "post_title": "How do you calculate a sum over a polynomial?"}}
{"input": "I've got an expression which is sum of products like:\n\n$$a_1 a_2 + b_1 b_2 + c_1 c_2 + \\cdots,$$\n\nbut the real problem I'm solving could be easily solved if I could convert this expression into something like :\n\n$$(a_1+b_1+c_1+\\cdots) \\cdot (a_2+b_2+c_2+\\cdots)$$\n\nIf some additional constants are added or subtracted, it's no problem, in fact, it's obvious.\nFirst query: Can we convert it into something like required form?\nSecond query: If yes, how?", "output": "In general, one can write a product of sums as a sum of a products:\n$$\\left(\\sum_{i \\in I} x_i\\right)\\left(\\sum_{j \\in J} y_j \\right) = \\sum_{i \\in I, j \\in J} x_i y_j.$$\nOne cannot, however, in general reverse this process, that is, write a sum of products, $$\\phantom{(\\ast) \\qquad} \\sum_{k \\in K} x_k y_k, \\qquad (\\ast)$$ as a product of sums. (In this answer we assume that the index sets, $I, J$, etc., are finite, though with some care we can extend them to infinite sets under suitable conditions.) Note that the factors $x_k, y_k$ of each summand in $(\\ast)$ are indexed by the same set $K$, whereas that is not (generally) the case for the sum of products in the first display equation. When they are, we can write the sum of products in terms of a product of sums with a correction term, namely as\n$$\\sum_{k \\in K} x_k y_k = \\left(\\sum_{k \\in K} x_k\\right) \\left(\\sum_{k' \\in K} y_{k'}\\right) - \\sum_{k, k' \\in K; k \\neq k'} x_k y_{k'},$$\nbut this is really just a reorganization, and not really an algebraic simplification.\nThe expression $(\\ast)$ can be factored in the sense that it is precisely the standard \"dot product\" on the space $\\oplus_{k \\in K} R$ of ordered $|K|$-tuples (with components indexed by some finite set $K$) of ${\\bf x} := (x_k)$ and ${\\bf y} := (y_k)$ with entries in some ring $R$,\n$${\\bf x} \\cdot {\\bf y} := \\sum_{k \\in K} x_k y_k,$$\nthough as the notation suggests, this is a definition and again not an simplification per se.", "meta": {"post_id": 1433059, "input_score": 28, "output_score": 40, "post_title": "Can we express sum of products as product of sums?"}}
{"input": "Suppose $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is continuous.\nIs it possible for $f$ to assume each value in its range an even number of times?\nTo clarify, some values might be taken 0 times, some 2, some 4, etc., but always an even (and therefore finite) number.\nI don't require that there be a value that is assumed any particular number of times. \nFor example, the function might be surjective, or never take on any value exactly twice.\nThis question is possibly the same as A continuous function cannot take every value an exact even number of times?.\nThat question may instead have meant, \"If $n$ is even, is it possible for $f$ to assume every value in its range exactly $n$ times?\" \nIn any event, it has not been answered.", "output": "Yes, this is possible.  Define $f$ as follows.  For $x\\leq 0$, $f(x)=-x$.  If $n\\in\\mathbb{N}$, then \n$$f(n+x)=\\begin{cases} n+3x &\\text{ if }0\\leq x\\leq 1/3 \\\\ n+2-3x &\\text{ if }1/3\\leq x\\leq 2/3 \\\\ n-2+3x &\\text{ if }2/3\\leq x\\leq 1 \\end{cases}$$\nA glance at a graph shows that $f$ is continuous and achieves every positive value four times, achieves the value $0$ twice, and is never negative.\n\nOn the other hand, it is impossible for every point in the range of $f$ to have the same even number of preimages.  For a contradiction, suppose $n$ is an even integer and $f:\\mathbb{R}\\to\\mathbb{R}$ achieves every value in its range exactly $n$ times.  Fix some value $a\\in f(\\mathbb{R})$ and let $x_1<x_2<\\dots<x_n$ be the preimages of $a$.  Let $p$ be the number of $x_i$ which are local minima of $f$, $r$ be the number of $x_i$ which are local maxima of $f$, and $q$ be the number of $x_i$ which are neither.  Then it follows from the intermediate value theorem that if $\\epsilon>0$ is sufficiently small, $f$ achieves the value $a+\\epsilon$ at least $2p+q$ times near the $x_i$ and $f$ achieves the value $a-\\epsilon$ at least $2r+q$ times near the $x_i$.  Thus $2p+q\\leq n$ and $q+2r\\leq n$.  But $p+q+r=n$, and so adding these two inequalities together we find that actually $2p+q=n=2r+q$ and thus $p=r$.  Since $n$ is even, this implies $q=n-p-r$ is also even.\nThat is, there are an even number of $x_i$ at which $f(x)-a$ changes sign.  Thus $f(x)-a$ has the same sign on both components of $\\mathbb{R}\\setminus [x_1,x_n]$.  Suppose WLOG that $f(x)-a$ is always positive on $\\mathbb{R}\\setminus [x_1,x_n]$.  It follows that $f$ has a global minimum value $b$ which it achieves somewhere on $[x_1,x_n]$.\nBut now replace $a$ by $b$ and repeat the argument above.  Every preimage of $b$ must be a local minimum, so $p=n$, which is clearly impossible.  This contradiction means that no such $f$ can exist.", "meta": {"post_id": 1439689, "input_score": 32, "output_score": 47, "post_title": "Can a continuous function from the reals to the reals assume each value an even number of times?"}}
{"input": "I recently asked a question and got a great answer that involved proving that \"X is almost surely one of the roots of P\".\nI know (now) that \"almost surely\" means \"with probability 1\", but I've never understood why that phrase exists.\nWhen something has a probability of 1, it's going to happen, no almost about it. What's the story there?  I've tried looking this up online, but I just got more definitions, not meaningful explanations.", "output": "In terms of the sample space of events $\u03a9$, an event $E$ happens almost surely if $P(E) = 1$, whereas an event happens surely if $E=\u03a9 $. \nAn example: suppose we are (independently) flipping a (fair) coin infinitely many times. The event\n$$\\{ \\text{I will get heads infinitely often}\\}$$ is an almost sure event (because it is possible get only a finite number of heads...but how likely is that? Rigorous proof uses Borel Cantelli, if you are interested)\nIn contrast, $$\\{\\text{ I will get heads or tails on my 16th flip} \\}$$ must happen. This is a sure event.", "meta": {"post_id": 1443015, "input_score": 34, "output_score": 34, "post_title": "Why do we say \"almost surely\" in Probability Theory?"}}
{"input": "I am an undergrad, math major, and I had basic combinatorics class using Combinatorics and Graph Theory by Harris et al before (undergrad level). Currently reading Stanley's Enumerative Combinatorics with other math folks. We have found this book somewhat challenging, especially the exercises~ Do you have any suggestions on other books to read? Or books to help going through Enumerative Combinatorics?", "output": "Here is a somewhat haphazard list of sources on algebraic combinatorics which appear to be suited to undergraduates (I have not personally read most of them, so I am making semi-educated guesses here). My notion of \"algebraic combinatorics\" includes such things as binomial coefficient identities, symmetric functions, lattice theory, enumerative problems, Young tableaux, determinant identities; it does not include graph theory (except for its most algebraic parts) or extremal combinatorics.\nGeneral remarks:\n\nCombinatorics is a living subject, and so are the authors of many of the sources listed below. If you find errors, do let them know!\n\nIf some of the links below are inaccessible, try adding https://web.archive.org/web/*/ before the link. For example, https://www.whitman.edu/mathematics/cgt_online/cgt.pdf would become https://web.archive.org/web/*/https://www.whitman.edu/mathematics/cgt_online/cgt.pdf. This will take you to an archived version of the link (assuming that archive.org has made such a version).\n\n\nTextbooks/notes on algebraic combinatorics in general:\nStanley's EC (Enumerative Combinatorics) is supposed to be a challenging read for graduate students. In its (rather successful) attempt at being encyclopedic, it has very little space for details and leaves a lot to the reader. There are many other texts on combinatorics, and I suspect that the average among them will be easier to read than EC1 (although probably less \"from the horse's mouth\"). In no particular order:\n\nJeremy Martin, Lecture Notes on Algebraic Combinatorics. This covers posets, matroids, hyperplane arrangements, symmetric functions, among other things.\n\nRichard Stanley, Algebraic Combinatorics. This is still Stanley, but now explicitly writing for undergrads. Unlike EC1-2, this is not trying to survey everything, and so what it does is given more space. The 2nd edition is out already.\n\nMiklos Bona, A Walk Through Combinatorics, 4th edition 2017. This is a student of Stanley. Errata.\n\nEdward A. Bender and S. Gill Williamson, Foundations of Combinatorics with Applications, 2006. This covers both enumeration and graph theory; the specific choice of topics appears tailored to computer scientists.\n\nBruce Sagan, Combinatorics: The Art of Counting. This is a draft of a book that has been published by AMS in 2020. It is meant for readers \"at the advanced undergraduate or beginning graduate level\" and appears to assume some knowledge of abstract algebra. Errata.\n\nAnna de Mier, Lecture notes for \"Enumerative Combinatorics\", 2004. (Alternative link via archive.org.)\n\nGeir Helleloid, Algebraic combinatorics (fall 2008). Rough draft of a grad-level course with some newer results. No longer easily found online, but downloadable from gen.lib (as many other books that are not easily found online...).\n\nKenneth P. Bogart, Combinatorics Through Guided Discovery. (Specifically, the 2017 version is the most up-to-date so far.) This is a heavily introductory text, written as a collection of exercises with ample hints and motivation. (Yes, there is a version with solutions in the tar.gz source archive.) I have seen lots of people use this text in classes, so I assume it's good.\n\nNicholas A. Loehr, Bijective Combinatorics, 2011. See his website for errata. What little I've seen of this book is great, and I have used it in my teaching. It is one of very few sources defining formal power series properly. There is a 2nd edition out, which is just called Combinatorics and seems to feature new sections on quasisymmetric functions as well as shuffled-around material on power series; your mileage may vary as to whether that update is worth the money.\n\nDavid R. Mazur, Combinatorics: A Guided Tour, MAA 2010. An introduction into various kinds of combinatorics (including both counting and graph theory). Claims to be graduate-level, although I would place it at (late) undergraduate level based on its content. I have heard good things about it.\n\nDrew Armstrong, Discrete mathematics, 2019. An introductory undergraduate class that includes the basics of enumerative combinatorics (up until Pr\u00fcfer codes for trees) and of graph theory, probably quite appropriate as an appetizer before most of the other texts here. Armstrong writes in an intriguing conversational way that exposes not just proofs but also motivations and thought processes (as well as tidbits of tangential context).\n\nDavid Guichard, An Introduction to Combinatorics and Graph Theory, 2017.\n\nAlexander Hulpke, Combinatorics is another fresh set of notes for a combinatorics class (2 semesters, graduate).\n\nNicolaas Govert de Bruijn, J. W. Nienhuys, Ling-Ju Hung, Tom Kloks, de Bruijn's Combinatorics. These are notes from a class by de Bruijn himself. Might be terse in places, but probably worth it for the informal classroom style. (The only useful file I've ever found on viXra...)\n\nPeter J. Cameron, Notes on Combinatorics; see also the rest of his notes, including the \"St Andrews Notes on Advanced Combinatorics\" (errata to part 1). Unfortunately, this has the classical British terseness as far as the proofs are concerned; I wouldn't recommend it for self-learning.\n\nGary MacGillivray, Course Notes Math 422 Combinatorial Mathematics, 2011.\n\nTorsten Ueckerdt, Lecture notes Combinatorics, scribed by Stefan Walzer; see also the class site. (Old version from 2015.) These notes are more suited for grad students due to their brevity, but an undergrad will likely find something of interest in there. Unofficial errata.\n\nUlrich Dempwolff, Algebraic Combinatorics. These, too, are lecture notes for a more graduate audience.\n\nRichard A. Brualdi, Introductory Combinatorics, 5th edition 2010. See his website for errata. From what I've seen, a thorough and detailed text with a classical focus (enumeration, flows&cuts, designs).\n\nMartin Aigner, A course in enumeration, Springer 2007.\n\nJ. H. van Lint, R. M. Wilson, A Course in Combinatorics, 2nd edition, CUP 2001. In the comments, Brian M. Scott warns that this is \"graduate level and not easy going\".\n\nJoseph P. S. Kung, Gian-Carlo Rota, Catherine H. Yan, Combinatorics: The Rota Way, CUP 2009 is a nonstandard textbook, originating from Rota's MIT classes and reflecting his interests (many of which have never hit the combinatorial mainstream). Errata and a sample chapter.\n\nCarl G. Wagner, Basic Combinatorics. This looks like another introductory set of notes for (enumerative) combinatorics, with a somewhat algebraic touch (generating functions, difference operators and linear recursions are major topics). Also, the first chapter of Carl G. Wagner, Choice, Chance and Inference (a textbook on probability) by the same author treats the very basics of enumerative combinatorics. Finally, the recent book Carl G. Wagner, A First Course in Enumerative Combinatorics (AMS 2020) by the same author goes deeper.\n\nDavid G. Wagner, C&O 330: Introduction to Combinatorial Enumeration.\n\nDavid G. Wagner's website (the link goes to archive.org) is a treasure trove of old material, although navigating it on archive.org can be a pain. Includes lecture notes by Chris Godsil.\n\nStephan Wagner, Combinatorics is yet another set of notes. Generating functions appear to be the red thread here. Seems terse, though.\n\nIan Goulden, Math 249 notes is more detailed than I would have expected given its length and coverage. The first half is an introduction to the uses (including some rather advanced ones) of generating functions; the second is devoted to graph theory.\n\nGregory G. Smith's Math 402 Fall 2019 lecture notes cover elementary counting techniques and generating functions.\n\nDavid Galvin's Math 60610 Spring 2017 lecture notes mostly on enumerative combinatorics. Currently, a more up-to-date version can be found on my website (but probably will go back to David's eventually). This is a grad-level course that should work for sufficiently cultured undergraduates as well.\n\nKevin Purbhoo's 630 notes (\"Algebraic Enumeration\") introduce various more advanced topics such as species, symmetric functions, representations of symmetric groups and cycle index functions.\n\nMelody Chan, Introduction to Higher Mathematics: Combinatorics and Graph Theory (scroll down) looks like a nice little introduction.\n\nMichael Tait, Lecture Notes for 21-301: Combinatorics, 2017 version and 2018 version and 2021 version. Most of this is about extremal combinatorics, but the first 30 or so pages of the 2017 version are a short introduction to enumeration.\n\nMichel Goemans, Math 18.310A (2015) is a rather eclectic discrete mathematics class; it includes notes on counting and on generating functions.\n\nStephen Melczer, An Invitation to Enumeration, 2023, an HTML/KaTeX set of notes.\n\nIn Fall 2019, I started writing lecture notes on enumerative combinatorics with the eventual goal of covering all the basics with Bourbakist rigor while remaining reasonably readable. At the moment, the first two chapters are finished; more is to come when I find some time. In Fall 2022, I have also written less detailed notes that, however, go deeper into the subject. I have also started writing introductory lecture notes on algebraic combinatorics in Spring 2021. None of these are fully comprehensive, but together (even excluding the unfinished parts) they should cover a lot. (Also, somewhat informal graph theory notes from Spring 2022.)\n\n\nI don't know these books/notes well enough to tell which of them are better suited for a first course (although I don't have any reasons to suspect any of them to be unsuitable), but it cannot hurt to try each of them and go as far as you can before meeting serious resistance. (And once you meet serious resistance, either keep going or try the next one.) Half of these are freely available (and so are the other half, if you search in the darker places).\nSpecific subjects:\nJust a few so far...\nEnumeration:\n\nMiklos Bona, Introduction to Enumerative Combinatorics, 2007. This is another Bona book, and explicitly directed at undergraduates, though it does percolate into some advanced topics as well (unimodality, magic square enumeration). A 2nd edition has come out in 2016 under the extended title Introduction to Enumerative and Analytic Combinatorics (adding, as the name change suggests, a chapter on analytic combinatorics). Errata.\n\nTero Harju, Combinatorial Enumeration (Fall 2011). This is a short introductory class with lecture notes and exercises.\n\nAlex Fink, Enumerative Combinatorics (Fall 2015). Another short course, doing some of the nicest stuff (in my opinion). The writing is enjoyable, but perhaps too terse for an undergraduate reader.\n\nArthur Benjamin and Jennifer Quinn, Proofs that Really Count. This looks rather introductory (focusing on proofs of identities involving binomial coefficients or Fibonacci numbers using bijections). (Suggested by JSchlather.)\n\nTitu Andreescu and Zuming Feng, A Path to Combinatorics for Undergraduates: Counting Strategies. Another introduction to enumerative combinatorics. This one takes a problem-solving approach, illustrating principles on olympiad-style problems. (Suggested by Marko Amnell.)\n\nChen Chuan-Chong and Koh Khee-Meng, Principles and Techniques in Combinatorics is another text that approaches the subject through olympiad problems.\n\nDominique Foata and Guo-Niu Han, Principes de Combinatoire Classique, 2008. These are the notes (in French) of a \"cursus de la ma\u00eetrise\" (equivalent to first year of master's degree) at Strasbourg; they include several subjects not commonly found in introductions (Eulerian polynomials, Lagrange inversion, symmetric functions). Foata is one of the major names in combinatorial enumeration.\n\nDominique Foata and Guo-Niu Han, The q-series in combinatorics; permutation statistics is a text on $q$-enumeration, focusing on its permutations. Some symmetric function theory included.\n\nRonald L. Graham, Donald E. Knuth, Oren Patashnik, Concrete Mathematics, 2nd edition 1994 (errata). This one is really sui generis and not fully a combinatorics book; it discusses elementary number theory, several important integer sequences, binomial coefficients and their multiple identities (unlike many texts, the focus here is less on bijective proofs and more on algebraic tricks), generating functions and some probability.\n\nDonald E. Knuth, The Art of Computer Programming was started in 1962 as an attempt at a comprehensive textbook for programming. Four volumes are out by now, thus giving computer science its own Song of Ice and Fire to wait on. Combinatorics (enumerative and algorithmic and occasionally even algebraic) is everywhere dense in them (at least in Volumes 1, 3 and 4A), and Knuth's propensity to wildly curious digressions (one gets the impression that he even digresses from digressions) makes these books an incredibly addictive nerd-read. Volume 3 is the one most relevant to algebraic combinatorics, with its \u00a75.1 devoted to permutations and tableaux. Few authors have dug as deep as Knuth into the history of the subject -- witness a draft of \u00a77.2.1.7.\n\nHerbert S. Wilf, generatingfunctionology, on enumeration using generating functions. This is the 2nd edition. The newer 3rd edition can be bought from Amazon. (Suggested by Fredrik Meyer.)\n\nAndr\u00e9s Aranda, Manuel Bodirsky, Combinatorics. These are notes for what would probably be a late-undergrad topics class (in Germany, a third year bachelor course). They cover flows/cuts, probabilistic method, Ramsey theory, generating functions. There is a more elementary prequel (Diskrete Strukturen) in German. Both are works in progress and welcome corrections.\n\nDavid M. Bressoud, Proofs and Confirmations: The Story of the Alternating-Sign Matrix Conjecture. This is a nonstandard introduction to algebraic combinatorics which, instead of covering the basics breadth-first, takes aim at a recent result (the alternating-sign matrix conjecture), introducing everything necessary for it along the way. (Errata.)\n\nPeter J. Cameron, Notes on Counting goes deeper into enumeration than his above-mentioned Notes on Combinatorics. A draft version is available online.\n\nJay Pantone, Graduate Combinatorics, 2016 lecture notes. This is about generating functions, including some advanced methods. Not so much written for undergraduates (see the title), but probably a good second confrontation with power series.\n\nQiaochu Yuan, Topics in generating functions. What the title says. Written for maths olympiad trainees, thus terser and less theoretical but also more eclectic than the usual texts, with various examples not commonly found.\n\nMarkus Fulmek, VO Combinatorics, 2018. Graduate topics course covering species, asymptotics and posets.\n\n\nYoung tableaux and representations of symmetric groups:\n\nWilliam Fulton, Young tableaux, CUP 1997. Part I and Appendix A are purely combinatorial, and (in my opinion) the most readable source on a lot of semistandard tableau theory.\n\nThe papers of Marc van Leeuwen (mostly on Young tableaux) have a significant expository component.\n\nBruce Sagan, The Symmetric Group, 2nd edition 2001. See also the errata on Sagan's website. This is somewhere between undergraduate and graduate in level, and covers symmetric functions, Young tableaux and representations of the symmetric group. I can recommend this. (There is surprisingly little intersection with Fulton's book.) Sagan's papers, including this expository one on Young tableaux, are also worth a mention.\n\nMark Wildon's lecture notes include some on symmetric functions (errata) and some on symmetric groups (highly recommended). (He is also writing a textbook on enumerative combinatorics, and the first two chapters are available.)\n\nAnthony Mendes, Jeffrey Remmel, Counting with Symmetric Functions is a treatment of the symmetric functions and the RSK algorithm that claims to be a graduate text, probably because it uses some abstract algebra; but it doesn't seem particularly unforgiving.\n\nEric Egge, An Introduction to Symmetric Functions and their Combinatorics, AMS 2019. Recent introduction to the subject, using mostly combinatorial methods. Requires rather little algebra, but the combinatorics can become somewhat vertiginous. Suited for advanced undergraduates or graduates.\n\nD. Laksov, A. Lascoux, P. Pragacz, and A. Thorup, The LLPT notes. This is a fairly readable introduction to the combinatorial part of Schubert calculus and the work of Lascoux and others. (Don't worry about the missing sections; nothing depends on them.)\n\nClaudio Procesi, Lie Groups -- An Approach through Invariants and Representations, Springer 2007. This is probably the most combinatorial textbook ever written on Lie groups (particularly, Chapters 9, 12 and 13).\n\nHanspeter Kraft and Claudio Procesi, Classical Invariant Theory -- A Primer. I made an unofficial list of errata for this long ago. It is partly a precursor of the Procesi book above, so expect quite some intersection.\n\nTullio Ceccherini-Silberstein, Fabio Scarabotti, and Filippo Tolli, Representation theory of the symmetric groups, CUP 2010. A slow-paced introduction to symmetric functions and representations of symmetric groups, taking a rather nonstandard approach. (Particularly recommended due to the inclusion of partition algebras, which are currently underexposed in textbooks.)\n\nAlistair Savage, Modern Group Theory is a set of lecture notes in development, following the Ceccherini-Silberstein-Scarabotti-Tolli book on representations of the symmetric groups. (Other notes by same author.)\n\nAnders Bj\u00f6rner and Francesco Brenti, Combinatorics of Coxeter groups, Springer 2005 (downloadable); see also the errata. This is a graduate text, but it starts off with rather elementary things.\n\nGeordie Williamson, Mind your $P$- and $Q$-symbols, Honours thesis 2003. This is actually an introduction into Coxeter groups, the RSK algorithm, and Hecke algebras, written by someone who went on to prove one of the main conjectures in the latter field.\n\n\nMonoids:\n\nBenjamin Steinberg, The representation theory of finite monoids, Springer 2016 (see also a public draft). You might not regard monoid/semigroup theory as combinatorics, but this book crosses over into combinatorics several times (particularly Chapters 7 and 15).\n\nTero Harju, Lecture Notes on Semigroups, 1996 and Tero Harju, Lecture Notes on Semigroups, 2017.\n\n\nCombinatorics on words:\n\nJ. Berstel, J. Karhum\u00e4ki, Combinatorics on Words -- A Tutorial.\n\nLothaire's Combinatorics on Words trilogy. The 2nd and 3rd volume are downloadable in PS format from this link; the 1st can be found elsewhere. These are edited volumes -- usually with a different author per chapter; but significant parts of them can be used as texts.\n\nJean Berstel, Aaron Lauve, Christophe Reutenauer, Franco Saliola, Combinatorics on Words: Christoffel Words and Repetitions in Words.\n\nDominique Perrin and Antonio Restivo, Enumerative Combinatorics on Words, a chapter from the \"Handbook of Enumerative Combinatorics\" (edited by Mikl\u00f3s B\u00f3na, CRC 2015). Unlike many other chapters of that Handbook, this one seems self-contained, and is the only exposition of many newer results around Lyndon words and the Gessel-Reutenauer transform. Some unofficial errata.\n\n\nChipfiring aka sandpiles:\nChip-firing is ostensibly about (a certain \"game\" on) graphs, but once you start studying it, algebraic structures quickly emerge. Thus, the subject is beloved by many combinatorialists that don't usually study graphs.\n\nAlexander E. Holroyd, Lionel Levine, Karola M\u00e9sz\u00e1ros, Yuval Peres, James Propp and David B. Wilson, Chip-Firing and Rotor-Routing on Directed Graphs, arXiv:0801.3306v4. This is a well-written (if somewhat terse) introduction; I learnt chip-firing here. Errata to an older version (some still apply).\n\nScott Corry, David Perkinson, Divisors and Sandpiles: An Introduction to Chip-Firing, AMS 2018 is an introduction written with undergraduate readers in mind. Draft version.\n\nCaroline J. Klivans, The Mathematics of Chip-firing, CRC Press 2019 is new and freely available. It covers various generalizations of chip-firing as well.", "meta": {"post_id": 1454339, "input_score": 35, "output_score": 80, "post_title": "Undergrad-level combinatorics texts easier than Stanley's Enumerative Combinatorics?"}}
{"input": "It is known that the set of non trivial zeros is an infinite set. But is it known if it is a countable, or uncountable infinite set?", "output": "If the set $Z$ of zeroes of $\\zeta(s)$ were uncountable, then it would have an accumulation point. Now, by certain version of identity theorem, this implies that $\\zeta(s)$ is identically zero on its domain, which is absurd.", "meta": {"post_id": 1491324, "input_score": 13, "output_score": 41, "post_title": "Are the nontrivial zeroes of the Riemann zeta function countable?"}}
{"input": "Problem : \nSolve $\\displaystyle \\int \\frac{e^{3x}+1}{e^x+1}dx$.\nAttempt :\nSince we have \n\\begin{align}\n\\int\\frac{e^{3x}+1}{e^{x}+1}dx & =\\int\\frac{e^{3x}}{e^{x}+1}dx+\\int\\frac{1}{e^{x}+1}dx\n\\end{align}\nand let $u=e^{x}+1$, then $e^{x}=u-1$ and $du=e^{x}dx$ \n\\begin{align}\n\\int\\frac{e^{3x}}{e^{x}+1}dx & =\\int\\frac{e^{2x}}{e^{x}+1}e^{x}dx\\\\\n & =\\int\\frac{\\left(u-1\\right)^{2}}{u}du\\\\\n & =\\int u-2+\\frac{1}{u}du\\\\\n & =\\frac{1}{2}u^{2}-2u+\\ln\\left(u\\right)\\\\\n & =\\frac{1}{2}\\left(e^{x}+1\\right)^{2}-2\\left(e^{x}+1\\right)+\\ln\\left(e^{x}+1\\right)+C_{1}\n\\end{align}\nOn the other hand, for the other part of the integral, let $v=e^{x}$,\nthen $dv=e^{x}dx$ and we have \n\\begin{align}\n\\int\\frac{1}{e^{x}+1}dx & =\\int\\frac{1}{e^{x}\\left(e^{x}+1\\right)}e^{x}dx\\\\\n & =\\int\\frac{1}{v\\left(v+1\\right)}dv\\\\\n & =\\int\\frac{1}{v}dv-\\int\\frac{1}{v+1}dv\\\\\n & =\\ln\\left(v\\right)-\\ln\\left(v+1\\right)\\\\\n & =x-\\ln\\left(e^{x}+1\\right)+C_{2}\n\\end{align}\nHence we have \n\\begin{align}\n\\int\\frac{e^{3x}}{e^{x}+1}dx & =\\frac{1}{2}\\left(e^{x}+1\\right)^{2}-2\\left(e^{x}+1\\right)+\\ln\\left(e^{x}+1\\right)+C_{1}+x-\\ln\\left(e^{x}+1\\right)+C_{2}\\\\\n & =\\frac{1}{2}e^{x}-e^{x}+x+C\n\\end{align}\nQuestion\nI think I have solved it correctly but I am wondering if this method is too complicated computationally. I am looking for a better way to attack this integral.", "output": "Hint: you can use the formula for sum of 2 cubes, since $e^{3x}=(e^x)^3$\n$$e^{3x}+1=(e^{2x}-e^x+1)(e^x+1)$$\nso you can cancel the denominator and have an easy integral left over.", "meta": {"post_id": 1503600, "input_score": 14, "output_score": 40, "post_title": "Alternative way to do this indefinite integral?"}}
{"input": "Can somebody please explain me the difference between linear transformations such as epimorphism, isomorphism, endomorphism or automorphism?\nI would appreciate if somebody can explain the idea with examples or guide to some good source to clear the concept.", "output": "For any algebraic structure, a homomorphism preserves the structure, and some types of homomorphisms are:\n\nEpimorphism: a homomorphism that is surjective (AKA onto)\nMonomorphism: a homomorphism that is injective (AKA one-to-one, 1-1, or univalent)\nIsomorphism: a homomorphism that is bijective (AKA 1-1 and onto); isomorphic objects are equivalent, but perhaps defined in different ways\nEndomorphism: a homomorphism from an object to itself\nAutomorphism: a bijective endomorphism (an isomorphism from an object onto itself, essentially just a re-labeling of elements)\n\nNote that these are common definitions in abstract algebra; in category theory, morphisms have generalized definitions which can in some cases be distinct from these (but are identical in the category of vector spaces).\nSo a linear transformation $A\\colon\\mathbb{R}^{n}\\to\\mathbb{R}^{m}$ is a homomorphism since it preserves the vector space structure (vector addition, scalar addition and multiplication, scalar multiplication of vectors), e.g. $A(av+w)=aA(v)+Aw$. It is an epimorphism if its image is $\\mathbb{R}^{m}$, a monomorphism if it has zero kernel, an endomorphism if $n=m$, and an automorphism (as well as an isomorphism) if all of these are true.\nThe below figure might be helpful. More details here.", "meta": {"post_id": 1510769, "input_score": 27, "output_score": 38, "post_title": "Difference between epimorphism, isomorphism, endomorphism and automorphism (with examples)"}}
{"input": "I am a little confused about what exactly are the difference(s) between simplicial complex, $\\Delta$-complex, and CW Complex.\nWhat I roughly understand is that $\\Delta$-complexes are generalisation of simplicial complexes (without the requirement that the intersection of two simplicial complexes is another simplicial complex), and CW Complex further generalises that (how?).\nAny explanation will be greatly appreciated.\nThanks for any enlightenment!", "output": "Simplicial complexes, $\\Delta$-complexes, and CW-complexes are all constructed by gluing together simplices.  However, for each one, there are different rules for what kinds of \"gluings\" you are allowed to use.\nFor CW-complexes, you are allowed to use almost any gluing.  Specifically, a CW-complex is constructed by induction, where at each step, you adjoin a new simplex by gluing its boundary to the complex you have already by any map.  More explicitly, if $Y$ is the CW-complex you have built so far and $f:\\partial \\Delta^n\\to Y$ is any continuous map, you can build a CW-complex $X=Y\\sqcup\\Delta^n/{\\sim}$, where $\\sim$ is the equivalence relation that identifies any $x\\in\\partial\\Delta^n$ with $f(x)\\in Y$.  The only restriction to this gluing process is that you have to add simplices in increasing order of dimension.  That is, you have to start with all the $0$-simplices, then glue in all the $1$-simplices, then glue in all the $2$-simplices, and so on.  You're not allowed to glue in a new $1$-simplex once you've already added a $2$-simplex.  (If you drop this ordering condition, you get a more general notion, which is sometimes called simply a \"cell complex\".)\nFor $\\Delta$-complexes, you do the same thing, except that the maps $f$ you can use when adding a new cell are highly restricted.  Specifically, for each $(n-1)$-simplex $A$ which is a face of $\\partial\\Delta^n$, the restriction of $f$ to $A$ must be equal to the inclusion of one of the $(n-1)$-simplices you already have.  That is, $f$ maps the $n$ vertices of $A$ (with their canonical ordering) to the $n$ vertices of some $(n-1)$-simplex you've already added to your complex (with the same ordering on the vertices), and $f$ extends to all of $A$ by just interpolating linearly.  Intuitively, this means that your complex is a union of simplices which are glued together by just gluing their faces together in the \"obvious\" linear way (for instance, as one encounters in a triangulation of a surface), rather than by arbitrary complicated continuous maps.  Note, however, that some faces of a single simplex might get glued to each other: the restriction on what $f$ can be only applies to each face of $\\partial\\Delta^n$ separately.  So, for instance, you can start with a single vertex, and then add an edge both of whose boundary vertices are the one vertex you started with (this gives you a circle).  You could then add a triangle such that each of its three sides are equal to the one edge you have (this gives a space which cannot be embedded in $\\mathbb{R}^3$ and is rather hard to visualize!).\nFinally, simplicial complexes are $\\Delta$-complexes which satisfy even more restrictions.  First, $f$ is required to map different faces of $\\partial\\Delta^n$ to different $(n-1)$-simplices, so the situation discussed at the end of the previous paragraph cannot happen.  In addition, you are not allowed to add two different $n$-simplices with the same set of vertices, so that a simplex in a simplicial complex is uniquely determined by its set of vertices (which, by the first requirement, you can show are all distinct).", "meta": {"post_id": 1528005, "input_score": 101, "output_score": 151, "post_title": "Simplicial Complex vs Delta Complex vs CW Complex"}}
{"input": "On the most recent Seton Hall Joseph W. Andrushkiw Competition, the final question was as follows:\n\nLet $A = (\\sqrt{3}+\\sqrt{2})^{2016}$. When A is written in decimal\n  form, what is its $31^{st}$ digit after the decimal point?\n\nBrute forcing it via wolfram alpha reveals that the answer is [edit: I found the 31st number from the start, not the 31st after the decimal point] zero, yet this competition does not allow the use of a calculator. It seems to me that as irrational numbers are in the base of the exponent, there should not be an identifiable pattern in the digits. \nSearching this site has made me think that perhaps the answer has something to do with the Euler phi function (something which I will admit up front I have never been acquainted with), but I can't find anything which I understand enough to give me a concrete way to start to approach this. Any help on this frustrating problem would be appreciated. Thanks!", "output": "Hmm. Pretty sure that the answer is $9$.\nThe key observation to this problem is noticing that $(\\sqrt{3}-\\sqrt{2})^{2016}+(\\sqrt{3}+\\sqrt{2})^{2016}$ is an integer. \nThe proof of this is expansion using Binomial Theorem. The odd powers of the square roots get canceled out.\nNow we have $(\\sqrt{3}+\\sqrt{2})^{2016} = N - (\\sqrt{3}-\\sqrt{2})^{2016}$, where $N$ is a positive integer.\nNow this is easy. Since $(\\sqrt{3}-\\sqrt{2})^{2016} < (0.4)^{2016} = (0.064)^{\\frac{2016}{3}} < (0.1)^{\\frac{2016}{3}} < (0.1)^{600}$, we have $(\\sqrt{3}+\\sqrt{2})^{2016}= (N-1)+0.99\\cdots 99$, and there are at least $500$ $9$'s there. The answer is $\\boxed{9}$.", "meta": {"post_id": 1544422, "input_score": 27, "output_score": 44, "post_title": "Calculating decimal digits by hand for extremely large numbers"}}
{"input": "Let's assume that $A\\subseteq X$ is product of $A_{i}\\subseteq X_{i}(i\\in I)$.\nThen product topology of $A$ is the same than the topology induced by $X$.\nI have proved this few different times now, and for this one I need help. I like to try all kinds of proofs.\nCollection $\\mathcal{B}_{A}$ has sets $V:=\\prod_{i\\in I} V_i$, where $V_i\\subseteq A_{i}$ by every $i\\in I$ and $V_i\\neq A_i$ finitely many $i$.\nCollection $\\mathcal{B}_{X}$ has sets $\\prod_{i}U_{i\\in I}\\cap \\prod_{i}A_{i\\in I}$, where and $U_{i}$ is element of basis of $X_{i}$. Notation $U:=\\prod_{i}U_{i\\in I}$.\nThere is theorem that says $\\mathcal{B}$ is basis for topology iff every open $U\\subseteq X$ can be shown in the form\n$$\nU=\\bigcup_{a\\in A} B_{a},\n$$\nwhere $B_{a}\\in\\mathcal{B}$ for all $a\\in A$.\nNow what I am trying to do in this proof attempt is that I want to try out the theorem above.\nThere are problems with the notion and that is one main thing where I need tips.\nI hope that you get the idea what I am after here.\nProof:\n$Z$ belongs in the product topology of $A$.\n$\\Leftrightarrow$ \n$$\nZ=\\bigcup_{i\\in I} \\big(\\prod_{i\\in I} V_{i} \\big)_{i}\\quad\\text{Where } V_{i}\\in \\mathcal{T}_{A_{i}}\\text{ for all }i\\in I.\n$$\n$\\Leftrightarrow$\n$$\nZ=\\bigcup_{i\\in I} \\big(\\prod_{i \\in I}U_{i}\\cap A_{i} \\big)\\quad\\text{Where }U_{i}\\in\\mathcal{T_{i}}\\text{ for all }i\\in I.\n$$\n$\\Leftrightarrow$\n$$\nZ=\\bigcup_{i\\in i} (\\prod_{i\\in I} U_{i}\\cap \\prod_{i\\in I} A_{i})_{i}\n$$\n$\\Leftrightarrow$\n$$\nZ=\\big(\\bigcup_{i\\in I}\\big(\\prod_{i\\in I} U_{i} \\big)_{i} \\big)\\cap \\prod_{i\\in I} A_{i}\n$$\n$\\Leftrightarrow$\n$Z$ belongs to product topology induced by $X$.", "output": "Let's start with a definition:\nLet $(X,\\mathcal{T})$ be a topological space.\nLet $I$ be an index set, and let $Y_i (i \\in I)$ be topological spaces\nand let $f_i: X \\rightarrow Y_i$ be a family of functions.\nThen $\\mathcal{T}$ is called the initial topology with respect to the maps $f_i$\niff\n\n$\\mathcal{T}$ makes all $f_i$ continuous.\nIf $\\mathcal{T}'$ is any other topology on $X$ that makes all $f_i$ continuous, then $\\mathcal{T} \\subseteq \\mathcal{T}'$.\n\nOr put more shortly: $\\mathcal{T}$ is the smallest (coarsest) topology that makes all $f_i$ continuous.\nRemark: it is not very useful to ask for the largest topology to make all\n$f_i$ continuous, then we would always get the discrete topology on $X$.\nThis is the largest topology on $X$ and it makes any map continuous.\nThis is in fact a common way to construct a topology on a set $X$, based\non functions to spaces $Y_i$ that already have a topology.\nE.g. in linear space theory we can consider an alternative topology on a linear\nspace $X$ as the smallest topology that makes all linear maps (in the original topology)\nfrom $X$ to $\\mathbb{R}$ continuous. This is called the weak topology on the linear space $X$\n(as it generally weaker (fewer open sets) than the original topology on $X$.)\nThat this works is based on the following simple:\nExistence theorem for initial topologies:\nLet $X$ be a set and $f_i : X \\rightarrow Y_i$ be a collection of topological spaces\nand maps. Then there is a topology on $X$ that is initial w.r.t. the maps $f_i$.\nMoreover, this topology is unique and a subbase of the topology is given by\n$\\mathcal{S} = \\{(f_i)^{-1}[O]: i \\in I, O \\text{ open in } Y_i\\}$.\n(a subbase is a collection $\\mathcal{S}$ of subsets of $X$ such that all finite intersections\nof elements of $\\mathcal{S}$ form a base for the topology).\nProof: Let $\\mathcal{T}$ be the topology generated by $\\mathcal{S}$ as a subbase.\nThis means that $\\mathcal{T}$ is the collection of all sets that can be written\nas unions of finite intersections from $\\mathcal{S}$.\nThen all $f_i$ are continuous, as for all open $O \\subseteq Y_i$ the inverse image under $f_i$ of $O$ is in $\\mathcal{T}$.\nAnd if $\\mathcal{T}'$ is any topology that makes all $f_i$ continuous, $\\mathcal{T}'$\nmust contain all sets of the form $(f_i)^{-1}[O]$ and so $\\mathcal{T}'$ must contain\n$\\mathcal{S}$, and as $\\mathcal{T}'$ is closed under finite\nintersections and unions, we have that $\\mathcal{T} \\subseteq \\mathcal{T}'$, as required.\nThe unicity is clear, because if $\\mathcal{T}$ and $\\mathcal{T}'$ are both initial then $\\mathcal{T} \\subseteq \\mathcal{T}'$\n(by 2) applied to $\\mathcal{T}$) and $\\mathcal{T}' \\subseteq \\mathcal{T}$ (by 2)\napplied to $\\mathcal{T}'$) and thus $\\mathcal{T} = \\mathcal{T}'$.\nExample 1: if $A$ is a subset of a topological space $X$, and $i: A \\rightarrow X$\nis the inclusion map from $A$ to $X$ (defined by $i(x) = x$ for all $x \\in A$),\nthen the subspace topology on $A$ is just the initial topology w.r.t. $i$.\nProof: the subspace topology is defined by $\\{O \\cap A: O \\text{ open in } X\\}$.\nBut $i^{-1}[O] = O \\cap A$ ($x \\in O$ and $x \\in A$, then $i(x) = x$ is in $O$, so\n$x \\in i^{-1}[O]$ and if $x \\in i^{-1}[O]$ then $x \\in A$ by definition and $x = i(x)$\nmust be in $O$, so $x$ is in $O \\cap A$).\nWe see that the subbase $\\mathcal{S}$ from the existence theorem is just equal\nto the subspace topology!\nRemark: in general, when we have just one function $f: X \\rightarrow Y$, and $X$ has the initial\ntopology w.r.t. $f$, we get the topology and not just a subbase. Also, if $f$ is moreover\ninjective (one-to-one), then $f$ is called an embedding.\nExample 2:\nIf $X_i, i \\in I$ is a family of topological spaces and $X$ is the Cartesian product of\nthe spaces $X_i$, then we have the projection maps $p_j: X \\rightarrow X_j$ defined\nby $p_j( (x_i)_{i \\in I}) = x_j$ for all $j \\in I$.\nThen the product topology on $X$ is just the initial topology w.r.t. the\nmaps $p_i$ ($i \\in I$).\nProof: the sets $(p_j)^{-1}[O]$ are just the product sets of the form $\\prod_i O_i$\nwhere all $O_i = X_i$ except $O_j = O$. So the finite intersections of the subbase\nelements are exactly all such sets $\\prod_i O_i$ where finitely many\n$O_i$ are some open set in their respective coordinate space\nand in all other coordinates $O_i$ are equal to $X_i$; this is precisely the\nstandard base for the product topology.\nSo two very common ways of making new spaces from old ones, subspaces and products,\nare special cases of initial topologies. In the following I will develop some\nbasic theory that will allow us to formulate and prove general principles that will\napply to all examples of initial topologies. Some well-known facts can then be seen\ntogether in a common framework.\n\nThe fact that a space $X$ has the initial topology w.r.t. a family of mappings, makes it\neasy to recognise continuous functions to $X$.\nWe have the following useful:\nUniversal theorem of continuity for initial topologies.\nLet $X$ be a space and $f_i : X \\rightarrow Y_i$ $(i \\in I)$ a family of mappings\nand spaces $Y_i$, such that $X$ has the initial topology with respect to the $f_i$.\nLet $W$ be any space and $g$ a function from $W$ to $X$.\nThen $g$ is continuous iff for all $i \\in I$: $f_i \\circ g$ is continuous from $W$ to $Y_i$.\nProof: if $g$ is continuous then all $f_i \\circ g$ are also continuous,\nbecause all $f_i$ are continuous and compositions of continuous maps are continuous.\nSuppose now that $f_i \\circ g$ is continuous for all $i$.\nLet $S \\subseteq X$ be any element of the subbase $\\mathcal{S}$ (from the existence theorem), so that\n$S = (f_i)^{-1}[O]$ for some open subset $O$ of $Y_i$.\nNow $$g^{-1}[S] = g^{-1}[(f_i)^{-1}[O]] = (f_i \\circ g)^{-1}[O]$$ which is open in $W$ because $f_i \\circ g$ is continuous.\nThis shows that inverse images of elements from $\\mathcal{S}$ are open.\nBut then as $g^{-1}$ preserves (finite) intersections and unions and as all open\nsubsets of $X$ are unions of finite intersections of elements from $\\mathcal{S}$,\nwe see that $g^{-1}[O]$ is open for all open subsets $O$ of $X$. Or: $g$ is continuous.\nThere is a converse to this as well:\nCharacterisation of the initial topology by the continuity theorem.\nLet $X$ be a space, and $f_i: X \\rightarrow Y_i$ be a family of spaces and functions.\nSuppose that $X$ satisfies the universal continuity theorem in the following sense:\n\n(*) for all spaces $Z$, for any function $g: Z \\rightarrow X$:  $(f_i \\circ g)$ is continuous for all $i$ iff $g$ is continuous.\n\nThen $X$ has the initial topology w.r.t. the maps $f_i$.\nProof: the identity on X is always continuous, so applying ($\\ast$) from right to left with $g = \\operatorname{id}$\ngives us that all $f_i$ are continuous. If $\\mathcal{T}'$ is another topology on $X$\nthat makes all $f_i$ continuous, then consider the map $g: (X, \\mathcal{T}') \\rightarrow (X, \\mathcal{T})$,\ndefined by $g(x) = x$. Then all maps $f_i \\circ g$ are just the maps $f_i$ as seen between\n$(X, \\mathcal{T}')$ and $Y_i$ which are by assumption continuous.\nSo by the other direction of (*) we see that $g$ is continuous,\nand thus (as $g(x) = x$, and thus $g^{-1}[O] = O$ for all $O$) we have that\n$\\mathcal{T} \\subseteq \\mathcal{T}'$,\nas required for the second property of the initial topology.\n\nApplications:\nCharacterisation of continuity into products:\na map $f$ into a product $\\prod_{i \\in I} X_i$ is continuous iff $p_i \\circ f$ is continuous\nfor all $i \\in I$.\nOr suppose that $X$ is any space, $Y'$ a subspace of a space $Y$, and $g: X \\rightarrow Y$\nis a map such that $g[X] \\subseteq Y'$.\nThen there is the \"image restriction\" $g': X \\rightarrow Y'$ of $g$, defined by $g'(x) = g(x)$.\nNote that, if $i: Y' \\rightarrow Y$ is the inclusion, then $Y'$ has the initial topology w.r.t. i,\nand moreover, $g = g' \\circ i$. So the theorem we just proved says: $g$ continuous iff $g'$ continuous.\nThis has the intuitive meaning that continuity of $g$ is only determined by $Y'$.\nSo if $g$ is an embedding (see above), then $g$ is a continuous bijection between\n$X$ and $g[X]$, which is also open as a map between these spaces, when we give $g[X]$\nthe subspace topology: let $O$ be open in $X$. Then $O = g^{-1}[O']$ for some open subset\nof $Y$ (by embedding = initial map), and then $O' \\cap g[X]$ is open in g[X],\nand $g[O] = O' \\cap g[X]$, by injectivity of $g$. The reverse is also quite easy to see\n(exercise): if $g:X \\rightarrow g[X] \\subseteq Y$ is a homeomorphism, then $g$ is an embedding from $X$\ninto Y. Many books actually define embeddings that way.\nNote that the restriction of $f$ to $A$, $f | A$, is just $f \\circ i$, where $i$ is the embedding\nof $A$ into $X$, so that $f | A$ is continuous as a composition of continuous maps.\n\nApplication: diagonal product map.\nLet $X$ be a space and let $Y_i$ ($i \\in I$) be a family of spaces, and $f_i : X \\rightarrow Y_i$\nbe a family of functions. Let $Y$ be the product of the $Y_i$, with projections $p_i$.\nDefine $f:X \\rightarrow Y$, the so-called diagonal product of the $f_i$, as follows:\n$f(x) = (f_i(x))_{i \\in I}$. Then $f$ is continuous iff for all $i \\in I$, $f_i$ is continuous.\nProof: immediate from the universal continuity theorem, because for all $i$ we have\nthat $p_i \\circ f = f_i$.\n\nApplication: product maps.\nLet $f_i : X_i \\rightarrow Y_i$ be a family of functions between spaces $X_i$ and $Y_i$,\nlet $X = \\prod_{i \\in I} X_i$, $Y = \\prod_{i \\in I} Y_i$, and let\n$f:X \\rightarrow Y$ be defined by $f((x_i)_i) = (f_i(x_i))_i$, which is called the product map of the $f_i$.\nThen $f$ is continuous iff for all $i \\in I$ we have that $f_i$ is continuous.\nProof: let $p_i$ be the projections from $Y$ to $Y_i$, and let $q_i$ be the\nprojections from $X$ to the $X_i$.\nThen for all $i$ we have $$p_i \\circ f = f_i \\circ q_i\\text{.}$$\nSuppose that all $f_i$ are continuous.\nThen, as all $q_i$ and $f_i$ are continuous, all maps $p_i \\circ f$ are continuous.\nAs $Y$ has the initial topology w.r.t. the $p_i$, we have by the universal continuity\ntheorem that $f$ is continuous.\nNow let $f$ be continuous. Fix $i$ in $I$. Also take a point $r= (r_i)_{i \\in I}$ from $X$.\nLet $(s_i)_{i \\in I}$ in $Y$ be its image $f(r)$.\nThen the map $k_i: X_i \\rightarrow X$, defined as the diagonal product of the identity\non $X_i$ and all constant maps onto the point $r_j$ for all $j \\neq i$.\nBy the previous application, this is continuous. Moreover, $q_i \\circ k_i$ is the identity on $X_i$,\nalso denoted by $\\operatorname{id}_{X_i}$. But note that\n$$ f_i = f_i \\circ \\operatorname{id}_{X_i} = f_i \\circ (q_i \\circ k_i) = \n(f_i \\circ q_i) \\circ k_i = (p_i \\circ f) \\circ k_i\\text{,}$$ which is continuous, as $f$, $p_i$ and $k_i$ are.  So $f_i$ is continuous, for all $i \\in I$.\n\nA very useful general fact is the following:\nTransitive law of initial topologies.\n\nSuppose that we have a family of spaces and maps $f_i : X \\rightarrow Y_i$\n( $i \\in I$) and for each $i \\in I$ an index set $I_i$, and a family\nof maps $g_{i,j} : Y_i \\rightarrow Z_j$ for $j$ in $I_i$.\nAssume that each $Y_i$ has the initial topology w.r.t. the $g_{i,j}$ ($j \\in I_i$).\nThen $X$ has the initial topology w.r.t. the maps $g_{i,j} \\circ f_i$ ($i \\in I, j \\in I_i$)\niff $X$ has the initial topology w.r.t. the $f_i$ ($i \\in I$).\n\nProof:\nSuppose that $X$ has the initial topology w.r.t. the $f_i$. Call this topology $\\mathcal{T}$.\nAll $g_{i,j}$ are continuous (part of being initial of the topology on $Y_i$)\nso all $g_{i,j} \\circ f_i$ are continuous. Suppose that $\\mathcal{T}'$ is another topology on $X$\nthat makes all $g_{i,j} \\circ f_i$ continuous.\nThen consider the maps $f'_i : (X,\\mathcal{T}') \\rightarrow Y_i$, defined by $f'_i(x) = f_i(x)$.\nSo we have $g_{i,j} \\circ f'_i = g_{i,j} \\circ f_i$ for all relevant indices.\nBy assumption all $g_{i,j} \\circ f'_i = g_{i,j} \\circ f_i : (X,\\mathcal{T}') \\rightarrow Z_j$\nare continuous, and as all $Y_i$\nhave the initial topology w.r.t. the $g_{i,j}$, we see that all $f'_i$ are (by the universal\ncontinuity theorem) continuous. If $\\operatorname{id}$ is the identity map from\n$(X,\\mathcal{T}') \\rightarrow (X,\\mathcal{T})$, then\n$f_i \\circ \\operatorname{id} = f'_i$ for all $i$. We have just seen that all $f'_i$ are continuous, and\nas $\\mathcal{T}$ is initial w.r.t. the $f_i$, we see that $\\operatorname{id}$\nis a continuous map by this same universal continuity theorem.\nBut the identity from $(X,\\mathcal{T}') \\rightarrow (X,\\mathcal{T})$ is continuous iff\n$\\mathcal{T} \\subset \\mathcal{T}'$\n(as $O \\in \\mathcal{T}$ means $\\operatorname{id}^{-1}[O] = O \\in \\mathcal{T}'$), so\n$\\mathcal{T}$ is indeed minimal w.r.t. the continuity\nof all maps $g_{i,j} \\circ f_i$, and $X$ has the initial topology w.r.t. these maps.\nSuppose on the other hand that $X$ has the topology $\\mathcal{T}$, which is initial w.r.t. the maps\n$g_{i,j} \\circ f_i$. Let $i$ be in $I$. For all $j \\in I_i$ we know that $g_{i,j} \\circ f_i$\nis continuous. As $Y_i$ has the initial topology w.r.t. the maps $g_{i,j}$ ($j \\in I_i$),\nwe see again by the universal continuity theorem that $f_i$ is continuous. So all $f_i$\n(from $(X,\\mathcal{T})$ to $Y_i$) are continuous.\nLet $\\mathcal{T}'$ be another topology on $X$ that makes all $f_i$ continuous.\nThis means that all $g_{i,j} \\circ f_i$ are continuous, and so by minimality of $\\mathcal{T}$\n(by the definition of initial topology w.r.t. the maps $g_{i,j} \\circ f_i$) we see that\n$\\mathcal{T} \\subseteq \\mathcal{T}'$.\nSo $\\mathcal{T}$ is the initial topology w.r.t. the $f_i$.\n\nTwo useful applications, to make all this less abstract:\nSubspaces of subspaces:\nLet $A$ be a subspace of $B$ and $B$ a subspace of $X$ ($A \\subseteq B \\subseteq X$) then\n$A$ is a subspace of $X$ (i.e. it has the subspace topology w.r.t. $X$).\nProof:\nApply the above to $i_B: B \\rightarrow X$, $i_{A,B}: A \\rightarrow B$,\n$i_A: A \\rightarrow X$, all basically the identity\nwith different domains and codomains. So by assumption $A$ has the initial topology w.r.t. $i_{A,B}$\nand $B$ has the initial topology w.r.t. $i_B$.\nNote that $i_A = i_B o i_{A,B}$, so by the transitivity theorem (right to left) we see that\n$A$ has the initial topology w.r.t. $i_A$, or $A$ has the subspace topology w.r.t. $X$.\nProducts and subspaces:\n\nLet $X_i$ ($i \\in I$) be a family of spaces, with subspaces $A_i \\subseteq X_i$.\nThen $A = \\prod_{i \\in I} A_i$ (in the product topology of the subspace topologies)\nis a subspace of $X = \\prod_{i \\in I} X_i$ (it has the initial topology w.r.t. the inclusion).\n\nProof:\nLet $k_i$ be the inclusion mapping from $A_i$ to $X_i$.\nLet $k: \\prod_i A_i \\rightarrow \\prod_i X_i$ the product mapping (as above).\nNote that $k$ is also the inclusion from $A$ into $X$.\nAgain let $p_i$ be the projections from $A$ onto the $A_i$, and $q_i$ the projections\nfrom $X$ onto the $X_i$. Then\n$$(\\ast) q_i \\circ k = k_i \\circ p_i \\text{ for all } i \\in I \\text{.} $$\n$A_i$ has the initial topology w.r.t. $k_i$, and $A$ has the initial topology w.r.t. the $p_i$.\nSo $A$ has the initial topology w.r.t. the maps $k_i \\circ p_i$ ($i \\in I$) by the\nright to left implication of the transitivity theorem. So $A$ has the initial topology w.r.t. the maps\n$q_i \\circ k$ by $(\\ast)$. But by the transitivity theorem (the other implication ) we see that $A$ has the initial topology w.r.t. the map $k$, which is, as said, the inclusion $A \\rightarrow X$. So $A$ has the subspace topology.\nSo we see that all these general considerations give a nice proof of\n\"a product of subspaces is a subspace), due to the special nature of these\ntopologies as initial topologies with respect to certain maps.\nRemark: for those who know inverse limits, which are subspaces of products\nof a certain kind, the above theorem also shows that in fact the inverse limit topology\nis itself an initial topology w.r.t. the restricted projection maps. This gives\nrise to a canonical subbase for the inverse limit, from the existence theorem, which\nis sometimes useful as well.\nAs a final remark: a similar theory can be developped for so-called \"final\" topologies.\nThis applies to situations where we have maps $f_i: X_i\\rightarrow X$ where we want $X$ to have\nthe largest topology that makes all $f_i$ continuous.\nSpecial cases include quotient topologies, disjoint sums and weak topologies induced by subspaces.\nAlso here we have an existence theorem, a universal continuity theorem and a transitive law. See here for the details.", "meta": {"post_id": 1548495, "input_score": 20, "output_score": 49, "post_title": "Proof that product topology of subspace is same as induced product topology"}}
{"input": "I've been learning about polar curves in my Calc class and the other day I saw this suspiciously $r=1-\\cos \\theta$  looking thing in my coffee cup (well actually $r=1-\\sin \\theta$ if we're being pedantic.) Some research revealed that it's called a caustic. I started working out why it would be like this, but hit a snag. Here's what I did so far:\nConsider the polar curves $r=1-\\cos \\theta$ and $r=1$. Since for a light ray being reflected off a surface (or the inside of my cup) the $\\angle$ of incidence =$\\angle$ of reflection, a point on the circle $(1,\\theta)\\to(1,2\\theta)$. It looks like this has something to do with the tangent lines, so I pretend there's an $xy$ plane centered at the pole to find the slope of the line connecting the points. Since it's the unit circle the corresponding rectangular coordinates are $(\\cos \\theta, \\sin \\theta)$ and $(\\cos 2\\theta, \\sin 2\\theta).$ So \n$$m={\\sin 2\\theta-\\sin \\theta \\over \\cos 2\\theta-\\cos \\theta}$$\nnow we see if it matches up with the slope of the lines tangent to the cardioid\n$$\\frac{dy}{dx}={r'\\sin \\theta+r \\cos \\theta \\over r' \\cos \\theta - r \\sin \\theta}={\\sin^2\\theta - \\cos^2 \\theta +\\cos \\theta \\over 2\\sin \\theta \\cos \\theta -\\sin\\theta}={\\cos \\theta - \\cos 2\\theta\\over \\sin 2\\theta-\\sin \\theta}$$\nThey're similar, but not identical. In particular $\\frac{dy}{dx}=-\\frac1m$. What error have I made, or what have I overlooked conceptually? Thanks in advance.", "output": "Light From Infinity\nMost likely, the light is from a distance that is large on the scale of the cup. Therefore, we will take the incoming rays to be parallel. If so, the caustic in the coffee cup is a nephroid.\nConsider the following diagram\n\nThe ray reflected at the point $(-\\cos(\\theta),\\sin(\\theta))$ is\n$$\n\\frac{y-\\sin(\\theta)}{x+\\cos(\\theta)}=-\\tan(2\\theta)\\tag{1}\n$$\nwhich is\n$$\nx\\sin(2\\theta)+y\\cos(2\\theta)=-\\sin(\\theta)\\tag{2}\n$$\nHere is a plot of the reflected rays from $(2)$ generated by uniformly distributed parallel incoming rays\n\nTaking the derivative of $(2)$ with respect to $\\theta$:\n$$\n2x\\cos(2\\theta)-2y\\sin(2\\theta)=-\\cos(\\theta)\\tag{3}\n$$\nSolving $(2)$ and $(3)$ simultaneously gives the envelope of the family of lines in $(1)$:\n$$\n\\begin{align}\n\\begin{bmatrix}\nx\\\\\ny\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\cos(2\\theta)&-\\sin(2\\theta)\\\\\n\\sin(2\\theta)&\\cos(2\\theta)\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n-\\frac12\\cos(\\theta)\\\\\n-\\sin(\\theta)\n\\end{bmatrix}\\\\[6pt]\n&=\\frac14\\begin{bmatrix}\n\\cos(3\\theta)-3\\cos(\\theta)\\\\\n3\\sin(\\theta)-\\sin(3\\theta)\n\\end{bmatrix}\\tag{4}\n\\end{align}\n$$\nThe curve from $(4)$ is added in red\n\nEquation $(4)$ describes a nephroid.\n\nLight From A Point On The Circle\nSince it is mentioned in a comment that a cardoid is formed by light coming from a point on the edge of the cup, we will do the same computation with light from a point on the circle.\nConsider the following diagram\n\nThe ray reflected at the point $(-\\cos(\\theta),\\sin(\\theta))$ is\n$$\n\\frac{y-\\sin(\\theta)}{x+\\cos(\\theta)}=-\\tan\\left(\\frac{3\\theta}2\\right)\\tag{5}\n$$\nwhich is\n$$\ny(1+\\cos(3\\theta))+x\\sin(3\\theta)=\\sin(\\theta)-\\sin(2\\theta)\\tag{6}\n$$\nHere is a plot of the reflected rays from $(6)$ reflected at uniformly spaced points on the circle\n\nTaking the derivative of $(6)$ with respect to $\\theta$:\n$$\n-3y(\\sin(3\\theta))+3x\\cos(3\\theta)=\\cos(\\theta)-2\\cos(2\\theta)\\tag{7}\n$$\nSolving $(6)$ and $(7)$ simultaneously gives the envelope of the family of lines in $(5)$:\n$$\n\\begin{align}\n\\begin{bmatrix}\nx\\\\\ny\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\cos(3\\theta)&-\\sin(3\\theta)\\\\\n\\sin(3\\theta)&1+\\cos(3\\theta)\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n\\frac13\\cos(\\theta)-\\frac23\\cos(2\\theta)\\\\\n\\sin(\\theta)-\\sin(2\\theta)\n\\end{bmatrix}\\\\[6pt]\n&=\\frac13\\begin{bmatrix}\n\\cos(2\\theta)-2\\cos(\\theta)\\\\\n2\\sin(\\theta)-\\sin(2\\theta)\n\\end{bmatrix}\\tag{8}\n\\end{align}\n$$\nThe curve from $(8)$ is added in red\n\nEquation $(8)$ describes a cardoid.", "meta": {"post_id": 1550259, "input_score": 105, "output_score": 39, "post_title": "Cardioid in coffee mug?"}}
{"input": "Is there a name for a polygon in which you could place a light bulb that would light up all of its area? (for which there exists a point so that for all points inside it the line connecting those two points does not cross one of its edges)\nExamples of \"lightable\" polygons:\n\nExamples of \"unlightable\" polygons:", "output": "Yes, those are called star-shaped polygons. They have numerous applications in mathematics, for example in complex analysis.", "meta": {"post_id": 1562729, "input_score": 54, "output_score": 74, "post_title": "Is there a name for this type of polygon?"}}
{"input": "Simplify the expression below into a seasonal greeting using commonly-used symbols in commonly-used formulas in maths and physics. Colours are purely ornamental!\n$$\n\\begin{align}\n\\frac{\n\\color{green}{(x+iy)}\n\\color{red}{(y^3-x^3)}\n\\color{orange}{(v^2-u^2)}\n\\color{red}{(3V_{\\text{sphere}})^{\\frac 13}}\n\\color{orange}{E\\cdot} \n\\color{green}{\\text{KE}}\n}\n{\n\\color{orange}{2^{\\frac 23}}\n\\color{green}{c^2}\n\\color{red}{e^{i\\theta}}\n\\color{orange}{v^2}\n\\color{green}{(x^2+xy+y^2)}}\n\\color{red}{\\sum_{n=0}^{\\infty}\\frac 1{n!}}\n\\color{orange}{\\bigg/}\n\\color{orange}{\\left(\\int_{-\\infty}^\\infty e^{-x^2} dx\\right)^{\\frac 23}}\n\\end{align}$$\nNB: Knowledge of the following would be helpful:  \nBasic Maths:  \n\nTaylor series expansion  \nNormalizing factor for the integral of a normal distribution  \nRectangular and polar forms for complex variables  \nVolume of a sphere   \n\nBasic Physics:  \n\nKinematics formulae for motion under constant acceleration   \nEinstein's equation   \nOne of the energy equations", "output": "$$\n\\begin{align}\n&\\frac{\n\\color{green}{(x+iy)}\n\\color{red}{(y^3-x^3)}\n\\color{orange}{(v^2-u^2)}\n\\color{red}{(3V_{\\text{sphere}})^{\\frac 13}}\n\\color{orange}{E\\cdot} \n\\color{green}{\\text{KE}}\n}\n{\n\\color{orange}{2^{\\frac 23}}\n\\color{green}{c^2}\n\\color{red}{e^{i\\theta}}\n\\color{orange}{v^2}\n\\color{green}{(x^2+xy+y^2)}}\n\\color{red}{\\sum_{n=0}^{\\infty}\\frac 1{n!}}\n\\color{orange}{\\bigg/}\n\\color{orange}{\\left(\\int_{-\\infty}^\\infty e^{-x^2} dx\\right)^{\\frac 23}}\\\\\n&=\n\\frac{\n\\color{green}{(x+iy)}\n\\color{red}{(y^3-x^3)}\n\\color{orange}{(v^2-u^2)}\n\\color{red}{(3V_{\\text{sphere}})^{\\frac 13}}\n\\color{orange}{E\\cdot} \n\\color{green}{\\text{KE}}\n}\n{\n\\color{red}{e^{i\\theta}}\n\\color{green}{(x^2+xy+y^2)}\n\\color{orange}{\\cdot2^{\\frac 23}}\n\\color{green}{c^2}\n\\color{orange}{v^2}\n}\n\\color{red}{\\sum_{n=0}^{\\infty}\\frac 1{n!}}\n\\color{orange}{\\bigg/}\n\\color{orange}{\\left(\\sqrt{\\pi}\\right)^{\\frac 23}}\\\\\n&=\n\\color{green}{\\left(\\frac{x+iy}{e^{i\\theta}}\\right)}\n\\color{red}{\\left(\\frac{y^3-x^3}{x^2+xy+y^2}\\right)}\n\\color{orange}{(v^2-u^2)}\n\\color{red}{\\left(\\frac {(3V_\\text{sphere})^\\frac 13}{\\left(2\\sqrt{\\pi}\\right)^{\\frac 23}}\\right)}\n\\color{orange}{\\left(\\frac{E}{c^2}\\right)}\n\\color{green}{\\left(\\frac{\\text{KE}}{v^2}\\right)}\n\\color{red}{\\sum_{n=0}^{\\infty}\\frac 1{n!}}\n\\\\\n&=\n\\color{green}{\\left(\\frac{re^{i\\theta}}{e^{i\\theta}}\\right)}\n\\color{red}{\\left(\\frac{(y-x)(y^2+xy+x^2)}{x^2+xy+y^2}\\right)}\n\\color{orange}{(v^2-u^2)}\n\\color{red}{\\left(\\frac {3\\cdot \\frac 43 \\pi r^3}{4\\pi}\\right)^\\frac 13}\n\\color{orange}{\\left(\\frac{mc^2}{c^2}\\right)}\n\\color{green}{\\left(\\frac{\\frac 12 mv^2}{v^2}\\right)}\n\\color{red}{(e)}\n\\\\\n&=\n\\color{green}{\\left(r\\right)}\n\\color{red}{\\left(y-x\\right)}\n\\color{orange}{(2as)}\n\\color{red}{\\left(r^3\\right)^\\frac 13}\n\\color{orange}{\\left(m\\right)}\n\\color{green}{\\left(\\frac 12m\\right)}\n\\color{red}{(e)}\n\\\\\n&=\n\\color{green}{\\left(r\\right)}\n\\color{red}{\\left(y-x\\right)}\n\\color{orange}{(as)}\n\\color{red}{\\left(r\\right)}\n\\color{orange}{\\left(m\\right)}\n\\color{green}{\\left( m\\right)}\n\\color{red}{(e)}\n\\\\\n&=\n\\color{orange}{\\left(m\\right)}\n\\color{red}{(e)}\n\\color{green}{\\left(r\\right)}\n\\color{red}{\\left(r\\right)}\n\\color{red}{\\left(y-x\\right)}\n\\color{green}{\\left(m\\right)}\n\\color{orange}{(as)}\n\\end{align}$$\nMerry Christmas, everyone!!\n\nThe following links might be helpful.\n- Complex numbers and polar coordinates\n- Difference of two cubes\n- Kinematics formulae for constant acceleration in a straight line\n- Volume of a sphere\n- Einstein's mass-energy equivalence\n- Kinetic energy\n- Taylor/Maclaurin series expansion of $e$\n- Gaussian integral (normalizing factor for the normal distribution)", "meta": {"post_id": 1587620, "input_score": 53, "output_score": 66, "post_title": "Xmas Maths 2015"}}
{"input": "I read in a paper that if $M$ is a real square matrix of size $n$, then we can consider the action of $M$ in the third exterior algebra $\\Lambda^3 \\mathbb{R}^n$, and the matrix of this action are the $3\\times 3$ minors of $M$. Here I am not clear what the action mentioned above is, and thus I do not understand the latter statement. Can someone help me? Thanks a lot!", "output": "Let me expand my comment into a complete answer.\nLet $V$ be a finite-dimensional vector space, with dimension $n$ and basis $\\left\\{b_1,\\ldots,b_n\\right\\}$. The $k^{\\text{th}}$ exterior power $\\Lambda^k(V)$ has dimension $n\\choose k$, and the basis for $V$ induces a standard basis on $\\Lambda^k(V)$ given by the collection of all (wedge) products of the form $b_{i_1}\\wedge\\cdots\\wedge b_{i_k}$, where the $i_j$ are increasing, i.e. $1\\leq i_j<i_{j+1}\\leq k$.\nNow here is the crucial point. A linear endomorphism at the \"bottom\" of the exterior algebra, $T:V\\to V$, induces endomorphisms at all the higher degrees of the algebra, from $k=1$ up to $k=n$. In particular, $T$ induces a linear map $\\Lambda^k(T):\\Lambda^k(V)\\to\\Lambda^k(V)$. Which map? It suffices to specify how it acts on basis elements; it does so by sending $b_{i_1}\\wedge\\cdots\\wedge b_{i_k}$ to \n$T(b_{i_1})\\wedge\\cdots\\wedge T(b_{i_k})$.\nYou can write down the ${n\\choose k}\\times{n\\choose k}$ matrix of $\\Lambda^k(T)$ with respect to the induced basis on $\\Lambda^k(V)$ by expanding the images $T(b_{i_1})\\wedge\\cdots\\wedge T(b_{i_k})$. The coordinates of these images with respect to the basis elements $b_{i_1}\\wedge\\cdots\\wedge b_{i_k}$ are precisely the familiar $k\\times k$ minors of the matrix of $T$ with respect to the basis $\\left\\{b_1,\\ldots,b_n\\right\\}$. In other words, the matrix of $\\Lambda^k(T)$ encodes the minors of $T$. I won't bother to show this in detail because the notation is horrendous and unenlightening in general, but I will give a concrete example below -- it is instructive to do such an example once in full detail and then never do it again.\nFirst, though, note that this construction shows there is nothing special about the third exterior power. Indeed, the most familiar induced map occurs at the top degree, $\\Lambda^n(V)$, which is ${n\\choose n}=1$ dimensional. So the induced map $\\Lambda^n(T)$ is just multiplication by a scalar; this scalar is precisely the determinant of $T$.\nNow for a concrete example. Take $V=\\mathbb{R}^3$ with basis $\\left\\{e_1, e_2, e_3\\right\\}$. Let $T$ be a linear transformation $\\mathbb{R}^3\\to\\mathbb{R}^3$ whose matrix with respect to this basis is\n$$\\begin{pmatrix}a^{11}&a^{12}&a^{13}\\\\a^{21}&a^{22}&a^{23}\\\\a^{31}&a^{32}&a^{33}\\end{pmatrix}$$\nThis means $T(e_1)=a^{11}e_1+a^{21}e_2+a^{31}e_3$, etc.\nLet's look at the map $\\Lambda^2(T)$ that $T$ induces on the second exterior power $\\Lambda^2(\\mathbb{R}^3)$, which is ${3\\choose 2}=3$ dimensional as well. (As a result, the matrix that represents $\\Lambda^2(T)$ will also be $3\\times 3$. This is a coincidence; the matrices of $T$ and of $\\Lambda^k(T)$ will have the same size only when $k=1$ and $k=n-1$.) The induced basis elements of $\\Lambda^2(\\mathbb{R}^3)$ are $e_1\\wedge e_2$, $e_1\\wedge e_3$, and $e_2\\wedge e_3$. In order to recover the familiar matrix of minors, I'll order this basis so that the first basis element is $e_2\\wedge e_3$, the second is $e_1\\wedge e_3$, and the third is $e_1\\wedge e_2$.\nNow let's compute the action of $\\Lambda^2(T)$ on these basis elements in this order. The key to this algebra is the alternating nature of the wedge product. Here's the computation for the first basis element; we're effectively wedging the second and third columns of the matrix for $T$:\n$$\n\\begin{eqnarray}\n\\Lambda^2(T)(e_2\\wedge e_3)&=&T(e_2)\\wedge T(e_3)\\\\\n&=&(a^{12}e_1+a^{22}e_2+a^{32}e_3)\\wedge(a^{13}e_1+a^{23}e_2+a^{33}e_3)\\\\\n&=&\\left(\\color{red}{a^{22}a^{33}-a^{32}a^{23}}\\right)e_2\\wedge e_3\n+\\left(\\color{blue}{a^{12}a^{33}-a^{32}a^{13}}\\right)e_1\\wedge e_3\n+\\left(\\color{orange}{a^{12}a^{23}-a^{22}a^{13}}\\right)e_1\\wedge e_2\n\\end{eqnarray}\n$$\nFrom the second line to the third line, I'm using the fact that $e_i\\wedge e_i=0$ and $e_i\\wedge e_j=-e_j\\wedge e_i$.\nThis calculation gives us the first column of the $3\\times 3$ matrix representation of $\\Lambda^2(T)$. You can verify at a glance that the red number, the $(1,1)$ entry of the matrix of $\\Lambda^2(T)$, is precisely the $(1,1)$ minor of the matrix of $T$, i.e. the $2\\times 2$ subdeterminant of $T$ we get when we delete the 1st row and 1st column: \n$$\\begin{pmatrix}a^{11}&a^{12}&a^{13}\\\\a^{21}&\\color{red}{a^{22}}&\\color{red}{a^{23}}\\\\a^{31}&\\color{red}{a^{32}}&\\color{red}{a^{33}}\\end{pmatrix}$$\nSimilarly, the $(2,1)$ entry of the matrix of $\\Lambda^2(T)$ (in blue) is precisely the $(2,1)$ minor, the subdeterminant when we delete the second row and first column:\n$$\\begin{pmatrix}a^{11}&\\color{blue}{a^{12}}&\\color{blue}{a^{13}}\\\\a^{21}&a^{22}&a^{23}\\\\a^{31}&\\color{blue}{a^{32}}&\\color{blue}{a^{33}}\\end{pmatrix}$$\nI trust you're now convinced that the $(3,1)$ entry (in orange) is precisely the $(3,1)$ minor:\n$$\\begin{pmatrix}a^{11}&\\color{orange}{a^{12}}&\\color{orange}{a^{13}}\\\\a^{21}&\\color{orange}{a^{22}}&\\color{orange}{a^{23}}\\\\a^{31}&a^{32}&a^{33}\\end{pmatrix}$$\nLet me end with one final remark. None of these constructions requires that $T$ be an endomorphism. Nor do they apply only to the exterior algebra. More generally, given two vector spaces $V$ and $W$, a linear map $T: V\\to W$ induces homomorphisms at every degree of the tensor (not necessarily exterior) algebra.\n\nReferences\nBourbaki, Algebra I: Chapters 1-3, Proposition 10, page 529.\nMacLane and Birkhoff, Algebra, pages 563-564.", "meta": {"post_id": 1604461, "input_score": 11, "output_score": 36, "post_title": "Action of a matrix on the exterior algebra"}}
{"input": "This (long) paper, \n\nGuozhen Wang, Zhouli Xu.\n  \"On the uniqueness of the smooth structure of the 61-sphere.\"\n  arXiv:1601.02184 [math.AT].\n\nproves that\n\nthe only odd dimensional spheres with a unique smooth structure are $S^1$, $S^3$, $S^5$, $S^{61}$.\n\nThe new result is for $S^{61}$.\nIs it possible to give some intuition on this remarkable result, for those\nnot steeped in algebraic and differential geometry, and so not intimately familiar with homotopy groups of spheres? Any attempt would be welcomed.", "output": "Results of this form, and my intuition from them, come from the Kervaire-Milnor paper on exotic spheres. (There was never a homotopy spheres II. The purported content of that unpublished paper appears to be summarized in these notes, though I haven't read them.) I'm going to need to jump into the algebra here; personally, I couldn't tell you the difference between $S^{57}$ and $S^{61}$ without it.\n\nFor $n \\not\\equiv 2 \\bmod 4$, there is an exact sequence $$0 \\to \\Theta_n^{bp} \\to \\Theta_n \\to \\pi_n/J_n \\to 0.$$ For $n=4k-2$, instead we have the exact sequence $$0 \\to \\Theta_n^{bp} \\to \\Theta_n \\to \\pi_n/J_n \\xrightarrow{\\Phi_k} \\Bbb Z/2 \\to \\Theta_{n-1}^{bp} \\to 0.$$\n\nLet's start by introducing the cast of characters.\n$\\Theta_n$ is the group of homotopy $n$-spheres. It's smooth manifolds, up to diffeomorphism, which are homotopy equivalent (hence by Smale's h-cobordism theorem, and in low dimensions Perelman's and Freedman's work, homeomorphic) to the $n$-sphere $S^n$. (Actually, we identify $h$-cobordant manifolds. Because $h$-cobordism is now known in all dimensions at least 5, it changes nothing for high-dimensional manifolds; but it explains why $\\Theta_4=1$ is possible even though it's an open problem, suspected to be false, that the 4-sphere admits a unique smooth structure. In any case, this is not an important aside.) The group operation is connected sum. The data we're really after is $|\\Theta_n|$ - the number of smooth structures.\n$\\Theta_n^{bp}$ is the subgroup of those $n$-spheres which bound parallelizable manifolds. This subgroup is essential, because it's usually the fellow forcing us to have exotic spheres in the other dimensions.\nThis group is always cyclic (Kervaire and Milnor provide an explicit generator). As a rough justification for this group: the way this goes is by taking an arbitrary element, writing down a parallelizable manifold it bounds, and using the parallelizability condition to do some simplifying algebra until this bounding manifold is particularly simple - at which point you identify it as a connected sum of standard ones, hence that $\\Theta_n^{bp}$ is cyclic generated by the standard one. I (or rather, Milnor and Kervaire) can tell you its order: If $n$ is even, $|\\Theta_n^{bp}| = 0$; if $n=4k-1$, $$|\\Theta_n^{bp}|=2^{2k-2}(2^{2k-1}-1) \\cdot \\text{the numerator of }\\frac{4B_k}{k}$$ is sort of nasty, but in particular always nonzero when $k>1$; and for $n=4k-3$, it is either 0 or $\\Bbb Z/2$, the first precisely if $\\Phi_k \\neq 0$ in the above exact sequence.\n$\\pi_n/J$, and the map $\\Theta_n \\to \\pi_n/J$, is a bit harder to state; $\\pi_n$ is the stable-homotopy group of spheres, $J$ is the image of a certain map, and the map from $\\Theta_n$ sends a homotopy 7-sphere, which is stably parallelizable, to its \"framed cobordism class\". The real point, though, is that this term $\\pi_n/J$ is entirely the realm of stable homotopy theory. This is precisely why people now say that the exotic spheres problem is \"a homotopy theory problem\". (To give the slightest bit more detail: The Thom-Pontryagin construction gives that $\\pi_n = \\Omega_n^{fr}$, the framed cobordism group, whose elements are equivalence classes of manifolds with trivializations of the \"stable tangent bunde\". Every homotopy sphere is stably trivial, and the image of $J$ is precisely the difference between any two stable trivializations.) This map $\\Theta_n \\to \\pi_n/J$ might motivate the introduction of $\\Theta_n^{bp}$ - since that is, more or less obviously, the kernel. The fact that this map is not always surjective - the obstruction supplied by $\\Phi_k$ - is the statement that not every framed manifold is framed cobordant to a sphere. I find it somewhat surprising that so many actually are!\nThe last thing you should know is about the map $\\Phi_k$. It's known as the Kervaire invariant. It's known to be nonzero in dimensions $k=1,2,4,8,16$, and might be nonzero in dimension $32$, but that's open. The remarkable result of Mike Hill, Mike Hopkins, and Doug Ravenel is that $\\Phi_k = 0$ for $k > 32$. I don't have much to say about this, other than that it's there. Summing up what we have so far:\n\nFor dimensions $n=4k-1>3$, there are always exotic spheres coming from $\\Theta_n^{bp}$ - lots of them! For dimensions $n=4k-3$, $\\Theta_n^{bp} = \\Bbb Z/2$ unless $k=1,2,4,8,16,32$. So the only possible odd-dimensional spheres with a unique smooth structure are $S^1$, $S^3, S^5, S^{13}, S^{29}, S^{61}$, and $S^{125}$.\n\nNow to deal with special cases. It is classical that $S^1$ and $S^3$ have a unique smooth structure ($S^3$ is due to Moise); $S^5$ is dealt with by 1) finding a 6-manifold of nonzero Kervaire invariant, showing that $\\Phi_2 \\neq 0$ and hence that $\\Theta_5^{bp}=0$; and then 2) calculating that $\\pi_5$, the fifth stable homotopy group of spheres, is zero. You can do this with Serre's spectral sequence calculations. (It was pointed out to me that this means that three different field's medalists work went into getting $\\Theta_5 = 1$ - Milnor, Serre, Smale. It is worth noting that there is a differential topological proof, coming from the explicit classification of smooth, simply-connected 5-manifolds, but it isn't substantially easier or anything.) \nFor $S^{13}$ and $S^{29}$, these are disqualified by the homotopy theory calculation that $\\pi_{13}/J$ and $\\pi_{29}/J$ are not zero. I do not know how these calculations are done - probably the Adams spectral sequence and a lot of auxiliary spectral sequences, which seems to be how a lot of these things are done. Maybe someone else can shed some light on that.\nFor $S^{125}$, the paper itself sketches why: There's a spectrum known as $tmf$, and the authors are able to write down a homomorphism $\\pi_n/J \\to \\pi_n{tmf}$ and find a class in $tmf$ that's hit when $n=125$.\nSo what we know now is that $\\pi_{61}/J \\cong \\Theta_n$. The content of the paper you're talking about is precisely the calculation that $\\pi_{61}/J = 0$. The authors access it through the Adams spectral sequence, as far as I can tell (I am a non-expert). Adams SS is notoriously hard to calculate anything with - mostly the entire content of the paper is identifying of a single differential in the whole spectral sequence. Once this is done, they're able to finish the calculation, but it's hard work. If you want a sketch of how this is done, I found the introduction to their paper readable - see section 3 of the paper.", "meta": {"post_id": 1609328, "input_score": 41, "output_score": 44, "post_title": "\"the only odd dimensional spheres with a unique smooth structure are $S^1$, $S^3$, $S^5$, $S^{61}$\""}}
{"input": "If $S = A \\cup B$, then $S$ is the collection of all points in $A$ and $B$\n\nWhat about $S = A \\sqcup B$?, I think disjoint union is the same as union, only $A, B$ are disjoint. So the notation is a bit misleading. Because it is not a new operation, but operation where the pair $A,B$ satisfies $A \\cap B = \\varnothing$. \nSo given $A \\cap B = \\varnothing$, $S = A \\sqcup B = A \\cup B$.\nIs my interpretation correct?", "output": "The notation $A\\sqcup B$ (and phrase \"disjoint union\") has (at least) two different meanings.  The first is the meaning you suggest: a union that happens to be disjoint.  That is, $A\\sqcup B$ is identical to $A\\cup B$, but you're only allowed to write $A\\sqcup B$ if $A$ and $B$ are disjoint.\nThe second meaning is that $A\\sqcup B$ is a union of sets that look like $A$ and $B$ but have been forced to be disjoint.  There are many ways of defining this precisely; for instance, you could define $A\\sqcup B= A\\times\\{0\\}\\cup B\\times \\{1\\}$.  This construction can also be described as the coproduct of $A$ and $B$ in the category of sets.\n(This ambiguity is similar to the ambiguity between \"internal\" and \"external\" direct sums; see for instance my answer here.)", "meta": {"post_id": 1631396, "input_score": 31, "output_score": 44, "post_title": "What is the difference between disjoint union and union?"}}
{"input": "Given $n\\in\\mathbb{N}$, I need to get just enough more than half of it. For example (you can think this is : number of games $\\rightarrow$ minimum turns to win)\n$$ 1 \\rightarrow 1 $$\n$$ 2 \\rightarrow 2 $$\n$$ 3 \\rightarrow 2 $$\n$$ 4 \\rightarrow 3 $$\n$$ 5 \\rightarrow 3 $$\n$$ 6 \\rightarrow 4 $$\n$$ 7 \\rightarrow 4 $$\n$$ \\vdots $$\n$$ 2i \\rightarrow i+1 $$\n$$ 2i+1 \\rightarrow i+1 $$\n$$ \\vdots $$\nIs it possible to create a simple formula without piecewise it into odd and even? Sorry for my bad English.", "output": "How about:\n$$\n\\frac{3+2n+(-1)^n}{4}\n$$\nor (continuous function of $n \\in \\mathbb R$ or even $\\mathbb C$):\n$$\n\\frac{3+2n+\\cos(\\pi n)}{4}\n$$", "meta": {"post_id": 1633901, "input_score": 9, "output_score": 62, "post_title": "Formula for the simple sequence 1, 2, 2, 3, 3, 4, 4, 5, 5, ..."}}
{"input": "As far as I know, mathematics is written based on a language which can be for example set theory or category theory. My concern is about the power of these languages. How can we realize which language is more powerful and we can construct more structures with them? Maybe, there is an assumption which implies all languages are equal, and actually, there is no strength preference although there is maybe the simplicity preference.   If it is the case, could you please clarify why you think this assumption is good and obvious?\np.s. : I am not an expert, so if you think my question is absolutely wrong or does not make sense at all, please let me know.", "output": "Set theory and category theory are both foundational theories of mathematics (they explain basics), but they attack different aspects of foundations. Set theory is largely concerned with \"how do we build mathematical objects (or what could we build)\" while category theory is largely concerned with \"what structure to mathematical objects have (or could have)\"?\nMathematicians work in informal set theory and informal category theory, which are immensly useful as lingua franca and as collections of universally useful concepts and techniques, but their formal versions are not actually needed by mathematicians for the most part. This is witnessed by the fact that the average mathematician is unable to list the axioms of Zermelo-Fraenkel set theory, and even of first-order logic. Yet, they are perfectly able to do complicated math.\nThe formal versions of set theory and category theory are of interest to people who study foundations of mathematics. These relationship between these two and computation has been known for a while. I highly recommend Bob Harper's blog post about the Holy Trinity for a quick read, and Steve Awodey's From Sets to Types to Categories to Sets if you would like know more about the connections and their significance.\nThe upshot is that we can mostly translate between set theory, type theory, and category theory, and that ordinary mathematicians could do their mathematics in either of the three systems, but the systems are not exclusive. In fact, a smart mathematician will be aware of their connections and will take advantage of them.", "meta": {"post_id": 1639982, "input_score": 7, "output_score": 34, "post_title": "Which is the most powerful language, set theory or category theory?"}}
{"input": "Prove the following without using L'Hospital's Rule, integration or Taylor Series:\n$$\\lim_{n \\to \\infty} \\frac{\\ln(n)}{n}=0 $$\nI began by rewriting the expression as: $$\\lim_{n \\to \\infty}{\\ln(n^{1/n})} $$\nSince the text shows $$\\lim_{n \\to \\infty}{n^{1/n} = 1} $$\nI was wondering is the proof just as simple as stating:\n$$\\lim_{n \\to \\infty}{\\ln(1) = 0} $$\nor do I need to apply the squeeze theorem,  use a   $\\varepsilon$-N proof, or etc?", "output": "Since $e^x> x$, we have $\\ln x < x$ for all $x  >0$. \nHence,\n$$0 \\leqslant \\frac{\\ln n}{n} = \\frac{2 \\ln \\sqrt{n}}{n} < \\frac{2 \\sqrt{n}}{n} = \\frac{2}{\\sqrt{n}} \\to 0$$", "meta": {"post_id": 1642671, "input_score": 7, "output_score": 36, "post_title": "Prove $\\lim_{n \\to \\infty} \\frac{\\ln(n)}{n}=0$ without L'Hospital's Rule"}}
{"input": "I was reading some topics in Homological Algebra when I came across the concepts of cone of a map of complexes and cylinder.\nMy knowledge of Algebraic Topology is pretty basic so I only used these concepts in a pure algebraic setting. What is the motivation for this? \nThe shapes of a cone for example appears only in the simplicial context of Algebraic Topology or is it possible to \"see\" the cone in algebraic terms ?", "output": "One possible motivation for the mapping cone is the fact that a morphism of chain complexes is a quasi-isomorphism iff its mapping cone has vanishing homology. So in this sense, the homology of the mapping cone of $f$ measures the default of $f$ to be a quasi-isomorphism.\nFrom an abstract homotopy theory point of view, one can first consider the mapping cylinder of $f : X_* \\to Y_*$. In algebraic topology, the mapping cylinder of $f : X \\to Y$ is $Y \\cup_{X \\times 0} X \\times I$. We'll try to see how this could be translated in homological algebra, and hopefully the picture will be clearer.\nIn homological algebra, the interval $I = [0,1]$ is replaced by the chain complex $I_*$ that has $I_0 = \\mathbb{Z} v_+ \\oplus \\mathbb{Z} v_-$ (this is a free abelian group of rank two) and  $I_1 = \\mathbb{Z} e$, $I_n = 0$ if $n \\neq 0,1$, and $d : I_1 \\to I_0$ is given by $d(e) = v_+ - v_-$. It's an acyclic chain complex that represents an interval (in some sense that can be made precise; it is a path object in the model category of chain complexes). Roughly speaking $v_+$ is the vertex $\\{1\\}$, $v_-$ is the vertex $0$, and $e$ is the edge between the two.\nThe product $X \\times I$ becomes the tensor product $X_* \\otimes I_*$, which has:\n$$(X \\otimes I)_n = X_n \\otimes v_+ \\oplus X_n \\otimes v_- \\oplus X_{n-1} \\otimes e$$\nand the differential is given by $$d(x \\otimes v_\\pm) = dx \\otimes v_\\pm, \\\\ d(x \\otimes e) = dx \\otimes e + x \\otimes v_+ - x \\otimes v_-.$$\nAnd now the mapping cylinder $Y \\cup_{X \\times 0} X \\times I$ is replaced by $\\operatorname{Cyl}(f) = Y \\oplus_{X \\otimes v_-} X \\otimes I$. It is the quotient of $Y \\oplus X \\otimes I$ where you identify $x \\otimes v_- \\in X \\otimes I$ with $f(x) \\in Y$ (recall that $v_-$ represents the vertex $0 \\in [0,1]$). So concretely we get:\n$$\\operatorname{Cyl}(f)_n = Y_n \\oplus X_n \\oplus X_{n-1} \\\\\nd(y, 0, 0) = (dy, 0, 0) \\\\\nd(0,x,0) = (0, dx, 0) \\\\\nd(0,0,x') = (-f(x'), x', dx')$$\nThe first factor is the image of $X \\otimes v_-$, which is identified with $Y$. The second factor is $X \\otimes v_+$, and the last part is $X \\otimes e$.\nNow to get the mapping cone from the mapping cylinder, in algebraic topology you collapse $X \\times 1$. The $X \\times 1$ part in homological algebra corresponds to the middle $X_n$ (really $X_n \\otimes v_+$) in $\\operatorname{Cyl}(f)_n$, so just quotient out by this ideal to get\n$$\\operatorname{Cone}(f)_n = Y_n \\oplus X_{n-1}\\\\\nd(y,0) = (dy, 0) \\\\\nd(0,x') = (-f(x'), dx)$$\nAnd this is exactly the definition of the mapping cone. There are various way to get to this result in a systematic manner. For example you can put what is called a model structure on the category of chain complexes, and then the mapping cone of $f$ becomes its homotopy cokernel. Or you can put a triangulated structure on it (though that's a bit circular, since you need to know what the mapping cone is to get the triangulated structure).\nPS: A lot of things that are true in algebraic topology are also true in homological algebra. For example, if you have $A \\subset X$, you can consider the cone on $A$ to get $X \\cup CA$, then you can cone $X$ inside it to get $(X \\cup CA) \\cup CX$, and this is homotopy equivalent to the suspension $\\Sigma A$ (the beginning of the Puppe sequence). Well, in homological algebra it's exactly the same: say you have a subcomplex $i : A_* \\to X_*$, you can take the cone $\\operatorname{Cone}(i)$, of which $X_*$ is a subcomplex; if you then take the cone of this inclusion, you get a complex homotopy equivalent to the suspension (shift in degree) of $A_*$. This is because all this can be encoded in the triangulated structure of chain complexes!\n$$ $$", "meta": {"post_id": 1667399, "input_score": 16, "output_score": 35, "post_title": "Motivation for the mapping cone complexes"}}
{"input": "Looking at the sum:\n$$\\sum_{n=1}^\\infty\\tan\\left(\\frac\\pi{2^n}\\right)$$\nI'd say that it does not converge, because for $n=1$ the tangent $\\tan\\left(\\frac\\pi 2\\right)$ should be undefined. But Wolframlpha thinks that the sum converges somewhere around $1.63312\u00d710^{16}$.\nWhat am I missing?", "output": "For floating point numbers stored in IEEE double precision format, the significant has $53$ bit of accuracy. The most significant bit is implied and is always one. Only $52$ bits are actually stored.\nSince $1 \\le \\frac{\\pi}{2} < 2$, among those numbers representable by IEEE,\nthe closest number to $\\frac{\\pi}{2}$ is\n$$\\left(\\frac{\\pi}{2}\\right)_{fp} \\stackrel{def}{=} 2^{-52}\\left\\lfloor \\frac{\\pi}{2} \\times 2^{52}\\right\\rfloor$$\nNumerically, we have $$\\frac{\\pi}{2} - \\left(\\frac{\\pi}{2}\\right)_{fp} \\approx 6.1232339957\\times 10^{-17}$$\nSince for $\\theta \\approx \\frac{\\pi}{2}$, $\\displaystyle\\;\\tan\\theta \\approx \\frac{1}{\\frac{\\pi}{2} - \\theta}$, we have\n$$\\tan\\left(\\frac{\\pi}{2}\\right)_{fp}\n\\approx \\frac{1}{6.1232339957\\times 10^{-17}}\n\\approx 1.6331239353 \\times 10^{16}$$\nThis is approximately the number you observed.", "meta": {"post_id": 1678363, "input_score": 34, "output_score": 52, "post_title": "Why does Wolframalpha think that this sum converges?"}}
{"input": "In propositional logic, there are truth tables. So you can check if the logical structure of your argument is, not correct per se, but if it's what you intended it to be.\nIn predicate logic, I have seen no reference to truth tables, nor have I seen any use (literal use) of truth tables when searching for examples where truth tables are used in PL.\nIt would be nice to check the logical structure of my own arguments, as I will not always have someone to validate my own work. I plan on employing my skills in logic, but I want a sure fire way to ensure that my form is correct :)", "output": "Truth tables are not enough to capture first-order logic (with quantifiers), so we use inference rules instead. Each inference rule is chosen to be sound, meaning that if you start with true statements and use the rule you will deduce only true statements. We say that these rules are truth-preserving. If you choose carefully enough, you can make it so that the rules are not just truth-preserving but also allow you to deduce every (well-formed) statement that is necessarily true (in all situations).\nWhat you are probably looking for (namely a practical way to rigorously check the logical validity of your arguments) is natural deduction. There are many different styles, the most intuitive type being Fitch-style, which mark subcontexts using indentation or some related visual demarcation. The following system uses indentation and follows the intuition most closely in my opinion.\n$\n\\def\\block#1{\\begin{array}{ll}\\ &{#1}\\end{array}}\n\\def\\fitch#1#2{\\begin{array}{|l}#1\\\\\\hline#2\\end{array}}\n\\def\\sub#1#2{\\text{#1}:\\\\\\block{#2}}\n\\def\\imp{\\Rightarrow}\n\\def\\eq{\\Leftrightarrow}\n\\def\\nn{\\mathbb{N}}\n\\def\\none{\\varnothing}\n\\def\\pow{\\mathcal{P}}\n$\n\nContexts\nEvery line is either a header or a statement. We shall put a colon after each header and a full-stop after each statement. Each header specifies some subcontext (contained by the current context), and the lines governed by that header is indicated by the indentation. The full context of each line is specified by all the headers that govern it (i.e. all the nearest headers above it at each lower indentation level).\nFor example a nested case analysis might look like:\n\u2003 $\\sub{If $A$}{\\sub{If $B$}{...} \\\\ \\sub{If $\u00acB$}{...}} \\\\ \\sub{If $\\neg A$}{...}$\nAnd reasoning about an arbitrary member of a collection $S$ would look like:\n\u2003 $\\sub{Given $x{\u2208}S$}{...}$\nNote that what is stated in some context may be invalid in other contexts. Once you understand the principle behind contexts and the indentation, the following rules are very natural. Also note that for first-order logic these two kinds of context headers (for conditional subcontexts and universal subcontexts respectively) are the only kinds needed.\nSyntax rules\nA statement must be an atomic (indivisible) proposition or a compound statement formed in the usual way using boolean operations or quantifiers, with the restriction that every variable that is bound by a quantifier is not already used to refer to some object in the current context, and that there are no nested quantifiers that bind the same variable.\nNatural deduction rules\nEach inference rule is of the form:\n\u2003 $\\fitch{\\text{X}}{\\text{Y}}$\nwhich means that if the last lines you have written match \"X\" then you can write \"Y\" immediately after that at the same level of indentation. Each application of an inference rule is also tied to the current context, namely the context of \"X\". We will not mention \"current context\" all the time.\n\nBoolean operations\nTake any statements $A,B,C$ (in the current context).\nrestate: If we prove something we can affirm it again in the same context.\n\u2003 $\\fitch{A.\\\\ ...}{A.}$\nNote that \"$...$\" denote any number of lines that are at least at the depicted indentation level. In the above rule, this means that all the lines written since the earlier writing of \"$A.$\" must be in the same context (or some subcontext).\nIn practice we never actually write the same line twice. To indicate that we can omit a line in a proof, I'll mark it with square-brackets like this:\n\u2003 $\\fitch{A. \\\\ ...}{[A.]}$\n\u21d2sub \u2003 \u2003 \u2003 \u21d2restate \u2003 \u2003 (We can create a conditional subcontext where $A$ holds.)\n\u2003 $\\fitch{}{\\sub{If $A$}{[A.]}}$\n\u2003 $\\fitch{B. \\\\ ... \\\\ \\sub{If $A$}{...}}{\\block{[B.]}}$\n\u21d2intro \u2003 \u2003 \u2003 \u21d2elim\n\u2003 $\\fitch{\\sub{If $A$}{... \\\\ B.}}{[A \\imp B.]}$\n\u2003 $\\fitch{A \\imp B. \\\\ A.}{B.}$\n\u2227intro \u2003 \u2003 \u2227elim\n\u2003 $\\fitch{A. \\\\ B.}{A \\land B.}$\n\u2003 $\\fitch{A \\land B.}{[A.] \\\\ [B.]}$\n\u2228intro \u2003 \u2003 \u2228elim\n\u2003 $\\fitch{A.}{[A \\lor B.] \\\\ [B \\lor A.]}$\n\u2003 $\\fitch{A \\lor B. \\\\ A \\imp C. \\\\ B \\imp C.}{C.}$\n\u00acintro \u2003 \u2003 \u00acelim \u2003 \u2003 \u00ac\u00acelim\n\u2003 $\\fitch{A \\imp \\bot.}{\\neg A.}$\n\u2003 $\\fitch{A. \\\\ \\neg A.}{\\bot.}$\n\u2003 $\\fitch{\\neg \\neg A.}{A.}$\nNote that by using \u00acintro and \u00ac\u00acelim we can get the following additional inference rule:\n\u2003 $\\fitch{\\neg A \\imp \\bot.}{A.}$\nwhich corresponds to how one would attempt to prove $A$ by contradiction, namely to show that assuming $\\neg A$ implies a falsehood.\n\u21d4intro \u2003 \u2003 \u21d4elim\n\u2003 $\\fitch{A \\imp B. \\\\ B \\imp A.}{A \\eq B.}$\n\u2003 $\\fitch{A \\eq B.}{[A \\imp B.] \\\\ [B \\imp A.]}$\nQuantifiers and equality\nThe rules here are for restricted quantifiers because usually we think in terms of them. First we need some definitions.\nUsed variable: A variable that is declared in the header of some containing \u2200-context or declared in some previous \u2203-elimination (\"let\") step in some containing context.\nUnused variable: A variable that is not used.\nFresh variable: A variable that does not appear in any previous statement in any containing context.\nObject expression: An expression that refers to an object (e.g. a used variable, or a function-symbol applied to object expressions).\nProperty with $k$ parameters: A string $P$ with some blanks where each blank has some label from $1$ to $k$, such that replacing each blank in $P$ by some object expression yields a statement. If $k = 2$, then $P(E,F)$ is the result of replacing each blank labelled $1$ by $E$ and replacing each blank labelled $2$ by $F$. Similarly for other $k$.\nIn this section, $E,F$ (if involved) can be any object expressions (in the current context).\nWe start with the following rules that provide a type of all objects.\nuniverse: $obj$ is a type.\n\u2003 $\\fitch{}{[E{\u2208}obj.]}$\nNow take any type $S$ and a 1-parameter property $P$ and an unused variable $x$ that does not appear in $S$ or $P$.\n\u2200sub \u2003 \u2003 \u2003 \u2003 \u2003 \u2200restate \u2003 \u2003 \u2003 \u2003 (We can create a \u2200-subcontext in which $x$ is of type $S$.)\n\u2003 $\\fitch{}{\\sub{Given $x{\u2208}S$}{[x{\u2208}S.]}}$\n\u2003 $\\fitch{A. \\\\ ... \\\\ \\sub{Given $x{\u2208}S$}{...}}{\\block{[A.]}}$ ($x$ must not appear in $A$)\n\u2200intro \u2003 \u2003 \u2003 \u2003 \u2003 \u2200elim\n\u2003 $\\fitch{\\sub{Given $x{\u2208}S$}{... \\\\ P(x).}}{\\forall x{\u2208}S\\ ( \\ P(x) \\ ).}$\n\u2003 $\\fitch{\\forall x{\u2208}S\\ ( \\ P(x) \\ ). \\\\ E{\u2208}S.}{P(E).}$ ($E$ must not share any unused variables with $P$)\n\u2203intro \u2003 \u2003 \u2003 \u2003 \u2003 \u2203elim\n\u2003 $\\fitch{E{\u2208}S. \\\\ P(E).}{\\exists x{\u2208}S\\ ( \\ P(x) \\ ).}$\n\u2003 $\\fitch{\\exists x{\u2208}S\\ ( \\ P(x) \\ ).}{\\text{Let $y{\u2208}S$ such that $P(y)$}. \\\\ [y{\u2208}S.] \\\\ [P(y).]}$ (where $y$ is a fresh variable)\n=intro \u2003 \u2003 \u2003 =elim\n\u2003 $\\fitch{}{[E=E.]}$\n\u2003 $\\fitch{E=F. \\\\ P(E).}{P(F).}$ ($F$ must not share any unused variable with $P$)\nVariable renaming\nFinally, the following rules for variable renaming are redundant, but would shorten proofs.\n\u2200rename \u2003 \u2003 \u2003 \u2003 \u2203rename\n\u2003 $\\fitch{\\forall x{\u2208}S\\ ( \\ P(x) \\ ).}{[\\forall y{\u2208}S\\ ( \\ P(y) \\ ).]}$\n\u2003 $\\fitch{\\exists x{\u2208}S\\ ( \\ P(x) \\ ).}{[\\exists y{\u2208}S\\ ( \\ P(y) \\ ).]}$\n\u2003 (where $y$ is an unused variable that does not appear in $P$)\nShort-forms\nFor convenience we write \"$\\forall x,y{\u2208}S\\ ( \\ P(x,y) \\ )$\" as short-form for \"$\\forall x{\u2208}S\\ ( \\ \\forall y{\u2208}S\\ ( \\ P(x,y) \\ ) \\ )$\", and similarly for more variables and for \"$\\exists$\". We shall also compress nested \u2200-subcontext headers in the following form:\n\u2003 $\\sub{Given $x{\u2208}S$}{\\sub{Given $y{\u2208}S$}{...}}$\nto:\n\u2003 $\\sub{Given $x,y{\u2208}S$}{...}$\nAdditionally, \"$\\exists! x{\u2208}S\\ ( \\ P(x) \\ )$\" is short-form for \"$\\exists x{\u2208}S\\ ( \\ P(x) \\land \\forall y{\u2208}S\\ ( \\ P(y) \\imp x=y \\ ) \\ )$\".\n\nExample\nHere is an example, where $S,T$ are any types and $P$ is any property with two parameters.\nFirst with all lines shown:\n\u2003 If $\\exists x{\u2208}S\\ ( \\ \\forall y{\u2208}T\\ ( \\ P(x,y) \\ ) \\ )$: \u2003 [\u21d2sub]\n\u2003 \u2003 $\\exists x{\u2208}S\\ ( \\ \\forall y{\u2208}T\\ ( \\ P(x,y) \\ ) \\ )$. \u2003 [\u21d2sub]\n\u2003 \u2003 Let $a{\u2208}S$ such that $\\forall y{\u2208}T\\ ( \\ P(a,y) \\ )$. \u2003 [\u2203elim]\n\u2003 \u2003 $a{\u2208}S$. \u2003 [\u2203elim]\n\u2003 \u2003 $\\forall y{\u2208}T\\ ( \\ P(a,y) \\ )$. \u2003 [\u2203elim]\n\u2003 \u2003 $\\forall z{\u2208}T\\ ( \\ P(a,z) \\ )$. \u2003 [\u2200rename]\n\u2003 \u2003 Given $y{\u2208}T$: \u2003 [\u2200sub]\n\u2003 \u2003 \u2003 $y{\u2208}T$. \u2003 [\u2200sub]\n\u2003 \u2003 \u2003 $\\forall z{\u2208}T\\ ( \\ P(a,z) \\ )$. \u2003 [\u2200restate]\n\u2003 \u2003 \u2003 $y{\u2208}T$. \u2003 [restate]\n\u2003 \u2003 \u2003 $P(a,y)$. \u2003 [\u2200elim]\n\u2003 \u2003 \u2003 $a{\u2208}S$. \u2003 [\u2200restate]\n\u2003 \u2003 \u2003 $\\exists x{\u2208}S\\ ( \\ P(x,y) \\ )$. \u2003 [\u2203intro]\n\u2003 \u2003 $\\forall y{\u2208}T\\ ( \\ \\exists x{\u2208}S\\ ( \\ P(x,y) \\ ) \\ )$. \u2003 [\u2200intro]\n\u2003 $\\exists x{\u2208}S\\ ( \\ \\forall y{\u2208}T\\ ( \\ P(x,y) \\ ) \\ ) \\imp \\forall y{\u2208}T\\ ( \\ \\exists x{\u2208}S\\ ( \\ P(x,y) \\ ) \\ )$. \u2003 [\u21d2intro]\nFinally with all lines in square-brackets removed:\n\u2003 If $\\exists x{\u2208}S\\ ( \\ \\forall y{\u2208}T\\ ( \\ P(x,y) \\ ) \\ )$: \u2003 [\u21d2sub]\n\u2003 \u2003 Let $a{\u2208}S$ such that $\\forall y{\u2208}T\\ ( \\ P(a,y) \\ )$. \u2003 [\u2203elim]\n\u2003 \u2003 Given $y{\u2208}T$: \u2003 [\u2200sub]\n\u2003 \u2003 \u2003 $P(a,y)$. \u2003 [\u2200elim]\n\u2003 \u2003 \u2003 $\\exists x{\u2208}S\\ ( \\ P(x,y) \\ )$. \u2003 [\u2203intro]\n\u2003 \u2003 $\\forall y{\u2208}T\\ ( \\ \\exists x{\u2208}S\\ ( \\ P(x,y) \\ ) \\ )$. \u2003 [\u2200intro]\n\u2003 $\\exists x{\u2208}S\\ ( \\ \\forall y{\u2208}T\\ ( \\ P(x,y) \\ ) \\ ) \\imp \\forall y{\u2208}T\\ ( \\ \\exists x{\u2208}S\\ ( \\ P(x,y) \\ ) \\ )$. \u2003 [\u21d2intro]\nThis final proof is clean yet still easily computer-verifiable.\nDefinitorial expansion\nTo facilitate definitions, which can significantly shorten proofs, we also have the following definitorial expansion rules.\nFor each $k$-parameter property $P$ and fresh predicate-symbol $Q$:\n\u2003 $\\fitch{}{\\text{Let $Q(x_1,...x_k) \u2261 P(x_1,...x_k)$ for each $x_1{\u2208}S_1$ and ... and $x_k{\u2208}S_k$.}\n \\\\ [\u2200x_1{\u2208}S_1\\ \\cdots \u2200x_k{\u2208}S_k\\ ( \\ Q(x_1,...x_k) \u21d4 P(x_1,...x_k) \\ ).]}$\nFor each $(k+1)$-parameter property $R$ and fresh function-symbol $f$:\n\u2003 $\\fitch{\u2200x_1{\u2208}S_1 \\cdots \u2200x_k{\u2208}S_k\\ \u2203!y{\u2208}T ( \\ R(x_1,...x_k,y) \\ ) }\n{\\text{Let $f : S_1{\u00d7}{\\cdots}{\u00d7}S_k{\u2192}T$ such that $R(x_1,...x_k,f(x_1,...x_k))$ for each $x_1{\u2208}S_1$ and ... and $x_k{\u2208}S_k$.}\n \\\\ [\u2200x_1{\u2208}S_1\\ \\cdots \u2200x_k{\u2208}S_k\\ ( \\ f(x_1,...x_k)\u2208T \u2227 R(x_1,...x_k,f(x_1,...x_k)) \\ ).]}$\nThese rules are redundant in the sense that any statement you can prove that does not use any of the new symbols can be proven without using definitorial expansion.\nNotes\nThe above rules avoid the usual trouble that many other systems have, where variables used for witnesses of existential statements must be distinguished from variables used for arbitrary objects. The reason is that every variable here is either specified by a \u2200-subcontext or by a \"let\"-statement; in other words there are no free variables. The fact that every variable is bound is strongly related to the fact that this system allows an empty universe, if there are no other axioms.\nAlso, every variable is specified by a unique header or \"let\"-statement in the current context; in other words there is no variable shadowing. This is by design, and in actual mathematical practice we also abide by this, though most other formal systems do not. As a consequence, sentences such as \"$\\exists x\\ \\forall x\\ ( x=x )$.\" simply cannot be written down in this system. If you wish to permit such kind of terrible sentences, you would have to modify the rules appropriately, but it will most probably cause a headache.\nFinally, there were some subtle technical decisions. For the quantifier rules, the reason I required that $x$ does not appear in $S,P$ is that, if we later on include rules for specifying types, we would usually have variable names in its syntax, which would cause problems. For example, if we have written in the current context \"$x\u2208\\{y:y\u2208S\u2227y\u2208T\\}$\" and \"$x\u2208U$\", it will not be sensible to allow writing \"$\u2203y\u2208U\\ ( y\u2208\\{y:y\u2208S\u2227y\u2208T\\} )$\". Similarly, if we have written \"$x=\\{y:P(y)\\}$\" and \"$\u2203y\u2208U\\ ( Q(x,y) )$\", we do not want to allow writing \"$\u2203y\u2208U\\ ( Q(\\{y:P(y)\\},y) )$\".\nAlso, to allow a variable to become fresh again after leaving the subcontext in which it was declared, I required that the \u21d2intro and \u2200intro rules can be applied only immediately after the corresponding \u21d2-subcontext or \u2200-subcontext. It would be simpler to simply define a fresh variable as one that does not appear in any previous line, but then we can easily run out of fresh variable names in a long proof.\n~ ~ ~ ~ ~ ~ ~\nTo illustrate the flexibility of this system, I will express both Peano Arithmetic and Set Theory as extra rules that can be simply added to the system.\nPeano Arithmetic\nAdd the type $\\nn$ and the symbols of PA, namely the constant-symbols $0,1$ and the $2$-input function-symbols $+,\u00b7$ and the $2$-input predicate-symbol $<$.\nAdd the axioms of PA$^-$, adapted from here:\n\n$\\forall x,y{\u2208}\\nn\\ ( \\ x+y \u2208 \\nn \\ )$.\n$\\forall x,y{\u2208}\\nn\\ ( \\ x\u00b7y \u2208 \\nn \\ )$.\n$\\forall x,y{\u2208}\\nn\\ ( \\ x+y=y+x \\ )$.\n$\\forall x,y{\u2208}\\nn\\ ( \\ x\u00b7y=y\u00b7x \\ )$.\n$\\forall x,y,z{\u2208}\\nn\\ ( \\ x+(y+z)=(x+y)+z \\ )$.\n$\\forall x,y,z{\u2208}\\nn\\ ( \\ x\u00b7(y\u00b7z)=(x\u00b7y)\u00b7z \\ )$.\n$\\forall x,y,z{\u2208}\\nn\\ ( \\ x\u00b7(y+z)=x\u00b7y+x\u00b7z \\ )$.\n$\\forall x{\u2208}\\nn\\ ( \\ x+0=x \\ )$.\n$\\forall x{\u2208}\\nn\\ ( \\ x\u00b71=x \\ )$.\n$\\forall x{\u2208}\\nn\\ ( \\ \\neg x<x \\ )$.\n$\\forall x,y{\u2208}\\nn\\ ( \\ x<y \\lor y<x \\lor x=y \\ )$.\n$\\forall x,y,z{\u2208}\\nn\\ ( \\ x<y \\land y<z \\imp x<z \\ )$.\n$\\forall x,y,z{\u2208}\\nn\\ ( \\ x<y \\imp x+z<y+z \\ )$.\n$\\forall x,y,z{\u2208}\\nn\\ ( \\ x<y \\land 0<z \\imp x\u00b7z<y\u00b7z \\ )$.\n$\\forall x,y{\u2208}\\nn\\ ( \\ x<y \\imp \\exists z{\u2208}\\nn\\ ( \\ x+z=y \\ ) \\ )$.\n$0<1$.\n$\\forall x{\u2208}\\nn\\ ( \\ 0=x \\lor 1=x \\lor 1<x \\ )$.\n\nAdd the induction axioms, namely for each property $P$ involving only the symbols of PA and quantifiers over $\\nn$ add the following axiom (where $k$ does not appear in $P$):\n\n$P(0) \\land \\forall k{\u2208}\\nn\\ ( \\ P(k) \\imp P(k+1) \\ ) \\imp \\forall k{\u2208}\\nn\\ ( \\ P(k) \\ )$.\n\nSet Theory\nAdd the type $set$ and the rule that every member of $set$ is also a type.\nAdd the unary function-symbols $\\pow,\\bigcup$, the binary function-symbols $\u00d7,\u2192$, and the constant-symbol $\\none$. We reuse the binary predicate-symbol $\\in$, as there will be no ambiguity. Also add the following rules (in every context) for the other notation:\n\nIf $E,F \\in obj$, then $(E,F) \\in obj$ and $\\{E,F\\} \\in set$.\nIf $S,T \\in set$, then $S\u00d7T \\in set$ and $(S{\u2192}T) \\in set$ and $\\pow(S) \\in set$.\nIf $S,T \\in set$ and $f \\in (S{\u2192}T)$ and $x \\in S$, then $f(x) \\in T$.\nIf $S \\in set$ and $\\forall x{\u2208}S\\ ( \\ x \\in set \\ )$, then $\\bigcup(S) \\in set$.\n\nAdd the following axioms:\n\nextensionality: \u2003 $\\forall S,T{\u2208}set\\ ( \\ S=T \\eq \\forall x{\u2208}obj\\ ( \\ x \\in S \\eq x \\in T \\ ) \\ ).$\nempty-set: \u2003 $\\forall x{\u2208}obj\\ ( \\ \\neg x \\in \\none \\ ).$\nnaturals: \u2003 $\\nn{\u2208}set$.\npower-set: \u2003 $\\forall S{\u2208}set\\ ( \\ \\pow(S) = \\{ T : T \\in set \\land \\forall x{\u2208}T\\ ( \\ x \\in S \\ ) \\} ).$\npair: \u2003 $\\forall x,y{\u2208}obj\\ ( \\ \\{x,y\\} = \\{ z : z=x \\lor z=y \\} ).$\nordered-pair: \u2003 $\\forall x,y{\u2208}obj\\ \\forall z,w{\u2208}obj\\ ( \\ (x,y)=(z,w) \\eq x=z \\land y=w \\ ).$\nproduct-type: \u2003 $\\forall S,T{\u2208}set\\ ( \\ S\u00d7T = \\{ t : \\exists x{\u2208}S\\ \\exists y{\u2208}T\\ ( \\ t=(x,y) \\ ) \\} \\ ).$\nfunction-type: \u2003 $\\forall S,T{\u2208}set\\ ( \\ (S\u2192T) = \\\\ \\{ F : F \\in \\pow(S\u00d7T) \\land \\forall x{\u2208}S\\ \\exists! y{\u2208}T\\ ( \\ (x,y) \\in F \\ ) \\} \\ ).$\nfunction-application: \u2003 $\\forall S,T{\u2208}set\\ \\forall f{\u2208}(S{\u2192}T)\\ \\forall x{\u2208}S\\ ( \\ (x,f(x)) \\in f \\ ).$\nunion: \u2003 $\\forall S{\u2208}set\\ ( \\ \\bigcup(S) = \\{ x : \\exists T{\u2208}set\\ ( \\ x \\in T \\land T \\in S \\ ) \\} \\ ).$\nchoice: \u2003 $\\forall S,T{\u2208}set\\ \\forall R{\u2208}\\pow(S\u00d7T)\\ ( \\ \\forall x{\u2208}S\\ \\exists y{\u2208}T\\ ( \\ (x,y) \\in R \\ ) \\\\ \\imp \\exists f{\u2208}(S{\u2192}T)\\ \\forall x{\u2208}S\\ ( \\ (x,f(x)) \\in R \\ ) \\ ).$\n\nAdd the following rules:\ntype-notation\nTake (in the current context) any property $P$ and object expression $E$ and unused variable $x$.\nThen $\\{ x : P(x) \\}$ is a type and its membership is governed by:\n\u2003 $\\fitch{}{E \\in \\{ x : P(x) \\} \\eq P(E).}$. ($x$ must not appear in $E$ or $P$)\ncomprehension\nTake any property $P$ and unused variable $x$.\n\u2003 $\\fitch{S \\in set.}{\\{ x : x \\in S \\land P(x) \\} \\in set.}$. ($x$ must not appear in $S$ or $P$)\nreplacement\nTake any $2$-parameter property $P$ and unused variables $x,y$.\n\u2003 $\\fitch{S \\in set. \\\\ \\forall x{\u2208}S\\ \\exists! y{\u2208}obj\\ ( \\ P(x,y) \\ ).}{\\{ y : \\exists x{\u2208}S\\ ( \\ P(x,y) \\ ) \\} \\in set.}$ ($x,y$ must not appear in $S$ or $P$)\ninduction\nTake any property $P$ with $1$ parameter from $\\nn$.\n\u2003 $\\fitch{P(0). \\\\ \\forall k{\u2208}\\nn\\ ( \\ P(k) \\imp P(k+1) \\ ).}{\\forall k{\u2208}\\nn\\ ( \\ P(k) \\ ).}$ ($k$ must not appear in $P$)\nThe induction rule subsumes the induction axioms for PA, and essentially the only difference is that the property $P$ can involve set operations and quantification over sets.\nfunction-notation\nThis rule is theoretically unnecessary but pragmatically very convenient (also known as lambda expressions in computer science).\nTake any set $S$ and any object expression $E$ with $1$ parameter from $S$, and unused variable $x$.\nThen $( S\\ x \\mapsto E(x) )$ is an object and its behaviour is governed by:\n\u2003 $\\fitch{\\forall x{\u2208}S\\ ( \\ E(x) \\in T \\ ). \\\\ f = ( S\\ x \\mapsto E(x) ).}{f \\in (S\u2192T) \\land \\forall x{\u2208}S ( \\ f(x) = E(x) \\ ).}$\nFoundational system\nCombining the above Peano Arithmetic plus Set Theory yields a foundational system that is essentially as strong as ZFC but much more user-friendly. It is also agnostic to the existence of objects that are not sets, and does not even assume that the members of $\\nn$ are sets. It also treats cartesian products and ordered pairs as inbuilt abstract notions. This is how we use them in actual mathematics. (More precisely, the above system directly interprets ZFC minus regularity.)", "meta": {"post_id": 1681857, "input_score": 26, "output_score": 40, "post_title": "Predicate logic: How do you self-check the logical structure of your own arguments?"}}
{"input": "I've been pondering over this question since a very long time. \nIf a complex number can be prime then which parts of the complex number needs to be prime for the whole complex number to be prime.", "output": "The notion of being \"prime\" is only meaningful relative to a base ring.\nFor instance, in the integers $\\mathbb{Z}$ the number 5 is prime, whereas in the Gaussian integers $\\mathbb{Z}[i]$ we have\n$$5 = (2 + i)(2 - i) = 2^2 - i^2 = 4 - (-1) = 5$$\nand in the ring $\\mathbb{Z}[\\sqrt{5}]$ we have\n$$5 = (\\sqrt{5})^2$$\nso over these rings 5 is not a prime number.\nThe definition of prime you're probably familiar with -- a number is prime if it is divisible only by itself and one -- doesn't even really work over the integers: for instance, 5 is divisible not only by 1 and 5, but also by -1 and -5.  So we need to formulate the definition of a prime differently, while still preserving the basic idea, to make sense of it in an arbitary ring.\nNotice the following difference between primes and composites: since 5 is prime, if we have two numbers $a$ and $b$ such that $ab$ is a multiple of 5, then obviously one of $a$ or $b$ has to be a multiple of 5 just by unique factorization.  On the other hand, if $ab$ is a multiple of 15, it may be the case that neither $a$ nor $b$ is a multiple of 15, because we might instead have $a$ a multiple of 3 but not 5 and $b$ a multiple of 5 and not 3. \u2020\nThis gives us our definition of \"prime\" for a general ring: a ring element is prime if it is neither zero nor a unit, and moreover has the property that whenever it divides a product it must divide at least one of the factors.\nThere are a great many rings contained in the complex numbers, and in many of these rings there are non-real complex numbers that are primes in that ring.  However, given any number that's prime in a given ring, there's a larger ring in which it's not prime, just as we saw above that 5 is prime over the integers, but not over the Gaussian integers $\\mathbb{Z}[i]$ or over $\\mathbb{Z}[\\sqrt{5}]$.\nIn particular, since $\\mathbb{C}$ is a field, every nonzero element is a unit, so nothing is prime over the complex numbers.  (Similarly, nothing is prime over the real numbers, or the rational numbers.)  However, I reiterate that many complex numbers are prime over smaller rings: for instance, it turns out that $2 + i$ is prime over $\\mathbb{Z}[i]$.\n\n\u2020 A slightly more straightforward generalization of the definition you're used to would be to look at nonzero, nonunit elements $r$ for which we only have $r = s t$ when either $s$ or $t$ is a unit.  This actually gives a weaker notion called \"irreducibility.\"  In Unique Factorization Domains the two notions are the same (which explains why they're the same for the integers), but in rings like $\\mathbb{Z}[\\sqrt{-5}]$ where we do not have unique factorization you can have situations like\n$$3 \\cdot 3 = 9 = (2 + \\sqrt{-5})(2 - \\sqrt{-5})$$\nHere each of $3$, $2 + \\sqrt{-5}$, and $2 - \\sqrt{-5}$ is irreducible, with none dividing any of the others.  We've decided that being \"prime\" is about unique prime factorization, so we chose a definition of \"prime\" under which each of the above is \"irreducible\" but none is \"prime.\"\n\nNote: several answers (including this one) have brought up the Gaussian integers specifically.  They're indeed an example of a subring of the complex numbers containing non-real complex numbers, but just to be clear they're in no way \"the\" natural example here -- they're on the same footing as all the others.", "meta": {"post_id": 1702779, "input_score": 41, "output_score": 66, "post_title": "Can a complex number be prime?"}}
{"input": "Let $P(x)=a_{2n}x^{2n}+a_{2n-1}x^{2n-1}+\\ldots+a_{0}$ be an even degree polynomial with positive coefficients. \nIs it possible to permute the coefficients of $P(x)$ so that the resulting polynomial will have NO real roots.", "output": "Yes: put the $n+1$ largest coefficients on the even powers of $x$, and the $n$ smallest coefficients on the odd powers of $x$.\nClearly the polynomial will have no nonnegative roots regardless of the permutation. Changing $x$ to $-x$, it suffices to show: if $\\min\\{a_{2k}\\} \\ge \\max\\{a_{2k+1}\\}$, then when $x>0$,$$a_{2n}x^{2n} - a_{2n-1}x^{2n-1} + \\cdots + a_2x^2 -a_1x+a_0$$is always positive.\n\nIf $x\\ge1$, this follows from\n$$\n(a_{2n}x^{2n} - a_{2n-1}x^{2n-1}) + \\cdots + (a_2x^2 -a_1x) +a_0 \\ge 0 + \\cdots + 0 + a_0 > 0.\n$$\nIf $0<x\\le1$, this follows from\n\\begin{multline*}\n(a_0 - a_1x) + (a_2x^2-a_3x^3) + \\cdots + (a_{2n-2}x^{2n-2}-a_{2n-1}x^{2n-1}) + a_{2n}x^{2n} \\\\\n\\ge 0 + \\cdots + 0 + a_{2n}x^{2n} > 0.\n\\end{multline*}", "meta": {"post_id": 1716027, "input_score": 21, "output_score": 36, "post_title": "Can we permute the coefficients of a polynomial so that it has NO real roots?"}}
{"input": "I am familiar with the mechanism of proof by contradiction: we want to prove $P$, so we assume $\u00acP$ and prove that this is false; hence $P$ must be true.\nI have the following devil's advocate question, which might seem to be more philosophy than mathematics, but I would prefer answers from a mathematician's point of view:\nWhen we prove that $\u00acP$ is \"false\", what we are really showing is that it is inconsistent with our underlying set of axioms. Could there ever be a case were, for some $P$ and some set of axioms, $P$ and $\u00acP$ are both inconsistent with those axioms (or both consistent, for that matter)?", "output": "The situation you ask about, where $P$ is inconsistent with our axioms and $\\neg P$ is also inconsistent with our axioms, would mean that the axioms themselves are inconsistent.  Specifically, the inconsistency of $P$ with the axioms would mean that $\\neg P$ is provable from those axioms.  If, in addition, $\\neg P$ is inconsistent with the axioms, then the axioms themselves are inconsistent --- they imply $\\neg P$ and then they contradict that.  (I have phrased this answer so that it remains correct even if the underlying logic of the axiom system is intuitionistic rather than classical.)", "meta": {"post_id": 1719503, "input_score": 40, "output_score": 52, "post_title": "Can proof by contradiction 'fail'?"}}
{"input": "After reading this question, I conjectured a generalization of it.\n\nConjecture:        Fix  $k\\in \\mathbb N$. Then, for all $n\\in \\mathbb N$, one of $n+1,\\ldots,n+k$ is coprime to the rest.   \n\nI tried some elementary ways, but wasn't successful.  \nObservation: One of the consequences of this conjecture is that there are infinitely many primes!", "output": "Surprisingly, the statement is false once $k\\ge17$, and the shortest counterexample is the sequence of length $17$ beginning with $2184$. This was the result of a line of work beginning with Pillai, and finally wrapped up by Brauer. See S.S. Pillai on Consecutive integers research paper?.\n\nPillai showed that it holds for $k<17$, but can fail for all $k$ between $17$ and $430$ - infinitely often, in fact!\nIn a sequence of results, this was improved until eventually Scott showed that there are infinitely many counterexamples for $17\\le k\\le 2491906561$ . . .\n. . . and then Brauer showed that there are infinitely many counterexamples for any $k\\ge 17$.", "meta": {"post_id": 1720497, "input_score": 24, "output_score": 37, "post_title": "One of any consecutive integers is coprime to the rest"}}
{"input": "Is left invariant vector field on  $\\mathbb{G}$,is same as a vector field which is invariant under group transformation.", "output": "I guess you need a plain english explanation.\nA vector field $X$ is a function that associate smoothly to every point $p$ of $G$ an element or vector $X_p$ of the tangent space of the group $G$ (which in this case is also a manifold). So for every point $p$ you have the vector $X_p \\in T_p(G)$.\nNow suppose you have a function $F$ that maps every point of $G$ to another point of $G$. Let's suppose that the point $p$ goes to $q$ i.e. $F(p)=q$.\nThen if you consider the vector field $X$ you already had, you can play two different games with this vector field:\n\nFirst of all you can calculate $X_{F(p)}$. This is the original vector field $X$ but evaluated in the new point $F(p)$ which is $q$, i.e. $X_{F(p)}=X_{q}$. In particular if you apply the vector to a function $f\\in C^{\\infty}(G)$, you have $X_{F(p)}(f)$;\n\nThe other game you can do is to calculate $(F_*X_{p})(f)$ which by definition is $X_p(f\\circ F)$. Here, $F_*X_{p}$ is given by taking the vector $X_p$ calculated from the old vector field and considering its image by a the linear application $F_*$ which is the differential of $F$.\n\n\nThe two games are in general different because in the first case you leave the vector field untouched and you change the point to which it is applied, while in the second case you use the old point you had and you evaluate the vector field in the point but then you move the resulting vector by applying the function.\nNow if the two games coincide then you have that the vector field is invariant i.e.\n$$X_{F(p)}(f)=X_p(f\\circ F)$$\nYou can think about this with a hypersimple physical example: Consider a river and a thermometer. The river is flowing so a particle of water is moved along the flow. If you're interested in the change of temperature along the flow of the river, at any given time you can either measure the temperature few istants after the time given measuring the temperature in the new place where the particles will be or you can measure now the temperature of the particles and assign this temperature to the place where you guess they will be given the velocity they have.\nNow your case: if the function $F$ is the function given by the left multiplication of the group, then you have a \"Left Invariant Vector Field\".\nActually it's easier to work out some example to get effectively the idea.", "meta": {"post_id": 1725730, "input_score": 13, "output_score": 48, "post_title": "What is a left-invariant Vector field?"}}
{"input": "So I've been looking for a general name of this type of mathematics notation (google hasn't been very useful) so that I can learn more about it. Basically, the symbols are in the form of functions and the numbers are the inputs to the function.  + is the function taking 4 and 5, outputting 9 so, (+4 5) = 9. Formulaic  (* a b ) = a * b.  Where ' * ' can be any basic math symbol.  More complicated forms are: (+ 4 (+ 5 1)) = 4 + (5+1) = 10. 4 being input for 'a' and (5+1) being 'b' in (* a b ).\nHope it makes sense.", "output": "It is normal Polish notation or prefix notation.", "meta": {"post_id": 1727854, "input_score": 14, "output_score": 42, "post_title": "What is this type of math notation called? (+ 4 5)"}}
{"input": "I am really struggling to understand what modular forms are and how I should think of them. Unfortunately I often see others being in the same shoes as me when it comes to modular forms, I imagine because the amount of background knowledge needed to fully appreciate and grasp the constructions and methods is rather large, so hopefully with this post some clarity can be offered, also for future readers.. The usual definitions one comes across are often of the form: (here taken from wikipedia)\n\nA modular form is a (complex) analytic function on the upper\n  half-plane satisfying a certain kind of functional equation with\n  respect to the group action of the modular group, and also satisfying\n  a growth condition.\nA modular form of weight $k$ for the modular group $$\n \\text{SL}(2,\\mathbb{Z})=\\left\\{\\begin{pmatrix} a & b \\\\ c & d\n \\end{pmatrix}| a,b,c,d \\in \\mathbb{Z} , ad-bc = 1 \\right\\} $$ is a\n  complex-valued function \u2009$f$\u2009 on the upper half-plane $\\mathbf{H}=\\{z\n \\in \\mathbb{C},\\text{Im}(z)>0 \\}$, satisfying the following three\n  conditions:\n\n$f$ is a holomorphic function on $\\mathbf{H}.$\nFor any $z \\in \\mathbf{H}$ and any matrix in $\\text{SL}(2,\\mathbb{Z})$ as above, we have: $$\n f\\left(\\frac{az+b}{cz+d}\\right)=(cz+d)^k f(z) $$\n$f$ is required to be holomorphic as $z\\to i\\infty.$\n\n\nQuestions:\n\n(a): I guess what I'm having least familiarity with is the modular group part. My interpretation of $\\text{SL}(2,\\mathbb{Z}):$ The set of all $2$ by $2$ matrices, with integer components, having their determinant equal to $1.$ But where does the name come from, as in why do we call this set a group and what modular entails?\n(b): If I understand correctly, the group operation here is function composition, of type: $\\begin{pmatrix}a & b \\\\ c & d\\end{pmatrix}z = \\frac{az+b}{cz+d}$ which is also called a linear fractional transformation. How should one interpret the condition $2.$ that $f$ has to satisfy? My observation is that, as a result of the group operation of $\\text{SL}$ on a given integer $z,$ the corresponding image is multiplied by a polynomial of order $k$ (which is the weight of the modular form). \n(c) The condition $3.$ I interpret as: $f$ should not exhibit any poles in the upper half plane, not even at infinity. About right? \n(d) A more general question: Given the definition above, it is tempting to see modular forms as particular classes of functions, much like the Schwartz class of functions, or $L^p$ functions and so on. Is this an acceptable assessment of modular forms?\n(e) Last question: It is often said that modular forms have interesting Fourier transforms, as in their Fourier coefficients are often interesting (or known) sequences. Is there an intuitive way of seeing, from the definition of modular forms, the above expectation of their Fourier transforms?", "output": "The definition of a modular form seems extremely unmotivated, and as @AndreaMori has pointed out, whilst the complex analytic approach gives us the quickest route to a definition, it also clouds some of what is really going on.\nA good place to start is with the theory of elliptic curves, which have long been objects of geometric and arithmetic interest. One definition of an elliptic curve (over $\\mathbb C$) is a quotient of $\\mathbb C$ by a lattice $\\Lambda = \\mathbb Z\\tau_1\\oplus\\mathbb Z\\tau_2$, where $\\tau_1,\\tau_2\\in\\mathbb C$ are linearly independent over $\\mathbb R$ ($\\mathbb C$ and $\\Lambda$ are viewed as additive groups): i.e. \n$$E\\cong \\mathbb C/\\Lambda.$$\nIn this viewpoint, one can study elliptic curves by studying lattices $\\Lambda\\subset\\mathbb C$. Modular forms will correspond to certain functions of lattices, and by extension, to certain functions of elliptic curves.\n\nWhy the upper half plane?\n\nFor simplicity, since $\\mathbb Z\\tau_1 = \\mathbb Z(-\\tau_1)$, there's no harm in assuming that $\\frac{\\tau_1}{\\tau_2}\\in \\mathbb H$. \n\nWhat about $\\mathrm{SL}_2(\\mathbb Z)$?\n\nWhen do $(\\tau_1,\\tau_2)$ and $(\\tau_1',\\tau_2')$ define the same lattice? Exactly when \n$$(\\tau_1',\\tau_2')=(a\\tau_1+b\\tau_2,c\\tau_1+d\\tau_2)$$where $\\begin{pmatrix}a&b\\\\c&d\\end{pmatrix}\\in\\mathrm{SL}_2(\\mathbb Z)$. Hence, if we want to consider functions on lattices, they had better be invariant under $\\mathrm{SL}_2(\\mathbb Z)$.\n\nFunctions on lattices:\n\nSuppose we have a function  $$F:\\{\\text{Lattices}\\}\\to\\mathbb C.$$ First observe that multiplying a lattice by a non-zero scalar (i.e. $\\lambda\\Lambda$ for $\\lambda\\in\\mathbb C^\\times$) amounts to rotating and rescaling the lattice. So our function shouldn't do anything crazy to rescaled lattices.\nIn fact, since we really care about elliptic curves, and $\\mathbb C/\\Lambda\\cong\\mathbb C/\\lambda\\Lambda$ under the isomorphism $z\\mapsto \\lambda z$, $F$ should be completely invariant under such rescalings - i.e. we should insist that \n$$F(\\lambda \\Lambda) = F(\\Lambda).$$\nHowever, if we define $F$ like this, we are forced to insist that $F$ has no poles. This is needlessly restrictive. So what we do instead is require that\n$$F(\\lambda\\Lambda) = \\lambda^{-k}F(\\Lambda)$$\nfor some integer $k$; the quotient $F/G$ of two weight $k$ functions gives a fully invariant function, this time with poles allowed.\n\nWhere do modular forms come in?\n\nIf $\\Lambda = \\mathbb Z\\tau\\oplus\\mathbb Z$ with $\\tau\\in\\mathbb H$, define a function $f:\\mathbb H\\to\\mathbb C$ by $f(\\tau)=F(\\Lambda)$. For a general lattice, we have\n$$\\begin{align}F(\\mathbb Z\\tau_1\\oplus\\mathbb Z\\tau_2)&=F\\left(\\tau_2(\\mathbb Z({\\tau_1}/{\\tau_2})\\oplus\\mathbb Z)\\right)\\\\\n&=\\tau_2^{-k}f({\\tau_1}/{\\tau_2})\n\\end{align}$$\nand in particular,\n$$\\begin{align}f(\\tau) &= F(\\mathbb Z\\tau\\oplus\\mathbb Z) \\\\&=F(\\mathbb Z(a\\tau+b)\\oplus\\mathbb Z(c\\tau+d)) &\\text{by }\\mathrm{SL}_2(\\mathbb Z)\\text{ invariance}\\\\&= (c\\tau+d)^{-k} f\\left(\\frac{a\\tau+b}{c\\tau+d}\\right).\\end{align}$$\nThis answers your first two questions. \nAt this point, there's no reason to assume that condition (3) holds, and one can study such functions without assuming condition (3). However, imposing cusp conditions is a useful thing to do, as it ensures that the space of weight $k$ modular forms is finite dimensional.\nTo answer your fourth question, yes, and this is exactly the viewpoint taken in most research done on modular forms and their generalisations, where one considers automorphic representations.", "meta": {"post_id": 1730352, "input_score": 47, "output_score": 39, "post_title": "Demystifying modular forms"}}
{"input": "Recently, I was looking into fractional approximations of pi, such as $\\frac{22}{7}$ or $\\frac{355}{113}$. I found that there was a name for these approximations, 'convergents' of pi, and I found a list of the first 100. Upon inspection of list, it seemed that a lot of the fractions had odd numerators or denominators. Furthermore, it seemed a lot of the fractions had prime numerators or denominators! \nA quick check with mathematica confirmed this:\n91 out of the first 100 convergents had prime numerators or denominators! Even more astounding, 6 out of the 9 that didn't hold this property occured within the first 11 convergents.\nThis seems like it can't simply be a coincidence, but I couldn't find anything about it online. Why is this true?", "output": "Your data seems to be exactly backwards.  Of the first 100 convergents of pi, only 9 of them have either a prime numerator or a prime denominator, with 91 of them having both composite numerator and composite denominator.  Moreover, 6 of the 9 that have a prime numerator or denominator are in the first 11 convergents.\nHere's the Mathematica code that I used:\nIn[1]:=   c = Convergents[Pi, 100];\n\nIn[2]:=   test = PrimeQ[Numerator[#]] || PrimeQ[Denominator[#]] &;\n\nIn[3]:=   Length[Select[c, test]]\nOut[3]:=  9\n\nAccording to Mathematica, these 9 are\n$$\n3,\\;\\; \\frac{22}{7},\\;\\; \\frac{355}{113},\\;\\; \\frac{103993}{33102},\\;\\; \\frac{833719}{265381},\\;\\; \\frac{4272943}{1360120},\\;\\; \n\\frac{411557987}{131002976},\\;\\; \\frac{2646693125139304345}{842468587426513207},\n$$\n$$\n\\frac{7809723338470423412693394150101387872685594299}{2485912146995414187767820081837036927319426665}.\n$$", "meta": {"post_id": 1735010, "input_score": 9, "output_score": 34, "post_title": "Why are there so many primes in the convergents of Pi?"}}
{"input": "If $ \\partial u/\\partial v=a $, then $ \\partial v/\\partial u=1/a$?", "output": "Functions of a Single Variable\nFor functions of one variable, if $y=f(x)$ is strictly monotone and differentiable on an interval, and $f'(x)\\ne 0$ in that interval, then the inverse function $x=f^{-1}(y)$ is also strictly monotone and differentiable in the corresponding interval and \n$$\\bbox[5px,border:2px solid #C0A000]{\\frac{dx}{dy}=\\frac{1}{\\frac{dy}{dx}}}\\tag 1$$\n\nEXAMPLE:\nSuppose $y=\\sin(x)$ for $x\\in (-\\pi/2,\\pi,2)$.  Note that the sine function is monotone and differentiable on $(-\\pi/2,\\pi/2)$ with $\\frac{dy}{dx}=\\cos(x)$ and $\\cos(x)\\ne 0$.  \nThe inverse function, call it $x=\\arcsin(y)$ for $y\\in (-1,1)$, is  therefore monotone and its derivative is \n$$\\frac{dx}{dy}=\\frac{1}{\\cos(x)}=\\frac{1}{\\sqrt{1-y^2}}$$\nTherefore, we have $\\frac{d\\,\\arcsin(y)}{dy}=\\frac{1}{\\sqrt{1-y^2}}$.\n\nFunctions of a Two Variables\nThe relationship in $(1)$ does not apply, in general, to functions of more than one variable.  As an example, examine the transformation of Cartesian coordinates $(x,y)$ to polar coordinates $(\\rho,\\phi)$ as given by \n$$\\begin{align}\n\\rho &=\\sqrt{x^2+y^2}\\\\\\\\\n\\phi &=\\operatorname{arctan2}(y,x)\n\\end{align}$$\nand\n$$\\begin{align}\nx&=\\rho \\cos(\\phi)\\\\\\\\\ny&=\\rho \\sin(\\phi)\n\\end{align}$$\nWe examine the relationship between $\\frac{\\partial \\rho }{\\partial x}$ and $\\frac{\\partial x}{\\partial \\rho}$ to see if $(1)$ holds. Note that \n$$\\begin{align}\n\\frac{\\partial \\rho }{\\partial x}&=\\frac{x}{\\rho}\\\\\\\\\n& =\\cos(\\phi)\\\\\\\\\n&=\\frac{\\partial x}{\\partial \\rho}\n\\end{align}$$\nTherefore, $\\frac{\\partial \\rho }{\\partial x}\\ne \\frac{1}{\\frac{\\partial x}{\\partial \\rho}}$ and $(1)$ does not hold (unless $y=0$).\nInstead of the relationship $(1)$ holding, we have instead\n$$\\begin{equation}\n\\begin{pmatrix}\n\\frac{\\partial x}{\\partial \\rho} & \\frac{\\partial x}{\\partial \\phi} \\\\\n\\frac{\\partial y}{\\partial \\rho} & \\frac{\\partial y}{\\partial \\phi}\n\\end{pmatrix}  \\begin{pmatrix}\n\\frac{\\partial \\rho}{\\partial x} & \\frac{\\partial \\rho}{\\partial y} \\\\\n\\frac{\\partial \\phi}{\\partial x} & \\frac{\\partial \\phi}{\\partial y}\n\\end{pmatrix}=\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}\n\\end{equation}$$\nwhereupon matrix inversion becomes\n$$\\begin{equation}\n\\begin{pmatrix}\n\\frac{\\partial x}{\\partial \\rho} & \\frac{\\partial x}{\\partial \\phi} \\\\\n\\frac{\\partial y}{\\partial \\rho} & \\frac{\\partial y}{\\partial \\phi}\n\\end{pmatrix}  =\\begin{pmatrix}\n\\frac{\\partial \\rho}{\\partial x} & \\frac{\\partial \\rho}{\\partial y} \\\\\n\\frac{\\partial \\phi}{\\partial x} & \\frac{\\partial \\phi}{\\partial y} \n\\end{pmatrix}^{-1} \\tag 2\\end{equation}$$\nNote that $(2)$ is the analog of $(1)$ and applies whenever a transformation and its inverse exists and is prescribed by differentiable functions.  Moreover, it can be generalized to any number of variables.", "meta": {"post_id": 1744014, "input_score": 22, "output_score": 34, "post_title": "Partial derivatives inverse question"}}
{"input": "I was going through some problems then I arrived at this question which I couldn't solve. Does anyone know the answer to this question?\n\nOne day, a person went to a horse racing area. Instead of counting the number of humans and horses, he counted $74$ heads and $196$ legs. How many humans and horses were there?", "output": "Just in case you don't like algebra and don't think in terms of centaurs, here's yet another approach (very close to the centaur version but omitting the mythology).  Suppose for a moment that all 74 heads belong to humans.  Then there would be $2\\times 74=148$ legs. That's 48 legs short of the specified number 196.  To get that many extra legs, we have to replace some of the humans with horses. Every time we replace a human with a horse, we gain two legs, so we should do $\\frac{48}2=24$ such replacements.  We started with 74 humans, and we need to replace 24 of them with horses, so at the end we have 24 horses and $74-24=50$ humans.", "meta": {"post_id": 1749853, "input_score": 41, "output_score": 46, "post_title": "If there are $74$ heads and $196$ legs, how many horses and humans are there?"}}
{"input": "I am interested in the properties of Fourier series under integration and differentiation, and I've noticed a \"strange\" phenomenon.\nSuppose I have a Fourier series which I Integrate, and suppose that I can integrate it term by term.\n$$f(x)=a_0+\\sum_{n=1}^\\infty a_n \\cos(nx)+\\sum_{n=1}^\\infty b_n \\sin(nx)$$\n$$\\int f(x)dx=C+a_0 x+\\sum_{n=1}^\\infty \\frac{a_n}{n} \\sin(nx)-\\sum_{n=1}^\\infty \\frac{b_n}{n} \\cos(nx) $$\nNow because I want the integral to be represented as a Fourier series as well, I use the Fourier series for $x$:\n$$x=\\sum_{n=1}^\\infty \\frac{2}{\\pi}\\frac{(-1)^{n+1}}{n}\\sin(nx)$$\nSo I have:\n$$\\int f(x)dx=C+\\sum_{n=1}^\\infty \\left(\\frac{2a_0}{\\pi}\\frac{(-1)^{n+1}}{n}+\\frac{a_n}{n}\\right)\\sin(nx)+\\sum_{n=1}^\\infty \\frac{b_n}{n}\\cos(nx)$$\nMy problem is that upon differentiation (term by term), this series does not appear to give back the original Fourier series, as the constant term has been shifted into the cosine terms.\nI think my mistake is the step I take converting the $x$ to a Fourier series, because it can't then be differentiated term by term (see this question).\nAm I correct? And does this mean the integral of a Fourier series cannot be another Fourier series?", "output": "Fourier series (as with infinite series in general) cannot always be term-by-term differentiated. For general series we have the following theorem\n\nTheorem: Term-by-term differentiation: If a series $\\sum f_k(x_0)$ converges at some point $x_0\\in [a,b]$ and the series of derivatives $g(x) = \\sum f_k'(x)$ converges uniformly on $[a,b]$ then the series $f(x) = \\sum f_k(x)$ converges uniformly for all $x\\in[a,b]$ and $f(x)$ is differentiable with $f'(x) = g(x)$.\n\nYour example fails this theorem dramatically as the derivative of the Fourier series of $x$ do not converge anywhere.\nWe also have some more specific results. The following theorem gives conditions for which this can be done for Fourier series:\n\nTheorem (Term-by-term differentiation of Fourier series): If $f$ is a piecewise smooth function and if $f$ is also continuous on $[-L,L]$, then the Fourier series of $f$ can be term-by-term differentiated if $f(-L) = f(L)$.\n\nThe last condition is not satisfied when the Fourier series has a jump-discontinuity as $x=L$ so we in general we don't expect to be able to term-by-term differentiate a Fourier series that has a jump-discontinuity (thought the theorem does not rule it out). For your example of the Fourier series of $f(x) = x$ the first condition are satisfyed as $f$ is both smooth and continuous on $[-1,1]$ however $f(-1) \\not= f(1)$ so the theorem does not apply.\nFor integration of Fourier series we have the following theorem\n\nTheorem (Term-by-term integration of Fourier series): The Fourier series of a piecewise smooth function $f$ can always be term-by-term integrated to give a convergent series that always\n  converges to the integral of $f$ for $x\\in[-L,L]$.\n\nNote that the resulting series does not have to be a Fourier series. For example if we have a Fourier series $f(x) = a_0 + \\ldots$ with $a_0\\not= 0$ then $\\int f(x){\\rm d}x = a_0x+ \\ldots$ and the presence of the $a_0x$ term makes this not be a Fourier series (though for this example one can probably expand $x$ in a Fourier series to get such a series).\nThe proofs of the theorems above can be found in any good textbook on Fourier series. You can also find them in the following course notes (by P. Laval).", "meta": {"post_id": 1754033, "input_score": 12, "output_score": 39, "post_title": "Integration and differentiation of Fourier series"}}
{"input": "Until recently, all my knowledge of measure theory and Lebesgue integration are from Rudin's book, which focuses solely on the Lebesgue measure, its construction and nothing else. I have just put my hands on a nice book \"Measure and Integration Theory\" by Heinz Bauer and I'm currently enjoying it. I have encountered the definition of a Dynkin system $\\mathcal D$, which is a family of subsets of a set $\\Omega$ satisfying\n\n1.) $\\Omega\\in\\mathcal D$.\n2.) If $A\\in\\mathcal D$, then $A^c\\in\\mathcal D$.\n3.) For $n\\in\\Bbb N$, if $A_n\\in\\mathcal D$ are pairwise disjoint then $\\bigcup_{n=1}^{\\infty}A_n\\in\\mathcal D$.\n\nI have some idea about what a $\\sigma$-algebra is, but not about a Dynkin system. I would really appreciate if someone could give me an intuition about Dynkin systems or what they're supposed to represent. What is the characteristics of a Dynkin system that let you recognize it once you see it?\nI know the $\\pi$-$\\lambda$ theorem and facts like a Dynkin system $\\mathcal D$ is a $\\sigma$-algebra if it is closed under intersection, it would be also nice if anyone could explain to me why should we expect such a result. Thank you in advance.", "output": "A motivation for Dynkin systems (and specially, the $\\pi$-$\\lambda$ Theorem) is the following problem: \n\nHow many sets should I check to be sure that two probability measures $\\mu$ and $\\nu$ are the same?\n\nFor the sake of definiteness, both $\\mu$ and $\\nu$ are defined on a measurable space $(\\Omega,\\Sigma)$. A trivial answer to the previous question is \u201cIf they coincide over $\\Sigma$, they are the same\u201d. True, and useless. So let's think about this for a moment. If I have checked that $\\mu(A) =\\nu(A)$ for some $A\\in\\Sigma$, it is not necessary to check for the complement, since\n$$\n\\mu(A^c) =1-\\mu(A) = 1-\\nu(A) = \\nu(A^c).\n$$\nIt is also immediate by $\\sigma$-additivity that if they coincide on a sequence of pairwise disjoint sets $A_n$, they must coincide on their union. Hence we conclude\n\nIf two probability measures coincide on a family $\\mathcal{A}\\subseteq\\Sigma$, then they coincide on the Dynkin system generated by $\\mathcal{A}$ (i.e., the smallest $\\lambda$-system containing it).\n\nThe above arguments can't be generalized to intersections; we can't compute $\\mu(A\\cap B)$ from $\\mu(A)$ and $\\mu(B)$. So, it would be desirable that our initial data (sets checked for coincidence) is a family of sets closed under binary intersections. Here, the $\\pi$-$\\lambda$ confirms this intuition: If $\\Sigma$ is generated by a family $\\mathcal{A}$, then it equals the smallest Dynkin system including the closure of $\\mathcal{A}$ under intersections (i.e., the $\\pi$-system generated by it). \nTherefore, to check that $\\mu$ and $\\nu$ are equal, it is enough to take a family $\\mathcal{A}$ such that of $\\Sigma =\\sigma(\\mathcal{A})$ and check that the measures coincide on every finite intersection of members of $\\mathcal{A}$.", "meta": {"post_id": 1769131, "input_score": 18, "output_score": 37, "post_title": "What is a Dynkin system? ($\\lambda$-system)"}}
{"input": "It is said that a matrix's null space can be derived from QR or SVD. I tried an example:\n$$A= \\begin{bmatrix} \n1&3\\\\\n1&2\\\\\n1&-1\\\\\n2&1\\\\\n\\end{bmatrix} \n$$ \nI'm convinced that QR (more precisely, the last two columns of Q) gives the null space:\n$$Q= \\begin{bmatrix} \n-0.37796&   -0.68252&   -0.17643&   -0.60015\\\\   \n-0.37796&   -0.36401&   0.73034&   0.43731\\\\   \n-0.37796&   0.59152&   0.43629&   -0.56293\\\\   \n-0.75593&   0.22751&   -0.4951&   0.36288\\\\   \n\\end{bmatrix} \n$$\nHowever, neither $U$ nor $V$ produced by SVD ($A=U\\Sigma V^*$) make $A$ zero (I tested with 3 libraries: JAMA, EJML, and Commons):\n$$ U= \\begin{bmatrix} \n0.73039&   0.27429\\\\   \n0.52378&   0.03187\\\\   \n-0.09603&   -0.69536\\\\   \n0.42775&   -0.66349\\\\\n\\end{bmatrix} \n$$\n$$ \\Sigma= \\begin{bmatrix} \n4.26745&   0\\\\   \n0&   1.94651\\\\   \n\\end{bmatrix} \n$$\n$$ V= \\begin{bmatrix} \n0.47186&   -0.88167\\\\   \n0.88167&   0.47186\\\\  \n\\end{bmatrix} \n$$\nThis is contradiction to\n\nUsing the SVD, if $A=U\\Sigma V^*$, then columns of $V^*$ corresponding to small singular values (i.e., small diagonal entries of $\\Sigma$\n  ) make up the a basis for the null space.", "output": "Summary\nComputing the full form of the singular value decomposition (SVD) will generate a set of orthonormal basis vectors for the null spaces $\\color{red}{\\mathcal{N} \\left( \\mathbf{A} \\right)}$ and $\\color{red}{\\mathcal{N} \\left( \\mathbf{A}^{*} \\right)}$.\nFundamental Theorem of Linear Algebra\nA matrix $\\mathbf{A} \\in \\mathbb{C}^{m\\times n}_{\\rho}$ induces four fundamental subspaces. These are range and null spaces for both the column and the row spaces.\n$$\n\\begin{align}\n%\n  \\mathbf{C}^{n} = \n    \\color{blue}{\\mathcal{R} \\left( \\mathbf{A}^{*} \\right)} \\oplus\n    \\color{red}{\\mathcal{N} \\left( \\mathbf{A} \\right)} \\\\\n%\n  \\mathbf{C}^{m} = \n    \\color{blue}{\\mathcal{R} \\left( \\mathbf{A} \\right)} \\oplus\n    \\color{red} {\\mathcal{N} \\left( \\mathbf{A}^{*} \\right)}\n%\n\\end{align}\n$$\nThe singular value decomposition provides an orthonormal basis for the four fundamental subspaces.\nSingular Value Decomposition\nEvery nonzero matrix can be expressed as the matrix product\n$$\n\\begin{align}\n  \\mathbf{A} &=\n  \\mathbf{U} \\, \\Sigma \\, \\mathbf{V}^{*} \\\\\n%\n &=\n% U \n  \\left[ \\begin{array}{cc}\n     \\color{blue}{\\mathbf{U}_{\\mathcal{R}}} & \\color{red}{\\mathbf{U}_{\\mathcal{N}}}\n  \\end{array} \\right]  \n% Sigma\n  \\left[ \\begin{array}{cccc|cc}\n     \\sigma_{1} & 0 & \\dots &  &   & \\dots &  0 \\\\\n     0 & \\sigma_{2}  \\\\\n     \\vdots && \\ddots \\\\\n       & & & \\sigma_{\\rho} \\\\\\hline\n       & & & & 0 & \\\\\n     \\vdots &&&&&\\ddots \\\\\n     0 & & &   &   &  & 0 \\\\\n  \\end{array} \\right]\n% V \n  \\left[ \\begin{array}{c}\n     \\color{blue}{\\mathbf{V}_{\\mathcal{R}}}^{*} \\\\ \n     \\color{red}{\\mathbf{V}_{\\mathcal{N}}}^{*}\n  \\end{array} \\right]  \\\\\n%\n  & =\n% U\n   \\left[ \\begin{array}{cccccccc}\n    \\color{blue}{u_{1}} & \\dots & \\color{blue}{u_{\\rho}} & \\color{red}{u_{\\rho+1}} & \\dots & \\color{red}{u_{m}}\n  \\end{array} \\right]\n% Sigma\n  \\left[ \\begin{array}{cc}\n     \\mathbf{S}_{\\rho\\times \\rho} & \\mathbf{0} \\\\\n     \\mathbf{0} & \\mathbf{0} \n  \\end{array} \\right]\n% V\n   \\left[ \\begin{array}{c}\n    \\color{blue}{v_{1}^{*}} \\\\ \n    \\vdots \\\\\n    \\color{blue}{v_{\\rho}^{*}} \\\\\n    \\color{red}{v_{\\rho+1}^{*}} \\\\\n    \\vdots \\\\ \n    \\color{red}{v_{n}^{*}}\n  \\end{array} \\right]\n%\n\\end{align}\n$$\nThe column vectors of $\\mathbf{U}$ are an orthonormal span of $\\mathbb{C}^{m}$ (column space), while  the column vectors of $\\mathbf{V}$ are an orthonormal span of $\\mathbb{C}^{n}$ (row space).\nThe $\\rho$ singular values are real and ordered (descending):\n$$\n  \\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{\\rho}>0.\n$$\nThese singular values for the diagonal matrix of singular values\n$$\n\\mathbf{S} = \\text{diagonal} (\\sigma_{1},\\sigma_{1},\\dots,\\sigma_{\\rho}) \\in\\mathbb{R}^{\\rho\\times\\rho}.\n$$\nThe $\\mathbf{S}$ matrix is embedded in the sabot matrix $\\Sigma\\in\\mathbb{R}^{m\\times n}$ whose shape insures conformability.\nPlease note that the singular values only correspond to $\\color{blue}{range}$ space vectors.\nThe column vectors form spans for the subspaces:\n$$\n\\begin{align} \n% R A\n\\color{blue}{\\mathcal{R} \\left( \\mathbf{A} \\right)} &=\n\\text{span} \\left\\{\n \\color{blue}{u_{1}}, \\dots , \\color{blue}{u_{\\rho}}\n\\right\\} \\\\\n% R A*\n\\color{blue}{\\mathcal{R} \\left( \\mathbf{A}^{*} \\right)} &=\n\\text{span} \\left\\{\n \\color{blue}{v_{1}}, \\dots , \\color{blue}{v_{\\rho}}\n\\right\\} \\\\\n% N A*\n\\color{red}{\\mathcal{N} \\left( \\mathbf{A}^{*} \\right)} &=\n\\text{span} \\left\\{\n\\color{red}{u_{\\rho+1}}, \\dots , \\color{red}{u_{m}}\n\\right\\} \\\\\n% N A\n\\color{red}{\\mathcal{N} \\left( \\mathbf{A} \\right)} &=\n\\text{span} \\left\\{\n\\color{red}{v_{\\rho+1}}, \\dots , \\color{red}{v_{n}}\n\\right\\} \\\\\n%\n\\end{align}\n$$\nThe conclusion is that the full SVD provides an orthonormal span for not only the two null spaces, but also both range spaces.\nExample\nSince there is some misunderstanding in the original question, let's show the rough outlines of constructing the SVD.\nFrom your data, we have $2$ singular values. Therefore the rank $\\rho = 2$. From this, we know the form of the SVD:\n$$\n\\mathbf{A} =\n% U \n  \\left[ \\begin{array}{cc}\n     \\color{blue}{\\mathbf{U}_{\\mathcal{R}}} & \\color{red}{\\mathbf{U}_{\\mathcal{N}}}\n  \\end{array} \\right]  \n% Sigma\n  \\left[ \\begin{array}{c}\n     \\mathbf{S}  \\\\ \\mathbf{0} \\\\\n  \\end{array} \\right]\n% V \n  \\left[ \\begin{array}{c}\n     \\color{blue}{\\mathbf{V}_{\\mathcal{R}}}^{*} \n  \\end{array} \\right]  \n$$\nThat is, the null space $\\color{red}{\\mathcal{N} \\left( \\mathbf{A} \\right)}$ is trivial.\nConstruct the matrix $\\Sigma$:\nForm the product matrix, and compute the eigenvalue spectrum\n$$\n \\lambda \\left( \\mathbf{A}^{*} \\mathbf{A} \\right) =\n \\lambda \\left( \n\\left[\n\\begin{array}{cc}\n 7 & 6 \\\\\n 6 & 15 \\\\\n\\end{array}\n\\right]\n \\right) =\n\\left\\{ 11 + 2 \\sqrt{13},11-2 \\sqrt{13} \\right\\}\n$$\nThe singular values are the square roots of the ordered eigenvalues:\n$$\n  \\sigma_{k} = \\lambda_{k},\\qquad k = 1, \\rho\n$$\nConstruct the diagonal matrix of singular values $\\mathbf{S}$ and embed this into the sabot matrix $\\Sigma$:\n$$\n\\mathbf{S} =\n\\left[\n\\begin{array}{cc}\n \\sqrt{11 + 2 \\sqrt{13}} & 0 \\\\\n 0 & \\sqrt{11-2 \\sqrt{13}} \n \\end{array}\n\\right], \n\\qquad \n%\n\\Sigma =\n\\left[\n\\begin{array}{c}\n \\mathbf{S} \\\\ \\mathbf{0}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}{cc}\n \\sqrt{11+2 \\sqrt{13}} & 0 \\\\\n 0 & \\sqrt{11-2 \\sqrt{13}} \\\\\\hline\n 0 & 0 \\\\\n 0 & 0 \\\\\n\\end{array}\n\\right]\n%\n$$\nConstruct the matrix $\\mathbf{V}$:\nSolve for the eigenvectors of the product matrix $\\mathbf{A}^{*} \\mathbf{A}$. They are\n$$\nv_{1} =\n\\color{blue}{\\left[\n\\begin{array}{c}\n \\frac{1}{3} \\left(-2+\\sqrt{13} \\right) \\\\ 1\n\\end{array}\n\\right]}, \\qquad\nv_{2}=\n\\color{blue}{\\left[\n\\begin{array}{c}\n \\frac{1}{3} \\left(-2-\\sqrt{13} \\right) \\\\ 1\n\\end{array}\n\\right]}\n$$\nThe normalized form of these vectors will form the columns of $\\color{blue}{\\mathbf{V}_{\\mathcal{R}}}$\n$$\n  \\color{blue}{\\mathbf{V}_{\\mathcal{R}}} = \n\\left[\n\\begin{array}{cc}\n% v1\n\\frac{3}{\\sqrt{26-4 \\sqrt{13}}}\n  \\color{blue}{\\left[ \\begin{array}{c}\n   \\frac{1}{3} \\left(-2+\\sqrt{13} \\right) \\\\ 1\n   \\end{array} \\right]}\n% v2\n\\frac{3}{\\sqrt{26+4 \\sqrt{13}}}\n  \\color{blue}{\\left[ \\begin{array}{c}\n   \\frac{1}{3} \\left(-2-\\sqrt{13} \\right) \\\\ 1\n   \\end{array} \\right]}\n\\end{array}\n%\n\\right]\n$$\nBecause the null space $\\color{red}{\\mathcal{N} \\left( \\mathbf{A} \\right)}$ is trivial,\n$$\n \\mathbf{V} = \\color{blue}{\\mathbf{V}_{\\mathcal{R}}}\n$$\nConstruct the matrix $\\mathbf{U}$:\nThe thin SVD is\n$$\n\\begin{align}\n  \\mathbf{A} &=\n% U \n \\color{blue}{\\mathbf{U}_{\\mathcal{R}}} \n% Sigma\n  \\mathbf{S} \\,\n% V \n  \\color{blue}{\\mathbf{V}_{\\mathcal{R}}}^{*} \n\\end{align}\n$$\nwhich can be solved as\n$$\n\\begin{align}\n \\color{blue}{\\mathbf{U}_{\\mathcal{R}}} &= \\mathbf{A} \\color{blue}{\\mathbf{V}_{\\mathcal{R}}} \\mathbf{S}^{-1} \\\\\n%%\n&=\n\\left[ \\begin{array}{cc}\n\\frac{1}{\\sqrt{182 + 8\\sqrt{13}}}\n\\color{blue}{\\left[\n\\begin{array}{r}\n  7 + \\sqrt{13} \\\\\n  4 + \\sqrt{13} \\\\\n -5 + \\sqrt{13} \\\\\n -1 + 2 \\sqrt{13} \\\\\n\\end{array}\n\\right]  }\n&\n%\n\\frac{1}{\\sqrt{182 - 8\\sqrt{13}}}\n\\color{blue}{\\left[\n\\begin{array}{r}\n  7 - \\sqrt{13} \\\\\n  4 - \\sqrt{13} \\\\\n -5 - \\sqrt{13} \\\\\n -1 - 2 \\sqrt{13} \\\\\n\\end{array}\n\\right]  }\n\\end{array} \\right]\n%%\n\\end{align}\n$$\nThe thin SVD is now complete. If you insist upon the full form of the SVD, we can compute the two missing null space vectors in $\\mathbf{U}$ using the Gram-Schmidt process. One such result is\n$$\n\\mathbf{U} =\n\\left[ \\begin{array}{cc}\n\\frac{1}{\\sqrt{182 + 8\\sqrt{13}}}\n\\color{blue}{\\left[\n\\begin{array}{r}\n  7 + \\sqrt{13} \\\\\n  4 + \\sqrt{13} \\\\\n -5 + \\sqrt{13} \\\\\n -1 + 2 \\sqrt{13} \\\\\n\\end{array}\n\\right]  }\n&\n%\n\\frac{1}{\\sqrt{182 - 8\\sqrt{13}}}\n\\color{blue}{\\left[\n\\begin{array}{r}\n  7 - \\sqrt{13} \\\\\n  4 - \\sqrt{13} \\\\\n -5 - \\sqrt{13} \\\\\n -1 - 2 \\sqrt{13} \\\\\n\\end{array}\n\\right]  }\n&\n\\frac{1}{\\sqrt{26}}\n\\color{red}{\\left[\n\\begin{array}{r}\n  3 \\\\\n -4 \\\\\n  1 \\\\\n  0 \\\\\n\\end{array}\n\\right]  }\n&\n%\n\\frac{1}{\\sqrt{35}}\n\\color{red}{\\left[\n\\begin{array}{r}\n  3 \\\\\n -5 \\\\\n  0 \\\\\n  1 \\\\\n\\end{array}\n\\right]  }\n%\n\\end{array} \\right]\n$$\nConclusion\nThe singular values only interact with the first two range space vectors.\n$$\n\\begin{align}\n\\mathbf{A} &=\n% U \n  \\left[ \\begin{array}{cc}\n     \\color{blue}{\\mathbf{U}_{\\mathcal{R}}} & \\color{red}{\\mathbf{U}_{\\mathcal{N}}}\n  \\end{array} \\right]  \n% Sigma\n  \\left[ \\begin{array}{c}\n     \\mathbf{S}  \\\\\n     \\mathbf{0} \\\\\n  \\end{array} \\right]\n% V \n  \\color{blue}{\\mathbf{V}_{\\mathcal{R}}}^{*} \\\\\n&=\n% U\n\\left[\n\\begin{array}{cccc}\n \\color{blue}{\\star} & \\color{blue}{\\star} & \\color{red}{\\star} & \\color{red}{\\star} \\\\\n \\color{blue}{\\star} & \\color{blue}{\\star} & \\color{red}{\\star} & \\color{red}{\\star} \\\\\n \\color{blue}{\\star} & \\color{blue}{\\star} & \\color{red}{\\star} & \\color{red}{\\star} \\\\\n \\color{blue}{\\star} & \\color{blue}{\\star} & \\color{red}{\\star} & \\color{red}{\\star} \\\\\n\\end{array}\n\\right]\n% S\n\\left[\n\\begin{array}{cc}\n \\sqrt{11+2 \\sqrt{13}} & 0 \\\\\n 0 & \\sqrt{11-2 \\sqrt{13}} \\\\\\hline\n 0 & 0 \\\\\n 0 & 0 \\\\\n\\end{array}\n\\right]\n% V\n\\left[\n\\begin{array}{cc}\n \\color{blue}{\\star} & \\color{blue}{\\star} \\\\\n \\color{blue}{\\star} & \\color{blue}{\\star} \\\\\n\\end{array}\n\\right]\n\\end{align}\n$$", "meta": {"post_id": 1771013, "input_score": 33, "output_score": 65, "post_title": "How is the null space related to singular value decomposition?"}}
{"input": "How can we turn any number (where the number is > 2) into a prime number by simply appending more digits? I'm referring to the right side of the number.\nSo \n4 is not a prime number\nBut If I append 1 or 3 or 7 it can become a prime number\n41, 43, 47 are all in fact prime numbers\nI \"know\" this is possible (however I don't have a proof for that) simply because of \"density\" of prime numbers, so given any number I can simply add many \"5\" digits and then starting searching prime numbers by adding \"1\" to the resulting number and test it for primality.\nHowever is there any smarter way to do that without using \"bruteforce\" (i.e. testing for primality a range of values )\nOut of the box solutions are preferred, in example.\nif the number is in the form XYZ then XYZ(ZYX+1) is prime.", "output": "It is known that there is a positive number $\\delta$ strictly less than 1 such that, if $n$ is large enough, then there's a prime between $n$ and $n+n^{\\delta}$. [I think the state of the art has $\\delta=.535$] If $n$ is large enough, then $n$ and $n+n^{\\delta}$ start with the same however-many-digits-you-like. So that proves it's always possible, although it doesn't give you a good way to do it. \nIt is conjectured that there's always a prime between $n$ and $n+C(\\log n)^2$ for some positive constant $C$ [$C=2$ may even do, at least for large $n$], but this is way beyong what anyone currently knows how to prove.", "meta": {"post_id": 1773889, "input_score": 32, "output_score": 37, "post_title": "How can we turn any number into a prime number by simply adding more digits?"}}
{"input": "The size of the set of functions that map $\\mathbb{R}\\to \\mathbb{R}$ equals $(\\#\\mathbb{R})^{\\#\\mathbb{R}}$. How many non-differentiable functions are there in this set?", "output": "Here is a simple way to get the answer:\nSuppose a function $f:\\mathbb R \\to \\mathbb R$ is equal to the function $g$ on $\\mathbb Q$, where $g:\\mathbb Q \\to \\mathbb Q$ is defined by $g(p/q) = q$ (and we choose the representation $p/q$ so that $q$ is the smallest possible positive integer). Then $f$ is nowhere differentiable, because it is unbounded on every interval.\nAnd the number of such $f$ is $|\\mathbb R^{\\mathbb R \\setminus \\mathbb Q}| = |\\mathbb R^{\\mathbb R}|$, because $|\\mathbb R \\setminus \\mathbb Q| = |\\mathbb R|$.\nHence there as many nowhere-differentiable functions $\\mathbb R \\to \\mathbb R$ as there are functions $\\mathbb R \\to \\mathbb R$.\n(This doesn't tell you how many differentiable functions there are. The number of somewhere-differentiable functions is the same as the set of all functions; but the number of everywhere-differentiable functions is $|\\mathbb R^\\mathbb Q| = |\\mathbb R|$, because such a function is determined by its values on the rationals.)", "meta": {"post_id": 1777344, "input_score": 18, "output_score": 49, "post_title": "How many non-differentiable functions exist?"}}
{"input": "In order to find a solution for the general linear SDE\n\\begin{align}\ndX_t = \\big( a(t) X_t + b(t) \\big) dt + \\big( g(t) X_t + h(t) \\big) dB_t,\n\\end{align}\nI assume that $a(t), b(t), g(t)$ and  $h(t)$ are given deterministic Borel functions on $\\mathbb{R}_+$ that are bounded on each compact time interval. \nTo find a suitable integrating factor $Z_t$,\n\\begin{align}\ndX_t - \\big( a(t) dt - g(t) dB_t \\big) X_t =  b(t) dt + h(t)  dB_t\n\\end{align}\nbrings me to\n\\begin{align}\nZ_t = e^{-\\int_0^t ( a(s) - \\frac{1}{2}g^2(s) )ds - \\int_0^t g(s) dB_s}.\n\\end{align}\nSo,\n\\begin{align}\nd(Z_tX_t) &=  Z_t \\big( b(t) dt + h(t)  dB_t \\big) \\\\\nX_t &= Z_t^{-1} \\big(X_0 + \\int_0^t  Z_s b(s) ds + \\int_0^t Z_s h(s)  dB_s \\big).\n\\end{align}\nTo find an explicit solution for the SDE, I am considering\n\\begin{align}\nX_t &= Z_t^{-1} Y_t \\\\\ndX_t &= d(Z_t^{-1} Y_t) \\\\\n&= Z_t^{-1} dY_t + Y_t dZ_t^{-1} + d[Z_t^{-1}, Y_t].\n\\end{align}\nWhere $dZ_t^{-1} = Z_t^{-1} \\big( a(t) dt + g(t) dB_t \\big)$ and $dY_t = Z_t \\big( b(t) dt + h(t) dB_t \\big)$.\nMy question is, how to find $d[Z_t^{-1}, Y_t]$ and how to work towards the explicit solution for the SDE?", "output": "Here is the complete solution to the problem including some special cases for an easy start.\nWith analogy to the integrating factor method from ODEs it seems natural to rearrange\n\\begin{align*}\n\\mathrm{d}X_t = (a(t)X_t+ b(t)) \\mathrm{d}t + (g(t)X_t+ h(t))\\mathrm{d}B_t \n\\end{align*}\nto the form\n\\begin{align*}\\mathrm{d}X_t - X_t \\left( a(t)\\mathrm{d}t+g(t)\\mathrm{d}B_t\\right) = b(t) \\mathrm{d}t + h(t)\\mathrm{d}B_t .\\end{align*}\nNow we want to find a \"nice\" stochastic process $Z_t$ such that\n\\begin{align*}( \\star)  \\ \\ \\ \\mathrm{d}(X_tZ_t) =&  Z_t\\mathrm{d}X_t \\underbrace{- Z_tX_t \\left( a(t)\\mathrm{d}t+g(t)\\mathrm{d}B_t\\right)}_{X_t\\mathrm{d}Z_t + \\mathrm{d}X_t\\mathrm{d}Z_t} \\\\ =& Z_t(b(t)\\mathrm{d}t + h(t)\\mathrm{d}B_t).\\end{align*}\nAssume that $Z_t$ is an It\u00f4 process such that\n$$\\mathrm{d}Z_t = f_1(t, Z_t)\\mathrm{d}t + f_2(t, Z_t)\\mathrm{d}B_t , \\ \\ Z_0 = 1.$$\nLet us apply It\u00f4's product formula to $\\mathrm{d}(X_tZ_t)$ we obtain that\n\\begin{align*}\\mathrm{d}(X_tZ_t) =& Z_t \\mathrm{d}X_t + X_t \\mathrm{d}Z_t + \\mathrm{d}X_t \\mathrm{d}Z_t  \\\\ ( \\star \\star) \\ \\ \\ \\  \\qquad = & Z_t\\mathrm{d}X_t + X_t \\left( f_1(t, Z_t)\\mathrm{d}t + f_2(t, Z_t)\\mathrm{d}B_t \\right) + (g(t)X_t + h(t))f_2(t, Z_t) \\mathrm{d}t.\\end{align*}\nComparing the above with the right hand-side of $(\\star)$ we arrive at\n\\begin{align*}- Z_tX_t \\left( a(t)\\mathrm{d}t+g(t)\\mathrm{d}B_t\\right) = X_t \\left( f_1(t, Z_t)\\mathrm{d}t + f_2(t, Z_t)\\mathrm{d}B_t \\right) + (g(t)X_t + h(t))f_2(t, Z_t) \\mathrm{d}t \\end{align*}\nand thus\n\\begin{align*} -Z_tX_tg(t)\\mathrm{d}B_t &= X_t f_2(t, Z_t)\\mathrm{d}B_t \\\\\n-Z_tX_ta(t)\\mathrm{d}t &= \\big(X_tf_1(t, Z_t)+(X(t)g(t)+h(t))f_2(t,Z_t)\\big)\\mathrm{d}t.\\end{align*}\nFrom the first equation we can deduce that $f_2(t, Z_t) = -Z_tg(t)$ and so the second one converts to\n$$-Z_tX_ta(t)\\mathrm{d}t = \\big(X_tf_1(t, Z_t)-Z_tg(t)(X(t)g(t)+h(t))\\big)\\mathrm{d}t,$$\nand so\n$$ Z_t (-X_ta(t)+X(t)g^2(t)+g(t)h(t)) \\mathrm{d}t = X_tf_1(t, Z_t)\\mathrm{d}t.$$\nHence,\n$$f_1(t, Z_t) = Z_t(-a(t)+g^2(t)+X_t^{-1}g(t)h(t)).$$\nHowever, we want $Z_t$ to be free of $X_t$ for that we consider two cases\nCASE 1. $g(t) \\neq 0$, $h(t)=0$\nThen $Z_t$ would satisfy\n$$\\mathrm{d}Z_t = Z_t(-a(t)+g^2(t))\\mathrm{d}t -Z_t g(t)\\mathrm{d}B_t , \\ \\ Z_0 = 1$$\n(We solve the above SDE by a standard trick involving Ito's formula, that is, first we divide by $Z_t$ to obtain the expression for $Z_t^{-1}\\mathrm{d}Z_t$ and then derive the expression $\\mathrm{d}(\\ln(Z_t))$)\nand so $$Z_t = \\exp\\left( \\int_0^t\\left( \\frac{1}{2}g^2(s) - a(s)\\right) \\mathrm{d}s - \\int_0^t g(s)\\mathrm{d}B_s\\right).$$\nThis is the integrating factor which you obtained, and it is not the correct one if $h(t) \\neq 0$.\nIf we continue then we obtain\n\\begin{align*}\\mathrm{d}(Z_tX_t) =& Z_tb(t)\\mathrm{d}t\n\\\\ Z_tX_t  =& X_0 + \\int_0^t Z_sb(s) \\mathrm{d}s\\\\\nX_t =& X_0Z_t^{-1}+ Z_t^{-1}\\int_0^t Z_sb(s) \\mathrm{d}s\\\\\nX_t =&  \\exp\\left( \\int_0^t\\left( a(s) - \\frac{1}{2}g^2(s) \\right) \\mathrm{d}s + \\int_0^t g(s)\\mathrm{d}B_s\\right) \\\\ &\\cdot \\left(X_0+ \\int_0^{t} b(s)\\exp\\left( \\int_0^s\\left( \\frac{1}{2}g^2(r) - a(r)\\right) \\mathrm{d}r - \\int_0^s g(r)\\mathrm{d}B_r\\right)\\mathrm{d}s\\right).\n\\end{align*}\nCASE 2. $g(t) = 0$, $h(t) \\neq 0$\nThen $(Z_t)$ satisfies\n$$\\mathrm{d}Z_t = -a(t)Z_t\\mathrm{d}t, \\ \\quad Z_0 =1 $$\nand so\n$$Z_t = \\exp\\left( -\\int_0^t a(s)\\mathrm{d}s\\right).$$\nTherefore,\n\\begin{align*}\\mathrm{d}(X_tZ_t) =& Z_tb(t)\\mathrm{d}t+ Z_t h(t)\\mathrm{d}B_t\n\\\\ X_tZ_t =& X_0 + \\int_0^t Z_sb(s) \\mathrm{d}s + \\int_0^t Z_s h(s) \\mathrm{d}B_s\\\\\nX_t = & X_0Z_t^{-1}+ Z_t^{-1}\\left( \\int_0^t Z_sb(s) \\mathrm{d}s + \\int_0^t Z_s h(s) \\mathrm{d}B_s\\right) \\\\\nX_t = & X_0 e^{ \\int_0^t a(s)\\mathrm{d}s}\\\\ &+ e^{ \\int_0^t a(s)\\mathrm{d}s}\\left( \\int_0^t e^{ -\\int_0^s a(r)\\mathrm{d}r}b(s) \\mathrm{d}s + \\int_0^t e^{ -\\int_0^s a(r)\\mathrm{d}r}h(s) \\mathrm{d}B_s\\right).\n\\end{align*}\nThe general case\nSo far we were not able to find the solution for the general case, to find it we need to modify and rearrange our initial equation in a slightly different way.\nLet write the SDE for $(X_t)$ as follows\n\\begin{align*}\\mathrm{d}X_t = (a(t)X_t + b(t)+g(t)h(t) - g(t)h(t))\\mathrm{d}t + (g(t)X_t+h(t))\\mathrm{d}B_t\n\\end{align*}\nafter rearranging we have\n\\begin{align*}\\mathrm{d}X_t - \\left((a(t)X_t+g(t)h(t))\\mathrm{d}t+ g(t)X_t\\mathrm{d}B_t\\right)= (b(t)- g(t)h(t))\\mathrm{d}t + h(t)\\mathrm{d}B_t\n\\end{align*}\nNow let ($Z_t$) be an Ito process that satisfies\n$$\\mathrm{d}Z_t = f_1(t, Z_t)\\mathrm{d}t + f_2(t, Z_t)\\mathrm{d}B_t , \\ \\ Z_0 = 1.$$\nAfter multiplying our SDE by $Z_t$ we obtain\n\\begin{align*}Z_t\\mathrm{d}X_t - Z_t\\left((a(t)X_t+g(t)h(t))\\mathrm{d}t+ g(t)X_t\\mathrm{d}B_t\\right)= Z_t \\left((b(t)- g(t)h(t))\\mathrm{d}t + h(t)\\mathrm{d}B_t\\right)\n\\end{align*}\nApplying Ito product formula to $X_tZ_t$ we get\n$$ \\mathrm{d}(X_tZ_t) = Z_t\\mathrm{d}X_t + X_t\\mathrm{d}Z_t+ \\mathrm{d}X_t\\mathrm{d}Z_t,$$\nwe want to have\n\\begin{align*}X_t\\mathrm{d}Z_t+ \\mathrm{d}X_t\\mathrm{d}Z_t = - Z_t\\left((a(t)X_t+g(t)h(t))\\mathrm{d}t+ g(t)X_t\\mathrm{d}B_t\\right)\\end{align*}\nThe LHS equals to\n\\begin{align*}X_t\\left(f_1(t, Z_t)\\mathrm{d}t + f_2(t, Z_t)\\mathrm{d}B_t \\right)+ (g(t)X_t+h(t))f_2(t, Z_t)\\mathrm{d}t.\\end{align*}\nWe compare with the RHS\n\\begin{align*}X_t\\left(f_1(t, Z_t)\\mathrm{d}t + f_2(t, Z_t)\\mathrm{d}B_t \\right)+ (g(t)X_t+h(t))f_2(t, Z_t)\\mathrm{d}t =  - Z_t\\left((a(t)X_t+g(t)h(t))\\mathrm{d}t+ g(t)X_t\\mathrm{d}B_t\\right)\\end{align*}\nand conclude that $f_2(t, Z_t) = -g(t)Z_t$. Now\n\\begin{align*}X_t\\left(f_1(t, Z_t)\\mathrm{d}t -g(t)Z_t\\mathrm{d}B_t \\right)- (g(t)X_t+h(t))g(t)Z_t\\mathrm{d}t =  - Z_t\\left((a(t)X_t+g(t)h(t))\\mathrm{d}t+ g(t)X_t\\mathrm{d}B_t\\right)\\end{align*}\nsimplifies to\n\\begin{align*}X_tf_1(t, Z_t)\\mathrm{d}t -g(t)^2Z_tX_t\\mathrm{d}t =  - a(t)Z_tX_t\\mathrm{d}t\\end{align*}\nand so\n\\begin{align*}X_tf_1(t, Z_t)\\mathrm{d}t  =  X_t\\left( (g(t)^2- a(t))Z_t\\right)\\mathrm{d}t\\end{align*}\nlet us conclude that\n$$ f_1(Z_t, t) = (-a(t)+g(t)^2)Z_t.$$\nThus $$\\mathrm{d}Z_t = (-a(t)+g^2(t))Z_t \\mathrm{d}t - g(t)Z_t\\mathrm{d}B_t,$$\nyou can see the explicit form of $Z_t$ in Case 1 and\n$$\\mathrm{d}(Z_tX_t) = (b(t)-h(t)g(t))Z_t\\mathrm{d}t + h(t)Z_t\\mathrm{d}B_t.$$\nUsing Ito's lemma we can show that $(Y_t):= (Z_t^{-1})$ is an Ito process such that it satisfies\n$$ \\mathrm{d}Y_t = a(t)Y_t \\mathrm{d}t + g(t)Y_t\\mathrm{d}B_t,\\ \\ Y_0 = 1$$\nit has an explicit form\n$$ Y_t =  \\exp\\left(\\int_0^t (a(s)-\\frac{1}{2}g^2(s)) \\mathrm{d}s +\\int_0^t g(s)\\mathrm{d}B_s\\right).$$\nFinally, we see that\n\\begin{align*}\\mathrm{d}(Z_tX_t) =& (b(t)-h(t)g(t))Z_t\\mathrm{d}t + h(t)Z_t\\mathrm{d}B_t \\end{align*}\nyields\n\\begin{align*}Z_tX_t =& Z_0X_0 + \\int_0^t (b(s)-h(s)g(s))Z_s\\mathrm{d}s + \\int_0^t h(s)Z_s\\mathrm{d}B_s \\end{align*}\nHence, we have the explicit solution\n\\begin{align*}X_t =& Z_0X_0Y_t + Y_t\\left(\\int_0^t (b(s)-h(s)g(s))Z_s\\mathrm{d}s + \\int_0^t h(s)Z_s\\mathrm{d}B_s\\right). \\end{align*}", "meta": {"post_id": 1788853, "input_score": 16, "output_score": 37, "post_title": "Solution to General Linear SDE"}}
{"input": "Sheaf cohomology was first introduced into algebraic geometry by Serre. He used \u010cech cohomology to define sheaf cohomology. Grothendieck then later gave a more abstract definition of the right derived functor of the global section functor.\nWhat I still don't understand what was the actual motivation for defining sheaf cohomology. What was the actual problem they were trying to solve?", "output": "Sheaves and sheaf cohomology were invented not by Serre, but by Jean Leray while he was a World War II  prisoner in Oflag XVII (Offizierlager=Officer Camp) in Austria.\nAfter the war he published his results in 1945 in the Journal de Liouville.\nHis remarkable but rather obscure results were clarified by Borel, Henri Cartan, Koszul, Serre and Weil in the late 1940's and early 1950's.    \nThe first spectacular application of Leray's new ideas was Weil's proof of De Rham's theorem: he computed the  cohomology of the constant sheaf $\\underline {\\mathbb R}$ on a manifold $M$ through its resolution by the acyclic complex of differential forms $\\Omega_M^*$ on $M$.     \nThe next success story for sheaves and their cohomology was the proof by Cartan and Serre of theorems $A$ and $B$ for Stein manifolds, which solved a whole series of difficult problems (like Cousin I and Cousin II) with the help of techniques and theorems of Oka, who can be said a posteriori to have implicitly introduced sheaves in complex analysis.\nThe German complex analysts (Behnke, Stein, Thullen,...) who had up to then be the masters of the field  were so impressed by the new cohomology techniques that they are reported to have exclaimed: \"the French have tanks and we have bows and arrows!\"     \nArmed with his deep knowledge of these weapons of complex analysis Serre took the incredibly bold step of introducing  sheaves and their cohomology on algebraic varieties endowed with their Zariski topology.\nThis was of remarkable audacity because of the coarseness of Zariski topology, which had led specialists to believe that it was  just some rather unimpressive tool allowing one for example to talk rigorously of generic properties.\nAs all algebraic geometers now know, Serre stunned his colleagues  by showing in FAC how cohomological methods yielded deep results, at the centre of which are theorems $A$ and $B$ for coherent sheaves on affine varieties.\nOther fundamental novelties obtained by Serre in FAC are his twisting sheaves $\\mathcal O(n)$, the computation of the cohomology of coherent sheaves on projective space, the vanishing of the cohomology groups $H^q(V,\\mathcal F(n))$ on a projective variety $V$ for $q\\gt0, n\\gt\\gt 0$,...     \nLast not least: the introduction of sheaves and their cohomology in FAC paved the way for Grothendieck's revolutionary introduction of schemes in algebraic geometry, as acknowledged in the preface of EGA.\nActually reading FAC was the secondary thesis of Grothendieck, accompanying his PhD on nuclear spaces in functional analysis.\nAfer his PhD defence, someone (Cartan if I remember the anecdote correctly) told him good-humouredly that he seemed not to have understood much in FAC.\nThe story goes that Grothendieck was piqued, invested much energy in understanding Serre, and the rest is history. Se non \u00e8 vero, \u00e8 ben trovato...", "meta": {"post_id": 1798667, "input_score": 43, "output_score": 59, "post_title": "Why was Sheaf cohomology invented?"}}
{"input": "I know $\\pi_1(\\mathbb{R}^2\\setminus\\{x,y\\}) = \\mathbb{Z}\\ast\\mathbb{Z} = \\langle a,b\\rangle$, but its non-abelian-ness isn't obvious to me. \nSpecifically, I draw a box and two points to represent $x$ and $y$. I call the clockwise loop around $x$ $a$, and the clockwise loop around $y$, $b$. \nIf I draw the loops $ab$ and $ba$ they look `obviously homotopic' to me (they are both a single clockwise loop containing both $x$ and $y$), so should be the same element in $\\pi_1$, which is clearly not the case if we take the free product of $\\mathbb{Z}$ with itself. \nIt seems that I must be making a big mistake somewhere. What is the obstruction to $ab \\simeq ba$? I would prefer something concrete, I know this fundamental group follows from standard theorems (van Kampen's theorem), but I want to see the homotopy fail, because I think they are pretty clearly homotopic - so there is something wrong with my intuition which I'd like to fix.", "output": "An important detail is that the fundamental group is built from loops that all start and end at a common base point.  We know that some loops can be continuously deformed into other loops; these loops are called homotopically equivalent. As a special case, some loops can be continuously deformed into other loops while keeping the base point fixed in place.\nThe fundamental group consists of loops which are considered equivalent when they are homotopic in this special base-point-preserving sense. The fundamental group fails to be commutative when there are loops that are homotopically equivalent in the generic sense, and yet not homotopically equivalent in the base-preserving sense.\nIn pictures, below you can see the plane with two holes along with three loops a, b, c with a common base point. \n\nNote that you can't continuously deform b into c unless you're allowed to move the base point. (This is our signal that the fundamental group doesn't commute, which we'll show in more detail.)\n\nLoops b and c aren't homotopic in a way that preserves the base point. Therefore, b \u2262 c in the fundamental group.\n\nEven so, b and c are homotopic in the general sense. For example, one (non-base-point-preserving) homotopy from b to c involves sliding the base point around the lower hole:\n\nA neat observation is that during this homotopy, the trajectory of the base point is recognizable as loop a! \n\nLoops b and c are homotopic in the general sense. The trajectory of the base point under that homotopy is loop a.\n\nFrom this, it follows that there's a base-preserving homotopy between loop c\u2022a (\"go around the hole, then perform c\") and loop a\u2022b (\"perform b, then go around the hole\"):\n\n\nLoops c\u2022a and a\u2022b are equivalent in the fundamental group. In other words, loops b and c are conjugates:\n  $$ca \\equiv ab$$ $$c \\equiv a\\cdot b \\cdot a^{-1}$$\n\nIf the fundamental group were abelian, then this conjugacy would imply that b = c. Because b \u2262 c, the fundamental group is nonabelian.\n\nIn general, when there's a homotopy between two loops b and c, let a be the trajectory of the base point under that homotopy. Then a is itself a loop with the same base point as b and c, and c= aba\u207b\u00b9 (b and c are conjugates). If there isn't a base-preserving homotopy, then a is nontrivial and b \u2262 c, so the fundamental group isn't abelian.\n\nFinal comment: Note that from the beginning, the source of the problem (noncommutativity) is the presence of holes. We saw intuitively that we could deform b into c, but only by moving the base point\u2014 there's a hole in the way.  Now we can see that property emerge algebraically: \nLoops b and c are deformable into one another when they're conjugate c= aba\u207b\u00b9.  The element a describes the trajectory of the base point during that deformation. So if they're deformable without moving the base point, this is equivalent to saying that conjugacy holds when a is trivial  (topologically, a is the constant loop at the base point; algebraically, it's the multiplicative unit) in which case we have b = c. Otherwise, if we must move the base point in order to deform one loop into the other, this is equivalent to saying that conjugacy c= aba\u207b\u00b9 only holds for a nontrivial choice of loop a. A loop is nontrivial just when it encircles a hole (!), hence this is an algebraic way of saying that a hole (call it hole A) stands in the way of deforming b into c.", "meta": {"post_id": 1802198, "input_score": 13, "output_score": 36, "post_title": "Why is the fundamental group of the plane with two holes non-abelian?"}}
{"input": "Let $Q=\\mathbb Q \\cap(0,1)= \\{r_1,r_2,\\ldots\\}$ be the rational numbers in $(0,1)$ listed out so we can count them. Define $x_n=\\frac{1}{n}\\sum_{k=1}^nr_n$ to be the average of the first $n$ rational numbers from the list.\nQuestions:\n\nWhat is required for $x_n$ to converge? Certainly $0< x_n < 1$ for all $n$.\nDoes $x_n$ converge to a rational or irrational?\nHow does the behavior of the sequence depend on the choice of list? I.e. what if we rearrange the list $\\mathbb Q \\cap(0,1)=\\{r_{p(1)},r_{p(2)},\\ldots\\}$ with some one-to-one permutation $p: \\mathbb N \\to \\mathbb N$? How does the behavior of $x_n$ depend on $p$?\n\n\nMy thoughts:\nIntuitively, I feel that we might be able to choose a $p$ so that $x_n\\rightarrow y$ for any $y\\in[0,1]$. However, it also makes intuitive sense that, if each rational appears only once in the list, that the limit is required to be $\\frac{1}{2}.$ Of course, intuition can be very misleading with infinities!\nIf we are allowed to repeat rational numbers with arbitrary frequency (but still capturing every rational eventually), then we might be able to choose a listing so that $x_n\\rightarrow y$ for any $y\\in(0,\\infty)$. \nThis last point might be proved by the fact that every positive real number has a sequence of positive rationals converging to it, and every rational in that list can be expressed as a sum of positive rationals less than one. However, the averaging may complicate that idea, and I'll have to think about it more.\n\nExample I:\nNo repetition:\n$$Q=\\bigcup_{n=1}^\\infty \\bigcup_{k=1}^n \\left\\{\\frac{k}{n+1}\\right\\} =\\left\\{\\frac{1}{2},\\frac{1}{3},\\frac{2}{3},\\frac{1}{4},\\frac{3}{4},\\frac{1}{5},\\ldots\\right\\}$$\nin which case $x_n\\rightarrow\\frac{1}{2},$ a very nice and simple example. Even if we keep the non-reduced fractions and allow repetition, i.e. with\n$Q=\\{\\frac{1}{2},\\frac{1}{3},\\frac{2}{3},\\frac{1}{4},\\boxed{\\frac{2}{4},}\\frac{3}{4},\\frac{1}{5},\\ldots\\},$\nthen $x_n\\rightarrow\\frac{1}{2}.$ The latter case is easy to prove since we have the subsequence $x_{n_k}=\\frac{1}{2}$ for $n_k=\\frac{k(k+1)}{2},$ and the deviations from $1/2$ decrease. The non-repetition case, I haven't proved, but simulated numerically, so there may be an error, but I figure there is an easy calculation to show whether it's correct.\n\nExample II:\nConsider the list generated from the Stern-Brocot tree:\n$$Q=\\left\\{\\frac{1}{2},\\frac{1}{3},\\frac{2}{3},\\frac{1}{4},\\frac{2}{5},\\frac{3}{5},\\frac{3}{4},\\ldots\\right\\}.$$\nI'm sure this list could be studied analytically, but for now, I've just done a numerical simulation. The sequence of averages $x_n$ hits $\\frac{1}{2}$ infinitely often, but may be oscillatory and hence not converge. If it converges, it does so much slower than the previous examples. It appears that $x_{2^k-1}=0.5$ for all $k$ and that between those values it comes very close to $0.44,$ e.g. $x_{95743}\\approx 0.4399.$ However, my computer code is probably not very efficient, and becomes very slow past this.", "output": "Depending on how you order the rationals to begin with, the sequence $x_n$ could tend to anything in $[0,1]$ or could diverge.\nSay $y\\in[0,1]$. Start with an enumeration $r_1,\\dots$ of the rationals in $(0,1)$. When I say \"choose a rational such that [whatever]\" I mean you should choose the first rational currently on that list that satisfies [whatever], and then cross it off the list.\nStart by choosing $10$ rationals in $I_1=(y-1/10,y+1/10)$. Then choose one rational in $[0,1]\\setminus I_1$. Then choose $100$ rationals in $I_2=(y-1/100,y+1/100)$, and then choose one rational in $[0,1]\\setminus I_2$. Etc.\nFirst, note we have in fact defined a reordering of the original list. No rational appears in the new ordering more than once, because it is crossed off the original list the first time it is chosen. And every rational appears on the new list. In fact you can show by induction on $n$ that $r_n$ must be chosen at some stage: By induction you can assume that every $r_j$ for $j<n$ is chosen at some stage. So at some stage $r_n$ is the first remaining entry on the original list; hence it will be chosen soon, since either it's in $I_k$ or not.\nAnd for large $n$ the vast majority of the rationals in the first $n$ elements of the new ordering are very close to $y$, hence $x_n\\to y$.\n(Similarly, to get $x_n$ to diverge: Start with a large number of rationals near $0$. Follow with a huge number of rationals near $1$, then a stupendous number of rationals near $0$...)", "meta": {"post_id": 1803446, "input_score": 66, "output_score": 57, "post_title": "What is the average rational number?"}}
{"input": "A formal language is defined as a set of strings of symbols. I want to know that if \"symbol\" is a primitive notion in mathematics i.e we don't define what a symbol is. If it is the case that in mathematics every thing(object) is a set and the members of a set are themselves sets, shouldn't we define symbols by set? I'm confused by what comes first, set theory or formal languages.", "output": "The things you actually write on the paper or some other medium are not definable as any kind of mathematical objects. Mathematical structures can at most be used to model (or approximate) the real world structures. For example we might say that we can have strings of symbols of arbitrary length, but in the real world we would run out of paper or ink or atoms or whatever it is we use to store our physical representations of strings.\nSo let's see what we can build non-circularly in what order.\nNatural language\nUltimately everything boils down to natural language. We simply cannot define everything before we use it. For example we cannot define \"define\"... What we hope to do, however, is to use as few and as intuitive concepts as possible (described in natural language) to bootstrap to formal systems that are more 'powerful'. So let's begin.\nNatural numbers and strings\nWe simply assume the usual properties of natural numbers (arithmetic and ordering) and strings (symbol extraction, length and concatenation). If we don't even assume these, we can't do string manipulation and can't define any syntax whatsoever. It is convenient to assume that every natural number is a string (say using binary encoding).\nProgram specification\nChoose any reasonable programming language. A program is a string that specifies a sequence of actions, each of which is either a basic string manipulation step or a conditional jump. In a basic string manipulation step we can refer to any strings by name. Initially all strings named in the program are empty, except for the string named $input$, which contains the input to the program. A conditional jump allows us to test if some basic condition is true (say that a number is nonzero) and jump to another step in the program iff it is so. We can easily implement a $k$-fold iteration of a sequence of actions by using a natural number counter that is set to $k$ before that sequence and is decreased by $1$ after the sequence, and jumping to the start of the sequence as long as $k$ is nonzero. The execution of a program on an input is simply following the program (with $input$ containing the input at the start) until we reach the end, at which point the program is said to halt, and whatever is stored in the string named $output$ will be taken as the output of the program. (It is possible that the program never reaches the end, in which case it does not halt. Note that at this point we don't (yet) want to affirm that every program execution either halts or does not halt. In special cases we might be able to observe that it will not halt, but if we cannot tell then we will just say \"We don't know.\" for now.)\nOne special class of programs are those where conditional jumps are only used to perform iteration (in the manner described above). These programs always terminate on every input, and so they are in some sense the most primitive. Indeed they are called primitive recursive. They are also the most acceptable in the sense that you can 'see clearly' that they always halt, and hence it is very 'well-defined' to talk about the collection of strings that they accept (output is not the empty string), since they always halt and either accept or don't accept. We call such collections primitive recursive as well. (As a side note, there are programs that always halt but are not primitive recursive.)\nFormal system specification\nWe can now use programs to represent a formal system. Specifically a useful formal system $T$ has a language $L$, which is a primitive recursive collection of strings, here called sentences over $T$, some of which are said to be provable over $T$. Often $T$ comes with a deductive system, which consists of rules that govern what sentences you can prove given sentences that you have already proven. We might express each rule in the form \"$\u03c6_1,\u03c6_2,...,\u03c6_k \\vdash \u03c8$\", which says that if you have already proven $\u03c6_1,\u03c6_2,...,\u03c6_k$ then you can prove $\u03c8$. There may even be infinitely many rules, but the key feature of a useful $T$ is that there is one single primitive recursive program that can be used to check a single deductive step, namely a single application of any one of the rules. Specifically, for such a $T$ there is a primitive recursive program $P$ that accepts a string $x$ iff $x$ encodes a sequence of sentences $\u03c6_1,\u03c6_2,...,\u03c6_k,\u03c8$ such that $\u03c6_1,\u03c6_2,...,\u03c6_k \\vdash \u03c8$.\nSince all useful formal systems have such a program, we can verify anyone's claim that a sentence $\u03c6$ is provable over $T$, as long as they provide the whole sequence of deductive steps, which is one possible form of proof.\nUp to now we see that all we need to be philosophically committed to is the ability to perform (finitely many) string manipulations, and we can get to the point where we can verify proofs over any useful formal system. This includes the first-order systems PA and ZFC. In this sense we clearly can do whatever ZFC can do, but whether or not our string manipulations have any meaning whatsoever cannot be answered without stronger ontological commitment.\nGodel's incompleteness theorems\nAt this point we can already 'obtain' Godel's incompleteness theorems, in both external and internal forms. In both we are given a useful formal system $T$ that can also prove whatever PA can prove (under suitable translation). Given any sentence $P$ over $T$, we can construct a sentence $Prov_T(P)$ over $T$ that is intended to say \"$P$ is provable over $T$\". Then we let $Con(T) = \u00acProv_T(\\bot)$. To 'obtain' the external form (if $T$ proves $Con(T)$ then $T$ proves $\\bot$), we can explicitly write down a program that given as input any proof of $Con(T)$ over $T$ produces as output a proof of $\\bot$ over $T$. And to 'obtain' the internal form we can explicitly write down a proof over $T$ of the sentence \"$Con(T) \\rightarrow \\neg Prov_T(Con(T))$\". (See this for more precise statements of this kind of result.)\nThe catch is that the sentence \"$Prov_T(P)$\" is completely meaningless unless we have some notion of interpretation of a sentence over $T$, which we have completely avoided so far so that everything is purely syntactic. We will get to a basic form of meaning in the next section.\nBasic model theory\nLet's say we want to be able to affirm that any given program on a given input either halts or does not halt. We can do so if we accept LEM (the law of excluded middle). With this we can now express basic properties about $T$, for example whether it is consistent (does not prove both a sentence and its negation), and whether it is complete (always proves either a sentence or its negation). This gives meaning to Godel's incompleteness theorems. From the external form, if $T$ is really consistent then it cannot prove $Con(T)$ even though $Con(T)$ corresponds via the translation to an assertion about the natural numbers that is true iff $T$ is consistent.\nBut if we further want to be able to talk about the collection of strings accepted by a program (not just primitive recursive ones), we are essentially asking for a stronger set-comprehension axiom, in this case $\u03a3^0_1$-comprehension (not just $\u0394^0_0$-comprehension). The area of Reverse Mathematics includes the study of distinction between such weak set-theoretic axioms, and the linked Wikipedia article mentions these concepts and others that I later talk about, but a much better reference is this short document by Henry Towsner. With $\u03a3^0_1$-comprehension we can talk about the collection of all sentences that are provable over $T$, whereas previously we could talk about any one such sentence but not the whole collection as a single object.\nNow to prove the compactness theorem, even for (classical) propositional logic, we need even more, namely WKL (weak Konig's lemma). And since the compactness theorem is a trivial consequence of the completeness theorem (say for natural deduction), WKL is also required to prove the completeness theorem. The same goes for first-order logic.\nTuring jumps\nNow it does not really make sense from a philosophical viewpoint to only have $\u03a3^0_1$-comprehension. After all, that is in some sense equivalent to having an oracle for the halting problem (for ordinary programs), which is the first Turing jump. The halting problem is undecidable, meaning that there is no program that always halts on any input $(P,x)$ and accepts iff $P$ halts on $x$. By allowing $\u03a3^0_1$-comprehension we are in a sense getting access to such an oracle. But then if we consider augmented programs that are allowed to use the first Turing jump (it will get the answer in one step), the halting problem for these programs will again be undecidable by any one of themselves, but we can conceive of an oracle for that too, which is the second Turing jump. Since we allowed the first one there is no really good reason to ban the second. And so on.\nThe end result is that we might as well accept full arithmetical comprehension (we can construct any set of strings or natural numbers definable by an formula where all quantifiers are over natural numbers or strings). And from a meta-logical perspective, we ought to have the full second-order induction schema too, because we already assume that we have only been accepting assumptions that hold for the standard natural numbers, namely those which are expressible in the form \"$1+1+\\cdots+1$\".\nNote that everything up to this point can be considered predicative, in the sense that at no point do we construct any object whose definition depends on the truth value of some assertion involving itself (such as via some quantifier whose range includes the object to be constructed). Thus most constructively-inclined logicians are perfectly happy up to here.\nHigher model theory\nIf you only accept countable predicative sets as ontologically justified, specifically predicative sets of strings (or equivalently subsets of the natural numbers), then the above is pretty much all you need. Note that from the beginning we have implicitly assumed a finite alphabet for all strings. This implies that we have only countably many strings, and hence we cannot have things like formal systems with uncountably many symbols. These occur frequently in higher model theory, so if we want to be able to construct anything uncountable we would need much more, such as ZFC.\nOne example of the use of the power of ZFC is in the construction of non-standard models via ultrapowers, which require the use of a weak kind of the axiom of choice. The nice thing about this construction is that it is elegant, and for instance the resulting non-standard model of the reals can be seen to capture quite nicely the idea of using sequences of reals modulo some equivalence relation as a model for the first-order theory of the real numbers, where having eventual consistent behaviour implies the corresponding property holds. The non-constructive ultrafilter is needed to specify whether the property holds in the case without eventually consistent behaviour.\nI hope I have shown convincingly that although we need very little to get started with defining and using a formal system, including even ZFC, all the symbol-pushing is devoid of meaning unless we assume more, and the more meaning we want to express or prove, the stronger assumptions we need. ZFC (excluding the axiom of foundation) is historically the first sufficiently strong system that can do everything mathematicians had been doing, and so it is not surprising that it is also used as a meta-system to study logic. But you're not going to be able to ontologically justify ZFC, if that is what you're looking for.\nSets in set theories\nFinally, your question might be based on a common misconception that in ZFC you have a notion of \"set\". Not really. ZFC is just another formal system and has no symbol representing \"set\". It is simply that the axioms of ZFC were made so that it seems reasonable to assume that they hold for some vague notion of \"sets\" in natural language. Inside ZFC every quantifier is over the entire domain, and so one cannot talk about sets as if there are other kinds of objects. If we use a meta-system that does not have sets, then a model of ZFC might not have any 'sets' at all, whatever \"set\" might mean!\nIn ZFC, one cannot talk about \"the Russell set\", since the comprehension axiom does not allow us to construct such a collection. In MK (Morse-Kelley) set theory, there is an internal notion of sets, and one can construct any class of sets definable by some formula, but one cannot construct anything that resembles a \"class of classes\" for the same reason as Russell's paradox.\nIn the non-mainstream set theory NFU, one has both sets and urelements (extensionality only applying to sets), and so one can make sense of talking about sets here. But NFU is not a very user-friendly system anyway.\nAnd this is also where my answer shall stop.", "meta": {"post_id": 1807800, "input_score": 21, "output_score": 42, "post_title": "Are sets and symbols the building blocks of mathematics?"}}
{"input": "Can path connectedness be defined without using the unit interval or more generally the real numbers?\nI.e., do we need Dedekind cuts or Cauchy convergence equivalence classes of the rational numbers (metric space completion) in order to define any object topologically equivalent to the unit interval?\n\nCompared to the definition of connectedness, which only uses open and closed sets, having to use the unit interval to define path connectedness seems somewhat like using a sledgehammer.\nI suspect that the answer might be no, since for every Hausdorff path connected space, the paths are homeomorphic to the unit interval (at least according to the relevant Wikipedia article). In particular, every locally path-connected Hausdorff space has a bunch of 1-manifolds as subsets.\nStill it is unclear to me, since it seems like it should be able to specify all of the unit interval's topological properties without having to recourse to its analytic definition.\nYour thoughts or help would both be greatly appreciated.\nEDIT: this question probably has something to do with homotopy theory: https://en.wikipedia.org/wiki/Homotopy, with which I am rudimentarily familiar at best.", "output": "There are really two separate questions here: can you define the unit interval space without talking about real numbers, and can you define path-connectedness without talking about the unit interval space?  The answer to both is yes; let me address the second question first.\nLet $P$ be a topological space and let $a,b\\in P$ be two points.  Say that a space $X$ is $(P,a,b)$-connected if for any $x,y\\in X$, there is a continuous map $f:P\\to X$ such that $f(a)=x$ and $f(b)=y$.  Of course, for $(P,a,b)=([0,1],0,1)$, this is just the usual definition of path-connectedness.\nHowever, there is a more \"universal\" characterization of path-connectedness that doesn't require you to know about the space $[0,1]$.  Namely, a space $X$ is path-connected iff it is $(P,a,b)$-connected for all compact Hausdorff spaces $P$ with two distinct points $a,b\\in P$.\nTo prove this, suppose $X$ is path-connected, $P$ is a compact Hausdorff space, $x,y\\in X$, and $a,b\\in P$ are distinct.  Since $X$ is path-connected, there is a path $g:[0,1]\\to X$ such that $g(0)=x$ and $g(1)=y$.  By Urysohn's lemma, there is a continuous map $h:P\\to [0,1]$ such that $h(a)=0$ and $h(b)=1$.  The composition $gh:P\\to X$ is then continuous and satisfies $g(a)=x$ and $g(b)=y$.\nThe idea here is that you could use any space $P$ with two chosen points $a$ and $b$ to define a notion of \"paths\" in a space.  However, if you restrict to compact Hausdorff spaces $P$, then the ordinary interval $[0,1]$ is the \"strongest possible kind of path\" you can have in a space: if you have a $[0,1]$-path between two points, then you have a $P$-path for every other compact Hausdorff space $P$ as well.\n(Of course, all we used about compact Hausdorff is that we know there is a map $P\\to [0,1]$ separating $a$ and $b$.  However, I phrased everything in terms of the compact Hausdorff condition since this is a natural condition you can define without already knowing about the space $[0,1]$.)\n\nOK, now let me say a little about the first question.  There are in fact many different ways to uniquely characterize the space $[0,1]$ up to homeomorphism without reference to the reals or anything that is essentially equivalent to constructing the reals.  In fact, you can deduce one from the answer I gave to the second question above.\nNamely, say that a compact Hausdorff space $(P,a,b)$ equipped with two distinct points is a universal path if it has the special property of $[0,1]$ noted above: whenever there is a $(P,a,b)$-path between two points $x$ and $y$ of an arbitrary space, there is also a $(Q,c,d)$-path from $x$ to $y$ for any compact Hausdorff space $Q$ with two distinct points.  Say that a universal path $(P,a,b)$ is minimal if for any other universal path $(Q,c,d)$, there is an embedding $P\\to Q$ sending $a$ to $c$ and $b$ to $d$.\nI now claim that $([0,1],0,1)$ is the unique minimal universal path (up to homeomorphism).  We know it is universal.  To show that it is minimal, let $(Q,c,d)$ be any universal path.  Since the identity map $Q\\to Q$ is a $(Q,c,d)$-path from $c$ to $d$ in $Q$, universality implies there is a $([0,1],0,1)$-path from $c$ to $d$ in $Q$.  But if there is a path between two points of a Hausdorff space, there is also a path which is an embedding (see Does path-connected imply simple path-connected?).  Thus there is an embedding $[0,1]\\to Q$ sending $0$ to $c$ and $1$ to $d$.\nNow suppose $(P,a,b)$ is any minimal universal path.  The previous paragraph shows that there is a $([0,1],0,1)$-path from $a$ to $b$ in $P$.  Now since $([0,1],0,1)$ is a universal path, minimality of $P$ says that $P$ embeds in $[0,1]$ sending $a$ to $0$ and $b$ to $1$.  But since $P$ contains a path from $a$ to $b$, the image of this embedding contains a path from $0$ to $1$, and thus contains all of $[0,1]$.  Thus the embedding is actually a homeomorphism $P\\to [0,1]$.\nAs I mentioned, this is just one of many ways of characterizing $[0,1]$.  For another characterization that also relates closely to the intuitive notion of \"paths\", see this answer by Tom Leinster on MO.", "meta": {"post_id": 1823987, "input_score": 36, "output_score": 63, "post_title": "Can path connectedness be defined without using the unit interval?"}}
{"input": "$$\\frac{0!}{4!}+\\frac{1!}{5!}+\\frac{2!}{6!}+\\frac{3!}{7!}+\\frac{4!}{8!}+\\frac{5!}{9!}+\\frac{6!}{10!}+\\cdots$$\nThis goes up to infinity. Trying finite cases may help.\nMy Attempt:It seems that it is going to be $\\frac{1}{18}$. My calculations show that its going near $\\frac{1}{18}$.", "output": "An alternative approach to Behrouz' fine one through Euler's beta function. We have:\n$$\\begin{eqnarray*} \\sum_{n\\geq 0}\\frac{n!}{(n+4)!}=\\sum_{n\\geq 0}\\frac{\\Gamma(n+1)}{\\Gamma(n+5)}&=&\\frac{1}{\\Gamma(4)}\\sum_{n\\geq 0}B(4,n+1)\\\\&=&\\frac{1}{6}\\int_{0}^{1}\\sum_{n\\geq 0}x^{n}(1-x)^3\\,dx\\\\&=&\\frac{1}{6}\\int_{0}^{1}(1-x)^2\\,dx\\\\&=&\\frac{1}{6}\\int_{0}^{1}x^2\\,dx = \\frac{1}{6}\\cdot\\frac{1}{3}=\\color{red}{\\frac{1}{18}}.\\end{eqnarray*}$$\n\nWith the same approach it is straightforward to check that:\n  $$ \\sum_{n\\geq 0}\\frac{n!}{(n+k)!} = \\color{red}{\\frac{1}{(k-1)\\cdot(k-1)!}}$$\n  for any $k\\in\\mathbb{N}^+$.\n\nThe same can be achieved by recognizing in the LHS a multiple of a telescopic series.", "meta": {"post_id": 1824603, "input_score": 20, "output_score": 40, "post_title": "Evaluate $\\frac{0!}{4!}+\\frac{1!}{5!}+\\frac{2!}{6!}+\\frac{3!}{7!}+\\frac{4!}{8!}+\\cdots$"}}
{"input": "Determine whether each of these pairs of sets are equal$$A = \\{\\{1\\}\\} \\qquad \\qquad B = \\{1, \\{1\\}\\}$$\n\nI believe $A$ is equal to $B$ because all elements in $A$ are in $B$, but the answer says that it's not.", "output": "Think of $A$ as a bag which contains within it another smaller bag with a one in it.\n$A=\\underbrace{\\{~~~~~~~\\overbrace{\\{1\\}}^{\\text{second bag}}~~~~~~~~\\}}_{\\text{first bag}}$\nOn the other hand, $B$ is a bag which contains in it not only a second bag with a one in it, but also a one which is loose.\n$B=\\underbrace{\\{~~~~~~~~\\overbrace{\\{1\\}}^{\\text{second bag}}~~~~~\\overbrace{1}^{\\text{this too}}~~~~~~~\\}}_{\\text{first bag}}$\n$1\\in B$ but $1\\not\\in A$.  There is no \"loose 1\" in $A$, there is only a bag with a one in it in $A$.\nThus, $A\\neq B$", "meta": {"post_id": 1860874, "input_score": 6, "output_score": 47, "post_title": "Why is $\\{\\{1\\}\\}$ not equal to $\\{1,\\{1\\}\\}$?"}}
{"input": "So here is what I understand:\n\nIf $f(x)$ is increasing/decreasing, then its derivative $f'(x)$ is positive/negative\n\nand...\n\nIf $f(x)$ is increasing/decreasing, then the derivative of $f'(x)$ (which is $f''(x)$) is concave up/concave down\n\nSo my question is: if a graph has a vertical asymptote, the derivative must also have a vertical asymptote, too, right? Does it also work vice versa? I feel like there is a trick to it, but I'm not sure.\nI have a graph from GeoGebra here. The dotted line is the derivative.", "output": "if a graph has vertical asymptote, the derivative must also have a vertical asymptote too, right? \n\nNo. A counterexample: $$f(x)=\\frac{1}{x}+\\sin\\left(\\frac{1}{x}\\right)$$ This function is monotone and has a vertical asymptote at $x=0$. But its derivative has no limit.", "meta": {"post_id": 1863341, "input_score": 26, "output_score": 50, "post_title": "If $f(x)$ has a vertical asymptote, does $f'(x)$ have one too?"}}
{"input": "Recently, I was looking for the reviews of some Analysis books while encountered terms such as Baby/Papa/Mama/Big Rudin. Firstly, I thought that these are the names of a book! But it turned out that these are some nick names used for the books of Walter Rudin. So I was thinking that\n$1$. What are the corresponding books of these nick names?\n$2$. Why such nick names are chosen? or What are their origins?", "output": "In order to sum up the above comments, the corresponding books for these nick names are\n$1$. Baby = Principles of Mathematical Analysis; \n$2$. Papa/Big = Real and Complex Analysis; \n$3$. Grandpa = Functional Analysis;\nand it seems that the difficulty of contents of the books grows with the age of the nick names! Firstly, you are a baby and things are easy to handle. Then you grow up and become a papa and things get more complicated. Finally, when you are a grandpa you should take care of your legacy very carefully which needs a hardwork! So $1$ is a prerequisite of $2$ and $2$ is prerequisite of $3$.", "meta": {"post_id": 1863512, "input_score": 58, "output_score": 58, "post_title": "Baby/Papa/Mama/Big Rudin"}}
{"input": "I am looking for a ring element which is irreducible but not prime. \n\nSo necessarily the ring can't be a PID. My idea was to consider $R=K[x,y]$ and $x+y\\in R$.\nThis is irreducible because in any product $x+y=fg$ only one factor, say f, can have a $x$ in it (otherwise we get $x^2$ in the product). And actually then there can be no $y$ in $g$ either because $x+y$ has no mixed terms. Thus $g$ is just an element from $K$, i.e. a unit.\nI got stuck at proving that $x+y$ is not prime. First off, is this even true? If so, how can I see it?", "output": "An wasy way: let $\\, R = \\mathbb Q + x\\:\\!\\mathbb R[x],\\, $ i.e. the ring of real polynomials with rational constant coefficient. Then $\\,x\\,$ is irreducible but not prime, since $\\,x\\mid (\\sqrt 2 x)^2\\,$ but $\\,x\\nmid \\sqrt 2 x,\\,$ by $\\sqrt 2\\not\\in \\Bbb Q$.\nIt should be clear how to give many further examples of this sort. Rings constructed like this provide a rich source of (counter)examples in various contexts, e.g. see this excellent survey on this and related topics: M. Zafrullah, Various facets of rings between $D[X]$ and $K[X]$.", "meta": {"post_id": 1871637, "input_score": 31, "output_score": 42, "post_title": "What are examples of irreducible but not prime elements?"}}
{"input": "In the figure displayed in the image below :\n\nTo find the remainder on dividing a number by $7$, start at node $0$, for each digit $D$ of the number, move along $D$ black arrows (for digit $0$ do not move at all), and as you pass from one digit to the next, move along a single white arrow.\nFor example, let $n = 325$. Start at node $0$, move along $3$ black arrows (to node $3$), then $1$ white arrow (to node $2$), then $2$ black arrows (to node $4$), then $1$ white arrow (to node $5$), and finally $5$ black arrows (to node $3$). Finishing at node $3$ shows that the remainder on dividing $325$ by $7$ is $3$.\nIf you try this for a number that is divisible by $7$, say $63$, you will always end up in node $0$. Therefore, it can also be used to test divisibility by $7$. In case while traversing the digits of number $n$, you end up in the node $0$, $n$ is divisible by $7$, else not.\nWhat exactly is the mathematical explanation for this? Are there  such type of graphs for any other integer(s) too?", "output": "Such graphs exist for any (non-zero) integer.  In fact, the arrows reflect two basic operations $\\bmod 7$:\n\nThe black arrows represent adding one (note that they go up from $0$ to $6$ and then cyclically back to $0$ again)\nSimilarly, the white arrows represent multiplication by $10$; note that $5$ goes to $1$, and this reflects the fact that $5\\times10=50\\equiv 1\\pmod 7$.\n\nAny given number can be written as a combination of these two operations on a starting value of zero; for instance, your example $N=325$ is equivalent to starting with zero, adding $1$ three times (giving $3$), multiplying by $10$ (giving $30$), adding $1$ twice more (giving $32$), multiplying by $10$ again (giving $320$), and then adding $1$ five more times (giving $325$). The graph just represents these operations $\\bmod 7$, and so if you end up at node zero it means that your original number was a multiple of 7; this works because both the operations of adding one and multiplying by ten 'commute' through the mod-7 operation (i.e., $(n+1)\\bmod 7$ $\\equiv (n\\bmod 7)+1\\pmod 7$ and $(n\\times10)\\bmod 7$ $\\equiv (n\\bmod 7)\\times 10\\pmod 7$ ).  Since the individual operations commute with the mod operation, so will any combination of them.\nBut there's nothing special about $7$ here, and in fact that's enough to understand how to construct the graph for any base $b$: number a set of $b$ nodes from $0$ to $b-1$, and then construct a black arrow from $i$ to $i+1\\pmod b$ for every node $i$, and a white arrow from $j$ to $j\\times10\\pmod b$ for every node $j$.  This will give the analogue of the 'Mickey Mouse' graph for that base.", "meta": {"post_id": 1874604, "input_score": 28, "output_score": 34, "post_title": "Can Mickey Mouse divide by $7$?"}}
{"input": "I know it's easy to prove with the help of linear inequalities, but this time I want to prove it with the help of trigonometry. Is it possible? If yes, then how?", "output": "A classic application of the incircle:", "meta": {"post_id": 1876715, "input_score": 17, "output_score": 34, "post_title": "Proving that the sum of any two sides of the triangle is greater than the third side"}}
{"input": "Has anyone noticed the paper On the zeros of the zeta function and eigenvalue problems by M. R. Pistorius, available on ArXiv?\nThe author claims a proof of RH, and also a growth condition on the zeros.\nIt was posted two weeks ago, and I expected it would have been shot down by now. Has there been any discussion or attempt at verification of this preprint?", "output": "I had a go reading through the paper and I think I found the error. The main argument in the paper can be summarized as follows:\n\nThe Riemann $\\Xi$-function $\\Xi(t) = \\xi\\left(\\frac{1}{2} + it\\right)$ satisfy $\\Xi(t) = \\Xi(0)\\nu_t(\\pi/2)$ where $$\\nu_t(x) = \\int_0^\\infty \\cos(t(y+\\cos(x))\\Phi(y){\\rm d}y$$ and $\\Phi$ is related to the Jacobi $\\theta$-function. This is a result by Riemann and holds true. The author then notes that when $t$ is such that $\\Xi(t) = 0$ then $\\nu_t(x)$ satisfy the Sturm\u2013Liouville (SL) problem \n  $$\\left(\\frac{\\nu_t'(x)}{\\sin(x)}\\right)' + t^2\\sin(x)\\nu_t(x) = 0,~~~\\nu_t'(0) = 0,~~~\\nu_t(\\pi/2) = 0$$\n  This is also true. The proof is completed by appealing to a theorem that says that this problem only has real eigenvalues. If this holds then it follows that $\\Xi(t) = 0\\implies t\\in\\mathbb{R}$ which is the Riemann hypotesis.\n\nThe error is in the last step. It is indeed true that a regular SL problem only has real eigenvalues, however this is not a regular SL problem as $\\frac{1}{\\sin(x)}$ has a pole at $x=0$. In this case there is no guarantee that the eigenvalues have to be real and we can show this explicitly with a simple counter-example: the function $\\nu_t(x) = \\sin(t\\cos(x))$ satisfy the SL problem above for any complex number $t$.\n\nGiven how much interest this question has generated, which I take to mean that many people thought (or perhaps just hoped) this was a potentially viable proof, I think it\u2019s useful to talk a bit about why it had to be wrong. Personally I have only been in academia for $\\sim 10$ years, but I have already managed to see $\\sim 50$ papers$^1$ like this where a major result is proven in a few pages using elementary methods. This is just another one. One soon learns that papers like this are never correct and the reason is often this: if it could have been solved this way then it would have been solved this way many many years ago - the techniques used are just too simple. As for some useful pointers for how to judge for yourself if a paper like this has the potential for being correct I reccommend Scott Aaronsons \"Ten Signs a Claimed Mathematical Breakthrough is Wrong\".\n$^1$ For some examples see the MSE questions [1], [2], [3]", "meta": {"post_id": 1896371, "input_score": 49, "output_score": 51, "post_title": "Proof (claimed) for Riemann hypothesis on ArXiv"}}
{"input": "I've always assumed on faulty intuition that if you have an event which occurs 1 in n chances, it will be super likely to happen at some point of that event occuring n times.  However, given some analysis, it doesn't actually seem to be all that super likely, and seems to converge at a particular value as the value of n rises.  That value is about 0.63212.\nIs this correct?  If so, is there a name for this value and is it considered significant within the field of probability?\nBelow is the Python code that I used to arrive at this value.\n>>> def p(x, r):\n...   return x + r * (1.0 - x)\n\n>>> def p_of_1(r):\n...   x = r\n...   while True:\n...     yield x\n...     x = p(x, r)\n\n>>> def p_of_n(n):\n...   g = p_of_1(1.0 / n)\n...   return [next(g) for x in range(n)]\n...\n\n>>> p_of_n(1)\n[1.0]\n>>> p_of_n(2)\n[0.5, 0.75]\n>>> p_of_n(3)\n[0.3333333333333333, 0.5555555555555556, 0.7037037037037037]\n>>> p_of_n(4)\n[0.25, 0.4375, 0.578125, 0.68359375]\n>>> p_of_n(5)\n[0.2, 0.36000000000000004, 0.488, 0.5904, 0.67232]\n\n>>> p_of_n(6)[-1]\n0.6651020233196159\n>>> p_of_n(10)[-1]\n0.6513215599000001\n>>> p_of_n(100)[-1]\n0.6339676587267709\n>>> p_of_n(10000)[-1]\n0.6321389535670703\n>>> p_of_n(10000000)[-1]\n0.6321205772225762", "output": "It's easier to work backwards.  The probability that the event does not occur on a single try is, of course, $1-\\frac 1n$.  It follows that the probability that it fails to occur in $n$ trials is $p_n=\\left(1-\\frac 1n\\right)^n$.  Therefore the probability that it occurs at least once in those $n$ trials  is $$1-p_n=1-\\left(1-\\frac 1n\\right)^n$$  If we now recall the limit definition of the exponential:  $$e^a=\\lim_{n\\to \\infty}\\left(1+\\frac an\\right)^n$$  We see that, for large $n$, $$1-p_n\\sim 1-\\frac 1e=0.632120559\\dots$$", "meta": {"post_id": 1898912, "input_score": 17, "output_score": 36, "post_title": "Why is $0.63212$ the probability of a $\\frac1n$-probability event happening in $n$ trials?"}}
{"input": "I was studying Numerical Analysis by K. Mukherjee; there he discussed Loss of Significant Figures by Subtraction, as followed:\n\nIn the subtraction of two approximate numbers, a serious type of error may be present when the numbers are nearly equal. ...\n\nHe showed how the number of significant figures got decreased after subtraction.\nIn order to dig more, I googled and came across the term catastrophic cancellation.\nI did try to read the wiki article but it was full of terms especially floating point arithmetic which I am really not acquainted it; I did try to read the latter's wiki article but couldn't comprehend the definition:\n\nThe term floating point refers to the fact that a number's radix point (decimal point, or, more commonly in computers, binary point) can \"float\"; that is, it can be placed anywhere relative to the significant digits of the number.\n\nRadix point can float? Unfortunately, I failed to visualise that.\nHowever, my main question, even if there is an error due to the loss of the significant figures, why is this error \"catastrophic\"?\nI found in a decade old page that \n\nThe relative error here is infinite.\n\nBut how could the loss of significant figures led to an infinite magnitude of relative error?\nCould anyone please explain the reason behind the term \"catastrophic\" keeping in mind that I'm not acquainted with floating point arithmetic?", "output": "The root of the evil is that calculators work with a finite number of digits (well, we can't afford machines with infinite resources yet). When you want to represent real-life numbers, some can be huge, some can be tiny, and this raises an issue.\nFloating-point:\nIf the decimal point is fixed, you can' represent them all, there can be so-called overflow or underflow; and for intermediate orders of magnitude, you lose significant digits.\nFor example, using the $5.5$ fixed-point representation, $127500.$ and $0.0000096695$ cannot be represented, and $\\pi=3.14159$ just uses six significant digits, four positions are wasted.\nThe floating-point representation solves these problems by specifying the position of the decimal point, as in the scientific notation:\n$127500.=1.275000000\\cdot10^5, 0.0000096695=9.669500000\\cdot10^{-6}, \\pi=3.141592654\\cdot10^0$.\nCatastrophic cancellation:\n(This concept is actually unrelated to floating-point.)\nAssume you want to play with the approximation $\\pi\\approx355/113=3.14159292\\cdots$.\nIf you want to take the sum with $\\pi$, all goes well.\n$$\\frac{3.14159265+3.14159292}2=3.14159278.$$\nBut now if you want to take the difference,\n$$\\frac{3.14159265-3.14159292}2=-0.000000135,$$\nonly three significant decimals remain. This is a catastrophy, because you started with nine figures and end-up with just three, and this is irreversible ! (Floating-point will not come to the rescue, $-0.000000135=-1.35\\cdot10^{-7}$ still has three significant digits.)\nThis phenomenon makes some problems very difficult to solve numerically.\n\nIn some cases, catastrophic cancellation can be avoided. For example, when solving the quadratic equation\n$$x^2-1000.001x+1=0,$$\nthe classical formula says\n$$x=\\frac{1000.001\\pm\\sqrt{1000.001^2-4}}2=\\frac{100.001\\pm999.999}2.$$\nAssuming you can only compute with five significant digits, using floating-point, you obtain\n$$x=\\frac{1000.00\\pm999.99}2=999.95\\text{ and }0.005,$$ where the second root is very very inaccurate.\nYou can get a much better estimate by using the fact that the product of the roots is $1$, and the second root evaluates to\n$$\\frac1{999.95}=0.0010005$$", "meta": {"post_id": 1920525, "input_score": 21, "output_score": 38, "post_title": "Why is 'catastrophic cancellation' called so?"}}
{"input": "Let $G$ be the Cantor set. It is well known that:\n\n$G$ is perfect and hence closed.\n$G$ has the cardinality of the continuum.\n$G$ has measure zero.\nFor any set $S \\subset \\mathbb{R}$ (I will not keep writing that we are in $\\mathbb{R}$) we have $S \\text{ is closed} \\Leftrightarrow S^c \\text{ is open}$.\nAny open set $O$ can be written as a --- in fact unique --- countable union of disjoint open intervals.\n\n$G^c$ can thus be written as a countable union of disjoint open intervals. We now imagine this union as being superposed on the real line graphically as follows:\nR: <<<----(....)---(..)--(.)---------(...)--->>>\n\nwhere the (...) represents the open disjoint intervals (of differing size) composing $G^c$, and the ---  represents the remaining non-covered real numbers (that is those in $G$). Now we can cover $\\mathbb{R}$ in its entirety by \"collecting\" the --- into disjoint closed intervals. Any one of these closed intervals might of course consist of only a single element. We now obtain:\nR: <<<[--](....)[-](..)[](.)[-------](...)[-]>>>\n\nTake the union of these disjoint closed intervals. This must be $(G^c)^c = G$. Now it is not hard to imagine a mapping from the (...)'s to the [...]'s. Just take the next (...) in line for each [...] (and do some trivial fixing at the ends). Therefore we have written $G$ as a countable union of disjoint closed sets. However, $G$ has measure zero and therefore cannot contain any closed sets other than the single element type. Hence $G$ is countable. Contradiction.\nWhere do I go wrong?", "output": "You're imagining that the open intervals of $G^c$ are ordered discretely, like the integers, so you have alternating open intervals in $G^c$ and closed intervals in $G$.  But actually, the open intervals of $G^c$ are densely ordered, and order-isomorphic to the rationals.  As a result, there is no \"next (...) in line for each [...]\" like you claim there is.  There are uncountably many closed intervals (actually, all of them are just single points) in between these open intervals, much like how there are uncountably many irrational numbers in between the rational numbers.  There is no \"next rational number\" after each irrational number that you can use to get a bijection between rationals and irrationals, and the same thing is happening here.", "meta": {"post_id": 1928500, "input_score": 17, "output_score": 36, "post_title": "Real analysis contradiction I cannot get rid of"}}
{"input": "I came across what seems to be a very difficult \"solve for $x$\" type of\nproblem, primarily because there should be $6$ real roots of this problem:\n$$(x^2 - 3x - 4)(x^2 - 5x + 6)(x^2 + 2x) + 30 = 0.$$\nMy first step was to (tediously) expand this into the degree $6$ polynomial\nthat it is:\n$$x^6-6 x^5+x^4+36 x^3-20 x^2-48 x+30 = 0.$$\nI would imagine that the first step would be to simplify this into a set of\nsmaller roots, i.e.\n$$(ax + b)(cx^5 + dx^4 + ex^3 + fx^2 + gx + h) = 0,$$\nand then perform a similar factorization on the polynomial of degree $5$.\nHowever, after trying hard to find the constant terms for this factorization,\nI can never quite get the correct factorization. Any recommendations on\nproblems like these? I would really like to solve this without using Wolfram.", "output": "Let $P(x)$ denote the LHS. Note that the first summand factors nicely:\n$$\n  P(x)=(x-2)(x-3)(x-4)x(x+1)(x+2)+30.\n$$\nFrom this it is clear that $P(x)$ is symmetric around $x=1$. Explicitly, setting $y=x-1$ we have\n$$\\begin{eqnarray*}\n  P(x)&=&(y-1)(y-2)(y-3)(y+1)(y+2)(y+3)+30\\\\\n    &=&(y^2-1)(y^2-4)(y^2-9)+30.\n\\end{eqnarray*}$$\nNote that this is a cubic in $y^2$, so it is possible to solve for $y^2$ using the formula for roots of a cubic, and hence find the roots of $P$ in terms of radicals.", "meta": {"post_id": 1936456, "input_score": 11, "output_score": 36, "post_title": "Solve 6th degree polynomial: $(x^2 - 3x - 4)(x^2 - 5x + 6)(x^2 + 2x) + 30$"}}
{"input": "I had the idea to make a script that generates a pattern like this:\n1\n2 3\n4 5 6\n7 8 9 10\n\n...\nand so on. After that, I replaced every non-prime by a '-' character and every prime number by a '|'. The output begins like that:\n- \n|| \n-|- \n|--- \n|-|-- \n-|-|-- \n-|----- \n|-|----- \n|---|-|-- \n-|-----|-- \n---|-|----- \n|---|-|----- \n|---|-----|-- \n-----|---|-|-- \n-|-|---|------- \n------|---|-----\n\n...\nWhat I realized is that, with the exception of the second and third lines (possibly because of 2), on every line until 12000, the two last numbers are never prime. Obviously, one of them is even, but the odd one isn't a prime number in any of these lines.\nIs there a way to prove that this is true for any line? Is there any theorem that could help?", "output": "Great credit to you for having discovered this, because it is a known phenomenon presented in an unknown gem.\nThe last element on each of the lines is the sum of the first $n$ natural numbers (for example, $1=1$, $3=1+2$, $6=1+2+3$,$10=1+2+3+4$ etc.). As it turns out,the sum of the first $n$ natural numbers has a formula, given by $\\frac{n(n+1)}2$. Therefore, the last number on every line is either divisible by the line number $n$ or its successor, $n+1$, whichever one is odd (one of them is odd).\nFor example, $6$ is divisible by its line number $3$.\n$10$ is divisible by one more than its line number, $5$.\nWhich is why, unless you are on the second line, the last number is never going to be prime. In fact, the formula even gives its divisors.\nSimilarly, the second last number on each line is $\\frac{n(n+1)}{2} - 1 = \\frac{n^2+n-2}{2} = \\frac{(n+2)(n-1)}{2}$, hence the factors of the second last number is the odd number out of $n + 2$ and $n - 1$, where $n$ is the row number. We can check this also:\n$9$ is divisible by $3$, one less than $4$.\n$14$ is divisible by $7$, $2$ more  than $5$.\nHence, again the second to last number is not going to be prime.\n\nI think I should add this: After some time, the fourth from last number is never going to be prime! (Think about this yourself).\nAfter some time, the seventh from last number is never going to be prime! (Think about this yourself).\nAfter some time, the eleventh from last number is never going to be prime! (Think about this yourself).\n\nIt is worth wondering about the nature of those elements which are not covered under the $\\frac{n(n+1)}{2} - (r-1)$ scheme below. The point is, for those numbers , we take the $r-1$ on top as usual to form the number $\\frac{n(n+1) - 2r + 2}{2}$, which we know is a natural number. The question is : is this composite or not?\nWhat we do know is that for $r = 1,2,4,7$ etc. we get factorizations of the expression  $n(n+1) - 2r+2$ into two linear terms in $n$, therefore we get composite numbers.\nWhat is true, in the case of the other $r$s, is that we definitely will get infinitely many composite numbers while going through $n(n+1) - 2r + 2$. This is because every non-constant polynomial takes infinitely many composite values. In our case, for example, taking $n = k(2r-2) + 1$ for $k \\in \\mathbb N$, we see that $n(n+1) - 2r+2 = (2r-2)((k(2r-2) + 1)k - 1)$ is composite for $r \\geq 3$. Therefore, for every $r$, there exist infinitely many rows such that the $r$th last number of that row is composite.\nAs for primality, the question is still open  as to whether quadratic expressions(in one variable) which are not linearly factorable take infinitely many prime values. Therefore this question will remain open for some time, whether the $r$th last number of each row will stop containing primes after some time or will have infinitely many primes.", "meta": {"post_id": 1938994, "input_score": 80, "output_score": 115, "post_title": "Why are the last two numbers of this sequence never prime?"}}
{"input": "I'm doing some marking for a year 8 (12 to 13-year-old) scholarship paper and I saw this symbol that looks like the Sputnik probe and I have no idea what it is, does anyone know?", "output": "I think it is a typesetting problem with the parentheses.\nMy best guess at the intended question would be:\n$$\\left( 2-\\frac 12 \\right) \\left( 2-\\frac 23 \\right) \\left( 2-\\frac 34 \\right) \\left( 2-\\frac 45 \\right)$$\nThis seems reasonable given the level of the other questions.", "meta": {"post_id": 1944716, "input_score": 20, "output_score": 43, "post_title": "Maths symbol that looks like Sputnik"}}
{"input": "Let's say I have been studying an older theorem statement and its proof.  I feel that it can be improved, e.g., I can make it a stronger double-implication theorem statement, with a different proof.  Would I still need to cite the paper from which I first studied the theorem?  Even though my work is completely organic and self-contained?\nThanks,", "output": "Adding this only because I think the point wasn't emphasized enough in the other answers, but it may actually hurt the credibility - or, at least, the perceived professionalism - of your paper if you did not cite the old result. An important part of research is to establish context and relevance, and anyone who happens to know or find the old result might (wrongly) conclude that you didn't carry out that part as expected if you didn't cite the prior work.", "meta": {"post_id": 1945872, "input_score": 25, "output_score": 43, "post_title": "Do I need to cite an old theorem, if I've strengthened it, wrote my own theorem statement, with a different proof?"}}
{"input": "A student asked me about a paper by Daniel Biss (MIT Ph.D. and Illinois state senator) proving that \"circles are really just bloated triangles.\" The only published source I could find was the young adult novel An Abundance of Katherines by John Green, which includes the following sentence:\nDaniel [Biss] is world famous in the math world, partly because of a paper he published a few years ago that apparently proves that circles are basically fat, bloated triangles.\nThis is probably just Green's attempt to replicate something Biss told him about topology (an example of homotopy, perhaps).\nBut the statement seems to have intrigued students and non-mathematicians online. So I'm curious: has anyone seen such a paper? Is this a simplified interpretation of a real result (maybe in The homotopy type of the matroid grassmannian?).", "output": "The book with those words was published in 2006, before the retraction of Biss' major results on combinatorial differential geometry.  In the cited paper, Biss had published an amazing breakthrough along the lines that Green understood, showing that certain continuous geometric objects were equivalent to discrete combinatorial objects.  That is similar to, but much more general than, the relation between a geometric circle and the triangle (in the sense of graph theory, 3 vertices connected by 3 edges, oriented to go around the triangle, and not a Euclidean geometry triangle or a physical triangular plate).\nI quote and annotate an excerpt from the introduction to Biss' article, to give the idea of how incredible it must have sounded at the time.  Had it been correct then \"world famous in the math world\" would have been a good description.\n\n[p.931]...the theory of matroid\n  bundles is the same as the theory of vector bundles [!!].  This gives substantial evidence that a CD [Combinatorial Differential] manifold has the capacity to model many properties of smooth manifolds.  To make this connection more precise, we give... a definition of morphisms that makes CD manifolds into a category admitting a functor from the category of smoothly triangulated manifolds.  Furthermore, these morphisms have appropriate naturality properties for matroid bundles\n  and hence [combinatorially defined] characteristic classes [!!!], so many maneuvers in differential topology\n  carry over verbatim to the CD setting. This represents the first demonstration\n  that the CD category succeeds in capturing structures contained in the smooth\n  but absent in the topological and PL categories [!!!!!!!], and suggests that it might be possible to develop a purely combinatorial approach to smooth manifold topology [!!!].\n\nThe boldface and bracketed material are my annotations, with ! marks as a subjective rating of how amazing each statement would have been if true. \nThe first sentence in boldface seems to include the combinatorial construction of Pontrjagin classes, a major research problem.   That would have been a big achievement, but the paper claims to do it as part of something even bigger, as the next two sentences elaborate.\nThe last two items in boldface, doing smooth (beyond the topological and piecewise-linear categories) topology on discrete combinatorial objects,  was considered science fiction. It was not expected then or now that such a thing is possible and to do it in 20 pages must have struck many people as some kind of dream.  Nevertheless, the error was apparently subtle enough to pass the reviewers, though experts were stating their skepticism about the paper soon after it was published.   \nThe official retraction happened several years later and the author of a book published in 2006 would not necessarily have known of the problems with the paper. People were challenging the correctness of the article by 2005, and the \"the problem was already acknowledged\nand discussed in private correspondence between experts in\nApril 2006\" but during the time the book (or his contribution to it) was written, Biss may have believed that his results were correct or could be fixed.\nBiss became a politician in Illinois, and his election opponents mentioned the situation with his papers during one of the election campaigns.", "meta": {"post_id": 1955665, "input_score": 43, "output_score": 63, "post_title": "Has anyone ever actually seen this Daniel Biss paper?"}}
{"input": "Definitions of positive definiteness usually look like this:\n\nA symmetric matrix $M$ is positive definite if $x^T M x > 0$ for all vectors $x \\neq 0$.\n\nWhy must $M$ be symmetric? The definition seems to make sense for general square matrices.", "output": "Let quadratic form $f$ be defined by\n$$f (\\mathrm x) := \\mathrm x^\\top \\mathrm A \\,\\mathrm x$$\nwhere $\\mathrm A \\in \\mathbb{R}^{n \\times n}$. Since $\\mathrm x^\\top \\mathrm A \\,\\mathrm x$ is a scalar, then $\\left(\\mathrm x^\\top \\mathrm A \\,\\mathrm x\\right)^\\top = \\mathrm x^\\top \\mathrm A \\,\\mathrm x$, i.e., $\\mathrm x^\\top \\mathrm A^\\top \\mathrm x = \\mathrm x^\\top \\mathrm A \\,\\mathrm x$. Hence,\n$$\\mathrm x^\\top \\left(\\frac{\\mathrm A - \\mathrm A^\\top}{2}\\right) \\mathrm x = 0$$\nThus, the skew-symmetric part of matrix $\\mathrm A$ does not contribute anything to the quadratic form. What is left is, then, the symmetric part\n$$\\frac{\\mathrm A + \\mathrm A^\\top}{2}$$\nwhich is diagonalizable and has real eigenvalues and orthogonal eigenvectors, all nice properties.\n\nAddendum\nTaking affine combinations of $\\mathrm A$ and $\\mathrm A^\\top$, we obtain\n$$\\mathrm x^\\top \\left( \\gamma \\mathrm A + (1-\\gamma) \\mathrm A^\\top \\right) \\mathrm x = f (\\mathrm x)$$\nwhich yields $f$ for all $\\gamma \\in \\mathbb{R}$. Choosing $\\gamma = \\frac{1}{2}$, we obtain the symmetric part of $\\mathrm A$.", "meta": {"post_id": 1964039, "input_score": 22, "output_score": 34, "post_title": "Why do positive definite matrices have to be symmetric?"}}
{"input": "This is my last homework problem and I've been looking at it for a while. I cannot nail down what is wrong with this proof even though its obvious it is wrong based on its conclusion. Here it is:\n\nFind the flaw in the following bogus proof by strong induction that\n  for all $n \\in \\Bbb N$, $7n = 0$.\nLet $P(n)$ denote the statement that $7n = 0$.\nBase case: Show $P(0)$ holds.\nSince $7 \\cdot 0 = 0$, $P(0)$ holds.\nInductive step: Assume $7\u00b7j = 0$ for all natural numbers $j$ where $0 \\le j \\le k$ (induction hypothesis). Show $P(k + 1)$: $7(k + 1) = 0$.\nWrite $k + 1 = i + j$, where $i$ and $j$ are natural numbers less than $k + 1$.  Then, using the induction hypothesis, we get $7(k + 1) = 7(i + j) = 7i + 7j = 0 + 0 = 0$. So $P(k + 1)$ holds.\nTherefore by strong induction, $P(n)$ holds for all $n \\in \\Bbb N$.\n\nSo the base case is true and I would be surprised if that's where the issue is.\nThe inductive step is likely where the flaw is. I don't see anything wrong with the strong induction declaration and hypothesis though and the math adds up! I feel like its so obvious that I'm just jumping over it in my head.", "output": "As a general rule: For fake induction proofs, find the smallest case where the conclusion does not hold, and then do each step in detail with the corresponding numbers inserted, so that it should proof that exact case. That way you will almost always quickly find the problem.\nIn this case, the smallest failing case is $P(1)$, as that claims $7\\cdot 1=0$ which is clearly wrong.\nTherefore the number to look at is $k+1=1$, that is, $k=0$.\nSo let's look at the inductive step, and insert $k=0$:\n\nInductive step: Assume $7\\cdot j=0$ for all natural numbers $j$ where $0\\le j\\le 0$ (induction hypothesis). Show $P(k+1): 7(k+1)=0$.\n\nThe only number with $0\\le j\\le 0$ is $j=0$, so the induction hypothesis is that $7\\cdot 0=0$, which clearly is true.\n\nWrite $0+1=i+j$, where $i$ and $j$ are natural numbers less than $k+1$.\n\nThe only natural number less than $1$ is $0$. Therefore we have to write $0+1 = 0+0$ \u2026 oops, that's not right! Error found!", "meta": {"post_id": 1966283, "input_score": 44, "output_score": 46, "post_title": "How do I find a flaw in this false proof that $7n = 0$ for all natural numbers?"}}
{"input": "Up till now, the only things I was able to come up/prove are the following properties:\n\n$\\prod\\lambda_i = \\pm 1$\n$ 0 \\leq \\sum \\lambda_i \\leq n$, where $n$ is the size of the matrix\neigenvalues of the permutation matrix lie on the unit circle \n\nI am curious whether there exist some other interesting properties.", "output": "A permutation matrix is an orthogonal matrix (orthogonality of column vectors and norm of column vectors = 1).\nAs such, because an orthogonal matrix \"is\" an isometry\n$$\\tag{1}\\|PV\\|=\\|V\\|$$\nIf $V$ is an eigenvector associated with eigenvalue $\\lambda$, substituting $PV=\\lambda V$ in (1) we deduce\n$$|\\lambda|=1.$$\nMoreover, as $P^p=I_n$ ($p$ is the order of the permutation) these eigenvalues are such that $\\lambda^p=1$; therefore\n$$\\lambda=e^{i k 2\\pi/p}$$\nfor some $k \\in \\mathbb{Z}$.\nLet us take an example: consider the following permutation decomposed into the product of two disjoint support cycles\na cycle $\\color{red}{(5 4 3 2 1)}$ of order $5$ and a cycle $\\color{blue}{(6 7 8)}$ of order $3$.\nIts associated matrix is:\n$$\\left(\\begin{array}{ccccc|ccc}\n0 & \\color{red}{1} & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & \\color{red}{1} & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & \\color{red}{1} & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\color{red}{1} & 0 & 0 & 0\\\\\n\\color{red}{1} & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\hline\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\color{blue}{1}\\\\\n0 & 0 & 0 & 0 & 0 & \\color{blue}{1} & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\color{blue}{1} & 0\\end{array}\\right)$$\nIts cycle structure is reflected (see picture) into the five eigenvalues $\\color{red}{e^{2i k\\pi/5}}$ and the three eigenvalues $\\color{blue}{e^{2i k\\pi/3}}$.\nPlease note that eigenvalue $1$ is - in a natural way - a double eigenvalue, and more generally with multiplicity $m$ if the permutation can be decomposed into $m$ disjoint cycles.", "meta": {"post_id": 1970702, "input_score": 23, "output_score": 39, "post_title": "What are the properties of eigenvalues of permutation matrices?"}}
{"input": "Show that a nonabelian group must have at least five distinct elements.\n\nI just learn abstract algebra by self study. I want help to solve this problem.\nJust give me a hint.", "output": "You need an instance of $ab\\ne ba$. That requires $a\\ne b$. Also $a\\ne 1$ and $b\\ne 1$ as $1$ commutes. Also, $a,b$ are not inverse of each other as those commute. Hence $1, a, b, ab, ba$ are pairwise distinct", "meta": {"post_id": 1971166, "input_score": 14, "output_score": 45, "post_title": "Show that a nonabelian group must have at least five distinct elements"}}
{"input": "Pascal's triangle extends infinitely downwards, meaning in only has two sides. Why then is it called a triangle if a triangle, by definition must have three sides?", "output": "I find Pascal's original form greatly preferable. Slightly modernized:\n$$\n\\begin{array}{ll|llllllllll}\n\\label{my-label}\n  &   & j & \u2192 &    &    &     &     &    &    &   &   \\\\\n  &   & 0 & 1 & 2  & 3  & 4   & 5   & 6  & 7  & 8 & 9 \\\\ \\hline\ni & 0 & 1 & 1 & 1  & 1  & 1   & 1   & 1  & 1  & 1 & 1 \\\\\n\u2193 & 1 & 1 & 2 & 3  & 4  & 5   & 6   & 7  & 8  & 9 &   \\\\\n  & 2 & 1 & 3 & 6  & 10 & 15  & 21  & 28 & 36 &   &   \\\\\n  & 3 & 1 & 4 & 10 & 20 & 35  & 56  & 84 &    &   &   \\\\\n  & 4 & 1 & 5 & 15 & 35 & 70  & 126 &    &    &   &   \\\\\n  & 5 & 1 & 6 & 21 & 56 & 126 &     &    &    &   &   \\\\\n  & 6 & 1 & 7 & 28 & 84 &     &     &    &    &   &   \\\\\n  & 7 & 1 & 8 & 36 &    &     &     &    &    &   &   \\\\\n  & 8 & 1 & 9 &    &    &     &     &    &    &   &   \\\\\n  & 9 & 1 &   &    &    &     &     &    &    &   &  \n\\end{array}\n$$\nThis way, it's defined for all $(i, j) \\in \\mathbb{N}^2$. The identities are nicer, too:\n$$T_{i,j} = T_{j, i}$$\n$$T_{i,j} = \\frac{(i+j)!}{i!j!}$$\n$$T_{i+1,j+1} = T_{i+1,j} + T_{i,j+1}$$\nRow sum:\n$$\\sum_{i+j=n}T_{i,j} = 2^n$$\nThe \"hockey stick\":\n$$T_{i,j+1} = \\sum_{0 \\leq k \\leq i}{T_{k,j}}$$\nAnd finally, for fans of the binomial coefficient:\n$$T_{i,j} = \\binom{i+j}{i} = \\binom{i+j}{j}$$\nOf course, the higher-dimensional forms are easy too:\n$$T_{\\hat{x}} = \\frac{(\\sum_{x \\in \\hat{x}}{x})!}{\\prod_{x \\in \\hat{x}}{(x!)}}$$\n$$T_{\\hat{x}} = T_{\\sigma(\\hat{x})}$$\n$$T_{\\hat{x}} = T_{x_1 - 1, x_2, ..., x_n} + T_{x_1, x_2 - 1, ..., x_n} + ... + T_{x_1, x_2, ..., x_n-1}$$", "meta": {"post_id": 1983942, "input_score": 19, "output_score": 37, "post_title": "Why is Pascal's Triangle called a Triangle?"}}
{"input": "How to prove that every polynomial with real coefficients is the sum of three polynomials raised to the 3rd degree? Formally the statement is:\n\n$\\forall f\\in\\mathbb{R}[x]\\quad \\exists g,h,p\\in\\mathbb{R}[x]\\quad f=g^3+h^3+p^3$", "output": "We have that the following identity holds\n$$(x+1)^3+2(-x)^3+(x-1)^3=6x.$$\nHence\n$$\\left(\\frac{f(x)+1}{6^{1/3}}\\right)^{3}+\\left(\\frac{-f(x)}{3^{1/3}}\\right)^{3}+\n\\left(\\frac{f(x)-1}{6^{1/3}}\\right)^{3}=f(x).$$", "meta": {"post_id": 1991228, "input_score": 46, "output_score": 81, "post_title": "Every polynomial with real coefficients is the sum of cubes of three polynomials"}}
{"input": "The numbers $14$ and $21$ are quite interesting.\nThe prime factorisation of $14$ is $2\\cdot 7$ and the prime factorisation of $14+1$ is $3\\cdot 5$. Note that $3$ is the prime after $2$ and $5$ is the prime before $7$.\nSimilarly, the prime factorisation of $21$ is $7\\cdot 3$ and the prime factorisation of $21+1$ is $11\\cdot 2$. Again, $11$ is the prime after $7$ and $2$ is the prime before $3$.\nIn other words, they both satisfy the following definition:\n\nDefinition: A positive integer $n$ is called interesting if it has a prime factorisation $n=pq$ with $p\\ne q$ such that the prime factorisation of $n+1$ is $p'q'$ where $p'$ is the prime after $p$ and $q'$ the prime before $q$.\n\nAre there other interesting numbers?", "output": "Note that exactly one of $n$ and $n+1$ is even. It follows that for $n$ to be interesting, either $n=3p$ and $n+1=2N(p)$ or $n=2p$ and $n+1=3P(p)$, where $P(p)$ and $N(p)$ are the previous and next primes to $p$ respectively. Rearranging we get that $p$ must satisfy one of the following two equations:\n$$\\frac{3p+1}2=N(p)\\tag1$$\n$$\\frac{2p+1}3=P(p)\\tag2$$\nHowever, by a 1952 result of Jitsuro Nagura, for $p\\ge25$ there is always a prime between $p$ and $\\frac65p$. In particular, if $p\\ge31$ is a prime:\n$$\\frac56p<P(p)<p<N(p)<\\frac65p$$\nBut when $p\\ge31$ the following inequalities are also true:\n$$\\frac{2p+1}3<\\frac56p\\qquad\\frac65p<\\frac{3p+1}2$$\nTherefore, if $p$ is to satisfy $(1)$ or $(2)$ above, it must be less than 31. This leaves a handful of cases to check for $p$, and we find that the only interesting numbers are 14 and 21 as conjectured.\n\nThe Nagura paper is a reference in the Wikipedia article on Bertrand's postulate. While those in the comments had saw it, sketching out the approach I use here, I already knew what to do; I did not read those comments in detail until after posting my answer.", "meta": {"post_id": 1996120, "input_score": 204, "output_score": 283, "post_title": "Are $14$ and $21$ the only \"interesting\" numbers?"}}
{"input": "Could someone explain how that derivative was arrived at.\nAccording to me, the derivative of $\\log(\\text{softmax})$ is\n$$\n\\nabla\\log(\\text{softmax}) =\n\\begin{cases}\n1-\\text{softmax},  & \\text{if $i=j$} \\\\\n-\\text{softmax}, & \\text{if $i \\neq j$}\n\\end{cases}\n$$\nWhere did that expectation come from?\n$\\phi(s,a)$ is a vector, $\\theta$ is also a vector. $\\pi(s,a)$ denotes the probability of taking action a in state s.", "output": "The derivation of the softmax score function (aka eligibility vector) is as follows:\nFirst, note that: $$\\pi_\\theta(s,a) = softmax =\n \\frac{e^{\\phi(s,a)^\\intercal\\theta}}{\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}}$$\nThe important bit here is that the slide only identifies the proportionality, not the full softmax function which requires the normalization factor.\nContinuing the derivation:\nUsing the $\\log$ identity $\\log(x/y) = \\log(x) - \\log(y)$ we can write $$\\log(\\pi_\\theta(s,a)) = \\log(e^{\\phi(s,a)^\\intercal\\theta}) - \\log(\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}) $$\nNow take the gradient:\n$$\\nabla_\\theta\\log(\\pi_\\theta(s,a)) = \\nabla_\\theta\\log(e^{\\phi(s,a)^\\intercal\\theta}) - \\nabla_\\theta\\log(\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta})$$\nThe left term simplifies as follows:\n$$left= \\nabla_\\theta\\log(e^{\\phi(s,a)^\\intercal\\theta}) = \\nabla_\\theta\\phi(s,a)^\\intercal\\theta = \\phi(s,a)$$\nThe right term simplifies as follows:\nUsing the chain rule: $$\\nabla_x\\log(f(x)) = \\frac{\\nabla_xf(x)}{f(x)}$$\nWe can write:\n$$right = \\nabla_\\theta\\log(\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}) = \\frac{\\nabla_\\theta\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}}{\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}}$$\nTaking the gradient of the numerator we get:\n$$right = \\frac{\\sum_{k=1}^N{\\phi(s,a_k)}e^{\\phi(s,a_k)^\\intercal\\theta}}{\\sum_{k=1}^Ne^{\\phi(s,a_k)^\\intercal\\theta}}$$\nSubstituting the definition of $\\pi_\\theta(s,a)$ we can simplify to:\n$$right = \\sum_{k=1}^N{\\phi(s,a_k)}\\pi_\\theta(s,a_k)$$\nGiven the definition of Expected Value:\n$$\\mathrm{E}[X] = X \\cdot P = x_1p_1+x_2p_2+ ... +x_np_n$$\nWhich in English is just the sum of each feature times its probability.\n$$X = features = {\\phi(s,a)}$$ \n$$P = probabilities =\\pi_\\theta(s,a)$$\nSo now we can write the expected value of the features: \n$$right = \\mathrm{E}_{\\pi_\\theta}[\\phi(s,\\cdot)]$$ \nwhere $\\cdot$ means all possible actions.\nPutting it all together:\n$$\\nabla_\\theta\\log(\\pi_\\theta(s,a)) = left - right = \\phi(s,a) - \\mathrm{E}_{\\pi_\\theta}[\\phi(s,\\cdot)]$$", "meta": {"post_id": 2013050, "input_score": 26, "output_score": 48, "post_title": "Log of Softmax function Derivative."}}
{"input": "I understand that\n$Var(X) = E(X^2) - E(X)^2 $\nAnd that the second moment, variance, is\n$E(X^2)$\nHow is variance simultaneously $E(X^2)$ and $E(X^2) - E(X)^2$?", "output": "$$\n\\mathbb{E}(X^n) = \\text{raw moment}\\\\\n\\mathbb{E}\\left[\\left(X-\\mathbb{E}(X)\\right)^n\\right] = \\text{central moment}\n$$\nwhere the 2nd central moments represents the variance.\nonly equal when $\\mathbb{E}(X) = 0$ as with $\\mathcal{N}(0,1)$.", "meta": {"post_id": 2030437, "input_score": 24, "output_score": 37, "post_title": "Difference between Variance and 2nd moment"}}
{"input": "How to prove\n\n$~~ \\forall n\\in\\mathbb{N}^+$,\n\\begin{align}I_n=\\int_0^1(1+x+x^2+x^3+\\cdot\\cdot\\cdot+x^{n-1})^2 (1+4x+7x^2+\\cdot\\cdot\\cdot+(3n-2)x^{n-1})~dx=n^3.\\end{align}\n\n\nMy Try:\nDefine $\\displaystyle S(n)=\\sum_{k=0}^{n-1}x^k=1+x+x^2+x^3+\\cdot\\cdot\\cdot+x^{n-1}=\\frac{x^n-1}{x-1}$. Then,\n\\begin{align}\\frac{d}{dx}S(n)=S'(n)=1+2x+3x^2+\\cdot\\cdot\\cdot(n-1)x^{n-2}=\\sum_{k=0}^{n-1}kx^{k-1}.\\end{align}\nTherefore,\n\\begin{align}\nI_n&=\\int_0^1 S^2(n)\\left(3S'(n+1)-2S(n)\\right)~dx\\\\\n&=3\\int_0^1 S^2(n)S'(n+1)~dx-2\\int_0^1 S^3(n)~dx\\\\\n&=3\\int_0^1 S^2(n)(S'(n)+nx^{n-1})~dx-2\\int_0^1 S^3(n)~dx\\\\\n&=3\\int_0^1 S^2(n)~d(S(n))+3\\int_0^1 S^2(n)(nx^{n-1})~dx-2\\int_0^1 S^3(n)~dx\\\\ &=n^3-1+\\int_0^1 S^2(n)(3nx^{n-1}-2S(n))~dx\\\\\n&=n^3-1+\\int_0^1 \\left(\\frac{x^n-1}{x-1}\\right)^2\\left(3nx^{n-1}-2\\cdot\\frac{x^n-1}{x-1}\\right)~dx\n\\end{align}\nSo the question becomes:\n\nProve \\begin{align}I'=\\int_0^1 \\left(\\frac{x^n-1}{x-1}\\right)^2\\left(3nx^{n-1}-2\\cdot\\frac{x^n-1}{x-1}\\right)~dx=1.\\end{align}\n\n\\begin{align}I'&=\\int_0^1 \\frac{3nx^{n-1}(x^n-1)^2}{(x-1)^2}-\\frac{2(x^n-1)^3}{(x-1)^3}~dx\\\\\n&=\\int_0^1 \\frac{(x-1)^2\\left(\\frac d {dx} (x^n-1)^3\\right)-2(x^n-1)^3(x-1)}{(x-1)^4}~dx\\\\\n&=\\int_0^1 \\frac d {dx} \\left(\\frac{(x^n-1)^3}{(x-1)^2}\\right)~dx\\\\\n&=\\lim_{x \\to 1} \\frac{(x^n-1)^3}{(x-1)^2}-\\frac{(0^n-1)^3}{(0-1)^2}\\\\\n\\end{align}\n$$\\therefore I'=1.$$\n\n\\begin{align}\\therefore I_n=n^3.\\end{align}\n\nThere MUST be other BETTER ways evaluating $I_n$.\nCould anyone give me some better solutions? Thanks.", "output": "First apply the substitution $x = t^3$. Then\n\\begin{align*}\nI_n\n&= \\int_{0}^{1} (1 + t^3 + \\cdots + t^{3n-3})^2 (1 + 4t^3 + \\cdots + (3n-2)t^{3n-3}) \\cdot 3t^2 \\, dt \\\\\n&= \\int_{0}^{1} 3 (t + t^4 + \\cdots + t^{3n-2})^2 (1 + 4t^3 + \\cdots + (3n-2)t^{3n-3}) \\, dt.\n\\end{align*}\nNow let $u = u(t) = t + t^4 + \\cdots + t^{3n-2}$. Then\n$$ 3 (t + t^4 + \\cdots + t^{3n-2})^2 (1 + 4t^3 + \\cdots + (3n-2)t^{3n-3}) = 3u^2 \\frac{du}{dt}.$$\nTherefore\n$$ I_n = \\left[ u(t)^3 \\right]_{t=0}^{t=1} = u(1)^3 - u(0)^3 = n^3. $$", "meta": {"post_id": 2042986, "input_score": 21, "output_score": 49, "post_title": "An interesting definite integral $\\int_0^1(1+x+x^2+x^3+\\cdot\\cdot\\cdot+x^{n-1})^2 (1+4x+7x^2+\\cdot\\cdot\\cdot+(3n-2)x^{n-1})~dx=n^3$"}}
{"input": "A001951 A Beatty sequence: a(n) = floor(n*sqrt(2)).\nIf $n = 5$ then\n$$\\left\\lfloor1\\sqrt{2}\\right\\rfloor+ \\left\\lfloor2\\sqrt{2}\\right\\rfloor + \\left\\lfloor3\\sqrt{2}\\right\\rfloor +\\left\\lfloor4 \\sqrt{2}\\right\\rfloor+ \\left\\lfloor5\\sqrt{2}\\right\\rfloor\n= 1+2+4+5+7 = 19$$\nSequence from $1$ to $20$ is:\n$S=\\{1,2,4,5,7,8,9,11,12,14,15,16,18,19,21,22,24,25,26,28\\}$\nI want to find answer for $n = 10^{100}$.", "output": "Let $S(\\alpha,n) = \\sum_{k=1}^n \\lfloor \\alpha k \\rfloor$ for $\\alpha$ some irrationnal positive number.\nif $\\alpha \\ge 2$ we let $\\beta = \\alpha-1$ and you get\n$S(\\alpha,n) = S(\\beta,n) + \\sum_{k=1}^n k \\\\\n= S(\\beta,n) + n(n+1)/2$\nif $1 < \\alpha < 2$, there is a theorem that says if $\\beta$ satisfies $\\alpha^{-1} + \\beta^{-1} = 1$, then the sequences $\\lfloor \\alpha n \\rfloor$ and $\\lfloor \\beta n \\rfloor$ for $n \\ge 1$ partition $\\Bbb N$ (not counting $0$)\nTherefore, letting $m = \\lfloor \\alpha n \\rfloor$, $S(\\alpha,n) + S(\\beta, \\lfloor m/\\beta \\rfloor) = \\sum_{k=1}^m k = m(m+1)/2$\nAlso, $\\lfloor m/ \\beta \\rfloor = m - \\lceil m/\\alpha \\rceil = m- n = \\lfloor (\\alpha-1)n \\rfloor$.\nThen, letting $n' = \\lfloor (\\alpha-1)n \\rfloor $ you have\n$S(\\alpha,n) = (n+n')(n+n'+1)/2 - S(\\beta,n')$\nSo those two formulas give you a very fast way to compute $S$ if you can compute $n' = \\lfloor (\\alpha-1) n \\rfloor$\n\nIn your case, $\\alpha = \\sqrt 2$, so you begin in the second case where you get $\\beta = 2+\\sqrt 2$. Since the sequence of $\\alpha$s you get is periodic, you can get a recurrence formula :\nLet $n' = \\lfloor (\\sqrt 2 -1) n \\rfloor$,\n$S(\\sqrt 2,n) = (n+n')(n+n'+1)/2 - S(2+\\sqrt 2,n') \\\\\n= (n+n')(n+n'+1)/2 - S(\\sqrt 2,n') - n'(n'+1) \\\\\n = nn'+n(n+1)/2-n'(n'+1)/2 - S(\\sqrt 2,n')$\nFor example this tells you that $S(\\sqrt 2,5) = 22 - S(\\sqrt 2, 2) = 22 - 3 + S(\\sqrt 2, 0) = 19.$\n\nSince at each step $n$ is approximately multiplied by $\\sqrt 2 - 1$, the arguments decrease exponentially. For $n = 10^{100}$ you need approximately $\\lceil {100 \\log {10}/\\log ({1\\over(\\sqrt 2-1)})} \\rceil = 262$ steps to complete the recursion. This is basically equivalent to computing the powers of $(\\sqrt 2-1)$ with enough precision and should be doable quickly on any computer.", "meta": {"post_id": 2052179, "input_score": 21, "output_score": 55, "post_title": "How to find $\\sum_{i=1}^n\\left\\lfloor i\\sqrt{2}\\right\\rfloor$ A001951 A Beatty sequence: a(n) = floor(n*sqrt(2))."}}
{"input": "I am wondering why we don't learn the multi-variate chain rule in Calculus I?  I know the name implies it is more suitable for multi-variable Calculus, but after learning it, I've found it very useful.  Notably, one does not need to remember product rule or quotient rule or regular chain rule, and I don't think you would have to learn about logarithmic differentiation either.\nSo with all these advantages, why don't we teach it?", "output": "I used to think this, too, until I taught Calculus I.\nIf you, as a math student and enthusiast, like to see the product rule, etc., as special cases of the multivariate chain rule, then that is good for you and deepens your understanding.\nHowever, my experience has been that reasoning from the general to the specific doesn't always sink in to the novice learner.  If the multivariate chain rule is mumbo-jumbo, nothing derived from it is understandable either.\nThe median student in Calculus I struggles with the concept of function, has trouble working with more than two variables, and can't keep straight whether $\\frac{1}{x}$ is the derivative of $\\ln x$ or the other way around.  I'm not trying to bash Calculus I students; only to recognize that they are in a different place mathematically than we are now, or even than we were when we first learned Calculus I.  To reach them, we have to understand where their frontiers are and what is just beyond them.", "meta": {"post_id": 2055481, "input_score": 19, "output_score": 55, "post_title": "Why not learn the multi-variate chain rule in Calculus I?"}}
{"input": "This is from my textbook:  \n\nIf $A=(a_{ij})\\in M_{mn}(\\Bbb F), B=(b_{ij})\\in M_{np}(\\Bbb F)$ then $C=A\\times B=(c_{ij})\\in M_{mp}(\\Bbb F)$.\n  $c_{ij}=\\sum_{k=1}^{n} a_{ik}b_{kj}$ where $i=1,...m, j=1,...p$\n\nI know how to multiply matrices but I don't understand this notation : $c_{ij}=\\sum_{k=1}^{n} a_{ik}b_{kj}$\nCan someone explain what that represents by giving me an example? And how did we get that formula?", "output": "Visualisation might help. I'll use your notations and dimensions of the given matrices:\n\nIf $A=(a_{ij})\\in M_{mn}(\\Bbb F), B=(b_{ij})\\in M_{np}(\\Bbb F)$ then $C=A\\times B=(c_{ij})\\in M_{mp}(\\Bbb F)$.\n$c_{ij}=\\sum_{k=1}^{n} a_{ik}b_{kj}$ where $i=1,...m, j=1,...p$\n\nYou say you know how to multiply matrices, so take a look at one specific element in the product $C=AB$, namely the element on position $(i,j)$, i.e. in the $i$th row and $j$th column.\nTo obtain this element, you:\n\nfirst multiply all elements of the $i$th row of the matrix $A$ pairwise with all the elements of the $j$th column of the matrix $B$;\nand then you add these $n$ products.\n\nYou have to repeat this procedure for every element of $C$, but let's zoom in on that one specific (but arbitrary) element on position $(i,j)$ for now:\n$$\\begin{pmatrix}\na_{11} &\\ldots  &a_{1n}\\\\\n\\vdots& \\ddots &\\vdots\\\\\n\\color{blue}{\\mathbf{a_{i1}}} &\\color{blue}{\\rightarrow}  &\\color{blue}{\\mathbf{a_{in}}}\\\\\n\\vdots&  \\ddots &\\vdots\\\\\na_{m1} &\\ldots &a_{mn}\n\\end{pmatrix}\n\\cdot\n\\begin{pmatrix}\nb_{11}&\\ldots &\\color{red}{\\mathbf{b_{1j}}} &\\ldots &b_{1p}\\\\\n\\vdots& \\ddots &\\color{red}{\\downarrow} &  \\ddots  &\\vdots\\\\\nb_{n1}&\\ldots &\\color{red}{\\mathbf{b_{nj}}}&\\ldots &b_{np}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nc_{11}&\\ldots& c_{1j} &\\ldots &c_{1p}\\\\\n\\vdots&  \\ddots & & &\\vdots\\\\\nc_{i1}& & \\color{purple}{\\mathbf{c_{ij}}} & &c_{ip}\\\\\n\\vdots& &  & \\ddots &\\vdots\\\\\nc_{m1} &\\ldots& c_{mj} &\\ldots &c_{mp}\n\\end{pmatrix}$$\nwith element $\\color{purple}{\\mathbf{c_{ij}}}$ equal to:\n$$\\mathbf{\\color{purple}{c_{ij}}  =  \\color{blue}{a_{i1}} \\color{red}{b_{1j}}  + \\color{blue}{a_{i2}} \\color{red}{b_{2j}}  +  \\cdots  + \\color{blue}{a_{in}} \\color{red}{b_{nj}}}$$\nNow notice that in the sum above, the left outer index is always $i$ ($i$th row of $A$) and the right outer index is always $j$ ($j$th column of $B$). The inner indices run from $1$ to $n$ so you can introduce a summation index $k$ and write this sum compactly using summation notation:\n$$\\color{purple}{\\mathbf{c_{ij}}}=\\sum_{k=1}^{n} \\color{blue}{\\mathbf{a_{ik}}}\\color{red}{\\mathbf{b_{kj}}}$$\nThe formule above thus gives you the element on position $(i,j)$ in the product matrix $C=AB$ and therefore completely defines $C$ by letting $i=1,...,m$ and $j=1,...,p$.\n\n\nCan someone explain what that represents by giving me an example? And how did we get that formula?\n\nThe illustration above should give you an idea of the general formula, but here's a concrete example where I took $3 \\times 3$ matrices for $A$ and $B$ and focus on the element on position $(2,3)$:\n$$\\begin{pmatrix}\na_{11} & a_{12}  &a_{13}\\\\\n\\color{blue}{1} &\\color{blue}{2}  &\\color{blue}{3}\\\\\na_{31} & a_{32} &a_{33}\n\\end{pmatrix}\n\\cdot\n\\begin{pmatrix}\nb_{11}&b_{12} &\\color{red}{6}\\\\\nb_{21}&b_{22} &\\color{red}{5}\\\\\nb_{31}&b_{32} &\\color{red}{4}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nc_{11}& c_{12} &c_{13}\\\\\nc_{21}& c_{22} &\\color{purple}{\\mathbf{c_{23}}}\\\\\nc_{31}& c_{32} &c_{33}\n\\end{pmatrix}$$\nwith element $\\color{purple}{\\mathbf{c_{23}}}$ equal to:\n$$\\begin{array}{rccccccc}\n\\color{purple}{c_{23}}\n& = & \\color{blue}{a_{21}} \\color{red}{b_{13}}  &+& \\color{blue}{a_{22}} \\color{red}{b_{23}}  &+& \\color{blue}{a_{23}} \\color{red}{b_{33}}\n& = &  \\displaystyle \\sum_{k=1}^{3} \\color{blue}{a_{2k}}\\color{red}{b_{k3}} \\\\\n& = & \\color{blue}{1} \\cdot \\color{red}{6}  &+& \\color{blue}{2}  \\cdot \\color{red}{5} &+& \\color{blue}{3}  \\cdot  \\color{red}{4} \\\\[8pt]\n& = & 6&+&10&+&12 & =&  28\n\\end{array}$$", "meta": {"post_id": 2063241, "input_score": 13, "output_score": 36, "post_title": "Matrix multiplication notation"}}
{"input": "I saw this method in some random PDF and am intrigued of the exact method used. I can't find any page of this method on the web because I'm not sure what you'd call this method.\nHere is the method:\n\nFor solving\n  $$\\int \\frac{2x^4 + x^3}{x^2 + x - 2} \\,\\text{d}x$$\n  Observe that\n  $$\\begin{align*}\n2x^4 + x^3\n&= 2x^2 (x^2+x-2) - x^3+4x^2 \\\\\n&= 2x^2 (x^2+x-2) - x(x^2+x-2) + 5x^2-2x \\\\\n&= 2x^2 (x^2+x-2) - x(x^2+x-2) + 5(x^2+x-2) - 7x+10 \\\\\n&= (2x^2-x+5)(x^2+x-2) - 7x+10\n\\end{align*}$$\n  and then\n  $$\n\\int \\frac{2x^4-x^3}{x^2+x-2} \\,\\text{d}x\n= \\int (2x^2-x+5)\\,\\text{d}x\n+ \\int \\frac{-7x+10}{x^2+x-2}\\,\\text{d}x$$", "output": "What you have is literally polynomial long division written out without division signs.  Indeed, what is written out here is the essence of polynomial long division, which is all about finding the coefficient of the factor that returns the highest degree term in the original polynomial.", "meta": {"post_id": 2099461, "input_score": 14, "output_score": 40, "post_title": "What's the name of the following method for dividing polynomials? It's not long-division nor synthetic division"}}
{"input": "How to prove that the $p$-adic units can be written as\n$$\\mathbb{Z}_p^\\times \\cong \\mu_{p-1}\\times(1 + p\\mathbb{Z}_p) \\cong \\mathbb{Z}/(p-1)\\mathbb{Z}\\times\\mathbb{Z}_p$$\nwhere $\\mu_n$ is the $n$-th roots of unity in $\\mathbb{Z}_p$?\nHere $p>2$ is a prime number.\nAny hint or link would be helpful.\nBesides, it looks strange: why the units of $\\mathbb{Z}_p$ is isomorphic to some guy which looks bigger than $\\mathbb{Z}_p$?", "output": "This must be covered in almost every text on the $p$-adic numbers; I think the book of Gouv\u00eaa is the best of these.\nAnd your statement is not quite true: for $p=2$, $\\mu_{p-1}$ is trivial all right, but the other part is isomorphic to $\\{\\pm1\\}\\times(1+4\\Bbb Z_2)$. My tale below omits the story for $p=2$, and you can fill this in yourself.\nFirst, you can consider the units, $\\Bbb Z_p^\\times$, and reduce them modulo $p$ to the multiplicative group of $\\Bbb F_p\\cong\\Bbb Z/p\\Bbb Z$. It\u2019s cyclic of order $p-1$, as I\u2019m sure you know. So we have an exact sequence:\n$$\n0\\longrightarrow K\\longrightarrow\\Bbb Z_p^\\times\\longrightarrow\\Bbb F_p^\\times\\longrightarrow0\\,;\n$$\nif you\u2019re unfamiliar with the notation of exact sequences this merely says that there\u2019s a surjective map from the middle term to the one to its right, with kernel equal to the one to its left.\nWhat\u2019s the kernel? It\u2019s the units that go to $1$ in the field $\\Bbb F_p$, in other words $1+p\\Bbb Z_p$. To prove that $\\Bbb Z_p^\\times$ is the direct product of the two things to either side of it, it\u2019s enough to show that there\u2019s a homomorphism from $\\Bbb F_p^\\times$ into it whose image hits $K$ only in the identity. This is the fun part:\nYou can find $(p-1)$-th roots of unity in $\\Bbb Z_p$ either by a routine application of any version of Hensel\u2019s Lemma that you like, or, my favorite method, take an element of $\\Bbb F_p^\\times$, lift it to any element of $\\Bbb Z_p$ that goes to it modulo $p$, and take successive $p$-th powers: $x\\mapsto x^p\\mapsto(x^p)^p\\mapsto\\cdots$ etc. I\u2019ll leave it to you to show that this is a good convergent sequence, and its limit is clearly a suitable root of unity.\nShowing that the multiplicative group $1+p\\Bbb Z_p$ is isomorphic to the additive group $\\Bbb Z_p^+$ is rather less fun. To tell you the truth, when I first saw the logarithmic argument, I didn\u2019t like it, but I now think it\u2019s the best one. You have to convince yourself that as long as the series for $\\log(1+x)$ that you saw in Calculus is convergent, then $\\log\\bigl[(1+x)(1+y)\\bigr]=\\log(1+x)+\\log(1+y)$, and for $x\\in p\\Bbb Z_p$, the series is convergent. And the values of the log are all in $p\\Bbb Z_p\\cong\\Bbb Z_p$ (as additive groups, of course), and fill out that group.", "meta": {"post_id": 2099514, "input_score": 15, "output_score": 38, "post_title": "Units of p-adic integers"}}
{"input": "My headphone cables formed this knot:\n\nhowever I don't know much about knot theory and cannot tell what it is. In my opinion it isn't a figure-eight knot and certainly not a trefoil. Since it has $6$ crossings that doesn't leave many other candidates!\nWhat is this knot? How could one figure it out for similarly simple knots?", "output": "Arthur's answer is completely correct, but for the record I thought I would give a general answer for solving problems of this type using the SnapPy software package.  The following procedure can be used to recognize almost any prime knot with a small number of crossings, and takes about 10-15 minutes for a new user.\nStep 1. Download and install the SnapPy software from the SnapPy installation page.  This is very quick and easy, and works in Mac OS X, Windows, or Linux.\nStep 2. Open the software and type:\nM = Manifold()\n\nto start the link editor.  (Here \"manifold\" refers to the knot complement.)\nStep 3.  Draw the shape of the knot.  Don't worry about crossings to start with: just draw a closed polygonal curve that traces the shape of the knot.  Here is the shape that I traced:\n\nIf you make a mistake, choose \"Clear\" from the Tools menu to start over.\nStep 4.  After you draw the shape of the knot, you can click on the crossings with your mouse to change which strand is on top.  Here is my version of the OP's knot:\n\nStep 5. Go to the \"Tools\" menu and select \"Send to SnapPy\".  My SnapPy shell now looks like this:\n\nStep 6. Type\nM.identify()\n\nThe software will give you various descriptions of the manifold, one of which will identify the prime knot using Alexander-Briggs notation.  In this case, the output is\n[5_1(0,0), K5a2(0,0)]\n\nand the first entry means that it's the $5_1$ knot.", "meta": {"post_id": 2113877, "input_score": 57, "output_score": 82, "post_title": "What knot is this?"}}
{"input": "What is the difference between the Taylor and the Maclaurin series? Is the series representing sine the same both ways? Can someone describe an example for both?", "output": "A Taylor series centered at $x=x_0$ is given as follows:\n$$f(x)=\\sum_{n=0}^\\infty\\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n$$\nwhile a Maclaurin series is the special case of being centered at $x=0$:\n$$f(x)=\\sum_{n=0}^\\infty\\frac{f^{(n)}(0)}{n!}x^n$$\nYou may find this very similar to a power series, which is of the form\n$$f(x)=\\sum_{n=0}^\\infty a_n(x-x_0)^n$$\nParticularly where $a_n=\\frac{f^{(n)}(x_0)}{n!}$.  If a function is equal to it's Taylor series locally, it is said to be an analytic function, and it has a lot of interesting properties.  However, not all functions are equal to their Taylor series, if a Taylor series exists.\nOne may note that most of the most famous Taylor series are a Maclaurin series, probably since they look nicer.  For example,\n$$\\sin(x)=\\sum_{n=0}^\\infty\\frac{(-1)^nx^{2n+1}}{(2n+1)!}$$\nor,\n$$\\sin(x)=\\sum_{n=0}^\\infty\\frac{(-1)^n(x-2\\pi)^{2n+1}}{(2n+1)!}$$\nWhich is trivially due to the fact that $\\sin$ is a periodic function.  So, if you had to choose, you'd probably choose the first representation.  Just a convention.\nThe geometric series is a rather beautifully known Maclaurin series, which one may derive algebraically without taking derivatives:\n$$\\frac1{1-x}=\\sum_{n=0}^\\infty x^n=1+x+x^2+x^3+\\dots$$\nHowever, it gets a little bit more involved when you try to take the Taylor series at a different point.", "meta": {"post_id": 2121695, "input_score": 30, "output_score": 39, "post_title": "What is the difference between the Taylor and Maclaurin series?"}}
{"input": "Let's define a sequence of numbers between 0 and 1. The first term, $r_1$ will be chosen uniformly randomly from $(0, 1)$, but now we iterate this process choosing $r_2$ from $(0, r_1)$, and so on, so $r_3\\in(0, r_2)$, $r_4\\in(0, r_3)$... The set of all possible sequences generated this way contains the sequence of the reciprocals of all natural numbers, which sum diverges; but it also contains all geometric sequences in which all terms are less than 1, and they all have convergent sums. The question is: does $\\sum_{n=1}^{\\infty} r_n$ converge in general? (I think this is called almost sure convergence?) If so, what is the distribution of the limits of all convergent series from this family?", "output": "The probability $f(x)$ that the result is $\\in(x,x+dx)$ is given by $$f(x) = \\exp(-\\gamma)\\rho(x)$$ where $\\rho$ is the Dickman function as @Hurkyl pointed out below. This follows from the the delay differential equation for $f$, $$f^\\prime(x) = -\\frac{f(x-1)}{x}$$ with the conditions $$f(x) = f(1) \\;\\rm{for}\\; 0\\le x \\le1 \\;\\rm{and}$$ $$\\int\\limits_0^\\infty f(x) = 1.$$ Derivation follows\n\nFrom the other answers, it looks like the probability is flat for the results less than 1. Let us prove this first.\nDefine $P(x,y)$ to be the probability that the final result lies in $(x,x+dx)$ if the first random number is chosen from the range $[0,y]$. What we want to find is $f(x) = P(x,1)$.\nNote that if the random range is changed to $[0,ay]$ the probability distribution gets stretched horizontally by $a$ (which means it has to compress vertically by $a$ as well). Hence $$P(x,y) = aP(ax,ay).$$\nWe will use this to find $f(x)$ for $x<1$.\nNote that if the first number chosen is greater than x we can never get a sum less than or equal to x. Hence $f(x)$ is equal to the probability that the first number chosen is less than or equal to $x$ multiplied by the probability for the random range $[0,x]$. That is, $$f(x) = P(x,1) = p(r_1<x)P(x,x)$$\nBut $p(r_1<x)$ is just $x$ and $P(x,x) = \\frac{1}{x}P(1,1)$ as found above. Hence $$f(x) = f(1).$$\nThe probability that the result is $x$ is constant for $x<1$.\nUsing this, we can now iteratively build up the probabilities for $x>1$ in terms of $f(1)$.\nFirst, note that when $x>1$ we have $$f(x) = P(x,1) = \\int\\limits_0^1 P(x-z,z) dz$$\nWe apply the compression again to obtain $$f(x) = \\int\\limits_0^1 \\frac{1}{z} f(\\frac{x}{z}-1) dz$$\nSetting $\\frac{x}{z}-1=t$, we get $$f(x) = \\int\\limits_{x-1}^\\infty \\frac{f(t)}{t+1} dt$$\nThis gives us the differential equation $$\\frac{df(x)}{dx} = -\\frac{f(x-1)}{x}$$\nSince we know that $f(x)$ is a constant for $x<1$, this is enough to solve the differential equation numerically for $x>1$, modulo the constant (which can be retrieved by integration in the end). Unfortunately, the solution is essentially piecewise from $n$ to $n+1$ and it is impossible to find a single function that works everywhere.\nFor example when $x\\in[1,2]$, $$f(x) = f(1) \\left[1-\\log(x)\\right]$$\nBut the expression gets really ugly even for $x \\in[2,3]$, requiring the logarithmic integral function $\\rm{Li}$.\nFinally, as a sanity check, let us compare the random simulation results with $f(x)$ found using numerical integration. The probabilities have been normalised so that $f(0) = 1$.\n\nThe match is near perfect. In particular, note how the analytical formula matches the numerical one exactly in the range $[1,2]$.\nThough we don't have a general analytic expression for $f(x)$, the differential equation can be used to show that the expectation value of $x$ is 1.\nFinally, note that the delay differential equation above is the same as that of the Dickman function $\\rho(x)$ and hence $f(x) = c \\rho(x)$. Its properties have been studied. For example the Laplace transform of the Dickman function is given by $$\\mathcal L \\rho(s) = \\exp\\left[\\gamma-\\rm{Ein}(s)\\right].$$\nThis gives $$\\int_0^\\infty \\rho(x) dx = \\exp(\\gamma).$$ Since we want $\\int_0^\\infty f(x) dx = 1,$ we obtain $$f(1) = \\exp(-\\gamma) \\rho(1) = \\exp(-\\gamma) \\approx 0.56145\\ldots$$ That is, $$f(x) = \\exp(-\\gamma) \\rho(x).$$\nThis completes the description of $f$.", "meta": {"post_id": 2130264, "input_score": 159, "output_score": 51, "post_title": "Sum of random decreasing numbers between 0 and 1: does it converge??"}}
{"input": "I've seen there are few questions similar, but I haven't seen anyone so precise or with a good answer. \nI'd like to understand the reason why we ask in the definition of a manifold the existence of a countable basis. Does anybody has an example of what can go wrong with an uncountable basis? When does the problem arise? Does it arise when we want to differentiate something or does it arise before?\nThank You", "output": "There is one point that is mentioned in passing in Moishe Cohen's nice answer that deserves a bit of elaboration, which is that a lot of the time it is not important for a manifold to have a countable basis.  Rather, what is important in most applications is for a manifold to be paracompact: this is what gives you partitions of unity, which are essential to an enormous amount of the theory of manifolds (for instance, as the other answer mentioned, proving that any manifold admits a Riemannian metric).\nParacompactness follows from second-countability, which is the main reason why second-countability is useful.  Paracompactness is weaker than second-countability (for instance, an uncountable discrete space is paracompact), but it turns out that it isn't weaker by much: a (Hausdorff) manifold is paracompact iff each of its connected components is second-countable.  To put it another way, a general paracompact manifold is just a disjoint union of (possibly uncountably many) second-countable manifolds.  So if you care mainly about connected manifolds (or even just manifolds with only countably many connected components), you lose no important generality by assuming second-countability rather than paracompactness.\nThere are also a few situations where it really is convenient to assume second-countability and not just paracompactness.  For instance, in the theory of Lie groups, it is convenient to be able to define a (not necessarily closed) Lie subgroup of a Lie group $G$ as a Lie group $H$ together with a smooth injective homomorphism $H\\to G$.  If you allowed your Lie groups to not be second-countable, you would have the awkward and unwanted example that $\\mathbb{R}$ as a discrete space is a Lie subgroup of $\\mathbb{R}$ with the usual $1$-dimensional smooth structure (via the identity map).  For instance, this example violates the theorem (true if you require second-countability) that a subgroup whose image is closed is actually an embedded submanifold.", "meta": {"post_id": 2131530, "input_score": 25, "output_score": 36, "post_title": "Why is important for a manifold to have countable basis?"}}
{"input": "I'm having trouble with whether Rudin actually proves what he's tried to prove.\nProposition 1.14; (page 6)\nThe axioms of addition imply the following statements:\na) if $x + y = x + z$ then $y = z$\nThe author's proof is as follows:\n$ y = (0 + y) = (x + -x) + y = -x + (x + \\textbf{y})$\n$$ = -x + (x + \\textbf{z}) = (-x + x) + z = (0 + z) = z $$\nI emphased the section which troubles me. \nHow does Rudin prove that $ y = z $ if he substituted $y = z$?", "output": "He didn't substitute $z$ for $y$; rather, he substituted $x+z$ for $x+y$. This is legitimate based on the assumption that $x+y = x+z$.", "meta": {"post_id": 2131633, "input_score": 20, "output_score": 53, "post_title": "Problems understanding proof of if $x + y = x + z$ then $y = z$ (Baby Rudin, Chapter 1, Proposition 1.14)"}}
{"input": "The number $$\\sqrt{308642}$$ has a crazy decimal representation : $$555.5555777777773333333511111102222222719999970133335210666544640008\\cdots $$\n\nIs there any mathematical reason for so many repetitions of the digits ?\n\nA long block containing only a single digit would be easier to understand. This could mean that there are extremely good rational approximations. But here we have many long one-digit-blocks , some consecutive, some interrupted by a few digits. I did not calculate the probability of such a \"digit-repitition-show\", but I think it is extremely small.\nDoes anyone have an explanation ?", "output": "The architect's answer, while explaining the absolutely crucial fact that $$\\sqrt{308642}\\approx 5000/9=555.555\\ldots,$$ didn't quite make it clear why we get several runs of repeating decimals. I try to shed additional light to that using a different tool.\nI want to emphasize the role of the binomial series. In particular the Taylor expansion\n$$\n\\sqrt{1+x}=1+\\frac x2-\\frac{x^2}8+\\frac{x^3}{16}-\\frac{5x^4}{128}+\\frac{7x^5}{256}-\\frac{21x^6}{1024}+\\cdots\n$$\nIf we plug in $x=2/(5000)^2=8\\cdot10^{-8}$, we get\n$$\nM:=\\sqrt{1+8\\cdot10^{-8}}=1+4\\cdot10^{-8}-8\\cdot10^{-16}+32\\cdot10^{-24}-160\\cdot10^{-32}+\\cdots.\n$$\nTherefore\n$$\n\\begin{aligned}\n\\sqrt{308462}&=\\frac{5000}9M=\\frac{5000}9+\\frac{20000}9\\cdot10^{-8}-\\frac{40000}9\\cdot10^{-16}+\\frac{160000}9\\cdot10^{-24}+\\cdots\\\\\n&=\\frac{5}9\\cdot10^3+\\frac29\\cdot10^{-4}-\\frac49\\cdot10^{-12}+\\frac{16}9\\cdot10^{-20}+\\cdots.\n\\end{aligned}\n$$\nThis explains both the runs, their starting points, as well as the origin and location of those extra digits not part of any run. For example, the run of $5+2=7$s begins when the first two terms of the above series are \"active\". When the third term joins in, we need to subtract a $4$ and a run of $3$s ensues et cetera.", "meta": {"post_id": 2134903, "input_score": 144, "output_score": 163, "post_title": "Is there any mathematical reason for this \"digit-repetition-show\"?"}}
{"input": "It is known that the converse of Lagrange's Theorem isn't true in general. More precisely it is known that the following proposition:\n\nIf $G$ is a finite group of order $n$ and $m\\mid n$ then there exists a subgroup $H$ of $G$ such that $\\operatorname{order}(H)=m$.\n\nisn't true for all finite groups $G$.\nMy questions are:\n\n\nFor which groups $G$ does the converse of Lagrange's Theorem (as stated above) hold? More precisely, if $G$ is a group for which the converse of Lagrange's Theorem as I mentioned above holds then what properties must $G$ satisfy?\n\nIf there is no complete classification of such $G$s then can someone give me references to works by other mathematicians where they try to give at least a partial classification of these $G$s?\n\n\n\nPlease note that I am not interested in knowing a complete classification of the groups for which a partial converse holds (Sylow's Theorems does the job in some sense). I want to know a complete classification of the groups for which the converse of Lagrange's Theorem as I mentioned above holds.", "output": "Such groups are called Lagrangian, or CLT-groups. They have been studied often in the literature. There is no complete classification, but many interesting criteria. Two (out of many) references are the following:\n\nH. G. Bray: A note on CLT groups, Pacific Journal of Mathematics 27 (1968), no. 2., 229-231.\nF. Barry, D. MacHale, A. N. She: Some Supersolvability conditions\nfor finite groups., Math. Proceedings of the Royal Irish Academy 167 (1996), 163--177.\n\n\nDefinition: A finite group $G$ is called Lagrangian if and only if for each positive divisor $d$ of $|G|$ there exists at least one subgroup $H\\le G$ with $|H|=d$.\n\nIt is easy to see that every Lagrangian group is solvable, and conversely every supersolvable group is Lagrangian. The inclusions are strict. In fact, every group $G=A_4\\times H$ with a group\n$H$ of odd order is solvable, but not Lagrangian; and for any Lagrangian group $G$, the group $(A_4\\times C_2)\\times G$ is Lagrangian, but not supersolvable. \nThe classical counterexample to Lagrange's Theorem is $A_4$.\nFor example, no group $S_n$ or $A_n$ with $n\\ge 5$ is Lagrangian. This follows from the fact that $A_n$ and $S_n$ are not solvable for $n\\ge 5$. There are some more interesting facts, which can be easily found in the literature. For example, we have:\n\nProposition: If $(G:Z(G))<12$ for the index, then $G$ is supersolvable, hence Lagrangian.\n\nThe group $A_4$ shows that the above result is best possible. We have $(A_4:Z(A_4))=12$.\nIn the paper of Barry et al. the following result is shown:\n\nProposition: If $|[G,G]|<4$, then $G$ is supersolvable, hence Lagrangian.\n\nAgain $A_4$ shows that this result is best possible.\n\nProposition: If  $|G|$ is odd and $|[G,G]|<25$, then $G$ is supersolvable, hence Lagrangian.\n\nIn fact, $[G_{75},G_{75}]\\simeq C_5\\times C_5$ has order $25$, so that\nthis result is best possible. Here $G_{75}$ denotes the unique non-abelian group of order $75$.\nDenote the number of different conjugacy classes of $G$ by $k(G)$.\n\nProposition: If $\\frac{k(G)}{|G|}>\\frac{1}{3}$, then $G$ is supersolvable, hence Lagrangian.\n\nBecause of $\\frac{k(A_4)}{|A_4|}=\\frac{1}{3}$ the result is best possible.\nIt means that if the average size of a conjugacy class of $G$ is less than $3$,\nthen $G$ is Lagrangian.\n\nProposition: If  $|G|$ is odd and $\\frac{k(G)}{|G|}>\\frac{11}{75}$, then $G$ is supersolvable, hence Lagrangian.\n\nIn fact, $\\frac{k(G_{75})}{|G_{75}|}=\\frac{11}{75}$, so that the result is\nbest possible. \nFinally, let us mention a result of Pinnock ($1998$), which is related to Burnside's $p^aq^b$-theorem on the solvability of groups of such order.\n\nProposition: Let $G$ be a group of order $pq^b$ with primes $p,q$ satisfying $q\\equiv 1 \\bmod p$. Then $G$ is supersolvable, hence Lagrangian.", "meta": {"post_id": 2144077, "input_score": 34, "output_score": 44, "post_title": "Complete classification of the groups for which converse of Lagrange's Theorem holds"}}
{"input": "We are allowed to use a calculator in our linear algebra exam. Luckily, my calculator can also do matrix calculations.\nLet's say there is a task like this:\n\nCalculate the rank of this matrix:\n$$M =\\begin{pmatrix} 5 & 6 & 7\\\\  12 &4  &9 \\\\  1 & 7 & 4\n\\end{pmatrix}$$\n\nThe problem with this matrix is we cannot use the trick with multiples, we cannot see multiples on first glance and thus cannot say whether the vectors rows / columns are linearly in/dependent.\nUsing Gauss is also very time consuming (especially in case we don't get a zero line and keep trying harder).\nEnough said, I took my calculator because we are allowed to use it and it gives me following results:\n$$M =\\begin{pmatrix} 1 & 0{,}3333 & 0{,}75\\\\  0 &1  &0{,}75 \\\\  0 & 0 & 1\n\\end{pmatrix}$$\nI quickly see that $\\text{rank(M)} = 3$ since there is no row full of zeroes.\nNow my question is, how can I convince the teacher that I calculated it? If the task says \"calculate\" and I just write down the result, I don't think I will get all the points. What would you do?\nAnd please give me some advice, this is really time consuming in an exam.", "output": "There is a very nice trick for showing that such matrix has full rank, it can be performed in a few seconds without any calculator or worrying \"moral bending\". The entries of $M$ are integers, so the determinant of $M$ is an integer, and $\\det M\\mod{2} = \\det(M\\mod{2})$. Since $M\\pmod{2}$ has the following structure\n$$ \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 0 & 1 \\\\ 1 & 1 & 0\\end{pmatrix} $$\nit is trivial that $\\det M$ is an odd integer. In particular, $\\det M\\neq 0$ and $\\text{rank}(M)=3$.", "meta": {"post_id": 2156863, "input_score": 42, "output_score": 162, "post_title": "How to efficiently use a calculator in a linear algebra exam, if allowed"}}
{"input": "Is there any rational number $r$ such that ln (r) is rational as well? \nIf so, what's the proof?\nIf proofs are too lengthy to be cointained as an answer here, I would truly appreciated any easy-to-understand references to study them.", "output": "Aside from $r=1$, no.  To prove it, suppose we had an example.  Then we'd write $$\\frac mn=e^{\\frac ab}\\implies e^a=\\left( \\frac mn \\right)^b$$  But, with $a\\neq 0$ this would tell us that $e$ was algebraic, which is not the case.", "meta": {"post_id": 2163085, "input_score": 22, "output_score": 57, "post_title": "On irrationality of natural logarithm"}}
{"input": "I don't get this mathematical joke - can someone explain?\n\nFrom Wikipedia:\nA physicist, a biologist and a mathematician are sitting in a street caf\u00e9 watching people entering and leaving the house on the other side of the street. First they see two people entering the house. Time passes. After a while they notice three people leaving the house. The physicist says, \"The measurement wasn't accurate.\" The biologist says, \"They must have reproduced.\" The mathematician says, \"If one more person enters the house then it will be empty.", "output": "I think the point of the punchline is that the mathematician simply solves the math problem as observed, and appears totally unconcerned with the impossibility of having -1 people in a house in the real world.  \nSciences like physics and biology are about explaining the real world, but in mathematics explaining the real world is not a requirement.", "meta": {"post_id": 2168558, "input_score": 36, "output_score": 38, "post_title": "Mathematical joke regarding three people leaving a house"}}
{"input": "This is Exercise 2.6.5 of F. M. Goodman's \"Algebra: Abstract and Concrete\". I want to check my proof.\n\nExercise 2.6.5: Show that a subgroup (of a group) is normal if and only if it is the union of conjugacy classes.\n\nMy Attempt:\nLet $N$ be a subgroup of a group $G$. Then\n$$\\begin{align}\nN\\text{ is normal }&\\Leftrightarrow \\forall g\\in G, N=gNg^{-1} \\\\\n&\\Leftrightarrow \\forall n\\in N \\forall g\\in G\\exists m_{n, g}\\in N, n=gm_{n, g}g^{-1} \\tag{1}\\\\\n&\\Leftrightarrow N=\\bigcup_{n\\in N}\\underbrace{\\bigcup_{g\\in G}\\left\\{gm_{n, g}g^{-1}\\right\\}}_{\\text{conjugacy class of }n}\\tag{2} \\\\\n&\\Leftrightarrow N=\\bigcup_{n\\in N}[n],\n\\end{align}$$ where $[n]$ is the conjugacy class of $n$.$\\square$\n\nIs this proof valid?\n\nThoughts:\nI hope to make $(1)$ to $(2)$ (and back) more explicit.\nPlease help :)", "output": "If $N$ is a normal subgroup of group $G$ and $n\\in N$ then $gng^{-1}\\in N$ for every $g\\in G$ or equivalently $[n]\\subseteq N$ where $[n]:=\\{gng^{-1}\\mid g\\in G\\}$ is the conjugacy class of $n$. \nThis tells us that: $$N=\\bigcup_{n\\in N}[n]$$\nIf conversely $N$ is a subgroup of group $G$ that satisfies $N=\\bigcup_{n\\in N}[n]$ then it is immediate that $gng^{-1}\\in N$ for every $n\\in N$ and $g\\in G$, so the conclusion that $N$ is a normal subgroup is justified.", "meta": {"post_id": 2175828, "input_score": 23, "output_score": 40, "post_title": "A normal subgroup is the union of conjugacy classes."}}
{"input": "Take a look at this symbol: \n$$ \\pi=3 + \\underset{k=1}{\\overset{\\infty}{\\large{\\mathrm K}}} \\frac{(2k-1)^2} 6 $$\n\nDoes it look familiar to you? If so please help me!", "output": "It is the notation for a continued fraction.\nIn general:\n$$b_0 + \\underset{k=1}{\\overset{\\infty}{\\large{\\mathrm K}}} \\left(\\frac{a_k}{b_k}\\right)=b_0 + \\cfrac{a_1}{b_1 + \\cfrac{a_2}{b_2 + \\cfrac{a_3}{b_3 + \\cfrac{a_4}{b_4 + \\ddots\\,}}}}$$\nTherefore, the continued fraction representation you have written above for $\\pi$ is:\n$$\\pi=3 + \\underset{k=1}{\\overset{\\infty}{\\large{\\mathrm K}}} \\frac{(2k-1)^2}{6}=3 + \\cfrac{1^2}{6 + \\cfrac{3^2}{6 + \\cfrac{5^2}{6 + \\cfrac{7^2}{6 + \\ddots\\,}}}}$$\nA proof of this result can be found on pages 399-401 of this document by Paul Loya.", "meta": {"post_id": 2179834, "input_score": 42, "output_score": 55, "post_title": "Weird large K symbol"}}
{"input": "So my prof gave me this proof:\n$f(x) = f(y) \u21d0\u21d2 f(y \u2212 x) = 0 \u21d0\u21d2 y \u2212 x \u2208 Ker f.$\nI dont see why this proof is enough, this only says $y-x \\in Ker f$", "output": "First suppose $f$ is injective.\n\nSince $f$ is linear, $f(0) = 0$, hence $0 \\in \\text{ker}(f)$.\n\nBut if $x$ is any element of $\\text{ker}(f)$, then\n\\begin{align*}\n&x \\in \\text{ker}(f)&&\\\\[4pt]\n\\implies\\; &f(x) = 0&&\\\\[4pt]\n\\implies\\; &f(x) = f(0)&&\\text{[since $f(0) = 0$]}\\\\[4pt]\n\\implies\\; &x = 0&&\\text{[since $f$ is injective]}\\\\[4pt]\n\\end{align*}\nIt follows that $\\text{ker}(f) = \\{0\\}$.\nThus, $f$ injective implies $\\text{ker}(f) = \\{0\\}$.\nNext, suppose $\\text{ker}(f) = \\{0\\}$. Then\n\\begin{align*}\n&f(x)=f(y)&&\\\\[4pt]\n\\implies\\; &f(x)-f(y) = 0&&\\\\[4pt]\n\\implies\\; &f(x-y) = 0&&\\text{[since $f$ is linear]}\\\\[4pt]\n\\implies\\; &x-y \\in  \\text{ker}(f)&&\\\\[4pt]\n\\implies\\; &x-y = 0&&\\text{[since $\\text{ker}(f) = \\{0\\}$]}\\\\[4pt]\n\\implies\\; &x=y&&\\\\[4pt]\n\\end{align*}\nhence $f$ is injective.\n\nThus, $\\text{ker}(f) = \\{0\\}$ implies $f$ is injective.\n\nHence, $f$ is injective $\\iff \\text{ker}(f) = \\{0\\}$, as was to be shown.", "meta": {"post_id": 2193333, "input_score": 15, "output_score": 54, "post_title": "Showing a linear map is injective if and only if kernel is {$ {0} $}"}}
{"input": "Following my question on Meta, I post this as a new question; it was originally asked by Gmgfg, and closed due to the lack of context, background, and shown  effort.\n\nLet $f\\colon[a,b]\\to\\mathbb{R}$ be a function continuous on $[a,b]$ and differentiable on $(a,b)$ with $f(a)=f(b)=0$. Show that there exists $c\\in(a,b)$ such that $f'(c)+f(c)=f(c)f'(c)$.\n\nNow, given the assumptions this looks like it should be a straightforward application of Rolle's theorem. However:\n\nas far as I can tell there is no simple auxiliary function $\\Phi$ such that $\\Phi'= f+f'-f\\cdot f'$ to which one could apply Rolle's theorem. (If there is one, I failed to find it.)\nindeed, while the RHS could come from $\\left(\\frac{f^2}{2}\\right)'$; it's mostly the LHS which looks difficult to handle (and there is no clear advantage I can see in introducing an antiderivative $F$ of $f$ to have it be $(F+f)'$).\n\nI verified the statement for some functions I could think of, such as $x\\mapsto x(1-x)$ and $x\\mapsto \\sin \\pi x$ on $[a,b]=[0,1]$. So, in that regard, it seems to hold at least against basic sanity checks.\nHowever, what bothers me is the lack of \"homogeneity.\" Usually, in things like that I'm used to saying that \"without loss of generality, one can assume $[a,b]=[0,1]$.\" It does not appear to be the case here: if, given $f$, one defines $g\\colon[0,1]\\to\\mathbb{R}$ by $g(x) = f((b-a)x+a)$, finding $c\\in(0,1)$ such that $g'(c)+g(c)=g(c)g'(c)$ does not directly yield $c'$ such that $f'(c')+f(c')=f(c')f'(c')$ (but rather would give $c'$ such that $f'(c')+(b-a)f(c')=f(c')f'(c')$, if I'm not mistaken).\n\nSo, in short: is it true? And how to prove or disprove it \u2014 I'm at a loss, and embarrassed about it.", "output": "Let\n$$ g(x) = f(x)e^{x-f(x)}. $$\nThen $g(a) = g(b) = 0$ since $f(a)=f(b)=0$, so $g'(c) = 0$ for some $c\\in(a,b)$. However,\n$$ g'(x) = f'(x)e^{x-f(x)} + f(x)e^{x-f(x)}(1-f'(x)) = e^{x-f(x)}(f'(x)+f(x)-f(x)f'(x)),$$\nso $g'(c) = 0\\implies f'(c)+f(c)-f(c)f'(c) = 0$, i.e. $f'(c)+f(c) = f(c)f'(c)$.", "meta": {"post_id": 2211094, "input_score": 37, "output_score": 38, "post_title": "Application of Rolle's theorem? Establish existence of $c\\in(a,b)$ such that $f(c)+f'(c)=f(c)f'(c)$"}}
{"input": "Each number occurs at least twice as $a={a\\choose1}={a\\choose a-1}$. If a number occurs somewhere else in the triangle (most likely twice, if it's not of the form ${2a\\choose a}$) then that number occurs $4$ times. After that, it becomes interesting. I found the following with a simple script:\n$$120={120\\choose1}={16\\choose 2}={10\\choose3}={10\\choose7}={16\\choose 14}={120\\choose119}$$\n$$210={210\\choose1}={21\\choose 2}={10\\choose4}={10\\choose6}={21\\choose 19}={210\\choose209}$$\n$$1540={1540\\choose1}={56\\choose 2}={22\\choose3}={22\\choose19}={56\\choose 54}={1540\\choose1539}$$\n$$7140={7140\\choose1}={120\\choose 2}={36\\choose3}={36\\choose33}={120\\choose 118}={7140\\choose7139}$$\nAnd a special one, that did not only occur six times, but eight:\n$$3003={3003\\choose1}={78\\choose 2}={15\\choose5}={14\\choose6}={14\\choose8}={15\\choose10}={78\\choose 76}={3003\\choose3002}$$\nI only checked the numbers up to $10000$; so here's my question\n\nBesides $1$, are there other numbers that occur infinitely often? Is there an upper bound known to how many times a number can occur in Pascal's Triangle?", "output": "Singmaster's conjecture (Wikipedia article) is that there is a finite upper bound on the multiplicities of entries in Pascal's triangle (other than 1). The article has all of the latest developments in this direction, and as explained in the article,\n\nIt is clear that the only number that appears infinitely many times in Pascal's triangle is $1$, because any other number $x$ can appear only within the first $x + 1$ rows of the triangle.", "meta": {"post_id": 2214853, "input_score": 42, "output_score": 51, "post_title": "How often can a number occur in Pascals Triangle?"}}
{"input": "First let us remind ourselves of the statement of the AM\u2013GM inequality:\n\nTheorem: (AM\u2013GM Inequality) For any sequence $(x_n)$ of $N\\geqslant 1$ non-negative real numbers, we have $$\\frac1N\\sum_k x_k \\geqslant \\left(\\prod_k x_k\\right)^{\\frac1N}$$\n\nIt is well known that the sum operator \u2211 can be generalised so that it operates on continuous functions rather than on discrete sequences. This generalisation is the integral operator \u222b.\nLikewise, we can generalise the product operator to operate on continuous functions. We begin with the following property of the discrete product: $$\\log\\prod_k A_k=\\sum_k \\log A_k$$ (assuming that $A_k>0$). Using this we can define the continuous product (also known as the geometric integral according to Wikipedia) as follows: $$\\prod_a^b f(x)\\ dx := \\exp\\int_a^b\\log f(x)\\ dx.$$\nassuming that the integral converges. From these definitions we can seek to generalise the AM\u2013GM inequality to continuous functions:\n\nProposition: (Continuous AM\u2013GM Inequality) For any suitably well-behaved non-negative function $f$ defined on $[a,b]$ with $a<b$, we have: $$\\frac1{b-a}\\int_a^bf(x)\\ dx \\geqslant \\left(\\prod_a^b f(x)\\ dx\\right)^\\frac1{b-a}$$ or in traditional notation: $$\\frac1{b-a}\\int_a^bf(x)\\ dx \\geqslant \\exp\\left(\\frac1{b-a} \\int_a^b \\log f(x)\\ dx\\right)$$\n\nAs a simple illustration, consider the function $f(x)=1+\\sin x$ and fix the lower bound $a=0$. On the following graph the $x$-axis represents the upper bound of integration $b$, and on the $y$-axis are represented the the arithmetic mean (blue) and the geometric mean (red) of $f$ on the interval $[a,b]$ where $a=0$ and $b=x$.\nClearly the claimed inequality seems to hold, and is in fact quite tight when $b\\approx0$.\n\nThis inequality is of interest not least because it is readily abe to produce a number of non-trivial numerical inequalities pertaining to known constants. For instance, again with the example $f(x)=1+\\sin x$, set $b=\\pi/2$. Then assuming the proposition holds, we have\n$$\\frac{2}{\\pi}\\int_0^{\\pi/2} 1 + \\sin x\\ dx \\geqslant \\exp \\left( \\frac{2}{\\pi} \\int_0^{\\pi/2} \\log(1+\\sin x)\\ dx \\right)$$ Evaluating these integrals and rearranging, we obtain: $$G \\leqslant \\frac\\pi4\\log\\left(2+\\frac2\\pi\\right) \\approx 0.9313$$ where $G\\approx 0.9159$ is Catalan's constant.\nAnyway I would like to ask, firstly:\n\nIs the claimed inequality true, and if so what is the proof and on what class of functions is it applicable?\n\nSecondly, and this is more of a fun little challenge:\n\nAssuming its veracity, can this inequality be used to prove remarkable numerical inequalities, e.g. $\\pi < 22/7$ or $e^\\pi - \\pi < 20$?", "output": "I would write this inequality as\n$$\\frac1{b-a}\\int_a^b\\exp(g(x))\\,dx\\ge\\exp\\left(\\frac1{b-a}\\int_a^b g(x)\\,dx\\right).$$\nIn disguise it is the case of Jensen's inequality for the convex function $\\phi(t)=\\exp(t)$.", "meta": {"post_id": 2247940, "input_score": 110, "output_score": 131, "post_title": "Is this continuous analogue to the AM\u2013GM inequality true?"}}
{"input": "I'm self-studying and was doing the following integral:\n$$I = \\int \\frac{e^{\\frac{1}{x}+\\tan^{-1}x}}{x^2+x^4} dx $$\nI solved it fine by letting $ u = \\frac{1}{x} + \\tan^{-1}x$. \nMy question is about an alternative method I saw in which it seems the product rule was not applied:\n$$ I = \\int \\left(\\frac { e^{\\frac{1}{x}}} {x^2}\\right) \\left( \\frac{e^{\\tan^{-1}x}}{x^2+1}\\right) dx $$\n$$ = \\int \\frac {e^{\\frac{1}{x}}}{x^2} dx \\cdot \\int \\frac{e^{\\tan^{-1}x}}{x^2+1}dx$$\nCompleting the work following this step leads to the same solution as I originally found. \nIt is this step that has confused me. I have checked using Wolfram and the two statements are equivalent but I do not understand why.\nWhy are we able to write the integral of products as the product of integrals here, and not apply the product rule?\nThanks in advance.", "output": "Why are we able to write the integral of products as the product of integrals here?\n\nAssume you have two differentiable functions $f,g$ such that\n$$\nf'+g'=f'\\cdot g' \\tag1\n$$ by multiplying by $\\displaystyle e^{f+g}$ one gets\n$$\n(f'+g')\\cdot e^{f+g}=\\left(f'e^{f} \\right)\\cdot \\left(g'e^{g} \\right) \\tag2\n$$ then by integrating both sides\n$$\ne^{f+g}=\\int\\left(f'e^{f} \\right)\\cdot \\left(g'e^{g} \\right) \\tag3\n$$ since $\\displaystyle e^f=\\int\\left(f'e^{f} \\right) $ and $\\displaystyle e^g=\\int\\left(g'e^{g} \\right)$ we have\n\n$$\n\\int\\left(f'e^{f} \\right)\\cdot \\int\\left(g'e^{g} \\right) =\\int\\left(f'e^{f} \\right)\\cdot \\left(g'e^{g} \\right). \\tag4\n$$\n\nBy taking, $f'=-\\dfrac1{x^2}$ and $g'=\\dfrac1{1+x^2}$ we have\n$$\nf'+g'=-\\frac1{x^2}+\\frac1{1+x^2}=-\\frac1{x^2(1+x^2)}=f'g'\n$$ which leads to $(4)$ with the given example.", "meta": {"post_id": 2250993, "input_score": 40, "output_score": 44, "post_title": "When the integral of products is the product of integrals."}}
{"input": "Consider a linear system of equations $Ax = b$.\n\nIf the system is overdetermined, the least squares (approximate) solution minimizes $||b - Ax||^2$. Some source sources also mention $||b - Ax||$.\nIf the system is underdetermined one can calculate the minimum norm solution. But it does also minimize  $||b - Ax||$, or am I wrong?\n\nBut if least squares is also a minimum norm, what is the difference, or the rationale of the different naming?", "output": "Linear system\n$$\n\\mathbf{A} x = b\n$$\nwhere $\\mathbf{A}\\in\\mathbb{C}^{m\\times n}_{\\rho}$, and the data vector $b\\in\\mathbf{C}^{m}$, the solution vector in $x\\in\\mathbf{C}^{n}$.\nLeast squares problem\nA least squares solution is guaranteed to exist and is defined by\n$$\n x_{LS} = \\left\\{\n x\\in\\mathbb{C}^{n} \\colon\n\\lVert\n \\mathbf{A} x - b\n\\rVert_{2}^{2}\n\\text{ is minimized}\n\\right\\}\n$$\nLeast squares solution\nThe general least squares problem offers a $\\color{blue}{particular}$ solution and a $\\color{red}{homogeneous}$ solution. The confusingly named \"solution of minimum norm\" is just the $\\color{blue}{particular}$ solution.\nThe minimizers are the affine set computed by\n$$\n x_{LS} = \n\\color{blue}{\\mathbf{A}^{+} b} +\n\\color{red}{ \n\\left(\n\\mathbf{I}_{n} - \\mathbf{A}^{+} \\mathbf{A}\n\\right) y}, \\quad y \\in \\mathbb{C}^{n}\n\\tag{1}\n$$\nwhere vectors are colored according to whether they reside in a $\\color{blue}{range}$ space or $\\color{red}{null}$ space. (See Laub, 2005, Theorem 8.1, p. 66)\nThe red dashed line below is the set of the least squares minimizers which appears when there is a row rank deficit $(\\rho < m)$. In these cases, the solution is not unique.\n\nLeast squares solution of minimum norm\nTo find the minimizers of the minimum norm, the shortest solution vector, compute the length of the solution vectors.\n$$\n%\n\\lVert x_{LS} \\rVert_{2}^{2} = \n%\n\\Big\\lVert \\color{blue}{\\mathbf{A}^{+} b} +\n\\color{red}{ \n\\left(\n\\mathbf{I}_{n} - \\mathbf{A}^{+} \\mathbf{A}\n\\right) y} \\Big\\rVert_{2}^{2}\n%\n=\n%\n\\Big\\lVert \\color{blue}{\\mathbf{A}^{+} b} \\Big\\rVert_{2}^{2} +\n\\Big\\lVert \\color{red}{ \n\\left(\n\\mathbf{I}_{n} - \\mathbf{A}^{+} \\mathbf{A}\n\\right) y} \\Big\\rVert_{2}^{2}\n%\n$$\nThe $\\color{blue}{range}$ space component is fixed, but we can control the $\\color{red}{null}$ space vector. In fact, chose the vector $y$ which forces this term to $0$.\nTherefore, the least squares solution of minimum norm is the particular solution\n$$\n\\color{blue}{x_{LS}} = \n\\color{blue}{\\mathbf{A}^{+} b}.\n$$\nThis is the point where the red dashed line punctures the blue plane. The least squares solution of minimum length is the point in $\\color{blue}{\\mathit{R}\\!\\left(\\mathbf{A}^{*}\\right)}$.\nFull column rank\nYou ask about the case of full column rank where $n=\\rho$. In this case,\n$$\n\\color{red}{\\mathit{N}\\left( \\mathbf{A} \\right)} = \n\\left\\{ \\mathbf{0} \\right\\},\n$$\nthe null space is trivial. There is no null space component, and the least squares solution is a point.\nIn other words, the complete least squares solution is just the $\n\\color{blue}{particular}$ solution\n$$\n\\color{blue}{x_{LS}} = \n\\color{blue}{\\mathbf{A}^{+} b}\n$$\nWhen the matrix has full column rank, there is no $\\color{red}{homogeneous}$ component to the solution. The solution is unique and is a point.", "meta": {"post_id": 2253443, "input_score": 29, "output_score": 34, "post_title": "Difference between least squares and minimum norm solution"}}
{"input": "I would like to find all functions $f:\\mathbb{R}\\backslash\\{0,1\\}\\rightarrow\\mathbb{R}$ such that \n$$f(x)+f\\left( \\frac{1}{1-x}\\right)=x.$$\nI do not know how to solve the problem. Can someone explain how to solve it? \nIn one of my attempts I did the following, which is confusing to me: By the substitution $y=1-\\frac{1}{x}$ one gets\n$f(y)+f\\left( \\frac{1}{1-y}\\right)=\\frac{1}{1-y}$. So with $x=y$ it follows that  $0=x-\\frac{1}{1-x}$. So it would follow that there is no solution. Is that possible or is there a mistake?\nBest regards", "output": "I would like to shed some light on this issue by taking a more abstract point of view.\nIn my answer to this recent question : (How to solve an equation of the form $f(x)=f(a)$ for a fixed real a.), I used the following group of functions (with the algebraic meaning of the word \"group\")\n$$\\begin{cases}\\phi_1(x)=x, & \\ \\ \\ \\ \\phi_2(x)=1-x, & \\ \\ \\ \\ \\ \n \\phi_3(x)=\\tfrac{1}{x},\\\\ \\phi_4(x)=1-\\tfrac{1}{x}, & \\ \\ \\ \\ \n \\phi_5(x)=\\tfrac{1}{1-x}, & \\ \\ \\ \\ \\ \\phi_6(x)=\\tfrac{x}{x-1}.\\end{cases}$$\nHere also, the presence of this group is natural because it provides all the potentially fruitful changes of variables leading ultimately to the solution.\nLet us take the following notation:\n$$\\psi_k(x):=f(\\phi_k(x))$$\nThus, the given functional equation can be written:\n$$\\tag{1} f(x)+f(\\phi_5(x))=x \\ \\ \\ \\iff \\ \\ \\ \\color{red}{f(x)+\\psi_5(x)=x},$$\nSubstitution $x \\to \\phi_4(x)$ in (1) gives:\n$$\\tag{2}f(\\phi_4(x))+f(\\underbrace{\\phi_5(\\phi_4(x))}_{\\phi_1(x)=x})=\\phi_4(x) \\ \\iff \\ \\color{red}{\\psi_4(x)+f(x)=1-\\tfrac{1}{x}},$$\nSubstitution $x \\to \\phi_5(x)$ in (1) gives:\n$$\\tag{3}f(\\phi_5(x))+f(\\underbrace{\\phi_5(\\phi_5(x))}_{\\phi_4(x)})=\\phi_5(x) \\ \\iff \\ \\color{red}{\\psi_5(x)+\\psi_4(x)=\\tfrac{1}{1-x}}.$$\nIt suffices now to make the following combination of equations (1)+(2)-(3) (the parts in red) to obtain:\n\n$$f(x)=\\frac12\\left(x+1-\\frac{1}{x}-\\frac{1}{1-x}\\right)$$\n\n\nRemark: the group of functions $\\phi_k$ has been recognized by Kummer in the mid-nineteenth century in connection with hypergeometric differential equations. See p. 306 of (http://www.springer.com/la/book/9781461457244), a fascinating book about the rise of complex function theory.\nThis group has also an interest in projective geometry; for this reason, it is sometimes called the \"cross-ratio group\". For a modern presentation of the projective invariant called the cross-ratio, take a look for example at (http://www.maths.gla.ac.uk/wws/cabripages/klein/pinvariant.html).", "meta": {"post_id": 2265891, "input_score": 20, "output_score": 39, "post_title": "Find all functions $f$ such that $f(x)+f(\\frac{1}{1-x})=x$"}}
{"input": "How can I prove that a function that is its own derivative exists? And how can I prove that this function is of the form $a(b^x)$?", "output": "$f(x) = 0$ is trivially its own derivative, and is of the form $a(b^x)$ for $a=0$ and any positive $b$. That's all we need to solve the problem posed.", "meta": {"post_id": 2266951, "input_score": 20, "output_score": 42, "post_title": "How can I prove that there is a function that is its own derivative?"}}
{"input": "I know that $p$-norm of $x\\in\\Bbb{R}^n$ is defined as, for all $p\\ge1$,$$\\Vert{x}\\Vert_p=\\left(\\sum_{i=1}^{n} \\vert{x_i}\\vert^p\\right)^{1/p}.$$\nThe textbook refers to \"Every norm is convex\" for an example of convex functions.\nI failed to prove $f(x)=\\Vert{x}\\Vert_p$ for all $p\\ge1$, then tried to find the proof on the internet but I cannot find it.\nCan someone let me understand why $p$-norm is convex for all $p\\ge1$.", "output": "The Definition of a norm is:\nBe V a Vectorspace, $\\|\\cdot\\|: V \\rightarrow \\mathbb{R} $ is a norm $:\\iff $\n\n$\\forall v \\in V: \\|v\\|\\ge0$ and $\\|v\\| =0 \\iff v=0$ (positive/definite)\n$\\forall v\\in V, \\lambda\\in \\mathbb{R}: |\\lambda|\\|v\\| =\\|\\lambda v\\|$ (absolutely scaleable)\n$\\forall v,w\\in V : \\|v+w\\| \\le \\|v\\|+\\|w\\|$ (Triangle inequality)\n\nThe Definition of convex is:\n$f:V\\rightarrow\\mathbb{R}$ is convex $:\\iff$\n$\\forall  v,w \\in V, \\lambda \\in [0,1]: f(\\lambda v+(1-\\lambda )w)\\le \\lambda f(v) +(1-\\lambda)f(w)$\nSo using the Triangle inequality and the fact that the norm is absolutely scalable, you can see that every Norm is convex:\n$$\\|\\lambda v+(1-\\lambda )w\\|\\le\\|\\lambda v\\|+\\|(1-\\lambda)w\\| = \\lambda\\|v\\|+(1- \\lambda)\\|w\\|$$\nSo by definition every norm is convex. What is left to show is, that the p-norm is in fact a norm. The first two requirements are pretty easy to show, the third is hard. That is why it has its own name: the Minkowski Inequality which is a result of the H\u00f6lder inequality and shows that the triangle inequality holds for every p-norm (if p>1) and thus that it is a norm.\n\nEDIT: Since this seems to be somewhat popular, I thought I would add a sketch of the proof of the Minkowski inequality.\n\nYou show Young's Inequality: $xy\\le \\frac{x^p}{p}+\\frac{y^q}{q}\\quad \\forall q,p>1 \\text{ with } \\frac{1}{p}+\\frac{1}{q}=1,\\ \\forall x,y\\ge 0$.\n\nYou can do that by looking at the function $f(x)=\\frac{x^p}{p}+\\frac{y^q}{q}-xy$ find the critical point, show it is a minimum and is greater zero (derivatives).\n\nYou show the H\u00f6lder Inequality: $\\|fg\\|_1 \\le \\|f\\|_p\\|g\\|_q \\quad \\forall q,p>1 \\text{ with } \\frac{1}{p}+\\frac{1}{q}=1$\n\nYou can do that by setting $x=\\frac{|f|}{||f||_p}$ and $y=\\frac{|g|}{||g||_q}$ and plug them into young's inequality.\nYou get\n\\begin{align}\n&&\\frac{|fg|}{\\|f\\|_p\\|g\\|_q}&\\le \\frac{|f|^p}{p\\|f\\|_p^p} + \\frac{|g|^q}{q\\|g\\|_q^q}\n\\\\\n\\Rightarrow &&\\int \\frac{|fg|}{\\|f\\|_p\\|g\\|_q} d\\mu &\\le \\int \\frac{|f|^p}{p\\|f\\|_p^p}d\\mu + \\int \\frac{|g|^q}{q\\|g\\|_q^q}d\\mu\n\\\\\n\\Rightarrow &&\\frac{\\|fg\\|_1}{\\|f\\|_p\\|g\\|_q}&\\le \\frac{1}{p}+\\frac{1}{q}=1\n\\end{align}\nIt works just the same for sequences or $\\mathbb{R}^n$, you just use young's inequality for every index and then sum over it instead of using the integral.\n\nAnd last the Minkowski Inequality: $\\|x+y\\|_p\\le\\|x\\|_p+\\|y\\|_p \\quad \\forall p>1$\n\nSet $q=\\frac{p}{p-1}$ thus $q(p-1)=p$ and $\\frac{1}{p}+\\frac{1}{q}=1$. Then:\n\\begin{align}\n\\|x+y\\|_p^p&=\\int |x+y|^pd\\mu\\le\\int |x+y|^{p-1}|x|d\\mu+ \\int |x+y|^{p-1}|y|d\\mu\n\\\\\n&\\le \\left(\\int|x+y|^{q(p-1)}d\\mu\\right)^{1/q}\\left(\\int|x|^pd\\mu\\right)^{1/p} + \\left(\\int|x+y|^{q(p-1)}d\\mu\\right)^{1/q}\\left(\\int|y|^pd\\mu\\right)^{1/p}\n\\\\\n&=\\left(\\int|x+y|^{p}d\\mu\\right)^{\\frac{1}{p}\\frac{p}{q}}(\\|x\\|_p+\\|y\\|_p)\n=\\|x+y\\|_p^{p/q}(\\|x\\|_p+\\|y\\|_p)\n\\end{align}\nIf you realize that $p-\\frac{p}{q}=p(1-\\frac{1}{q})=1$ you are done.", "meta": {"post_id": 2280341, "input_score": 33, "output_score": 62, "post_title": "Why is every $p$-norm convex?"}}
{"input": "Often times in multi-variable calculus you would have expressions for the differentials of area and volume like this $dA =dxdy$ or $dV = dxdydz$  which we are supposed to just accept because it makes sense in that if you take a tiny piece of area/volume it looks like a square blah blah....But  cannot get my head around it especially when doing integral substitutions. In single variable calculus it made sense, for example consider the integral\n$$\\int f'(g) \\frac{dg}{dx} dx = \\int f'(g)dg = f(x) + c$$\nthis makes sense as $f'(x) = g'(x)f'(g)$ by the chain rule but when preforming a substitution in a double integral \n$$\\int \\int f(x,y) dx dy = \\int \\int f(u,v) \\det(J)du dv$$ where $$J = \\frac{\\partial(x,y)}{\\partial(u,v)} = \\begin{pmatrix} \\frac{\\partial x}{\\partial u} &  \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u}  \n & \\frac{\\partial y}{\\partial v}\\end{pmatrix}$$\nHow on earth have they magically jumped and concluded that $dxdy$ (which itself is not explained) equal to $\\det(J)dudv$ ? This has caused me so many problems conceptually and i was not able to find any clear explanation to why this is the case and even what is meant by the term $dxdy$", "output": "$\\DeclareMathOperator{\\Area}{Area}$Edit: In the original answer, I was a bit careless with signed versus unsigned area. The original question implicitly asks about signed area (i.e., area where \"handedness\" matters; $dv\\, du = -du\\, dv$), while most accounts in multivariable calculus treat unsigned area (i.e., the \"geometric notion of content\"; $|dv\\, du| = |du\\, dv|$).\nThe argument below is tweaked to incorporate sign consistently. Particularly, the \"$(u, v)$-plane\" is oriented, and $\\Area$ refers throughout to signed area. Algebraically, the arguments can be made \"unsigned\" by placing absolute value signs around determinants, deleting the adjectives \"signed\" and \"oriented\" where they appear, and interpreting $\\Area$ as unsigned area.\n\nTo give a geometric interpretation: Suppose you apply a linear change of variables $(x, y) = T(u, v)$ to the plane:\n$$\n\\begin{aligned}\n  x &= au + bv, \\\\\n  y &= cu + dv;\n\\end{aligned}\n\\quad\\text{i.e.,}\\qquad\n\\left[\\begin{array}{c}\n    x \\\\\n    y \\\\\n  \\end{array}\\right]\n= \\left[\\begin{array}{cc}\n    a & b \\\\\n    c & d \\\\\n  \\end{array}\\right]\n\\left[\\begin{array}{c}\n    u \\\\\n    v \\\\\n  \\end{array}\\right].\n$$\nSince $T$ is linear, $T = dT(u_{0}, v_{0})$ for every point $(u_{0}, v_{0})$.\nThe oriented rectangle $[u_{0}, u_{0} + \\Delta u] \\times [v_{0}, v_{0} + \\Delta v]$, which has signed area $\\Delta u\\, \\Delta v$, maps to a parallelogram whose signed area is, from linear algebra,\n$$\n(ad - bc)\\, \\Delta u\\, \\Delta v = \\det(dT(u_{0}, v_{0}))\\, \\Delta u\\, \\Delta v.\n$$\n\nIf instead your change of variables $(x, y) = F(u, v)$ is continuously-differentiable, the preceding discussion still holds \"approximately at small scales\": The oriented rectangle $[u_{0}, u_{0} + \\Delta u] \\times [v_{0}, v_{0} + \\Delta v]$ at left maps to a near-parallelogram at right whose signed area is\n$$\n\\Area(F(R)) = \\det(dF(u_{0}, v_{0}))\\, \\Delta u\\, \\Delta v + \\text{error},\n$$\nwith error asymptotically small in absolute value compared to $\\Delta u\\, \\Delta v$. Using  infinitesimal notation,  this state of affairs may be expressed by saying\n$$\n\\Area\\bigl(F([u_{0} + du] \\times [v_{0} + dv])\\bigr) = \\det dF(u_{0}, v_{0})\\, du\\, dv.\n$$\n\nTo connect this with integration, let $D$ denote the oriented rectangle on the left, think of a continuous, real-valued function $f$ defined over the region $F(D)$ on the right, and consider the problem of expressing the integral as an integral over $D$ itself. The change of variables formula says (assuming $F$ is one-to-one)\n$$\n\\iint_{F(D)} f(x, y)\\, dx\\, dy = \\iint_{D} f(F(u, v)) \\det dF(u, v)\\, du\\, dv.\n$$\nThis is the sum of infinitesimal contributions of the type\n\\begin{align*}\n  \\iint_{F(R)} f(x_{0}, y_{0})\\, dx\\, dy\n  &= f(F(u_{0}, v_{0})) \\Area(F(R)) \\\\\n  &= f(F(u_{0}, v_{0})) \\det dF(u, v) \\Area(R) \\\\\n  &= \\iint_{R} f(F(u_{0}, v_{0})) \\det dF(u, v)\\, du\\, dv.\n\\end{align*}\n(If $R$ is sufficiently small, the continuous functions $f$ and $f \\circ F$ are nearly constant.)\nAnalogous pictures hold in arbitrary (finite) dimension.", "meta": {"post_id": 2296020, "input_score": 35, "output_score": 40, "post_title": "What does it mean to multiply differentials?"}}
{"input": "As the title says, i want to project 3D points with known (x, y, z) coordinates into a 2D plane with (x', y') coordinates, knowing that the x and y axes are respectively identical to the x' and y' axes ( The (OXY) plane is the same as the (OX'Y') plane) and they have the same measure unit.\nSo basically what i need is the formula to transform the a point's (x, y, z) coordinates into (x', y') coordinates with a perspective projection ( The further an object is from the 2D plane the smaller it seems).", "output": "3D projection is really very simple. The hard part is understanding how it is done; and that is what I shall try to explain here. It is all based on optics, and (linear) algebra.\nLet's assume you stand in front of a window, looking out. If you stand in the center of the window, looking out through the center of the window, then we can treat the center of your eye (more precisely, the center of the lens in the pupil of your dominant eye) the origin in 3D coordinates. Using OP's conventions, $x$ axis increases up, $y$ axis right, and $z$ axis outside the window. Thus, the center of the window is at $(0, 0, d)$, where $d$ is the distance from the eye to the window.\nIf we know the 3D coordinates in the above coordinate system of interesting details outside, 3D projection tells us their coordinates on the surface of the window. These coordinates are what OP needs to draw 3D pictures to a 2D surface.\nHere is a rough diagram of the situation:\n\nThe blue pane is the window, the eye is at the lower left corner, and we are interested in the projected coordinates (projected to the window, that is) of the four corners of some cube at some distance.\nIn a very real sense, those coordinates are obtained by linear interpolation, except that one end of the line segment is at the eye (which we already decided is the origin, so coordinates $(0, 0, 0)$, the other end is at the 3D coordinates of the detail we wish to project, and the interpolation point is where that sight line (usually called \"ray\") intersects the view plane (the window, in our case).\nLet's say one of the 3D coordinates of an interesting detail, say a corner of the greenish cube above, are $(x , y , z)$. That ray intersects the window at\n$$\\begin{cases} x' = x \\frac{d}{z} \\\\\ny' = y \\frac{d}{z} \\\\\nz' = z \\frac{d}{z} = d \\end{cases}$$\nTherefore, the 2D coordinates of that detail on the window are\n$$(x' , y') = \\left( x \\frac{d}{z} ,\\, y \\frac{d}{z} \\right) = \\frac{d}{z} ( x , y )$$\nor, in other words, you simply multiply the $x$ and $y$ 3D coordinates by $d/z$.\nIf some detail has a $z$ coordinate smaller than $d$, it is basically between the window and the eye. These are problematic, because projecting them to the window no longer makes sense wrt. optics.  In visualization software, such details are normally simply not drawn; then, you can think of the window as being the \"camera surface\" of some pyramid-shaped probe, the tip of the pyramid being the 3D coordinate system origin.\nIn games, details with $z$ coordinate smaller than $d$ often produce visual glitches, like seeing through a wall, or similar. The real numerical problems occur near the origin, near the eye. If any graphical primitive, like a line, plane, or sphere, intersects with the eye, it means the eye is badly hurt in real life; with 3D projection, we get unrealistic results (like polygons getting twisted through origin), because our model breaks down at the eye point.\n\nI very warmly recommend you familiarize yourself with basic 2D and 3D vector algebra. Using vector addition, subtraction, scaling, dot product, and cross product, many of the operations you need to work with 3D worlds become much simpler.\nDo not bother with Euler angles (or Tait-Bryan angles); they have limitations (especially gimbal lock) and ambiguities (the order of the rotations). Instead, learn about versors, or unit quaternions that represent rotations. They have four components, $$\\mathbf{q} = ( w , i , j , k )$$\nwhere $w^2 + i^2 + j^2 + k^2 = 1$. You can easily interpolate (\"blend\") between different versors, for example to simulate a camera panning and rotating from one orientation to the next.\nAlthough quaternions have a reputation (among programmers) of being hard to grok, their unit quaternion or versor subset is actually very programmer-friendly. They are numerically stable (you can always divide the components by $\\sqrt{w^2 + i^2 + j^2 + k^2}$ to scale it back to unit length, and it won't bias the rotation in any specific way).\nFor computation, you expand (convert) the versor to a 3\u00d73 rotation matrix, which unlike those for Euler or Tait-Bryan angles, is unique for versors. There are no \"gotchas\" or ambiguities.\nYou will also need to learn about matrix-matrix multiplication, and matrix-vector multiplication, so that you can efficiently apply the rotations to vectors.\nMatrix-matrix multiplication is used to combine rotations or transformations described by matrices, to other such matrices. This means you only need to use one matrix to transform any vector, but that matrix can be the result of several different transformations itself. (For example, if you have a robot arm with ball joints, you can describe each joint using a versor, and a rotation matrix derived from that versor. When you start at the base, you simply multiply the current transformation matrix by the ball joint transformation matrix, to get the local coordinate system in the part that follows each ball joint.)\nVersor-versor multiplication (Hamilton product) does the exact same for versors:\nmultiplying $\\mathbf{q}_1 \\mathbf{q}_2$ yields a versor that represents a rotation by versor $\\mathbf{q}_1$ followed by a rotation by versor $\\mathbf{q}_2$. (Numerically, you'll want to divide each component of the result by $\\sqrt{w^2 + i^2 + j^2 + k^2}$, to ensure it has unit length; as I wrote earlier, this impacts no bias to the result, and allows you to apply as many consecutive rotations as you want, without any distortion -- unlike for example for matrices, which would require orthonormalization after a dozen or so steps, even if using double-precision floating-point numbers.)\n\nThe reverse problem, trying to find the object and the point on an object, when you know the ray arriving at the eye, is called ray casting. If you then continue to trace the possible rays, to find out which ones might originate in light sources, you get to ray tracing.\nIf you transform the 3D coordinate system so that your eye (or camera) is always at origin, the intersection tests become much easier. In particular, consider the sphere case: let $\\vec{c}$ is the center of the sphere, $r$ is its radius, and $\\hat{n}$ is the unit vector ($\\lVert\\hat{n}\\rVert = 1$) showing the direction where the ray came to the eye. Let\n$$D = r^2 + \\left ( \\hat{n} \\cdot \\vec{c} \\right )^2 - \\vec{c} \\cdot \\vec{c}$$\nIf $D \\lt 0$, the ray did not intersect the sphere. If $D = 0$, the ray grazed the sphere, i.e. intersected it tangentially (at one point). If $D \\gt 0$, the ray intersects the sphere at distance $R$ from origin:\n$$R = \\hat{n} \\cdot \\vec{c} \\pm \\sqrt{ D }$$\nat point $R \\hat{n}$. If we are outside the sphere, use $-$ above; if we are inside the sphere, use $+$ above. In general, pick the sign that yields the smaller, but positive, $R$.\nSince ball-and-stick models are often a favourite starting point for physicists and chemists interested in 3D graphics, I've collected the formulas needed to do the above with cylinders (without end caps, with flat end caps, or with spherical end caps) to my Wikipedia user page.", "meta": {"post_id": 2305792, "input_score": 22, "output_score": 52, "post_title": "3D projection on a 2D plane ( weak maths ressources )"}}
{"input": "Recently I came up with an algebra problem with a nice geometric representation. Basically, I would like to know what happens if we repeatedly circumscribe a rectangle by another rectangle which is rotated by $\\alpha \\in \\left( 0, \\frac {\\pi} {4}\\right)$ radians. Use this picture as reference:\n\nIn particular, do the resulting rectangles converge to a square?\nIt is rather easy to show that the rectangles do converge to a square if $\\alpha$ is constant throughout the process. However, if we make it more general by defineing a sequence $\\left(\\alpha_n\\right)_{n=1}^{\\infty}$ of angles and use $\\alpha_i$ in the $i$'th operation, then the answer seems to depend on the chosen sequence. So, for which sequences $\\left(\\alpha_n\\right)_{n=1}^{\\infty}$ do the rectangles converge to a square?\nAlgebraically this problem can be defined like this: \nDefine two real sequences by $A_0=a, B_0=b, a \\neq b, a,b \\in R_{\\gt0}$ and $A_{n+1}=B_n\\sin\\alpha_n + A_n\\cos\\alpha_n, B_{n+1}=A_n\\sin\\alpha_n + B_n\\cos\\alpha_n \\forall n \\in N_{\\gt 0}$, where $\\alpha_i \\in \\left( 0, \\frac {\\pi} {4}\\right) \\forall i \\in N_{\\ge 0}$. Is it true that $\\lim_{n \\to \\infty}\\frac{A_n}{B_n}=1$?\nI tried out a few sequences in C++ to notice some patterns. Interestingly, rectangles seem to converge to a square if and only if $\\lim_{n \\to \\infty} \\left( \\sum_{i=0}^n \\alpha_i \\right) = \\infty$. In particular, for $\\alpha_n = \\frac{1}{n}$ the convergence is really slow, however, it still seems to be converging.\nAlso, I believe that showing $\\lim_{n \\to \\infty}\\left(A_n-B_n\\right)=1$ would be an even stronger result for this problem. Does such a replacement have any influence on the result?\nFor the record, I am still a high school student, so I have no idea how hard this problem might actually be. Any help would be highly appreciated.\nP.S. This is my first question on the site, so please don't judge my wording and style too much. Feel free to ask questions if anything is unclear.", "output": "This is a particularly good first question!\nUsing a little linear algebra, we can actually write sufficiently explicit formulas for $A_n$ and $B_n$ to confirm your conjecture, namely that the divergence of $\\sum_{i = 1}^{\\infty} \\alpha_i$ is sufficient and necessary.\nFirst, by writing the recurrence relations for $A_n$ and $B_n$ in matrix notation and applying a straightforward induction, we get that, for all nonnegative integers $n$,\n$$\\require{cancel}\n\\pmatrix{A_n \\\\ B_n}\n=\n\\left(\\prod_{i = 1}^n \\underbrace{\\pmatrix{\\cos \\alpha_i & \\sin \\alpha_i \\\\ \\sin \\alpha_i & \\cos \\alpha_i}}_{\\Gamma_i}\\right) \\pmatrix{a \\\\ b} .\n$$\nWe can make this more explicit by employing the standard trick of diagonalization: The eigenvalues of $\\Gamma_i$ are $\\lambda_i^{\\pm} := \\cos \\alpha_i \\pm \\sin \\alpha_i$, so for $\\alpha_i$ in the given range, we have that\n$$0 < \\lambda_i^- < \\lambda_i^+ ,$$\nand in particular the eigenvalues are distinct, guaranteeing that we can write\n$$\\Gamma_i = P_i \\pmatrix{\\lambda_i^- & 0 \\\\ 0 & \\lambda_i^+} P_i^{-1}$$ for some $P_i$. In fact, since the rotations $\\Gamma_i$ all commute with one another (and are diagonalizable) they are simultaneously diagonalizable, that is, we can choose all of the $P_i$ to be the same matrix $P$. Computing eigenvectors corresponding to the eigenvalues shows that we may take $$P = \\pmatrix{\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}},$$ the rotation matrix for a clockwise rotation of $\\frac{\\pi}{4}$ (see the remark below).\nOur observation about being able to use the same $P$ for all rotations pays off immediately, as the expression for the product $\\prod_{i = 1}^n \\Gamma_i$ of rotations simplifies dramatically:\n$$\n\\prod_{i = 1}^n \\Gamma_i\n= \\prod_{i = 1}^n \\left[P \\pmatrix{\\lambda_i^- & 0 \\\\ 0 & \\lambda_i^+} P^{-1} \\right]\n= P \\pmatrix{\\prod_{i = 1}^n \\lambda_i^- & 0 \\\\ 0 & \\prod_{i = 1}^n \\lambda_i^+} P^{-1}.\n$$\nThis yields explicit formulas for $A_n, B_n$ in terms of $a, b, (\\alpha_i)$:\n$$\\pmatrix{A_n \\\\ B_n} = \\left(\\prod_{i = 1}^n \\Gamma_i\\right) \\pmatrix{a \\\\ b} = \\pmatrix{\\frac{1}{2}(a + b) \\prod_{i = 1}^n \\lambda_i^+ + \\frac{1}{2} (a - b) \\prod_{i = 1}^n \\lambda_i^- \\\\ \\frac{1}{2}(a + b) \\prod_{i = 1}^n \\lambda_i^+ + \\frac{1}{2}(b - a) \\prod_{i = 1}^n \\lambda_i^-} .$$\nDividing and rewriting gives\n$$\\frac{A_n}{B_n} = \\frac{\\frac{1}{2}(a + b) \\prod_{i = 1}^n \\lambda_i^+ + \\frac{1}{2} (a - b) \\prod_{i = 1}^n \\lambda_i^-}{\\frac{1}{2}(a + b) \\prod_{i = 1}^n \\lambda_i^+ + \\frac{1}{2}(b - a) \\prod_{i = 1}^n \\lambda_i^-} = \\frac{1 + \\mu_n}{1 - \\mu_n},$$\nwhere\n$$\n\\mu_n\n:= \\frac{a - b}{a + b} \\prod_{i = 1}^n \\frac{\\lambda_i^-}{\\lambda_i^+}\n= \\frac{a - b}{a + b} \\prod_{i = 1}^n \\frac{\\cos \\alpha_i - \\sin \\alpha_i}{\\cos \\alpha_i + \\sin \\alpha_i}\n= \\frac{a - b}{a + b} \\prod_{i = 1}^n \\tan \\left(\\frac{\\pi}{4} - \\alpha_i\\right) ,\n$$\nand $0 < \\mu_n < 1$.\nSo, if $\\limsup \\alpha_i > 0$, then $\\mu_n \\to 0$ and hence $\\frac{A_n}{B_n} \\to 1$. If, on the other hand, $\\limsup \\alpha_i = 0$, we need not have $\\mu_n \\to 0$. For small $\\alpha_i$, $\\tan \\left(\\frac{\\pi}{4} - \\alpha_i\\right) \\sim 1 - 2 \\alpha_i$, so for $\\frac{A_n}{B_n} \\to 1$ I believe it's sufficient and necessary for $\\sum_{i = 1}^{\\infty} \\alpha_i$ to diverge.\nSimilarly, we get\n\\begin{align*}\\lim_{n \\to \\infty} (A_n - B_n)\n&= (a - b) \\prod_{i = 1}^n (\\cos \\alpha_i - \\sin \\alpha_i) .\n\\end{align*}\nAgain, $\\limsup \\alpha_i > 0$ is sufficient to guarantee vanishing. For the case $\\limsup \\alpha_i = 0$ we can use $\\cos \\alpha_i - \\sin \\alpha_i \\sim 1 - \\alpha_i$ to conclude that $A_n - B_n \\to 0$ iff $\\sum_{i = 1}^{\\infty} \\alpha_i$ diverges.\nRemark The Jordan decomposition $\\Gamma_i = P \\pmatrix{\\lambda_i^- & 0 \\\\ 0 & \\lambda_i^+} P^{-1}$ is in this problem more than a convenient tool---it also gives some geometrical insight into why rotating and circumscribing makes rectangles more squarelike. The matrix $P^{-1}$ rotates a vector in the $ab$-plane (which we can view as a vector from the rectangle to one of its corners) by $\\frac{\\pi}{4}$. Then $\\Gamma_i$ shears the rotated vector, lengthening its component in the direction that corresponds to the quantity $a + b$ and shortening its component in the direction that corresponds to $a - b$, which we may view as the absolute defect from squareness. Finally, the matrix $P$ rotates back the one-eighth of a turn, which we may view as returning back to the original $ab$-coordinates.", "meta": {"post_id": 2308748, "input_score": 97, "output_score": 40, "post_title": "When does a sequence of rotated-and-circumscribed rectangles converge to a square?"}}
{"input": "The series in question:\n$$\\frac{5}{7^2+11^2} + \\frac{9}{11^2+15^2} + \\frac{13}{15^2+19^2} + \\dots$$\nOr in a concise form:\n$$\\sum_{i=1}^\\infty {\\frac{4i+1}{(4i+3)^2+(4i+7)^2}}$$\nI tried to solve, and find a closed form of the above summation but got no luck. The denominator could not be factorised and decomposed and I couldn't transform the series into a telescopic one to solve it either. \nI asked this to my maths professor and he looked at the series in question for a while, and declared it as a diverging one, so it can't be solved. He looked unsure. \nWas he right? Is it a divergent series? If not, how can I solve it, if I can? Thanks!", "output": "Heuristically, the summand is the ratio of a linear polynomial to a quadratic polynomial, and so it grows similarly to the series $\\sum_{i=1}^\\infty \\frac 1 i$, which diverges. This tells us that the original sum diverges as well. To show this, note that for sufficiently large $i$ ($i>80$, to be exact) we have\n$$\\frac{4i+1}{(4i+3)^2+(4i+7)^2} = \\frac{4i+1}{32i^2+80i+58} \\geq \\frac{4i}{33i^2} = \\frac{4}{33}\\frac 1 i$$\nNow you can use the comparison test.", "meta": {"post_id": 2311687, "input_score": 15, "output_score": 36, "post_title": "Is this series diverging? If not, what's the sum?"}}
{"input": "What I tried:\nLet $A(2,0,4)$, $B(4,1,-1)$, $C(6,7,7)$ then\n$$\\vec{AB}=(2,1,-5), \\vec{AC}=(4,7,3), \\vec{BC}=(2,6,8)$$\nThen I calculated the angle between vectors:\n$$\\begin{aligned}\n\\alpha_1 &= \\cos^{-1}\\left(\\frac{(2,1,-5)(4,7,3)}{\\sqrt{2^2+1^2+(-5)^2}\\sqrt{4^2+7^2+3^2}}\\right) \\\\\n&= \\cos^{-1}(0)=90\u00b0 \\\\\n\\alpha_2 &= \\cos^{-1}\\left(\\frac{(4,7,3)(2,6,8)}{\\sqrt{4^2+7^2+3^2}\\sqrt{2^2+6^2+8^2}}\\right) \\\\\n&= \\cos^{-1}\\left(\\frac{74}{\\sqrt{74}\\sqrt{104}}\\right)=32.49\\\\\n\\alpha_3 &= \\cos^{-1}\\left(\\frac{(2,6,8)(2,1,-5)}{\\sqrt{2^2+6^2+8^2}\\sqrt{2^2+1^2+(-5)^2}}\\right) \\\\\n&= \\cos^{-1}\\left(\\frac{-30}{\\sqrt{104}\\sqrt{30}}\\right)=122.5\u00b0\n\\end {aligned}$$\nAs you can see, these angles don't even form a triangle, what am I doing wrong, any thoughts?", "output": "The distances satisfy the Pythagorean theorem.\n$d(A, B) = \\sqrt{2^2 + 1^2 + 5^2} = \\sqrt{30}$\n$d(A, C) = \\sqrt{4^2 + 7^2 + 3^2} = \\sqrt{74}$\n$d(B, C) = \\sqrt{2^2 + 6^2 + 8^2} = \\sqrt{104}$\nAnd indeed:\n$\\sqrt{30}^2 + \\sqrt{74}^2 = \\sqrt{104}^2$\nTherefore it is a right triangle (link).", "meta": {"post_id": 2324551, "input_score": 19, "output_score": 66, "post_title": "Show that $(2,0,4) , (4,1,-1) , (6,7,7)$ form a right triangle"}}
{"input": "Let\n  $$f(x) = 2/(4^x + 2)$$\n  for real numbers $x$. Evaluate\n  $$f(1/2001) + f(2/2001) + f(3/2001) + \\cdots + f(2001/2001)$$\n\nAny idea?", "output": "We have $\\displaystyle f(1-x)=\\frac{2}{4^{1-x}+2}=\\frac{2\\cdot 4^x}{4+2\\cdot 4^x}=\\frac{4^x}{2+ 4^x}$\n$\\displaystyle f(x)+f(1-x)=\\frac{2}{4^x+2}+\\frac{4^x}{2+ 4^x}=1$\nThe required sum is \n$$\\frac{2000}{2}\\times 1+f(1)=1000+\\frac{2}{4+2}=\\frac{3001}{3}$$", "meta": {"post_id": 2327510, "input_score": 32, "output_score": 81, "post_title": "Witty functional equation"}}
{"input": "More precisely, does the set of non-diagonalizable (over $\\mathbb C$) matrices have Lebesgue measure zero in $\\mathbb R^{n\\times n}$ or $\\mathbb C^{n\\times n}$? \nIntuitively, I would think yes, since in order for a matrix to be non-diagonalizable its characteristic polynomial would have to have a multiple root. But most monic polynomials of degree $n$ have distinct roots. Can this argument be formalized?", "output": "Yes. Here is a proof over $\\mathbb{C} $.\n\nMatrices with repeated eigenvalues are cut out as the zero locus of the discriminant of the characteristic polynomial, thus are algebraic sets. \nSome matrices have unique eigenvalues, so this algebraic set is proper.\nProper closed algebraic sets have measure $0.$ (intuitively, a proper closed algebraic set is a locally finite union of embedded submanifolds of lower dimension)\n(over $\\mathbb{C} $) The set of matrices that aren't diagonalizable is contained in this set, so it also has measure $0$. (not over $\\mathbb{R}$, see this comment https://math.stackexchange.com/a/207785/565)", "meta": {"post_id": 2329255, "input_score": 41, "output_score": 37, "post_title": "Are most matrices diagonalizable?"}}
{"input": "I have a basic question about the notation for writing functions. Say that I have a function defined as\n$$f(x) = g(x) - 1.$$\nIs it then correct to write\n$$ f = g -1?$$\nAre there any problems with writing functions like this?", "output": "If you use $f$ as the symbol for some function, then $f(x)$ is the image of $x$ under that function, i.e. $f$ represents a function and $f(x)$ is a number, the result of $x \\mapsto f(x)$. So it makes sense to write $f(x)-1$ as a function value where $1$ is subtracted from $f(x)$: there's no real risk on wrong interpretations here, this is standard notation.\nHowever, you can also do operations on functions such as the sum of functions $f+g$ or the product of functions $fg$. In that context, you could interpret \"$f-1$\" as a difference of functions: the difference of $f$ and a constant function $x \\mapsto 1$ (for all $x$), abbreviated simply as \"$1$\". \nNote then that the blue $1$ in $f(x)-\\color{blue}{1}$, a number, doesn't play the same role as the red $1$ in $f-\\color{red}{1}$, meant to denote a function. This can raise confusion so only do this when it's sufficiently clear from the context what you mean.", "meta": {"post_id": 2329794, "input_score": 18, "output_score": 43, "post_title": "Notation for functions"}}
{"input": "Given a $9 \\times 9$ solved Sudoku game with $3 \\times 3$ regions, is it possible that one (or more) of the regions are invalid if all rows and columns are valid (i.e. have a unique sequence of $1-9$)?", "output": "Yes, it can happen that all $3 \\times 3$ regions are invalid:\n\\begin{array}{|ccc|ccc|ccc|} \\hline\n 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\\\\n 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 1 \\\\\n 3 & 4 & 5 & 6 & 7 & 8 & 9 & 1 & 2 \\\\ \\hline\n 4 & 5 & 6 & 7 & 8 & 9 & 1 & 2 & 3 \\\\\n 5 & 6 & 7 & 8 & 9 & 1 & 2 & 3 & 4 \\\\\n 6 & 7 & 8 & 9 & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n 7 & 8 & 9 & 1 & 2 & 3 & 4 & 5 & 6 \\\\\n 8 & 9 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\\\\n 9 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\\\ \\hline\n\\end{array}", "meta": {"post_id": 2331022, "input_score": 15, "output_score": 48, "post_title": "Can a solved Sudoku game have an invalid region if all rows and columns are valid?"}}
{"input": "Let $p(x)=a_n x^n + a_{n-1} x^{n-1} + ... + a_1 x + a_0,\\enspace a_i\\in\\mathbb{C}$ some polynomial.\nSuppose that $|a_0|$ is very small (compared to the other coefficients' magnitude).\nIs there any way the (complex) roots of $p(x)$ could be heavily affected by setting $a_0 = 0$ ?\nI think the answer could be no because the polynomial is continuous in the constant term and thus small changes in the constant term will affect the function only slightly. But does this hold true for the location of the roots?\nWhy am I asking this?\nI try to incorporate a not selfwritten custom root finder into my Matlab-program but unfortunately in some rare cases one of its loops doesn't converge if the input vector's constant coefficient is of the magnitude $\\approx 10^{-18}$ and then the algorithm crashes. However it does converge if I set the constant term to zero but I worry whether I could get wrong results.\n\nEDIT 1:\nBased on the answer by @Fixed Point, I could find a successor (NAClab) of the Matlab root finder that I was using and it doesn't crash anymore. I then went on to quickly investigate the polynomials Fixed Point proposed. Here are the results:\n\nFigure1:\nUp to degree 17 the root finder keeps the results on the real axis.\nFrom degree 18 onwards the calculated roots gain an imaginary part which grows linearly with the degree of the polynomial.\nThe ratio between the constant coefficient $a_0$ and the highest order coefficient $a_n$ is in the order of $10^{-16}$ when the roots begin to diverge from the real axis and grows with approximately one order of magnitude per increase of polynomial degree.\nFigure2:\nHere the constant coefficient is set to zero, one can see that the roots that are close to each other get perturbed quite significantly.\nThe Matlab code to reproduce the results can be downloaded here.\n\nEDIT 2: To address Fixed Point's questions:\n\nSince Brent's algorithm is guaranteed to converge (it could be slow but it will converge), I am curious as to why you were having the problem that you said you were.\n\nWhen developing the MIMO extension for ANP (animated nyquist diagram, a leisure project for educational purposes) I came to realize that the program would have to deal with high order polynomials even for small MIMO systems. I then noticed that Matlab's 'roots' would produce very inaccurate results when there were roots with high multiplicity present - even in trivial, obvious cases like $(x+1)^4=0$ (try roots(poly([-1,-1,-1,-1]))).\nEven if my program should only be used for entertainment, that wasn't good enough. After finding Multroot (by Zeng) and unit-testing it with quite some success using randomized MIMO systems I found that besides some trivial to solve crashes it had a more severe flaw that had to do with a small constant term.\n\nHow/why was your application crashing?\n\nOne such polynomial can be defined in Matlab as follows (it's the one that finally lead me to this SE question):\np = hex2num(['bfae7873980ada44';'bfd79794c0074ef6';'bfe9e4c737c98680';'bfe5502ed16afae0';'bf81513e302abba0';'3fc59ae0b4d97164';'bc80000000000000'])';\n\nUse it as input to Multroot: multroot(p)\nMost likely it will end with an error saying that an output argument hasn't been assigned. (Beware that the algorithm uses randomized initial vectors and thus succeeds with a small chance)\n\nWas this MATLAB's fzero which was crashing?\n\nAs explained I didn't use 'fzero' and unfortunately it can't help me here, as it says in the documentation that it needs a change of sign to detect a zero - which isn't the case for all roots of a general polynomial.", "output": "Consider the polynomial, \n$$p_1(x)=(x-1)(x-2)...(x-19)(x-20).$$\nAfter expanding it, you get,\n\\begin{array}{|r|r|}\n\\hline\n\\textrm{Exponent} & \\textrm{Coefficients of $p(x)$}\\\\\\hline\n 0 & 2432902008176640000 \\\\\n 1 & -8752948036761600000 \\\\\n 2 & 13803759753640704000 \\\\\n 3 & -12870931245150988800 \\\\\n 4 & 8037811822645051776 \\\\\n 5 & -3599979517947607200 \\\\\n 6 & 1206647803780373360 \\\\\n 7 & -311333643161390640 \\\\\n 8 & 63030812099294896 \\\\\n 9 & -10142299865511450 \\\\\n 10 & 1307535010540395 \\\\\n 11 & -135585182899530 \\\\\n 12 & 11310276995381 \\\\\n 13 & -756111184500 \\\\\n 14 & 40171771630 \\\\\n 15 & -1672280820 \\\\\n 16 & 53327946 \\\\\n 17 & -1256850 \\\\\n 18 & 20615 \\\\\n 19 & -210 \\\\\n 20 & 1 \\\\\\hline\n\\end{array}\nNote that the constant term here is $20!$. Now let us consider the polynomial\n$$p_2(x)=x^{20}p_1(1/x)$$\nwhich basically flips the coefficients, giving us\n\\begin{array}{|r|r|}\n\\hline\n\\textrm{Exponent} & \\textrm{Coefficients of $p_2(x)$}\\\\\\hline\n 0 & 1 \\\\\n 1 & -210 \\\\\n 2 & 20615 \\\\\n 3 & -1256850 \\\\\n 4 & 53327946 \\\\\n 5 & -1672280820 \\\\\n 6 & 40171771630 \\\\\n 7 & -756111184500 \\\\\n 8 & 11310276995381 \\\\\n 9 & -135585182899530 \\\\\n 10 & 1307535010540395 \\\\\n 11 & -10142299865511450 \\\\\n 12 & 63030812099294896 \\\\\n 13 & -311333643161390640 \\\\\n 14 & 1206647803780373360 \\\\\n 15 & -3599979517947607200 \\\\\n 16 & 8037811822645051776 \\\\\n 17 & -12870931245150988800 \\\\\n 18 & 13803759753640704000 \\\\\n 19 & -8752948036761600000 \\\\\n 20 & 2432902008176640000 \\\\\\hline\n\\end{array}\nThis polynomial $p_2(x)$, I present as an answer to your question. The constant term here is very small compared to the other coefficients. The constant term is just one. The roots of this $p_2(x)$ polynomial are just\n$$x=\\frac{1}{20},\\frac{1}{19},\\frac{1}{18},...,\\frac{1}{3},\\frac{1}{2},1.$$\nThe roots are all distinct, rational, and reasonably separated on the real line. Now let us define $p_3(x)=p_2(x)$ except that the constant term is equal to zero instead of one and compare the roots.\n\\begin{array}{|l|l|}\n\\hline\n\\textrm{Old roots of $p_2(x)$} & \\textrm{New roots of $p_3(x)$}\\\\\\hline\n 0.05 & 0\\\\\n 0.0526316 & 0.00606612 - 0.0292961i\\\\\n 0.0555556 & 0.00606612 + 0.0292961i\\\\\n 0.0588235 & 0.0236616 - 0.0549143i\\\\\n 0.0625 & 0.0236616 + 0.0549143i\\\\\n 0.0666667 & 0.0510481 - 0.0735013i\\\\\n 0.0714286 & 0.0510481 + 0.0735013i\\\\\n 0.0769231 & 0.0855378 - 0.0823447i\\\\\n 0.0833333 & 0.0855378 + 0.0823447i\\\\\n 0.0909091 & 0.123755 - 0.0796379i\\\\\n 0.1 & 0.123755 + 0.0796379i\\\\\n 0.111111 & 0.16195 - 0.064656i\\\\\n 0.125 & 0.16195 + 0.064656i\\\\\n 0.142857 & 0.196345 - 0.0378412i\\\\\n 0.166667 & 0.196345 + 0.0378412i\\\\\n 0.2 & 0.218259\\\\\n 0.25 & 0.249\\\\\n 0.333333 & 0.333333\\\\\n 0.5 & 0.5\\\\\n 1 & 1\\\\ \\hline\n\\end{array}\nAs you can see that some of the roots stay the same or don't change much. But the majority of the roots \"changed significantly\" and became complex instead of purely real. So the answer to your question is, don't set the constant term to be zero. It won't work in general and can give you wacky answers.\nIf these coefficients are too large for you, then just divide the polynomial $p_2(x)$ by $20!$ and you get the coefficients\n\\begin{array}{|r|r|}\n\\hline\n\\textrm{Exponent} & \\textrm{Coefficients of $\\frac{p_2(x)}{20!}$}\\\\\\hline\n 0 & 4.110317623312165\\times10^{-19}\\\\\n 1 & -8.631667008955546\\times10^{-17}\\\\\n 2 & 8.473419780458027\\times10^{-15}\\\\\n 3 & -5.166052704859894\\times10^{-13}\\\\\n 4 & 2.1919479625883945\\times10^{-11}\\\\\n 5 & -6.873605325572918\\times10^{-10}\\\\\n 6 & 1.6511874089046065\\times10^{-8}\\\\\n 7 & -3.1078571268337856\\times10^{-7}\\\\\n 8 & 4.6488830858656685\\times10^{-6}\\\\\n 9 & -0.0000557298 \\\\\n 10 & 0.000537438 \\\\\n 11 & -0.00416881 \\\\\n 12 & 0.0259077 \\\\\n 13 & -0.127968 \\\\\n 14 & 0.495971 \\\\\n 15 & -1.47971 \\\\\n 16 & 3.3038 \\\\\n 17 & -5.29036 \\\\\n 18 & 5.67378 \\\\\n 19 & -3.59774 \\\\\n 20 & 1 \\\\\\hline\n\\end{array}\nMaking that small $10^{-19}$ constant to zero will give you the same problem with the roots because multiplying a polynomial by a constant doesn't change its roots. The thing to realize here is that the roots of a polynomial do depend continuously on the coefficients but they can be extremely sensitive and you must consider the complex plane as a whole with the real line embedded in it. The real line is just a tiny part of the entire complex plane. The real line is nothing special in this context. Changing the coefficients makes the roots wander but they can wander in any direction in the complex plane even if they were originally strictly on the real line. There is nothing constraining the roots to the real line. You can have real roots becoming complex or the other way around by slightly changing a polynomial's coefficients.\n\nJames Wilkinson is one of the most respected numerical analysts of the 20th century and his specialty was coming up with counter examples. He demonstrated that the problem of finding the roots of an arbitrary polynomial using its coefficients is an ill-conditioned problem in general.\nHe presented a specific example,\n$$p(x)=(x-1)(x-2)...(x-19)(x-20).$$\nThe roots are easy to find; $x=1,2,3,...,19,20$ so they are well-seperated. If the polynomial is expanded, the coefficient of the $x^{19}$ term is $-210$. Let us perturb this coefficient by $2^{-23}$ and then round it to $\u2212210.0000001192$ and let's call this new polynomial $q(x)$. The following happens:\n\nThe value of $p(20)=0$ blows up to $q(20)\\approx-6.24949\\times10^{17}$.\nThe roots at $x=1,2,3,4,5,6$ stay roughly the same.\nThe roots at $x=7,8,9$ have noticeable change.\nThe next ten roots actually turn complex (pairs, because the coefficients are still real). All ten of these roots have a not-so-small imaginary part. The smallest imaginary part is $\\approx 0.643439$ so you decide how far this is from being a real number.\nThe root which was at $x=20$ has moved to $x\\approx20.8469$.\n\nRemember, this is despite the fact that the coefficients were integers (albeit very large integers). The roots were all real integers and well-separated and look what happened by a tiny perturbation. This polynomial is the one on which my answer is based.\nWilkinson presented another polynomial. Define\n$$q(x)=(x-2^{-1})(x-2^{-2})(x-2^{-3})...(x-2^{-19})(x-2^{-20}).$$\nHe showed that this polynomial $q(x)$ is rather insensitive to relatively large changes in the coefficients. Therefore in general, one cannot say anything either way. Check out this page for some more detail and I also urge you to read his published works (like the references at the bottom of the wikipedia page). They may be a bit hard to find but they are totally worth it.\n\nAddendum (too long to be a comment)\nIn response to the OP's comment,\n\nJust out of curiosity - how did you calculate the new roots (for p3) in the third table? Is it an analytical result?\n\nthe answer is that no, this is not an analytical result. Plain floating point arithmetic was used to numerically estimate the roots of $p_3(x)$.\nI want to point out here that lots of new or improved methods were presented in the past century. We know quite a bit about polynomials and their roots. But there is (still) no single numerical method which will work for all polynomials in an optimal fashion. You can always come up with counterexamples where a well-liked method will be \"slow\" or sub-optimal for finding the roots of a polynomial. However, there are methods which are much better than others.\nOne of the most popular is Brent's method which is a hybrid method. Brent actually wrote a book, \"Algorithms for Minimization without Derivatives\" but the book is now out of print. So they scanned a copy and made it available to public here. In this books, you want chapter 4, \"An Algorithm with Guaranteed Convergence for Finding a Zero of a Function\" which describes his method. His method has also been peer-reviewed published. \nFurthermore, one of the cofounders of MATLAB, Cleve Moler has a blog (useful in itself) and once he had a three-part post (one, two, and three) describing various algorithms and how MATLAB's fzero works. Part two is where he discusses Brent's method which is implemented in MATLAB's fzero. Since Brent's algorithm is guaranteed to converge (it could be slow but it will converge), I am curious as to why you were having the problem that you said you were. How/why was your application crashing? Was this something you yourself wrote? But then I would advise you against this. There are plenty of canned routines and there shouldn't be a need to reinvent the wheel. Was this MATLAB's fzero which was crashing? In which case I would be very interested in knowing those polynomials. Perhaps you can post a few examples here? Lastly, there is this fun book, one of my all-time favorites and talks quite a bit about zeros of polynomials and how to find them.\n\nAddendum - 72 Days Later\nThe OP provided with a specific example of a polynomial with which he was having a problem. Just out of curiosity, I wanted to take a look at it and see what was happening. The OP provided the coefficients in floating point (in hexadecimal). Since I cannot tell what the actual coefficients were supposed to be, I will assume that the provided floating point coefficients represent the coefficients exactly as rational numbers. First, convert the hexadecimal form into base ten fractions and then decimals just to see what they are.\n$$\n\\begin{array}{|l|l|l|r|r|}\n\\hline\n&\\text{Exponent} & \\text{Hex Form} & \\text{Fractional Form} & \\text{Decimal Form} \\\\\\hline\na_6 & 6 & \\text{BFAE7873980ADA44} & -\\frac{2144171792184977}{36028797018963968} &\n   -0.05951272231 \\\\\n a_5 & 5 & \\text{BFD79794C0074EF6} & -\\frac{3320294798501755}{9007199254740992} &\n   -0.3686267734 \\\\\n a_4 & 4 & \\text{BFE9E4C737C98680} & -\\frac{56940771119885}{70368744177664} &\n   -0.8091770258 \\\\\n a_3 & 3 & \\text{BFE5502ED16AFAE0} & -\\frac{187473016346583}{281474976710656} &\n   -0.6660379496 \\\\\n a_2 & 2 & \\text{BF81513E302ABBA0} & -\\frac{152325066937821}{18014398509481984} &\n   -0.008455739827 \\\\\n a_1 & 1 & \\text{3FC59AE0B4D97164} & \\frac{1520316102106201}{9007199254740992} &\n   0.1687889941 \\\\\n a_0 & 0 & \\text{BC80000000000000} & \\frac{1}{36028797018963968} &\n   2.775557561562\\times10^{-17} \\\\ \\hline\n\\end{array}\n$$\nUsing the exact fractional form of the coefficients, define the polynomials\n$$p(x)=a_0+a_1x+a_2x^2+a_3x^3+a_4x^4+a_5x^5+a_6x^6$$\n$$q(x)=a_1x+a_2x^2+a_3x^3+a_4x^4+a_5x^5+a_6x^6$$\nwhere $q(x)$ is just $p(x)$ with the constant term set to zero.\nAfter looking at these, I know that all of the roots are irrational, except for the trivial root $x=0$ for $q(x)$. Further, I don't know for sure, but I suspect that both of these polynomials are not solvable in the radicals (check this and this if the reader doesn't know what this means) so there are no solutions in the radicals. Now, we have no choice but to rely on numerical methods.\n\nThis picture above plots both $p(x)$ and $q(x)$. Look at the axis limits and the scales. I can see the three simple real distinct roots easily enough (the three right-most roots). On the left, the graph is very flat near the $x$-axis but there is at least one root there for sure. It could be just one real root with multiplicity three or it could be three distinct real roots very close together or any other combination in between. My guess would be there aren't any complex roots. Notice how both polynomials are indistinguishable from each other. We just see a single curve.\n\nThis second image, above, zooms in on that flat region. The third simple root (the one on the right in this plot) is even more obvious now. But we still can't tell what is happening with the other three roots on the left. The graph is still too flat. Note we still cannot distinguish between $p(x)$ and $q(x)$. This means that for this specific polynomial, the polynomial which prompted this question, setting the constant term to zero actually doesn't change the roots by much. We'll verify this now.\nStarting with the exact coefficients and carrying about a hundred digits throughout the computations for precision and accuracy, I got the following roots.\n$$\n\\begin{array}{|r|r|r|}\\hline\n\\text{Roots of $p(x)$} & \\text{Roots of $q(x)$} & \\frac{|\\text{Roots}(p(x))-\\text{Roots}(q(x))|}{|\\text{Roots}(p(x))|}\\cdot100\\%\\\\ \\hline\n -1.7613269 & -1.7613263 & 0.00003277 \\\\\n -1.3071486 & -1.3071486 & 1.70941691\\times10^{-13} \\\\\n -1.64439487\\times10^{-16} & 0 &\n   100 \\\\\n 0.39708223 & 0.39708223 & 1.72600660\\times10^{-14} \\\\\n -1.7613452-0.0000106 i & -1.7613455-0.0000111 i & 0.00003276 \\\\\n -1.7613452+0.0000106 i & -1.7613455+0.0000111 i & 0.00003276 \\\\\\hline\n\\end{array}\n$$\nThe first column shows the roots of $p(x)$. The second column shows the roots of the perturbed $q(x)$. The third column shows the relative difference in the roots in percent. The absolute value in the third column is the usual norm in $\\mathbb{R}$ or $\\mathbb{C}$ as appropriate. The relative changes are small so that you can see that none of the roots really changed significantly. \nThe magnitude of the entire difference vector of the roots is\n$$||\\text{Roots}(p(x))-\\text{Roots}(q(x))||_2=9.995367\\times10^{-7}$$\nand indeed the roots changed very little. This is all relative because another perspective is that a change of the order $10^{-17}$ to the coefficient vector caused the magnitude of the roots vector to change by almost $10^{-6}$ which is eleven orders of magnitude. This is gigantic from another point of view. The vector function which maps the coefficients of a polynomial to its roots is indeed continuous. But being continuous doesn't restrict the gradient in anyway. It can be large or small. And in this case, the gradient of this vector function just happens to be very large at this particular point in its domain.", "meta": {"post_id": 2361040, "input_score": 28, "output_score": 41, "post_title": "Influence of small constant term on roots of polynomial"}}
{"input": "I'm looking into nonstandard analysis, and am in a chapter which introduces the whole load of basic terms they'll use. \nOne of this is a proof for ordered pairs (Kuratowski definition) by induction. The ordered pairs are defined like this:\n\n\n\\begin{equation}\n\\begin{aligned}\n(a)_k    :&=\\{a\\}  \\\\\n(a,b)_{k} :&= \\{\\{a\\},\\{a,b\\}\\} \\\\\n(a_1,\\,...\\,,a_n)_k :&= ((a_1,\\,...\\,,a_{n-1}),a_n)\n\\end{aligned}\n\\end{equation}\n\n\nThe theorem to show is :\n$(a_1,\\,...\\,,a_n) = (b_1,\\,...\\,,b_n) \\Rightarrow a_k = b_k \\text{  for k = 1, ... , n}$\nThey do it by induction: Case n = 1 is trivial, and case n = 2 (the part I don't understand) goes like this:\nIt is $(a_1 , a_2) = (b_1,b_2) $. This is per definition equal to $\\{\\{a_1\\},\\{a_1,a_2\\}\\} = \\{\\{b_1\\},\\{b_1,b_2\\}\\} $.\nNow the following cases are possible:\n$\n\\begin{align}\n \\{a_1\\} &= \\{b_1\\} &\\text{and}&\\quad\\quad \\{a_1,a_2\\} &= \\{b_1,b_2\\}  ,\\\\\n\\{a_1\\} &= \\{b_1,b_2\\} &\\text{and}& \\quad\\quad\\{b_1\\} &= \\{a_1,a_2\\}\n\\end{align}\n$\nFirst case seems simple enough, but I don't understand how a set with one element can be equal to a set with two elements. Even worse, they say for both cases follows \n$ a_1 = b_1 $ and $ a_2 = b_2$ ... but why?", "output": "Hint: the set $\\{1, 1\\}$ does not have two elements.", "meta": {"post_id": 2384998, "input_score": 20, "output_score": 47, "post_title": "How can a set with one element be equal to a set with two elements"}}
{"input": "Well if one looks at the formula of circular permutations $Pc = (n-1)!$\nBut as we come to that formula, I need a concrete example and an explanation.\nAnother thing, I have seen that when working with a bracelet, the formula changes. A concrete example please along with an explanation\nFinally how the formula is worked when there are repeated elements that are permuted.\nThank you", "output": "Circular permutations\nConsider an arrangement of blue, cyan, green, yellow, red, and magenta beads in a circle.\n\nFor this particular arrangement of the six beads, there are six ways to list the arrangement of the beads in counterclockwise order, depending on whether we start the list with the blue, cyan, green, yellow, red, or magenta bead.  They correspond to the six linear arrangements shown in the rows below.\n\nConversely, each of these six linear arrangements can be transformed into the circular arrangement above by joining the ends of a row.\nMore generally, any circular arrangement of these six beads corresponds to six linear arrangements.  Since there are $6!$ linear arrangements of six distinct beads, the number of distinguishable circular arrangements is\n$$\\frac{6!}{6} = 5!$$\nUnless other specified, only the relative order of the objects matters in a circular permutation.  Therefore, circular arrangements are considered to be rotationally invariant.\nGiven a circular arrangement of $n$ objects, they can be rotated $0, 1, 2, \\ldots, n - 1$ places clockwise without changing the relative order of the objects.  Hence, the number of distinguishable arrangements of $n$ objects in a circle is the number of linear arrangements divided by $n$, which yields\n$$\\frac{n!}{n} = (n - 1)!$$\nAlternatively, given $n$ objects, we measure the order relative to a given object.  Fix that object.  As we proceed counterclockwise around the circle, the remaining objects can be arranged in $(n - 1)!$ orders.\nBracelets\nNow suppose we place these beads on a bracelet.\n\nObserve that if you remove the bracelet at left from your wrist, twist it through a half-turn, then place it back on your wrist, it will look like the bracelet at right, where the beads are arranged in the opposite order as you proceed counterclockwise around the circle.  Thus, we can form the same bracelet by arranging the blue, cyan, green, yellow, red, and magenta in clockwise or counterclockwise order.  Hence, the number of bracelets we can form with the six beads given above is\n$$\\frac{5!}{2}$$\nMore generally, if a bracelet has no clasp or opening that allows us to distinguish a linear order, it is invariant with respect to both rotations and reflection.  Hence, the number of distinguishable arrangements of a bracelet with $n$ objects is\n$$\\frac{1}{2} \\frac{n!}{n} = \\frac{(n - 1)!}{2}$$\nprovided $n > 2$.  If $n = 1$, there is only one possible arrangement for the bracelet.  If $n = 2$, there is only one distinguishable arrangement for the bracelet.\nCircular permutations of a multiset\nThis is a much trickier problem.  To see why, consider an arrangement of nine blue and three red beads in a circle.  Two such arrangements are shown below.\n\nThe first time I saw such a problem, I attempted to solve it by choosing three of the $12$ positions for the red beads, then divide by $12$ to account for rotational invariance.\n$$\\frac{1}{12}\\binom{12}{3} = \\frac{1}{12} \\cdot \\frac{12!}{3!9!} = \\frac{11!}{3!9!}$$\nAlas, this is not an integer.  The reason it is not an integer is the arrangement at left.  While the circular arrangement at right corresponds to $12$ different linear arrangements, the one at left does not.  Given its symmetry, there are only four distinguishable linear arrangements corresponding to the twelve possible starting points of the linear arrangement, depending on whether the first red bead is in the first, second, third, or fourth position of the linear arrangement.  Therefore, we have counted this linear arrangement $1/3$ times.  Hence, the actual number of circular arrangements is\n$$\\frac{1}{12}\\binom{12}{3} + \\frac{2}{3}$$\nWhile that observation solves this particular problem, in general, you will need to master the use of Burnside's lemma or the Polya enumeration theorem to handle these problems.", "meta": {"post_id": 2387149, "input_score": 15, "output_score": 52, "post_title": "Explanation circular permutation"}}
{"input": "Is the closure of a compact subspace of a topological space always compact? \nI want to say no, but i can't think of/find any counterexamples. I think its probably true with added conditions, like Hausdorff property? Because the closure is always closed and closed sets in a compact hausdorff space are compact. \nAny counterexamples for a non-Hausdorff space?", "output": "Consider the space $X = \\mathbb{Z}$ equipped with the topology where the non-empty open sets are precisely those containing $0$. \nAs a finite set $\\{ 0 \\} $ is compact. Further we have that $\\overline{\\{0\\}} = X$ and $X$ is not compact since $\\bigcup_{x \\in X} \\{0,x\\}$ is an open cover with no finite subcover.\nIt is however always the case that the closure of a compact set in a Hausdorff space is compact since in Hausdorff spaces compact sets are closed. (not because of the similar fact that you give that closed sets in compact spaces are compact, even if those spaces aren't Hausdorff)", "meta": {"post_id": 2399624, "input_score": 11, "output_score": 35, "post_title": "Closure of a compact space always compact?"}}
{"input": "Why is the exterior algebra called the \"exterior algebra?\" What makes it \"exterior?\" Is it just because a module can be universally embedded into its exterior algebra, so one could view the exterior algebra as surrounding the module? Why is it not just called the \"alternating algebra?\"", "output": "It was Grassmann that called it exterior because to have a non-null product the multiplicands must be geometrically one to the exterior of the other. For instance $$\\mathbf{x}\\wedge\\mathbf{y}\\wedge\\mathbf{z}=0$$\nif $\\mathbf{x}$ lies in (is not exterior of) the subspace spanned by the $\\mathbf{y}$ and $\\mathbf{z}$. So the product is called exterior product, and consequently the algebra with this product is called exterior algebra.", "meta": {"post_id": 2421566, "input_score": 44, "output_score": 55, "post_title": "Why is the exterior algebra called the \"exterior algebra?\" What makes it \"exterior?\""}}
{"input": "I found this pop math article saying that there was a paper published last year that proved that the cardinalities of the reals and naturals are equal. Is this true or is it a misinterpretation of the result? If it is true, I'm dying to know what the bijection between the two sets is.", "output": "This could have been written clearer. I think the culprit is the section:\n\nThe problem was first identified over a century ago. At the time, mathematicians knew that \u201cthe real numbers are bigger than the natural numbers, but not how much bigger. Is it the next biggest size, or is there a size in between?\u201d said Maryanthe Malliaris of the University of Chicago, co-author of the new work along with Saharon Shelah of the Hebrew University of Jerusalem and Rutgers University.\nIn their new work, Malliaris and Shelah resolve a related 70-year-old question about whether one infinity (call it p) is smaller than another infinity (call it t). They proved the two are in fact equal, much to the surprise of mathematicians.\n\nIf read quickly, this suggests that $\\mathfrak{p}$ and $\\mathfrak{t}$ refer to the cardinality of the set of reals and the set of naturals, respectively. This is not the case, though.\n\nSo what sort of thing are $\\mathfrak{p}$ and $\\mathfrak{t}$, then?\n$\\mathfrak{p}$ and $\\mathfrak{t}$ are what are known as cardinal characteristics of the continuum (CCCs) - cardinals which are (i) known to be uncountable, and (ii) measure how big a set of reals has to be to have some \"universality\" property.\nFor example, one simple CCC is the dominating number, $\\mathfrak{d}$: this is the smallest cardinality of a set $F$ of functions $\\mathbb{N}\\rightarrow\\mathbb{N}$ such that for each $g:\\mathbb{N}\\rightarrow\\mathbb{N}$ there is some $f\\in F$ such that $f(n)>g(n)$ for all but finitely many $n$ (we say $f$ dominates $g$). Clearly $\\mathfrak{d}$ is at most continuum (since that's how many functions $\\mathbb{N}\\rightarrow\\mathbb{N}$ there are in the first place), and it's also uncountable: if $f_i:\\mathbb{N}\\rightarrow\\mathbb{N}$ for $i\\in\\mathbb{N}$, the function $$h(i)=\\sum_{j\\le i}f_j(i)=f_1(i)+f_2(i)+...+f_i(i)$$ is not dominated by any of the $f_i$s.\nAnother simple CCC is the bounding number, $\\mathfrak{b}$. This is \"dual\" to $\\mathfrak{d}$ (in a sense that can be made precise): $\\mathfrak{b}$ is the smallest size of any family $G$ of functions $\\mathbb{N}\\rightarrow\\mathbb{N}$ such that no single $f$ dominates all functions in $G$. Again, $\\mathfrak{b}$ is clearly at most continuum, and is uncountable since any countably many functions can be dominated by a single function (think about the construction of the $h$ above).\nNow cardinal arithmetic is notoriously badly behaved - even basic facts about it tend to be undecidable in ZFC. For example, ZFC doesn't even prove that $\\kappa<\\lambda \\implies 2^\\kappa<2^\\lambda$. So it's really exciting to see ZFC-provable facts about infinite cardinalities; conversely, it's important to understand when certain questions can't be resolved in ZFC alone. In this context, what we care about is comparing CCCs. We can think about it this way: the two trivial CCCs are $\\omega_1$ (\"the smallest size of an uncountable set of reals\") and $2^{\\aleph_0}$ (\"the smallest size of a set containing all the reals\"); and in between we have the interesting CCCs. Of course, if $\\omega_1=2^{\\aleph_0}$ then the whole picture collapses; this is the continuum hypothesis, and it's consistent with ZFC. At the far other end, it's known that we can separate certain CCCs - e.g. that it is consistent with ZFC that $\\mathfrak{b}<\\mathfrak{d}$. (An interesting topic is separating multiple CCCs simultaneously - see this MO question.)\nThis leaves open:\n\nWhat equalities between CCCs can we prove in ZFC? What inequalities can we disprove?\n\nAs an example of the latter, ZFC proves that $\\mathfrak{b}\\le\\mathfrak{d}$ - we can't have $\\mathfrak{b}>\\mathfrak{d}$ (this is a good exercise). More broadly, the collection of disprovable inequalities between many CCCs (not including $\\mathfrak{p}$ and $\\mathfrak{t}$, though) is summed up in Cichon's diagram. Malliaris and Shelah proved a result of the former kind - showing that two CCCs were in fact equal. My understanding is that this type of result is much, much rarer even in general, and of course in this particular case it was extremely surprising (see Shelah's quote in the linked article).\nOf course, I haven't tried to define $\\mathfrak{p}$ and $\\mathfrak{t}$; the definitions are there, but they're a bit technical, and more to the point it's hard to see why someone would care. A good analysis of them is not something I can fit into an MSE answer; but hopefully what I've written explains a bit about where this sort of thing can come from!", "meta": {"post_id": 2427404, "input_score": 18, "output_score": 80, "post_title": "Is there a bijection between the reals and naturals?"}}
{"input": "A satanic prime is a prime number with $666$ in the decimal representation.\nThe smallest satanic prime is $6661$.\nProve that there are infinitely many satanic primes.\n\nI used Dirichlet's theorem for the progression $10000n+6661$ and it is done.\nI'm interested in solutions without Dirichlet's theorem.", "output": "Consider the set $S$ of all numbers without 666 in their base 10 expression.  Here's a fun fact: the sum $\\sum_{s\\in S} \\frac{1}{s}$ converges.  It's actually pretty easy to prove, so I'll leave it as an exercise (or google \"Kempner series\").\nOn the other hand, a famous result of Euler says the sum of the reciprocals of the prime numbers diverges.", "meta": {"post_id": 2440482, "input_score": 34, "output_score": 108, "post_title": "Prove that there are infinitely many primes with $666$ in their decimal representation without Dirichlet's theorem."}}
{"input": "Let $\\alpha$, $\\beta$, $\\gamma$, $\\delta$ be the roots of $$z^4-2z^3+z^2+z-7=0$$ then find value of $$(\\alpha^2+1)(\\beta^2+1)(\\gamma^2+1)(\\delta^2+1)$$\nAre Vieta's formulas appropriate?", "output": "There is no need to use Vieta's formulas. Let \n$$f(z)=z^4-2z^3+z^2+z-7=(z-\\alpha)(z-\\beta)(z-\\gamma)(z-\\delta).$$\nThen, since $(i-a)(-i-a)=-i^2+a^2=1+a^2$, it follows that\n$$(\\alpha^2+1)(\\beta ^2+1)(\\gamma^2+1)(\\delta^2+1)=f(i)f(-i)=|f(i)|^2=|-7+3i|^2=49+9=58.$$", "meta": {"post_id": 2463164, "input_score": 16, "output_score": 47, "post_title": "Value of $(\\alpha^2+1)(\\beta^2+1)(\\gamma^2+1)(\\delta^2+1)$ if $z^4-2z^3+z^2+z-7=0$ for $z=\\alpha$, $\\beta$, $\\gamma$, $\\delta$"}}
{"input": "Question: Solve the trigonometric equation: $\\sin x + \\cos x=\\sin 2x + \\cos 2x$.\n\nMy attempt: \n$\\sin x + \\cos x=\\sin 2x + \\cos 2x$\n$\\implies \\sin x + \\cos x=2\\sin x \\cos x + \\cos^2 x - \\sin^2 x$\n$\\implies \\sin x + \\cos x=2\\sin x \\cos x + \\cos^2 x - (1-\\cos^2 x)$\n$\\implies \\sin x + \\cos x=2\\sin x \\cos x + 2\\cos^2 x - 1$\n$\\implies \\sin x - 2\\sin x \\cos x + \\cos x - 2\\cos^2 x= - 1$\n$\\implies \\sin x(1-2\\cos x)+\\cos x(1-2\\cos x)=-1$\n$\\implies (1-2\\cos x)(\\sin x+\\cos x)=-1$\n$\\implies (1-2\\cos x)=-1$ or $(\\sin x +\\cos x)=-1$\n$\\implies \\cos x=1$ or $\\sin^2 x +\\cos^2 x + 2\\sin x\\cos x=1$ \n$\\implies x=2n\\pi$ or $\\sin 2x=0$\n$\\implies x=2n\\pi$ or $2x=n\\pi$\n$\\therefore x=2n\\pi$ or $x=\\frac{n\\pi}{2}$ \nBut the answers given in my book are $x=2n\\pi$ and $x=\\frac{(4n+1)\\pi}{6}$. Where have I gone wrong? Please help.", "output": "To expand on @gribouillis 's comment, the error in your argument is this step:\n$(1-2\\cos x)(\\sin x+\\cos x)=-1$\n$\\implies (1-2\\cos x)=-1$ or $(\\sin x +\\cos x)=-1$\nThis is an incorrect implication.\n$ab=c$ only implies $a=c$ or $b=c$ when $c=0$.\nFor $c=-1$ as in this case, you could have $a=1,b=-1$ or $a=5,b=-0.2$ or $a=-1000,b=0.001$ or an infinite number of other combinations.", "meta": {"post_id": 2477920, "input_score": 9, "output_score": 39, "post_title": "How to solve the trigonometric equation $\\sin x + \\cos x=\\sin 2x + \\cos 2x$?"}}
{"input": "Extra conditions that put a formal solution out of my reach: the centre cell must contain a $0$, and two grids are equal if they have a symmetry, e.g.\n$$\\left( \\begin{array}{ccc}\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 1 & 1\\end{array} \\right)\n=\n\\left( \\begin{array}{ccc}\n0 & 1 & 1 \\\\\n1 & 0 & 1 \\\\\n0 & 0 & 0 \\end{array} \\right)\n=\n\\left( \\begin{array}{ccc}\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 0\\end{array} \\right)\n$$\nFor context, this question is part of an investigation into the number of possible checkmate patterns in chess.", "output": "Use Burnside's lemma. The number of symmetries of the matrix is eight:\n\nthe identity, leaving $2^8$ admissible matrices unchanged (the centre cell being fixed)\ntwo 90\u00b0 rotations leaving $2^2$ matrices unchanged each\na 180\u00b0 rotation leaving $2^4$ matrices unchanged\nfour reflections leaving $2^5$ matrices unchanged each\n\nSo the number of possible matrices up to symmetry is\n$$\\frac{2^8+2\\cdot2^2+2^4+4\\cdot2^5}8=32+1+2+16=51$$", "meta": {"post_id": 2485531, "input_score": 24, "output_score": 47, "post_title": "How many ways are there to fill a 3 \u00d7 3 grid with 0s and 1s?"}}
{"input": "How would the Godel/Rosser incompleteness theorems look like from a computability viewpoint?\n\nOften people present the incompleteness theorems as concerning arithmetic, but some people such as Scott Aaronson have expressed the opinion that the heart of the incompleteness phenomenon is uncomputability, and that even Godel numbering (with the associated \u03b2-lemma) is not actually crucial. So are there purely computability-based proofs and discussion of the incompleteness theorems and related phenomena?\nI am also interested in knowing if there is a reference text containing this kind of discussion in a rigorous presentation (not blog posts).\nIn my answer below, I provide both computability-based statements and proofs of the generalized incompleteness theorems, and some reference texts. I was motivated to write this up because good quality self-answered questions are encouraged by both the StackExchange guidelines as well as community consensus.", "output": "Here I shall present very simple computability-based proofs of Godel/Rosser's incompleteness theorem, which require only basic knowledge about programs. I feel that these proofs are little known despite giving a very general form of the incompleteness theorems, and also easy to make rigorous without even depending on much background knowledge in logic. Typical proofs use something like the fixed-point lemma, which is essentially a fixed-point combinator 'applied' to provability, but that is quite a bit harder to understand and prove rigorously than the unsolvability of the halting problem. It suffices to say that all proofs use diagonalization in some way or another.\nTake any practical programming language L in which programs can perform basic operations on string/integer variables and conditional jumps (or while-loops). From now on all the programs that we shall refer to are programs in L. For convenience we shall also consider every string to be a program. If it is not ordinarily a valid program, we shall consider it to represent a program that simply does an infinite-loop. (Any interpreter for L can easily be modified to implement this.)\nFirst I shall show how the unsolvability of the halting problem implies essentially Godel's (first) incompleteness theorem.\nHalting problem\nDefine the halting problem to be:\n\u2003 Given a program P and input X:\n\u2003 \u2003 If P halts on X, then the answer is \"true\".\n\u2003 \u2003 If P does not halt on X, then the answer is \"false\".  \nIt is not hard to prove that there is no program that solves the halting problem. For a program to solve the halting problem, it must halt on every input (P,X), and also must output the right answer as specified in the problem. If you do not already know the proof, try before looking at the proof below!\nGiven any program H:\n\u2003 Let C be the program that does the following on input P:\n\u2003 \u2003 If H(P,P) = \"false\" then output \"haha\" (and halt) otherwise infinite-loop (not halting).\n\u2003 If H solves the halting problem:\n\u2003 \u2003 H(C,C) halts, and hence C(C) halts iff H(C,C) = \"false\", by definition of C.\n\u2003 \u2003 Contradiction with definition of H.\n\u2003 Thus H does not solve the halting problem.  \nKey definitions concerning formal systems\nTake any formal system T.\nWe say that V is a proof verifier for T iff all the following hold:\n\u2003 V is a program.\n\u2003 Given any sentence \u03c6 over T and proof x:\n\u2003 \u2003 V(\u03c6,x) decides (halts and answers) whether x is a proof of \u03c6.\nWe say that T can reason about programs iff:\n\u2003 For every program P that halts on an input X and outputs Y:\n\u2003 \u2003 T can prove the following for any string Z distinct from Y:\n\u2003 \u2003 \u2003 \"The program P halts on input X.\"\n\u2003 \u2003 \u2003 \"The program P halts on input X and outputs Y.\"\n\u2003 \u2003 \u2003 \"It is not true that the program P halts on input X and outputs Z.\"\n\u2003 \u2003 \u2003 (Here P,X,Y,Z are inserted as encoded strings, but you should get the idea.)\nWe say that T is consistent iff:\n\u2003 There is no sentence \u03c6 about programs such that T proves both \u03c6 and its negation.\nWe say that T is complete iff:\n\u2003 For every sentence \u03c6 about programs we have that T proves either \u03c6 or its negation.\nWe say that T is sound for program halting iff:\n\u2003 If T proves that a program halts on an input then it really halts.  \nGodel's incompleteness theorem via the halting problem\nTake any formal system T with proof verifier V that can reason about programs.\nLet H be the following program on input (P,X):\n\u2003 For each string s in length-lexicographic order:\n\u2003 \u2003 If V( \"The program P halts on input X.\" , s ) then output \"true\".\n\u2003 \u2003 If V( \"The program P does not halt on input X.\" , s ) then output \"false\".\nIf T is complete and consistent and sound for program halting:\n\u2003 Given any program P and input X:\n\u2003 \u2003 T proves exactly one of the following sentences:\n\u2003 \u2003 \u2003 A = \"The program P halts on input X.\"\n\u2003 \u2003 \u2003 B = \"The program P does not halt on input X.\"\n\u2003 \u2003 Thus H halts on input (P,X) because s will eventually be a proof of A or of B.\n\u2003 \u2003 If P halts on X:\n\u2003 \u2003 \u2003 T can prove A, since T can reason about programs, and hence H(P,X) = \"true\".\n\u2003 \u2003 If P does not halt on X:\n\u2003 \u2003 \u2003 T does not prove A, by soundness for program halting.\n\u2003 \u2003 \u2003 Thus T proves B, and hence H(P,X) = \"false\".\n\u2003 \u2003 Therefore H(P,X) is the correct answer to whether P halts on X.\n\u2003 Therefore H solves the halting problem.\n\u2003 Contradiction with unsolvability of the halting problem.\nTherefore T is either incomplete or inconsistent or unsound for program halting.\nRosser's strengthening of Godel's incompleteness theorem\nAfter Godel's theorem was published, Rosser came up with a trick to strengthen it, and I came across a blog post by Scott Aaronson that shows that if we use something called the zero-guessing problem, instead of the halting problem, we can get the same strengthening! Specifically, we can then drop the condition of soundness for program halting. I shall give a simplified self-contained version of the zero-guessing problem and the proof of Rosser's incompleteness theorem, exactly parallel to how the halting problem unsolvability implies Godel's incompleteness theorem. If you want a challenge, you can first read the definition of the zero-guessing problem and then try to find the proof on your own by following the ideas in the earlier proof.\nZero-guessing problem\nDefine the zero-guessing problem to be:\n\u2003 Given a program P and input X:\n\u2003 \u2003 If P halts on X, then the answer is 0 if P(X) = 0 and is 1 otherwise.\n\u2003 \u2003 (If P does not halt on X, then any answer is fine.)  \nFor a program to solve the zero-guessing problem, it must halt on every input (P,X), and also must output the right answer as specified in the problem. This implies that a solver is allowed to output arbitrary nonsense if P does not halt on X. Like the halting problem, the zero-guessing problem cannot be solved by a program. Try to prove this before looking at the proof below!\nGiven any program G :\n\u2003 Let D be the program that does the following on input P:\n\u2003 \u2003 If G(P,P) = 0 then output 1 otherwise output 0.\n\u2003 If G solves the zero-guessing problem:\n\u2003 \u2003 G(D,D) halts, and hence D(D) \u2260 0 iff G(D,D) = 0, by definition of D.\n\u2003 \u2003 Contradiction with definition of G.\n\u2003 Therefore G does not solve the zero-guessing problem.  \nNote that the choice of 0 and 1 is really arbitrary. You can assume 0 and 1 stand for any fixed distinct strings that you like.\nRosser's incompleteness theorem via the zero-guessing problem\nTake any formal system T with proof verifier V that can reason about programs.\nLet G be the following program on input (P,X):\n\u2003 For each string s in length-lexicographic order:\n\u2003 \u2003 If V( \"The program P halts on input X and outputs 0.\" , s ) then output 0.\n\u2003 \u2003 If V( \"It is not true that the program P halts on input X and outputs 0.\" , s ) then output 1.\nIf T is complete and consistent:\n\u2003 Given any program P and input X:\n\u2003 \u2003 T proves exactly one of the following sentences:\n\u2003 \u2003 \u2003 A = \"The program P halts on input X and outputs 0.\"\n\u2003 \u2003 \u2003 B = \"It is not true that the program P halts on input X and output 0.\"\n\u2003 \u2003 Thus G halts on input (P,X) because s will eventually be a proof of A or of B.\n\u2003 \u2003 If P halts on X:\n\u2003 \u2003 \u2003 Recall that T can prove the correct output of P on X.\n\u2003 \u2003 \u2003 If P(X) = 0, then T can prove A and hence G(P,X) = 0.\n\u2003 \u2003 \u2003 If P(X) \u2260 0, then T can prove B and hence G(P,X) = 1.\n\u2003 Therefore G solves the zero-guessing problem.\n\u2003 Contradiction with unsolvability of the zero-guessing problem.\nTherefore T is either not complete or not consistent.  \nExplicitly independent sentence\nIn the above proofs we used the unsolvability of some computability problem as a black-box to show that T is incomplete. In both cases, we can actually merge the unsolvability proof with the incompleteness proof to obtain explicit sentences that are independent over T, meaning that T can prove neither it nor its negation.\nLet H be the program constructed in the proof of Godel's incompleteness theorem.\nLet C be the program constructed in the proof that H does not solve the halting problem.\nLet Q = \"The program C halts on input C.\".\nLet Q' be the negation of Q.\nIf T is consistent and sound for program halting:\n\u2003 If C(C) halts:\n\u2003 \u2003 T proves Q but not Q', since T can reason about programs.\n\u2003 \u2003 Thus H(C,C) = \"true\", and hence C(C) does not halt.\n\u2003 \u2003 Contradiction.\n\u2003 Therefore C(C) does not halt.\n\u2003 Thus T does not prove Q, since T is sound for program halting.\n\u2003 If T proves Q':\n\u2003 \u2003 H(C,C) = \"false\", and hence C(C) halts.\n\u2003 \u2003 Contradiction as above.\n\u2003 Therefore T does not prove Q'.\n\u2003 Thus T proves neither Q nor Q', but Q' is actually true.  \nLet G be the program constructed in the proof of Rosser's incompleteness theorem.\nLet D be the program constructed in the proof that G does not solve the zero-guessing problem.\nLet W = \"The program D halts on input D and outputs 0.\".\nLet W' be the negation of W.\nIf T is consistent:\n\u2003 If D(D) halts:\n\u2003 \u2003 Recall that T can reason about programs.\n\u2003 \u2003 Thus T proves W if D(D) = 0 and T proves W' if D(D) = 1.\n\u2003 \u2003 Thus G(D,D) = D(D), by definition of G.\n\u2003 \u2003 But D(D) \u2260 G(D,D), by definition of D.\n\u2003 \u2003 Contradiction.\n\u2003 Therefore D(D) does not halt.\n\u2003 If T proves W or W':\n\u2003 \u2003 G(D,D) halts, and hence D(D) halts.\n\u2003 \u2003 Contradiction as above.\n\u2003 Therefore T proves neither W nor W', but W' is actually true.  \nFrom the computability perspective, although T is complete for program halting (since it can reason about programs), it is incomplete for program non-halting (some program on some input will not halt and T cannot prove that fact).\nZero-guessing versus halting\nNote that we avoid the need for T to be sound for program halting in the above proofs based on the zero-guessing problem because it has a weaker requirement than the halting problem in the case when the given program P does not halt on the given input X.\nSoundness versus consistency\nIt is customary to assume that T is classical (for programs), meaning that T can use the rules of classical logic in deducing sentences about programs. But the above proofs do not assume that T is classical. Note that if T is classical (or at least has the principle of explosion), then soundness of T for any sentence would imply consistency of T, because if T is inconsistent then T proves every sentence about programs. Also, if T is classical then its soundness for program halting is stronger than its consistency, because it is possible (which we shall prove in a later paragraph) that T proves a sentence of the form \"The program P halts on input X.\" and yet P actually does not halt on input X in reality. Note also that for classical T, soundness of T for program non-halting is equivalent to just consistency of T, because if a program P actually halts on input X in reality, then T can prove that fact, and so if T proves \"The program P does not halt on input X.\" then T is also inconsistent.\nAnother fact for classical T is that T is consistent iff Q' is true. We have already shown that if T is consistent then Q' is true. If T is inconsistent and classical then by the principle of explosion T proves both Q and Q', and so H(C,C) halts, and hence C(C) does not halt. Similarly if T is classical then T is consistent iff D(D) does not halt.\nTo show the claim in the first paragraph, we need to construct a formal system that is classical and consistent but yet unsound for program halting. To do so, let S be a formal system that can perform merely classical reasoning about finite binary strings, and hence can reason about programs because it can reason about the execution of any program for any finite number of steps. (We shall explain in a later section how S can do this. Of course we need to translate sentences about programs to sentences about finite binary strings, but there is a natural computable translation.) We believe that S is consistent and sound for finite binary strings, and hence is also sound for program halting. We have already seen that S proves neither Q nor Q', but Q is actually false. Now let S' be S+Q, namely that S' is the formal system that proves every theorem that can be deduced classically from the axioms of S plus Q. Then S' is classical and has a proof verifier (exercise for you) and is unsound for program halting. But S' is consistent, because otherwise there is a proof of contradiction over S+Q, which can be converted into a proof of Q' over S. This last claim is an instance of the deduction theorem, which is obvious for Fitch-style natural deduction.\nGodel's original theorem required T to be \u03c9-consistent, but his proof in fact only requires T to be \u03a31-sound. By a trick of Godel's called the \u03b2-lemma, \u03a31-soundness is essentially equivalent to soundness for program-halting. So in this precise sense one can say that the weaker theorem is essentially equivalent to the theorem shown by Godel's original proof. Actually soundness for program halting is always taken for granted for any formal system that we use in practice, since we would really want it not to prove false sentences about programs. But the stronger theorem is beautiful from the modern CS perspective, since it reveals severe fundamental limitations in any consistent formal system that can reason about finite program execution, which is a very concrete notion in the real world!\nEncoding program execution in a string\nIn this section we explain how program execution can be expressed in a single finite binary string, so that we can use sentences over these strings to describe program behaviour, including halting. First notice that binary is not a severe restriction, and there are many ways to go around it. The simplest way is to use unary numbers (k is encoded as a string of k ones) separated by zeroes to represent finite strings of naturals! This representation gives a one-to-one correspondence between finite binary string to natural strings. Next observe that we can represent finite sequences of natural strings using a single natural string, by adding one to each item and using zeroes as separators. For example the sequence ((3,1,4),(1),(),(5,9)) would be represented by (4,2,5,0,2,0,0,6,10). Now every program can be represented easily as a natural string. Furthermore, the entire state of any given program executing on a given input can be captured, by a pair of natural strings representing the program and input with the 'current step' highlighted, plus a sequence of pairs of natural strings where each pair (x,v) denotes that variable x has value v. So the entire program execution state can be represented by a single natural string. If L is simple enough, you should be able to imagine how to express by a classical sentence about strings s,t that s\u2192t is a valid program state transition, meaning that after one step from state s the program will be in state t. Since a finite sequence of program states can be represented as a single natural string, we can express by a classical sentence about strings p,x,y that the program p on input x will halt and output y, which is basically the sentence ( There is a finite sequence of program states, starting with program p having input x, in which every pair of adjacent states in that sequence is a valid state transition, and ending with output y. ). Finally, note that if a program p really halts on input x, then any formal system S that can reason about natural strings can prove each step of the execution of p on x, and then string all these proofs together to prove the fact of halting.\nIn the last part of the above paragraph, we appealed slightly to our intuition that we can do the appropriate translation of that sentence given any conceivable practical programming language. It is not really illuminating to completely formalize this, but it could be done in many ways. One way is to do it only for some specific universal flavour of Turing machines. Another is to do it only for some specific assembly-like language. A third is to do it for some variant of lambda calculus. Whatever it is, it has to be equivalent to Turing machines. This issue is not peculiar to this version of the incompleteness theorem, since the original theorem concerns systems that can reason about basic arithmetic, which turns out to be equivalent due to Godel's \u03b2-lemma. The reason Godel's theorem was about arithmetic seems to be because that was what mathematicians at that time thought was fundamental to mathematics. The main benefits of proving my version of the theorem are that it avoids the number theory in Godel's \u03b2-lemma and the concept of primitive recursion, and shows that just basic facts of string concatenation (not even any form of induction) are enough for the incompleteness phenomenon to arise, and does not require the formal system to be based on classical logic.\nPopular misconceptions about incompleteness\nWhen people first encounter the statement about incompleteness of Peano Arithmetic (PA), many wrongly suspect various aspects of PA as being the 'cause'.\nIt is not due to induction, nor even the infinite number of axioms of PA. The reason is that PA\u2212 is enough, and PA\u2212 has finitely many axioms. PA\u2212 plus induction gives PA. Similarly, the Theory of Concatenation (TC) described here, which is a simple candidate for the formal system S above that can merely reason about finite binary strings, has only finitely many axioms.\nIt is not due to any deep number-theoretic phenomenon. I myself wrongly thought that it was, because Presburger arithmetic is consistent and complete, until I saw the computability-based proof above, which applies to TC since TC can reason about programs. It is true that TC is in some sense equivalent to PA\u2212, but TC has nothing but concatenation, and the axioms of TC are just a few 'obvious' facts about strings.\nIt is not due to classical logic. This a common 'criticism' of the incompleteness theorems, but is completely unfounded. As shown in the proof above, it applies to any formal system that has a proof verifier and can reason about programs, whether or not it is classical. Notice that nowhere did I say anything about syntactic or deductive rules, because there need not be any. The formal system T could even be so crazy that if we are given an arbitrary program that halts on some input the easiest way to find a proof of that fact \u03c6 over T would be to run V(\u03c6,x) for every possible string x until you find one that V says is valid! For a trivial but relevant example, consider the formal system R (for \"runner\") whose proof verifier does the following given input (\u03c6,k): It first checks if \u03c6 is of the form \"[It is not true that] the program P halts on input X [and outputs Y].\" (where the phrases in square brackets are optional), and then runs P on X for length(k) steps, and then answers that the proof is valid if P has halted [and its output is as claimed], and answers that the proof is invalid in all other cases. You can see that R indeed halts on every input (\u03c6,k), and only affirms validity of the so-called proof when \u03c6 is actually true and k is long enough. You can also see that R does not affirm validity of any proof when \u03c6 is of some other form or P does not halt. Thus R satisfies the requirements of the above incompleteness theorem. If you wish, you can let R* be the closure of R under intuitionistic deduction, and then R* would be an intuitionistic non-classical example.\nIn my opinion the phenomenon actually 'responsible' for causing incompleteness is the ability to reason about programs. Someone once said that Godel's proof was essentially like constructing a compiler in arithmetic, just so that he could run primitive recursive programs (those that only use for-loops whose counter cannot be changed inside the loop).\nFinally, the foundational system needed to prove the incompleteness theorems can be very weak. A key assumption is that program behaviour is well-defined, namely that given any program P and input X, either P halts or P does not halt, and the output if any is unique. This assumption is necessary otherwise even the concepts of consistency and completeness are not well-defined. In short, having classical logic for program behaviour suffices. Note that since program behaviour can be encoded as a sentence about strings (as in the previous section), this implies that in some sense we only need to assume classical logic for strings to be able to prove the incompleteness theorems in an encoded form. If you want to prove it in a more natural form, then you would need the foundational system to natively support reasoning about finite sequences.\nGeneralization\nWe can fully generalize the incompleteness theorems by relaxing the condition that the formal system T has a proof verifier V that always halts. It is sufficient to require that V(\u03c6,x) outputs \"yes\" exactly when x is a proof of \u03c6, and it does not matter if V does not halt when x is not a proof of \u03c6! The proof is the same except that you simply need to construct the program to parallelize all the calls to V. In any reasonable programming language, this can be done as follows. Each call to V triggers a step-by-step simulation of the execution of V on the given inputs in parallel with the rest of the program, so at any time there may be multiple (but finitely many) ongoing simulations. If any simulation reaches the end, the entire program is terminated and the output in that simulation is returned as the output of the entire program. As before, the proof will show that if T is consistent then exactly one of A,B will be proven and so the output will correspond to which one is proven, and the rest of the proof is unchanged.\nNote also that this full generalization is equivalent to replacing the criterion of T having a proof verifier with the criterion of T having a theorem generator M that is a program that runs forever and eventually outputs every theorem of T and does not output any string that is not a theorem of T. Then the program in the proof merely needs to simulate M and wait until M generates A or B and then terminating the entire program and outputting accordingly. In the above proofs I did not use the full generalization because it is not obvious what kind of programming languages are strong enough that programs in them can simulate other programs, and all practical formal systems do have a proof verifier anyway.\nGeneralization to uncomputable formal systems\nOne nice aspect of this computability-based viewpoint is that it automatically relativizes to any kind of class \u03a9 of oracle programs. In particular, the same proof yields the incompleteness theorems for formal systems whose proof verifier is a program in \u03a9 and that can reason about programs in \u03a9. This result can be used to prove that the arithmetical hierarchy does not collapse to any level, like shown in this post.\nFurther reading\nThe blog post by Scott Aaronson that inspired some of this, citing Kleene's 1967 Mathematical Logic text for a similar proof of Rosser's theorem (Theorem VIII and Corollary I on pages 286\u2212288).\nA 1944 paper by Emil Post sketching a proof corresponding loosely to the above proof via the halting problem for formal systems that are sound for program halting. (Thanks Philip White!)\nA formal version of the above proof of Rosser's theorem for formal systems that interpret PA\u2212.\nA discussion of foundational issues regarding the halting problem and the incompleteness theorem.\nAn explanation of the fixed-point combinator in \u03bb-calculus mentioned in the opening paragraph.", "meta": {"post_id": 2486348, "input_score": 35, "output_score": 51, "post_title": "Computability viewpoint of Godel/Rosser's incompleteness theorem"}}
{"input": "Could the sum of an even number of distinct odd numbers be divisible by each of the odd numbers ?\nLet $k\\geq 4$ be an even number. Can one find $k$ distinct positive odd numbers $x_1,\\ldots,x_k$ such that each $x_i$ divides $S = \\sum_{i=1}^k x_i$ ?\nIs it possible at least for $k$ big enough ?", "output": "Yes, it is possible.  Divide your sum by $S$ and you have \n$$1=\\sum_i \\frac {x_i}S$$\nwhich is an Egyptian fraction expression of $1$ where all the denominators have the same number of factors of $2$.  This is known to be solvable with all denominators odd, but all known solutions have an odd number of terms.   A survey paper is here.  The section of interest is $9.5$.  One example is: \n$$1=\\frac 13+\\frac 15+\\frac 17 + \\frac 19+\\frac 1{11}+\\frac 1{15}+\\frac 1{35}+\\frac 1{45}+\\frac 1{231}$$ \nwhere the denominators have least common multiple $3465$ so we can write:\n$$3465=1155+693+495+385+315+231+99+77+15$$ \nwith every term dividing the sum.  Now if we add $3465$ to each side we have a solution with an even number of terms:\n$$6930=3465+1155+693+495+385+315+231+99+77+15$$\nAny Egyptian fraction decomposition of $1$ into fractions with odd denominators yields a solution to your problem.  The sum will be twice the least common multiple of the denominators in the decomposition.  The paper shows that there is such a decomposition for all odd numbers of terms $9$ or above.  You can multiply any solution by any odd number to get another.  \nWhat is happening is we are converting the Egyptian fraction decomposition of $1$ with all denominators odd into one that looks like $$1=\\frac 12+\\frac12\\left(\\text{all other terms}\\right)$$", "meta": {"post_id": 2496631, "input_score": 48, "output_score": 89, "post_title": "Could the sum of an even number of distinct positive odd numbers be divisible by each of the odd numbers?"}}
{"input": "Consider the first example using repeated l'H\u00f4pital:\n$$\\lim_{x \\rightarrow 0} \\frac{x^4}{x^4+x^2} = \\lim_{x \\rightarrow 0} \\frac{\\frac{d}{dx}(x^4)}{\\frac{d}{dx}(x^4+x^2)} = \\lim_{x \\rightarrow 0} \\frac{4x^3}{4x^3+2x} = ... =  \\lim_{x \\rightarrow 0}\\frac{\\frac{d}{dx}(24x)}{\\frac{d}{dx}(24x)} = \\frac{24}{24}=1 $$\nConsider the following example using a different method:\n$$ \\lim_{x \\rightarrow 0} \\frac{x^4}{x^4+x^2} = \\lim_{x \\rightarrow 0}\\frac{\\frac{x^4}{x^4}}{\\frac{x^4}{x^4}+\\frac{x^2}{x^4}} = \\lim_{x \\rightarrow 0} \\frac {1}{1 +\\frac{1}{x^2}} = \\frac {1}{1+\\infty} = \\frac{1}{\\infty}=0 $$\nThe graph here clearly tells me the limit should be $0$, but why does l'H\u00f4pital fail?", "output": "$$\\lim_{x \\rightarrow 0} \\frac{x^4}{x^4+x^2} = \\lim_{x \\rightarrow 0} \\frac{\\frac{d}{dx}(x^4)}{\\frac{d}{dx}(x^4+x^2)} = \\lim_{x \\rightarrow 0} \\frac{4x^3}{4x^3+2x} = \\lim_{x\\to0} \\frac{12x^2}{12x^2+2} = \\frac{0}{0+2} = 0$$\nThere. You can't apply l'Hospital there because the denominator doesn't go to $0$.", "meta": {"post_id": 2499036, "input_score": 19, "output_score": 62, "post_title": "l'H\u00f4pital vs Other Methods"}}
{"input": "Set-theory is widely taken to be foundational to the rest of mathematics. So is category-theory. My question is: Are they two alternative, rival candidates for the role of a foundational theory of mathematics or is there a sense in which one is more fundamental than the other? 'Fundamentality' can be cached out in terms of expressive power. So another variant on the same question: Do category and set theories share the same expressive power? If not, which one is more expressive?", "output": "I think questions like these are often asked by people who don't have a clear/coherent idea of what foundations are and what purpose they serve. This isn't meant as some kind of insult. I think many, probably the majority, of mathematicians are in this situation1. Specifically, I believe that if you asked most mathematicians which \"foundations\" they use, they'd say \"set theory\". If you asked which set theory, they'd say ZFC. If you then asked, what are the axioms of ZFC, they'd have trouble listing them out by name, let alone explicitly giving the axioms. As the coup de gr\u00e2ce, if you ask them why they chose ZFC over, say, Tarski-Grothendieck set theory (TG), or more generally set theory versus category theory or type theory, it will become clear that they didn't make that choice (e.g. because they are not even aware what the alternatives are). They didn't make any choice. In fact, I think most will outright say that they were told that set theory and specifically ZFC can serve as a foundation of mathematics, and they just take it for granted that everything they do can be formulated in ZFC and don't otherwise concern themselves with the issue.\nThe reason they can do this is that most theorems don't significantly depend on the choice of foundations. Or to put it another way, most theorems of interest to mathematicians can be proven even in fairly weak foundations. You can see this in the exercise of Reverse Mathematics which tries to work out what axioms are actually used by typical theorems.\nTurning to your question more specifically. There are a lot of issues, some of which are alluded to in the previous paragraphs. First, which set theory do you mean? There are several named systems as well as many, many more you could create. Similarly, for category theory, though here there aren't too many named systems. Most categorists (let alone mathematicians in general) are perfectly happy to say that category theory is grounded in some set theory, e.g. Mac Lane in \"Categories for the Working Mathematician\" and Grothendieck. For their purposes, like most other mathematicians, it's just not important what the foundations actually are. Roughly speaking, they are just going to assume the categories they need exist, and they'll take any foundations that establish them. That said, choice of foundations actually matters here. The category of (ZFC) sets does not exist in ZFC. Typically, to formulate category theory as used into something like ZFC requires axiomatically adding inaccessible cardinals or even Grothendieck universes. On the other hand, the way category theory is typically used already assumes set theory. If you want a foundational system on par with set theory, you can use the Elementary Theory of the Category of Sets (ETCS). ETCS is equivalent to Bounded Zermelo set theory (BZ) which is weaker than ZFC. Really, most people when they talk about category theory serving as a \"foundation\" for mathematics, usually say things like \"practical foundations\" and they mean something like category theory can serve as a framework for organizing mathematics. Set theory is just a rung in this framework, not a competing system from this perspective. Often \"categorical foundations\" really means \"topos theoretic foundations\" or closely related concepts, e.g. via the free topos. Pushed further, you may get some \"competition\" between set theory and, really, type theory which is its own approach to foundations, but is intimately related to category theory. There are aspects about typical set theoretic reasoning that are a bit anathema to type theory and category theory.\nThe next issue is you talk about \"expressive power\" but you don't really define it. This isn't necessarily as straightforward as you might think. For example, type theory and set theory are different sorts of things. Type theory is more like an extension of logic, while set theory is usually presented as a first-order theory within classical first-order logic. This is less of an issue for category theory, where e.g. ETCS is also a first-order theory of classical first-order logic. Nevertheless, let's continue on the assumption of some workable notion of \"expressive power\". Your question gives the impression that you believe that \"expressive power\" gives a total ordering on theories. It can easily be that neither is \"more expressive\", i.e. that two theories are incomparable. Most pertinently from a philosophical perspective, the entire approach suggested is a backwards. From a philosophical perspective, you decide what \"mathematical objects\" or \"mathematics\" is, and then you find/make a foundations that reifies that understanding. Less \"expressive\" foundations are then overly conservative, while more \"expressive\" foundations are making unjustified assumptions (and incomparable foundations are grounds for a holy war). For example, consider constructivists. They have a take on what \"doing math\" means which leads them to reject the law of excluded middle. Classical logics that accept the law of excluded middle are therefore trivially more \"expressive\", in that you can prove more theorems, but this is a defect from a constructivist's perspective. Some constructivists go further and assume anti-classical axioms which leads to incomparable foundations. Personally, I think mathematicians are missing a lot of mathematical value by constantly working in overly powerful foundations. At any rate, G\u00f6del's Incompleteness theorem guarantees that there is always a \"more expressive\" foundations. A line needs to be drawn somewhere.\nFinally, I wonder what you plan to do with an answer to your question. Let's say I said \"set theory is more 'fundamental' than category theory\". Now what? Are you not going to learn category theory then? That would be as absurd as deciding not to learn differential geometry because set theory is \"more fundamental\". My impression is that nowadays most mathematicians have a cosmopolitan attitude toward foundations (usually via apathy, but even when restricting to those who do care). There are many foundations, and it's interesting to see how they relate and how each views the mathematical landscape. Shifting between approaches can be quite useful. For example, consider Synthetic Differential Geometry (SDG). Much of its development was motivated by category theoretic (specifically topos theoretic) thinking. By using the tool of internal languages, we can make a constructive type theory in which we can do differential geometry in a way that looks a lot like \"normal\" math (we just need to be careful to use constructive reasoning) but where \"magical\" and extremely handy things exist. For example, there is a type, $D$, that's analogous to $\\{x\\in\\mathbb{R}\\mid x^2=0\\}$ but, in this type theory, is distinct from $\\{0\\}$ (which is an anti-classical result). With $D$, a tangent vector in a manifold $M$ is just a function $D\\to M$, thus the tangent bundle of $M$ is just the type of functions, $M^D$. This approach can dramatically simplify the proof of some results. Basically, things which are intuitions in classical differential geometry are theorems in SDG.2 For example, elements of $D$ behave like \"infinitesimals\" to some degree, e.g. the derivative of $f$ is defined to be the unique function $f'$ such that $\\forall d\\in D.f(x+d)=f(x)+f'(x)d$. Of course, we'll want to connect this back to classical differential geometry which we can do with the notion of a well-adapted model. The end result is we can prove many (but not all) results of classical differential geometry by using category theory as a bridge to a constructive type theory where these results are much easier to prove, and, via a \"meta-theorem\", we are assured that there is a \"classical\" proof of the result, but we don't need to find it and it is likely much uglier. Doing things like this is a much more valuable use of \"foundations\" than trying to rank-order them in terms of \"fundamentality\".\n1 The ones that aren't are likely logicians, set theorists, or type theorists, or at least have gone a decent ways beyond an introduction to these fields.\n2 A similar thing happens with synthetic topology and Homotopy Type Theory.", "meta": {"post_id": 2509877, "input_score": 23, "output_score": 59, "post_title": "Are category-theory and set-theory on the equal foundational footing?"}}
{"input": "In the definition of a ring $R$, one has\n\n$a(b+c) = ab + ac$ and \n$(a+b)c = ac + bc$ \n\nfor all $a,b,c\\in R$\nMy question is (just out of curiosity) if one really needs both of these. I can't think of an example of something that is not a ring that only satisfies one of the sides of the distributive law. So can one prove that if $a(b+c) = ab + ac$ for all $a,b,c$, then $(a+b)c = ac + bc$ for all $a,b,c$.\nEdit: I maybe should add that all rings in my definition have a unity $1$.", "output": "Here is an example that fails precisely in left distributivity.\nConsider $\\mathbb{R}[X]$ - the polynomials with coefficients from $\\mathbb{R}$ with the usual operation of pointwise addition (in fact, the ring of scalars is irrelevant here).\nThe tricky part is how we define multiplication: let $p \\cdot q$ be the composition $p \\circ q$. This multiplication is associative, and even has an identity, which is the identity polynomial $p(x)=x$.\nNow, trivially $$(p_1 + p_2) \\circ q = p_1 \\circ q + p_2 \\circ q,$$ but in general $$p \\circ (q_1 + q_2) \\color{red} \\neq p \\circ q_1 + p \\circ q_2.$$", "meta": {"post_id": 2529177, "input_score": 32, "output_score": 65, "post_title": "Question about distributive law in definition of a ring"}}
{"input": "Let $f$ be a Riemann Integrable function over $\\mathbb{R}^2$. When can we do this?\n$$\\frac{\\partial}{\\partial\\theta}\\int_{a}^{b}f(x,\\theta)dx=\\int_{a}^{b}\\frac{\\partial}{\\partial\\theta}f(x,\\theta)dx$$\n(Here, $a$ and $b$ are not a function of $\\theta$.)\nIn the problem, which I am solving recently, are like this:\n$f_{\\theta}(x)$, here $\\theta$ is constant and $\\theta\\in\\mathbb{R}$ (usually). For example $f_{\\theta}(x)=x^2\\theta$. So, I am blindly interchanging integration and differentiation because of continuity over $\\theta$. But I want to know little bit more.\nAlso, what happens if $a$ and $b$ are function of $\\theta$? Thanks.", "output": "You may interchange integration and differentiation precisely when Leibniz says you may.  In your notation, for Riemann integrals: when $f$ and $\\frac{\\partial f(x,t)}{\\partial x}$ are continuous in $x$ and $t$ (both) in an open neighborhood of $\\{x\\} \\times [a,b]$.\nThere is a similar statement for Lebesgue integrals.", "meta": {"post_id": 2530213, "input_score": 52, "output_score": 45, "post_title": "when can we interchange integration and differentiation"}}
{"input": "Someone deduced without using complex analysis that \n$$ \\int \\frac{\\pi(t)}{t^2} \\mathrm{d}t \\sim \\log\\log t $$\nwhere $\\pi$ is the prime counting function.\nBy differentiating the above, he then arrives at\n$$\\frac{\\pi(t)}{t^2} \\sim \\frac{1}{t\\log t} $$ which is exactly the Prime Number Theorem.\nHowever, he feels that something should be wrong with this approach, but not sure exactly what ?", "output": "The reasoning is flawed because $f\\sim g$ most certainly does NOT imply $f\u2019\\sim g\u2019$.\nFor example, take $f(x)\\equiv 0$ and $g(x)= \\frac1N \\sin N^2x$.", "meta": {"post_id": 2538105, "input_score": 13, "output_score": 34, "post_title": "Why shouldn't this prove the prime number theorem?"}}
{"input": "My understanding of power series turns out to be less-well-formed than I thought.  To confess, I took my two courses in analysis in grad school (one real, one complex) and got out.  \nSince this is my Calc II class, let's keep everything in real variables, please.  It's not hard to derive the power series for $\\arctan(x)$ as\n$$\n\\arctan(x) = \\sum_{n=0}^\\infty \\frac{(-1)^n}{2n+1} x^{2n+1}, \\ -1 \\leq x \\leq 1.\n$$\nAlso not hard to work out the interval of convergence for the right-hand side.  So far, so good.\nHere's my question and why I suddenly see how naive I am.  I tend to think of $\\arctan$ as an incredibly nice function, so I expect its power/Taylor series to converge everywhere.  In short, I view $\\arctan$ as being just as nice as $f(x) = e^x$, whose power series representation converges everywhere (domain of the power series matches the domain of the function).  Same story for $\\sin(x)$ and $\\cos(x)$.  They're \"nice\" so their power series converge on their entire domain. \nWhen the power series for something like $\\ln (x)$ or $\\frac{1}{x}$ has finite radius, I'm completely fine with that as there is an obvious discontinuity that you bump into as you work your way out from the center.  But why does the power series for $\\arctan(x)$ have a finite radius?  I know that something goes wrong with Taylor's remainder and this is what prevents the series from representing $\\arctan(x)$ everywhere, but I would appreciate an explanation from the point of view of properties of $\\arctan(x)$ and not its power series:  what is it about $\\arctan(x)$ that prevents its power series from being optimally \"nice\"?", "output": "Your insistence \"let's keep everything in real variables, please\" is precisely the problem: the cause of the finite radius of convergence is due to the function's behavior in $\\mathbf C$, not $\\mathbf R$.\nA much simpler example than $\\arctan x$ is $1/(1+x^2)$, which is defined and infinitely differentiable on the whole real line but its power series at $0$ (a geometric series with $-x^2$ in place of $x$) has radius of convergence $1$, not $\\infty$. To use your language, \"there is an obvious discontinuity that you bump into as you work your way out from the center,\" namely at $x = \\pm i$ where the function blows up. In fact, if you expand $1/(1+x^2)$ into a power series at a real number $a$, not necessarily at $0$, the radius of convergence will be $\\sqrt{a^2+1} = |a-i|$ -- the distance from the center out to $i$. This phenomenon is bewildering if you refuse to use complex numbers and extremely clear if you use them. Choose wisely.\nIf $f(x)$ is a rational function in reduced form with a nonconstant denominator and its denominator does not vanish at $a$, its power series at $a$ has radius of convergence $|a-\\rho|$ where $\\rho$ is the root of the denominator in $\\mathbf C$ that is closest to $a$. This simple geometric result can not be explained in terms of real variables if the roots of the denominator are not all real. \nTo reinforce how poorly the real numbers are compared to the complex numbers as a predictive tool for the radius of convergence, there are functions $\\mathbf R \\rightarrow \\mathbf R$ that are infinitely differentiable on the whole real line but their power series at each real number $a$ has radius of convergence zero for all $a$ in $\\mathbf R$. \nStrictly speaking, the real numbers have enough information in principle to compute the radius of convergence $R$ of a power series $\\sum c_n(x-a)^n$ with all real coefficients using Hadamard's formula $1/R = \\varlimsup\\limits_{n\\to\\infty} \\sqrt[n]{|c_n|}$, but this formula is often not feasible to compute in practice.", "meta": {"post_id": 2539520, "input_score": 34, "output_score": 41, "post_title": "Power series representation of arctangent: fails to converge everywhere"}}
{"input": "$\\mathbf{Theorem 2.14:}$\nLet $A$ be the set of all sequences whose elements are the digits $0$ and $1$. Then A is uncountable, meaning there does not exist a one-to-one mapping of A onto $\\mathbb{Z}$.\nFor reference, elements of $A$ have this form $(0,1,0,1,0,0,0,1,1,1,1,1,\\cdots)$\n$\\mathbf{Question:}$ Please bear with me. I know I'm wrong I don't know why. What is wrong with this logic?\nLet $E_1$ be the set containing all the sequences with just one $1$ in the sequence. i.e. $E_1 = \\left\\{(1,0,0,\\cdots),(0,1,0,0,\\cdots),(0,0,1,0,0,\\cdots),\\cdots\\right\\}$. $E_1$ is countable.\nLet $E_{2k}$ be the set containing all the sequences with only two $1$'s in the sequence where the next $1$ in the sequence is $k$ units next to the first $1$.\n$E_{21} = \\left\\{(1,1,0,\\cdots),(0,1,1,0,0,\\cdots),(0,0,1,1,0,0,\\cdots),\\cdots \\right\\}$\n$E_{22} = \\left\\{(1,0,1,0,\\cdots),(0,1,0,1,0,\\cdots),(0,0,1,0,1,0,\\cdots),\\cdots \\right\\}$\nLet the union of the sets $E_{2k}$ for ($k=1,2,3,\\cdots$) be called $E_2$. $E_2$ is countable.\nLet $E_{3ij}$ be the set of sequences with only three $1$'s where the second $1$ is $i$ units away from the first $1$, and the third $1$ is $j$ units away from the second.\nFor example:\n$E_{311} = \\left\\{(1,1,1,0,\\cdots),(0,1,1,1,0,\\cdots),(0,0,1,1,1,0,\\cdots),\\cdots\\right\\}$\n$E_{312} = \\left\\{(1,1,0,1,\\cdots),(0,1,1,0,1\\cdots),(0,0,1,1,0,1,0,\\cdots)\\cdots\\right\\}$\n$E_{322} = \\left\\{(1,0,1,0,1,\\cdots),(0,1,0,1,0,1,0\\cdots),(0,0,1,0,1,0,1\\cdots)\\cdots\\right\\}$\n$E_{333} = \\left\\{(1,0,0,1,0,0,1\\cdots),(0,1,0,0,1,0,0,1,\\cdots),(0,0,1,0,0,1,0,0,1,\\cdots)\\cdots\\right\\}$\nLet $E_3$ be the union of all the sets $E_{3ij}$ with ($i,j=1,2,3,\\cdots$).\nSince each $E_{3ij}$ is countable the union of them is countable, so $E_3$ is countable.\nThe zero sequence is just one sequence, $E_1$ is countable, $E_2$ is countable, $E_3$ is countable. Now if we continue defining the sets in this manner then each set $E_k$ (where $k$ is the number of $1$'s in the sequences) will be a countable set. \nSo, the union of all these sets will be countable and it will represent all the sequences in A.\nThank you in advance to anyone who reads all of this and answers!", "output": "Your union of $E_{ijk\\cdots}$ does not contain the following element\n$$(0,1,0,1,0,1,0,1, \\cdots).$$\nIndeed those $E$'s contain elements in $A$ with finitely many nonzero entries. You have just proved: \n\nThe subset of $A$ consisting of all sequences with finitely many $1$'s is countable.", "meta": {"post_id": 2549971, "input_score": 37, "output_score": 34, "post_title": "Rudin's Principle of Mathematical Analysis Theorem 2.14 Question"}}
{"input": "I was recently asked by a student whether there exists a topological space which is not compact, but in which every proper open subset is compact. I haven't been able to give an example, or a proof that no such space exists. So far the best I've been able to show is that such a space cannot contain a closed compact subset (in particular, it cannot be T1).", "output": "Consider $\\Bbb N$, with proper open sets given by $U_n = \\{x: x\\le n\\}$ and the empty set.  Arbitrary unions of the $U_n$ are open, (either given by $U_{m}$ the maximum of the $n$ or by $\\Bbb N$ if the $n$ are unbounded), as too are finite intersections.\nHere every proper open set is finite, and thus trivially compact.  However, it is easy to see that $\\Bbb N$ itself is not compact with this topology, since it is covered by the collection of all proper open subsets, which admits no finite subcover.", "meta": {"post_id": 2551645, "input_score": 35, "output_score": 60, "post_title": "$\\exists$ a non-compact space in which every proper open subset is compact?"}}
{"input": "On the first page of the classical book \"Ordinary Differential Equations\" by Jack Hale (Revised Edition, 1980) there is the following definition:\n\nAn abstract linear vector space (or linear space) $\\mathcal{X}$ over $\\mathbb{R}$ is a collection of elements $\\{x,y,\\ldots\\}$ such that for each $x,y \\in \\mathcal{X}$, the sum $x+y$ is defined, $x+y \\in \\mathcal{X}$, $x+y=y+x$ and there is an element $0 \\in \\mathcal{X}$ such that $x+0=x$ for all $x \\in \\mathcal{X}$. Also, for any number $a,b \\in\\mathbb{R}$, scalar multiplication $ax$ is defined, $ax \\in \\mathcal{X}$ and $1 \\cdot x = x$, $(ab)x=a(bx)=b(ax)$, $(a+b)x=ax+bx$ for all $x,y \\in \\mathcal{X}$.\n\nThe terminology linear vector space is the same as vector space (i.e., without the adjective linear)? I am asking this because a classical axiom of vector spaces is missing here: given an $x \\in \\mathcal{X}$ there is an element $z \\in \\mathcal{X}$ such that $x+z=0$, where the element $0$ was defined above.\nQuestion improvement: with respect to the definition of vector space, more axioms seem to be missing too, namely the associativity under $+$ and the scalar distributivity as $a(x+y) = ax + ay$. This was mentioned by more than one comment/post of contributors. \nWhy is that?", "output": "Here is a counterexample.  Consider $\\mathcal{X}=\\{0,1\\}$, with addition defined by $x+y=\\max(x,y)$ and scalar multiplication defined by $ax=x$ for all $a\\in\\mathbb{R}$ and $x\\in\\mathcal{X}$.  This satisfies all of Hale's axioms, but $1$ has no additive inverse.\nHere's a slightly less trivial example.  Consider the set $\\mathcal{X}=\\mathbb{R}\\cup\\{z\\}$, with addition and scalar multiplication defined as usual for elements of $\\mathbb{R}$, $x+z=z+x=x$ for all $x\\in\\mathcal{X}$, and $az=z$ for all $a\\in\\mathbb{R}$.  This satisfies the given axioms, with $z$ as the zero element.  However, no element other than $z$ has an additive inverse.\n(In fact, any example without additive inverses contains a copy of the first counterexample.  If $\\mathcal{X}$ satisfies Hale's axioms and $x\\in\\mathcal{X}$ has no additive inverse, then $\\{0,0\\cdot x\\}\\subseteq \\mathcal{X}$ will be closed under addition and scalar multiplication and isomorphic to the first example, sending $0\\cdot x$ to $1$.  We must have $0\\cdot x\\neq 0$ since $x+(-1)\\cdot x=0\\cdot x$ so $x$ would have an additive inverse if $0\\cdot x=0$.)\n\nNote, though, that additive inverses aren't the only axiom that is missing.  Associativity of $+$ and $a(x+y)=ax+ay$ are missing too!  Here's an example that has additive inverses but which fails associativity.  Let $\\mathcal{X}=\\mathbb{R}\\times\\{0,1\\}\\setminus\\{(0,1)\\}$.  We define addition by $(x,i)+(y,j)=(x+y,\\max(i,j))$ and scalar multiplication by $a(x,i)=(ax,i)$, except that if either operation gives an output of $(0,1)$, we change it to $(0,0)$ instead (so for instance, $0(x,1)=(0,0)$ for any $x$).  This satisfies Hale's axioms, and has additive inverses ($(-x,i)$ is the inverse of $(x,i)$).  However, it fails associativity, since $$((x,0)+(-x,0))+(x,1)=(0,0)+(x,1)=(x,1)$$ whereas $$(x,0)+((-x,0)+(x,1))=(x,0)+(0,0)=(x,0)$$ for any $x\\neq 0$.\n\nThe author almost certainly does not intend to give a different definition from the usual one, though--this is just an error in the book.  It is definitely not standard to use the term \"linear vector space\" to refer to this weaker definition.", "meta": {"post_id": 2552524, "input_score": 18, "output_score": 35, "post_title": "Is a linear vector space a vector space?"}}
{"input": "If you input the trig identity:\n$$\\cot (x)+\\tan(x)=\\csc(x)\\sec(x)$$\nInto WolframAlpha, it gives the following proof:\nExpand into basic trigonometric parts:\n$$\\frac{\\cos(x)}{\\sin(x)} + \\frac{\\sin(x)}{\\cos(x)} \\stackrel{?}{=} \\frac{1}{\\sin(x)\\cos(x)}$$ \nPut over a common denominator:\n$$\\frac{\\cos^2(x)+\\sin^2(x)}{\\cos(x)\\sin(x)} \\stackrel{?}{=} \\frac{1}{\\sin(x)\\cos(x)}$$ \nUse the Pythagorean identity $\\cos^2(x)+\\sin^2(x)=1$:\n$$\\frac{1}{\\sin(x)\\cos(x)}  \\stackrel{?}{=} \\frac{1}{\\sin(x)\\cos(x)}$$ \nAnd finally simplify into \n$$1\\stackrel{?}{=} 1$$\nThe left and right side are identical, so the identity has been verified.\nHowever, I take some issue with this. All this is doing is manipulating a statement that we don't know the veracity of into a true statement. And I've learned that any false statement can prove any true statement, so if this identity was wrong you could also reduce it to a true statement.\nObviously, this proof can be easily adapted into a proof by simply manipulating one side into the other, but:\nIs this proof correct on its own? And can the steps WolframAlpha takes be justified, or is it completely wrong?", "output": "It is good that you are wary of proving identities this way. Indeed, I could \"prove\" $0=1$ by saying\n\\begin{align*}\n0 &\\stackrel{?}{=}1\\\\\n0\\cdot 0 &\\stackrel{?}{=} 0 \\cdot 1\\\\\n0 &=0.\n\\end{align*}\nThe important point is that every step WolframAlpha did is reversible, while the step I took (multiplying by $0$) was not. That is what allows the proof from WolframAlpha to be rearranged into a proof that starts with one side of the identity and ends at the other:\n\\begin{align*}\n\\cot(x)+\\tan(x) &= \\frac{\\cos(x)}{\\sin(x)} + \\frac{\\sin(x)}{\\cos(x)}\\\\\n&= \\frac{\\cos^2(x)}{\\sin(x)\\cos(x)} + \\frac{\\sin^2(x)}{\\sin(x)\\cos(x)}\\\\\n&= \\frac{\\sin^2(x)+\\cos^2(x)}{\\sin(x)\\cos(x)}\\\\\n&=\\frac{1}{\\sin(x)\\cos(x)}\\\\\n&=\\csc(x)\\sec(x).\n\\end{align*}\nSo no, the WolframAlpha proof is not wrong, but it neglects to emphasize the important fact that every step is reversible. I am not a fan of that sort of proof, as it gives students the idea that they can prove an identity by manipulating both sides in any way they like to arrive at a true statement.", "meta": {"post_id": 2555645, "input_score": 21, "output_score": 35, "post_title": "Is this an incorrect proof of $\\cot (x)+\\tan(x)=\\csc(x)\\sec(x)$?"}}
{"input": "In a lecture, our professor gave an example for a ring. He took it out of another source and mentioned that he does not know the motivation for the chosen operation.\nOf course, it's likely that somebody just invented an arbitrary operation satisfying ring axioms. I'd still like to try my luck whether anyone here can decipher the operation and give any kind of motivation for that example.\nOn $\\mathbb{R}^3$ define the operations $+$ and $\\cdot$ by\n$$ \\begin{aligned} (a_1, a_2, a_3) + (b_1,b_2,b_3) &= (a_1+b_1,a_2+b_2,a_3+b_3)\n\\\\ (a_1, a_2, a_3) \\cdot (b_1, b_2, b_3) &= (a_1 \\cdot b_1, a_2 \\cdot b_2, a_1 \\cdot b_3 + a_3 \\cdot b_2).\n\\end{aligned} $$\n(The $+$ and $\\cdot$ operations on the right side are the usual addition and multiplication from $\\mathbb{R}$.)\nWith those operations, one can confirm that $\\left(\\mathbb{R}^3, +, \\cdot \\right)$ is a ring.", "output": "This is just matrix multiplication in disguise.  Specifically, if you identify $(a_1,a_2,a_3)$ with the matrix $\\begin{pmatrix}a_1 & a_3 \\\\ 0 & a_2\\end{pmatrix}$, these operations are the usual matrix operations:\n$$\\begin{pmatrix}a_1 & a_3 \\\\ 0 & a_2\\end{pmatrix}+\\begin{pmatrix}b_1 & b_3 \\\\ 0 & b_2\\end{pmatrix}=\\begin{pmatrix}a_1+b_1 & a_3+b_3 \\\\ 0 & a_2+b_2\\end{pmatrix}$$\n$$\\begin{pmatrix}a_1 & a_3 \\\\ 0 & a_2\\end{pmatrix}\\begin{pmatrix}b_1 & b_3 \\\\ 0 & b_2\\end{pmatrix}=\\begin{pmatrix}a_1b_1 & a_1b_3+a_3b_2 \\\\ 0 & a_2b_2\\end{pmatrix}$$", "meta": {"post_id": 2557479, "input_score": 33, "output_score": 55, "post_title": "Motivation for the ring product rule $(a_1, a_2, a_3) \\cdot (b_1, b_2, b_3) = (a_1 \\cdot b_1, a_2 \\cdot b_2, a_1 \\cdot b_3 + a_3 \\cdot b_2)$"}}
{"input": "I cannot figure out what is wrong:\nWe will attempt to show that $\\mathcal{P} (\\mathbb{N})$ is countable. We use the following corollary from Rudin's Principles of Mathematical Analysis, p. 29:\n\nSuppose $A$ is at most countable, and, for every $\\alpha\\in A$, $B_{\\alpha}$ is at most countable. Put\n$$T=\\bigcup_{\\alpha \\in A}B_{\\alpha}$$\nThen $T$ is at most countable.\n\n\"Proof\" 1:\nLet $A = \\mathbb{N}$ and for every $\\alpha \\in A$ let $B_{\\alpha}=\\{S \\in \\mathcal{P} (\\mathbb{N})| \\text{the sum of the elements of } S \\text{ is } \\alpha \\}$. $A$ is countable and for every $\\alpha \\in A$, $B_{\\alpha}$ is finite. Therefore\n$$\\bigcup_{\\alpha \\in A}B_{\\alpha}$$\nis countable. But $\\displaystyle \\bigcup_{\\alpha \\in A}B_{\\alpha}=\\mathcal{P} (\\mathbb{N})$, so $\\mathcal{P} (\\mathbb{N})$ is countable.\n\n\"Proof\" 2:\nLet $A= \\mathbb{N}$ and for every $\\alpha \\in A$ let $B_{\\alpha}=\\{ S \\in \\mathcal{P} (\\mathbb{N}): |S| = \\alpha \\}$. I think that I can show by induction (if requested) that for each $\\alpha \\in A$, $B_{\\alpha}$ is countable. Thus\n$$\\bigcup_{\\alpha \\in A}B_{\\alpha}$$\nis countable. But again, $\\bigcup_{\\alpha \\in A}B_{\\alpha} = \\mathcal{P} (\\mathbb{N})$", "output": "In both your \"proofs\", it is not true that $\\displaystyle \\bigcup_{\\alpha \\in A}B_{\\alpha}=\\mathcal{P} (\\mathbb{N})$.  Indeed, if $S\\in\\mathcal{P}(\\mathbb{N})$ is any infinite set, then $S$ is not in any $B_\\alpha$ (by either definition).\nWhat both your arguments show correctly is that the set of all finite subsets of $\\mathbb{N}$ is countable.", "meta": {"post_id": 2571222, "input_score": 30, "output_score": 55, "post_title": "What is wrong with my \"disproof\" of Cantor's Theorem?"}}
{"input": "Say I want to evaluate this sum:\n$$\\sum_{x=2}^\\infty \\ln(x^3+1)-\\ln(x^3-1)$$\nWe can rewrite the sum as\n$$\\ln\\left(\\prod_{x=2}^\\infty \\frac{x^3+1}{x^3-1}\\right)$$\nWe can split the product into two products:\n$$\\ln\\left(\\prod_{x=2}^\\infty \\frac{x+1}{x-1}\\right)+\\ln\\left(\\prod_{x=2}^\\infty \\frac{x^2-x+1}{x^2+x+1}\\right)$$\nThese are both telescopic products! We can rewrite them as \n$$\\ln\\left(\\prod_{x=2}^\\infty \\frac{(x+2)-1}{x-1}\\right)+\\ln\\left(\\prod_{x=2}^\\infty \\frac{(x-1)^2+(x-1)+1}{x^2+x+1}\\right)$$\nPlugging in numbers makes this pattern more obvious;\n$$\\ln\\left(\\prod_{x=2}^\\infty \\frac{\\color{green}{3}}{1}\\frac {\\color{red}{4}}{2} \\frac{\\color{blue}{5}}{\\color{green}{3}} \\frac{\\color{bluedark}{6}}{\\color{red}{4}} \\right) +\n\\ln\\left(\\prod_{x=2}^\\infty \\frac{3}{\\color{green}{7}} \\frac{\\color{green}{7}}{\\color{red}{13}} \\frac{\\color{red}{13}}{\\color{blue}{21}} \\right)$$\nSimplifying, we get that the sum is equal to\n$$\\ln(1/2)+\\ln(3)=\\ln\\left(\\frac 32 \\right) \\approx 0.405465108108164381978013115464349136571990423462494197614$$\nHowever, when I put the sum in Wolfram Alpha directly, I get the following number: \n$$0.4054588737136331726677370820628648457601892466568342890929$$\n\nWhy are these two numbers different? It's not a small difference either; it's on the 5th number after the decimal point! How can Wolfram make such an error?", "output": "In Mathematica, the input FullSimplify[Sum[Log[x^3+1]-Log[x^3-1],{x,2,Infinity}]] gives Log[3/2], exactly.  In WolframAlpha, the issue is that when you request more digits of accuracy, it converts your input into the command NSum[Log[x^3 + 1] - Log[x^3 - 1], {x, 2, Infinity}, WorkingPrecision -> 104] which of course is insufficient working precision for the number of decimal digits that it displays due to the very slow convergence of the sum.\nSomewhat ironically, if you enter Product[(x^3+1)/(x^3-1),{x,2,n}] into WolframAlpha, you get exact output:  $$\\frac{3n(n+1)}{2(n^2+n+1)},$$ which is correct, furthermore upon taking Limit[3n(n+1)/(2(n^2+n+1)), n -> Infinity], you get the correct answer $3/2$.  So it isn't as if WolframAlpha cannot compute the original sum symbolically as Mathematica did.  It just needs a little extra help, it seems.\nUsers of WolframAlpha don't always realize that although it is using the same underlying algorithms as Mathematica, it is not the same thing.  There are things that one does that the other does not.  Precise control of processing of input, for example, is something that the former does not do.\n\nUpdate.  I believe the above response is not entirely sufficient to explain the behavior of WolframAlpha.  When I changed the summand to $$\\log \\left(1 + \\frac{2}{x^3-1}\\right),$$  WolframAlpha still gives the wrong result when more digits are requested, despite the fact that the generated code NSum[Log[1 + 2/(x^3 - 1)], {x, 2, Infinity}, WorkingPrecision -> 104] yields the correct result.  So this points to an internal inconsistency with WolframAlpha that cannot be solely explained by the insufficient precision used in NSum.\nTo confirm, in Mathematica I input both variants with NSum, namely \nNSum[Log[x^3 + 1] - Log[x^3 - 1], {x, 2, Infinity}, WorkingPrecision -> 104]\n\nas well as\nNSum[Log[1 + 2/(x^3 - 1)], {x, 2, Infinity}, WorkingPrecision -> 104]\n\nThe first input, as expected, generates a warning NIntegrate::ncvb.  Also as expected, the second one does not.  But if this is the case, then WolframAlpha should not still present the wrong result in the second case when more digits are requested, given that this is the exact code that it generated.  When you open up the computable notebook (click the orange cloud) and evaluate the expression, you are basically running a cloud version of Mathematica.  Doing this gets the right answer.  So I suspect that there is some kind of bug in WolframAlpha that fails to present the correct output.", "meta": {"post_id": 2589653, "input_score": 32, "output_score": 52, "post_title": "Manual calculation doesn't match Wolfram Alpha. Why?"}}
{"input": "The fact that so many students confuse functional inverse notation\n$$f^{-1}(x)$$\nwith multiplicative inverse notation\n$$[f(x)]^{-1}$$\ngot me to thinking... does there exist a function whose inverse is its inverse? That is, is there a function $f:\\mathbb R_+\\mapsto \\mathbb R_+$ whose functional inverse is also its multiplicative inverse, so that\n$$f^{-1}(x)=[f(x)]^{-1}, \\space\\space\\space \\forall x\\in\\mathbb R_+$$\nAny ideas? I'll impose the restriction of continuity to deter nasty solutions.", "output": "No, it is impossible.\nIf $f: (0,\\infty) \\to (0,\\infty)$ is continuous and $f^{-1}$ exists, then $f$ is either increasing or decreasing.  If $f$ is increasing, $f^{-1}$ is increasing but $1/f$ is decreasing.  If $f$ is decreasing, $f^{-1}$ is decreasing but $1/f$ is increasing.\nEDIT: However, for $f: \\mathbb R \\backslash \\{0\\} \\to \\mathbb R \\backslash \\{0\\}$ it is possible.  Take\n$$ f(x) = \\cases{ -x & if $x > 0$\\cr\n                 -1/x & if $x < 0$\\cr} $$", "meta": {"post_id": 2591008, "input_score": 29, "output_score": 48, "post_title": "Functional equation: what function is its inverse's reciprocal?"}}
{"input": "Is a Lipschitz function differentiable?\nI have been wondering whether or not this property applies to all functions. \n\nI do not need a formal proof, just the concept behind it. \n  Let $f: [a,b] \\to [c,d]$ be a continuous function (What is more - it is uniformly continuous!) And let's assusme that it's also Lipschitz continuous on this interval. \n\nDoes this set of assumptions imply that $f$ is differentiable  on $(a,b)$?", "output": "It is not always true  indeed, good counterexample could be $x\\mapsto |x-a|$. But rather, we have \n\nTheorem: Radamacher theorem says every Lipschitz function is almost everywhere differentiable\n\nFine a nice proof of this theorem here: An Elementary Proof of Rademacher's Theorem - James Murphy or Here using distribution theory", "meta": {"post_id": 2609765, "input_score": 24, "output_score": 39, "post_title": "Is a Lipschitz function differentiable?"}}
{"input": "I have been thinking about the difference between provability and truth and think this example can illustrate what I have been wondering about:\nWe know that Goodstein's theorem (G) is unprovable in Peano arithmetic (PA), yet true in certain extended formal systems. However, it seems like the theorem has a kind of truth that transcends the formal system you use: if you compute the Goodstein sequence for any natural number, it will end at 0 no matter what formal system you use. If you were to consider the system PA $+$ $\\lnot G$, wouldn't G still hold in the sense that you could never find a counterexample? Is any of this true, or am I just misunderstanding something?", "output": "Good question!\n\nIf you were to consider the system PA + $\\neg$G, wouldn't G still hold in the sense that you could never find a counterexample?\n\nThis gets at an important subtlety here - the issue of models. Any first-order theory like PA, PA+$\\neg$G, ZFC, ... has (assuming it's consistent!) many models. Some theories, like PA, have an \"intended model,\" but by the compactness theorem every (interesting) theory will have lots of nonisomorphic models. In particular, in addition to the standard model $(\\mathbb{N}; +, \\times, 0,1, <)$ of PA, there will also be lots of \"nonstandard\" models of PA. These models are difficult to describe (for good reason), but very vaguely they're ordered semirings (like $\\mathbb{N}$) with \"$\\mathbb{N}$-ish\" properties - in particular, they have a definable notion of exponentiation which behaves the way we're used to and lets the model talk about \"finite\" sequences of \"numbers.\" The key difference is that a nonstandard model of PA will have elements which are actually infinite. \nSuch a nonstandard element can constitute an apparent failure of G within that model. If $M$ is a model of PA+$\\neg$G, then there is some $m\\in M$ which according to $M$ is a counterexample to $G$. However, this $m$ won't actually be a \"true\" natural number: it won't be $0$, or $1$, or $1+1$, or ...\n\nAt this point it might help to get a little more concrete. As I said above, nonstandard models of PA are difficult to visualize. However, if we look at a weaker theory of arithmetic - say, Robinson's Q - things get much better. Q is a very weak theory indeed, and has many easily-describable nonstandard models. Perhaps the nicest of these is what I'll call $\\mathcal{P}$, the set of integer-coefficient polynomials in one variable $x$ with positive leading coefficient (okay fine and also the zero polynomial should be included). \n$\\mathcal{P}$ has a number of bizarre arithmetic properties. For instance, \"every number is even or odd\" is false in $\\mathcal{P}$! The polynomial \"$x$\" is neither even nor odd, since no element $p$ of $\\mathcal{P}$ satisfies either $p+p=x$ or $p+p+1=x$. There are many other examples of such strangeness. (Incidentally, this shows that you need induction to prove that every number is either even or odd! :P)\nSo what's going on? Well, of course in \"reality\" (that is, $\\mathbb{N}$) every number is even or odd. The problem is that $\\mathcal{P}$, while satisfying the axioms of $Q$, has \"too many numbers\" including some very strange ones which as far as $\\mathcal{P}$ is concerned are in fact counterexamples to the statement \"every number is either even or odd.\" So this shows that the statement \"every number is either even or odd\" can't be proved from $Q$ alone. \nExactly the same thing is going on with Goodstein's theorem G; it's just that the theory in question being PA, the \"bad\" models are much harder to visualize and so the independence is more mysterious.", "meta": {"post_id": 2611588, "input_score": 42, "output_score": 50, "post_title": "Difference between provability and truth of Goodstein's theorem"}}
{"input": "I'm not looking for the definition of discrete topology given in textbooks; I'm wondering why the word 'discrete' was chosen.\nI mean, the concept of discrete topology is built up from sets, which are built from objects--which are discrete. So, if we're looking for a word to differentiate power sets as topologies from other topologies, and we use the adjective 'discrete' to accomplish that differentiation because the power set is composed of discrete objects--then, by similar reasoning, couldn't we call all topologies on sets 'discrete'.\nBecause they're built from discrete objects and compositions, too. All of them. All topologies.\nThere must be some other reason we call discrete topologies 'discrete'. What is it?", "output": "The discrete topology has a topological structure which perfectly reveals the discrete nature of the underlying set of points\nYou can consider a set to be a discrete collection of objects. To a given set $X$, you can assign a variety of topologies. Let's argue for the appropriateness of calling this particular topology \"discrete\".\n\nThe discrete topology is the finest topology\u2014it cannot be subdivided further. If you think of the elements of the set as indivisible \"discrete\" atoms, each one appears as a singleton set. You can effectively \"see\" the individual points in the topology itself.\nContrast this with the indiscrete topology, consisting only of $X$ itself and $\\varnothing$. This topology obscures everything about how many points were in the original set. It fully agglomerates the points of the set together.\nRevisiting this point, it's sometimes helpful to think of topologies as obscuring or blurring together the underlying points of the set. Topologies are all about nearness relations: points in an open set are in the vicinity of one another. If there are two points that never appear alone in an open set, those points are topologically indistinguishable. From the perspective of the topology, they are so close as to be identical.\nIt is therefore remarkable that the discrete topology has no indistinguishable points. The discrete topology is the topology that obscures nothing about the underlying set. Each point in the set is clearly highlighted and distinguishable and recoverable as an open singleton set in the topology.\nIf you think of topologies that can arise from metrics, the discrete topology arises from metrics such as $d(x,y) = \\begin{cases}0 & x=y\\\\1&x\\neq y\\end{cases}$. This metric \"shatters\" the points $X$, isolating each one within its own unit ball. In such a space, the only convergent sequences are the ones that are eventually constant; you can't find points arbitrarily close to any other points. Because points are isolated in this way, it makes sense to call the space \"discrete\".\nEvery function from a discrete space is automatically continuous. I'd argue that for this reason, the discrete topology is the one that best \"represents\" $X$ in topological space. Indeed, in many ways the nature of a set is characterized by its functions, and the nature of a topological space is characterized by its continuous functions. \nSo, note that if $T$ is any topological space, there's a natural bijective correspondence between functions $f:X\\rightarrow \\mathsf{set}(T)$ and continuous morphisms $g:\\mathsf{discrete}(X)\\rightarrow T$. For every function on $X$, you can find a continuous function on $\\mathsf{discrete}(X)$, and given any continuous function on $\\mathsf{discrete}(X)$, you can uniquely recover a function on $X$. \nThe discrete topology best represents the structure of the set $X$ which, as you say, is discretized into individual points.\nThroughout abstract algebra, isomorphisms describe which structures are \"the same\". A topological isomorphism (a homeomorphism) between two topologies says that they are essentially the same topology. An isomorphism of sets is just a bijection; it says that the sets contain the same number of elements.\nContinuing the discussion of functions above, two discrete topologies are topologically isomorphic (homeomorphic) if and only if their underlying sets are isomorphic as sets (bijective). Put casually, this means that the discrete-topology-creating process maintains the similarity and differences between the underlying sets: discrete topologies are the same if and only if their underlying sets are.\nThis is all the more important when we realize that sets are the same when they have the same number of points. Hence discrete topologies are the same when (and only when) their underlying sets have \"discrete points\" in the same quantity. You can count the points in a discrete topology through isomorphisms, and the discrete topology is the only topology for which this is possible.", "meta": {"post_id": 2614268, "input_score": 25, "output_score": 43, "post_title": "Why is a discrete topology called a discrete topology?"}}
{"input": "The Nielsen\u2013Schreier theorem states (in part):\n\nLet $F$ be a free group, and $H\\le F$ be any subgroup. Then $H$ is isomorphic to a free group.\n\nI have seen the topological proof of this theorem using the correspondence between coverings and subgroups of the fundamental group. This has always struck me as using rather strong theory for what it is being used to prove (though I do appreciate the beauty of the argument). \nIn my head, I see the following (loose) argument: \n\nLet $H$ be a subgroup of $F$, and assume that $H$ is not free. Then there exists some nontrivial relation $h_1h_2\\dots h_n = 1$. But then this is also a nontrivial relation in $F$ implying that $F$ is not free, which is absurd. Thus $H$ is free.\n\nClearly, there must be some problem with this. What are the stumbling blocks here? An issue I see is that the exact notation a relation has always seemed a little vague to me (some reduced word equal to the identity?), but that doesn't seem like it ought to be a large enough problem to invalidate the argument.", "output": "But then this is also a nontrivial relation in $F$\n\nThis step is not obvious at all.  To get a nontrivial relation in $F$, you need to know that when you expand $h_1,\\dots,h_n$ in terms of the free generators of $F$, then $h_1\\dots h_n$ reduces to a nontrivial reduced word.  Why should that be true?\nIndeed, it may not be true.  You have glossed over the fact that you need to choose some specific subset of $H$ which will be your free generators.  If you choose incorrectly, then there will be nontrivial relations.  For instance, suppose $F$ is free on generators $a$ and $b$, and you take $H$ to be the subgroup generated by $h_1=ab$, $h_2=aba$, $h_3=bab$.  Then $h_1^3h_3^{-1}h_2^{-1}=1$ is a nontrivial relation among these generators of $H$ (but when you expand this out in terms of the generators of $F$, it reduces to the trivial word, so this does not contradict freeness of $F$).  So $H$ is not freely generated by $\\{h_1,h_2,h_3\\}$.  To prove $H$ is freely generated, you have to somehow come up with a special set of generators for which there will be no nontrivial relations.\nNow for this particular $H$, that is not so hard (in fact, $H$ is all of $F$, so you can take $a$ and $b$ as your free generators).  But if you have some completely arbitrary subgroup of $F$, it is not at all obvious how you would come up with a generating set such that any relation between them would still be a nontrivial relation in terms of the free generators of $F$, thus giving a contradiction as you suggest.", "meta": {"post_id": 2615108, "input_score": 25, "output_score": 44, "post_title": "What is tricky about proving the Nielsen\u2013Schreier theorem?"}}
{"input": "How would you go about finding prime factors of a number like $7999973$? I have trivial knowledge about divisor-searching algorithms.", "output": "The thing to notice here is that 7,999,973 is close to 8,000,000.  In fact it is $8000000 - 27$.  Both of these are perfect cubes.  Differences of cubes always factor: $$a^3 - b^3 = (a-b)(a^2+ab+b^2)$$\nHere we have $a=200, b=3$, so $a-b= 197$ is a factor.", "meta": {"post_id": 2615426, "input_score": 48, "output_score": 212, "post_title": "Find a prime factor of $7999973$ without a calculator"}}
{"input": "If I have already known a topological space $N$ is homeomorphic to a smooth manifold $M$ then will it be a smooth manifold? The atlas of $N$ is the preimage of the atlas of $M$ and the coordinate map is the composition of homeomorphism composites the coordinate map?", "output": "Let me emphasize something that is perhaps not readily apparent in the other answers. \"Smooth\" is not something that a topological space is. You can't formally say \"this topological space is a smooth manifold.\" It is not a property of a topological space. It is extra structure that you add on top of the already existing space.\nIndeed, you have to choose an atlas such that all the coordinate change maps are smooth. If you choose an atlas at random, chances are, it won't be smooth, even if you know otherwise that the space is homeomorphic to a smooth manifold.\nHowever there is a property here: can the space be attributed a smooth manifold structure? That is a possible property, it's a yes/no question. In this case the answer is yes! As the other answer points out, you can use the homeomorphism in a manner called transport of structure to choose an atlas that you know will be smooth. But there may be others! For example the $7$-sphere $S^7$ has famously several different, non-equivalent smooth structures (the ones other than the standard one are called exotic).\nTherefore it's important to distinguish property and structure. When you ask \"Is this space a smooth manifold?\" you're implicitly saying that \"being a smooth manifold\" is a property, which it isn't. And in this particular case it may help you clear up the question: when you wonder \"can this space be made into a smooth manifold\", then you see immediately that you need to add some stuff on top of what you already have, namely, an atlas. You can't just look at the space and ask yourself \"is this smooth\" in the same way that you look at a car and ask \"is this red\". And since you now know what you have to do, it's certainly easier to actually do it. (See also this other answer of mine or this nLab article.)", "meta": {"post_id": 2615883, "input_score": 19, "output_score": 37, "post_title": "If a topological space is homeomorphic to a smooth manifold, then will it be a smooth manifold?"}}
{"input": "This question seems obvious, but I'm not secure of my proof.\n\nIf a compact set $V\\subset \\mathbb{R^n}$ is covered by a finite union of open balls of common radii $C(r):=\\bigcup_{i=1}^m B(c_i,r)$, then is it true that there exists $0<s<r$ such that $V\\subseteq C(s)$ as well? The centers are fixed.\n\nI believe this statement is true and this is my attempt to prove it:\nEach point of $v\\in V$ is an interior point of least one ball (suppose its index is $j_v$), that is, there exists $\\varepsilon_v>0$ such that $B(v,\\varepsilon_v)\\subseteq B(c_{j_v},r)$, so $v\\in B(c_{j_v},r-\\varepsilon_v)$. Lets consider only the greatest $\\varepsilon_v$ such that this holds. Then defining $\\varepsilon:=\\inf\\{\\varepsilon_v\\mid v\\in V\\}$ and $s=r-\\varepsilon$ we get $V\\subseteq C(s)$.\n\nBut why is $\\varepsilon$ not zero? I thought that considering the greatest $\\varepsilon_v$ was important, but still couldn't convince myself.\n\nI would appreciate any help.", "output": "Replace each open ball $B_i$ of radius $r$ in the cover by the union of concentric open balls of radii strictly smaller than $r$.  You get an infinite cover of $V$.  By compactness there is a finite subcover.  By construction the radii are smaller than before.  Finally we choose the maximal radius (for all of the finitely many balls) which is still smaller than $r$.", "meta": {"post_id": 2616280, "input_score": 20, "output_score": 37, "post_title": "If a compact set is covered by a finite union of open balls of same radii, can we always get a lesser radius?"}}
{"input": "We had this problem in exam  class  yesterday on Combinatoric  and it was supposed to be the new year gift from our teacher. The exercise was entitled A Gift Problem for the Year 2018 \n\nProblem:\nThe numbers  $1,\\frac{1}{2},\\frac{1}{3},\\frac{1}{4},\\cdots,\\frac{1}{2018} $ are written on the blackboards. John chooses any two numbers say $x$ and $y$ erases them and writes the number $x+y+xy$.\n  He continues to do so until there is only one number left on the board.\n  What are the possible value of the final number?\n\nI understood the problem as follows for instance if John take $x=1$ and $y=\\frac{1}{2}$ then $x+y+xy =2$ and the new list becomes $$2,\\frac{1}{3},\\frac{1}{4},\\cdots,\\frac{1}{2018} $$ \n continuing like this and so on.....\nPlease bear with me  that I do not want to propose my solution since I fell like it was wrong and I don't want to fail the exam before the result get out. but by the way I found, $2017$, $2018$ and $2019$ but I am still suspicious. \nYou may help is you have an idea.", "output": "Consider the multiplicative  law on $\\Bbb R$ defines by $$x*y =x+y+xy =(x+1)(y+1)-1 $$\nyou can check that it is associative and commutative on $\\Bbb R$. Therefore at the end the remaining number is \n$$\\begin{align}x_0*x_1*x_2*\\cdots x_{2018} &= 1*\\frac{1}{2}*\\frac{1}{3}*\\cdots *\\frac{1}{2018} \\\\&=\\left[\\prod_{i=1}^{2018}(1+x_i)\\right]-1\\\\\n&=\\left[\\prod_{i=1}^{2018}\\left(1+\\frac{1}{i}\\right)\\right]-1 \\\\\n&=\\frac{2}{1}\\cdot \\frac{3}{2}\\cdot \\frac{4}{3}\\cdot \\ldots \\cdot \\frac{2018+1}{2018}-1=\\color{red}{2019-1=2018.}\n\\end{align}$$", "meta": {"post_id": 2617684, "input_score": 23, "output_score": 39, "post_title": "A Gift Problem for the Year 2018"}}
{"input": "Why is that if every row of a matrix sums to $1$ then the rows of its inverse matrix sum to $1$ too?\n\nFor example, consider \n$$A=\\begin{pmatrix}\n1/3 & 2/3 \\\\\n3/4 & 1/4\n\\end{pmatrix}$$\nthen its inverse is\n$$A^{-1}=\\begin{pmatrix}\n-3/5 & 8/5 \\\\\n9/5 & -4/5\n\\end{pmatrix},$$\nwhich satisfies the condition. Is it true for every such matrix?", "output": "Let $v = (1, 1, \\ldots , 1)'$ be a column vector of all $1$s. Then the rows of $A$ adding to $1$ is equivalent to saying $Av = v$. So when $A$ is invertible, we will have $$A^{-1}v = A^{-1}Av = v$$ Thus $A^{-1}$ has rows summing to $1$ as well. (Note that $A$ will not always be invertible.)", "meta": {"post_id": 2619258, "input_score": 48, "output_score": 107, "post_title": "Why is that if every row of a matrix sums to 1, then the rows of the inverse matrix sums to 1 too?"}}
{"input": "Here's the function I created (I have a very long proof too):\n$$(-1)^{\\dfrac{4\\Gamma(\n(x-1)(1-(\\lceil x\\rceil-\\lfloor x\\rfloor)+1)\n)+4}{(x-1)(1-(\\lceil x\\rceil-\\lfloor x\\rfloor))+1}}-1,\\quad x>1$$\nIs there any use for this? As far as I know it's the first of its kind. Wolfram Alpha link.", "output": "Your function simplifies to\n$$(-1)^{4((x-1)!+1)/x}-1$$\nand is just a restatement of Wilson's theorem: $(n-1)!\\equiv-1\\bmod n$ iff $n$ is prime.\nIf $x$ is prime, $((x-1)!+1)/x=k$ is an integer and $(-1)^{4k}-1$ evaluates to zero. If $x$ is composite and $\\ge6$, $k=n+\\frac1x$ where $n$ is an integer, so $(-1)^{4k}-1$ does not evaluate to zero. The function evaluated at $x=4$ gives $k=7/4$, which makes $(-1)^{4k}-1=-2\\ne0$.", "meta": {"post_id": 2626285, "input_score": 7, "output_score": 47, "post_title": "Created a function with roots at primes and only primes, are there useful applications?"}}
{"input": "Why is it true that if 7 divides 91 then $(2^7-1) $ divides $(2^{91}-1)$?\n1) $2^{91}-1$\n$7|91  \\implies (2^7-1)|(2^{91}-1)$\n$\\implies 2^7-1$ is factor \n2) $2^{1001}-1$\n$7|1001  \\implies (2^7-1)|(2^{1001}-1)$\n$\\implies 2^7-1$ is factor", "output": "It may be illustrative to write the numbers out in binary.  I'll use $2^{21} - 1 = (2^7)^3 - 1$ instead of $2^{91} - 1$, since it's shorter:\n$$\\begin{aligned}\n2^{21} - 1\n&= \\underbrace{111111111111111111111}_{21\\text{ digits}}\\,\\vphantom1_2 \\\\\n&= \\underbrace{1111111}_{7\\text{ digits}}\\,\\underbrace{1111111}_{7\\text{ digits}}\\,\\underbrace{1111111}_{7\\text{ digits}}\\,\\vphantom1_2 \\\\\n&= 1111111_2 \\times 100000010000001_2 \\\\\n&= (2^7 - 1) \\times (2^{14} + 2^7 + 1).\n\\end{aligned}$$", "meta": {"post_id": 2626598, "input_score": 13, "output_score": 34, "post_title": "Factor of a Mersenne number"}}
{"input": "I need to prove the following bound $$n! \\le e \\sqrt n \\left( \\frac n e \\right)^n$$\nI can bound $\\ln 1 + \\ln 2 + \\dots + \\ln n$ as a Riemann sum with the function $\\ln(n+1)$ and the trapezoidal rule:\n$$(\\ln 1)/2 + \\sum_{i=2}^n \\ln i \\ + (\\ln(n+1))/2 < \\int_0^n \\ln (x+1) dx $$\nIntegrating I get the bound $$n! < \\left( \\frac{n+1}{e} \\right)^n \\sqrt{n+1}$$\nAs $n$ goes to infinity, my bound and the required bound get arbitrarily close (for $n=1$, the error is about $4\\%$), but the one I need to prove is slightly tighter. How can I modify my bound to get the required bound?", "output": "So you have\n$n! \n\\lt \\left( \\frac{n+1}{e} \\right)^n \\sqrt{n+1}\n$.\nI'll play and see what happens.\nReplacing $n$ by $n-1$,\nwe get\n$(n-1)! \n\\lt \\left( \\frac{n}{e} \\right)^{n-1} \\sqrt{n}\n$.\nMultiplying by $n$,\nwe get\n$\\begin{array}\\\\\nn! \n&\\lt n\\left( \\frac{n}{e} \\right)^{n-1} \\sqrt{n}\\\\\n&= e\\left( \\frac{n}{e} \\right)^{n} \\sqrt{n}\\\\\n\\end{array}\n$\nwhich is what you want!!!", "meta": {"post_id": 2632270, "input_score": 21, "output_score": 37, "post_title": "Almost Stirling's Approximation"}}
{"input": "We call a divisor of a positive integer interesting if it is a divisor of a number and differs from 1 and the number itself. We call a number $X$ very interesting if it has at least two interesting divisors and it is divisible by the difference between any two of its interesting divisors. \nHow can one find the product of all very interesting numbers?\nI experimented numerically and came to the conclusion that such numbers never exceed 1000. Is it really so?", "output": "Suppose $n$ is very interesting.\n\nThen $n$ must be composite, hence we can write, $n=ab$, where $a$ is the least prime factor of $n$, and $b > 1$.\n\nIf $b \\le a$, then by minimality of $a$, we must have $b=a$, but then $n=a^2$, which is not possible, since the square of a prime is not a very interesting number. Therefore $b > a$.\n\nIf $a$ is odd, then $n$ is odd, hence so is $b$.\n\nBut then $b-a$ is even, hence, since $(b-a){\\,\\mid\\,} n$, it follows that $2{\\,\\mid\\,}n$, contradiction.\n\nHence we must have $a=2$, so $n=2b$.\n\nThen $b-2$ must divide $n$, but then, since \n$$n = 2b = 2(b-2) + 4$$\nit follows that $(b-2){\\,\\mid\\,}4$, hence $b \\le 6$, so $n \\le 12$.\n\nFor $1 \\le n \\le 12$, the only candidates are $6,8,10,12$, since those are the only even composite numbers which are not the square of a prime. \n\nThe number $10$ is not very interesting, since $5-2 = 3$, which is not a divisor of $10$.\n\nIt's easily verified that the numbers $6,8,12$ are very interesting, hence those are the only very interesting numbers.", "meta": {"post_id": 2641461, "input_score": 12, "output_score": 38, "post_title": "About \"interesting\" numbers #2"}}
{"input": "I have been trying to solve this one problem from the Duke Math Meet, which does not provide a solution:\n\nFind all solutions of $(x^2+7x+6)^2 + 7(x^2+7x+6) + 6=x$\n\nAt first I tried to factorize the polynomial, but always had the right-hand side $x$ remain, which was inconvenient. I then tried using the fact that one can write the left-hand side as a composition of functions, and equated that with the inverse of the quadratic plugged into the function, but that was a very nasty equation with square roots, still ending up with a quartic.\nWhat other solution paths are viable for this problem, and is there a way to factor the quartic?", "output": "Let $$y=x^2+7x+6$$\n$$x=y^2+7y+6$$\nthus $$y-x=(x-y)(x+y)+7(x-y)$$ so either \n$$x=y$$ or \n$$-1=x+y+7$$\nBoth of these cases reduce the problem to simple quadratic equations.\nAnd you get $$x=-4\\pm\\sqrt{2},-3\\pm\\sqrt{3}$$", "meta": {"post_id": 2680131, "input_score": 22, "output_score": 34, "post_title": "Finding the roots of $(x^2+7x+6)^2 + 7(x^2+7x+6) + 6=x$"}}
{"input": "Consider the cosine function $f = \\cos : \\Bbb R \\to \\Bbb R$. \nIs it true that the set of iterates\n$$\\left\\{f_n := \\cos \\circ \\dotsb \\circ \\cos,  \\; n \\text{  times }   \\mid  n \\geq 1\\right\\}$$\nis linearly independent over $\\Bbb R$ ?\nThat is, I am wondering if,  for any $r \\geq 1$ and any real numbers $a_k$, we have :\n$$\\sum_{k=1}^r a_k f_k = 0 : \\Bbb R \\to \\Bbb R \\implies a_k=0 \\;\\forall k.$$\nI know that this true if we consider the powers of $\\cos( \\cdot )$, but I don't know how to deal with compositions.\nWhat I tried is to take derivative, or induction on the minimal length of linear dependence relation.", "output": "If you can use the fact that $\\cos(x)$ and its iterates are entire functions of a complex variable, you can use the following idea (I use your notations): We proceed by inductioon, the case $n=1$ is obvious.\nLet $n\\geq 2$, and suppose that $a_1f_1(x)+a_2f_2(x)+\\cdots+a_nf_n(x)=0$ for all $x\\in \\mathbb{R}$. Then this imply that $g(z)=a_1z+a_2f_1(z)+\\cdots+a_nf_{n-1}(z)$ is zero for all $z\\in [-1,1]$ (because $g(\\cos(x))=0$, we have put $z=\\cos(x)$). As $g$ is entire, this imply that $g(z)=0$ for all $z\\in \\mathbb{C}$, and that $a_1z$ is periodic with period $2\\pi$. Hence $a_1=0$. Now , putting $b_1=a_2,...$ etc, we have $b_1f_1(x)+\\cdots+b_{n-1}f_{n-1}(x)=0$ for all $x$. The  induction hypothesis apply, and we are done. .", "meta": {"post_id": 2702135, "input_score": 35, "output_score": 41, "post_title": "Are the iterates of the cosine linearly independent?"}}
{"input": "Exactly the title: can you take the derivative of a function at infinity?\nI asked my maths teacher, and while she thought it was an original question, she didn't know the answer, and I couldn't find anything online about this.\nMaybe this is just me completely misunderstanding derivatives and functions at infinity, but to me, a high schooler, it makes sense that you can. For example, I'd imagine that a function with a horizontal asymptote would have a derivative of zero at infinity.", "output": "In a very natural sense, you can! If $\\lim_{x \\to \\infty} f(x) = \\lim_{x \\to -\\infty} f(x) = L$ is some real number, then it makes sense to define $f(\\infty) = L$, where we identify $\\infty$ and $-\\infty$ in something called the one-point compactification of the real numbers (making it look like a circle). \nIn that case, $f'(\\infty)$ can be defined as\n$$f'(\\infty) = \\lim_{x \\to \\infty} x \\big(f(x) - f(\\infty)\\big).$$\nWhen you learn something about analytic functions and Taylor series, it will be helpful to notice that this is the same as differentiating $f(1/x)$ at zero. \nNotice that this is actually not the same as $\\lim_{x \\to \\infty} f'(x)$.\nThese ideas actually show up quite a bit in analytic capacity, so this is a rather nice idea to have.\n\nI wanted to expand this answer a bit to give some explanation about why this is the \"correct\" generalization of differentiation at infinity. and hopefully address some points raised in the comments.\nAlthough $\\lim_{x \\to \\infty} f'(x)$ might feel like the natural object to study, it is quite badly behaved. There are functions which decay very quickly to zero and have horizontal asymptotes, but where $f'$ is unbounded as we tend to infinity; consider something like $\\sin(x^a) / x^b$ for various $a, b$. Furthermore, $\\lim_{x \\to \\infty} f'(x) = 0$ is not sufficient to guarantee a horizontal asymptote, as $\\sqrt{x}$ shows.\nSo why should we consider the definition I proposed above? Consider the natural change of variables interchanging zero and infinity*, swapping $x$ and $1/x$. Then if $g(x) := f(1/x)$ we have the relationship\n$$\\lim_{x \\to 0} \\frac{g(x) - g(0)}{x} = \\lim_{x \\to \\infty} x \\big(f(x) - f(\\infty)\\big).$$\nThat is to say, $g'(0) = f'(\\infty)$. Now via this change of variables, neighborhoods of zero for $g$ correspond to neighborhoods of $\\infty$ for $f$. So if we think of the derivative as a measure of local variation, we now have something that actually plays the correct role.\nFinally, we can see from this that this definition of $f'(\\infty)$ gives the coefficient $a_1$ in the Laurent series $\\sum_{i \\ge 0} a_i x^{-i}$ of $f$. Again, this corresponds to our idea of what the derivative really is.\n* This is one of the reasons why I used the one-point compactification above. Otherwise, everything that follows must be a one-sided limit or a one-sided derivative.", "meta": {"post_id": 2747593, "input_score": 44, "output_score": 65, "post_title": "Can you take the derivative of a function at infinity?"}}
{"input": "Is there a way to visualize the differential forms - \nLike I found that if $\\alpha$ is a 1 - form then $d \\alpha$ represents curl.\nand $\\sigma$ is a 2 - form then $d \\sigma$ represents the divergence.\nand $d^{2} \\alpha = 0$ resembles $\\nabla \\times (\\nabla()) = 0$.\nHow do I visualize the above, is it intuitive?", "output": "I will try to give a visual representation of forms and provide intuition into exterior derivatives using Stokes' theorem:\n$$\\int_{\\partial M} \\alpha = \\int_{M} d\\alpha.$$\n\nDifferential 1-forms\nA differential 1-form $\\alpha$ is pointwisely a \"covector\", a function $\\alpha|_p$ which assigns a real value to each vector tangent to $p \\in M$. This assignment is linear:\n\n(For simplicity, I use a 2-D manifold as the total space, rather than $\\mathbb R^3$.)\nIn the picture above, no covectors are drawn, but only vectors. This is because unlike (column) vectors, covectors are linear functions $\\mathbb R^n\\to \\mathbb R$, which are hard to draw.\nHowever, a covector may be regarded as a \"row vector\" since a linear map of the type $\\mathbb R^n\\to \\mathbb R$ is equivalent to a left-multiplication by some $1\\times n$ matrix. So if needed, one could still draw an arrow (column vector) $v$ that represents the function $f(x) = \\langle v, x \\rangle$ or $v^T$.\n\nUnder this representation, it is possible to draw a 1-form $\\alpha$ as a vector field (or in this case, a co-vector field). Physically, integrating the co-vector field (1-form) along a curve is identical to measuring the work done by a vector field.\n$$\n\\int_\\gamma \\alpha\n= \\int_\\gamma \\overset \\rightharpoonup \\alpha\\cdot \\overset \\rightharpoonup {dr}\n$$\nwhere $\\overset \\rightharpoonup \\alpha$ denotes the vector field \"representing\" the 1-form $\\alpha$, and $\\overset \\rightharpoonup {dr}$ is the infinitesimal movement of the curve $\\gamma$.\n\nClarification:\nAlthough I draw arrows for covectors, I agree with John Hughes that covectors (dual vectors) should be regraded as functions rather than arrows eventually, to get a really suitable intuition.\n\nDifferential 2-forms\nLikewise, a 2-form $\\sigma$ is pointwisely an alternating linear map (which we call a 2-covector) that assigns to each pair of vectors a real number ...\n... or we could say that $\\sigma|_p$ assigns to each oriented parallelogram based at $p$ a real number, since an ordered pair of tangent vectors at a same point $p\\in M$ determines an oriented parallelogram:\n\nThe \"alternating\" property requires that $\\sigma|_p$ satisfy $\\sigma|_p(v,v)=0$ for any $v \\in T_p(M)$. Thus $\\sigma|_p(v_1,v_2)=-\\sigma|_p(v_2,v_1)$, which characterizes the notion of signed area.\nRecall that for a 1-form $\\sigma$, we draw an arrow representing a covector at each point $p \\in M$. For a fixed $p \\in M$, this arrow points in the unit direction for which the linear function $\\sigma|_p$ attains a maximal value. Similarly for 2-forms, we can associate to each $p \\in M$ an oriented parallelogram (with fixed area) in the direction that attains the maximal value.\n\nThe directions of the faces representing 2-covectors form a vector field, which is how people regard a 2-form as a vector field that points the directions in which a parallelogram can get a maximal value, but now we know that that's somewhat different.\n\n\nVerification:\nLet $$v_1:(a_1,b_1,c_1),v_2:(a_2,b_2,c_2)$$ be unit vectors that determine the parallelogram facing the direction that achieves the maximal value under the 2-covector\n$$\n\\sigma = p\\, dy\\wedge dz + q\\, dz \\wedge dx + r \\, dx \\wedge dy.\n$$\nA calculation reveals that\n$$\\begin{aligned}\n\\sigma(v_1,v_2)\n&= p(b_1c_2-b_2c_1) + q(c_1a_2-c_2a_1) + r(a_1b_2-b_2a_1)\\\\[0.6em]\n&= (p,q,r) \\cdot(v_1 \\times v_2).\n\\end{aligned}$$\nHence, the cross product of the two vectors must be parallel to the $(p,q,r)$ so that the evaluation $\\sigma(v_1, v_2)$ become maximal.\n\nExterior derivative and Exact 2-forms\nThe exterior derivative of a 1-form $\\alpha$ is a 2-form $d\\alpha$. Stokes' theorem states that for a manifold $M$ with boundary $\\partial M$, integrating $\\alpha$ along $\\partial M$ is equivalent to integrating $d\\alpha$ over $M$:\n$$\n\\int_{\\partial M} \\alpha = \\int_{M} d\\alpha.\n$$\nSo, how do we evaluate the 2-form $d\\alpha$ over $M$? Assume that a pair of vectors $v_1,v_2$ determines an infinitesmal oriented parallelogram $P$, then we may regard\n$$\nd\\alpha(v_1,v_2)=d\\alpha(P)\\approx \\int_{P} d\\alpha= \\int_{\\partial P} \\alpha.\n$$\n\n\nClarification:\nIt requires more work to show that this intuition works in some sense (maybe you could skip this part). Let's assume our manifold is just $\\mathbb R^3$, and prepare a plane $N\\subset \\mathbb R^3$ that contains $P$. Then $d\\alpha$ is a top form when restricted to $N$. That is, there exists some real valued function $f$ (area density function) such that  $$d\\alpha|_q=f(q)\\,ds\\wedge dt,\\;\\; \\forall q\\in N$$ where $(s,t)$ is an orthonormal coordinate on $N$. Then by mean value theorem $$\\int_{\\partial P} \\alpha = \\int_P d\\alpha\\; \\underset{q\\in P}{\\overset{M.V.T}{=\\!=\\!=}} \\; f(q)\\cdot \\text{Area}(P)\\approx f(p)\\cdot\\text{Area}(P).$$ Write $$v_1=c_{1,1}\\vec s+c_{1,2}\\vec t\\;\\;,\\;\\;v_2=c_{2,1}\\vec s+c_{2,2}\\vec t.$$ Then $$\\begin{aligned}\n...& \\approx f(p)\\cdot\\text{Area}(P) = f(p)\\cdot \\begin{vmatrix}c_{11}& c_{11}\\\\ c_{21}& c_{22}\\end{vmatrix} = f(p)\\cdot (ds\\wedge dv)(c_{1,1}\\vec s+c_{1,2}\\vec t,c_{2,1}\\vec s+c_{2,2}\\vec t)\\\\[0.7em] \n&= f(p)\\cdot (ds\\wedge dv)(v_1,v_2) = \\sigma_p(v_1,v_2).\\end{aligned}$$\nHowever, in general it isn't possible to embed a parallelogram in any manifold $M$. Moreover, $T_p M$ and $M$ are not always assumed to lie in some ambient space. So ... maybe it is better to leave it as a mere intuition.\n\nThis also shows that: if the set of arrows representing the 1-form $\\alpha$ rotates counter-clockwisely in the same plane of the parallelogram, then $d\\alpha$ assigns a positive value to the parallelogram (curl).\n\n(This picture may be misleading: even if the closed path does not contain the center of rotation, it is still possible for the integral to be positive.)\nDifferential 3-Forms\nA differential 3-form gives each parallelepiped spaned by three vectors a value.\n\nExterior derivative and Exact 3-forms\nLikewise, a two form $\\sigma$ can may assign values to the boundary of a parallelepiped. This induces a 3-form $d\\sigma$:\n\nNote that the boundary of the parallelepiped in this example is positively oriented, i.e. every normal vector points outward.\nWe have shown that a 2-form can evaluate a parallelogram (spanned by $v_1,v_2$) via the inner product $(p,q,r) \\cdot (v_1 \\times v_2)$. So, if a vector field representing a 2-form $\\sigma$ has divergence, then $d\\sigma$ (integration along the boundary of a small 3D region) is positive.\n\nFor simplicity, I drew a polyhedron rather than a 3D region with smooth boundary.\nIntuition for $d^2=0$\nBecause the integrals along the edges of the polyhedron cancel out, summing $d\\alpha$ over each face, i.e. $d(d\\alpha)$ yields zero.\n\nThis is why $d^2\\alpha=0$.", "meta": {"post_id": 2748750, "input_score": 13, "output_score": 38, "post_title": "Intuitive thinking of differential forms in terms of gradient, divergence and curl?"}}
{"input": "Some weeks ago I saw in a blog post an anecdote of a mathematician who once gave a 'talk' (not really). The special thing was that he came directly to the chalkboard and started doing one computation that took several empty boards. Without saying a word during all the process (in fact all the presentation), when he put the last dot in the computation, the crowd started clapping excitedly. \nI sort of remember and I wanted to come back to it to read it with more time, but I lost it. Now I have the doubt. Does anyone know about this anecdote? Who can this mysterious mathematician be?\nThanks!", "output": "Probably it was about Frank Nelson Cole's factorization of $2^{67}-1$. As Wikipedia says:\n\nOn October 31, 1903, Cole famously made a presentation to a meeting of the American Mathematical Society where he identified the factors of the Mersenne number $2^{67} \u2212 1$, or $M_{67}$. \u00c9douard Lucas had demonstrated in 1876 that $M_{67}$ must have factors (i.e., is not prime), but he was unable to determine what those factors were. During Cole's so-called \"lecture\", he approached the chalkboard and in complete silence proceeded to calculate the value of $M_{67}$, with the result being $147,573,952,589,676,412,927$. Cole then moved to the other side of the board and wrote $193,707,721 \\times 761,838,257,287$, and worked through the tedious calculations by hand. Upon completing the multiplication and demonstrating that the result equaled $M_{67}$, Cole returned to his seat, not having uttered a word during the hour-long presentation. His audience greeted the presentation with a standing ovation. Cole later admitted that finding the factors had taken \"three years of Sundays.\"\n\nThis MathOverflow question has a few more mathematical details, as well as a link to Cole's paper where he described his methods.", "meta": {"post_id": 2771735, "input_score": 34, "output_score": 67, "post_title": "Lost anecdote of a mathematician who gave a presentation without saying a word"}}
{"input": "I imagine this problem is a common one, however without having any source to refer to I don't know its usual name, and am having trouble finding an answer. \nI am taking the definition: A cover $C$ of a set $S$ is a set such that $\\cup C = S$\nI want to know if every cover has a minimal subcover. \nThanks.", "output": "How about $S=\\Bbb R$ and the cover composed of the intervals $(-n,n)$?\nAny subcover of this cover remains a subcover if you omit one of its elements.", "meta": {"post_id": 2805639, "input_score": 12, "output_score": 39, "post_title": "Does every cover of a set have a minimal subcover?"}}
{"input": "My knowledge of geometry is just a little bit above high school level and I know absolutely nothing about topology. So, what is the point of this meme?\n\n(Original unedited webcomic: \u201cJuncrow\u201d by False Knees)", "output": "I rebut your meme with a meme of my own", "meta": {"post_id": 2806389, "input_score": 67, "output_score": 40, "post_title": "Explain this mathematical meme (Geometers bird interrupting Topologists bird)"}}
{"input": "What is the longest path in a square grid from one corner to the diagonally opposite corner?\nEdges in the grid may only be traversed once, but grid points can be used multiple times (in a square grid that means maximum twice). \nAfter fiddling with this a while, it seems to me the following paths are the longest:\n\nIn a $2 \\times 2$ grid, the longest path is $8$. \n\nIn a $3 \\times 3$ grid, the longest path is $18$. \n\nIn a $4 \\times 4$ grid, the longest path is $32$.\n\nIn a $5 \\times 5$ grid, the longest path is $50$.\nIn general, the rule seems to be that the longest path for an $n \\times n$ grid is $2n^2$. \nCan anyone confirm this? I've searched for a proof of this, but wasn't able to find it.", "output": "There are $2n(n+1)$ potential edges in the grid, but some of them need to be left out because the grid points on the boundary have only $3$ potential edges meeting, and only two of them can be in your path.\nAdditionally, one of the edges incident to the start and end node must be left out.\nIf we count the number of left-out half-edges we get at least\n$$ 4(n-1) + 2 = 4n-2 $$\nso at least $2n-1$ of the $2n(n+1)$ edges must be missing. So at most there are\n$$ 2n(n+1)-(2n-1) = 2n^2 +1 $$\nedges in the path.\nHowever, the length of the path must be even: Color the grid points alternately black and white in a checkerboard pattern. Two opposite corners will have the same color, but every move changes the color, so there must be an even number of moves.\nThis shows that the the length of the path is at most $2n^2$.\nOn the other hand, it should be clear that the pattern you have found achieves this number for larger $n$ too, so it is the actual maximum.", "meta": {"post_id": 2824382, "input_score": 23, "output_score": 36, "post_title": "Longest path in a square grid"}}
{"input": "I am wondering what exactly is the relationship between the three aforementioned spaced. All of them seem to show up many times in: Linear Algebra, Topology, and Analysis. However, I feel like I'm missing the bigger picture of how these spaces relate to each other. For example, in my course in multi-dimensional analysis, we started out talking about metric spaces, but later suddenly switched to normed vector spaces, without any explicit mention of this transition. In linear algebra we usually talked about inner product spaces, and in topology we talked about metric spaces and topological spaces. \nThe bigger picture of the relation between these three is still unclear to me. Which is used where, for what reason, and how do they relate?\nI do know the definitions of all three of them: \nA metric space is a pair $(S,d)$ with $S$ a set and $d: S \\times S \\to \\mathbb{R}_{\\geq 0}$ a metric:\n\n$d(x,x) = 0$ for all $x \\in S$ and $d(x,y) >0$ for $x \\neq y$, \n$d(x,y) = d(y,x)$, \n$d(x,z) \\leq d(x,y) + d(y,z)$. \n\nA (real) inner product space is a pair $(V,\\langle \\cdot \\rangle)$ where $V$ is a (real) vector space and $\\langle \\cdot \\rangle: V \\times V \\to \\mathbb{R}$ is an inner product: \n\n$\\langle v,w \\rangle = \\langle w,v \\rangle$, \n$\\langle a_1 v_1 + a_2v_2,w \\rangle = a_1\\langle v_1,w \\rangle + a_2\\langle v_2,w \\rangle$ for all $a_1,a_2 \\in \\mathbb{R}$, \n$v \\neq 0 \\Longrightarrow \\langle v,v \\rangle > 0$. \n\nA (real) normed vector space is a pair $(V,\\|\\cdot\\|)$ where $V$ is a (real) vector space and $\\|\\cdot\\|: V \\to \\mathbb{R}_{\\geq 0}: v \\mapsto \\|v\\|$ is a norm on $V$: \n\n$\\|v\\| \\geq 0$ and $\\|v\\|  = 0 \\ \\Longleftrightarrow \\ v = 0$. \nFor $t \\in \\mathbb{R}$ and $v \\in V$ we have $\\|tv\\| = |t|\\|v\\|$\n$\\|v+w\\| \\leq \\|v\\| + \\|w\\|$. \n\nI also know that an inner product gives rise to a norm by taking $\\|v\\| = \\sqrt{\\langle v,v \\rangle}$, for example the Euclidean norm derives from the standard inner product on $\\mathbb{R}^n$ in this way. And Cauchy-Schwarz: $|\\langle x,y \\rangle| \\leq \\|x\\|\\|y\\|$. \nI'm not interested in details about the definitions but in the intuition and bigger picture of these three spaces, and how they show up in Analysis.", "output": "You have the following inclusions:\n$$\\{ \\textrm{inner product vector spaces} \\} \\subsetneq \\{ \\textrm{normed vector spaces} \\} \\subsetneq \\{ \\textrm{metric spaces} \\} \\subsetneq \\{ \\textrm{topological spaces} \\}.$$\nGoing from the left to the right in the above chain of inclusions, each \"category of spaces\" carries less structure. In inner product spaces, you can use the inner product to talk about both the length and the angle of vectors (because the inner product induces a norm). In a normed vector space, you can only talk about the length of vectors and use it to define a special metric on your space which will measure the distance between two vectors. In a metric space, the elements of the space don't even have to be vectors (and even if they are, the metric itself doesn't have to come from a norm) but you can still talk about the distance between two points in the space, open balls, etc. In a topological space, you can't talk about the distance between two points but you can talk about open neighborhoods.\nBecause of this inclusion, everything that works for general topological spaces will work in particular for all other spaces, but there are some things you can do in (say) normed vector spaces which don't make sense in a general topological space. For example, if you have a function $f \\colon V \\rightarrow \\mathbb{R}$ on a normed vector space, you can define the directional derivative of $f$ at $p \\in V$ in the direction $v \\in V$ by the limit\n$$ \\lim_{t \\to 0} \\frac{f(p + tv) - f(p)}{t}. $$\nIn the definition, you are using the fact that you can add the vector $tv$ to the point $p$. If you try to mimick this definition in a topological space, then since the set itself doesn't have the structure of a vector space, you can't add two elements so this definition doesn't make sense. That's why during your studies you sometimes restrict your attention to a smaller category of spaces which has more structure so you can do more things in it. \nYou can discuss the notions of continuity, compactness only in the category (context) of topological spaces (but for reasons of simplicity it is often done in the beginning of one's studies in the category of metric spaces). However, once you want to discuss differentiability, then (in first approximation, before moving to manifolds) you need to restrict your category and work with normed vector spaces. If you also want to discuss the angle that two curves make, you will need to further restrict your category and work with inner product vector spaces in which the notion of angle makes sense, etc.", "meta": {"post_id": 2841855, "input_score": 22, "output_score": 48, "post_title": "Relation between metric spaces, normed vector spaces, and inner product space."}}
{"input": "I was writing some exercises about the AM-GM inequality and I got carried away by the following (pretty nontrivial, I believe) question:\n\nQ: By properly folding a common $210mm\\times 297mm$ sheet of paper, what\n  is the maximum amount of water such a sheet is able to contain?\n\n\nThe volume of the optimal box (on the right) is about $1.128l$. But the volume of the butterfly (in my left hand) seems to be much bigger and I am not sure at all about the shape of the optimal folded sheet. Is is something boat-like?\nClarifications: we may assume to have a magical glue to prevent water from leaking through the cracks, or for glueing together points of the surface. Solutions where parts of the sheet are cut out, then glued back together deserve to be considered as separate cases. On the other hand these cases are trivial, as pointed by joriki in the comments below. The isoperimetric inequality gives that the maximum volume is $<2.072l$.\nAs pointed out by Rahul, here it is a way for realizing the optimal configuration: the maximum capacity of the following A4+A4 bag exceeds $2.8l$.", "output": "This is equivalent to the paper bag problem, which asks for the maximum possible volume attainable by inflating an initially flat rectangular pillow made of inextensible material. Separate the two sides of the pillow while keeping their shape, and you obtain (two copies of) your optimal sheet.", "meta": {"post_id": 2855975, "input_score": 264, "output_score": 69, "post_title": "What is the maximum volume that can be contained by a sheet of paper?"}}
{"input": "I am currently reading various differential geometry books. From what I understand differential forms allow us to generalize calculus to manifolds and thus perform integration on manifolds. I gather that it is, in general, completely distinct from Lebesgue measure theory and is more like a generalization of Riemann integration.\nOk so here's the problem. I have always viewed Lebesgue measure theory as 'solving the issues with Riemann integration'. For example, a big problem with Riemann integration  is that the space of Riemann integral functions is not complete. The fact that $L^p$ spaces in the Lebesgue theory are complete seems like a huge improvement on the Riemann situation, and is vital for so many concepts in functional analysis, PDEs, operator theory, and numerical analysis.\nSo if we then consider differential geometry and integration via differential forms, unless I am misunderstanding something, we lose all the benefits of Lebesgue theory?\nIt seems like if do lose all those benefits we are in a very bad situation. For example, how are we supposed to rigorously define solution spaces for PDEs if we can't use $L^p$ spaces and thus can't use Sobolev spaces? How can we obtain acceptable convergence of some sequence that may arise during our work if we are operating in this generalized Riemann setting where we lack completeness?\nIn summary, if differential forms are a generalization of Riemann integration how are we supposed to perform analysis when we no longer have the power and utility of Lebesgue measure theory?", "output": "People use measure theory in tandem with differential forms all the time\u2014there's no contradiction whatsoever between the formalisms. Be aware, though, that the adjective \u201cRiemannian\u201d in the context of differential geometry refers to constructions depending on Riemannian metrics (which are \u201cRiemannian\u201d in the sense of originating in the work of Bernhard Riemann), not to Riemann integration.\nSuppose that $M$ is a smooth $n$-manifold. By definition, it's locally diffeomorphic to $\\mathbb{R}^n$, so that you can define a set $S \\subset M$ to be measurable if and only if $x(S \\cap U) \\subset \\mathbb{R}^n$ is Lebesgue measurable for every local coordinate chart $x: U \\to x(U) \\subset \\mathbb{R}^n$. This gives you a $\\sigma$-algebra of Lebesgue measurable sets on $M$ that correctly completes the Borel $\\sigma$-algebra generated by the open sets on $M$ as a topological space. At this point, you have everything you need to define measurable functions, vector fields, differential forms, tensor fields, etc., in a manner compatible with calculations in local coordinates.\nNow, suppose that $M$ is a Riemannian manifold, so that it comes equipped with a Riemannian metric $g$\u2014again, the \u201cRiemannian\u201d here does not refer to Riemann integration, but to Riemann himself and his work on differential geometry. On any local coordinate chart $x : U \\to x(U) \\subset \\mathbb{R}^n$, you can define a measure $\\lambda_{g,x}$ on $U$ by setting\n$$\n \\lambda_{g,x}(S \\cap U) := \\int_{x(S \\cap U)} \\sqrt{\\det\\left(g\\left(\\tfrac{\\partial}{\\partial x^i},\\tfrac{\\partial}{\\partial x^j}\\right)\\right)} \\,d\\lambda\n$$\nfor any Lebesgue measurable $S \\subset M$, where $\\lambda$ denotes Lebesgue measure on $\\mathbb{R}^n$. By paracompactness of the manifold $M$, one can cover $M$ by a locally finite open cover of such local coordinate charts, and hence use a smooth partition of unity subordinate to this cover to patch these local scaled pullbacks of Lebesgue measure together into a single measure $\\lambda_g$, the Riemannian measure [!] on $M$ with respect to $g$, which is a complete $\\sigma$-finite measure on the $\\sigma$-algebra of Lebesgue measurable sets in $M$.\nLet me now describe the basic properties of $\\lambda_g$.\n\nThe measure $\\lambda_g$ is compatible with calculations in local coordinates, in the precise sense that $\\lambda_g(S \\cap U) = \\lambda_{g,x}(S \\cap U)$ for any Lebesgue measurable $S$ and any local coordinate chart $x : U \\to x(U) \\subset \\mathbb{R}^n$.\n\nIf $g^\\prime$ is any another Riemannian metric, then the Riemannian measures $\\lambda_g$ and $\\lambda_{g^\\prime}$ will be mutually absolutely continuous $\\sigma$-finite measures with smooth Radon\u2013Nikodym derivative computable directly in terms of $g$ and $g^\\prime$.\n\nSuppose that $M$ is orientable, and let $\\mathrm{vol}_g \\in \\Omega^n(M)$ be the Riemannian volume form defined by $g$. Then for any Riemann integrable $f$ on $M$,\n$$\n \\int_M f \\, \\mathrm{vol}_g = \\int_M f \\,d\\lambda_g,\n$$\nso that $\\lambda_g$ really is the (completed) Radon measure on $M$ corresponding to the positive functional $C_c(M) \\ni f \\mapsto \\int_M f \\, \\mathrm{vol}_g$ via the Riesz representation theorem. In other words, integration with respect to $\\lambda_g$ really is the \u201cLebesgue-ification\u201d of integration against the top-degree form $\\mathrm{vol}_g$.\n\n\nOnce you've constructed the Riemannian measure on your Riemannian manifold $(M,g)$, the sky is now the limit\u2014you can construct $L^p$ and Sobolev spaces of functions, vector fields, differential forms, tensor fields, etc., and in particular, you can use them to study, for instance, the geometric partial differential operators (e.g., generalisations of the Laplacian and the Dirac operator) and their associated partial differential equations (e.g., heat equations) to great mathematical effect. As a mathematical researcher, I'm personally most familiar with the mathematical ecosystem centred around the Atiyah\u2013Singer index theorem, which relates quantities from algebraic topology to functional-analytic computations on Riemannian manifolds, but you should be aware, for instance, that Perelman's proof of the Poincar\u00e9 conjecture involved the detailed analysis of a certain highly non-linear PDE for the Riemannian metric itself [!]. Perhaps the most accessible example of these methods in action is Hodge theory, which basically computes the cohomology of a compact manifold in terms of solutions of the Laplace equation (with respect to some Riemannian metric) on differential forms of various degrees.\nP.S. People tend to take the extension of Lebesgue theory from $\\mathbb{R}^n$ to manifolds more or less for granted, so precise accounts of this can be oddly hard to find. However, a precise if terse account of Lebesgue theory on manifolds can be found in Dieudonn\u00e9's Treatise of Analysis, Volume 3, Section 16.22 (especially Theorem 16.22.2 and the following discussion). Dieudonn\u00e9 doesn't require a Riemannian metric, but the point is that Riemannian metric gives a canonical choice of Lebesgue measure in the sense of Dieudonn\u00e9, in exactly the same way that it gives a canonical volume form in the orientable case. In fact, Lebesgue measures in the sense of Dieudonn\u00e9 can be identified with nowhere vanishing $1$-densities, and the construction of the Riemannian measure $\\lambda_g$ is really the construction of the canonical $1$-density $\\lvert \\mathrm{vol}_g \\rvert$ associated to $g$.\n\nADDENDUM\nOne can define a measurable $k$-form on $M$ to be a map $\\omega : M \\to \\wedge^k T^\\ast M$, such that the following hold.\n\nFor every $m \\in M$, $\\omega(m) \\in \\wedge^k T^\\ast M_m$ (i.e., $\\omega$ is a set-theoretic section of $\\wedge^k T^\\ast M$).\nFor every local coordinate chart $x : U \\to x(U) \\subset \\mathbb{R}^n$, the pullback $(x^{-1})^\\ast \\omega : x(U) \\to \\wedge^k \\mathbb{R}^n$ defined by\n$$\n (x^{-1})^\\ast\\omega := \\sum_{i_1 < \\cdots < i_k} \\omega\\left(\\tfrac{\\partial}{\\partial x^{i_1}},\\dotsc,\\tfrac{\\partial}{\\partial x^{i_k}}\\right) dx^{i_1} \\wedge \\cdots \\wedge dx^{i_k}\n$$\n(with the usual abuses of notation) is measurable; this turns out to be equivalent to requiring that $\\omega(X_1,\\dotsc,X_k) : M \\to \\mathbb{R}$ be measurable (in the above sense) for any smooth vector fields $X_1,\\dots,X_k \\in \\mathfrak{X}(M)$.\n\nNow, suppose that $N$ is an oriented $k$-dimensional submanifold of $M$ (compact and without boundary, for simplicity), and let $x : U \\to x(U) \\subset \\mathbb{R}^n$ be a local coordinate chart of $M$, such that $x(N \\cap U) = V_{x,N} \\times \\{0\\}$ for some open $V_{x,N} \\subset \\mathbb{R}^k$, and such that restriction of $x$ to a diffeomorphism $N \\cap U \\to V_{x,N}$ is orientation-preserving. Then we can define\n$$\n \\int_{N \\cap U} \\omega := \\int_{V_{x,N}} \\omega\\left(\\tfrac{\\partial}{\\partial x^{1}},\\dotsc,\\tfrac{\\partial}{\\partial x^{k}}\\right) d\\lambda_{\\mathbb{R}^k}\n$$\nwhenever the Lebesgue integral on the right-hand side exists (with $\\lambda_{\\mathbb{R}^k}$ the Lebesgue measure on $\\mathbb{R}^k$). We can then define $\\omega$ to be integrable on $N$ whenever it's integrable in this way on $N \\cap U$ for any suitable local coordinate chart $x : U \\to \\mathbb{R}^n$, and then, by exactly the same arguments as in the Riemann integral case, patch these local integrals into a global Lebesgue integral $\\int_N \\omega$, which turns out to be independent of all the choices of local coordinate chart and partition of unity made along the way.", "meta": {"post_id": 2857374, "input_score": 103, "output_score": 133, "post_title": "Lebesgue measure theory vs differential forms?"}}
{"input": "First, by definition  I assume  that $0.999...$ actually is defined as:\n$$\\text{lim}_{n\\rightarrow\\infty}\\sum_{i=1}^n 9/10^i$$\nNow by geometric series we already know that this equals one. But nonetheless here  is an explicit proof. The statement is:\n$$\\forall\\epsilon_+\\exists\\delta\\forall n (n>\\delta\\rightarrow|\\sum_{i=1}^n 9/10^i-1|<\\epsilon)$$\nWhich is equivalent to:\n$$\\forall\\epsilon_+\\exists\\delta\\forall n (n>\\delta\\rightarrow|\\sum_{i=0}^n 9/10^i-10|<\\epsilon)$$\nLet $\\epsilon>0$ be a real  number. Now note that $\\sum_{i=0}^{n} 9/10^i=10-1/10^n$. Choose $\\delta=\\text{max}(1,\\text{ceil(log}(1/\\epsilon)))$\nAs such: $|10-10-1/10^n|=1/10^n<1/10^{\\text{ceil(log}(1/\\epsilon))}\\leq 1/10^{\\text{log}(1/\\epsilon)}=\\epsilon$\nWhich works out nice...\nHowever, I have learned that $0.999...=1$  doesn't hold in all number systems such as hyperreals and surreals and  what not. I am not even  sure about rational numbers (although the proof looks like it would work for rationals with small tweaks)... I believe that the statement can't even be formulated in the first order theory of real closed fields as the $n$ is quantified over naturals, so there are number systems which can't even express the fact.\nWhat in the proof goes wrong in the non-standard number systems and what are the  primary features of the systems that cause this? Also, a proof of the fact NOT holding in said systems is welcome!", "output": "$0.\\bar{9}= 1$ does hold the hyperreals.\nThe fact you're referring to is something different, and unfortunately that difference is usually not made clear which leads to confusion like you had. What people are trying to say is that in the hyperreals, you can have a terminating decimal that nonetheless has infinitely many $9$'s. \nMore precisely, you still have decimal notation for the hyperreals, but the places are indexed by hyperintegers rather than ordinary integers. And if you take an infinite hyperinteger $H$, then\n$$ 1 - 10^{-H} = \\sum_{n=1}^H 9 \\cdot 10^{-n} = 0.\\underbrace{999\\ldots999}_{H\\text{ nines}} \\neq 1$$\nHowever, $0.\\bar{9}$ still refers to the nonterminating decimal that has a $9$ in every fractional place, and $0.\\bar{9} = 1$.", "meta": {"post_id": 2861566, "input_score": 13, "output_score": 34, "post_title": "How can $0.999\\dots$ not equal $1$?"}}
{"input": "I'm slightly confused on the subject of conjugates and how to define them.\nI know that for a complex number $ a - bi $ the conjugate is $ a + bi $ and similarly for $ 1 + \\sqrt 2 $ the conjugate is $ 1 - \\sqrt2 $ because when multiplied it gives a rational answer. \nBut how about for just a simple real number like 1 or 2, what would be the conjugate for this? Does a conjugate exist for a real number?\nI'm new to this topic and have tried searching Maths SE and Google in vain; any help would be appreciated.", "output": "Careful! These are two different notions of conjugate.\nFirst we have the complex conjugate, given by $\\overline{a+bi} = a-bi$. Then, since we can write a real number $x$ as $x+0i$, the complex conjugate of a real number is itself. \nThere is also a second idea of a rational conjugate, where as in your example, if $a,b$ are rational and $d$ is squarefree, the conjugate of $a+b\\sqrt{d}$ is $a-b\\sqrt{d}$. \nThere is a connection between these two ideas. In general, given a field extension $E/F$, take an algebraic element $\\alpha$ of $E$, and let $m(x)$ be it's minimal polynomial over $F$. Then we call the other roots of $m$ in $E$ the conjugates of $\\alpha$. \nIn the case of the extensions $\\mathbb{C}/\\mathbb{R}$ and  $\\mathbb{Q}(\\sqrt{d})/ \\mathbb{Q}$ this agrees with the above.", "meta": {"post_id": 2866943, "input_score": 12, "output_score": 43, "post_title": "Conjugate of real number"}}
{"input": "Consider the birthday problem. Given $N$ people, how many ways are there for there to exist some pair of people with the same birthday?\nEnumerating the possibilities quickly becomes tedious\nHowever, the complement problem (Given $N$ people, how many ways are there for no one to have the same birthday?) is trivial.\nIn fields like probability, this has obvious applications, due to the \"complement law\":\nif $A \\cup A^c = S$, where $S$ is the entire sample space, then $$P(A) + P(A^c) = 1 \\implies P(A) = 1 - P(A^c)$$\nIn general, this pattern is very common. Intuitively, I sense:\n\nsomehow, the complement problem is asking for a lot less information\nif one has something like the \"complement law\" in probability, then in some restricted scope of problems, the \"complement law\" gives in some sense, the \"same amount of information\"\n\nWhat do mathematicians call what I am getting at here? Am I overblowing how common a trend it is?", "output": "In combinatorics answering \u201cand\u201d style questions is easy because it is a multiplication. This is easy since you can remove common factors between denominators and numerators, and use the binomial/choice function.  Also any time a 1 or 0 comes up the operation becomes trivial.\nHowever asking \"or\" style questions is difficult since you have to add the numbers and then work out where you have a double count and subtract them.\nDe Morgan's laws $\\neg ( a \\vee b) = ( \\neg a \\wedge \\neg b)$ allows you to transform a \u201cor\u201d problem into a \u201cnot and\u201d problem which is easier.", "meta": {"post_id": 2869676, "input_score": 17, "output_score": 34, "post_title": "In combinatorics, why is asking the opposite problem often times easier?"}}
{"input": "You are a student, assigned to work in the cafeteria today, and it is your duty to divide the available food between all students. The food today is a sausage of 1m length, and you need to cut it into as many pieces as students come for lunch, including yourself.\nThe problem is, the knife is operated by the rotating door through which the students enter, so every time a student comes in, the knife comes down and you place the cut. There is no way for you to know if more students will come or not, so after each cut, the sausage should be cut into pieces of approximately equal length. \nSo here the question - is it possible to place the cuts in a manner to ensure the ratio of the largest and the smallest piece is always below 2?\nAnd if so, what is the smallest possible ratio?\nExample 1 (unit is cm):\n\n1st cut: 50 : 50     ratio: 1  \n2nd cut: 50 : 25 : 25   ratio: 2 - bad\n\nExample 2\n\n1st cut: 40 : 60              ratio: 1.5\n2nd cut: 40 : 30 : 30            ratio: 1.33\n3rd cut: 20 : 20 : 30 : 30    ratio: 1.5\n4th cut: 20 : 20 : 30 : 15 : 15  ratio: 2 - bad\n\nSorry for the awful analogy, I think this is a math problem but I have no real idea how to formulate this in a proper mathematical way.", "output": "TLDR: $a_n=\\log_2(1+1/n)$ works, and is the only smooth solution.\nThis problem hints at a deeper mathematical question, as follows. As has been observed by Pongr\u00e1cz, there is a great deal of possible variation in solutions to this problem. I would like to find a \"best\" solution, where the sequence of pieces is somehow as evenly distributed as possible, given the constraints.\nLet us fix the following strategy: at stage $n$ there are $n$ pieces, of lengths $a_n,\\dots,a_{2n-1}$, ordered in decreasing length. You cut $a_n$ into two pieces, forming $a_{2n}$ and $a_{2n+1}$. We have the following constraints:\n$$a_1=1\\qquad a_n=a_{2n}+a_{2n+1}\\qquad a_n\\ge a_{n+1}\\qquad a_n<2a_{2n-1}$$\nI would like to find a nice function $f(x)$ that interpolates all these $a_n$s (and possibly generalizes the relation $a_n=a_{2n}+a_{2n+1}$ as well).\nFirst, it is clear that the only degree of freedom is in the choice of cut, which is to say if we take any sequence $b_n\\in (1/2,1)$ then we can define $a_{2n}=a_nb_n$ and $a_{2n+1}=a_n(1-b_n)$, and this will completely define the sequence $a_n$.\nNow we should expect that $a_n$ is asymptotic to $1/n$, since it drops by a factor of $2$ every time $n$ doubles. Thus one regularity condition we can impose is that $na_n$ converges. If we consider the \"baseline solution\" where every cut is at $1/2$, producing the sequence\n$$1,\\frac12,\\frac12,\\frac14,\\frac14,\\frac14,\\frac14,\\frac18,\\frac18,\\frac18,\\frac18,\\frac18,\\frac18,\\frac18,\\frac18,\\dots$$\n(which is not technically a solution because of the strict inequality, but is on the boundary of solutions), then we see that $na_n$ in fact does not tend to a limit - it varies between $1$ and $2$.\nIf we average this exponentially, by considering the function $g(x)=2^xa_{\\lfloor 2^x\\rfloor}$, then we get a function which gets closer and closer to being periodic with period $1$. That is, there is a function $h(x):[0,1]\\to\\Bbb R$ such that $g(x+n)\\to h(x)$, and we need this function to be constant if we want $g(x)$ itself to have a limit.\nThere is a very direct relation between $h(x)$ and the $b_n$s. If we increase $b_1$ while leaving everything else the same, then $h(x)$ will be scaled up on $[0,\\log_2 (3/2)]$ and scaled down on $[\\log_2 (3/2),1]$. None of the other $b_i$'s control this left-right balance - they make $h(x)$ larger in some subregion of one or the other of these intervals only, but preserving $\\int_0^{\\log_2(3/2)}h(x)\\,dx$ and $\\int_{\\log_2(3/2)}^1h(x)\\,dx$.\nThus, to keep these balanced we should let $b_1=\\log_2(3/2)$. More generally, each $b_n$ controls the balance of $h$ on the intervals $[\\log_2(2n),\\log_2(2n+1)]$ and $[\\log_2(2n+1),\\log_2(2n+2)]$ (reduced$\\bmod 1$), so we must set them to\n$$b_n=\\frac{\\log_2(2n+1)-\\log_2(2n)}{\\log_2(2n+2)-\\log_2(2n)}=\\frac{\\log(1+1/2n)}{\\log(1+1/n)}.$$\nWhen we do this, a miracle occurs, and $a_n=\\log_2(1+1/n)$ becomes analytically solvable:\n\\begin{align}\na_1&=\\log_2(1+1/1)=1\\\\\na_{2n}+a_{2n+1}&=\\log_2\\Big(1+\\frac1{2n}\\Big)+\\log_2\\Big(1+\\frac1{2n+1}\\Big)\\\\\n&=\\log_2\\left[\\Big(1+\\frac1{2n}\\Big)\\Big(1+\\frac1{2n+1}\\Big)\\right]\\\\\n&=\\log_2\\left[1+\\frac{2n+(2n+1)+1}{2n(2n+1)}\\right]\\\\\n&=\\log_2\\left[1+\\frac1n\\right]=a_n.\n\\end{align}\nAs a bonus, we obviously have that the $a_n$ sequence is decreasing, and if $m<2n$, then\n\\begin{align}\n2a_m&=2\\log_2\\Big(1+\\frac1m\\Big)=\\log_2\\Big(1+\\frac1m\\Big)^2=\\log_2\\Big(1+\\frac2m+\\frac1{m^2}\\Big)\\\\\n&\\ge\\log_2\\Big(1+\\frac2m\\Big)>\\log_2\\Big(1+\\frac2{2n}\\Big)=a_n,\n\\end{align}\nso this is indeed a proper solution, and we have also attained our smoothness goal \u2014 $na_n$ converges, to $\\frac 1{\\log 2}=\\log_2e$. It is also worth noting that the difference between the largest and smallest piece has limit exactly $2$, which validates Henning Makholm's observation that you can't do better than $2$ in the limit.\nIt looks like this (rounded to the nearest hundred, so the numbers may not add to 100 exactly):\n\n$58:42$, ratio = $1.41$\n$42:32:26$, ratio = $1.58$\n$32:26:22:19$, ratio = $1.67$\n$26:22:19:17:15$, ratio = $1.73$\n$22:19:17:15:14:13$, ratio = $1.77$\n\nIf you are working with a sequence of points treated$\\bmod 1$, where the intervals between the points are the \"sausages\", then this sequence of segments is generated by $p_n=\\log_2(2n+1)\\bmod 1$. The result is beautifully uniform but with a noticeable sweep edge:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nA more concrete optimality condition that picks this solution uniquely is the following: we require that for any fraction $0\\le x\\le 1$, the sausage at the $x$ position (give or take a sausage) in the list, sorted in decreasing order, should be at most $c(x)$ times smaller than the largest at all times. This solution achieves $c(x)=x+1$ for all $0\\le x\\le 1$, and no solution can do better than that (in the limit) for any $x$.", "meta": {"post_id": 2882265, "input_score": 65, "output_score": 64, "post_title": "Optimal strategy for cutting a sausage?"}}
{"input": "Suppose $$A=\\pmatrix{1&2&3&4&5\\\\2&3&4&5&6}$$ Find $\\det(A^TA)$.\n\nI know exactly how to calculate it by writing it as a $5\\times5$ matrix. But how to calculate it smartly?", "output": "Rank of $A$ is $2$, hence $A^TA$ cannot be rank $5$ and it must be singular.\nHence the determinant must be $0$.", "meta": {"post_id": 2887805, "input_score": 9, "output_score": 38, "post_title": "$A=\\pmatrix{1&2&3&4&5\\\\2&3&4&5&6}.$ Find $\\det(A^TA)$."}}
{"input": "I recently started to study topology, I have no idea about the subject so my question could be very simple but I need a clear explanation.  It is about the page number 19 of Introducton to Topology by Colin Adams and Robert Franzosa; it said that the shapes:\n\n\nare equivalent in topology, but one has just one hole and the other has two. is possible to add holes or stick holes?", "output": "Look a bit more closely at the second picture. There's a couple of little dotted lines connecting the two holes that may be a bit hard to see.\n\nThat is meant to convey the impression they are the two ends of a single, long, curved hole through the interior.", "meta": {"post_id": 2894411, "input_score": 24, "output_score": 55, "post_title": "How is possible that those shapes are equivalent in topology?"}}
{"input": "Following is the question asked in a recent aptitude exam:  \nGiven that :\n$$ a+b+ab=10\\\\ b+c+bc=20 \\\\ c+a+ac=30$$\nWhat is the value of $a+b+c+abc$ ?\nI can solve it by finding the individual values by:\n$$ a = \\frac{10-b}{b+1} ,\\\\ c = \\frac{20-b}{b+1}$$\nPutting these in the third equation:\n$$ \\frac{10-b}{b+1}+\\frac{20-b}{b+1} + \\frac{(10-b)\\times(20-b)}{(b+1)^2}=30\\\\\n(30-2b)\\times(b+1)+(b-20)\\times(b-10)=30\\times(b+1)\\\\{30\\times(b+1)}-2b(b+1)+b^2-30b+200={30\\times(b+1)}\\\\-2b^2-2b+b^2-30b+200=0\\\\-b^2-32b+200=0\\\\b^2+32b-200=0$$\nWhich can be solved to:\n$$b= \\frac{-32+-\\sqrt{32^2-4\\times(-200)}}{2} \\approx \\frac{-32+-42.71}{2} = 5.35 \\text{  or} -37.35 $$\nAnd we can get the values for $a$ and $c$ as well:\n$$a \\approx 9.157 \\text{    or} -1.297\\\\ c\\approx 2.30 \\text{    or} -1.577$$\nHere, the for the first two equations, $b=-37.35$, $a=-1.297$ and $c=-1.577$ works. Whereas, the third equation is satisfied by $a=9.157$ and $c=2.30$. This doesn't seem correct. Since only a single value of $b$ works for the first two equations, therefore the other value of $b$ is rejected. This also rejects the derived values of $a$ and $c$. So, the positive values of $a$ and $c$ should not be used. But the former values don't satisfy the third equation, but the latter values do. What is the problem here?\nMoreover, is there any shorter way of solving this problem?", "output": "Just add one\nAdd one to each equation  and factorize to get :\n$$\n(a+1)(b+1) = 11 \\\\\n(b+1)(c+1) = 21 \\\\\n(a+1)(c+1) = 31 \\\\\n$$\nNow, set $x ,y,z = a+1,b+1,c+1$ respectively.This gives $xy = 11, yz = 21,zx = 31$, so $x^2y^2z^2 = 11 \\times 21\\times 31$ by multiplying these.\nNow divide this equation by the other equations suitably to get the values of $x,y,z$, and use the fact that $a+b+c + abc = x+y+z - 3 + (x-1)(y-1)(z-1)$ to get the answer.", "meta": {"post_id": 2903586, "input_score": 14, "output_score": 46, "post_title": "Solving an equality with 3 equations, and 3 variables"}}
{"input": "I found an easy method for division and it depends on some factors.\nI wanted to find an answer for $1000/101$ with easy steps. My starting point is here. I formulated this method by 2 hours of hard work. It is an infinite series, but taking 4 or 6 units of the series we can arrive at an answer easily.\nTo find $100/11$\nWe know $100/10 = 10$, then $100/11 = 10 - 1 + 0.1 - 0.01$ (each term is the preceding term divided by $10$; division by $10$ is easy) $= 9 + 0.09$ (simplify by taking two members of the series, so easy steps) $= 9.09$\nTo find $100/12$\nWe know $100/11 = 9.09$, then $100/12 = 9.09 - 0.909 + 0.0909 - 0.00909$ (each member of the series is found by dividing by $10$ with the preceding member) giving $8.181+.08181$ (calculations are easy) or $8.29$\nI took only $4$ members of the series, and if we take $6$ members of the series, we get a better result.\nTo find $1000/101$\nWe know $1000/100 = 10$, then $1000/101 = 10 - 0.1 + 0.001 - .0001\n= 9.9 + 0.0009 = 9.9009$\nTo find $1000/102$\nWe know, $1000/101 = 9.9$ approximately, then $1000/102 = 9.9 - 0.099 + 0.00099- 0.0000099 = 9.801 + 0.0009801 = 9.801$ (approx)\nTo find $100/3$\nWe know $100/2 = 50$, then $100/3 = 50 - 25 + 12.5 - 6.25 + 3.125 - 1.5625 = 25 + 6.25 + 1.5625 = 32.8125$ or approximately $33$.\nSince they are a small number to get a perfect answer (we need to take more members of the series; for big numbers $4$ members of the series is sufficient)\nHas somebody found this before me? Where should I submit the infinite series that I have found for further evaluation?", "output": "What you are using are so called geometric series:\n$$\\frac1{1+n}=\\frac1n\\frac1{1-(-1/n)}=\\frac1n\\sum_{k=0}^{\\infty}\\left(-\\frac1n\\right)^k$$\nthus\n$$\\frac{x}{1+n}=\\frac xn\\sum_{k=0}^{\\infty}\\left(-\\frac1n\\right)^k=\\frac{x}n-\\frac{x}{n^2}+\\frac{x}{n^3}-\\dots.$$\nOf course the fact that you found the result by observation is quite impressive.", "meta": {"post_id": 2910471, "input_score": 32, "output_score": 56, "post_title": "I have found a formula for dividing numbers in easy steps"}}
{"input": "Suppose $X$ and $Y$ are two metric spaces and $f: X\\to Y$ be a continuous bijection.\nNow my question is does the completeness of $X$ and $Y$ implies $f$ to be a Homeomorphism?\n\nMy idea. First of all I try to prove $f$ to be a closed map assuming $X$ and $Y$ be a complete metric space. But this idea didn't work.\nI know if $X$ is given to be compact then whether or not $Y$ complete given initially  $f$ becomes a homeomorphism. But that is not the case here. So I try to find a counter example.\nI take $Y=\\Bbb{R}$ and try to choose $X$ to be a non compact but closed subset of $\\Bbb{R}$ (and  $\\Bbb{R}^2$) but the problem is in that situation the bijections I found was not continuous. Also I cannot found any example beyond the metric spaces $\\Bbb{R}$ or $\\Bbb{R}^2$ as my $X$.\nCan any one help me to figure out how to construct an counter example here. Thanks ...", "output": "The identity map from $\\mathbb  R$ with discrete metric into $\\mathbb  R$ with usual metric  is a continuous bijection which is not  a homeomorphism. Both spaces are complete.", "meta": {"post_id": 2912793, "input_score": 15, "output_score": 41, "post_title": "A continuous bijection between two complete metric spaces that is not a homeomorphism."}}
{"input": "I am reading this text:\n\nWhen in the real world are we checking to see if sets are vector spaces or not? The examples above seem like really specific sets...\nAre there any places where we redefined scalar multiplication like this?", "output": "You'll never have to prove something is a vector space in real life. You may want to prove something is a vector space, because vector spaces have a simply enormous amount of theory proven about them, and the non-trivial fact that you wish to establish about your specific object might boil down to a much more general, well-known result about vector spaces.\nHere's my favourite example. It's still a little artificial, but I came by it through simple curiousity, rather than any course, or the search for an example.\nConsider the logic puzzle here. It's a classic. You have a $5 \\times 5$ grid of squares, coloured black or white. Every time you press a square, it changes the square and the (up to four) adjacent squares from black to white or white to black. Your job is to press squares in such a way that you end up with every square being white.\nSo, my question to you is, can you form any configuration of white and black squares by pressing these squares? Put another way, is any $5 \\times 5$ grid of black and white squares a valid puzzle with a valid solution?\nWell, it turns out that this can be easily answered using linear algebra. We form a vector space of $5 \\times 5$ matrices whose entries are in the field $\\mathbb{Z}_2 = \\lbrace 0, 1 \\rbrace$. We represent white squares with $0$ and black squares with $1$. Such a vector space is finite, and contains $2^{25}$ vectors. Note that every vector is its own additive inverse (as is the case for any vector space over $\\mathbb{Z}_2$).\nAlso note that the usual standard basis, consisting of matrices with $0$ everywhere, except for a single $1$ in one entry, forms a basis for our vector space. Therefore, the dimension of the space is $25$.\nPressing each square corresponds to adding one of $25$ vectors to the current vector. For example, pressing the top left square will add the vector\n$$\\begin{pmatrix}\n1 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{pmatrix}.$$\nWe are trying to find, therefore, a linear combination of these $25$ vectors that will sum to the current vector (remember $-v = v$ for all our vectors $v$).\nSo, my question that I posed to you, boils down to asking whether these $25$ vectors span the $25$-dimensional space. Due to standard results in finite-dimensional linear algebra, this is equivalent to asking whether the set of $25$ vectors is linearly independent.\nThe answer is, no, they are not linearly independent. In particular, if you press the buttons highlighted in the following picture, you'll obtain the white grid again, i.e. the additive identity.\n\nTherefore, we have a non-trivial linear combination of the vectors, so they are linearly dependent, and hence not spanning. That is, there must exist certain configurations that cannot be attained, i.e. there are invalid puzzles with no solution.\nThe linear dependency I found while playing the game myself, and noticing some of the asymmetries of the automatically generated solutions, even when the problem itself was symmetric. Proving the linear dependence is as easy as showing the above picture. I still don't know of an elegant way to find an example of a puzzle that can't be solved though! So, my proof is somewhat non-constructive, and very easy if you know some linear algebra, and are willing to prove that a set is a vector space.", "meta": {"post_id": 2919706, "input_score": 19, "output_score": 68, "post_title": "Vector spaces. When in the real world are we checking if it's a vector space or not?"}}
{"input": "I'm asking this question because I was unable to find an answer elsewhere as most questions are about the summation of different irrational numbers, which is not what this question is about. Here, I'm interested in demonstrating that the result of the summation of the same irrational number is always irrational: $\\sum_{i=1}^n a$, where $n$ is a non-negative integer $>0$ and $a$ is an irrational constant.", "output": "It is trivially so. If you sum the irrational number $x$ $n$ times ($n$ being an integer \"of course\"), you end up with $nx$. \nIf $nx=\\frac ab$, with $(a,b)$ integers, then $x=\\frac{a}{nb}$, thus is rational, which is not true...", "meta": {"post_id": 2929008, "input_score": 18, "output_score": 77, "post_title": "Why is $\\sum_{i=1}^n a$ always irrational if $n>0$ and $a$ is irrational?"}}
{"input": "In terms of purported proof of Atiyah's Riemann Hypothesis, my question is what is the Todd function that seems to be very important in the proof of Riemann's Hypothesis?", "output": "While there might be an interesting question here about the older math that Atiyah references, it is worth pointing out what Atiyah actually says about the function $T$:\n\n$T : \\mathbb{C} \\to \\mathbb{C}$ (this is in Section 3.4 of the longer paper \"The Fine Structure Constant\").\n$T$ is \"real\" (see 2.2 of the shorter paper \"The Riemann Hypothesis\" - it doesn't mean \"real-valued\").\n$T(1) = 1$ (2.3 of the shorter paper \"The Riemann Hypothesis\").\nOn any compact, convex set $K$, $T$ is a polynomial of some degree and the degree is in principle allowed to depend on the set $K$ (this is in the start of Section 2).\nIf $f$ and $g$ are power series with no constant term then\n$$\nT\\Bigl( 1 + f(s) + g(s) + f(s)g(s)\\Bigr) = T\\Bigl(1 + f(s) + g(s)\\Bigr)\n$$\n(this is 2.6 of shorter paper).\n\nThe following proof is from a Redditor :\nSet $f(s) = e^s - 1$ and $g(s) = 1 - e^s$. Point 5. then implies that\n$$\nT\\Bigl( 1 + e^s - 1 + 1 - e^s + (e^s - 1)(1 - e^s)\\Bigr) = T(1)\n$$\ni.e.(using 3.):\n$$\nT\\Bigl( e^s(2-e^s))\\Bigr) = 1.\n$$\nNow notice that the function $e^s(2-e^s)\\rvert_{\\mathbb{R}}$ takes any value in $(-\\infty,1)$. To see this claim you can solve\n$$\ne^x(2-e^x) = y\\ \\Leftrightarrow e^{2x} - 2e^x + y = 0\n$$\nby using the quadratic formula and taking logarithms to get a real solution when $y < 1$. This shows that $T\\rvert_{(-\\infty,1)}$ is constant.\nOK so now take a compact, convex set $K \\subset \\{ \\mathrm{Re}(z) < 1\\}$ that contains an interval on the real line, i.e. $K$ contains a subinterval $I$ of $(-\\infty,1)$. From the properties 2. and 4. we know that $T\\rvert_K$ is a polynomial with real coefficients. But we also know it is constant on $I$ which means $T \\rvert_K$ is constant.\nSince $K$ was arbitrary we can easily exhaust$ \\{ \\mathrm{Re}(z) < 1\\}$ by compact, convex sets to show that $T$ is constant on $\\{ \\mathrm{Re}(z) < 1\\}$, in particular, this includes the critical strip.", "meta": {"post_id": 2930742, "input_score": 47, "output_score": 38, "post_title": "What is the Todd's function in Atiyah's paper?"}}
{"input": "I was watching a video and the lecturer discusses the function\n\n$$\\frac{1}{1+x^2} = \\sum_{n=0}^{\\infty} {(-1)^n x^{2n}}$$\n$$|x| < 1$$\n\nexplaining that the radius of convergence for this Taylor series centered at $x=0$ is 1 because it is being affected by $i$ and $-i$. Then, he goes on to talk about how real analysis is a glimpse into complex analysis.\nIn the same video, the lecturer also provides the following example where a complex function is defined by using real Taylor series:\n\n\\begin{align}\ne^{ix}\n&= \\cos(x) +i \\sin(x) \\\\\n&= \\sum_{n=0}^\\infty \\frac{(ix)^n}{n!} \\\\\n&= \\sum_{n=0}^\\infty (-1)^n \\frac{x^{2n}}{2n!} + i \\sum_{n=0}^\\infty (-1)^n \\frac{x^{2n+1}}{(2n+1)!}\n\\end{align}\n\nCan someone help elaborate by what the lecturer probably meant? What is the connection between real analysis and complex analysis?\nI understand that there are two different types of analyticity: real analytic and complex analytic. Are they connected?", "output": "This is an interesting question, but one that might be hard to address completely. Let me see what I can do to help.\n\nReal Power Series:\nThe easiest way to address the connection between the two subjects is through the study of power series, as you have already had alluded to you. A power series is a particular kind of infinite sum (there are many different kinds of these) of the form\n$$\nf(x) = \\sum_{k=0}^{\\infty}a_k x^k.\n$$\nThey get their name from the fact that we are adding together powers of $x$ with different coefficients. In real analysis the argument of such a function (the \"$x$\" in $f(x)$) is taken to be a real number. And depending on the coefficients that are being multiplied with the powers of $x$, we get different intervals of convergence (intervals on which the sequence of partial sums converges.) For example, if $a_k$ is the $k$th Fibonacci number, then the radius of convergence ends up being $1/\\phi$, where $\\phi = (1+\\sqrt{5})/2$ is the \"golden ratio\".\nThe most common kind of power series that come up in calculus (and real analysis) are Taylor series or Maclaurin series. These series are created to represent (a portion) of some differentiable function. Let me try to make this a little more concrete. Pick some function $f(x)$ that is infinitely differentiable at a fixed value, say at $x=a$. The Taylor series corresponding to $f(x)$, centered at $a$, is given by\n$$\n\\sum_{k=0}^{\\infty}\\frac{f^{(k)}(a)}{k!}(x-a)^k.\n$$\nA Maclaurin series is just a Taylor series where $a=0$. After playing around with the series a bit, you may notice a few things about it.\n\nWhen $x=a$, the only non-zero term in the series is the first one: the constant $f(a)$. This means that no matter what the radius of convergence is for the series, it will at least agree with the original function at this one point.\nTaking derivatives of the power series and evaluating them at $x=a$, we see that the power series and the original function $f(x)$ have the same derivatives at $a$. This is by construction, and not a coincidence.\nIf the radius of convergence is positive, then the interval on which the series converges will converge to the original function $f(x)$. In this way, we can say that $f(x)$ \"equals\" it's Taylor series, understanding that this equality may only hold on some interval centered at $a$, and perhaps not on the entire domain that $f(x)$ is defined on. You've already seen such an example with $(1+x^2)^{-1}$. In some extreme cases, such as with $\\sin x$ and $\\cos x$ as mentioned above, this equality ends up holding for ALL real values of $x$, and so the convergence is on all of $\\mathbb{R}$ and there is no harm completely equating the function with it's Taylor series.\nEven if the radius of convergence of a particular Taylor series (centered at $a$) is finite, it does not mean you cannot take other Taylor series (centered at other values than $a$) that also have a positive radius of convergence. For example, even though the Maclaurin series for $(1+x^2)^{-1}$ does not converge outside of the interval of radius 1, you can compute the Taylor series centered at $a=1$,\n\\begin{align}\n\\frac{1}{1+x^2} &= \\sum_{k=0}^{\\infty} \\left( \\frac{1}{k!}  \\frac{d^k}{dx^k}\\left(\\frac{1}{1+x^2}\\right)\\Bigg|_{x=1}(x-1)^k \\right) \\\\\n&= \\frac{1}{2}-\\frac{1}{2}(x-1)+\\frac{1}{2}(x-1)^2-3(x-1)^4+\\cdots, \\quad \\text{for $|x-1|<\\sqrt{2}$}.\n\\end{align}\n\nYou will then find that this new power series converges on a slightly larger radius than 1 (that's the $\\sqrt{2}$ mentioned above), and that the two power series (one centered at 0, the other centered at 1) overlap and agree for certain values of $x$.\nThe big take away that you should have about power series and Taylor series is that they are one and the same. Defined a function by a power series, and then take it's Taylor series centered at the same point; you will get the same series. Conversely, any infinitely-differentiable function that has a Taylor series with positive radius of convergence is uniquely determined by that power series.\nThis is where complex analysis beings to come into play...\n\nComplex Power Series:\nComplex numbers have many similar properties to real numbers. They form and algebra (you can do arithmetic with them); the only number you still cannot divide by is zero; and the absolute value of a complex number still tells you the distance from 0 that number is. In particular there is nothing stopping you from defining power series of complex numbers, with z=x+iy:\n$$\nf(z) = \\sum_{k=0}^{\\infty}c_k z^k.\n$$\nThe only difference now is that the coefficients $c_k$ can be complex numbers, and the radius of convergence is now relating to the radius of a circle (as opposed to the radius of an interval). Things may seem exactly like in the real-valued situation, but there is more lurking beneath the surface.\nFor starters, let me define some new vocabulary terms.\n\nWe say that a complex function is complex differentiable at some fixed value $z=w$ if the following limit exists:\n$$\n\\lim_{z\\to w}\\frac{f(z)-f(w)}{z-w}.\n$$\nIf this limit exists, we denote the value of the limit by $f'(w)$. This should look familiar, as it is the same limit definition for real-valued derivatives.\nIn the same way that we could create Taylor series for real-valued functions, we can also create Taylor series  (center at $z=w$) for complex-valued functions, provided the functions have an infinite number of derivatives at $w$ (sometimes referred to as being holomorphic at $w$):\n$$\n\\sum_{k=0}^{\\infty}\\frac{f^{(k)}(w)}{k!}(z-w)^k.\n$$\nIf the Taylor series defined above has a positive radius of convergence then we say that $f$ is analytic at $w$. This vocabulary is also used in the case of real-valued Taylor series.\n\nPerhaps the biggest surprise in complex analysis is that the following conditions are all equivalent:\n\n$f(z)$ is complex differentiable at $z=w$.\n$f(z)$ is holomorphic at $w$. That is $f$ has an infinite number of derivatives at $z=w$. In real analysis this condition is sometimes referred to as being \"smooth\".\n$f(z)$ is analytic at $z=w$. That is it's Taylor series converges to $f(z)$ with some positive radius of convergence.\n\nThis means that being differentiable in the complex sense is a much harder thing to accomplish than in the real sense. Consider the contrast with the \"real-valued\" equivalents of the points made above:\n\nIn real analysis there are a number of functions with only finitely many derivatives. For example, $x^2 \\sin(1/x)$ has only one derivative at $x=0$, and $f'(x)$ is not even continuous, let alone twice differentiable.\nThere are also real-valued functions that are smooth (infinitely-differentiable) yet do not have a convergent Taylor series. An example is the function $e^{-1/x^2}$ which is smooth for every $x$, but for which every order of derivative at $x=0$ is equal to zero; this means every coefficient in the Maclaurin series is zero, and the radius of convergence is zero as well.\n\nThese kinds of pathologies do not occur in the complex world: one derivative is as good as an infinite number of derivatives; differentiability at one point translates to differentiability on a neighborhood of that point.\n\nLaurent  Series:\nA natural question that one might ask is what dictates the radius of convergence for a power series? In the real-valued case things seemed to be fairly unpredictable. However, in the complex-valued case things are much more elegant.\nLet $f(z)$ be differentiable at some point $z=w$. Then the radius of convergence for the Taylor series of $f(z)$ centered at $w$ will be the distance to the nearest complex number at which $f(z)$ fails to be differentiable. Think of it like dropping a pebble into a pool of water. The ripples will extend radially outward from the initial point of differentiability, all the way until the circular edge of the ripple hits the first \"singularity\" -- a point where $f(z)$ fails to be differentiable.\nTake the complex version of our previous example,\n$$\nf(z) = \\frac{1}{1+z^2}.\n$$\nThis is a ration function, and will be smooth for all values of $z$ where the denominator is non-zero. Since the only roots of $z^2 + 1$ are $z = i$ and $z = -i$, then $f(z)$ is differentiable/smooth/analytic at all values $w\\neq \\pm i$. This is precisely why the radius of convergence for the real-valued Maclaurin series is 1, as you've already noted: the shortest distance from $z=0$ to $z=\\pm i$ is 1. The real-valued Maclaurin series is just a \"snapshot\" or \"sliver\" of the complex-valued Taylor series centered at $z=0$. This is also why the radius of convergence for the real-valued Taylor series increases when you move away from zero; the distance to $\\pm i$ becomes greater, and so the complex-valued Taylor series can converge on a larger disk.\nSo now should come the question: when exactly does a complex function fail to be differentiable?\nWithout going into too many details from complex analysis, suffice it to say that complex functions fail to be differentiable when one of three things occurs:\n\nThe function has a \"singularity\" (think division by zero).\nThe function is defined in terms of the complex conjugate, $\\bar{z}$. For example, $f(z) = |z|^2 = z\\,\\bar{z}$.\nThe function involves logarithms or non-integer exponents (these two ideas are actually related).\n\nNumber 2. is perhaps the most egregious of the three issues, and it means that functions like $f(z) = |z|$ are actually differentiable nowhere. This is in stark contrast to the real-valued version $f(x) = |x|$, which is differentiable everywhere except at $x=0$ where there is a \"corner\".\nNumber 3. is actually not too bad, and it turns out that these kinds of functions are usually differentiable everywhere except along certain rays or line segments. To get into it further, however, will take us too far off course.\nNumber 1. is the best case scenario, and is the focus of this section of our discussion. Essentially, singularities are places where division by zero has occurred, and the extend to which something has \"gone wrong\" can be quantified. Let me try to elaborate.\nConsider the previous example of $f(z) = (1+z^2)^{-1}$. Again, since the denominator can factor into the product $(z+i)(z-i)$, then this means we could \"erase\" the singularity at $z=i$ by multiplying the function by a copy of $z-i$. In other words if\n$$\n g(z) = (z+i)f(z) = \\frac{z-i}{1+z^2},\n$$\nthen $g(z) = z+i$ for all $z\\neq i$, and $\\lim_{z\\to i}g(z) = 2i$ exists, and is no longer a singularity.\nSimilarly, if $f(z) = 1/(1+z^2)^3$, then we again have singularities at $z=\\pm i$. This time, however, multiplying $f(z)$ by only one copy of $z-i$ will not remove the singularity at $z=i$. Instead, we would need to multiply by three copies to get\n$$\n g(z) = (z-i)^3 f(z) = \\frac{(z-i)^3}{(1+z^2)^3},\n$$\nwhich again means that $g(z) = (z+i)^3$ for all $z\\neq i$, and that $\\lim_{z\\to i}g(z) = (2i)^3 = -8i$ exists.\nSingularities like this --- ones that can be \"removed\" through the use of multiplication of a finite number of linear terms --- are called poles. The order of the pole is the minimum number of liner terms need to remove the singularity.\nThe real-valued Maclaurin series for $\\sin x$ is given by\n$$\n \\sin x = \\sum_{k=0}^{\\infty} \\frac{(-1)^{k}}{(2k+1)!}x^{2k+1},\n$$\nand has a infinite radius of convergence. This means that the complex version\n$$\n \\sin z = \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{(2k+1)!}z^{2k+1} = z - \\frac{1}{3!}z^3 + \\frac{1}{5!}z^5 - \\cdots\n$$\nalso has an infinite range of convergence (such functions are called entire) and hence no singularities. From here it's easy to see that the function $(\\sin z)/z$ is analytic as well, with Taylor series\n$$\n\\frac{\\sin z}{z} = \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{(2k+1)!}z^{2k} = 1 - \\frac{1}{3!}z^2 + \\frac{1}{5!}z^4 - \\cdots\n$$\nHowever, a function like $(\\sin z)/z^3$ is not analytic at $z=0$, since dividing $\\sin z$ by $z^3$ would give us the following expression:\n$$\n \\frac{\\sin z}{z^3} = \\frac{1}{z^2} - \\frac{1}{3!} + \\frac{1}{5!}z^2 - \\cdots\n$$\nBut notice that if we were to subtract the term $1/z^2$ from both sides we would be left again with a proper Taylor series\n$$\n \\frac{\\sin z}{z^3} - \\frac{1}{z^2} = \\frac{\\sin z - z}{z^3} = \\frac{1}{3!} + \\frac{1}{5!}z^2 - \\frac{1}{7!}z^4 + \\cdots\n$$\nThis idea of extending the idea of Taylor series to include terms with negative powers of $z$ is what is referred to a Laurent series. A Laurent series is a power series in which the powers of $z$ are allowed to take on negative values, as well as positive:\n$$\n f(z) := \\sum_{k = -\\infty}^{\\infty} c_k z^k\n$$\nIn this way we can expand complex functions around singular points in a fashion similar to expanding around analytic points.\nA pole, it turns out, is a singular point for which there are a finite number of terms with negative powers, such as with $(\\sin z)/z^3$. If, however, an infinite number of negative powers are needed to fully express a Laurent series, then this type of singular point is called an essential singularity. An excellent example of such a function can be made by taking an analytic function (one with a Taylor series) and replacing $z$ by $1/z$:\n\\begin{align}\n\\sin(1/z) &= \\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k+1)!}(1/z)^k \\\\\n&= \\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k+1)!}z^{-k} \\\\\n&= \\sum_{k=-\\infty}^{0}\\frac{(-1)^{-k}}{(1-2k)!}z^{k}\n\\end{align}\nThese kinds of singularities are quite severe and the behavior of complex functions around such a point is rather erratic. This also explains why the real-valued function $e^{-1/x^2}$ was so pathological. The Taylor series for $e^z$ is given by\n$$\ne^z = \\sum_{k=0}^{\\infty}\\frac{1}{k!}z^k\n$$\nand so\n\\begin{align}\ne^{-1/z^2} &= \\sum_{k=0}^{\\infty}\\frac{1}{k!}(-1/z^2)^k \\\\\n&= \\sum_{k=0}^{\\infty}\\frac{1}{k!}(-1)^k z^{-2k} \\\\\n&= \\sum_{k=-\\infty}^{0}\\frac{1}{(-k)!}(-1)^{-k} z^{2k}.\n\\end{align}\nHence there is an essential singularity at $z=0$, and so even though the real-valued version is smooth at $x=0$, there is no hope of differentiability in a disk around $z=0$.", "meta": {"post_id": 2936156, "input_score": 29, "output_score": 44, "post_title": "Are real and complex analysis related in any way?"}}
{"input": "I am a bit confused about the derivation of MLE of Uniform$(0,\\theta)$.\nI understand that $L(\\theta)={\\theta}^{-n}$ is a decreasing function and to find the MLE we want to maximize the likelihood function.\nWhat is confusing me is that if a function is decreasing, then wouldn't the function be maximized at the smallest input rather than the largest?\nThank you in advance for your help.", "output": "Welcome back to MSE.\nThis is one of those things that once you're explained it correctly the first time, without any gaps in explanation, that it makes sense. Unfortunately, most answers and even professors don't explain all of the details, in my experience.\nSuppose $X_1, \\dots, X_n$ are independent and distributed $\\text{Uniform}(0, \\theta)$, with $\\theta > 0$.\nLet $\\mathbf{I}$ denote the indicator function, where\n$$\\mathbf{I}(\\cdot) = \\begin{cases}\n1, & \\cdot \\text{ is true} \\\\\n0, & \\cdot \\text{ is false.}\n\\end{cases}$$\nThe probability density function of any of the $X_i$, for $i \\in \\{1, \\dots, n\\}$, can be written like so:\n$$f_{X_i}(x_i \\mid \\theta) = \\dfrac{1}{\\theta}\\cdot\\mathbf{I}(0<x_i<\\theta)\\text{.}$$\nThe likelihood function is thus given by\n$$\\begin{align}\nL(\\theta)&=f_{X_1, \\dots, X_n}(x_1, \\dots, x_n \\mid \\theta)\\\\\n&=\\prod_{i=1}^{n}f_{X_i}(x_i \\mid \\theta) \\\\\n&= \\dfrac{1}{\\theta^n}\\prod_{i=1}^{n}\\mathbf{I}(0 < x_i < \\theta)\\text{.}\n\\end{align}$$\nThe following claim, although used, is often omitted from explanations:\n\nClaim. Let $A$ and $B$ be events. Then $\\mathbf{I}(A)\\cdot \\mathbf{I}(B)=\\mathbf{I}(A \\cap B)$.\n\nI leave the proof of this to you. Note that $ 0 < x_i < \\theta$ is the same as requiring both $x_i > 0$ and $x_i < \\theta$. Hence, we write\n$$\\begin{align}\nL(\\theta)&=\\dfrac{1}{\\theta^n}\\prod_{i=1}^{n}\\mathbf{I}(0 < x_i < \\theta) \\\\\n&= \\dfrac{1}{\\theta^n}\\prod_{i=1}^{n}[\\mathbf{I}(x_i > 0)\\mathbf{I}(x_i < \\theta)] \\\\\n&= \\dfrac{1}{\\theta^n}\\prod_{i=1}^{n}[\\mathbf{I}(x_i > 0)]\\prod_{j=1}^{n}[\\mathbf{I}(x_j < \\theta)]\\text{.}\n\\end{align}$$\nIt will be clear why I split the product as above in a bit.\nThe claim given above is true if we were to extend to an arbitrary number of events as well. Thus,\n$$\\prod_{i=1}^{n}[\\mathbf{I}(x_i > 0)] = \\mathbf{I}(x_1 > 0 \\cap x_2 > 0 \\cap \\cdots \\cap x_n > 0)$$\nand\n$$\\prod_{j=1}^{n}[\\mathbf{I}(x_j < \\theta)] = \\mathbf{I}(x_1 < \\theta \\cap x_2 < \\theta \\cap \\cdots \\cap x_n < \\theta)\\text{.}$$\nThe next claims are often omitted as well from explanations:\n\nClaim 1. Given $x_1, \\dots, x_n \\in \\mathbb{R}$, $x_1, \\dots, x_n < k$ if and only if $$x_{(n)}:=\\max_{1 \\leq i \\leq n}x_i  < k\\text{.}$$\nClaim 2. Given $x_1, \\dots, x_n \\in \\mathbb{R}$, $x_1, \\dots, x_n > k$ if and only if $$x_{(1)}:=\\min_{1 \\leq i \\leq n}x_i > k\\text{.}$$\n\nThus\n$$\\prod_{i=1}^{n}[\\mathbf{I}(x_i > 0)] = \\mathbf{I}(x_1 > 0 \\cap x_2 > 0 \\cap \\cdots \\cap x_n > 0) = \\mathbf{I}(x_{(1)} > 0)$$\nand\n$$\\prod_{j=1}^{n}[\\mathbf{I}(x_j < \\theta)] = \\mathbf{I}(x_1 < \\theta \\cap x_2 < \\theta \\cap \\cdots \\cap x_n < \\theta) = \\mathbf{I}(x_{(n)} < \\theta)\\text{.}$$\nThe likelihood function is thus\n$$L(\\theta) = \\dfrac{1}{\\theta^n}\\mathbf{I}(x_{(1)} > 0)\\mathbf{I}(x_{(n)} < \\theta)\\text{.}\\tag{*}$$\nNow, consider the above as a function of $\\theta$. For all intents and purposes, $\\mathbf{I}(x_{(1)} > 0)$ is irrelevant when it comes to maximization of $L$ with respect to $\\theta$, because it is independent of $\\theta$. So, the part that really matters is\n$$L(\\theta) \\propto \\dfrac{1}{\\theta^n}\\mathbf{I}(x_{(n)} < \\theta) = \\dfrac{1}{\\theta^n}\\mathbf{I}(\\theta > x_{(n)})\\text{.}\\tag{**}$$\nGenerally, when doing maximum-likelihood estimation, we assume that the observed $x_i$ fall within the support of the given distribution, so we'll just assume $x_{(1)} > 0$.\nRemember to view (**) as a function of $\\theta$. If $\\theta \\leq x_{(n)}$, note that $L(\\theta) = 0$ because of the indicator function. This is not the maximized value of $L$; $L$ is, at its crux, a probability density function: $0$ is in fact the smallest value that a probability density function can take.\nSo, in attempting to maximize $L$, suppose that $\\theta > x_{(n)}$. For $n$ fixed, we obtain\n$$L(\\theta) \\propto\\dfrac{1}{\\theta^n}\\text{.}$$\nNow, note that $\\dfrac{1}{\\theta^n}$ is indeed a decreasing function of $\\theta$ with $n$ fixed. Thus, we must make $\\theta$ as small as possible, given our restriction of $\\theta > x_{(n)}$.\n\nNote. Technically, no such $\\theta$ exists (because $\\theta$ is strictly greater than $x_{(n)}$ per our assumptions). This is often ignored in many textbooks.\n\nMost textbooks will then say that the maximum likelihood estimator of $\\theta$ is\n$$\\hat{\\theta}_{\\text{MLE}} = X_{(n)}\\text{.}$$\n\nNote. Technically, the above result is false. The MLE does not exist, because $\\theta$ cannot take on the value $x_{(n)}$ itself. For this answer to be correct, the support of the uniform PDF must include $\\theta$ itself (because the maximum likelihood estimator equals one of the $X_i$). The reason for this is discussed in the Lecture 2: Maximum Likelihood Estimators from MIT OpenCourseWare 18-443 Statistics for Applications, found here. As the question currently stands, $(0, \\theta)$ should be $(0, \\theta]$.", "meta": {"post_id": 2941187, "input_score": 17, "output_score": 45, "post_title": "MLE for Uniform $(0,\\theta)$"}}
{"input": "The aim of this question is to collect interesting examples and counterexamples in martingale theory. There is a huge variety of such (counter)examples available here on StackExchange but I always have a hard time when I try to locate a specific example/question. I believe that it would be a benefit to make this knowledge easier to access. For this reason I would like to create a (big) list with references to related threads.\nMartingale theory is a broad topic, and therefore I suggest to focus on time-discrete martingales $(M_n)_{n \\in \\mathbb{N}}$. I am well aware that this is still a quite broad field. To make this list a helpful tool (e.g. for answering questions) please make sure to give a short but concise description of each (counter)example which you list in your answer.\nRelated literature:\n\nJordan M. Stoyanov: Counterexamples in Probability, Dover.\nJoseph P. Romano, Andrew F. Siegel: Counterexamples in probability and statistics, CRC Press.", "output": "convergence results:\n\npointwise convergence of martingale $M_n$ does not imply $\\sup_n \\mathbb{E}(M_n^+)<\\infty$ (this means that the converse of the martingal convergence theorem does not hold true)\nmartingale which converges almost surely but not in $L^1$ (see also here)\nmartingale $(M_n)_n$ such that $M_n \\to -\\infty$ almost surely (consider $-M_n$ to get a martingale such that $M_n \\to \\infty$ a.s.)\nnon-trivial martingale which converges almost surely to $0$ (see also here)\nmartingale which converges in probability but not almost surely (see also here)\nmartingale which converges in distribution but not almost surely/in probability (see also Section 2.2 here)\n\nuniform integrability:\n\nmartingale $(M_n)_n$ for which $M_{\\infty} = \\lim_n M_n$ exists a.s. but $\\mathbb{E}(M_{\\infty} \\mid \\mathcal{F}_n) \\neq M_n$ (see also here and here; note that such a martingale cannot be uniformly integrable and cannot converge in $L^1$)\nuniformly integrable martingale $(M_n)_n$ such that $\\mathbb{E}\\left( \\sup_{n \\in \\mathbb{N}} |M_n| \\right) = \\infty$.\n\nsample path behaviour:\n\noscillating martingale with bounded sample paths\nnon-trivial martingale which is constant with positive probability\nmartingale which is non-constant and non-negative\n\nStopping times (Optional stopping/sampling theorem):\n\nmartingale $(M_n)_{n \\in \\mathbb{N}}$ and stopping time $\\tau$ such that $\\mathbb{E}(M_{\\tau}) \\neq \\mathbb{E}(M_0)$\nmartingale $(M_n)_{n \\in \\mathbb{N}}$ and stopping time $\\tau$ such that $M_{n \\wedge \\tau} \\to M_{\\tau}$ almost surely but not in $L^1$ (see the very first part of the the linked answer)\nmartingale $(M_n)_{n \\in \\mathbb{N}}$ and stopping time $\\tau$ such that $\\tau<\\infty$ almost surely and $\\mathbb{E}(\\tau)=\\infty$\n\nOther\n\nstochastic process $(M_n)_n$ which satisfies $\\mathbb{E}(M_{n+1} \\mid M_n) = M_n$ for all $n$ but which is not a martingale\nmartingale which is not bounded in $L^1$", "meta": {"post_id": 2974382, "input_score": 36, "output_score": 35, "post_title": "Martingale theory: Collection of examples and counterexamples"}}
{"input": "Hi Im fairly new to SDE theory and am struggling with the difference between a weak ( or martingale ) solution and a strong solution to an SDE : \n$$ d(X_{t})=b(t,X_{t})dt + \\sigma(t,X_{t})dW_{t}  $$\nAre these two differences and what do they really mean in detail?\n\nFor a strong solution we are given an initial value, whereas for weak solutions only a probability law?\nFor strong solutions we know what probability space we are working in and have a Brownian Motion $W$ in that space. For a weak solution we can only say that there exists some probability space where the SDE holds (with a new brownian motion in the space). \n\nAs you can tell I am confused with this topic some clarifications would be amazing.", "output": "The main difference between weak and strong solutions is indeed that for strong solutions we are given a Brownian motion on a given probability space whereas for weak solutions we are free to choose the Brownian motion and the probability space.\n\nDefinition: Let $(B_t)_{t \\geq 0}$ be a Brownian motion with admissible filtration $(\\mathcal{F}_t)_{t \\geq 0}$. A progressively measurable process $(X_t,\\mathcal{F}_t)$ is a strong solution with initial condition $\\xi$ if $$X_t-X_0 = \\int_0^t \\sigma(s,X_s) \\, dB_s + \\int_0^t b(s,X_s) \\, ds, \\qquad X_0 =\\xi \\tag{1}$$ holds almost surely for all $t \\geq 0$.\nDefinition: A stochastic process $(X_t,\\mathcal{F}_t)$ on some probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$ is called a weak solution with initial distribution $\\mu$ if there exists a Brownian motion $(B_t)_{t \\geq 0}$ on $(\\Omega,\\mathcal{F},\\mathbb{P})$ such that $(\\mathcal{F}_t)_{t \\geq 0}$ is an admissible filtration, $\\mathbb{P}(X_0 \\in \\cdot) = \\mu(\\cdot)$ and $$X_t-X_0 = \\int_0^t \\sigma(s,X_s) \\, dB_s + \\int_0^t b(s,X_s) \\, ds$$ holds almost surely for all $t \\geq 0$.\n\nAs a consequence of these definitions, we have to consider different notions of uniqueness. For strong solutions we are typically looking for pathwise unique solutions, i.e. if $(X_t^{(1)})_{t \\geq 0}$ and $(X_t^{(2)})_{t \\geq 0}$ are strong solutions to $(1)$ with the same initial condition, then pathwise uniqueness means $$\\mathbb{P} \\left( \\sup_{t \\geq 0} |X_t^{(1)}-X_t^{(2)}|=0 \\right)=1.$$ As the following simple example shows it doesn't make sense to talk about pathwise uniqueness of weak solutions.\n\nExample 1: Let $(W_t^{(1)})_{t \\geq 0}$ and $(W_t^{(2)})_{t \\geq 0}$ be two Brownian motions (possibly defined on different probability spaces), then both $X_t^{(1)} := W_t^{(1)}$ and $X_t^{(2)} := W_t^{(2)}$ are weak solutions to the SDE $$dX_t = dB_t, \\qquad X_0 = 0$$ Why? According to the definition we are free choose the driving Brownian motion, so we can set $B_t^{(1)} := W_t^{(1)}$ and $B_t^{(2)} := W_t^{(2)}$, respectively, and then $$dX_t^{(i)} = dB_t^{(i)} \\quad \\text{for $i=1,2$}.$$\n\nWhat do we learn from this? Since weak solutions might be defined on different probability spaces, there is no (immediate) way to compute probabilities of the form $\\mathbb{P}(X_t^{(1)}=X_t^{(2)})$ for two weak solutions $(X_t^{(1)})_{t \\geq 0}$ and $(X_t^{(2)})_{t \\geq 0}$, and therefore we cannot even attempt to talk about pathwise uniqueness. For the same reason, it doesn't make sense to talk about pointwise initial conditions $\\xi$ for weak solutions (... for this we would need to fix some probability space on which $\\xi$ lives...); instead we only prescribe the initial distribution of $X_0$.\nThe next example shows that we cannot expect to have pathwise uniqueness even if the weak solutions are defined on the same probability space.\n\nExample 2: Let $(W_t)_{t \\geq 0}$ be a Brownian motion. It follows from Example 1 that $X_t^{(1)} := W_t$ and $X_t^{(2)} := -W_t$ are weak solutions to the SDE $$dX_t = dB_t, \\qquad X_0 =0.$$ Clearly, $\\mathbb{P}(X_t^{(1)} = X_t^{(2)}) = \\mathbb{P}(W_t=0)=0$.\n\nThe \"good\" notion of uniqueness for weak solutions is weak uniqueness, i.e. uniqueness in distribution (= the solutions have the same finite-dimensional distributions).\nTypically it is much easier to prove the existence (and/or uniqueness of) a weak solution the the existence (and/or uniqueness) of a strong solution.\n\nExample 3: The SDE $$dX_t = - \\text{sgn}\\,(X_t) \\, dB_t, \\qquad X_0 = 0 \\tag{2}$$ has a weak solution but no strong solution.\n\nLet's prove that the SDE has a weak solution. Let $(X_t,\\mathcal{F}_t)_{t \\geq 0}$ be some Brownian motion and define $$W_t := -\\int_0^t \\text{sgn} \\, (X_s) \\, dX_s.$$ It follows from L\u00e9vy's characterization that $(W_t,\\mathcal{F}_t)$ is also a Brownian motion. Since $$dW_t = - \\text{sgn} \\, (X_t) \\, dX_t$$ implies $$dX_t = - \\text{sgn} \\, (X_t) \\, dW_t$$ this means that $(X_t)_{t \\geq 0}$ is a weak solution to $(2)$. For a proof that a strong solution does not exist see e.g. Example 19.16 in the book by Schilling & Partzsch on Brownian motion.\nLet me finally mention that weak solutions are closely related to martingale problems; in this answer I tried to give some insights on the connection between the two notions.", "meta": {"post_id": 2987743, "input_score": 43, "output_score": 50, "post_title": "Difference between weak ( or martingale ) and strong solutions to SDEs"}}
{"input": "Prove that $$\\lim\\limits_{n\\to\\infty} e^{-n} \\sum_{k=0}^{n} \\frac{n^k}{k!} = \\frac{1}{2}$$\nThis problem appeared on MSE many times, but each time it was solved using Poisson distribution or lots of integrals. I am wondering, is there any way to prove it using some basic properties of limits (their arithmetics, squeeze theorem etc.), definition of $e^x$ as $\\lim\\limits_{n\\to\\infty}(1+\\frac{x}{n})^n$, basic limits with $e$, binomial expansion and logarithms, but without using integrals, series, Stirling formula, asymptotics, Taylor series?\nThis problem was given to me by my mathematical analysis teacher, but it's not a homework, just additional problem to think on. My teacher claims it can be solved with knowledge introduced on lectures so far, which is not much, mainly things mentioned above. Of course, I can use theorems not mentioned on the lectures, but then I have to prove them, and again, with the baisc knowledge. I've been thinking about it for a few days and couldn't do any major progress in my attempts.", "output": "Table of Content.\n\nHeuristic argument\nElementary proof, version 1.\nElementary proof, version 2. (NEW!)\n\n\n1. Heuristic argument.\nAlthough far from being rigorous, one can provide a heuristic computation which explains why we expect the answer to be $\\frac{1}{2}$. Notice that\n$$ \\frac{n^{n+j}/(n+j)!}{n^n / n!} = \\begin{cases}\n\\prod_{k=1}^{j} \\frac{n}{n+k}, & j \\geq 1 \\\\\n1, & j = 0, -1 \\\\\n\\prod_{k=1}^{-j-1} \\frac{n-k}{n}, & j \\leq -2 \n\\end{cases}\n$$\nIn any of cases, taking logarithm and applying the approximation $\\log(1+x) \\approx x$ shows that the above quantity is approximately $e^{-j^2/2n}$. So\n$$\ne^{-n} \\sum_{k=0}^{n} \\frac{n^k}{k!}\n= \\frac{\\sum_{j=-n}^{0} \\frac{n!}{n^n \\sqrt{n}} \\frac{n^{n+j}}{(n+j)!} }{\\sum_{j=-n}^{\\infty} \\frac{n!}{n^n \\sqrt{n}}\\frac{n^{n+j}}{(n+j)!} }\n\\approx \\frac{\\sum_{j=-n}^{0} e^{-(j/\\sqrt{n})^2/2} \\frac{1}{\\sqrt{n}} }{\\sum_{j=-n}^{\\infty} e^{-(j/\\sqrt{n})^2/2} \\frac{1}{\\sqrt{n}} }\n\\approx \\frac{\\int_{-\\infty}^{0} e^{-x^2/2} \\, dx}{\\int_{-\\infty}^{\\infty} e^{-x^2/2} \\, dx}\n= \\frac{1}{2}.\n$$\nMost of the solutions that I know is more or less a rigorous realization of this kind of observation, and so, it is either involved or requiring extra knowledge.\n\n2. Elementary proof, version 1.\nDefine $A_n$, $B_n$ and $C_n$ by\n$$ A_n := e^{-n} \\sum_{k=0}^{n} \\frac{n^k}{k!}, \\qquad B_n := e^{-n} \\sum_{k=n+1}^{\\infty} \\frac{n^k}{k!}, \\qquad C_n = \\frac{n^{n} e^{-n}}{n!}. $$\nFrom the Taylor expansion of the exponential function, we know that $A_n + B_n = 1$. In order to show the desired convergence, it suffices to show that the following claim holds.\n\nClaim. $A_n/B_n \\to 1$ as $n\\to\\infty$.\n\nIndeed, once this is proved, then both $A_n$ and $B_n$ converge to $\\frac{1}{2}$ as $n\\to\\infty$.\nProof of Claim. Using the substitution $k = n-j$ followed by $p = j-1$, one may write\n\\begin{align*}\n\\frac{A_n}{C_n}\n&= \\sum_{j=0}^{n} \\frac{n!}{(n-j)!n^j}\n = 2 + \\sum_{p=1}^{n-1} \\prod_{l=1}^{p} \\left( 1 - \\frac{l}{n} \\right).\n\\end{align*}\nSimilarly, applying the substitution $k = n+p$, one may write\n\\begin{align*}\n\\frac{B_n}{C_n}\n&= \\sum_{p=1}^{\\infty} \\frac{n!n^p}{(n+p)!}\n = \\sum_{p=1}^{\\infty} \\prod_{l=1}^{p} \\left( \\frac{1}{1 + \\frac{l}{n}} \\right).\n\\end{align*}\n1. Estimation of leading sums. Fix $\\alpha \\in \\left( 0, \\frac{1}{3} \\right)$ and set $N = N(\\alpha) = \\left\\lceil n^{(1+\\alpha)/2} \\right\\rceil$. Then using the asymptotic formula $1+x = e^{x+\\mathcal{O}(x^2)}$ as $x \\to 0$, for $1 \\leq p \\leq N$ we have\n$$ \\prod_{l=1}^{p} \\left( 1 - \\frac{l}{n} \\right)\n= \\exp\\left\\{ -\\frac{p^2}{2n} + \\mathcal{O}\\left( n^{-(1-3\\alpha)/2} \\right) \\right\\}\n= \\prod_{l=1}^{p} \\left( \\frac{1}{1 + \\frac{l}{n}} \\right). $$\nIn particular, there exists a constant $C > 0$, depending only on $\\alpha$, such that\n$$ \\max\\Bigg\\{ \\prod_{l=1}^{N} \\left( 1 - \\frac{l}{n} \\right), \\prod_{l=1}^{N} \\left( \\frac{1}{1 + \\frac{l}{n}} \\right) \\Bigg\\} \\leq C e^{-\\frac{1}{2}n^{\\alpha}}. $$\n2. Estimation of tail sums. In this time, consider $p > N$. Then we have\n$$ \\prod_{l=1}^{p} \\left( 1 - \\frac{l}{n} \\right)\n\\leq C e^{-\\frac{1}{2}n^{\\alpha}} \\left( 1 - \\frac{N}{n} \\right)^{p-N}\n\\quad \\text{and} \\quad\n\\prod_{l=1}^{p} \\left( \\frac{1}{1 + \\frac{l}{n}} \\right)\n\\leq C e^{-\\frac{1}{2}n^{\\alpha}} \\left( \\frac{1}{1 + \\frac{N}{n}} \\right)^{p-N}. $$\nSo the tail sum for $A_n/C_n$ can be bounded from above by\n$$ \\sum_{p=N+1}^{n-1} \\prod_{l=1}^{p} \\left( 1 - \\frac{l}{n} \\right)\n\\leq Ce^{-\\frac{1}{2}n^{\\alpha}} \\sum_{k = 0}^{\\infty} \\left( 1 - \\frac{N}{n} \\right)^k\n\\leq \\frac{Cn}{N} e^{-\\frac{1}{2}n^{\\alpha}}\n\\leq Cn^{(1-\\alpha)/2} e^{-\\frac{1}{2}n^{\\alpha}}, $$\nand likewise,\n$$ \\sum_{p=N+1}^{\\infty} \\prod_{l=1}^{p} \\left( \\frac{1}{1 + \\frac{l}{n}} \\right)\n\\leq Ce^{-\\frac{1}{2}n^{\\alpha}} \\sum_{k = 0}^{\\infty} \\left( 1 - \\frac{N}{N+n} \\right)^k\n\\leq \\frac{2Cn}{N} e^{-\\frac{1}{2}n^{\\alpha}}\n\\leq 2Cn^{(1-\\alpha)/2} e^{-\\frac{1}{2}n^{\\alpha}}. $$\n3. Conclusion. Combining altogether,\n$$ \\frac{A_n}{B_n} = \\frac{\\left( 1 + o(1) \\right) \\sum_{p=1}^{N} e^{-\\frac{p^2}{2n}} + \\mathcal{O}(1)}{\\left( 1 + o(1) \\right) \\sum_{p=1}^{N} e^{-\\frac{p^2}{2n}} + o(1)}, $$\nwhich can be easily shown to converge to $1$ as $n\\to\\infty$, since $\\sum_{p=1}^{N} e^{-\\frac{p^2}{2n}} \\geq \\sqrt{n} \\, e^{-1/2} \\to \\infty$ as $n\\to\\infty$. (In fact, this sum is $(1+o(1))\\sqrt{\\pi n/2}$ as $n\\to\\infty$.)\n\n3. Elementary proof, version 2.\nIn this answer, we do appeal to the concentration behavior of the sum, but rather utilize a mysterious identity from combinatorics.\nWrite $A_n = e^{-n} \\sum_{k=0}^{n} \\frac{n^k}{k!}$ for the sequence of our interest. We also introduce the following auxiliary sequences:\n$$\na_n = \\frac{n^n}{n!e^n}, \\qquad\nb_n = (-1)^n \\binom{-1/2}{n} = \\frac{1}{4^n} \\binom{2n}{n},\n$$\nBefore proceeding, we make some observations. The key ingredients are the following identities\n$$ A_n = \\sum_{k=0}^{n} a_k a_{n-k}, \\qquad 1 = \\sum_{k=0}^{n} b_k b_{n-k}. $$\nThe former one is quite non-trivial, and a proof can be found here. On the other hand, the latter one is easily proved by comparing both sides of $ \n\\sum_{n=0}^{\\infty} x^n = \\frac{1}{1-x} = \\left( \\frac{1}{\\sqrt{1-x}} \\right)^2 = \\left( \\sum_{n=0}^{\\infty} b_n x^n \\right)^2$. Next, we have the following observation.\n\nLemma. $ \\frac{a_n}{b_n} \\to \\frac{1}{\\sqrt{2}} $ as $n\\to\\infty$.\n\nThis lemma tells that, roughly $a_{k}a_{n-k} \\approx \\frac{1}{2} b_k b_{n-k}$ and hence $ A_n \\approx \\frac{1}{2} \\sum_{k=0}^{n} b_k b_{n-k} = \\frac{1}{2}$. Indeed, this is an instance of the philosophy that `limit should be preserved under averaging', and so, it can be proved by a standard machinery. We separate the rigorous claim into a standalone result:\n\nProposition. Let $(a_n), (b_n)$ be sequences in $(0, \\infty)$ such that\n\n$a_n/b_n \\to \\ell \\in (0, \\infty)$;\n$b_n \\to 0$ as $n\\to\\infty$;\n$\\sum_{k=0}^{n} b_k b_{n-k} = 1$ for all $n$.\n\nThen $\\sum_{k=0}^{n} a_k a_{n-k} \\to \\ell^2$ as $n\\to\\infty$.\n\nTherefore, $A_n \\to \\frac{1}{2}$ is a direct consequence of this proposition together with the well-known fact that $b_n \\to 0$. Indeed, this can be proved as follows:\n$$ b_n^2\n= \\left( \\frac{1 \\cdot 3 \\cdots (2n-1)}{2 \\cdot 4 \\cdots (2n)} \\right)^2\n= \\left( \\frac{1 \\cdot 3}{2 \\cdot 2} \\right) \\left( \\frac{3 \\cdot 5}{4 \\cdot 4} \\right) \\cdots \\left( \\frac{(2n-3)(2n-1)}{(2n-2)(2n-2)} \\right) \\frac{2n-1}{4n^2}\n\\leq \\frac{1}{2n}. $$\nFinally, we prove the claims above.\n\nProof of Lemma. Using the identity $-\\int_{0}^{1} \\frac{u}{a+u} \\, du = a \\log (a+1) - a \\log a - 1$ for $a > 0$, we notice that\n\\begin{align*}\n&- \\sum_{k=1}^{n} \\int_{0}^{1} \\frac{u}{n+k-1+u} \\, du \\\\\n&= \\sum_{k=1}^{n} \\left[ (n+k-1)\\log (n+k) - (n+k-1)\\log(n+k-1) - 1 \\right] \\\\\n&= (2n)\\log(2n) - n \\log n - n - \\sum_{k=1}^{n} \\log(n+k) \\\\\n&= \\log \\left[ \\left( \\frac{4n}{e} \\right)^n \\frac{n!}{(2n)!} \\right]\n = \\log \\left( \\frac{a_n}{b_n} \\right).\n\\end{align*}\nThen, using $ \\frac{1}{2(n+k)}\n\\leq  \\int_{0}^{1} \\frac{u}{n+k-1+u} \\, du\n\\leq \\frac{1}{2(n+k-1)} $, we obtain\n$$ -\\frac{1}{2}\\sum_{k=1}^{n} \\frac{1}{n+k-1}\n\\leq \\log \\left( \\frac{a_n}{b_n} \\right)\n\\leq -\\frac{1}{2}\\sum_{k=1}^{n} \\frac{1}{n+k}. $$\nTherefore the conclusion follows from the well-known limit $ \\sum_{k=1}^{n} \\frac{1}{1+\\frac{k}{n}} \\frac{1}{n} \\to \\int_{0}^{1} \\frac{dx}{1+x} = \\log 2$.\nProof of Proposition. Let $\\alpha, \\beta $ satisfy $0 < \\alpha < \\ell < \\beta$. Then there exists $N$ such that $\\alpha \\leq \\frac{a_n}{b_n} \\leq \\beta$ for all $n \\geq N$. So, if $n \\geq 2N$, then\n$$ \\alpha^2 \\sum_{k=N}^{n-N} b_k b_{n-k}\n\\leq \\sum_{k=N}^{n-N} a_k a_{n-k}\n\\leq \\beta^2 \\sum_{k=N}^{n-N} b_k b_{n-k}. $$\nNow let $M > 0$ be a bound of $a_n/b_n$. Since $b_n \\to 0$ as $n\\to\\infty$, we have\n$$ \\sum_{k=0}^{N-1} a_k a_{n-k} + \\sum_{k=n-N+1}^{n} a_k a_{n-k}\n= 2\\sum_{k=0}^{N-1} a_k a_{n-k}\n\\leq 2M^2 \\sum_{k=0}^{N-1} b_k b_{n-k} \n\\xrightarrow[n\\to\\infty]{} 0 $$\nCombining altogether and using $1 = \\sum_{k=0}^{n} b_k b_{n-k}$,\n$$ \\alpha^2 \\leq \\liminf_{n\\to\\infty} \\sum_{k=0}^{n} a_k a_{n-k} \\leq \\limsup_{n\\to\\infty} \\sum_{k=0}^{n} a_k a_{n-k} \\leq \\beta^2. $$\nLetting $\\alpha \\uparrow \\ell$ and $\\beta \\downarrow \\ell$ proves the desired identity.\n\nWe conclude with some remarks.\n\nRemark. If we do not care about elementary solution, this approach can be simplified by showing that\n\n$A_n$ is bounded and decreasing, hence converges.\nBy the identity $A_n = \\sum_{k=0}^{n} a_k a_{n-k}$, we have\n$$ (1-x) \\sum_{n=0}^{\\infty} A_n x^n = \\left( \\frac{\\sum_{n=0}^{\\infty} a_n x^n}{\\sum_{n=0}^{\\infty} b_n x^n} \\right)^2, $$\nhence by a version of abelian theorem, we obtain\n$$ \\lim_{n\\to\\infty} A_n = \\lim_{n\\to\\infty} \\left( \\frac{a_n}{b_n} \\right)^2 = \\frac{1}{2}. $$", "meta": {"post_id": 3000437, "input_score": 25, "output_score": 35, "post_title": "$\\lim_{n\\to\\infty} e^{-n} \\sum_{k=0}^{n} \\frac{n^k}{k!} = \\frac{1}{2}$ - basic methods"}}
{"input": "Cauchy-Schwarz inequality applied to Trace of two products $\\mathbf{Tr}(A'B)$ has the form\n$$\n\\mathbf{Tr}(A'B) \\leq \\sqrt{\\mathbf{Tr}(A'A)} \\sqrt{\\mathbf{Tr}(B'B)}\n$$\nI saw many places where people use this inequality.  But did not see a formal proof.  Is it difficult to prove ?  Anyone can give a simple proof ?", "output": "The Cauchy-Schwarz inequality is valid for any inner product, so you just need to show $\\operatorname{\\textbf{Tr}}A'B$ is an inner product. It's clearly bilinear (or sesquilinear if by $'$ you meant a complex adjoint), with $$\\operatorname{\\textbf{Tr}}A'A=\\sum_i (A'A)_{i}=\\sum_{ij}A'_{ij}A_{ji}.$$Depending on whether you're working with the real or complex case, this quantity is either $\\sum_{ij}A_{ji}^2$ or $\\sum_{ij}|A_{ji}|^2$. Either way it's non-negative, completing the proof.", "meta": {"post_id": 3006109, "input_score": 11, "output_score": 36, "post_title": "Proof for Cauchy-Schwarz inequality for Trace"}}
{"input": "I thought maybe we can start with congruent triangle and try to cut it similar to how we create a Sierpinski's Triangle?  However, the number of smaller triangles we get is a power of $4$ so it does not work. Any ideas?", "output": "The decomposition is possible because $2005 = 5\\cdot 401$ and both $5$ and $401$ are primes of the form $4k+1$. This allow $2005$ to be written as a sum of squares.\n$$2005 = 22^2 + 39^2 = 18^2+41^2$$\nFor any integer $n = p^2 + q^2$ that can be written as a sum of squares. \nConsider a right-angled triangle $ABC$ with \n$$AB = p\\sqrt{n}, AC = q\\sqrt{n}\\quad\\text{ and }\\quad BC = n$$\nLet $D$ on $BC$ be the foot of attitude at $A$. It is easy to\nsee $\\triangle DBA$ and $\\triangle DAC$ are similar to $\\triangle ABC$ with\n$$BD = p^2, AD = pq\\quad\\text{ and }\\quad CD = q^2$$\nOne can split $\\triangle DBA$ into $p^2$ and $\\triangle DAC$ into $q^2$\ntriangles with sides $p, q$ and $\\sqrt{n}$.\nAs an example, following is a subdivision of a triangle into $13 = 2^2 + 3^2$ congruent triangles.  \n\nIn the literature, this is known as a biquadratic tiling of a triangle. For more information about subdividing triangles into congruent triangles, look at answers in this MO post. In particular, the list of papers by Michael Beeson there. The construction described here is based on what I have learned from one of Michael's papers.", "meta": {"post_id": 3013709, "input_score": 26, "output_score": 48, "post_title": "Prove that there exists a triangle which can be cut into 2005 congruent triangles."}}
{"input": "Years ago, before everyone (or anyone) had electronic calculators, I had a pocket slide rule which I used in secondary school until the first TI-30 cane out.\nRecently I dug it out. Here's a photo of one end of it.\n\nAs you can see, there's a number $C$ marked at about $1.128$ (times some power of $10$; with a slide rule you supply that yourself) on the C and D scales. Reading across to the A scale, its square is about $1.27$. By the C1 scale (which reads reciprocals of the C scale) its reciprocal is about $0.886$ (times some power of $10$).\nThe only two special numbers marked are $C$ and $\\pi$.\nI'm not sure whether it's some frequently used constant that's used (eg) in some branch of engineering, or a number which is useful for some trick for using the slide rule.\nUnlike $\\pi$, which is marked on most of the scales, this mysterious $C$ only appears on the C and D scales, which are the main ones used for multiplication and division.\nIf you need me to, I can give more explanation of the various scales on the rule and how calculations are done. That might give some clues as to what $C$ is for.\nI'm sure the instructions explained what $C$ was, but I last saw those in the 1970s.\nHas anyone any idea what $C$ is and why it would be useful on a slide rule?", "output": "I found the answer by googling \"slide rule markings\"! It took me straight to the Glossary of the International Slide Rule Museum, which gives C its own entry:\n\nC - Gauge mark found on the C and D scales denoting $\\sqrt{4/\\pi} = 1.128$ for calculating the area of a circle and the volume of a cylinder. Place the C mark on the C scale over the diameter of a circle on the D scale. The area of the circle is found above the index on the A scale. If this is the base of a cylinder, without moving the slide, move the cursor to the height of the cylinder on the B scale. The volume is read on the A scale. This gauge mark was rendered obsolete with the advent of multi-lined cursors.\n\nAnd there is, of course, so much more at that site.", "meta": {"post_id": 3019564, "input_score": 77, "output_score": 82, "post_title": "What does the mysterious constant marked by C on a slide rule indicate?"}}
{"input": "(Don't be alarmed by the title; this is a question about mathematics, not programming.)\nIn the documentation for the decimal module in the Python Standard Library, an example is given for computing the digits of $\\pi$ to a given precision:\ndef pi():\n    \"\"\"Compute Pi to the current precision.\n\n    >>> print(pi())\n    3.141592653589793238462643383\n\n    \"\"\"\n    getcontext().prec += 2  # extra digits for intermediate steps\n    three = Decimal(3)      # substitute \"three=3.0\" for regular floats\n    lasts, t, s, n, na, d, da = 0, three, 3, 1, 0, 0, 24\n    while s != lasts:\n        lasts = s\n        n, na = n+na, na+8\n        d, da = d+da, da+32\n        t = (t * n) / d\n        s += t\n    getcontext().prec -= 2\n    return +s               # unary plus applies the new precision\n\nI was not able to find any reference for what formula or fact about $\\pi$ this computation uses, hence this question. \nTranslating from code into more typical mathematical notation, and using some calculation and observation, this amounts to a formula for $\\pi$ that begins like:\n$$\\begin{align}\\pi \n&= 3+\\frac{1}{8}+\\frac{9}{640}+\\frac{15}{7168}+\\frac{35}{98304}+\\frac{189}{2883584}+\\frac{693}{54525952}+\\frac{429}{167772160} + \\dots\\\\\n&= 3\\left(1+\\frac{1}{24}+\\frac{1}{24}\\frac{9}{80}+\\frac{1}{24}\\frac{9}{80}\\frac{25}{168}+\\frac{1}{24}\\frac{9}{80}\\frac{25}{168}\\frac{49}{288}+\\frac{1}{24}\\frac{9}{80}\\frac{25}{168}\\frac{49}{288}\\frac{81}{440}+\\frac{1}{24}\\frac{9}{80}\\frac{25}{168}\\frac{49}{288}\\frac{81}{440}\\frac{121}{624}+\\frac{1}{24}\\frac{9}{80}\\frac{25}{168}\\frac{49}{288}\\frac{81}{440}\\frac{121}{624}\\frac{169}{840}+\\dots\\right)\n\\end{align}$$\nor, more compactly,\n$$\\pi = 3\\left(1 + \\sum_{n=1}^{\\infty}\\prod_{k=1}^{n}\\frac{(2k-1)^2}{8k(2k+1)}\\right)$$\nIs this a well-known formula for $\\pi$? How is it proved? How does it compare to other methods, in terms of how how quickly it converges, numerical stability issues, etc? At a glance I didn't see it on the Wikipedia page for List of formulae involving \u03c0 or on the MathWorld page for Pi Formulas.", "output": "This approximation for $\\pi$ is attributed to Issac Newton:\n\nhttps://loresayer.com/2016/03/14/pi-infinite-sum-approximation/\nhttp://www.geom.uiuc.edu/~huberty/math5337/groupe/expresspi.html\nhttp://www.pi314.net/eng/newton.php\n\nWhen I wrote that code shown in the Python docs, I got the formula came from p.53 in \"The Joy of \u03c0\".  Of the many formulas listed, it was the first that:\n\nconverged quickly,\nwas short,\nwas something I understood well-enough to derive by hand, and\ncould be implemented using cheap operations: several additions with only a single multiply and single divide for each term.  This allowed the estimate of $\\pi$ to be easily be written as an efficient function using Python's floats, or with the decimal module, or with Python's multi-precision integers.\n\nThe formula solves for \u03c0 in the equation $sin(\\pi/6)=\\frac{1}{2}$.\nWolframAlpha gives the Maclaurin series for $6 \\arcsin{(x)}$ as:\n$$6 \\arcsin{(x)} = 6 x + x^{3} + \\frac{9 x^{5}}{20} + \\frac{15 x^{7}}{56} + \\frac{35 x^{9}}{192} + \\dots\n$$\nEvaluating the series at $x = \\frac{1}{2}$ gives:\n$$\n\\pi \\approx 3+3 \\frac{1}{24}+3 \\frac{1}{24}\\frac{9}{80}+3 \\frac{1}{24}\\frac{9}{80}\\frac{25}{168}+\\dots + \\frac{(2k+1)^2}{16k^2+40k+24} + \\dots\\\\\n$$\nFrom there, I used finite differences, to incrementally compute the numerators and denominators.  The numerator differences were 8, 16, 24, ..., hence the numerator adjustment na+8 in the code.  The denominator differences were 56, 88, 120, ..., hence the denominator adjustment da+32 in the code:\n 1     9    25    49    numerators\n    8    16    24       1st differences\n       8     8          2nd differences\n\n\n24    80   168   288    denominator \n   56    88   120       1st differences\n      32    32          2nd differences\n\nHere is the original code I wrote back in 1999 using Python's multi-precision integers (this predates the decimal module):\ndef pi(places=10):\n    \"Computes pi to given number of decimal places\"\n    # From p.53 in \"The Joy of Pi\".  sin(pi/6) = 1/2\n    # 3 + 3*(1/24) + 3*(1/24)*(9/80) + 3*(1/24)*(9/80)*(25/168)\n    # The numerators 1, 9, 25, ... are given by  (2x + 1) ^ 2\n    # The denominators 24, 80, 168 are given by 16x^2 +40x + 24\n    extra = 8\n    one = 10 ** (places+extra)\n    t, c, n, na, d, da = 3*one, 3*one, 1, 0, 0, 24\n    while t > 1:\n        n, na, d, da  = n+na, na+8, d+da, da+32\n        t = t * n // d\n        c += t\n    return c // (10 ** extra)", "meta": {"post_id": 3028868, "input_score": 58, "output_score": 45, "post_title": "What is the formula for pi used in the Python decimal library?"}}
{"input": "What proportion of positive integers have two factors that differ by 1?\nThis question occurred to me\nwhile trying to figure out\nwhy there are 7 days in a week.\nI looked at 364,\nthe number of days closest to a year\n(there are about 364.2422\ndays in a year, iirc).\nSince\n$364 = 2\\cdot 2 \\cdot 7 \\cdot 13$,\nthe number of possible\nnumber that evenly divide a year\nare\n2, 4, 7, 13, 14, 26, 28,\nand larger.\nGiven this,\n7 looks reasonable -\n2 and 4 are too short\nand 13 is too long.\nAnyway,\nI noticed that\n13 and 14 are there,\nand wondered how often\nthis happens.\nI wasn't able to figure out\na nice way to specify the\nprobability\n(as in a Hardy-Littlewood\nproduct),\nand wasn't able to \ndo it from the inverse direction\n(i.e., sort of a sieve\nwith n(n+1) going into\nthe array of integers).\nIdeally, I would like\nan asymptotic function\nf(x) such that\n$\\lim_{n \\to \\infty} \\dfrac{\\text{number of such integers } \\ge 2 \\le nx}{n}\n=f(x)\n$\nor find $c$ such that\n$\\lim_{n \\to \\infty} \\dfrac{\\text{number of such integers } \\ge 2 \\le n}{n}\n=c\n$.\nMy guess is that,\nin the latter case,\n$c = 0$ or 1,\nbut I have no idea which is true.\nMaybe its \n$1-\\frac1{e}$.\nNote: I have modified this\nto not allow 1 as a divisor.", "output": "What kind of numbers have this property?\n\nAll multiples of 6 (because 6 = 2 \u00d7 3).  So that's 1/6 of the integers.\nAll multiples of 12 (12 = 3 \u00d7 4), but these have already been counted as multiples of 6.\nAll multiples of 20 (20 = 4 \u00d7 5), so add 1/20 of the integers.  But we've double-counted multiples of 60 (LCD of 6 and 20), so subtract 1/60.  This gives us 1/6 + 1/20 - 1/60 = 1/5.\nAll multiples of 30 (5 \u00d7 6) or 42 (6 \u00d7 7), but again, these have already been counted as multiples of 6.\nAll multiples of 56 (7 \u00d7 8), but don't double-count the ones that are also multiples of 6 or 20.  If I did the arithmetic correctly, this brings us up to 22/105.\nAll multiples of 72 (8 \u00d7 9) or 90 (9 \u00d7 10), but these are already multiples of 6.\nAll multiples of 110 (10 \u00d7 11), being careful not to double-count multiples of 6, 20, or 56.  We're now at 491/2310.\n\nContinue the pattern to get a lower bound on the probability.  I bet it converges to something, but I haven't bothered to compute what.", "meta": {"post_id": 3038712, "input_score": 59, "output_score": 35, "post_title": "What proportion of positive integers have two factors that differ by 1?"}}
{"input": "We are all familiar with the standard nomenclature for the smallish\nnatural numbers, such as\n\none, two, three, ..., one hundred, one hundred one, ..., fifteen\n   thousand two hundred forty-nine.\n\nI have in mind the simple American number naming\nconventions,\ntogether with the names for large\nnumbers. (Update Names of large numbers seems to be more thorough. Note to Wikipedians: should probably merge those two pages somehow.)\nPreliminary question. Is there a sensible naming system that\nprovides a canonical name for every natural number?\nThat is, I want a naming system that extends the current naming\nsystem sensibly in such a way that every number gets a unique name. Please provide a system and explain why it is sensible.\nFor example, if there were some natural way to extend the Latin naming convention indefinitely, that would be great.\nLet me assume that some of you will be able to provide such a\nnaming system.\nMain Question. What is the order-type of the set of natural\nnumbers, when written in alphabetical order?\nFor example, the order will not be the same as the order $\\omega$\nof the natural number themselves, since presumably there will be\ninfinitely many numbers starting with \"o\", as in one hundred, one\nmillion, one thousand, and so on, and these will all be\nalphabetically preceding two hundred, two million, two thousand and\nso on.\nSo the order type will probably be related naturally $L\\times 26$\nfor some order $L$, or actually, less than $26$, since probably not\nevery letter will be a legitimate first letter of a number name.\nIt is conceivable that the order type will depend on syntactic features of the naming convention.\nHere is a part of the order, for numbers up to 100: (from herv\u00e9\ngraumann\n1988)\n1) eight\n\n2) eighteen\n\n3) eighty\n\n4) eighty-eight\n\n5) eighty-five\n\n6) eighty-four\n\n7) eighty-nine\n\n8) eighty-one\n\n9) eighty-seven\n\n10) eighty-six\n\n11) eighty-three\n\n12) eighty-two\n\n13) eleven\n\n14) fifteen\n\n15) fifty\n\n16) fifty-eight\n\n17) fifty-five\n\n18) fifty-four\n\n19) fifty-nine\n\n20) fifty-one\n\n21) fifty-seven\n\n22) fifty-six\n\n23) fifty-three\n\n24) fifty-two\n\n25) five\n\n26) forty\n\n27) forty-eight\n\n28) forty-five\n\n29) forty-four\n\n30) forty-nine\n\n31) forty-one\n\n32) forty-seven\n\n33) forty-six\n\n34) forty-three\n\n35) forty-two\n\n36) four\n\n37) fourteen\n\n38) hundred\n\n39) nine\n\n40) nineteen\n\n41) ninety\n\n42) ninety-eight\n\n43) ninety-five\n\n44) ninety-four\n\n45) ninety-nine\n\n46) ninety-one\n\n47) ninety-seven\n\n48) ninety-six\n\n49) ninety-three\n\n50) ninety-two\n\n51) one\n\n52) seven\n\n53) seventeen\n\n54) seventy\n\n55) seventy-eight\n\n56) seventy-five\n\n57) seventy-four\n\n58) seventy-nine\n\n59) seventy-one\n\n60) seventy-seven\n\n61) seventy-six\n\n62) seventy-three\n\n63) seventy-two\n\n64) six\n\n65) sixteen\n\n66) sixty\n\n67) sixty-eight\n\n68) sixty-five\n\n69) sixty-four\n\n70) sixty-nine\n\n71) sixty-one\n\n72) sixty-seven\n\n73) sixty-six\n\n74) sixty-three\n\n75) sixty-two\n\n76) ten\n\n77) thirteen\n\n78) thirty\n\n79) thirty-eight\n\n80) thirty-five\n\n81) thirty-four\n\n82) thirty-nine\n\n83) thirty-one\n\n84) thirty-seven\n\n85) thirty-six\n\n86) thirty-three\n\n87) thirty-two\n\n88) three\n\n89) twelve\n\n90) twenty\n\n91) twenty-eight\n\n92) twenty-five\n\n93) twenty-four\n\n94) twenty-nine\n\n95) twenty-one\n\n96) twenty-seven\n\n97) twenty-six\n\n98) twenty-three\n\n99) twenty-two\n\n100) two\n\n101) zero\n\nLet me add that I don't necessarily expect that the order is a well-order. For example, if we have a naming convention whereby $10^k$ is represented for large $k$ simply by repeating \"penpenpenpen$\\cdots$pen\", then we could make a descending sequence via penpenpenpen$\\cdots$pen twelve, which would descend as the number of pen's increased, since we would be replacing t with p.", "output": "Let us consider the digit-pronunciation naming system, by which\none simply pronounces the digits of a number in order, so that\n$7216$ is pronounced \"seven two one six\" and so on for any number.\nThus, we obtain a naming system of the numbers, and while it does\nnot extend the standard nomenclature, nevertheless I find it to be\nperfectly sensible, providing a definite unique name for every\nnatural number. This naming system is sometimes actually used for\nvery large numbers, such as reading off the number on a credit\ncard, and it is also commonly used to help disambiguate small\nnumbers, such as $50$ and $15$. So I find it to be a reasonable\nnaming system.\nLet us place the natural numbers in alphabetical order with respect\nto this naming system. Thus, $882746$ appears alphabetically before\n$87$, which appears before $8734$. Note that any prefix of a word\nappears earlier in the alphabetical order.\nTheorem. The order type of the natural numbers, in alphabetical\norder with respect to the digit-pronunciation naming system, is\nexactly $$\\omega\\cdot(1+\\mathbb{Q})+1.$$\nProof. That is, we have $1+\\mathbb{Q}$ many copies of $\\omega$, with a\nfinal point on top.\nI will analyze the naming system with respect to base ten, but a\nsimilar analysis works regardless of the base.\nConsider first the alphabetical order of the ten digits themselves:\n\neight, five, four, nine, one, seven, six, three, two, zero\n\nNotice that these digit names are prefix-free \u2014 none of them\nis an initial segment of another. Thus, when comparing the names of\ntwo numbers, we will never be in a situation where part of one\ndigit is combined with part of another in order to make the\nalphabetical comparison. Rather, the alphabetical order is the same\nas the lexical order on the strings of digits themselves,\nconsidered in the alphabetical digit order above.\nThe largest number of all, in the alphabetical order, is zero,\nsince no other number starts with the letter \"z\", and so this\nnumber will appear as the very last entry alphabetically. This\nexplains the final $+1$ in the theorem claim.\nThe smallest number in alphabetical order, in contrast, is $8$,\nsince it begins with \"e\", and the only other numbers beginning with\n\"e\" also begin with $8$, followed possibly by additional digits,\nand thus will appear after the single-digit $8$.\nThe next number after $8$, alphabetically, is $88$ and then $888$\nand $8888$ and so on. I claim that every number (except $0$) has an\nalphabetical successor, which is simply to add a digit $8$ at the\nend of the decimal representation of the number. For example, the\nnext number after $532876$ is $5328768$, because any other digit\nsequence above the first number must either extend it or deviate\nfrom one of those digits. But $5328768$ will be below any other\nhigher deviation or extension, and so it is a successor. Similarly,\n$53287688$ and $532876888$ are the next few numbers, simply adding\nmore $8$'s at the end.\nThus, every number except $0$ in the alphabetical order is followed\nby a sequence of order type $\\omega$, which is obtained by simply\ntacking on additional $8$s. And so the order will be a number of\ncopies of $\\omega$, plus one more point $0$ at the top.\nLet me argue that those copies of $\\omega$ are themselves densely\nordered. If one number $m$ precedes another $n$ alphabetically, but\n$n$ is not just adding $8$'s to the end of the decimal\nrepresentation of $m$, then either there is some alphabetically\nupward deviation in the digits of $m$ to form $n$, or else $n$\nextends the digits of $m$, but eventually using some digits other\nthan $8$. It is easy to see that we can find another number in\nbetween, which also won't be just adding $8$s.\nPerhaps it is easiest to see this by example. The number $7536$ is\nalphabetically prior to $752$, since \"three\" is alphabetically\nearlier than \"two\". In between these numbers, we can find $75366$,\nwhich has it own copy of $\\omega$ arising from $753668$, $7536688$,\n$75366888$ and so on.\nThus, the blocks of $\\omega$ obtained by appending $8$'s are\nthemselves densely ordered: between any two of them we can find\nanother.\nNotice that there is a very first such block of $\\omega$ in the\nalphabetical order the numbers, namely, the block consisting of\n$8$, $88$, $888$ and so on, which appears at the very beginning of\nthe numbers in alphabetical order.\nThere is in contrast no largest block, before the final $0$,\nbecause if we are given any number $n$, we can append some other\ndigits other than $8$ to the end of the decimal representation, and\nthereby find another copy of $\\omega$ above $n$ in the alphabetical\norder.\nThus, the $\\omega$ blocks arising from appending $8$'s are\nthemselves densely ordered, with a first such block and no last\nsuch block. Since there are only countably many numbers, we must\nhave exactly $1+\\mathbb{Q}$ many such blocks of size $\\omega$. And\nwith the final point $0$ at the very top, it follows that the order\ntype of the natural numbers in the digit-pronunciation naming\nsystem is precisely $$\\omega\\cdot(1+\\mathbb{Q})+1,$$ as claimed.\n$\\Box$.\nSeveral of us had discussed this problem over beers last night in\nM\u00fcnster, including Stefan Hoffelner and Stefan Mesken, following my talk at the M\u00fcnster Logic\nOberseminar. Stefan Hoffelner had suggested that we consider the digit-pronunciation naming system.\nLet me say finally that it seems to me that the features of the\ndigit-pronunciation naming system will appear essentially in all\nthe naming systems, and so I expect this kind of analysis to be\nable to extend to the other nomenclatures, with perhaps slightly\ndifferent endpoint effects.\n\nUpdate (January 2023). I wrote an essay providing an elementary account of this question and its answer and related matters on my substack blog, The Book of Numbers.", "meta": {"post_id": 3047540, "input_score": 28, "output_score": 34, "post_title": "What is the order-type of the set of natural numbers, when written in alphabetical order?"}}
{"input": "This is a variant of Prime number building game.\nPlayer $A$ begins by choosing a single-digit prime number. Player $B$ then appends any digit to that number such that the result is still prime, and players alternate in this fashion until one player loses by being unable to form a prime.\nFor instance, play might proceed as follows:\n\n$A$ chooses 5\n$B$ chooses 3, forming 53\n$A$ loses since there are no primes of the form 53x\n\nIs there a known solution to this game? It seems like I might be able to try a programmatic search...or might some math knowledge help here?", "output": "As mentioned by others, it isn't too hard to create the whole trie.\nPlayer $A$ is green and Player $B$ is orange:\n\nFor reference purposes, here's the corresponding Python code. It uses networkx and graphviz:\nimport networkx as nx\nfrom networkx.drawing.nx_agraph import to_agraph\n\ndef is_prime(n):\n    if n == 2:\n        return True\n    if n < 2 or n % 2 == 0:\n        return False\n    for d in range(3, int(n**0.5) + 1, 2):\n        if n % d == 0:\n            return False\n    return True\n\n\ndef add_prime_leaves_recursively(current_number=0, current_representation='',\n                                 base=10, graph=nx.DiGraph(), level=0,\n                                 colors=['#FF851B', '#2E8B57']):\n    graph.add_node(current_number,\n                   label=current_representation,\n                   color=colors[level % 2])\n    for next_digit in range(base):\n        next_number = current_number * base + next_digit\n        if is_prime(next_number):\n            graph.add_edge(current_number, next_number)\n            add_prime_leaves_recursively(\n                next_number,\n                current_representation + '0123456789ABCDEFGHIJ'[next_digit],\n                base, graph, level + 1)\n    return graph\n\n\nG = add_prime_leaves_recursively(base=10)\nG.nodes[0]['color'] = 'black'\n\nA = to_agraph(G)\nA.draw('prime_number_construction_game.png', prog='dot')\n\nThis code can generate the diagram for any base below 20. The game is boring in base 3:", "meta": {"post_id": 3048245, "input_score": 50, "output_score": 43, "post_title": "Prime number construction game"}}
{"input": "I have a question about a mathematical riddle which I already solved but still looking for a shorter/simpler solution:\nFollowing problem: We consider a standard $8 \\times 8$ chessboard and we cover it (completely!) with dominos of size $2 \\times 1$ (therefore every domino tile cover exactly $2$ fields).\nThe question is if we can find a cover such that there doesn't exist a $2 \\times 2$ subsquare which is covered exactly by two domino tiles or in other words in the cover there don't occure two \"direct\" neighbour domino tiles from following shape:\n\nI have it already solved in following way: I claim that such covering isn't possible.\nArgue via contradiction: Assume that it's possible. Consider the $2 \\times 2$ squares of the chess board and consider the partial cover of directly neighboured $2 \\times 2$ squares. If a cover as above really exist then up to symmetry on the common edge of the two neighboured squares there could only occure two following cases (here only the vertical pairs; horizontally: analogous):\n\n\nThe two neighboured squares share a common domino tile (the orange one)\nthey don't share any domino tile on the common edge\n\nNow there are exactly $24$ such pairings between neighboured $2 \\times 2$ squares (note we don't consider the diagonal neighbour pairs).\nNow we count all domino tiles in following way:\n-each pair of neighbour squares which share a unique common domino tile contribute a $1$ (the orange one)\n-each pair of neighbour squares don't share a  common domino tile contribute a $1$ with the unique tile beeing fully contained in only one square and intersecting the common edge (the grey one).\nThat's all. But then we get only $24$ tiles althought there are $32$. Contradiction. \nI guess this argument works but I think that it's too cumbersome. Does anybody have an easier / not too circumstaneous way to show it?", "output": "How about this: \nLet's number the rows $1$ through $8$ from top to bottom, and the columns $1$ through $8$ from left to right. Cell $(x,y)$ means the cell in row $x$ and column $y$\nYou need a tile to cover cell $(1,1)$.This can be done in two ways, but by symmetry, we only have to consider one of these, so let's consider placing it horizontally, i.e. cover cells $(1,1)$ and $(1,2)$:\n\nNow we need to cover cell $(2,1)$. In order to avoid making a $2\\times 2$ subsquare made up of two tiles, there is only way way to place a tile under it, so that will cover cells $(2,1)$ and $(3,1)$:\n\nNow we need to cover $(2,2)$. Again, there is only way way to do this in order to avoid making a $2\\times 2$ subsquare made up of two tiles: cover cells $(2,2)$ and $(2,3)$.\n\nOK, and now keep placing tiles along this basic 'diagonal' of the chess-board: you'll find all the placements are forced if you want to avoid a $2\\times 2$ subsquare made up of two tiles. But, at the end of the diagonal, you end up having to use one tile to cover $(7,6)$ and $(8,6)$, and another one to cover $(7,7)$ and $(7,8)$:\n\n... and now you are forced to also place one on $(8,7)$ and $(8,8)$, and get a $2\\times 2$ subsquare made up of two tiles. (also note that the two parts of the board as of yet uncovered each have an odd number of squares left, so they can no longer be completely covered)\nSo, it is indeed impossible to do a complete tiling without getting a $2\\times 2$ subsquare made up of two tiles.", "meta": {"post_id": 3052354, "input_score": 14, "output_score": 40, "post_title": "Cover Chessboard with Domino Tiles"}}
{"input": "All numbers $1$ to $155$ are written on a blackboard, one time each. We randomly choose two numbers and delete them, by replacing one of them with their product plus their sum. We repeat the process until there is only one number left. What is the average value of this number?\n\nI don't know how to approach it: For two numbers, $1$ and $2$, the only number is $1\\cdot 2+1+2=5$\nFor three numbers, $1, 2$ and $3$, we can opt to replace $1$ and $2$ with $5$ and then $3$ and $5$ with $23$, or\n$1$ and $3$ with $7$ and then $2$, $7$ with $23$ or\n$2$, $3$ with $11$ and $1$, $11$ with $23$ \nso we see that no matter which two numbers we choose, the average number remains the same. Does this lead us anywhere?", "output": "Another way to think of Sorin's observation, without appealing to induction explicitly:\nSuppose your original numbers (both the original 155 numbers and later results) are written in white chalk. Now above each white number write that number plus one, in red chalk. Write new red companions to each new white number, and erase the red numbers when their white partners go away.\nWhen we erase $x$ and $y$ and write $x+y+xy$, the new red number is $x+y+xy+1=(x+1)(y+1)$, exactly the product of the two red companions we're erasing.\nSo we can reformulate the entire game as:\n\nWrite in red the numbers from $2$ to $156$. Keep erasing two numbers and writing their product instead. At the end when you have one red number left, subtract one and write the result in white.\n\nSince the order of factors is immaterial, the result must be $2\\cdot 3\\cdots 156-1$.", "meta": {"post_id": 3054103, "input_score": 42, "output_score": 46, "post_title": "Numbers on blackboard"}}
{"input": "I'm a beginner in metric space. So many books I've read, there is only the notion of open covers. I want to know why do we worry about open covers to define the compactness of metric spaces and why don't we use closed covers? What is the problem in defining closed cover of a set? Can we use the alternative definition of compactness: \"Every closed cover has a finite subcover\"?", "output": "It is important to understand that, although definitions often look arbitrary, they never are. Mathematical objects are intended to model something, and you can't understand why the definition is the way it is until you understand what it is trying to model.  The question you asked is exactly the right one: why is it defined this way and not some other way?  What is it trying to model?\n(For example, why does a topology say that arbitrary unions of open sets are open, but infinite intersections of open sets might not be?  It's because topology is intended to be an abstraction of certain properties of the line and the plane, and open sets are intended to be a more general version of open intervals of the line and open discs in plane, and that is how the intervals and discs behave.)\nThis case is similar.  Mathematicians noticed that there are certain sorts of \u201cwell-behaved\u201d subsets of the line and of metric spaces in general.  For example:\n\nA continuous function is always uniformly continuous \u2014 if and only if its domain is well-behaved in this way\nA continuous real-valued function is always bounded \u2014 if and only if its domain is well-behaved in this way\nIf $f$ is a continuous real-valued function on some domain, there may be some $m$ at which $f$ is maximized: $f(x) \u2264 f(m)$ for all $x$. This is true of all such $f$ if and only if the domain is well-behaved in this way\nEvery sequence of points from a subset of $\\Bbb R^n$ contains a convergent subsequence \u2014 if and only if the subset is well-behaved in this way\n\nand so on.  It took mathematicians quite a long time to understand this properly, but the answer turned out to be that the \"well-behaved\" property is compactness.  There are several equivalent formulations of it, including the open cover formulation you mentioned.\nIn contrast, the alternative property you propose, with closed covers, turns out not to model anything interesting, and actually to be trivial, as the comments point out.  It ends nowhere.  But even if it ended somewhere nontrivial, it would be a curiosity, of not much interest, unless it had started from a desire to better understand of something we already wanted to understand.  It's quite easy to make up new mathematical properties at random, and to prove theorems about those properties, and sometimes it might seem like that is what we are doing.  But we never are.\nProperly formulated, compactness turns out to be surprisingly deep.  Before compactness, mathematics already had an idea of what a finite set was.  Finite sets are always discrete, but not all discrete sets are finite.\nCompactness is the missing ingredient: a finite set is one that is both discrete and compact.  With the discovery of compactness, we were able to understand finiteness as a conjunction of two properties that are more fundamental!  Some of the properties we associate with finiteness actually come from discreteness; others come from compactness.  (Some come from both.)  Isn't that interesting?\nAnd formulating compactness correctly helps us better understand the original space, $\\Bbb R^n$ and metric spaces in general.  Once we get compactness right, we see that the properties of \"well-behaved\" sets I mentioned above are not true of all compact spaces; metric spaces are special in several ways, which we didn't formerly appreciate.\nKeep asking these questions.  Every definition is made for a reason.", "meta": {"post_id": 3097385, "input_score": 18, "output_score": 41, "post_title": "Why don't we use closed covers to define compactness of metric space?"}}
{"input": "Zorn's Lemma states that if every chain $\\mathcal{C}$ in a partially ordered set $\\mathcal{S}$ has an upper bound in $\\mathcal{S}$, then there is at least one maximal element in $\\mathcal{S}$.\nWhy can't we apply Zorn's Lemma in the following case? \nLet $\\mathcal{S}$ be the set of all groups. Define a partial order $\\preceq$ as follows: for $H, G \\in \\mathcal{S}$, define $H \\prec G$ if and only if $H$ is a subgroup of $G$. Then every chain $\\mathcal{C}=(G_{\\alpha})_{\\alpha \\in A}$ in $\\mathcal{S}$ has an upper bound $\\cup_{\\alpha \\in A} G_{\\alpha}$ in $\\mathcal{S}$. But certainly there is no maximal element in $\\mathcal{S}$. \nCould anyone tell me what is wrong with this argument?", "output": "There is no set of all groups, but Zorn's Lemma can be phrased for partially ordered classes as well, but we need to require that any chain has an upper bound, also proper class chains. However in that case it is easy to define a proper class chain which has no upper bound.\nSimply take for each ordinal $\\alpha$ the free group generated by $\\alpha$. There are natural injections given by the natural injections between two ordinals. However this chain does not have an upper bound, since an upper bound would be a group, which is a set, that embeds all ordinals. This is a contradiction to Hartogs theorem, of course.\n(You can think about this in the following analog: an infinite chain of finite sets, or finite groups, will not have an upper bound which is also finite.)", "meta": {"post_id": 3103685, "input_score": 23, "output_score": 51, "post_title": "Why does Zorn's Lemma fail to produce a largest group?"}}
{"input": "This post discusses the integral, \n$$I(k)=\\int_0^k\\pi(x)\\pi(k-x)dx$$\nwhere $\\pi(x)$ is the prime-counting function. For example,\n$$I(13)=\\int_0^{13}\\pi(x)\\pi(13-x)dx = 73$$\nUsing WolframAlpha, the first 50 values for $k=1,2,3,\\dots$ are,\n$$I(k) = 0, 0, 0, 0, 1, 4, 8, 14, 22, 32, 45, 58, 73, 90, 110, 132, 158, 184, 214, 246, 282, 320, 363, 406, 455, 506, 562, 618, 678, 738, 804, 872, 944, 1018, 1099, 1180, 1269, 1358, 1450, 1544, 1644, 1744, 1852, 1962, 2078, 2196, 2321, 2446, 2581, 2718,\\dots$$\nWhile trying to find if the above sequence obeyed a pattern, I noticed a rather unexpected relationship:\n\n\nQ: For all $n>0$, is it true,\n  $$I(6n+4) - 2\\,I(6n+5) + I(6n+6) \\overset{\\color{red}?}= 0$$\n\nExample, for $n=1,2$, then \n$$I(10)-2I(11)+I(12)=32-2*45+58 = 0$$\n$$I(16)-2I(17)+I(18)=132-2*158+184= 0$$\nand so on.", "output": "The answer is yes. Sketch of solution:\n$$\nI(k) = \\int_0^k \\sum_{p\\le x} \\sum_{q\\le k-x} 1 \\,dx = \\sum_p \\sum_{q\\le k-p} \\int_p^{k-q} dx = \\sum_p \\sum_{q\\le k-p} (k-(p+q)) = \\sum_{m\\le k} r(m)(k-m),\n$$\nwhere $r(m)$ is the number of ways of writing $m$ as the sum of two primes. Then\n\\begin{align}\nI(6n+6) &{}-2I(6n+5)+I(6n+4) \\\\\n&= \\sum_{m \\le 6n+4} r(m)\\big((6n+6-m)-2(6n+5-m) +(6m+4-m)\\big) + r(6n+5) \\\\&= 0 + r(6n+5);\n\\end{align}\nand $r(6n+5)=0$ for every $n\\ge1$, since the only way the odd integer $6n+5$ can be the sum of two primes is $6n+5=2+(6n+3)$, but $6n+3=3(2n+1)$ is always composite when $n\\ge1$.\nThe same argument gives $I(6n+2)-2I(6n+1)+I(6n) = r(6n+1)$, which is $2$ if $6n-1$ is prime and $0$ otherwise; this is why (as observed by John Omielan) it equals $2$ for $1\\le n\\le 5$ but $0$ for $n=6$.", "meta": {"post_id": 3128367, "input_score": 54, "output_score": 77, "post_title": "A curious equality of integrals involving the prime counting function?"}}
{"input": "It's my observation.\nLet\n$$n=p_1\u00d7p_2\u00d7p_3\u00d7\\dots\u00d7p_r$$\nwhere $p_i$ are prime factors and\n$f$ and $g$ are the functions\n$$f(n)=1+2+\\dots+n$$\nAnd\n$$g(n)=p_1+p_2+\\dots+p_r$$\nIf we put $n=21$ then\n$$g(f(21))=g(231)=21.$$\nI checked it upto $n=10000$, I did not find another number with this property $g(f(n))=n$.\nCan we prove that other such numbers do not exist?", "output": "This is a very interesting question\u2026\n$\\newcommand{sopfr}{\\operatorname{sopfr}}$\n$f(n)=\\frac{n(n+1)}2$ and $g(n)=\\sopfr(n)$, the sum of prime factors of $n$ with repeats (OEIS A001414). We want $n$ such that $g(f(n))=n$ or\n$$\\sopfr\\left(\\frac{n(n+1)}2\\right)=n\\tag1$$\nwhich can be split into two cases due to the property $\\sopfr(ab)=\\sopfr(a)+\\sopfr(b)$.\n\nIf $n$ is even, then $\\sopfr\\left(\\frac n2\\right)+\\sopfr(n+1)=n$. We know that $\\sopfr(n)\\le n$, so $\\sopfr\\left(\\frac n2\\right)\\le\\frac n2$ and consequently $\\sopfr(n+1)\\ge\\frac n2$. Either $n+1$ is a prime, in which case the LHS of $(1)$ is greater than $n$ and so the equality cannot hold, or $n+1$ is odd composite and so has a least prime factor at least 3*, yielding $\\sopfr(n+1)\\le3+\\frac{n+1}3$ and thus\n$$\\frac n2\\le3+\\frac{n+1}3$$\nwhich is only true for $n\\le20$. Checking those $n$ reveals no solutions to $(1)$.\nIf $n$ is odd, the reasoning is similar: $\\sopfr\\left(\\frac{n+1}2\\right)+\\sopfr(n)=n$, where $\\sopfr\\left(\\frac{n+1}2\\right)\\le\\frac{n+1}2$ and so $\\sopfr(n)\\ge\\frac{n-1}2$. Since $n$ is odd, either it is prime and the LHS of $(1)$ is greater than $n$, or it has a least prime factor at least 3* and $\\sopfr(n)\\le3+\\frac n3$, giving\n$$\\frac{n-1}2\\le3+\\frac n3$$\nwhich only holds for $n\\le21$. 21 is the solution to $(1)$ pointed out in the original question; we have just shown it is the only one.\n\n\n*Technically we have to repeat the argument for other possible least prime factors $k$ of $n$ or $n+1$ \u2013 and the upper bound $N_k$ of the solution to the inequalities in $n$ increases accordingly, each 3 replaced with $k$. However, the least composite number with least prime factor $k$ is $k^2$, and this increases much faster than $N_k$ (which is $\\sim\\frac k2$). Indeed, $5^2$ already exceeds $N_5$ for both inequalities.\nThe method I use above has very strong similarities to the method I used in my most famous answer of all. It is sheer coincidence that 21 is a solution to both the problems I answered.", "meta": {"post_id": 3131137, "input_score": 41, "output_score": 54, "post_title": "Is there any other number that has similar properties as $21$?"}}
{"input": "Let $\\{p_{n}\\}$ be a sequence of polynomials and $f$ a continuous function\non $[0,1]$ such that $\\int\\limits_{0}^{1}|p_{n}(x)-f(x)|dx\\to 0$.\nLet $c_{n,k}$ be the coefficient of $x^{k}$ in $p_{n}(x)$. Can we conclude\nthat $\\underset{n\\rightarrow \\infty }{\\lim }c_{n,k}$ exists for each $k$?.\nWhat I know so far: if the degrees of $p_{n}^{\\prime }s$ are bounded then\nthis is true. In fact, we can replace $L^{1}$ convergence by convergence in\nany norm on $C[0,1]$; to see this we just have to note that for fixed $N$, $%\n\\sum_{k=0}^{N}c_{i}x^{i}\\rightarrow (c_{0},c_{1},...,c_{N})$ is a\nlinear map on a finite-dimensional subspace and hence it is continuous. My\nguess is that the result fails when there is no restriction on the degrees.\nBut if $p_{n}(z)$ converges uniformly in some disk around $0$ in the complex\nplane then the conclusion holds. To construct a counterexample we have to\navoid this situation. Maybe there is a very simple example but I haven't been\nto find one. Thank you for investing your time on this.", "output": "Consider the sequence of polynomials\n$$\np_n(x)=\\left\\{\n  \\begin{array}{@{}ll@{}}\n    (1-x)^n, & \\text{if}\\ n\\equiv 0 \\mod 2 \\\\\n    x^n, & \\text{if}\\ n\\equiv 1 \\mod 2\n  \\end{array}\\right.\n$$\nThen $p_n(x)$ converges to $0$ but the constant term is oscillating.", "meta": {"post_id": 3152512, "input_score": 47, "output_score": 57, "post_title": "Does convergence of polynomials imply that of its coefficients?"}}
{"input": "Recently I was browsing math Wikipedia, and found that Harald Helfgott announced the complete proof of the weak Goldbach Conjecture in 2013, a proof which has been accepted widely by the math community, but according to Wikipedia hasn\u2019t been accepted by any major journals. My first question:\n\nHas Helfgott\u2019s proof been verified as of now? Why hasn\u2019t it been published in a peer-reviewed journal yet (or has it and I\u2019m just ignorant)?\n\nSecondly, I found that he announced his proof the same day Yitang Zhang announced his result of the 70,000,000-prime bound (a remarkable coincidence indeed). Zhang\u2019s result got a lot of coverage, from Numberphile (who made like 5 videos about it compared to like 1 video about the Goldbach conjecture mentioning Helfgott in a passing comment), to science newspapers/magazines, to Terry Tao, James Maynard, and the polymath project. I mean, his work made it to the Annals of Mathematics in Princeton!\nComparatively, I found very low coverage on Helfgott\u2019s result, and it seems like people rank the importance of Zhang\u2019s result above Helfgott\u2019s in math ranking sites, such as https://www.mathnasium.com/top-10-mathematical-achievements-in-past-5-years, which explicitly gives the top spot to Zhang with \u201cno surprise\u201d. Also as I\u2019ve mentioned before I don\u2019t think he published in a major journal compared to Zhang who published in the Annals. Second question: \n\nWhy did Helfgott\u2019s proof produce less of a stir in the math community than Zhang\u2019s work? Was Helfgott\u2019s work not groundbreaking enough?\n\n(Is it perhaps because of the fact that Vinogradov had already proven the weak Goldbach Conjecture for sufficiently large numbers in 1937 and Helfgott \u201csimply\u201d lowered the bound, whereas Zhang\u2019s work shrank the bound from infinity to a finite quantity? Still wouldn\u2019t Helfgott\u2019s work deserve publication in a peer-reviewed journal?)", "output": "Update: I'm making most of the current version of the book publicly accessible. Comments and other feedback are much appreciated!\n\nJust a few remarks so as to keep everybody informed.  (I came across this page\nby chance while looking for something else.)\nAs far as I know, nobody has found any serious issues with the proof.\n(There was a rather annoying but non-threatening error that I found in section 11.2 and fixed myself, and of course some typos and slips here are there; none affect the overall strategy or the final result.)\nA manuscript containing the full proof was accepted for publication at\nAnnals of Mathematics Studies back in 2015. I was asked to rewrite matters\nfairly substantially for expository reasons, though the extent of the revisions\nwas left up to my discretion.\nPublishing a lengthy proof (about 240 pages in its shortest complete version,\nwhich was considered too terse by some) is never trivial. Publishing it in top\njournals, where the backlog is often very large, is even more complicated.\n(Many thanks are due to the editors of a top journal -- which does often\npublish rather long articles -- for their candid description of complicated decisions in the\neditorial process.) I was thus delighted when the manuscript was accepted for\npublication in Annals of Mathematics Studies, which publishes book-length\nresearch monographs.\nA very detailed referee report was certainly helpful; it was as detailed as\none could reasonably ask from a single author. At the same time, I felt that\nit would be best for everybody if there were a second round of refereeing,\nwith individual referees taking care of separate chapters. So, I asked the publishers for such a second round, and they graciously accepted.\nOne of the (first-round) referees had suggested that I treat the manuscript as a draft to be fairly thoroughly restructured, and that I add several introductory\nchapters. While I found the request a little overwhelming at first, and while\nthe editors did not demand as much of me, I became convinced that the referee\nwas right, and set about the task.\nWhat follows is a long, still not quite finished story of a process that took\nlonger than expected, in part due to my commitments to other projects, in part\nperhaps due to a certain perfectionism on my part, in part due to publishing\nmishaps that you definitely do not want to hear about, and above all because\nit became clear to me, not only that the proof had had fairly few\nthorough readers, but that it would be worthwhile for it to have a\nsubstantially wider readership.\nTo expand on what has been said by other people who replied to or commented\non the original poster's question: knowing that ternary Goldbach holds for all\neven integers $n\\geq 4$ is not likely to have very many applications, though\nit does have some. In that sense it may be seen as the end of a road. The\nfurther use of the proof will reside mainly in the techniques that had to be\napplied, developed and sharpened for its sake. For that matter, the same is\narguably true of Vinogradov's work -- it arguably brought the circle method to\nits full maturity, after the foundational work of Hardy, Littlewood and\nRamanujan, besides showing the power that combinatorial identities can have in\nwork on the primes. \nFrom that perspective, it makes sense for the proof to be published as a book\nthat, say, a graduate student, or a specialist in a neighboring field,\ncan read with profit. Of course it is still fair and necessary to assume that the reader has taken the equivalent of a first graduate course in analytic number theory.\nIn the current version, the first hundred pages are taken by an\nintroduction and by chapters on what can be called the basics of analytic\nnumber theory from an explicit and computational viewpoint. Then come 40 pages on further\ngroundwork on the estimation of common sums in analytic number theory\n - sums over primes, sums of $\\mu(n)$, sums of $\\mu^2(n)/\\phi(n)$, etc. (I should\nsingle out the contributions of O. Ramar\u00e9 to the explicit understanding of\nsums of $\\mu(n)/n$ and $\\mu^2(n)/\\phi(n)$ as invaluable.) Then there\nare close to 120 pages on improvements or generalizations on various versions\nof the large sieve, their connection to the circle method, and also\non an upper-bound quadratic sieve. (This last subject got a little too\ninteresting at some point; I am glad my treatment is done!) Then comes an\nexplicit treatment of exponential sums, in some sense the core of the proof.\n(The smoothing function used here has been changed from that in the original\nversion.)\nThen comes the truly complex-analytic part. I am editing that part a little\nso that people who are not interested mainly in ternary Goldbach\nwill be able to take what they need on parabolic cylinder functions, the\nsaddle-point method or explicit formulas (explicit explicit formulas?). Then\ncomes the part where different smoothing functions have to be chosen - again,\nI am currently editing so that others can readily pick up ideas that probably\nhave wider applicability. The calculations that are needed for the ternary\nGoldbach problem and no other purpose take fewer than 20 pages at the end.\nI believe I can say the heavy part is mostly over; I am currently\ndoing some editing on the second half (or rather the last two fifths) of the\nbook while waiting to hear from several of the second-round referees I\nrequested myself. Of course I am also working on other things as well.\nAll being said, I would not necessarily recommend any non-masochist friend to\nwrite a book-length monograph in the future -- though some other people seem to\nmanage -- not just because the time things take seems to be quadratic on the\nlength of the text, which itself increases monotonically, but also because it\nis frustrating that it is hard to post periodic updates (certainly\nharder than for independent papers), in that always some part of the whole is\nundergoing construction. At the same time, I hope to be happy with the end\nresult.", "meta": {"post_id": 3164020, "input_score": 33, "output_score": 57, "post_title": "The significance and acceptance of Helfgott\u2019s proof of the weak Goldbach Conjecture"}}
{"input": "I know that QR decomposition is a mean to solve a system $Ax=b$ by doing $A = QR$ and then solving $Qy = b$ and then $Rx=y$.\nI know that the least squares method is used to find $\\min ||Ax-b||$, that is, it can find the $x$ that is closest to solve $Ax=b$ or that solves it exactly.\nI often see QR decomposition in context of least squares but I can't see what they have in common.", "output": "Overview\nDifficulties in understanding why we use QR-decompositions to find linear least squares could arise either because the mathematics is poorly motivated (why would we choose this method?) or because the actual mechanics of the calculation are mysterious (how would we do it this way?). Below, I try to address both sets of issues.\nWe'll look at:\n\nWhy we would want to do linear least squares.\nHow to set up the least squares problem as an optimization problem.\nHow to solve this problem analytically.\nWhy numerical issues motivate us to use QR to solve the problem.\nHow to calculate QR.\nHow to use QR.\n\nWhy Are We Doing This?\ntl;dr To make a predictive model from some data.\nImagine that we have a set of data defined by $(x,y)$ values. These $(x,y)$ values make a scatterplot. We would like to develop a mathematical model of this data so that for any $x$ value we can predict the corresponding $y$ value. You might say that we are searching for a function $f(x)$ that predicts $y$ well.\nGraphically, we want to go from this:\n\nto this:\n\nWe would like $f(x)$ to be a simple model (we'll discuss more complicated models below) so that it is easy to interpret and fast to evaluate. Therefore, we choose a line. Thus, our model has the form $f(x)=a*x+b$.\nIf we choose $a$ and $b$ well, we will have a line that is a good fit to our data.\nWhat Are Our Fitting Options?\nBut what is a good fit? Why is the green line below a better choice than the orange and blue lines?\n\nThere are many different ways to define what a good fit is, but probably all of them involve the distances between the points and the fit line. Pretend we have already chosen a value for $a$ and a value for $b$. Together, these define a line. Now, for each point in the dataset, we can measure its distance to the line. There are different ways we can do this. We could measure the vertical, horizontal, or shortest-path distance between the fit line and the data point, like so:\n\nThe offsets shown in (a) are called perpendicular offsets. These offsets are useful in situations where the $x$ values of our data have an uncertainty/error component. Total least squares is used to find this fit line.\nThe offsets shown in (c) are horizontal. We will never concern ourselves with this situation mathematically because we can flip the axes on (c) to get (b).\nThe offsets shown in (b) are the vertical offsets we'll be discussing at length here. These are useful in situations where the uncertainty in our $x$ values is much less than the uncertainty in our $y$ values. We can find the best fit line to this using linear least squares. LLS is a generalization of ordinary least squares, which assumes all $y$ have the same uncertainty, and also includes weighted least squares (in which each $y$ may have different uncertainty) and generalized least squares (which is used when the residuals of a model have correlations).\nToday, we'll assume all of our $y$ have the same uncertainty and will find the best-fit line using ordinary least squares.\nPedagogically, we'll also do this because the math has nice analytic forms and relatively easy explanations. Much later, you'll forget that all these other options existed and spend the rest of your life developing sub-standard models, but that's another story.\nChoosing An Error Function\nThe foregoing means that for a given point $i$ with coordinates $(x,y)$ and a model $f(x)$, the error of that point with respect to the model is $e_i=g(f(x_i),y_i)$ where $g(\\cdot)$ is some function. The total error is the sum of all of these errors: $\\sum_i e_i$.\nHow can we choose an appropriate $g(\\cdot)$?\nSince we're summing $g(\\cdot)$ across many inputs, it's clear that $g(\\cdot)$ should always be positive, otherwise the error of one point would cancel another. So $g(\\cdot)$ needs to map the range $(-\\infty,\\infty)$ to the range $[0, \\infty)$. A couple of obvious functions that do this are the square function and the absolute value function. We will prefer the square function because:\n\nIt has nice derivatives, allowing for easy mathematical analysis.\nIt implies that the uncertainties in the $y$ terms are normally distributed.\nIf we assume that our $y$ datapoints are independent and normally distributed, the least squares method using a squared error term is the result of maximum likelihood.\nIt penalizes points farther from the line more than points closer to the line, which has a certain karmic justice about it.\n\nSo, for a point $(x,y)$ and a model $f(x)$ the error of that point with respect to the model is $e_i=(f(x_i)-y_i)^2$. The total error is still $\\sum_i e_i$.\nRegardless of which error we choose, if we search all possible combinations of values of a and b and choose the one with the smallest sum of errors, we call this the best-fitting model. It is the model that minimizes the squared error or, in other words, it is the least-squares fit.\nIf we were using total least squares with perpendicular errors, examining all the models might look like this, except that we'd also have to search all the possible offsets of the line:\n\nHeading for Higher Dimensions\nNow, for our $(x,y)$ points above, $x$ might have represented the horsepower of a car while $y$ represented miles per gallon (MPG). But maybe horsepower alone might be a poor indicator of MPG, so we would like to use more data. Maybe using Horsepower, Weight, and Displacement together could give us a better model for predicting MPG.\n\nSo, we make a matrix $X$. Each row of $X$ is some data we know about a car: it's horsepower, weight, and displacement. A vector y still contains the MPG.\nIf we want a simple model to predict MPG, our simplest model still looks like a line, $f(X) = X\\beta+b$, except that $\\beta$ and $b$ are vectors.\nOur error is still $f(X)-y=X\\beta+b-y$. But note that we can simplify this equation! If we add a column of 1 to the left-hand side of $X$ and an additional entry to $\\beta$, to get the vector $<\\beta_\\textrm{new}, \\beta_1, \\beta_2, \\beta_3, \\ldots>$ then $X\\beta+b$ with this modified $X$ and new $\\beta$ automagically includes the +b offset from $X\\beta+b-y$. That is, we've made this change:\n$$\n\\begin{bmatrix}\nX_{1,1} & X_{1,2} & X_{1,3} \\\\\nX_{2,1} & X_{2,2} & X_{2,3} \\\\\nX_{3,1} & X_{3,2} & X_{3,3}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3\n\\end{bmatrix}\n\\rightarrow\n\\begin{bmatrix}\n1 & X_{1,1} & X_{1,2} & X_{1,3} \\\\\n1 & X_{2,1} & X_{2,2} & X_{2,3} \\\\\n1 & X_{3,1} & X_{3,2} & X_{3,3}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3\n\\end{bmatrix}\n$$\nThus, we can write the error in a simpler form as $X\\beta-y$. This returns a vector. The sum of squares is the dot product of this vector with itself: $(X\\beta-y)^T(X\\beta-y)$. So minimizing this gives the best-fit line in the least-squares sense.\nDeveloping An Optimization Problem\nSo how do we get $\\beta$? Since everything in math is better if we're solving for $x$ let's rewrite our equation $X\\beta-y$ above as $Ax-b$.\nFormally, the problem we wish to solve is\n$$\n\\textrm{argmin}_x \\lVert Ax-b \\rVert_2\n$$\nwhere argmin gives us the value of $x$ that minimizes the function.\nNote that any minimizer of the 2-norm (aka the Euclidean norm or p=2 norm) is also a minimizer of the square of the 2-norm, so finding a solution to $\\lVert Ax-b\\rVert_2^2=(Ax-b)^T(Ax-b)$ also solves the problem, as discussed above.\nWe can find an analytical solution to this!\nFirst, expand:\n$$\n(Ax-b)^T(Ax-b) = x^TA^T A x -x^TA^Tb -b^TAx + b^Tb\n$$\nSince $x^TA^Tb$ gives a scalar, swapping the transposes doesn't affect its shape, so we get\n$$\nx^TA^T A x -2b^TAx + b^Tb\n$$\nFrom matrix calculus we know that:\n$$\n\\begin{aligned}\n\\nabla_x(x^TA^Tb) &= A^Tb \\\\\n\\nabla_x(x^TA^TAx) &= 2A^TAx \\\\\n\\nabla_x(b^T b) &= 0\n\\end{aligned}\n$$\nSetting the derivative equal to zero and solving gives:\n$$\n\\begin{aligned}\n0&=2A^TAx - 2A^Tb \\\\\nA^TAx&=A^Tb \\\\\n\\end{aligned}\n$$\nThis is called the normal equation.\nWhy the normal equation won't do\nWe can solve the normal equation with some simple matrix algebra to get:\n$$\nx=(A^TA)^{-1}A^Tb\n$$\n(Note that $(A^TA)^{-1}A^T$ is the Moore-Penrose inverse.)\nUnfortunately, it is a rule in numerical linear algebra that we never want to calculate the inverse of any matrix if we can possibly avoid it because:\n\nAlthough $A$ might be sparse, $A^{-1}$ is rarely so. For large matrices this means that taking $A^{-1}$ can cause your computer to run out of memory.\nThe inverse is used often mathematically to solve the equation $Ax=b$, but in practice computing $A^{-1}$ is slower than solving the equation $Ax=b$ using other methods we'll discuss below.\nYou might think that if you are solving $Ax=b$ for many values of $b$ then finding $A^{-1}$ only once would save time, but when we solve $Ax=b$ we will factor $A$ in a way we can reuse.\nFinding and using $A^{-1}$ often results in answers that are not as exact as they could be. Remember, computers use floating-point arithmetic so they are susceptible to numerical error. (The many reasons why are explained in the classic, but lengthy, article What every computer scientist should know about floating-point arithmetic.) These articles show some numerics: 1, 2.\n\nTherefore, we would like a way of finding x that doesn't use inverses.\nSolving Without Inverses\nThere is a beautiful thing called a QR-decomposition in the world.\nIt takes a matrix $A$ and builds two matrices $Q$ and $R$ such that $A=QR$. These matrices have special properties:\n\n$Q$ is an orthogonal matrix\n$R$ is an upper-traingular matrix\n\nFrom above, we know that the equation we need to solve is:\n$$\nA^T A x = A^Tb\n$$\nIf we plug $A=QR$ into this equation we get:\n$$\n\\begin{aligned}\nA^T A x &= A^Tb \\\\\n(QR)^T (QR) x &= (QR)^T b \\\\\nR^T Q^T QR x &= R^T Q^T b\n\\end{aligned}\n$$\nSince Q is orthogonal, we know that $Q^T Q = I$, so...\n$$\n\\begin{aligned}\nR^T Q^T QR x &= R^T Q^T b \\\\\nR^T R x &= R^T Q^T b  \\\\\nR x &= Q^T b\n\\end{aligned}\n$$\n$Q^Tb$ is just a vector $v$ so we have\n$$\nRx=v\n$$\nBut, since $R$ is upper-triangular we can easily solve this equation by using back substitution.\nSo, we now have a nice way of solving the least squares problem without taking an inverse!\nSome Clarifications\nFinding the QR-decomposition. A QR-decomposition is any procedure that gives you a $QR$ for a matrix $A$. The textbook approach to this is the Gram-Schmidt algorithm. In practice, Gram-Schmidt is not numerically stable enough so no computer software uses it. Instead, techniques like Householder reflections are used.\nIt is not the QR algorithm! Folks who are sloppy with their language may refer to finding a QR-decomposition as \"the QR algorithm\". This is bad. The actual QR algorithm is used to find the eigenvalues of a matrix. Folks should say \"Gram-Schmidt\" algorithm instead.\nThe Gram-Schmidt Process\nWe will use the Gram-Schmidt Process to find the QR-decomposition of our matrix $A$.\nTo do so, we represent the columns of $A$ as $a_i$:\n$$\nA =\n\\left[\n\\begin{array}{c|c|c|c}\na_1 & a_2 & \\ldots & a_n\n\\end{array}\n\\right]\n$$\nThe Gram-Schmidt process then finds a series of vectors by subtracting successively subtrating projections of the columns of $A$ from each other.\nIf we define the projection operator of two vectors $u,v$ as:\n$$\n\\textrm{proj}_u(v) = \\frac{u\\cdot v}{u \\cdot u} u\n$$\nthen Gram-Schmidt looks like:\n$$\n\\begin{aligned}\nu_1&=a_1, & e_1&=\\frac{u_1}{\\lVert u_1 \\rVert} \\\\\nu_2&=a_2 - \\textrm{proj}_{u_1}(a_2), & e_2&=\\frac{u_2}{\\lVert u_2 \\rVert}\\\\\nu_3&=a_3 - \\textrm{proj}_{u_1}(a_3) - \\textrm{proj}_{u_2}(a_3), & e_3&=\\frac{u_3}{\\lVert u_3 \\rVert}\\\\\nu_k&=a_k - \\textrm{proj}_{u_1}(a_k) - \\ldots - \\textrm{proj}_{u_{k-1}}(a_k), & e_k&=\\frac{u_k}{\\lVert u_k \\rVert}\n\\end{aligned}\n$$\nIf we use the $e$ vectors to our advantage, we can rewrite this as:\n$$\n\\begin{aligned}\nu_1&=a_1, & e_1&=\\frac{u_1}{\\lVert u_1 \\rVert} \\\\\nu_2&=a_2 - (a_2 \\cdot e_1)e_1, & e_2&=\\frac{u_2}{\\lVert u_2 \\rVert}\\\\\nu_3&=a_3 - (a_3 \\cdot e_1)e_1 - (a_3\\cdot e_2)e_2, & e_3&=\\frac{u_3}{\\lVert u_3 \\rVert}\\\\\nu_k&=a_k - (a_k \\cdot e_1)e_1 - \\ldots - (a_k\\cdot e_{k-1})e_{k-1}, & e_k&=\\frac{u_k}{\\lVert u_k \\rVert}\n\\end{aligned}\n$$\nOur QR factorization $Q=AR$ is then:\n$$\nA =\n\\left[\n\\begin{array}{c|c|c|c}\na_1 & a_2 & \\ldots & a_n\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}{c|c|c|c}\ne_1 & e_2 & \\ldots & e_n\n\\end{array}\n\\right]\n\\begin{bmatrix}\na_1 \\cdot e_1 & a_2\\cdot e_1 & \\ldots & a_n \\cdot e_1 \\\\\n0             & a_2\\cdot e_2 & \\ldots & a_n \\cdot e_2 \\\\\n\\vdots        & \\vdots       & \\ddots &        \\vdots \\\\\n0             & 0            & \\ldots & a_n \\cdot e_n \\\\\n\\end{bmatrix}\n$$\nNote that QR-decompositions are only unique up to their sign. Meaning that\n$$A=QR=(-Q)(-R)$$\ntherefore, you should not be surprised if different algorithms give you matrices that are the negative of the ones you find.\nQuadratic Fitting Example\nNow, imagine we have the matrix the following 2D data:\n x  y\n-2  4\n-1  1\n 1  2\n 2  1\n 3  5\n 4  6\n\nWe'll start off by doing some exploratory data analysis (EDA) because we don't want to miss any gorillas in our dataset. We'll do this by plotting it:\n\nThe data goes down and back up again, so it's plausible to thing that a quadratic polynomial is a good fit for this small dataset. That polynomial is given by:\n$$\nP(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2\n$$\nWe have several points, so we have the system of equations:\n$$\n\\begin{aligned}\nP(x_1) &= y_1 &= \\beta_0 + \\beta_1 x_1 + \\beta_2 {x_1}^2 \\\\\nP(x_2) &= y_2 &= \\beta_0 + \\beta_1 x_2 + \\beta_2 {x_2}^2 \\\\\n\\vdots &= \\vdots &= \\vdots \\\\\nP(x_n) &= y_n &= \\beta_0 + \\beta_1 x_n + \\beta_2 {x_n}^2 \\\\\n\\end{aligned}\n$$\nWe can write this in matrix form as:\n$$\n\\begin{bmatrix}\n1 & x_1 & {x_1}^2 \\\\\n1 & x_2 & {x_2}^2 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & x_n & {x_n}^2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\ \\beta_1 \\\\ \\beta_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nP(x_1) \\\\\nP(x_2) \\\\\n\\vdots \\\\\nP(x_n) \\\\\n\\end{bmatrix}\n$$\nThis has exactly the form of the matrices above, so we can find a least-squares fit using a QR-decomposition! Note that the matrix on the left-hand side of the equation above is the Vandermonde matrix.\nFor our numerical problem we have the matrix $A$:\n1 -2  4\n1 -1  1\n1  1  1\n1  2  4\n1  3  9\n1  4 16\n\nLet's run Gram-Schmidt on it by hand:\nu1 = A_1 = [1,1,1,1,1,1]\n\ne1 = u1 / || u1 ||\n   = [1,1,1,1,1,1] / || [1,1,1,1,1,1] ||\n   = [1,1,1,1,1,1] / \u221a6\n\nu2 = a2 - (a2 \u00b7 e1)e1\n   = [-2,-1,1,2,3,4] - ([-2,-1,1,2,3,4] \u00b7 [1,1,1,1,1,1] / \u221a6) [1,1,1,1,1,1] / \u221a6\n   = [-2,-1,1,2,3,4] - 2.857738033247042 * [1,1,1,1,1,1] / \u221a6\n   = [-2,-1,1,2,3,4] - [1.166, 1.166, 1.166, 1.166, 1.166, 1.166]\n   = [-3.166, -2.166, -0.166, 0.833, 1.833, 2.833]\n\ne2 = u2 / || u2 ||\n   = [-3.166, -2.166, -0.166, 0.833, 1.833, 2.833] / || [-3.166, -2.166, -0.166, 0.833, 1.833, 2.833] ||\n   = [-3.166, -2.166, -0.166, 0.833, 1.833, 2.833] / 5.180090089306685\n   = [-0.611, -0.418, -0.0321, 0.1608, 0.3539, 0.54696]\n\nu3 = a3 - (a3 \u00b7 e1)e1 - (a3 \u00b7 e2)e2\n   = [4,1,1,4,9,16]\n     - ([4,1,1,4,9,16] \u00b7 [1,1,1,1,1,1] / \u221a6) [1,1,1,1,1,1] / \u221a6\n     - ([4,1,1,4,9,16] \u00b7 [-0.611, -0.418, -0.0321, 0.1608, 0.3539, 0.54696]) [-0.611, -0.418, -0.0321, 0.1608, 0.3539, 0.54696]\n   = [4,1,1,4,9,16]\n     - 14.288690166235206 * [1,1,1,1,1,1] / \u221a6\n     - 9.68451625392119 * [-0.611, -0.418, -0.0321, 0.1608, 0.3539, 0.54696]\n   = [4,1,1,4,9,16]\n     - [5.83, 5.83, 5.83, 5.83, 5.83, 5.83]\n     - [-5.92, -4.05, -0.31, 1.56, 3.43, 5.30]\n   = [4.09, -0.78, -4.52, -3.39, -0.26, 4.87]\n\ne3 = u3 / || u3 ||\n   = [4.09, -0.78, -4.52, -3.39, -0.26, 4.87] / || [4.09, -0.78, -4.52, -3.39, -0.26, 4.87] ||\n   = [4.09, -0.78, -4.52, -3.39, -0.26, 4.87] / 8.546547739343035\n   = [0.478, -0.092, -0.529, -0.396, -0.031, 0.5697]\n\nThis gives us a Q matrix of\nQ = [e1, e2, e2]\n  = |-0.40824829 -0.61131498  0.47819969|\n    |-0.40824829 -0.41826814 -0.09157015|\n    |-0.40824829 -0.03217447 -0.529072  |\n    |-0.40824829  0.16087236 -0.396804  |\n    |-0.40824829  0.3539192  -0.03052338|\n    |-0.40824829  0.54696603  0.56976985|\n\nWe can double-check this in code using Python:\nimport numpy as np\n\ndata = [-2,-1,1,2,3,4]\ny = [4,1,2,1,5,6]\n\nA = np.vander(data, N=3)\nA = np.fliplr(A)\n\nQ, R = np.linalg.qr(A)\n\nx_from_qr = np.linalg.solve(R, Q.T@y)\n\nx_from_lstsq, residuals, rank, singular_valeus = np.linalg.lstsq(A,y)\n\ndiff=np.linalg.norm(x_from_qr-x_from_lstsq)\n\nThis gives\n>>> Q\narray([[-0.40824829, -0.61131498,  0.47819969],\n       [-0.40824829, -0.41826814, -0.09157015],\n       [-0.40824829, -0.03217447, -0.529072  ],\n       [-0.40824829,  0.16087236, -0.396804  ],\n       [-0.40824829,  0.3539192 , -0.03052338],\n       [-0.40824829,  0.54696603,  0.56976985]])\n>>> R\narray([[ -2.44948974,  -2.85773803, -14.28869017],\n       [  0.        ,   5.18009009,   9.68451625],\n       [  0.        ,   0.        ,   8.54654774]])\n>>> x_from_qr\narray([ 1.1       , -0.35357143,  0.425     ])\n>>> x_from_lstsq\narray([ 1.1       , -0.35357143,  0.425     ])\n>>> diff\n1.1564453092915794e-15\n\nIf we take these coefficients and plug them into the equation:\n$$\nP(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2\n$$\nwe get\n$$\nP(x) = 1.1 + (-0.35357143)*x + 0.425*x^2\n$$\nPlotting this against the original data gives us:", "meta": {"post_id": 3185211, "input_score": 24, "output_score": 37, "post_title": "What does QR decomposition have to do with least squares method?"}}
{"input": "I want to colour the US (only the states) map with Yellow, Green, Red and Blue. I was wondering what would be the lowest number of states with the colour of Green. We can of course use the other colours as much as we want. Please note that I want to follow the Four Color Theorem rules.\nMotivation:\nI am studying graph theory and I want to know if there is a way that we could limit the use of the fourth colour as much as possible. This is not a homework problem.\nMy attempt:\nI have tried many variations and can limit it to 6 and it seems like the \nminimum possible but there are many possibilities to try ($4^{50}$). Therefore I was wondering if there is a simpler method? Thank you in advance.\nClarification:\nI am interested in only the mainland of USA. For states like Michigan that are split, I used the same colour for both parts.", "output": "The minimum is two states that use the fourth color. Nevada and its five neighbors cannot be colored with only three colors, and similarly West Virginia and its five neighbors cannot be colored with only three colors. In both cases, once you color the center state one color (say, red), you can't use it again on its neighbors: without using green, they'd have to alternate yellow-blue-yellow-blue, but because the number of neighbors is odd, you'd get stuck at the end.\n(In the comments, David K points out that Kentucky is a third state with the same problem: it has seven neighbors. But this doesn't force us to use a third green state, because Kentucky and West Virginia share a border and some common neighbors.) \nUsing only two green states is possible. If we color Arizona (dealing with the Nevada situation) and Ohio (dealing with West Virginia and Kentucky) both green, then the remainder of the map can be completed using only blue, red, and yellow:\n\nAdjacencies between the states may be easier to see here.", "meta": {"post_id": 3186983, "input_score": 27, "output_score": 38, "post_title": "How to colour the US map with Yellow, Green, Red and Blue to minimize the number of states with the color of Green"}}
{"input": "When trying to solve a physics problem on decoupling a system of ODEs, I found myself needing to address the following problem:\n\nLet $A_n\\in M_n(\\mathbb R)$ be the matrix with all $1$s above its main diagonal, all $-1$s below its diagonal, and $0$s everywhere else. Is $A_n$ always diagonalisable? If so, what is its diagonalisation (equivalently: what are its eigenvalues and corresponding eigenvectors)?\n\nFor example,\n$$A_3=\\begin{bmatrix}0&1&0\\\\-1&0&1\\\\0&-1&0\\end{bmatrix},\\quad A_5=\\begin{bmatrix}0&1&0&0&0\\\\-1&0&1&0&0\\\\0&-1&0&1&0\\\\0&0&-1&0&1\\\\0&0&0&-1&0\\end{bmatrix}.$$\n\nAssuming my code is correct, Mathematica has been able to verify that $A_n$ is always diagonalisable up to $n=1000$. If we use $\\chi_n(t)\\in\\mathbb Z[t]$ to denote the characteristic polynomial of $A_n$, a straightforward evaluation also shows that\n$$\\chi_n(t)=-t\\chi_{n-1}(t)+\\chi_{n-2}(t)\\tag{1}$$\nfor all $n\\geq4$. Furthermore, note that $A_n=-A_n^t$ so that, in the case where the dimension is even,\n$$\\det(A_{2n}-\\lambda I)=\\det(A_{2n}^t-\\lambda I)=\\det(-A_{2n}-\\lambda I)=\\det(A_{2n}+\\lambda I).$$\nThis implies that whenever $\\lambda$ is an eigenvalue of $A_{2n}$, so is $-\\lambda$. In other words, $\\chi_{2n}(t)$ is always of the form $(t^2-\\lambda _1^2)(t^2-\\lambda_2^2)\\dotsm(t^2-\\lambda_n^2)$ for some $\\lambda_i$.\nAnd this is where I am stuck. In order for $A_n$ to be diagonalisable, we must have that all the eigenvalues are distinct, but trying to use the recurrence $(1)$ and strong induction, or trying to use the formula for the even case have not helped at all. It seems like the most probable line of attack would be to somehow show that\n$$\\chi_{2n}'(t)=2t\\sum_{k=1}^n\\frac{\\chi_{2n}(t)}{t^2-\\lambda_k^2}$$\nnever shares a common zero with $\\chi_{2n}$ (which would resolve the even case), though I don't see how to make this work.\n\nNote: I do not have any clue how to actually find the eigenvalues/eigenvectors even in the case where the $A_n$ are diagonalisable. As such even if someone cannot answer the second part of the question, but can prove that the $A_n$ are diagonalisable, I would appreciate that as an answer as well. Above I tried to look at the special case where the dimension is even, though of course the proof for all odd and even $n$ is more valuable. Even if this is not possible, for my purposes I just need an unbounded subset $S\\subseteq\\mathbb Z$ for which the conclusion is proven for $n\\in S$, so any such approach is welcome too.\nThank you in advance!", "output": "All those matrices are anti-symmetric and therefore they are normal matrices. And every normal matrix is diagonalizable over $\\mathbb C$, by the spectral theorem.", "meta": {"post_id": 3198918, "input_score": 14, "output_score": 36, "post_title": "Are these square matrices always diagonalisable?"}}
{"input": "Prove that there is no group $G$ s.t. $\\operatorname{Aut}(G)=\\mathbb{Q}$\nI get the feeling that we should proceed by contradiction. \nSo let $G$ be a group s.t. $\\operatorname{Aut}(G)=\\mathbb{Q}$. Then we can identify elements of $\\mathbb{Q}$ with automorphisms of $G$... and identities such as $\\frac{1}{2}*2(g)=1(g)=g$\nCan somebody help me find a contradiction?", "output": "Suppose $G$ is a group with $\\operatorname{Aut}(G)\\cong\\mathbb{Q}$.  If $G$ is abelian, then $f(x)=-x$ is an automorphism of $G$ which satisfies $f^2=1$.  But $\\mathbb{Q}$ is torsion-free, so this implies $f=1$.  But then $G$ is a vector space over $\\mathbb{Z}/(2)$, and so its automorphism group is nonabelian if its dimension is greater than $1$ and finite otherwise.\nSo, $G$ must be nonabelian; say $x,y\\in G$ do not commute.  Now note that $G$ acts on itself by conjugation, and this gives a homomorphism $\\varphi:G\\to\\operatorname{Aut}(G)\\cong\\mathbb{Q}$ whose kernel is $Z(G)$, the center of $G$.  Note that the subgroup of $\\mathbb{Q}$ generated by $\\varphi(x)$ and $\\varphi(y)$ is cyclic (since every finitely generated subgroup of $\\mathbb{Q}$ is cyclic); say it is generated by $\\varphi(a)$ for some $a\\in G$.  Then there are $m,n\\in\\mathbb{Z}$ and $z,z'\\in Z(G)$ such that $x=a^nz$ and $y=a^mz'$.  But now we see that $x$ and $y$ actually do commute (since $z$ and $z'$ commute with everything), so we have a contradiction.", "meta": {"post_id": 3210220, "input_score": 20, "output_score": 34, "post_title": "Prove that there is no group $G$ s.t. $\\operatorname{Aut}(G)=\\mathbb{Q}$"}}
{"input": "Suppose, I want to find a function such that its Taylor series expansion is \n$$f(x) = \\sum_{n=0}^{\\infty}\\frac{x^{n+1}}{(n+1)a^n}$$\nI could start with $$\\frac{1}{1-x}=\\sum_{n=0}^{\\infty}x^n$$\nIntegrate it, substitute $x\\rightarrow  \\frac{x}{a}$, multiply by $a$ and get\n$$F(x) = -\\ln|x-1| = \\sum_{n=0}^{\\infty}\\frac{x^{n+1}}{n+1}$$\n$$a F\\left(\\frac{x}{a}\\right) = -a \\ln\\left|\\frac{x}{a}-1\\right| = \\sum_{n=0}^{\\infty}\\frac{x^{n+1}}{(n+1)a^n}$$\nOn the other hand, I could start with subtituting $x \\rightarrow \\frac{x}{a}$ before integration to get\n$$\\frac{a}{a-x} = \\sum_{n=0}^{\\infty}\\frac{x^n}{a^n}$$\nand then integrate it to get\n$$-a\\ln|x-a| = \\sum_{n=0}^{\\infty}\\frac{x^{n+1}}{(n+1)a^n}$$ \nAs you can see, arguments of $\\ln$ are not equal. Where did it go wrong?", "output": "When you integrate, you should include a constant of integration. What you see here is that when integrating the functions, you get different constants of integration. This is why your answers differ by only a constant, namely $a\\ln a$ (you can see this by use of $\\log$ rules).\nIf you take care with the limits or boundary conditions in the integration step, then the answers will agree exactly.", "meta": {"post_id": 3228101, "input_score": 18, "output_score": 39, "post_title": "Taylor series leads to two different functions - why?"}}
{"input": "For all my homework in real analysis, when I've been asked to show that a function is continuous, I just found a single $x_n \\in D$ and showed that when $x_n \\rightarrow x_0$, $f(x_n) \\rightarrow f(x_0)$. Apparently, the sequence definition (as opposed to the epsilon delta definition) is (basically) only used to prove a function is not continuous, and I can't prove a function is continuous because then I'd have to show this is true for all possible sequences? Am I doing the math wrongly? Should I always use the epsilon delta definition when trying to prove that a function is continuous?", "output": "This is indeed incorrect. Take for example the function \n$$f(x) = \\begin{cases}\n1, \\text{ if } x\\in \\mathbb{Q}\\\\\n0, \\text{ otherwise}\n\\end{cases}$$ \nThis is obviously not a continuous function. However, if you look at its behavior along a sequence of rational points, it would appear to be constant (hence continuous).", "meta": {"post_id": 3240987, "input_score": 22, "output_score": 35, "post_title": "Is showing that $x_n \\rightarrow x_0\\Rightarrow f(x_n) \\rightarrow f(x_0)$ for a single sequence enough to prove continuity?"}}
{"input": "I am now confused with such problem as title goes. To be exact, the problem is\n\nDoes there exist a functor from $A:\\mathsf{Field}\\to \\mathsf{Field}$ with a natural transformation from identity functor $\\iota: \\operatorname{id}\\to A$ such that for each $F$, $A(F)$ is the algebraically closure of $F$ through $\\iota_F:F\\to A(F)$?\n\nIt is not easy rather than first glimpse. Let me explain.\nNote that, the existence of algebraic closure only ensures that there exist a map from $\\operatorname{Obj}(\\mathsf{Field})$ to itself. Since the \"extension\" property is not unique, it is not generally true that we can extend the map to $\\operatorname{Mor}(\\mathsf{Field})$ for arbitrary choice of algebraic closure.\n\nFor example, consider the fields\n$$\\begin{array}{ccc}\n\\mathbb{Q}[\\sqrt[3]{2}, \\sqrt{2}] &\\to & \\mathbb{Q}[\\omega\\sqrt[3]{2}, \\sqrt{2}]\\\\\n\\uparrow &&\\uparrow \\\\\n\\mathbb{Q}[\\sqrt[3]{2}] & \\to & \\mathbb{Q}[\\omega\\sqrt[3]{2}]\n\\end{array}$$\nIf we choose the algebraically closure of $\\left[\\begin{matrix}\\mathbb{Q}[\\sqrt[3]{2}, \\sqrt{2}] & \\mathbb{Q}[\\omega\\sqrt[3]{2}, \\sqrt{2}]\\\\ & \\mathbb{Q}[\\omega\\sqrt[3]{2}]\\end{matrix}\\right]$ by inclusion to $\\overline{\\mathbb{Q}}$, and the closure of $\\mathbb{Q}[\\sqrt[3]{2}]\\to \\overline{\\mathbb{Q}}$ by $\\sqrt[3]{2}\\mapsto \\omega\\sqrt[3]{2}$. \nWe cannot extend a well-defined functor. Similar problem exists for transcendental extension, for example, square like this\n$$\\begin{array}{ccc}\n\\mathbb{C}[X,Y] &\\to & \\mathbb{C}[X^2,Y]\\\\\n\\uparrow &&\\uparrow \\\\\n\\mathbb{C}[X] & \\to & \\mathbb{C}[X^2]\n\\end{array}$$\nA reasonable method is to avoid phenomenon above is as follow. \nFix an algebraically closed field $F$, and take all of its subfields as \"skeleton\", then fix an isomorphism to a subfields of $F$ from all fields whose algebraic closure is $F$ up to an isomorphism. The isomorphic class of algebraically closure are completely dependen by its characteristic and the transcendental dimension over prime field $\\mathbb{Q}$ or $\\mathbb{F}_p$. \nNow the problem is how to naturally chose extensions for endmorphisms. But unfortunately, the choice is fragile. For instance, consider the following diagram\n$$\\begin{array}{ccccl}\n\\mathbb{Q}[\\sqrt{3}, \\sqrt{2}] &\\to & \\mathbb{Q}[\\sqrt{3}, \\sqrt{2}] &: &\\sqrt{3}\\mapsto -\\sqrt{3},\\sqrt{2}\\mapsto \\pm \\sqrt{2}\\\\\n\\uparrow &&\\uparrow \\\\\n\\mathbb{Q}[\\sqrt{3}] & \\to & \\mathbb{Q}[\\sqrt{3}] &:&\\sqrt{3}\\mapsto -\\sqrt{3}\n\\end{array}$$\nThere is no suitable choice such that \n$$\\begin{array}{ccccl}\n\\overline{\\mathbb{Q}} &\\to & \\overline{\\mathbb{Q}} &: &\\sqrt{3}\\mapsto -\\sqrt{3},\\sqrt{2}\\mapsto \\pm \\sqrt{2}\\\\\n\\parallel &&\\parallel \\\\\n\\overline{\\mathbb{Q}}& \\to & \\overline{\\mathbb{Q}} &:&\\sqrt{3}\\mapsto -\\sqrt{3}\n\\end{array}$$\ncommutes for both $\\pm=+$ and $\\pm=-$.", "output": "No, this is not possible.  For instance, let $K$ be any field with a automorphism $f:K\\to K$ whose order is finite and greater than $2$.  Then $A(f):A(K)\\to A(K)$ would be an automorphism of the same order extending $f$.  But no such automorphism exists: by the Artin-Schreier theorem, any finite-order automorphism of an algebraically closed field has order at most $2$.\nOr without using any big theorems, you can find problems just looking at finite extensions.  For instance, if $f$ is the Frobenius automorphism of $\\mathbb{F}_{p^2}$ then $F(f)$ is an extension to an algebraic closure which still has order $2$.  Since $\\mathbb{F}_{p^4}$ is normal over $\\mathbb{F}_{p}$, $F(f)$ restricts to an automorphism of $\\mathbb{F}_{p^4}$, which must be the Frobenius squared in order to have order $2$.  But the Frobenius squared does not restrict to $f$ on $\\mathbb{F}_{p^2}$, so this is a contradiction.", "meta": {"post_id": 3300131, "input_score": 26, "output_score": 40, "post_title": "Can \"Taking algebraic closure\" be made into a functor?"}}
{"input": "I was doing some software engineering and wanted to have a thread do something in the background to basically just waste CPU time for a certain test.\nWhile I could have done something really boring like for(i < 10000000) { j = 2 * i }, I ended up having the program start with $1$, and then for a million steps choose a random real number $r$ in the interval $[0,R]$ (uniformly distributed) and multiply the result by $r$ at each step.\n\nWhen $R = 2$, it converged to $0$.\nWhen $R = 3$, it exploded to infinity. \n\nSo of course, the question anyone with a modicum of curiosity would ask: for what $R$ do we have the transition. And then, I tried the first number between $2$ and $3$ that we would all think of, Euler's number $e$, and sure enough, this conjecture was right. Would love to see a proof of this.  \nNow when I should be working, I'm instead wondering about the behavior of this script. \nIronically, rather than wasting my CPUs time, I'm wasting my own time. But it's a beautiful phenomenon. I don't regret it. $\\ddot\\smile$", "output": "EDIT: I saw that you solved it yourself. Congrats! I'm posting this anyway because I was most of the way through typing it when your answer hit. \nInfinite products are hard, in general; infinite sums are better, because we have lots of tools at our disposal for handling them. Fortunately, we can always turn a product into a sum via a logarithm.\nLet $X_i \\sim \\operatorname{Uniform}(0, r)$, and let $Y_n = \\prod_{i=1}^{n} X_i$. Note that $\\log(Y_n) = \\sum_{i=1}^n \\log(X_i)$. The eventual emergence of $e$ as important is already somewhat clear, even though we haven't really done anything yet.\nThe more useful formulation here is that $\\frac{\\log(Y_n)}{n} = \\frac 1 n \\sum \\log(X_i)$, because we know from the Strong Law of Large Numbers that the right side converges almost surely to $\\mathbb E[\\log(X_i)]$. We have\n$$\\mathbb E \\log(X_i) = \\int_0^r \\log(x) \\cdot \\frac 1 r \\, \\textrm d x = \\frac 1 r [x \\log(x) - x] \\bigg|_0^r = \\log(r) - 1.$$\nIf $r < e$, then $\\log(Y_n) / n \\to c < 0$, which implies that $\\log(Y_n) \\to -\\infty$, hence $Y_n \\to 0$. Similarly, if $r > e$, then $\\log(Y_n) / n \\to c > 0$, whence $Y_n \\to \\infty$. The fun case is: what happens when $r = e$?", "meta": {"post_id": 3355435, "input_score": 73, "output_score": 49, "post_title": "Numerical phenomenon. Who can explain?"}}
{"input": "From Halmos's Naive Set Theory, section 1:\n\nObserve, along the same lines, that inclusion is transitive, whereas belonging is not. Everyday examples, involving, for instance, super-organizations whose members are organizations, will readily occur to the interested reader.\n\nBelonging seems transitive. Can someone explain?", "output": "The difference between $\\subset$ and $\\in$ is that the former applies to expressions at the same level of nesting and the latter applies to expressions at one level of nesting apart from each other. So when you chain two $\\in$'s together you get something at two levels of nesting, which is not in general comparable to a single $\\in$. On the other hand, since $\\subset$ doesn't change the level of nesting it doesn't have this problem.\nThis is the idea behind the example given in other answers of\n$$\n\\varnothing\\in \\{\\varnothing\\}\\in \\{\\{\\varnothing\\}\\},\\qquad \\varnothing \\not\\in \\{\\{\\varnothing\\}\\}.\n$$", "meta": {"post_id": 3355751, "input_score": 23, "output_score": 47, "post_title": "Why is belonging not transitive?"}}
{"input": "A goat is tied to the corner of a shed 12 feet long and 10 feet wide. If the rope is 15 feet long, over how many square feet can the goat graze ?\n\nI know that this question has already been asked a number of time, but no matter what I do I cannot find the same answer as the one provided in the textbook. I proceed like in this thread so I have :\n$\\frac{3}{4}15\u00b2\\pi + \\frac{1}{4}3\u00b2\\pi + \\frac{1}{4}5\u00b2\\pi = \\frac{1}{4}709\\pi$\nHowever the answer given by the textbook is $177\\frac{1}{4}\\pi$.\nAm I missing something here or is the textbook wrong ?", "output": "In business and the trades, at least before everything went to decimal notation for fractions, you would almost never see someone write a number as (for example) $\\frac 52.$ Instead they would write $2\\frac12,$ which by convention was read as a single number equal to $2+\\frac12.$\nThis notation is called a mixed fraction. It is highly discouraged in most mathematical settings, but you can still see it used sometimes, especially in old puzzle books. \n\nWhile I was trying not to be U.S.-centric in this answer, I should acknowledge that mixed fractions are still extremely common in the U.S. for many kinds of measurements, and as noted in the comments are seen in some contexts in at least a few other countries.", "meta": {"post_id": 3374247, "input_score": 16, "output_score": 39, "post_title": "A goat is tied to the corner of a shed"}}
{"input": "From Linear map, the sixth example:\nThe translation $x \\rightarrow x+1$ is not a linear transformation. Why?\nWhat about $x \\rightarrow x +dx$? Is this translation a linear transformation?\nDoes it matter if the transformation is not linear?", "output": "OP's transformations are affine transformations. Whether they are called linear transformations depends on context and conventions. \n\nWithin the context of linear algebra, a linear transformation maps the zero vector into the zero vector. Then OP's transformations are generically not linear.\nIn other contexts/conventions, linear & affine transformations are the same thing.", "meta": {"post_id": 3453270, "input_score": 19, "output_score": 42, "post_title": "Why is this translation not a linear transformation?"}}
{"input": "This is exercise 1.3.14 in page 80 of Hatcher's book Algebraic topology.\nIt's equivalent to consider subgroups of $\\pi_1(X_1\\vee X_2)=\\mathbb Z_2 * \\mathbb Z_2 =\\langle a \\rangle *\\langle b \\rangle$.\nTo move this question out of the unanswered list, I put my solution in answer.", "output": "In the following pictures, green dots means basepoints, black curve means its two endpoints are attached. Covering map maps blue $S^2$ to $X_1$ and red $S^2$ to $X_2$. \nLet $X_1$ and $X_2$ denote the first and second copy of $\\mathbb RP^2$.\n$\\pi_1(X_1)=\\mathbb Z_2=\\langle a \\rangle,\\ \\pi_1(X_2)=\\mathbb Z_2=\\langle b \\rangle$.\n\n$1$. For trivial subgroup $1$, it corresponds to the the universal cover, i.e. the infinite chain of $S^2$.\n$2$. For subgroup isomorphic to infinite cyclic group $\\mathbb Z$, it is generated by $(ab)^n$ or $(ba)^n$ of index $2n$ $(n \\geqslant 1)$ and it corresponds to a \"necklace\" of $2n$ copies of $S^2$. \n\n$3$. For subgroup isomorphic to $\\mathbb Z_2$, it's generated by $(ab)^{m}\\cdot a$ or $(ba)^{m}\\cdot b$ $(k\\geqslant 0)$ and it corresponds to $\\mathbb RP^2$ attached to an infinite chain of $S^2$.\n\n$4$. For subgroup isomorphic to the infinite dihedral group $\\mathbb Z_2 * \\mathbb Z_2$, it's generated by $(ab)^n$ and $(ab)^m \\cdot a$ $(m\\leqslant n)$ and it corresponds to a finite chain of $S^2$'s with both ends attached an $\\mathbb RP^2$.", "meta": {"post_id": 3472796, "input_score": 27, "output_score": 44, "post_title": "Find all connected covering space of $\\mathbb RP^2\\vee \\mathbb RP^2$"}}
{"input": "Is the number $$(11!)!+11!+1$$ a prime number ?\n\nI do not expect that a probable-prime-test is feasible, but if someone actually wants to let it run, this would of course be very nice. The main hope is to find a factor to show that the number is not prime. If we do not find a factor, it will be difficult to check the number for primality. I highly expect a probable-prime-test to reveal that the number is composite.  \"Composite\" would be a surely correct result. Only if the result would be \"probable prime\", there would remain slight doubts, but I would be confident with such a test anyway.\nMotivation : $(n!)!+n!+1$ can only be prime if $\\ n!+1\\ $ is prime. This is because a non-trivial factor of $\\ n!+1\\ $ would also divide $\\ (n!)!+n!+1\\ $. The cases $\\ n=2,3\\ $ are easy , but the case $\\ n=11\\ $ is the first non-trivial case. We only know that there is no factor upto $\\ p=11!+1\\ $\nWhat I want to know : Can we calculate $$(11!)!\\mod \\ p$$ for $\\ p\\ $ having $\\ 8-12\\ $ digits with a trick ? I ask because pari/gp takes relatively long to calculate this residue directly. So, I am looking for an acceleration of this trial division.", "output": "I let $p_1=1+11!$ for convenience. By Wilson's theorem if there's a prime $p$ that divides $1+11!+(11!)! = p_1 + (p_1-1)!$ then\n$$(p-1)!\\equiv -1\\pmod p$$\nAnd also\n$$(p_1-1)!\\equiv -p_1$$\nSo\n$$(p-1)(p-2)...p_1\\cdot(p_1-1)!\\equiv -1$$\n$$(p-1)(p-2)...p_1\\cdot p_1\\equiv 1$$\nThis way I was able to check all the primes from $p_1$ to 74000000 in 12 hours. This gives a 3.4% chance of finding a factor according to big prime country's heuristic. The algorithm has bad asymptotic complexity because to check a prime $p$ you need to perform $p-11!$ modular multiplications so there's not much hope of completing the calculation.\nNote that I haven't used that $p_1$ is prime, so maybe that can still help somehow. Here's the algorithm in c++:\n// compile with g++ main.cpp -o main -lpthread -O3\n\n#include <iostream>\n#include <vector>\n#include <string>\n\n#include <boost/process.hpp>\n\n#include <thread>\n\nnamespace bp = boost::process;\n\nconst constexpr unsigned int p1 = 1 * 2 * 3 * 4 * 5 * 6 * 7 * 8 * 9 * 10 * 11 + 1; // 11!+1\nconst constexpr unsigned int max = 100'000'000;                                    // maximum to trial divide\nstd::vector<unsigned int> primes;\nunsigned int progress = 40;\n\nvoid trial_division(unsigned int n) { // check the primes congruent to 2n+1 mod 16\n    for(auto p : primes) {\n        if(p % 16 != (2 * n + 1)) continue;\n        uint64_t prod = 1;\n        for(uint64_t i = p - 1; i >= p1; --i) {\n            prod = (prod * i) % p;\n        }\n        if((prod * p1) % p == 1) {\n            std::cout << p << \"\\n\"; \n        }\n        if(n == 0 && p > progress * 1'000'000) {\n            std::cout << progress * 1'000'000 << \"\\n\";\n            ++progress;\n        }\n    }\n}\n\nint main() {\n    bp::ipstream is;\n    bp::child primegen(\"./primes\", std::to_string(p1), std::to_string(max), bp::std_out > is);\n    // this is https://cr.yp.to/primegen.html\n    // the size of these primes don't really justify using such a specialized tool, I'm just lazy\n\n    std::string line;   \n    while (primegen.running() && std::getline(is, line) && !line.empty()) {\n        primes.push_back(std::stoi(line));\n    } // building the primes vector\n\n    // start 8 threads, one for each core for on my computer, each checking one residue class mod 16\n    // By Dirichlet's theorem on arithmetic progressions they should progress at the same speed\n    // the 16n+1 thread owns the progress counter\n    std::thread t0(trial_division, 0);\n    std::thread t1(trial_division, 1);\n    std::thread t2(trial_division, 2);\n    std::thread t3(trial_division, 3);\n    std::thread t4(trial_division, 4);\n    std::thread t5(trial_division, 5);\n    std::thread t6(trial_division, 6);\n    std::thread t7(trial_division, 7);\n\n    t0.join();\n    t1.join();\n    t2.join();\n    t3.join();\n    t4.join();\n    t5.join();\n    t6.join();\n    t7.join();\n}\n\nI only need to multiply integers of the order of $11!$ so standard 64 bit ints suffice.\nEDIT: Divisor found! $1590429889$\nSo first of all, the Wilson's theorem trick slows down instead of speeding up after $2p_1$. Secondly, the trial division function is nearly infinitely parallelizable, which means that it's prone to being computed with a GPU. My friend wrote an implementation that can be found here. This can be run on CUDA compatible nvidia GPUs. Finding the factor took about 18 hours on a Nvidia GTX Titan X pascal.", "meta": {"post_id": 3481048, "input_score": 58, "output_score": 41, "post_title": "Can I search for factors of $\\ (11!)!+11!+1\\ $ efficiently?"}}
{"input": "Given some function involving two variables, $\\mathit p(x,y)$, is the formula $$\\mathit \\exists!x\\exists yp(x,y)$$ equivalent to $$\\mathit\\exists y\\exists!xp(x,y)$$\nI have tried writing out the formal definition for the unique existential quantifier, but I get stuck simplifying the statement when the unique quantifier is first. I have also tried giving $\\mathit p(x,y)$ some meaning and talking through it, but I can't decide if they mean the same thing. Any help is appreciated!", "output": "No, they do not commute.\nConsider for example the nonnegative reals as a linear order. Then $$\\exists x\\exists !y(y\\le x)$$ is true (take $x=0$), but $$\\exists!y \\exists x(y\\le x)$$ is false since for every $y$ there is some $x$ with $y\\le x$.\n\nThis becomes less surprising when we \"unpack\" $\\exists!$. There are a few ways to do this, but I think the simplest is to view \"$\\exists !x\\varphi(x,...)$\" as an abbreviation for \"$\\exists x\\forall z(\\varphi(z,...)\\leftrightarrow x=z)$.\" Then we have:\n\n\"$\\exists x\\exists!y\\varphi(x,y)$\" is an abbreviation for $$\\exists x\\exists y\\forall z(\\varphi(x,z)\\leftrightarrow y=z),$$\nwhile \"$\\exists!y\\exists x\\varphi(x,y)$\" is an abbreviation for $$\\exists y\\forall z[(\\exists x\\varphi(x,z))\\leftrightarrow y=z].$$\n\nThis has a clear instance of swapping $\\forall$ and $\\exists$ (in fact, it's even messier than that - unpack \"$\\leftrightarrow$\" ...), so we should expect them to be inequivalent in general.\n(There are of course other ways to unpack \"$\\exists!$,\" but they'll all yield the same general picture: despite the symbol suggesting only existentiality, it's hiding a very important universal quantifier, and so the usual dangers of $\\forall/\\exists$-switching carry over to $\\exists!/\\exists$-switching.)", "meta": {"post_id": 3528948, "input_score": 24, "output_score": 40, "post_title": "Does the unique existential quantifier commute with the existential quantifier?"}}
{"input": "I was sitting in analysis yesterday and, naturally, we took the limit of some expression. It occurred to me that \"taking the limit\" of some expression abides the rules of a linear transformation\n$$\\lim_{x \\rightarrow k}\\ c(f(x)+g(x)) = c \\lim_{x \\rightarrow k} f(x) + c\\ \\lim_{x \\rightarrow k} g(x),$$\nand (my group theory is virtually non existent) appears also to be a homomorphism:\n$$\\lim_{x \\rightarrow k} (fg)(x) = \\lim_{x \\rightarrow k} f(x)g(x), $$\netc.\nAnyway, my real question is, what mathematical construct is the limit?", "output": "In general, let $X, Y$ be topological spaces, and $x_0$ a non-isolated point of $X$.  Then strictly speaking, \"$\\lim_{x\\to x_0} f(x) = L$\" is a relation between functions $f : X \\to Y$ and points $L \\in Y$ (the equality notation being misleading in general).\nNow, if $Y$ is a Hausdorff topological space, it happens that this relation is what is known as a partial function: for any $f : X \\to Y$, there is at most one $L \\in Y$ such that $\\lim_{x\\to x_0} f(x) = L$.  Now, for any relation $R \\subseteq (X \\to Y) \\times Y$ which is a partial function, we can define a corresponding function $\\{ f \\in (X \\to Y) \\mid \\exists y \\in Y, (f, y) \\in R \\} \\to Y$ by sending $f$ satisfying this condition to the unique $y$ with $(f, y) \\in R$.  Then that somewhat justifies the \"equality\" in the notation $\\lim_{x\\to x_0} f(x) = L$, though you still need to keep in mind that it is a partial function where $\\lim_{x\\to x_0} f(x)$ is not defined for all $f$.  (This part relates to the answer by Jos\u00e9 Carlos Santos.)\nBuilding on top of this, in the special case of $Y = \\mathbb{R}$, we can put a ring structure on $X \\to Y$ by pointwise addition, pointwise multiplication, etc.  Then $\\{ f : X \\to \\mathbb{R} \\mid \\exists L \\in \\mathbb{R}, \\lim_{x\\to x_0} f(x) = L \\}$ turns out to be a subring of $X \\to \\mathbb{R}$, and the induced function from this subring to $\\mathbb{R}$ is a ring homomorphism.  (More generally, this will work if $Y$ is a topological ring.  Similarly, if $Y$ is a topological vector space, then the set of $f$ with a limit at $x_0$ is a linear subspace of $X \\to Y$ and the limit gives a linear transformation; if $Y$ is a topological group, you get a subgroup of $X \\to Y$ and a group homomorphism; and so on.)", "meta": {"post_id": 3530539, "input_score": 48, "output_score": 55, "post_title": "Is \"taking a limit\" a function? Is it a procedure? A ternary operation?"}}
{"input": "Description\nIn the game of Yahtzee, 5 dice are rolled to determine a score. One of the resulting rolls is called a Yahtzee.\nTo roll a Yahtzee you must have 5 of a kind. (5 1's or 5 2's or 5 3's etc..).\nIn the game of Yahtzee you can only have 5 dice. However, for the purpose of this question I want to entertain adding more dice to the equation. Therefore I'd like to define a Yahtzee as follows:\nTo roll a Yahtzee you must have exactly 5 of a kind, no more or no less. (5 1's or 5 2's or 5 3's etc..).\nExamples\nLet's look at some rolls with 6 dice\nThe following would be a Yahtzee:\n\n1 1 1 1 1 4\n6 3 3 3 3 3\n5 5 3 5 5 5\n\nThe following would not be a Yahtzee:\n\n1 1 1 3 3 3\n1 1 1 1 5 3\n1 1 1 1 1 1\n\n- Note that the last roll does technically contain 5 1's, however because the roll as an entirety contains 6 1's this is not a Yahtzee.\n\nLet's look at some rolls with 12 dice\nThe following would be a Yahtzee:\n\n1 1 2 1 2 1 4 4 1 3 6 2\n1 1 1 1 1 2 2 2 2 2 3 3\n1 1 1 1 1 2 2 2 2 2 2 2\n\n- Note that the first roll is a Yahtzee with 5 1's, this roll is to illustrate that order doesn't matter.\n- Note that the second roll has 2 Yahtzees, this is a roll that counts as a Yahtzee\n- Note that the third roll has a Yahtzee with 1's but has 7 2's. This roll is a Yahtzee because it contains exactly 5 1's. The 7 2's do not nullify this roll.\nThe following would not be a Yahtzee:\n\n1 1 1 2 2 2 3 3 3 4 4 4\n1 1 1 1 1 1 6 6 6 6 6 6\n\n- Note that the last roll has 6 1's and 6 6's. Because exactly 5 of one number (no more, no less) is not present, this roll does not contain a Yahtzee.\nThe Question\nWhat is the optimal number of dice to roll a Yahtzee in one roll?\nA more generalized form of the question is as follows: Given $n$ dice, what is the probability of rolling a Yahtzee of length $y$ in one roll.", "output": "By inclusion-exclusion, the full probability of Yahtzee is:\n$$\\frac{1}{6^n}\\sum_{k=1}^{\\min(6,n/5)} (-1)^{k+1} \\binom{6}{k} (6-k)^{n-5k} \\prod_{j=0}^{k-1} \\binom{n-5j}{5}.$$\nIf you prefer, write the product with a multinomial:\n$$\\prod_{j=0}^{k-1} \\binom{n-5j}{5}=\\binom{n}{5k}\\binom{5k}{5,\\dots,5}.$$\nLooks like $n=29$ is the uniquely optimal number of dice:\n\\begin{matrix}\nn &p\\\\\n\\hline\n28 &0.71591452705020 \\\\\n29 &0.71810623718825 \\\\\n30 &0.71770441391497 \\\\\n\\end{matrix}\n\nHere is the SAS code I used:\nproc optmodel;\n   set NSET = 1..100;\n   num p {n in NSET} = \n      (1/6^n) * sum {k in 1..min(6,n/5)} (-1)^(k+1) \n      * comb(6,k) * (if k = 6 and n = 5*k then 1 else (6-k)^(n-5*k)) \n      * prod {j in 0..k-1} comb(n-5*j,5);\n   print p best20.;\n   create data outdata from [n] p;\nquit;\n\nproc sgplot data=outdata;\n   scatter x=n y=p;\n   refline 29 / axis=x;\n   xaxis values=(0 20 29 40 60 80 100);\nrun;", "meta": {"post_id": 3540068, "input_score": 46, "output_score": 56, "post_title": "What is the optimal number of dice to roll a Yahtzee in one roll?"}}
{"input": "Evaluate $$\\int_{-a\\pi}^{a\\pi} \\frac{\\cos^5(x)+1}{e^x+1}dx, \\quad a \\in \\mathbb{N}$$\n\nIn the beginning, I didn't have any ideas of how to solve this. The one that later came to mind was to try using Feynman's technique, but I couldn't think of the proper function to use for the second variable. \nAny ideas?", "output": "Let $I$ be our integral. Substitute $t=-x \\Rightarrow dt = -dx$. Then:\n$$I=\\int_{-a\\pi}^{a\\pi} \\frac{\\cos^5 t+1}{e^{-t}+1}\\,dt=\\int_{-a\\pi}^{a\\pi} \\frac{e^t(\\cos^5 t+1)}{e^{t}+1}\\,dt=\\int_{-a\\pi}^{a\\pi} \\frac{e^x(\\cos^5 x+1)}{e^{x}+1}\\,dx$$\nTherefore:\n$$2I=\\int_{-a\\pi}^{a\\pi} \\frac{e^x(\\cos^5 x+1)}{e^{x}+1}\\,dx+\\int_{-a\\pi}^{a\\pi} \\frac{\\cos^5 x+1}{e^{x}+1}\\,dx=\\int_{-a\\pi}^{a\\pi} (\\cos^5 x+1)\\,dx$$\n$$=\\bigg[x + \\frac{5}{8} \\sin(x) + \\frac{5}{48} \\sin 3 x + \\frac{1}{80} \\sin 5 x\\bigg]_{-a\\pi}^{a\\pi}=2a\\pi$$\nThus $I=a\\pi$.", "meta": {"post_id": 3566831, "input_score": 14, "output_score": 42, "post_title": "Problem from the 2020 Latvian \"Sophomore's Dream\" competition"}}
{"input": "I am looking for two convex polygons $P,Q \\subset \\Bbb R^2$ such that\n$P$ does not tile the plane, $Q$ does not tile the plane, but if we allowed to use $P,Q$ together, then we can tile the plane.\nHere I do not require the tilings to be lattice tilings, or even periodic tilings. I allow tilings by congruent copies of $P$ and/or of $Q$, i.e. I am allowing rotations and reflections!\nI haven't found any example, and maybe there could be none.", "output": "There is a tiling of the plane made from regular heptagons and irregular pentagons.\nWe know that regular heptagons cannot tile the plane.\nThe irregular pentagon has four equal sides and one shorter side. A tiling of the plane by these pentagons would require two pentagons to share the short side (as they do in the image), but the resulting angle cannot then be tiled by other pentagons, so this irregular pentagon does not tile the plane.\nImage via: https://twitter.com/gregegansf/status/1003181379469758464\nI think the reference is to this paper: https://erikdemaine.org/papers/Sliceform_Symmetry/paper.pdf", "meta": {"post_id": 3611969, "input_score": 21, "output_score": 35, "post_title": "Convex polygons that do not tile the plane individually, but together they do"}}
{"input": "Antoine's necklace is an embedding of the Cantor set in $\\mathbb{R}^3$ constructed by taking a torus, replacing it with a necklace of smaller interlinked tori lying inside it, replacing each smaller torus with a necklace of interlinked tori lying inside it, and continuing the process ad infinitum; Antoine's necklace is the intersection of all iterations.\n\nA number of sources claim that this necklace 'cannot fall apart' (e.g. here). Given that the necklace is totally disconnected this obviously has to be taken somewhat loosely, but I tried to figure out exactly what is meant by this. Most sources seem to point to this paper (which it must be noted contains some truly remarkable images, e.g. Figure 12). There the authors make the same point that Antoine's necklace 'cannot fall apart'. Nevertheless, all they seem to show in the paper is that it cannot be separated by a sphere (every sphere with a point of the necklace inside it and a point of the necklace outside it has a point of the necklace on it).\nIt seems to me to be a reasonably trivial exercise to construct a geometrical object in $\\mathbb{R}^3$ which cannot be separated by a sphere, and yet can still 'fall apart'.\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \nIn the spirit of the construction of Antoine's necklace, these two interlinked tori cannot be separated by a sphere (any sphere containing a point of one torus inside it will contain a point of that torus on its surface), but this seems to have no relation to the fact that they cannot fall apart - if we remove a segment of one of the tori the object still cannot be separated by a sphere, and yet can fall apart macroscopically.\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \nThe fact mentioned here that the complement of the necklace is not simply connected, and the fact mentioned here that there are loops that cannot be unlinked from the necklace shouldn't impact whether it can be pulled apart either, as both are true of our broken rings\nMy question is this: Is it possible to let me know either:\n\nHow I have misunderstood separation by a sphere (so that it may still be relevant to an object being able to fall apart),\nWhat property Antoine's necklace does satisfy so that it cannot fall apart (if I have missed this), or\nWhat is actually meant when it is said to be unable to fall apart (if I have misunderstood this)", "output": "if we remove a segment of one of the tori the object still cannot be separated by a sphere, and yet can fall apart macroscopically.\n\nWell, sure it can. The key observation is that \"sphere\" means \"homeomorphic image of a sphere\" in this context.\nTurn the ring with the part removed (the \"C\") and pull the in tact ring  (the \"O\") through the missing piece. Then you have two separate components, the C and the O, which it is easy to see could be placed inside and outside of a sphere. Let's put the C on the inside and the O on the outside.\nNow, think about shrinking that sphere very tightly (but not touching) around the C, then putting the C back where it was. You've separated the two by the homeomorphic image of a sphere.\nThink about if we tried to do this with two Os, like your first figure. If the Os weren't interlocked, sure, it's easy, just like when the C and the O were separated. But if two Os are interlocked, there isn't any way to fit a sphere around one of them in the same way as you could with the C. And if you can't do it with only two Os, you certainly can't do it with Antoine's Necklace. Thus, it will stay together.", "meta": {"post_id": 3636009, "input_score": 53, "output_score": 55, "post_title": "Why can't Antoine's necklace fall apart?"}}
{"input": "Let \n$$f(n) = \\max\\{\\text{length of shortest proof of }\\varphi \\mid \\varphi \\text{ is a provable ZFC sentence of length } \\leq n\\}$$\nHow fast does $f$ grow? Is it polynomial, exponential, more than exponential, etc.?", "output": "This function grows really fast: there is no computable function which bounds it!\nTo see this, note that if we had a computable bound on $f$ we could tell whether a sentence $\\sigma$ is consistent with $\\mathsf{ZFC}$ (just search over all proofs of length $<f(\\vert\\sigma\\vert+1)$ for a $\\mathsf{ZFC}$-proof of $\\neg\\sigma$). But from this information we could in turn build a computable complete consistent extension of $\\mathsf{ZFC}$: \n\nFix an appropriate enumeration $(\\sigma_i)_{i\\in\\mathbb{N}}$ of the sentences in the language of set theory.\nDefine a new sequence $(\\tau_i)_{i\\in\\mathbb{N}}$ of sentences by recursion as follows: \n\n$\\tau_0=\\sigma_0$ if $\\sigma_0$ is consistent with $\\mathsf{ZFC}$, and $\\tau_0=\\neg\\sigma_0$ otherwise.\n$\\tau_{i+1}=\\sigma_{i+1}$ if $\\sigma_{i+1}\\wedge\\bigwedge_{j\\le i}\\tau_i$ is consistent with $\\mathsf{ZFC}$, and $\\tau_{i+1}=\\neg\\sigma_{i+1}$ otherwise.\n\nThe set $\\{\\tau_i:i\\in\\mathbb{N}\\}$ is then a complete computable consistent theory containing $\\mathsf{ZFC}$ (note that when $\\sigma_i$ is an axiom of $\\mathsf{ZFC}$ we'll have $\\tau_i=\\sigma_i$).\n\nHowever, this contradicts the first incompleteness theorem. (Or Church's theorem, if you like - basically the above is the proof of Church's theorem from the first incompleteness theorem.)\n\nNote that we really used very little about $\\mathsf{ZFC}$ here. The first incompleteness theorem applies to a huge range of theories, ranging from much weaker than $\\mathsf{ZFC}$ to much stronger than $\\mathsf{ZFC}$; briefly, any consistent computably axiomatizable theory which satisfies a very mild technical \"strength condition\" (basically: at least as powerful as Robinson's $Q$) is subject to this phenomenon. See section $4$ of this paper of Beklemishev for more details on this point.\nTo be precise, the form of the first incompleteness theorem I'm using is: \"Every computably axiomatizable consistent theory which interprets Robinson's $\\mathsf{Q}$ is incomplete.\" Note that we don't need an $\\omega$-consistency assumption here; while present in Godel's original proof, it was later removed by Rosser.", "meta": {"post_id": 3671456, "input_score": 37, "output_score": 40, "post_title": "How long can proofs be?"}}
{"input": "I know there is a lot of topic regarding this on the internet, and trust me, I've googled it. But things are getting more and more confused for me.\nFrom my understanding, The gradient is the slope of the most rapid descent. Modifying your position by descending along this gradient will most rapidly cause your cost function to become minimal (the typical goal).\nCould anyone explain in simple words (and maybe with an example) what the difference between the Jacobian, Hessian, and the Gradient?", "output": "Some good resources on this would be any introductory vector calculus text.   I'll try to be as consistent as I can be with Stewart's Calculus, perhaps the most popular calculus textbook in North America.\nThe Gradient\nLet $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a scalar field.  The gradient, $\\nabla f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$ is a vector, such that $(\\nabla f)_j = \\partial f/ \\partial x_j$.  Because  every point in $\\text{dom}(f)$ is mapped to a vector, then $\\nabla f$ is a vector field.\nThe Jacobian\nLet $\\operatorname{F}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ be a vector field.  The Jacobian can be considered as the derivative of a vector field.  Considering each component of $\\mbox{F}$ as a single function (like $f$ above), then the Jacobian is a matrix in which the $i^{th}$ row is the gradient of the $i^{th}$ component of $\\operatorname{F}$.  If $\\mathbf{J}$ is the Jacobian, then\n$$\\mathbf{J}_{i,j} = \\dfrac{\\partial \\operatorname{F}_i}{\\partial x_j}$$\nThe Hessian\nSimply, the Hessian is the matrix of second order mixed partials of a scalar field.\n$$\\mathbf{H}_{i, j}=\\frac{\\partial^{2} f}{\\partial x_{i} \\partial x_{j}}$$\nIn summation:\n\nGradient: Vector of first order derivatives of a scalar field\nJacobian: Matrix of gradients for components of a vector field\nHessian: Matrix of second order mixed partials of a scalar field.\n\nExample\nSquared error loss $f(\\beta_0, \\beta_1) = \\sum_i (y_i - \\beta_0 - \\beta_1x_i)^2$ is a scalar field.  We map every pair of coefficients to a loss value.\n\nThe gradient of this scalar field is $$\\nabla f = \\left< -2 \\sum_i( y_i - \\beta_0 - \\beta_1x_i), -2\\sum_i x_i(y_i - \\beta_0 - \\beta_1x_i) \\right>$$\nNow, each component of $\\nabla f$ is itself a scalar field.  Take gradients of those and set them to be rows of a matrix and you've got yourself the Jacobian\n\n$$ \\left[\\begin{array}{cc}\n\\sum_{i=1}^{n} 2 & \\sum_{i=1}^{n} 2 x_{i} \\\\\n\\sum_{i=1}^{n} 2 x_{i} & \\sum_{i=1}^{n} 2 x_{i}^{2}\n\\end{array}\\right]$$\n\nThe Hessian of $f$ is the same as the Jacobian of $\\nabla f$.  It would behoove you to prove this to yourself.\n\nResources: Calculus: Early Transcendentals by James Stewart, or earlier editions, as well as Wikipedia which is surprisingly good for these topics.", "meta": {"post_id": 3680708, "input_score": 67, "output_score": 77, "post_title": "What is the difference between the Jacobian, Hessian and the Gradient?"}}
{"input": "The diagram shows 12 small circles of radius 1 and a large circle, inside a square.\nEach side of the square is a tangent to the large circle and four of the small circles.\nEach small circle touches two other circles.\nWhat is the length of each side of the square?\nThe answer is 18\nCONTEXT: \nThis question came up in a Team Maths Challenge I did back in November. No one on our team knew how to do it and we ended up guessing the answer (please understand that time was scarce and we did several other questions without guessing!) I just remembered this question and thought I'd have a go but I am still struggling with it. \nThere are no worked solutions online (only the answer) so I reaching out to this website as a final resort. Any help would be greatly appreciated. Thank you!", "output": "It is instructive to consider the general case.  Suppose we have a circle of radius $r$ that is inscribed in a square of side length $2r$.  Suppose $n$ tangent circles of unit radius can be drawn along the inside \"corner\" of the square.  What is the relationship between $r$ and $n$?  Your question is the case $n = 2$, the third circle drawn in the corner being redundant.  The figure below illustrates the case $n = 5$:\n\nThe solution is straightforward.  The right triangle shown in the diagram has legs $r-1$ and $r-(2n-1)$, and hypotenuse $r+1$.  Therefore, $$(r-1)^2 + (r-2n+1)^2 = (r+1)^2,$$ from which it follows that $$r = (1 + \\sqrt{2n})^2.$$  For $n = 2$, this gives $r = 9$ and the side length of the square is $18$.  For $n = 5$, we have $r = 11 + 2 \\sqrt{10}$.  Whenever $n$ is twice a square, i.e. $n = 2m^2$ for a positive integer $m$, then $r = (1 + 2m)^2$ is also an integer and the circumscribing square has integer sides.\n\nAs a related but different question, given $n$ such circles, what is the total number of externally tangent unit circles that can be placed in the corner such that their centers form a square lattice and do not intersect the large circle?  So for $n = 2$, this number is $f(n) = 3$ as shown in your figure.  For $n = 5$, it is $f(5) = 12$.", "meta": {"post_id": 3691332, "input_score": 61, "output_score": 44, "post_title": "What is the size of each side of the square?"}}
{"input": "Compared to the categories of other \u201ccommon\u201d algebraic objects like groups and rings, it seems that fields as a whole are missing some important properties:\n\nThere are no initial or terminal objects\nThere are no free fields\nNo products or coproducts\nEvery arrow is a mono (maybe not a bad thing, but still indicates how restrictive the category is)\n\nA logician once told me in passing that part of the reason is that the properties for fields contain a decidedly \u201cweird\u201d property, namely that every element in a field except zero has a multiplicative inverse. If I understood him correctly, this property is sufficiently different from the others that the category of all such objects loses some features. But I have no idea if this was a heuristic or a proven theorem.", "output": "There is a precise sense in which the concept of field is not algebraic like, say, the concept of ring or group or vector space etc.: it is a theorem that any kind of mathematical structure that is defined as having a set of elements and some fixed list of total operations of constant finite arity obeying some fixed list of unconditional equations gives rise to a category with certain nice properties (which I omit for the moment). The usual definition of field has a partially defined operation \u2013 inversion \u2013 as well as an inequality ($0 \\ne 1$), which means the theorem is not applicable; the fact that the category of fields does not have the nice properties of algebraic categories tells us there is actually no way of defining fields so that the theorem applies.\nSo what does being algebraic buy us, and how do we recognise an algebraic category without thinking about the logical form of the definition? Well, a category is equivalent to a category of algebraic structures if and only if it has all of the following properties:\n\nIt has limits for all small diagrams and colimits for small filtered diagrams.\nThere is an object $A$ such that the functor $\\mathrm{Hom} (A, -)$ has a left adjoint, is monadic, and preserves colimits for small filtered diagrams.\n\nIn fact, it follows that such a category has colimits for small diagrams in general, but this fact is not needed in the theorem. Note that the object $A$ is not unique up to isomorphism; this is essentially the phenomenon of Morita equivalence.", "meta": {"post_id": 3756136, "input_score": 47, "output_score": 57, "post_title": "Why is the category of fields seemingly so poorly behaved?"}}
{"input": "I always get a little annoyed when engineers or math textbooks use the word \"antilogarithm.\" Isn't it just exponentiation? Like if $\\log(2) \\approx 0.301$, then $10^{0.301}\\approx 2$ . Why say \"antilogarithm?\" Is there some subtly different meaning? Am I missing something?", "output": "$\\DeclareMathOperator\\antilog{antilog}$\nFirst, when Napier invented logarithms, his application was not the inverse of the exponential function.  He was originally multiplying quantities called \"sines\", which are not what you're thinking when you see that word.  These \"sines\" are vaguely similar to the positions of the arrow in Zeno's paradox of the arrow.  To each \"sine\" (a quantity) was associated a quantity, the logarithm of that \"sine\".  Napier arranged for his logarithms to have the property that when the \"sines\" decreased in geometric proportion, the logarithms increased in arithmetic proportion.  So he transformed multiplication of \"sines\" into addition of their logarithms.\nNow to your question: to go from a \"sine\" to its logarithm was performed by table lookup.  To go from a logarithm to its \"sine\" was performed by backwards table lookup.  So, quite literally, an antilogarithm is found by using the table backwards.  Since we are not working on usual quantities, but on \"sines\", the inverse operation is not exponentiation.\nNapier and Briggs then modified the logarithm to work with \"normal\" quantities instead of \"sines\".  At this point, the inverse of the logarithm was exponentiation.\nNote that the inverse of the logarithm couldn't be called exponentiation by Napier since he was writing in 1614 and subsequently.  In 1748 Euler wrote \"consider exponentials or powers in which the exponent itself is a variable\" (\"Primum ergo considerand\u00e6 sunt quantitates exponentiales, seu Potestates, quarum Exponens ipse est quantitas variabilis.\", from Introductio in analysin infinitorum) , which seems to be the first time the exponent was not a constant positive integer.  Until we make the generalization of exponents to arbitrary powers, there is no hope of describing the inverse logarithm as an exponential function.\nOne \"convenience\" of the antilog notation is that the following equation\n$$  \\log \\antilog x = x = \\antilog \\log x  $$\nis true both for Napier's \"sines\" and subsequent inverse exponential logarithms.  Rewriting this where the base is variable (which is not what Napier was considering)\n$$  \\log_b \\antilog_b x = x = \\antilog_b \\log_b x  \\text{,}  $$\nwhich is (as many students have shown) more parsable than\n$$  \\log_b b^x = x = b^{\\log_b x}  \\text{.}  $$", "meta": {"post_id": 3785751, "input_score": 47, "output_score": 114, "post_title": "Why is it called \"antilog\" or \"anti-logarithm\" rather than exponentiation?"}}
{"input": "Given an 8-digit decimal number $N$, output a new 8-digit number $f(N)$ whose first digit is the number of zeroes in $N$, the second the number of ones, ..., the seventh the number of sixes, and the eight the number of distinct digits of $N$.\nThe MoMath posted a puzzle that boils down to \"find the (unique) fixed point of $f$\", and the solution given was to start with an arbitrary seed number $N$ and apply $f$ until one finds the fixed point. They comment on why there's no reason a priori this would work, and admit they're not sure why this works. Here are my related questions:\n\nIs there a way to see that $f$ has a unique fixed point?\n\nIs there a way to see that applying $f$ starting from any arbitrary seed $N$, you get to the fixed point and don't get caught in a cycle when applying $f$?\n\nThey remark that no matter what seed you pick, $f$ finds its fixed point relatively quickly (say within $10$ applications of $f$). Does anyone have a reason for why one should find the fixed point so soon? I don't have a good sense for how to bound how quickly this happens.", "output": "One sort of obvious observation is that shuffling the digits of the input number $N$ doesn't affect the value of $f(N)$ at all.*\nThis alone significantly limits the number of possible values $f(N)$ can take.  While there are $10^8$ distinct non-negative decimal numbers with up to eight digits (or, equivalently, $10^8$ distinct octuples of decimal digits), the number of distinct inputs ignoring the order of the digits is only ${10+8-1 \\choose 8} = 24310$.\nAlso, on every step of the iteration, the number of values that the $k$ times iterated function $f^{(k)}(N)$ can take becomes more and more restricted.  For example, for any $0 \\le N < 10^8$:\n\nThe last digit of $f(N)$ must be at least $1$, the rest of its digits can sum to at most $8$, and it can contain at most one digit greater than $4$.  (And if it does contain a digits greater than $4$, it cannot also contain all digits from $0$ to $4$, as that would violate the sum condition!)  Also, the digits of $f(N)$ cannot all be equal.\nThus, the last digit of $f^{(2)}(N) = f(f(N))$ must be at least $2$ and at most $5$, and thus its first seven digits must include at least two zeros (and cannot be all zeros).\nThus, in addition to all of the constraints above, the first digit of $f^{(3)}(N) = f(f(f(N)))$ must be at least $2$ and at most $6$, etc.\n\nIn such a fashion, one could presumably proceed to build a chain of logical arguments eventually showing that the only possible value of $f^{(8)}(N)$ is $23110105$.\n\nInstead of doing that, though, I decided to write a simple Python script to enumerate all the possible values of $f^{(k)}(N)$ for each $k$, and in particular to print out the range of possible values of each digit.  Its output looks like this:\nstep 1: 0-8, 0-8, 0-8, 0-8, 0-8, 0-8, 0-8, 1-8 (8943 distinct values)\nstep 2: 0-7, 0-7, 0-4, 0-3, 0-2, 0-1, 0-1, 2-5 (96 distinct values)\nstep 3: 2-6, 0-4, 0-2, 0-2, 0-2, 0-1, 0-1, 3-5 (18 distinct values)\nstep 4: 2-5, 1-4, 0-2, 0-2, 0-2, 0-1, 0-1, 4-5 (9 distinct values)\nstep 5: 2-3, 1-4, 0-2, 0-2, 0-2, 0-1, 0-0, 4-5 (6 distinct values)\nstep 6: 2-3, 1-3, 0-2, 0-2, 0-2, 0-1, 0-0, 4-5 (4 distinct values)\nstep 7: 2-3, 1-3, 1-2, 1-1, 0-1, 0-1, 0-0, 5-5 (2 distinct values)\nstep 8: 2-2, 3-3, 1-1, 1-1, 0-0, 1-1, 0-0, 5-5 (1 distinct value)\n\nFrom the output above, we can see that the first two iterations are enough to reduce all $10^8$ possible inputs to just 96 different outputs, and the third iteration reduces those further down to just 18 choices: $23110105$, $24001104$, $31211005$, $32021004$, $32102004$, $33001104$, $40211004$, $41021004$, $41102004$, $41110105$, $42001104$, $42010014$, $50021003$, $50110104$, $50200013$, $51010014$, $51100004$ and $60100003$.  The remaining five iterations are then needed to reduce these 18 values down to just one.\nFor a closer look at what happens during those last five iterations, a slight modification to the Python script lets it print the path that each of these 18 values takes to reach the unique fixed point as a nice Unicode tree:\n\u250c\u25ba f(23110105) = 23110105\n\u2514\u2500\u252c\u2500 f(31211005) = 23110105\n  \u251c\u2500\u252c\u2500 f(32021004) = 31211005\n  \u2502 \u2514\u2500\u252c\u2500 f(33001104) = 32021004\n  \u2502   \u251c\u2500\u2500\u2500 f(50110104) = 33001104\n  \u2502   \u2514\u2500\u252c\u2500 f(51010014) = 33001104\n  \u2502     \u2514\u2500\u2500\u2500 f(60100003) = 51010014\n  \u2514\u2500\u252c\u2500 f(32102004) = 31211005\n    \u251c\u2500\u252c\u2500 f(24001104) = 32102004\n    \u2502 \u2514\u2500\u252c\u2500 f(41110105) = 24001104\n    \u2502   \u251c\u2500\u2500\u2500 f(50021003) = 41110105\n    \u2502   \u2514\u2500\u2500\u2500 f(50200013) = 41110105\n    \u251c\u2500\u2500\u2500 f(40211004) = 32102004\n    \u251c\u2500\u2500\u2500 f(41021004) = 32102004\n    \u251c\u2500\u2500\u2500 f(41102004) = 32102004\n    \u251c\u2500\u252c\u2500 f(42001104) = 32102004\n    \u2502 \u2514\u2500\u2500\u2500 f(51100004) = 42001104\n    \u2514\u2500\u2500\u2500 f(42010014) = 32102004\n\nIn this tree, the fixed point $23110105$ is on the first row at the top, marked by the arrow tip.  Underneath it is the value $31211005$, which is the only one of the 18 values (other than $23110105$ itself) that yields $23110105$ when $f$ is applied to it.  Below that are the values $32021004$ and $32102004$ that both yield $23110105$ when fed through $f$, and below each of those are all the inputs that yield each of them in turn, and so on.\nTo be honest, though, I'm not convinced that there's any particular insight to be gleaned from this graph.  Certainly I don't see any obvious or natural candidates for a monotone property $p$ such that $p(f(N)) \\ge p(N)$ (with the inequality being strict unless $N$ is the unique fixed point of $f$), although that of course doesn't rule out the possibility that someone smarter than me might find one.\n(Of course, given that iteration of $f$ clearly does converge, we could always artificially construct such a property $p$: for example, we could trivially let $p(N)$ be the highest $k \\le 8$ such that $N = f^{(k)}(N')$ for some $0 \\le N' < 10^8$.  But such an artificial construction would yield no useful insight whatsoever, nor would it make proving the convergence of the iteration any easier.)\n\nSo it seems that the permutation invariance mostly explains the rapid initial convergence of the iteration into a small number of possible values, and may also explain the general statistical behavior of the size of the image of $f^{(k)}$ as a function of $k$.  What it does not explain is the final convergence to just a single fixed point, as opposed to multiple fixed points or limit cycles.\nIn fact, I believe that this may be just a coincidence, and that arbitrary minor changes to the definition of $f$ may change the eventual result of the iteration.\nTo test this hypothesis, I decided to see what would happen if we instead considered the function $g(N) = f(N)-1$.  (Recall that the last digit of $f(N)$ is always at least $1$, so $f(N)$ and $g(N)$ only differ in their last digit.)\nAs it turns out, in this case the iteration converges in nine steps to a limit set of five values:\nstep 1: 0-8, 0-8, 0-8, 0-8, 0-8, 0-8, 0-8, 0-7 (8943 distinct values)\nstep 2: 0-8, 0-7, 0-4, 0-3, 0-2, 0-1, 0-1, 0-4 (92 distinct values)\nstep 3: 2-7, 0-4, 0-3, 0-3, 0-2, 0-1, 0-1, 1-4 (17 distinct values)\nstep 4: 2-6, 0-4, 0-3, 0-3, 0-2, 0-1, 0-1, 2-4 (13 distinct values)\nstep 5: 2-5, 0-4, 0-3, 0-3, 0-2, 0-1, 0-1, 2-4 (11 distinct values)\nstep 6: 2-4, 0-4, 0-3, 0-3, 0-2, 0-1, 0-0, 2-4 (9 distinct values)\nstep 7: 2-4, 0-4, 0-3, 0-3, 0-2, 0-0, 0-0, 2-4 (7 distinct values)\nstep 8: 2-4, 0-4, 0-3, 0-2, 0-2, 0-0, 0-0, 2-4 (6 distinct values)\nstep 9: 2-4, 0-4, 0-3, 0-2, 0-2, 0-0, 0-0, 2-4 (5 distinct values)\n\nThese five limit values consist of two fixed points ($23111004$ and $31220003$, the latter having no other ancestors within the range of $g^{(3)}$) and a single cycle of three values ($24002002$, $40301002$ and $41111004$), as show in the tree below (slightly hand-edited from the output of the Python code to show the cycle more clearly):\n\u250c\u25ba g(23111004) = 23111004\n\u2514\u2500\u252c\u2500 g(32111004) = 23111004\n  \u251c\u2500\u252c\u2500 g(41200103) = 32111004\n  \u2502 \u2514\u2500\u252c\u2500 g(50200102) = 41200103\n  \u2502   \u2514\u2500\u2500\u2500 g(52000002) = 50200102\n  \u2514\u2500\u2500\u2500 g(42100013) = 32111004\n\n\u250c\u2500\u252c\u2500 g(24002002) = 40301002\n\u2502 \u2514\u2500\u252c\u2500 g(41111004) = 24002002\n\u2514\u2500\u25ba \u2514\u2500\u252c\u2500 g(40301002) = 41111004\n      \u2514\u2500\u252c\u2500 g(40220002) = 40301002\n        \u2514\u2500\u252c\u2500 g(32030002) = 40220002\n          \u2514\u2500\u252c\u2500 g(33010103) = 32030002\n            \u251c\u2500\u2500\u2500 g(51010103) = 33010103\n            \u2514\u2500\u252c\u2500 g(51100013) = 33010103\n              \u2514\u2500\u252c\u2500 g(61000002) = 51100013\n                \u2514\u2500\u2500\u2500 g(70000001) = 61000002\n\n\u2500\u25ba g(31220003) = 31220003\n\nGiven this observation, I'm inclined to say that the fact that the limit set of the original iterated function $f$ consists of a single fixed point is mostly just pure luck, aided by the rapid shrinking of the iterated image due to the permutation invariance of the function.\n\n*) Except for the possible ambiguity regarding whether leading zeros should be counted or not.  Above, I'm assuming that they should be counted, and that all inputs to $f$ should effectively be zero-padded to eight digits.  In any case, this only affects the first few iterations, since it's easy to show that, whether leading zeros are counted or not, $f^{(2)}(N)$ must contain at least one non-leading zero for all $N$, and therefore $f^{(3)}(N)$ and all further iterates must have eight digits with no leading zeros.", "meta": {"post_id": 3816844, "input_score": 43, "output_score": 36, "post_title": "Why the \"self-referential number\" function eventually fixes every point"}}
{"input": "Problem\nThe premise is almost the same as in this question. I'll restate for convenience.\n\nLet $A$, $B$, $C$ be independent random variables uniformly distributed between $(-1,+1)$. What is the probability that the polynomial $Ax^2+Bx+C$ has real roots?\n\nNote: The distribution is now $-1$ to $+1$ instead of $0$ to $1$.\nMy Attempt\nPreparation\nWhen the coefficients are sampled from $\\mathcal{U}(0,1)$, the probability for the discriminant to be non-negative that is, $P(B^2-4AC\\geq0) \\approx 25.4\\% $. This value can be obtained theoretically as well as experimentally. The link I shared above to the older question has several good answers discussing both approaches.\nChanging the sampling interval to $(-1, +1)$ makes things a bit difficult from the theoretical perspective. Experimentally, it is rather simple. This is the code I wrote to simulate the experiment for $\\mathcal{U}(0,1)$. Changing it from (0, theta) to (-1, +1) gives me an average probability of $62.7\\%$ with a standard deviation of $0.3\\%$\nI plotted the simulated PDF and CDF. In that order, they are:\n\nSo I'm aiming to find a CDF that looks like the second image.\nTheoretical Approach\nThe approach that I find easy to understand is outlined in this answer. Proceeding in a similar manner, we have\n$$\nf_A(a) = \\begin{cases}\n\\frac{1}{2}, &-1\\leq a\\leq+1\\\\\n0,           &\\text{ otherwise}\n\\end{cases}\n$$\nThe PDFs are similar for $B$ and $C$.\nThe CDF for $A$ is\n$$\nF_A(a) = \\begin{cases}\n\\frac{a + 1}{2}, &-1\\leq a\\geq +1\\\\\n0,&a<-1\\\\\n1,&a>+1\n\\end{cases}\n$$\nLet us assume $X=AC$. I proceed to calculate the CDF for $X$ (for $x>0$) as:\n$$\n\\begin{align}\nF_X(x) &= P(X\\leq x)\\\\\n&= P(AC\\leq x)\\\\\n&= \\int_{c=-1}^{+1}P(Ac\\leq x)f_C(c)dc\\\\\n&= \\frac{1}{2}\\left(\\int_{c=-1}^{+1}P(Ac\\leq x)dc\\right)\\\\\n&= \\frac{1}{2}\\left(\\int_{c=-1}^{+1}P\\left(A\\leq \\frac{x}{c}\\right)dc\\right)\\\\\n\\end{align}\n$$\nWe take a quick detour to make some observations. First, when $0<c<x$, we have $\\frac{x}{c}>1$. Similarly, $-x<c<0$ implies $\\frac{x}{c}<-1$. Also, $A$ is constrained to the interval $[-1, +1]$. Also, we're only interested when $x\\geq 0$ because $B^2\\geq 0$.\nContinuing, the calculation\n$$\n\\begin{align}\nF_X(x) &= \\frac{1}{2}\\left(\\int_{c=-1}^{+1}P\\left(A\\leq \\frac{x}{c}\\right)dc\\right)\\\\\n&= \\frac{1}{2}\\left(\\int_{c=-1}^{-x}P\\left(A\\leq \\frac{x}{c}\\right)dc + \\int_{c=-x}^{0}P\\left(A\\leq \\frac{x}{c}\\right)dc + \\int_{c=0}^{x}P\\left(A\\leq \\frac{x}{c}\\right)dc + \\int_{c=x}^{+1}P\\left(A\\leq \\frac{x}{c}\\right)dc\\right)\\\\\n&= \\frac{1}{2}\\left(\\int_{c=-1}^{-x}P\\left(A\\leq \\frac{x}{c}\\right)dc + 0 + 1 + \\int_{c=x}^{+1}P\\left(A\\leq \\frac{x}{c}\\right)dc\\right)\\\\\n&= \\frac{1}{2}\\left(\\int_{c=-1}^{-x}\\frac{x+c}{2c}dc + 0 + 1 + \\int_{c=x}^{+1}\\frac{x+c}{2c}dc\\right)\\\\\n&= \\frac{1}{2}\\left(\\frac{1}{2}(-x+x(\\log(-x)-\\log(-1)+1) + 0 + 1 + \\frac{1}{2}(-x+x(-\\log(x)-\\log(1)+1)\\right)\\\\\n&= \\frac{1}{2}\\left(2 + \\frac{1}{2}(-x+x(\\log(x)) -x + x(-\\log(x))\\right)\\\\\n&= 1 - x\n\\end{align}\n$$\nI don't think this is correct.\nMy Specific Questions\n\nWhat mistake am I making? Can I even obtain the CDF through integration?\nIs there an easier way? I used this approach because I was able to understand it well. There are shorter approaches possible (as is evident with the $\\mathcal{U}(0,1)$ case) but perhaps I need to read more before I can comprehend them. Any pointers in the right direction would be helpful.", "output": "$\\newcommand{\\bbx}[1]{\\,\\bbox[15px,border:1px groove navy]{\\displaystyle{#1}}\\,}\n \\newcommand{\\braces}[1]{\\left\\lbrace\\,{#1}\\,\\right\\rbrace}\n \\newcommand{\\bracks}[1]{\\left\\lbrack\\,{#1}\\,\\right\\rbrack}\n \\newcommand{\\dd}{\\mathrm{d}}\n \\newcommand{\\ds}[1]{\\displaystyle{#1}}\n \\newcommand{\\expo}[1]{\\,\\mathrm{e}^{#1}\\,}\n \\newcommand{\\ic}{\\mathrm{i}}\n \\newcommand{\\mc}[1]{\\mathcal{#1}}\n \\newcommand{\\mrm}[1]{\\mathrm{#1}}\n \\newcommand{\\pars}[1]{\\left(\\,{#1}\\,\\right)}\n \\newcommand{\\partiald}[3][]{\\frac{\\partial^{#1} #2}{\\partial #3^{#1}}}\n \\newcommand{\\root}[2][]{\\,\\sqrt[#1]{\\,{#2}\\,}\\,}\n \\newcommand{\\totald}[3][]{\\frac{\\mathrm{d}^{#1} #2}{\\mathrm{d} #3^{#1}}}\n \\newcommand{\\verts}[1]{\\left\\vert\\,{#1}\\,\\right\\vert}$\nHereafter, $\\ds{\\bracks{P}}$ is an\nIverson Bracket. Namely,\n$\\ds{\\bracks{P} = \\color{red}{1}}$ whenever $\\ds{P}$ is $\\ds{\\tt true}$ and $\\ds{\\color{red}{0}}$\n$\\ds{\\tt otherwise}$. They are very convenient whenever we have to manipulate constraints.\n\n\\begin{align}\n&\\bbox[5px,#ffd]{\\int_{-1}^{1}{1 \\over 2}\\int_{-1}^{1}\n{1 \\over 2}\\int_{-1}^{1}{1 \\over 2}\\bracks{b^{2} - 4ac > 0}\n\\dd c\\,\\dd a\\,\\dd b}\n\\\\[5mm] = &\\\n{1 \\over 4}\\int_{0}^{1}\\int_{-1}^{1}\n\\int_{-1}^{1}\\bracks{b^{2} - 4ac > 0}\n\\dd c\\,\\dd a\\,\\dd b\n\\\\[5mm] = &\\\n{1 \\over 4}\\int_{0}^{1}\\int_{-1}^{1}\n\\int_{0}^{1}\\braces{\\bracks{b^{2} - 4ac > 0} +\n\\bracks{b^{2} + 4ac > 0}}\n\\dd c\\,\\dd a\\,\\dd b\n\\\\[5mm] = &\\\n{1 \\over 4}\\int_{0}^{1}\\int_{0}^{1}\n\\int_{0}^{1}\\left\\{\\bracks{b^{2} - 4ac > 0} +\n\\bracks{b^{2} + 4ac > 0}\\right.\n\\\\[2mm] &\\ \\phantom{{1 \\over 4}\\int_{0}^{1}\\int_{-1}^{1}\n\\int_{0}^{1}}\n\\left. + \\bracks{b^{2} + 4ac > 0} +\n\\bracks{b^{2} - 4ac > 0}\\right\\}\\dd c\\,\\dd a\\,\\dd b\n\\\\[5mm] = &\\\n{1 \\over 2} +\n{1 \\over 2}\\int_{0}^{1}\\int_{0}^{1}\\int_{0}^{1}\n\\bracks{b^{2} - 4ac > 0}\\dd c\\,\\dd a\\,\\dd b\n\\\\[5mm] = &\\\n{1 \\over 2} +\n{1 \\over 2}\\int_{0}^{1}\\int_{0}^{1}{1 \\over a}\\int_{0}^{a}\n\\bracks{b^{2} - 4c > 0}\\dd c\\,\\dd a\\,\\dd b \n\\\\[5mm] = &\\\n{1 \\over 2} +\n{1 \\over 2}\\int_{0}^{1}\\int_{0}^{1}\\bracks{b^{2} - 4c > 0}\n\\int_{c}^{1}{1 \\over a}\\,\\dd a\\,\\dd c\\,\\dd b\n\\\\[5mm] = &\\\n{1 \\over 2} -\n{1 \\over 2}\\int_{0}^{1}\\int_{0}^{1}\n\\bracks{c < {b^{2} \\over 4}}\\ln\\pars{c}\\,\\dd c\\,\\dd b \n\\\\[5mm] = &\\\n{1 \\over 2} -\n{1 \\over 2}\\int_{0}^{1}\\int_{0}^{b^{2}/4}\n\\ln\\pars{c}\\,\\dd c\\,\\dd b\n\\\\[5mm] = &\\\n{1 \\over 2} -\n{1 \\over 2}\\int_{0}^{1}\\bracks{%\n-\\,{1 + 2\\ln\\pars{2} \\over 4}\\,b^{2} +\n{1 \\over 2}\\,b^{2}\\ln\\pars{b}}\\,\\dd b\n\\\\[5mm] = &\n\\bbx{{\\ln\\pars{2} \\over 12} + {41 \\over 72}}\n\\approx 0.6272 \\\\ &\n\\end{align}", "meta": {"post_id": 3818919, "input_score": 58, "output_score": 41, "post_title": "Probability that a quadratic equation has real roots"}}
{"input": "Consider following two infinite groups: group of all permutations of natural numbers (i.e. group of all bijections $f: \\mathbb{N} \\to \\mathbb{N}$) and group of all rotations of a plane. Does group of permutations contain subgroup isomorphic to the group of rotations?\nBoth groups have cardinality of the continuum, so simple cardinality considerations do not work.", "output": "Edit #2: Here is, to me, an even more surprising result along these lines. Kallmann proved that for any field $F$ of cardinality at most the continuum, $GL_n(F)$ embeds into $S_{\\infty}$. In particular, for example, $SO(3)$ has a subgroup of countable index, which is very surprising to me.\nEdit: Okay, as suspected the answer to this question is independent of ZF. There's a model of ZF constructed by Shelah in which every set of real numbers has the Baire property. This implies, if I understand correctly, that there are no nonzero homomorphisms from $\\mathbb{R}$ to any countable abelian group (since any countable abelian group with the discrete topology is a Polish group, so in this model any homomorphism from $\\mathbb{R}$ to such a group is automatically measurable and so automatically continuous). So $\\mathbb{R}$, and $SO(2)$, have no subgroups of countable index in this model.\nAmong other things, in this model $\\mathbb{R}$ is a $\\mathbb{Q}$-vector space whose $\\mathbb{Q}$-linear dual is trivial.\n\nThe answer is yes (assuming the axiom of choice; I am quite surprised by this).\nMore generally, let $A$ be an abelian group and let's see what we can say about the smallest set on which it acts faithfully. If $X$ is a set on which $A$ acts, it breaks up into a disjoint union of orbits $A/A_i$ where the $A_i$ are subgroups of $A$. Because $A$ is abelian, the kernel of $A$ acting on $A/A_i$ is $A_i$, so the kernel of $A$ acting on $X$ is the intersection $\\cap_i A_i$.\nSpecializing to $A = SO(2)$, the question of whether $A$ embeds into $\\text{Aut}(\\mathbb{N})$ is equivalent to the question of whether $A$ acts faithfully on a countable set, which is in turn equivalent to the question of whether we can find an at-most-countable collection of subgroups $A_i$ of $A$ of at-most-countable index whose intersection is trivial. Now, by the axiom of choice we have an abstract isomorphism\n$$SO(2) \\cong \\mathbb{Q}/\\mathbb{Z} \\oplus \\bigoplus_{i \\in I} \\mathbb{Q}$$\ncoming from writing $SO(2) \\cong \\mathbb{R}/\\mathbb{Z}$ and picking a basis of $\\mathbb{R}$ as a $\\mathbb{Q}$-vector space containing $\\{ 1 \\}$. The index set $I$ above is uncountable. By a second application of the axiom of choice, $\\bigoplus_{i \\in I} \\mathbb{Q}$ is abstractly isomorphic to $\\mathbb{Q}^{\\mathbb{N}}$ (the index set is now countable), so\n$$SO(2) \\cong \\mathbb{Q}/\\mathbb{Z} \\times \\mathbb{Q}^{\\mathbb{N}}.$$\nNow we can argue as follows. Let $A_i$ be the kernels of the projections to each of the factors $\\mathbb{Q}/\\mathbb{Z}$ and $\\mathbb{Q}$ above. Then by construction the $A_i$ are a countable collection of subgroups of countable index, and their intersection is trivial. This means $SO(2)$ acts faithfully on the countable set $\\mathbb{Q}/\\mathbb{Z} \\sqcup (\\mathbb{Q} \\times \\mathbb{N})$ given by the disjoint union of the factors.\n(While writing this answer I was repeatedly tempted to conjecture that the intersection of a countable collection of subgroups of countable index has countable index, which is just false, and $\\mathbb{Q}^{\\mathbb{N}}$ is a counterexample. That's what led to the above construction.)\nOf course this argument is deeply inexplicit. Without the axiom of choice I don't know if you can exhibit even a single nonzero homomorphism $SO(2) \\to \\mathbb{Q}$. All you have is a short exact sequence $0 \\to \\mathbb{Q}/\\mathbb{Z} \\to \\mathbb{R}/\\mathbb{Z} \\to \\mathbb{R}/\\mathbb{Q} \\to 0$ and it's very unclear what to say about the rightmost term $\\mathbb{R}/\\mathbb{Q}$ without choice, beyond that it's a $\\mathbb{Q}$-vector space.", "meta": {"post_id": 3877006, "input_score": 36, "output_score": 34, "post_title": "Does group of permutations of natural numbers contain subgroup isomorphic to $SO(2)$?"}}
{"input": "I love solving Rubik's cube (the usual 3D one). But, a lecture by Matt Parker at the Royal Institute (YouTube Link) led me to an app that can simulate a four dimensional rubik's cube. But unfortunately it was so complex, that I soon got bored as I failed to solve it.\nThe website to the 4D cube : http://superliminal.com/cube/cube.htm\n\nFollowing which today, I also found an app that can simulate a 5D cube!!\nThe website to the 5D cube: http://www.gravitation3d.com/magiccube5d/\n\nSo, my two questions are :\n\nIs there a general (non brute-force) algorithm that can be used to solve a well-scrambled cube of any dimension (even though it may not be very efficient, but yet is not a simple search over all the available space of move sequences) ? [Point modified after reading @RavenclawPrefect's answer :D ]\nMathematically, what is common in all these cubes, and \"hypercubes\"?\n\n\nNOTE: By dimensions I mean physical dimension and not the number of rows of pieces on a face of the cube. So a four-dimension cube is a cube in x,y,z,$\\delta$ dimensions where $\\hat x,\\hat y,\\hat z,\\hat \\delta$ are mutually orthogonal unit vectors in the respective dimensions.\nFor example, here is how the aforementioned 4D cube moves:\nhttps://miro.medium.com/max/2552/1*ga32DoV_Hc6e8t6PC1hFHw.gif\n\n\nAddendum:\nList of resources that may help finding an answer to this question:\n\nSolving Rubik's cube and other permutation puzzles", "output": "The algorithmic problem is a special case of ``constructive membership test in permutation groups'' -- given a permutation (you label every face of the cube with a number, then every slice rotation corresponds to a permutation of the numbers, as does the mixed state), write it as a product of generators.\nThis does not care for the dimension of the cube you work in, you just get more and more labels.\nThe generic solution to the problem is called the Schreier-Sims algorithm (https://en.wikipedia.org/wiki/Schreier\u2013Sims_algorithm). However in its naive form, it produces very long solutions (length >100k for the $3\\times3\\times 3$ cube).\nOne can add heuristics (basically to pre-seed with short products as \"extra\" generators), see for example:\nhttps://doi.org/10.1006/jsco.1998.0202 .\nWith such heuristics, the $3\\times3\\times 3$ cube gets solutions of length $\\sim 40-100$, so a magnitude too large, but not astronomically so. One can tune heuristics (use more memory) to get better solutions, though the only (known) way to find a shortest solution is brute-force.", "meta": {"post_id": 3925859, "input_score": 36, "output_score": 45, "post_title": "Is there (or can there be) a general algorithm to solve Rubik's cubes of any dimension?"}}
{"input": "Let $M$ and $N$ be topological manifolds that admit differential structures and let $f:M\\to N$ be continuous. Can $M$ and $N$ always be given differential structures to become differentiable manifolds $\\widetilde M$ and $\\widetilde N$ such that $f:\\widetilde M\\to\\widetilde N$ is differentiable? What if we impose further restrictions, such as $\\widetilde M$ and $\\widetilde N$ being smooth manifolds, or $f$ becoming smooth? Are there certain differential structures which we can't do this with (i.e. if we want to make $f$ differentiable, we can never make $N$ diffeomorphic to $\\overline N$ where $\\overline N$ is some differential structure on $N$)? What if $M$ already has a fixed differential structure?", "output": "There is not necessarily a way to make a map smooth.  For example, suppose $M=\\mathbb{R}$, and suppose $N$ is any topological manifold of dimension $2$ or more.  Let $f:M\\rightarrow N$ be any continuous function whose image contains a non-empty open subset of $N$ (e.g., take a space filling curve onto $\\mathbb{R}^n$ and then think of this $\\mathbb{R}^n$ as a chart).\nThen there are no smooth structures on $M$ and $N$ which makes $f$ smooth.  In fact, you cannot even make $f$ continuously differentiable.  One way to see this is to use Sard's Theorem: if you could make $f$ continuously differentiable, then the set of regular values would be open and dense.  Because $M$ has a lower dimension than $N$, regular values are points of $N$ which are not in the image of $f$.  But then the rest that the image of $f$ contains an open subset of $N$ means the set of regular values of $f$ is not dense.", "meta": {"post_id": 3968922, "input_score": 21, "output_score": 36, "post_title": "Can every continuous function between topological manifolds be turned into a differentiable map?"}}
{"input": "There is apparently cutting-edge research by Dustin Clausen & Peter Scholze (and probably others) under the name Condensed Mathematics, which is meant to show that the notion of Topological Space is not so well-chosen, and that Condensed Sets lead to better behaved structures.\n\nWhat is a simple low-tech example to see the difference?\n\nI am looking for some explicit construction with quite simple topological spaces where some bad behaviour occur, and how their condensed analog fix that.\nI am aware of the nlab entry and of an introductory text by F. Deglise on this page but it goes quite far too quickly and I am missing knowledge to grasp it.", "output": "Here's a one-paragraph answer: Topological spaces formalize the idea of spaces with a notion of \"nearness\" of points. However, they fail to handle the idea of \"points that are infinitely near, but distinct\" in a useful way. Condensed sets handle this idea in a useful way.\nLet me give a few examples. Say you have a nice space like the line $X=\\mathbb R$, and acting on it you have a group $G$. If $G$ acts nicely, for example $G=\\mathbb Z$ through translations, then the quotient space $X/G$ is well-behaved as a topological space, giving the circle $S^1$ in this case. However, if $G$ has dense orbits, for example $G=\\mathbb Q$, then $X/G$ is not well-behaved as a topological space. In fact, in this example $\\mathbb R/\\mathbb Q$ has many distinct points, but they are all infinitely near: any neighborhood of one point $x$ will also contain any other point $y$. Thus, as a topological space $\\mathbb R/\\mathbb Q$ is indiscrete; in other words, it is not remembering any nontrivial topological structure.\nOne could also consider nonabelian examples, like the quotient $\\mathrm{GL}_2(\\mathbb R)/\\mathrm{GL}_2(\\mathbb Z[\\tfrac 12])$.\nSimilar things also happen in functional analysis. For example, one can embed summable sequences\n$$\\ell^1(\\mathbb N)=\\{(x_n)_n\\mid x_n\\in \\mathbb R, \\sum_n |x_n|<\\infty\\}$$\ninto square-summable sequences\n$$\\ell^2(\\mathbb N)=\\{(x_n)_n\\mid x_n\\in \\mathbb R, \\sum_n |x_n|^2<\\infty\\}$$\nand consider the quotient $\\ell^2(\\mathbb N)/\\ell^1(\\mathbb N)$. As a topological vector space, this is indiscrete, and so has no further structure than the abstract $\\mathbb R$-vector space. For this reason, quotients of this type are usually avoided, although they may come up naturally!\nWithout repeating the definition of condensed sets here, let me just advertise that they can formalize the idea of \"points that are infinitely close, but distinct\", and all the above quotients can be taken in condensed sets and are reasonable. As we have seen, this means that one must abandon \"neighborhoods\" as the primitive concept, as in these examples all neighborhoods of one point already contain all other points. Roughly, what is formalized instead is the notion of convergence, possibly allowing that one sequence converges in several different ways.\nSo in the condensed world, it becomes possible to consider quotients like $\\ell^2(\\mathbb N)/\\ell^1(\\mathbb N)$, and inside all condensed $\\mathbb R$-vector spaces they are about as strange as torsion abelian groups like $\\mathbb Z/2\\mathbb Z$ are inside all abelian groups. However, there are some surprising new phenomena: For example, there is a nonzero map of condensed $\\mathbb R$-vector spaces\n$$\\ell^1(\\mathbb N)\\to \\ell^2(\\mathbb N)/\\ell^1(\\mathbb N)$$\ninduced by the map\n$$(x_n)_n\\mapsto (x_n \\log |x_n|)_n,$$\nin other words $x\\mapsto x\\log |x|$ pretends to be a linear map! (Let me leave this as a (fun) exercise.)\n(These are liquid $\\mathbb R$-vector spaces, and the presence of such strange maps makes it hard to set up the basic theory of liquid vector spaces; but arguably makes it more interesting!)", "meta": {"post_id": 4044728, "input_score": 24, "output_score": 68, "post_title": "Examples of the difference between Topological Spaces and Condensed Sets"}}
{"input": "What is the remainder when $6^{273} + 8^{273}$ is divided by $49$?\n\nI tried this question through two methods and both are giving different answers so I wanted to know which is the correct one, and why the other is incorrect.\nApproach $1$:\nHere, I have tried to express everything in terms of $\\pmod7$\nFor odd numbers\n$$a^n+b^n = (a+b)(a^{n-1}-a^{n-2}b+a^{n-3}b^2-....+b^{n-1})$$\nSo\n$$6^{273}+8^{273} = (6+8)(6^{272}-6^{271}\\cdot8+6^{270}\\cdot8^2-....+8^{272})$$\n$6^{272}\\equiv(-1)\\pmod7$\n$8^{272}\\equiv1\\pmod7$\nSo, the second bracket reduces to:\n$$((-1)^{272} - (-1)^{271}\\cdot1 +(-1)^270\\cdot1....+1^{271})\\pmod7$$\nWhich is $273\\pmod7$, basically it is divisible by $7$ and even the $(6+8)$ part is divisible by $7$.\n$$6^{273}+8^{273} = 7^2k$$ so $$6^{273}+8^{273}\\equiv 0 \\pmod{49}$$\n\nApproach $2$:\n$6^3 \\equiv 20 \\pmod{49}$\nSo by cyclicity: $6^{273}\\equiv 20 \\pmod{49}$\nSimilarly,\n$8^3 \\equiv 22\\pmod{49}$\nSo, $8^{273}\\equiv 22 \\pmod{49}$\nTherefore:\n$$6^{273}+8^{273}\\equiv 42 \\pmod{49}$$\n\nHelp, which is correct?", "output": "The binomial formula gives\n$$(7\\pm1)^{273}={273\\choose1}7^1(\\pm1)^{272}+{273\\choose0}7^0(\\pm1)^{273}=273\\cdot 7\\pm1=\\pm1\\qquad({\\rm mod}\\  49)\\ ,$$\nsince all other terms are divisible by $7^2$. It follows that the answer to your question is $0$.", "meta": {"post_id": 4131584, "input_score": 6, "output_score": 41, "post_title": "What is the remainder when $6^{273} + 8^{273}$ is divided by $49$?"}}
{"input": "If you search for \"Turn! Turn! Turn!\" on Google, then the second result is this YouTube video of The Byrds performing the Pete Seeger song of that name. But the first result is Google's internal calculator displaying \"241217.524881\". With a bit of experimentation, it appears that this number is a numerical approximation to $$\\frac{\\Gamma(2\\pi+1)^2}{2 \\pi},$$\nwhere $\\Gamma$ represents the Euler gamma function.\nI sort of understand why Google is interpreting \"Turn\" to mean $2\\pi$, and the exclamation mark to mean $x! := \\Gamma(x+1)$, as this is a relatively common (although not universal) choice of interpolation of the factorial function to the real numbers. But in that case, I would expect Google to interpret \"Turn! Turn! Turn!\" to represent $\\Gamma(2\\pi+1)^3 \\approx 18\\, 658\\, 774\\, 329$ instead of the expression above. Why isn't it?\nA possible partial solution: if you search \"Turn! Turn\" then you get the expected result $7735.248 \\approx \\Gamma(2\\pi+1) 2\\pi$. But if you search \"Turn! Turn!\" then you do not get the expected result $\\Gamma(2\\pi+1)^2 \\approx 1\\, 515\\, 614$. Instead, you get 195.936, which appears to be the numerical approximation of $\\Gamma(2\\pi+1)/(2\\pi)$. Moreover, Google reparses the input as \"Turn ! (Turn !)\". To me, this suggests that it's interpreting the second explamation mark as a factorial symbol, but the first exclamation mark to mean $a ! b := b/a$, i.e. division but with the usual order of arguments reversed. This explains the orginal result if Google is interpreting \"Turn! Turn! Turn!\" with the first exclamation mark representing reversed division (with a lower order-of-operations precedence than multiplication) but the second two exclamation marks representing factorial:\n$$2\\pi \"!\" (((2\\pi)!)\\ ((2\\pi)!)) = \\frac{\\Gamma(2\\pi+1)^2}{2\\pi}.$$\nIs this notation $a!b := b/a$ standard? I've never seen it before. Can anyone explain how Google is parsing this string?\n(This is one of those awkward questions where the (unknown) solution determines whether or not the question is on-topic for Math Stack Exchange. If the solution does indeed come down to unusual math notation, as I suspect, then the question is on-topic for Math SE. But if the resolution is just some black-box machine learning magic, then maybe the question isn't on topic. I'm not quite sure what one does in this kind of situation.)", "output": "It appears that Google search uses the following syntax for conversions between different units or currencies: target_unit! <expression> (Though I can't find any documentation for this). For example, searching for cm! 1m + 5 yards yields 557.2 centimeters. Interpreting a turn to be a dimensionless quantity equal to $2\\pi$, we can try to make sense of the results:\n\nTurn ! (Turn !) means \"calculate $\\text{Turn}!$ and express it in the unit turn\", which comes out to $\\frac{\\Gamma(2\\pi +1)}{2\\pi}\\text{ Turn}$, or approximately $195.936116 \\text{ Turn}$ (as observed by Mikael).\nTurn! Turn! Turn! is interpreted as (Turn ! (Turn !)) * (Turn !), where we make the calculation from the previous bullet point and are left with a product of something in the unit turn and a number, which Google evaluates by throwing away the unit information in the first term and calculating the product of the two numbers. Hence, we end up with $\\frac{\\Gamma(2\\pi+1)}{2\\pi}\\cdot \\Gamma(2\\pi+1)$ as the result.", "meta": {"post_id": 4207222, "input_score": 93, "output_score": 73, "post_title": "Why does \"Turn! Turn! Turn!\" equal 241217.524881?"}}
{"input": "I often hear people saying that (co)homology is really useful in many areas of mathematics.\n\nIn which way is (co)homology used in different areas of mathematics to prove theorems?\n\nI only know one example: the proof of Brouwer's fixed point theorem uses homology: every continuous function $f\\colon D^n\\to D^n$ has a fixed point, where $D^n$ is the $n$-disk. Suppose not. Then construct a continuous map $h\\colon D^n\\to S^{n-1}$ by sending each $x\\in D^n$ to the intersection of the line connecting $x$ and $f(x)$ with the boundary $S^{n-1}$ of $D^n$. Now, note that $h$ has a right inverse, given by the inclusion\n$$i\\colon S^{n-1}\\to D^n, \\, x\\mapsto x,$$\nthat is,\n$$h\\circ i=\\mathrm{id}_{S^{n-1}}.$$\nThen, by the functoriality of the homology functor $H_{n-1}\\colon \\mathbf{Top}\\to\\mathbf{Ab}$, it follows that\n$$H_{n-1}(h)\\circ H_{n-1}(i)=\\mathrm{id}_{H_{n-1}(S^{n-1})},$$\nwhich implies that $H_{n-1}(h)\\colon H_{n-1}(D^n)\\to H_{n-1}(S^{n-1})$ has a right inverse (in particular is surjective). But this can't be possible, since $H_{n-1}(D^n)$ is the trivial group, whereas $H_{n-1}(S^{n-1})$ is infinite.\nSo using homology, every property of the category $\\mathbf{Top}$ (e.g., whether a certain morphism has a left inverse) gets translated into a property of the category $\\mathbf{Ab}$ (and, by contraposition, if a property isn't true in $\\mathbf{Ab}$, then it can't be true in $\\mathbf{Top}$).\n\nHas every use of (co)homology the form of the above argument, i.e., uses that properties of the category $\\mathbf{Top}$ get translated into a property of the category $\\mathbf{Ab}$?\n\nI would think no, because I doubt that interesting properties can always be formulated as categorical properties.\nBut then, can (co)homology be used in another way? For instance, I heard that cohomology was used in the proof of the Weil conjectures, and I also heard that there's a cohomology theory (Group cohomology) which can be used in group theory. This seems crazy. I just want to get a clue at why (co)homology is useful in these areas and how it is applied there.", "output": "This question is extremely broad, and a lot of ink has been spilled on the subject of \"why is cohomology useful\" (see, for example, here, here, here, here, here, here, here, or here, all from the first page of google results for exactly that search).\nThat said, I can answer your question about whether it must always be used in the way you outline (detecting nonexistence of topological features by showing nonexistence of algebraic features): No. There are lots of other uses which do not follow this outline.\nOne modern view of cohomology is that it witnesses the obstructions to solving some equation.\nThis is most simply seen in De Rham Cohomology, where we want to solve a differential equation $df = g$. It turns out that locally we can always solve this equation, but globally we might not be able to. The \"obstruction\" to solving this equation is also topological! If our space is simply connected then we can always solve it. Indeed, we can always integrate a function defined on $\\mathbb{C}$, say. But there are nice functions defined in the punctured plane which have no global antiderivative. The famous example is $\\frac{1}{z}$.\nWe might also be interested in solving the equation $f^2 = g$. Again, we know how to do this locally, but there might be no way to solve it on the whole complex plane at once. Also again, we find the obstruction to solving this equation is cohomological.\nAs a more algebraic example, say you want to solve $x^n = y$ in some field $k$. Then we know how to solve this equation in some algebraic closure $\\overline{k}$, and we know that a solution to this equation in $\\overline{k}$ actually exists in $k$ if and only if that solution is fixed by the action of the galois group $G$. So we find ourselves interested in the cohomology of $G$.\n\nMore concretely, how does this work? Well if we have a mapping between some objects (in the examples above, the mappings were $d$, $(-)^2$, and $(-)^n$ respectively) we can \"solve\" an equation exactly when we understand the image of the mapping.\nFor example, we can solve $df = g$ exactly when $g$ is in the image of $d$, or $f^2 = g$ whenever $g$ is in the image of $(-)^2$, etc.\nThe key idea is to use exact sequences to turn the question of being in the image (which is hard) into the question of being in the kernel (which is comparatively easy). Cohomology theories (and more generally derived functors) give us access to long exact sequences which we can use to check if an equation is (globally) solvable.\nFor example, let $\\mathcal{F}^\\times$ be the sheaf of nonnvanishing holomorphic functions on $\\mathbb{C}$ under multiplication. Then we have a short exact sequence\n$$\n0 \\to \\{ \\pm 1 \\} \\to \\mathcal{F}^\\times \\overset{(-)^2}{\\to} \\mathcal{F}^\\times \\to 0\n$$\nNow our function $g$ is a section of $\\mathcal{F}^\\times$, and we want to know if it's the image of a section of $\\mathcal{F}^\\times$ under $(-)^2$. That is, if we can find an $f$ with $f^2 = g$.\nWell, we apply sheaf cohomology to this exact sequence to get a (long) exact sequence\n$$\n0 \n\\to H^0(\\mathbb{C}, \\{ \\pm 1 \\}) \n\\to H^0(\\mathbb{C}, \\mathcal{F}^\\times) \n\\overset{(-)^2}{\\to} H^0(\\mathbb{C}, \\mathcal{F}^\\times)\n\\to H^1(\\mathbb{C}, \\{ \\pm 1 \\})\n\\to \\cdots\n$$\nHere $H^0$ of a sheaf is exactly the global sections. So $g$ lives in the second copy of $H^0(\\mathbb{C}, \\mathcal{F}^\\times)$. We want to know if it lies in the image of the first copy, and we can do that by checking if it lies in the kernel of the map to $H^1(\\mathbb{C}, \\{ \\pm 1 \\})$. One of the magical things about cohomology is that in special cases we can often compute these cohomology groups and the maps between them! So we can actually use this machinery to check if a solution exists.\n\nI hope this helps ^_^", "meta": {"post_id": 4232919, "input_score": 21, "output_score": 44, "post_title": "Why is (co)homology useful and in which way?"}}
{"input": "I have trouble understanding the difference between initial and terminal objects in category theory. I get the definitions, but when seeing examples, I am confused.\nExample 1: The empty set is the unique initial object in the category of sets. Every one-element set is a terminal object in this category.\n\nWhy there can be morphism from empty set to any other set? And why there is not morphism to empty set as well? I would find it more intuitive if one-element set would be initial object too.\n\nExample 2: Similarly, the empty space is the unique initial object in Top, the category of topological spaces and every one-point space is a terminal object in this category.\n\nWhy we can create a topological space from empty space? And why the space cannot be mapped into empty space uniquely too?\n\nI don't know why I am stuck with this. Maybe I have a wrong idea about how morphisms behave?\nThank you for your help.", "output": "Your confusion seems to be not so much about initial and terminal objects, but about what those look like in the category of sets.    Looking at the formal definition of \u201cfunction\u201d will help make clear some of the unusual cases such as functions with empty domains.\nA function from $A$ to $B$ can be understood as a set of pairs $$\\langle a,b\\rangle$$ where $a\\in A$ and $b\\in B$.  And:\n\nThere must be exactly one pair $\\langle a,b\\rangle$ for each element $a$ of $A$.\n\nExactly one, no more and no less, or the set of pairs is not a function.\nFor example, the function that takes an integer $n$ and yields its square $n^2$ can be understood as the (infinite) set of ordered pairs:\n$$\\{ \\ldots ,\\langle -2, 4\\rangle,\n\\langle -1, 1\\rangle,\n\\langle 0, 0\\rangle ,\\langle 1, 1\\rangle,\n\\langle 2, 4\\rangle\\ldots\\}$$\nAnd for each integer $n$ there is exactly one pair $\\langle n, n^2\\rangle$.  Some numbers can be missing on the right side (for example, there is no pair $\\langle n, 3\\rangle$) and some numbers can be repeated on the right (for example the function contains both $\\langle -2, 4\\rangle$\nand $\\langle 2, 4\\rangle$) but on the left each number appears exactly once.\n\nNow suppose $A$ is some set $\\{a_1, a_2, \\ldots\\}$ and $B$ is a set with only one element $\\{b\\}$.  What does a function from $A$ to $B$ look like?  There is only one possible function: it must be: $$\\{\n\\langle a_1, b\\rangle,\n\\langle a_2, b\\rangle,\n\\ldots\\}.$$ There is no choice about the left-side elements of the pairs, because there must be exactly one pair for each element of $A$.  There is also no choice about the right-side element of each pair.  $B$ has only one element, $b$, so the right-side element of each pair must be $b$.\nSo, if $B$ is a one-element set, there is exactly one function from $A$ to $B$. This is the definition of \u201cterminal\u201d, and one-element sets are terminal.\nNow what if it's $A$ that has only one element?  We have $A=\\{a\\}$ and $B=\\{b_1, b_2, \\ldots\\}$.  How many functions are there now?  Only one?\nOne function is $$\\{\\langle a, b_1\\rangle\\}$$\nanother is $$\\{\\langle a, b_2\\rangle\\}$$\nand another is $$\\{\\langle a, b_3\\rangle\\}$$\nand so on.  Each function is a set of pairs where the left-side elements come from $A$, and each element of $A$ is in exactly one pair.  $A$ has only one element, so there can only be one pair in each function.  Still, the functions are all different.\nYou said:\n\nI would find it more intuitive if one-element set would be initial object too.\n\nBut for a one-element set $A$ to be initial, there must be exactly one function $A\\to B$ for each $B$.  And we see above that usually there are many functions $A\\to B$.\n\nNow we do functions on the empty set.  Suppose $A$ is $\\{a_1, a_2, \\ldots\\}$ and $B$ is empty.  What does a function from $A\\to B$ look like?  It must be a set of pairs, it must have exactly one pair for each element of $a$, and the right-side of each pair must be an element of $B$. But $B$ has no elements, so this is impossible:  $$\\{\\langle a_1, ?\\rangle, \\langle a_2, ?\\rangle, \\ldots\\}.$$\nThere is nothing to put on the right side of the pairs.  So there are no functions $A\\to\\varnothing$.   (There is one exception to this claim, which we will see in a minute.)\nWhat if $A$ is empty and $B$ is not, say $\\{b_1, b_2, \\ldots\\}$?  A  function $A\\to B$ is a set of pairs that has exactly one pair for each element of $A$.  But $A$ has no elements.  No problem, the function has no pairs! $$\\{\\}$$\nA function is a set of pairs, and the set can be empty.  This is called the \u201cempty function\u201d.  When $A$ is the empty set, there is exactly one function from $A\\to B$, the empty function, no matter what $B$ is.  This is the definition of \u201cinitial\u201d, so the empty set is initial.\nDoes the empty set have an identity morphism?  It does; the empty function $\\{ \\}$ is its identity morphism.  This is the one exception to the claim that there are no functions from $A\\to\\varnothing$: if $A$ is also empty, the empty function is such a function, the only one.\n\nThe issue for topological spaces is exactly the same:\n\nWhen $B$ has only one element, there is exactly one continuous map  $A\\to B$ for every $A$.\nWhen $A$ is empty, there is exactly one continuous map $A\\to B$ for every $B$: the empty function is the unique map.\nWhen $A$ has only one element, there are usually many continuous maps $A\\to B$, one different one for each element of $B$.\n\nThere are categories in which the initial and terminal objects are the same.  For example, in the category of groups the trivial group (with one element) is both initial and terminal.\nI hope this was some help.", "meta": {"post_id": 4235989, "input_score": 15, "output_score": 47, "post_title": "I don't understand the difference between initial and terminal objects"}}
{"input": "Question\nA lot of things I'm trying to prove just now are turning into \"notational hell\", which I think makes them very hard to read. I've tried to cut down on this by assuming my reader will understand what definitions are in play, modularising my proofs and skipping explanation of steps that I hope are obvious. I've also tried relabeling formulae with short names (i.e., $\\def\\val#1{V_\\pli(#1)}\\def\\p{\\phi}\\def\\q{\\psi}\\def\\s{\\vDash_{\\tiny\\text{PL}}}\\def\\ns{\\nvDash_{\\tiny\\text{PL}}}\\def\\pli{\\mathscr{I}}\\def\\aa{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q}\\def\\ab{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\ns\\q}\\def\\ba{\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}\\def\\bb{\\ns\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots)):=\\,\\s Q),$ but for proofs of any length it seems to be more confusing than helpful. How do I make proofs more readable without sacrificing clarity?\n\nExample Proof\nLet $\\p$ and $\\q$ be wffs and $n\\in\\mathbb{N}$ (please note that $0\\not\\in\\mathbb{N}$). We want to show $\\def\\p{\\phi}\\def\\q{\\psi}\\def\\s{\\vDash_{\\tiny\\text{PL}}}\\def\\ns{\\nvDash_{\\tiny\\text{PL}}}\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q$ iff $\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))$.\n\nIn (L1) I prove both directions of the biconditional, which I don't think I need to do because we're dealing with \"=\" - is this correct? I also think that (L1) is so basic that \"by inspection\" is appropriate - is that fair?\n\nLemma 1 (L1)\nWe want to show by induction that for some PL-interpretation, $\\pli,$ $\\val{\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}=0$ iff $\\val{\\p_n}=\\val{\\p_{n-1}}=\\cdots=\\val{\\p_1}=1$ and $\\val{\\q}=0$.\nBase Case\n\nIf $\\val{\\p\\to\\q}=0$ then, by definition, $\\val{\\p}=1$ and $\\val{\\q}=0$. If $\\val{\\p}=1$ and $\\val{\\q}=0$, then, by definition, $\\val{\\p\\to\\q}=0$\n\nInduction Hypothesis (IH)\n\nAssume for some arbitrary $k\\in\\mathbb{N}$ that $\\val{p_k\\to(\\p_{k-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}=0$ and $\\val{\\p_k}=\\val{\\p_{k-1}}=\\cdots=\\val{\\p_1}=1$ and $\\val{\\q}=0$\n\nInduction Step\n\nIf $\\val{\\p_{k+1}\\to(p_k\\to(\\p_{k-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots)))}=0,$ then, as we know $\\val{p_k\\to(\\p_{k-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}=0$ from the (IH), $\\val{\\p_{k+1}}=1$. From the (IH) $\\val{\\p_k}=\\val{\\p_{k-1}}=\\cdots=\\val{\\p_1}=1$ and $\\val{\\q}=0$, thus $\\val{\\p_{k+1}}=\\val{\\p_k}=\\val{\\p_{k-1}}=\\cdots=\\val{\\p_1}=1$ and $\\val{\\q}=0$\n\nLet $\\val{\\p_{k+1}}=1$. From the (IH) $\\val{\\p_k}=\\val{\\p_{k-1}}=\\cdots=\\val{\\p_1}=1,$ $\\val{\\q}=0,$ and $\\val{p_k\\to(\\p_{k-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}=0,$ thus $\\val{\\p_{k+1}\\to(p_k\\to(\\p_{k-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots)))}=0$\n\n\nProof of First Direction (P1)\n\nFor reductio, suppose it is not the case that $\\aa\\implies\\ba$\n\nIt follows from (1) that there exists an $\\def\\pli{\\mathscr{I}}\\pli$ such that $\\def\\aa{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q}\\def\\ab{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\ns\\q}\\def\\ba{\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}\\def\\bb{\\ns\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}\\aa$ and $\\bb$\n\nIt follows from (2) that $\\def\\val#1{V_\\pli(#1)}\\val{\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}=0$\n\nFrom (L1), the valuation on (3) can only occur when $\\val{\\p_n}=\\val{\\p_{n-1}}=\\cdots=\\val{\\p_1}=1$ and $\\val{\\q}=0$\n\nIt follows from (4) that $\\ab$, which contradicts (2) and proves our first direction\n\n\nProof of Second Direction (P2)\n\nFor reductio, suppose it's not the case that $\\ba\\implies\\aa$\n\nIt follows from (1) that there exists an $\\pli$ such that $\\ba\\text{ and }\\ab$\n\nFrom (2) we have $\\ab$, thus, $\\val{\\p_n}=\\val{\\p_{n-1}}=\\cdots=\\val{\\p_1}=1$ and $\\val{\\q}=0$\n\nIt follows from (3) and (L1) that $\\bb\\text{,}$ which contradicts (2) and proves our second direction\n\n\n(P1) and (P2) prove both directions of the biconditional, hence $\\def\\p{\\phi}\\def\\q{\\psi}\\def\\s{\\vDash_{\\tiny\\text{PL}}}\\def\\ns{\\nvDash_{\\tiny\\text{PL}}}\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q$ iff $\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))\\,\\square.$", "output": "Main suggestion\nInstead of\n\nWe want to show $\\def\\p{\\phi}\\def\\q{\\psi}\\def\\s{\\vDash_{\\tiny\\text{PL}}}\\def\\ns{\\nvDash_{\\tiny\\text{PL}}}\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q$ iff $\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))$\n\ntry it like this:\n\nWe want to show that $$\\def\\p{\\phi}\\def\\q{\\psi}\\def\\s{\\vDash_{\\tiny\\text{PL}}}\\def\\ns{\\nvDash_{\\tiny\\text{PL}}}\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q \\tag{1}$$ if and only if\n$$\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots)).\\tag{2}$$\n\nAfter that, instead of repeating the long formulas every time, just call them $(1)$ and $(2)$:\n\nFor reductio, suppose it's not the case that $(2)\\implies (1)$.  Then there must exist an $\\mathscr I$ such that $(2)$ holds but $(1)$ does not.\n\nLesser suggestions\n\nAbbreviate $$\\phi_n\\to(\\phi_{n-1}\\to(\\cdots\\to(\\phi_1\\to\\q)\\cdots))$$ as $$\\Phi_n.$$  (Don't use $Q$.  Why would you use $Q$?)\nInstead of $$V_\\mathscr{I}({p_k\\to(\\p_{k-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))})=0$$ you can now write $$V_\\mathscr{I}({p_k\\to\\Phi_{k-1}})=0$$ and the reader won't miss that the first variable is a  $p$ and not a $\\phi$.\nYou said abbreviating seems \u201cmore confusing than helpful\u201d.  It's not.\n\nAbbreviate $\\phi_1,\\phi_2,\\ldots,\\phi_n$ as $\\vec\\phi$.\n\nAbbreviate $$V_\\mathscr{I}(\\phi_n)=V_\\mathscr{I}(\\phi_{n-1})=\\cdots=V_\\mathscr{I}(\\phi_1)=1$$ as $$V_\\mathscr{I}(\\phi_i) = 1\\quad (i=1\\ldots n)$$ or perhaps $$V_\\mathscr{I}(\\phi_{1\\ldots n}) = 1.$$\n\nYou're abbreviating the wrong things.  You don't need to abbreviate \u201cif and only if\u201d as \u201ciff\u201d, or \u201cLemma 1\u201d as \u201cL1\u201d. The goal here is not to remove all the normal English from your proof.  These abbreviations are more confusing than helpful.\n\n\nDon't make the reader compare two long formulas to make sure they are the same, or to wonder why they are not.  Design your notation to highlight the differences between similar formulas.\nNotation, like language, is flexible. There are no rules; you are allowed to make things up.  $\\vec\\phi$ is not really a vector.  It doesn't matter. You can explain it briefly: \u201cWe will abbreviate $\\phi_1,\\phi_2,\\ldots,\\phi_n$ as $\\vec\\phi$.\u201d  Nobody will be confused or forget what it means.  My suggestion $V_\\mathscr{I}(\\phi_{1\\ldots n})$ is not standard.  It doesn't matter; the meaning is clear.\nOrthogonal suggestions\n\nYou're not using TeX correctly.  You don't need to keep repeating \\defs.  Once you \\def a new control sequence, the definition remains in force until the end of the group, or the document.  Define the important macros once, at the beginning of the file, or in an \\included file.\n\nDefine better macros.  The structure of the macros should follow the syntactic structure of your formulas.  Instead of typing out\n\\def\\aa{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\s\\q}\n\\def\\ab{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n\\ns\\q}\n\\def\\ba{\\s\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))}  \n\\def\\bb{\\ns\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\q)\\cdots))\n\ntry it this way:\n\\def\\ps{\\p_1,\\,\\p_2,\\,\\ldots,\\,\\p_n}\n\\def\\aa{\\ps\\s\\psi}\n\\def\\ab{\\ps\\ns\\psi}\n\\def\\pformn{\\p_n\\to(\\p_{n-1}\\to(\\cdots\\to(\\p_1\\to\\psi)\\cdots))}\n% now you don't need \\ba or \\bb, just use \\s\\pformn and \\ns\\pformn", "meta": {"post_id": 4256057, "input_score": 42, "output_score": 68, "post_title": "How do I make proofs with long formulae more readable without sacrificing clarity?"}}
{"input": "Question. If I attempt to prove that space $X$ is complete by pursuing the strategy, \u201cAssume $x_n \\rightarrow x$; the space $X$ is complete if $x \\in X$,\u201d then why is that wrong?\n\nContext. I know the definition of Cauchy sequences and convergent sequences, and that the definition of completeness is that Cauchy sequences in the space converge. And so I know that if one is attempting to prove that a space is complete, then the usual proof should start, \u201cAssume that $x_n$ is a Cauchy sequence; we will show that $x_n$ converges in $X$.\u201d\nThe misconception I seem to be battling is this: It seems like a Cauchy sequence must converge to something, just that the something might not be in the space. So it seems to me like the question really is, \u201cIs the limit in the space or is it not in the space?\u201d The classic example is the sequence of rationals that converges to $\\sqrt{2}$. The sequence is Cauchy within the space of the rationals, and also the sequence does converge, just to a limit that is outside the space in consideration. So recently, I began a proof of completeness with the line, \u201cAssume $f_n \\rightarrow f$. We want to show that $f \\in X$.\u201d And the feedback was, \u201cUnclear what is being proved. Nothing related to completeness. 0/4 points.\u201d It seems to me that showing that the limit of a convergent sequence resides in the space is equivalent to saying that Cauchy sequences converge. Why is that wrong?\nThank you!", "output": "You are correct in the narrow sense that every Cauchy sequence does converge in some space.\nTo be precise, let $(X;d_1)$ be any metric space with at least two points, let $Y$ be the set of Cauchy sequences in $X$, and define $d_2:Y^2\\to\\mathbb{R}$; $$d_2(\\{x_n\\}_n,\\{y_n\\}_n)=\\lim_{n\\to\\infty}{d_1(x_n,y_n)}$$  Then it is easy (well, a decent homework problem, anyways) to verify that\n\n$Y$ is not a metric space under $d_2$; different points of $Y$ might be distance-$0$ from each other.\nFor each $y\\in Y$, there exists an equivalence class $c(y)=\\{z:d_2(y,z)=0\\}$.  Let $Z$ be the set of all equivalence classes, i.e. $Z=\\{c(y):y\\in Y\\}$.  Then $d_2$ extends to $Z^2\\to\\mathbb{R}$ in the natural way.\n$(Z;d_2)$ is a metric space.\n$(X;d_1)$ embeds homeomorphically into $(Z;d_2)$ via $x\\mapsto c(x,x,x,\\dots)$.\n$(Z;d_2)$ is complete.\n\nThus if we identify $X$ with the embedded subspace of $Z$, then any Cauchy sequence in $X$ converges in $Z$.  The end limit might be $X$, or it might not; to show $X$ complete is to show that the end limit is in fact in $X$.\nFor this reason, $Z$ is called the completion of $X$.\nWith that said, some space is much more general than you give it credit for.\nGiven your notation ($f_n\\to f$), I think you assumed that the limit of a Cauchy sequence of functions is itself a function.  Probably $X$ was the space of continuous functions under the uniform norm, and so you thought that you just had to prove $f$ was itself a continuous function.\nBut $f$ could be far worse!  For example, let $X$ be the space of continuous functions on $[0,1]$ with the following norm: $$d_1(f,g)=\\int_0^1{|f(x)-g(x)|\\,dx}$$  Then $X$ has a well-known completion: $L^1([0,1])$, the space of Lebesgue-integrable functions on $[0,1]$, up to a.e. equivalence.\nIf you haven't seen the Lebesgue integral, don't worry; the pathology carries over to Riemann-integrable functions.  In particular, one can show that any piecewise-continuous function lies in the equivalent of $(Y;d_2)$ (to borrow notation from above).[*]  So, for any $r$, the function $\\delta_r$ where $$\\delta_r(x)=\\begin{cases}r&x=0\\\\0&x\\neq0\\end{cases}$$  is in $Y$.\nBut all those functions collapse to the same point in $(Z;d_2)$.  That is, given any convergent sequence $f_n$, we can find some $f_{\\infty}$ such that $f_n\\to f_{\\infty}$\u2026and $f_n\\to f_{\\infty}+\\delta_r$ too!\nSo we can't define an evaluation map to describe $f_{\\infty}(0)$.  Indeed, for any fixed point $x$, $f_{\\infty}(x)$ is not well-defined.\n[*]: As Noiralef pointed out in comments, I'm cheating here.  I previously defined $Y$ as an equivalence set of Cauchy sequences; now I'm saying a function $\\delta_r$ is in $Y$.  What I mean here is, there exists a (uniformly bounded) Cauchy sequence of continuous functions converging pointwise to $\\delta_r$.", "meta": {"post_id": 4273341, "input_score": 80, "output_score": 91, "post_title": "Does every Cauchy sequence converge to *something*, just possibly in a different space?"}}
{"input": "I was reading Maxwell's relations and came across: $$\\oint pdV=\\oint TdS\\Rightarrow \\iint dpdV=\\iint dTdS.$$\nI know this is straightforward to see since they both represent the surface area, but I've never seen a math theorem on textbooks that indicates $$\\oint ydx=\\iint dxdy.$$ Is this just a trivial corollary?", "output": "This is Stokes' theorem,\n$$ \\int_S \\mathrm{d}\\omega = \\int_{\\partial S} \\omega$$\nfor a 1-form $\\omega = y\\mathrm{d}x$, $S$ a surface and $\\partial S$ its boundary.", "meta": {"post_id": 4392567, "input_score": 13, "output_score": 40, "post_title": "Is there a math theorem by which a contour integral is equal to a double integral?"}}
{"input": "I am reading Peter Smith's An Introduction to G\u00f6del's Theorems. In chapter 10, he defines \"baby arithmetic\" $\\mathsf{BA}$ to be the zeroth-order version of Peano arithmetic ($\\mathsf{PA}$) without induction. That is, $\\mathsf{BA}$ is the zeroth-order theory (meaning there are no quantifiers or variables) with primitive constant symbol $\\mathsf0$, unary function $\\mathsf S$, binary functions $+$ and $\\times$, and the following axiom schemas:\n\n$\\mathsf{Sn\\neq0}$\n$\\mathsf{(Sn=Sm)\\to(n=m)}$\n$\\mathsf{m+0=m}$\n$\\mathsf{m+Sn=S(m+n)}$\n$\\mathsf{m\\times0=0}$\n$\\mathsf{m\\times Sn=(m\\times n)+m}$\n\nThe symbols $\\mathsf{n}$ and $\\mathsf{m}$ appearing in the schemas are placeholders for any term of the theory, where terms are defined via the standard recursive definition for predicate logic.\nMy question is, does $\\mathsf{PA}\\vdash\\mathsf{Con(BA)}$? Note that this is a very concrete question about whether the arithmetical formula $\\mathsf{Con(BA)}$ is formally derivable in $\\mathsf{PA}$, not to be confused with more philosophical questions about whether $\\mathsf{PA}$ itself is consistent, discussed elsewhere.\nG\u00f6del's second incompleteness theorem shows that $\\mathsf{PA}$ cannot prove $\\mathsf{Con(PA)}$. Meanwhile we cannot even ask whether $\\mathsf{BA}$ can prove $\\mathsf{Con(BA)}$ because $\\mathsf{Con(BA)}$ involves quantifiers and is therefore not even in the language of $\\mathsf{BA}$. But it seems plausible that $\\mathsf{PA}$ could prove $\\mathsf{Con(BA)}$, which would be a nice, reassuring result at the very least.\n\nAs a footnote, I'll say that part of my motivation for this question is indeed wondering about whether $\\mathsf{PA}$ is consistent. A common argument goes something like: \"If you believe $\\mathsf{PA}$ formalizes valid reasoning about arithmetic, then you believe what it proves. Hence it cannot prove $\\mathsf{0=1}$, because $0$ is not really equal to $1$. Hence you believe $\\mathsf{PA}$ is consistent.\" But I think this argument would be more secure if it didn't rest on definite facts about \"true arithmetic,\" and in particular the assumption that \"$0$ is not actually equal to $1$,\" but instead referred only to $\\mathsf{BA}$. Thus, in this light, it is important to know whether $\\mathsf{BA}$ proves $\\mathsf{0=1}$, i.e., whether $\\mathsf{BA}$ is consistent.\nI'm not putting this here to start a huge philosophical discussion/argument, just to provide motivation.", "output": "$\\mathsf{PA}$ has a very interesting property, namely that it proves the consistency of each of its finitely axiomatizable subtheories. This is usually called the reflection principle for $\\mathsf{PA}$ (and incidentally the same result holds for $\\mathsf{ZFC}$). The theory $\\mathsf{BA}$ is the quantifier-free part of a finitely axiomatizable theory (just throw on \"$\\forall x$\"s everywhere) $\\mathsf{BA}'$; consequently, we do in fact have $\\mathsf{PA}\\vdash \\mathsf{Con(BA)}$.\n\nWe have to be careful here: $\\mathsf{PA}$ does not prove \"Every finite subtheory of $\\mathsf{PA}$ is consistent.\" Rather, for each finite subtheory $T$ of $\\mathsf{PA}$, $\\mathsf{PA}$ proves \"$T$ is consistent.\" So we don't get a $\\mathsf{PA}$-proof of $\\mathsf{Con(PA)}$ itself from the reflection principle. ($\\mathsf{PA}$ also proves \"$\\mathsf{PA}$ proves the consistency of each finite subtheory of $\\mathsf{PA}$,\" but again this falls short of actually being a problem.)\n\nAdmittedly, this is massive overkill, but the reflection principle is a very cute trick that's worth knowing. With more care we can prove the consistency of $\\mathsf{BA}$ in the very weak fragment $\\mathsf{I\\Sigma_1}$, or indeed much less (although when we go below $\\mathsf{I\\Sigma_1}$ things often get finicky so I usually don't).", "meta": {"post_id": 4421760, "input_score": 29, "output_score": 35, "post_title": "Can Peano arithmetic prove the consistency of \"baby arithmetic\"?"}}
{"input": "Is every $3$-dimensional vector $v$ with integer coordinates a cross product of two other vectors with integer coordinates?\n\nI have written a program to check for $v$ with entries between $-7$ and $7$. Every $v$ that small can be expressed as a cross product of two other vectors with integer coordinates.\nBut I can't come up with a general proof.\nApart from the empirical evidence from my experiment on small $v$, another reason to think this is true is that it's almost enough to find two small independent vectors, $u$ and $w$, with integer coordinates that are perpendicular to $v$. Playing around with integer relation algorithms has taught me that such $u$ and $w$ should be plentiful. The cross product of $u$ and $w$ is a scalar multiple of $v$ - call it $kv$. $k$ is an integer; in most cases $|k| = 1$. If it isn't, pick a different $u$ and $w$.\nA similar but much easier question was this: Is every vector in $\\Bbb R^3$ a cross product?.\nNote: The answer given here was used to solve Diophantine equations so the question is about number theory.", "output": "Let us write $v=(a, b, c)$, and consider the three vectors\n$$ w_1=(0, c, -b), \\quad w_2=(-c,0,a), \\qquad w_3=(b, -a, 0). $$\nNote that $w_1 \\times w_2=cv$, $w_1 \\times w_3=-bv$. This way, let $d=\\gcd(b, c)$, and write it as $d=\\lambda b+\\mu c$, so that\n$$ \\frac{w_1}{d} \\times\\left( \\mu w_2-\\lambda w_3 \\right)=v. $$\nHere $\\mu w_2-\\lambda w_3$ and $w_1/d$ have integer coordinates, as we can easily check.", "meta": {"post_id": 4428714, "input_score": 33, "output_score": 38, "post_title": "Is every vector in $\\mathbb Z^3$ a cross product?"}}
{"input": "I am recently reading about Fourier transforms and convolutions. It was a surprise to me that it takes quite several paragraphs to prove the measurability of innocent looking $f(x-y)$ (reference: proof that $\\hat{f}(x,y)=f(x-y)$ is measurable if $f$ is measurable, Stein & Shakarchi Prop 3.9). It makes me wonder:\nDoes it ever happen  in history that mathematicians publish wrong results because they assumed measurability of some (innocent looking) sets  or functions?", "output": "There is the famous mistake of Lebesgue. Quoting from Descriptive Set Theory: Second Edition by Yiannis N. Moschovakis , page 2:  \"Lebesgue's argument was \u201csimple, short but false.\u201d The wrong step in the proof was hidden in a lemma taken as (basically) trivial, that a set in the line which is the projection of a Borel measurable set in the plane is itself Borel measurable. Ten years later the error was spotted by Suslin, then a young student of Lusin at the University of Moscow, who rushed to tell his professor in a scene charmingly described in Sierpinski [1950].  Suslin called the projections of Borel sets analytic and showed that indeed there are analytic sets which are not Borel measurable. Together with Lusin they quickly established most of the basic properties of analytic sets...\"\nMikhail Suslin died before his 25th birthday, and only published a few pages during his lifetime, but his discovery of Lebesgue's error and the fundamental way he addressed it, laid the foundation of descriptive set theory.\nhttps://en.wikipedia.org/wiki/Mikhail_Suslin\nhttps://en.wikipedia.org/wiki/Analytic_set\nEdit: Following a question from @KurtG. in the comments, I add below the precise relevant quote from Sierpinski (1950) (http://www.numdam.org/item/MSM_1950__112__1_0.pdf):\n\"Par hasard j'\u00e9tais pr\u00e9sent au moment o\u00f9 Michel Souslin communiqua \u00e0 M. Lusin sa remarque et lui donna le manuscrit de son\npremier travail. C'est tout simplement que M. Lusin a trait\u00e9 le\njeune \u00e9tudiant qui lui a d\u00e9clar\u00e9 avoir trouv\u00e9 une faute dans un\nM\u00e9moire d'un savant \u00e9minent. Je fus aussi un des premiers qui,\nimm\u00e9diatement apr\u00e8s M. Lusin \u00e0 lu les manuscrits de Michel Souslin ;\nje sais donc bien combien M. Lusin a aid\u00e9 son \u00e9l\u00e8ve et comme il le\nguidait dans ses recherches. Les ensembles analytiques sont appel\u00e9s\npar plusieurs auteurs ensembles de Souslin : il serait plus juste de\nles appeler ensembles de Souslin et Lusin.\nM. Souslin ne se contenta pas de constater que le lemme de\nM. Lebesgue est faux. Il se mit \u00e0 examiner si les cons\u00e9quences que\nM. Lebesgue en a d\u00e9duites \u00e9taient vraies. Une d'elles \u00e9tait la proposition de M. Lebesgue qu'une projection sur une droite d'un ensemble\nplan mesurable B est mesurable B [ce qui r\u00e9sulterait imm\u00e9diatement\ndu (faux) lemme de M. Lebesgue sur la projection d'un produit\ndescendant d'ensembles, vu que la projection d'une somme (quelconque) d'ensembles est la somme des projections de ces ensembles\net que la projection d'un rectangle est un segment]. Pour construire\nun exemple d'un ensemble plan mesurable B dont la projection est\nnon mesurable B, M. Souslin a cr\u00e9\u00e9 toute une th\u00e9orie qu'il appela\nth\u00e9orie des ensembles (A) (analytiques).\nCette th\u00e9orie a \u00e9t\u00e9 ensuite simplifi\u00e9e et d\u00e9velopp\u00e9e par M. Lusin\nqui a aussi d\u00e9montr\u00e9 \u00e0 l'aide d'elle que le th\u00e9or\u00e8me de M. Lebesgue\nsur l'inversion des fonctions repr\u00e9sentatives analytiquement, bien\nque d\u00e9duit d'un lemme faux, est cependant vrai.\"\nApproximate English translation:\n\"By chance I was present when Mikhail Souslin communicated his remarks to Mr. Lusin and gave him the manuscript of his\nfirst work. Mr. Lusin took quite seriously the\nyoung student who told him that he had found a fault in a\nMemoir of an eminent scholar. I was also one of the first who,\nimmediately after Mr. Lusin, read the manuscript  of Mikhail Souslin; I therefore know well how much Lusin helped his pupil and how he guided his research. Several authors use the term \"Souslin sets\" for the class of analytic sets; I believe it would be fairer to call them Souslin-Lusin sets.\nM. Souslin was not satisfied with noting that the lemma of\nH. Lebesgue is wrong. He began to examine whether the consequences Lebesgue inferred from the lemma were true. One of them was Lebesgue's proposition that a projection on a line L of a Borel set in the plane, is a Borel set in L [which would immediately result from the (false) lemma of H. Lebesgue on the projection of the intersection of a descending sequence of sets, since the projection of any union of sets is the union of the projections of these sets, and the projection of a rectangle is a segment].\nTo build an example of a Borel measurable planar set whose projection is not a Borel set, Mr. Souslin created a whole theory which he called the theory of analytic (A) sets.\nThis theory was then simplified and developed by Lusin,\nwho also demonstrated, with Suslin's assistance, that the theorem stated by Lebesgue  concerning implicitly analytically definable functions is true, although it was originally inferred from a false lemma.\"", "meta": {"post_id": 4445456, "input_score": 32, "output_score": 43, "post_title": "Historical Mistake of Assuming Measurability"}}
{"input": "We have a special function $S$ from the real interval $[0, 1)$ to the real-interval $(0, 1]$ which I will define near the end of this post.\nSomeone claims that the following proof-schema is valid:\n\nWe wish to prove some statement $P(x)$ for each real number in the closed interval $[0, 1]$.\nFirst, we show that the statement $P(0)$ is true.\nNext we let $x$ be an arbitrary element of the interval  $[0, 1]$. We assume that $P(x)$ is true, and then we prove that $P \\begin{pmatrix} S(x) \\end{pmatrix}$ is also be true.\nWe conclude that $\\forall x \\in [0, 1], P(x)$\n\nAn answer to this stack-exchange question is a proof that the above proof-schema is correct or incorrect.\nIf you think that induction on real-numbers is not valid, then an answer to this question is a proof of the existence of a predicate $P$ such that:\n\n$P(0)$ is true.\n$\\exists x \\in (0, 1]$ such that $P(x)$ is false.\nThere is no real number $x$ in the interval $[0, 1)$ such that $P(x)$ is true and $P(S(x))$ is false. Equivalently, show that $\\forall x \\in [0, 1)$ if the statement $P(S(x))$ is false then the statement $P(x)$ is also false.\n\nInformal Description of function $S$\nInformally,you can compute $S(x)$ by the following procedure:\n\nSTEP ONE : Get a decimal-expansion of $x$. Look only at digits to the right of the decimal point. Go from left to right until you find a digit which is not nine. For example, if $x = 0.9990123$ then the left-most digit which is not a nine is a zero. Now add one to the digit which is not a nine. After adding $1$ to the left-most-non-nine digit in $0.9990123$ we get $0.9991123$.\nSTEP TWO: Replace the leftmost nines with zeros. if $x = 0.9990123$ then $S(x) = 0.0001123$\n\n\n\n\n\nApproximation of $x$\nApproximation of $S(x)$\n$x$\n$S(x)$\n\n\n\n\n$0.99531416$\n$0.99631416$\n$ 0.995 + \\frac{pi}{10^{-4}}$\n$ x + 10^{-3}$\n\n\n$0.141421356237$\n$0.241421356237$\n$\\frac{\\sqrt{2}}{10}$\n$ 0.1 + \\frac{\\sqrt{2}}{10}$\n\n\n$0$\n$0.1$\n$0$\n$0.1$\n\n\n$0.1$\n$0.2$\n$0.1$\n$0.2$\n\n\n$0.1$\n$0.2$\n$0.1$\n$0.2$\n\n\n$0.2$\n$0.3$\n$0.2$\n$0.3$\n\n\n$0.3$\n$0.4$\n$0.3$\n$0.4$\n\n\n$0.999991$\n$0.999992$\n$0.999991$\n$0.999992$\n\n\n$0.93$\n$0.94$\n$0.93$\n$0.94$\n\n\n$0.999999999 \\dots 0000 \\dots$\n$0.000000000 \\dots 10000 \\dots$\n$1- 10^{-100}$\n$10^{-101}$\n\n\n\n\nIf $S(x) = 1$ then $x = 0.9$\nIf $S(x) = 0.9$ then $x = 0.8$\nIf $S(x) = 0.8$ then $x = 0.7$\n$$\\dots$$\nIf $S(x) = 0.2$ then $x = 0.1$\nFormal Definition of Function $S$\nLet $S$ be a function from the real interval $[0, 1)$ to the real-interval $(0, 1]$ such that $\\forall x \\in [0, 1)$, $S(x) = x  +10^{-(1 + g(x))} + 10^{-g(x)}$.\nFunction $g$ is defined as follows:\n$$\\forall x \\in [0, 1], g(x) =\n\\begin{cases}\n1,  & \\text{if  } \\lfloor 10* x \\rfloor \\neq 9 \\\\\n1 + g(10*x - \\lfloor 10* x \\rfloor), & \\text{ otherwise }\n\\end{cases}$$\nSome Notes About Function $S$\nNote 1: Irrational Inputs like $\\pi$ are Okay\n$S(x)$ is well-defined for irrational inputs such as $x = \\pi$ and $x = \\sqrt{2}$\nNote 2: What is $\\lfloor 10* x \\rfloor \\neq 9$?\nNote that $\\lfloor 10* x \\rfloor \\neq 9$ if and only if the left-most digit is $9$. For example, $\\frac{\\pi}{10}$ is approximately $0.31459$, which has a $3$ as its left-most digit. So, $\\lfloor 10*\\frac{\\pi}{10} \\rfloor \\neq 9$\n$\\lfloor x \\rfloor$ is the \"floor\" function.\nWhat if the $9$s never end?\nNote that the only real number in the interval $[0, 1)$ which has only nines in its decimal-expansion is the number $1$ which can be expanded as $0.999999 \\dots$. However, $1$ is not a valid input to function $S$. All numbers in $[0, 1]$ besides the number $1$ have at least one non-nine digit. As such, we can always find the left-most digit which is not-a-nine.\nA Different Informal way to Compute $S(x)$\nInformally, we can:\n\nFind a decimal expansion of real number $x$\nDelete the decimal point and write the digits in reverse order. For example, if $x = \\sqrt{2} \u2248 0.1414213 \\dots$ then write $\\dots 3124141$\nAdd one to the result from step $2$ as if the step $2$ result was a natural number (it is not actually a natural. The result is not eventually all zero as you move leftward).\nRe-reverse the digits.\n\nNote that the result step $2$ is NOT a natural number. The result of step $2$ is a function $F$ from $\\mathbb{N}$ to the digits $\\begin{Bmatrix} 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\\end{Bmatrix}$ such that we probably have: $\\not\\exists n \\in \\mathbb{N}: \\forall m \\geq n, F(m) = 0$\nFor example if $x = \\frac{\\pi}{10}$ then:\n\n$F(1) = 3$\n$F(2) = 1$\n$F(3) = 4$\n$F(4) = 1$\n$F(5) = 5$\n$F(6) = 9$\nIn general, $F(k)$ is one of the digits of the decimal expansion of $\\pi$.", "output": "This is obviously invalid regardless of what $S$ is: let $$\\mathscr{X}=\\{0,S(0),S(S(0)),...\\}=\\{S^i(0): i\\in\\mathbb{N}\\}$$ (using the convention that $0\\in\\mathbb{N}$ and $S^0(x)=x$ here), consider the property $P(x):=x\\in\\mathscr{X}$, and remember that $[0,1]$ is uncountable.\n\nThere is a type of argument which works on $[0,1]$ and is arguably \"induction-like,\" but it's quite different - see these notes of Pete Clark.", "meta": {"post_id": 4515842, "input_score": 11, "output_score": 70, "post_title": "Can you prove that proof-by-induction is invalid for the real interval [0, 1]?"}}
{"input": "I'm currently at a stage where I think I'm quite comfortable with the appearance of local non-archimedean fields in the maths I encounter, having seen a fair bit of technology built upon their structure and applications to connected areas, yet I somehow still feel like I have an unsatisfying understanding of why their introduction is absolutely crucial to study algebraic number theory and arithmetic geometry, especially since all these applications are hugely advanced compared to the point at which $p$-adic numbers are usually introduced in a student's career I think. If I want to motivate their definition, I sort of naturally go through the following implications in my head:\n\nto study a problem over the ring of integers $\\mathbb{Z}$ (or more generally, the ring of integers of a number field) the strategy is to localise the problem at a prime $p$, as to focus around it and worry about the problem 'one point at a time', so-to-speak;\nwe (literally, now) localise at the prime $p$, just as one would do with the ring of regular functions on a variety, and replace $\\mathbb{Z}$ with the ring $\\mathbb{Z}_{(p)} := \\{ \\frac{x}{y} \\in \\mathbb{Q} \\mid p\\nmid y \\}$ ;\nwe can then complete at the maximal ideal $(p) \\subseteq \\mathbb{Z}_{(p)}$ to obtain the $p$-adic integers $\\mathbb{Z}_p$, with the benefit that now there's access to approximation techniques such as Hensel's lemma, and the newly obtained ring has pretty much all the same algebraic properties as $\\mathbb{Z}_{(p)}$ because of its identical valuation theory.\n\nIt's this last step which still confuses me... even though I can appreciate the utility of approximation techniques, it still feels extremely arbitrary why it turns out to be so inevitable with all the theory that builds upon this measly little step. Somehow, every time I've seen the study of formal neighbourhoods in algebraic geometry it always feels like it's a tool to tackle a problem, and not the main object of study, whereas in my head the $p$-adic numbers have turned out to be the main character in many areas of mathematics, which sort-of goes against this intuition of mine I find.\nSince this intuition really doesn't come from any of my teachers and I'm not quite sure it makes much sense, I wanted to ask if it's an apt way to think about the use of non-archimedean fields in mathematics; I'd also be very interested in learning about their role in the history of algebraic number theory since I'm not quite sure I can properly place their use and introduction on a timeline in a coherent way with all of the theory I have in mind.\nI apologise if my question is very hand-wavy, and I'd be super grateful for any sort of insight :) Thank you very much for your time!!", "output": "Let me explain how I think about one way in which $\\mathbb{Q}_p$ (or more to the point $\\mathbb{Z}_p$) appears from an arithmetic-geometric perspective. Let me approach this from a vantage point that should be understandable to any advanced undergraduate. In particular, let me take 'solving Diophantine equations' as the goal.\n(For a parallel and more in-depth discussion, you can read this extended version of a talk I gave. For a perspective more in-line with that discussed by the user hunter you can look at these old notes of mine [Disclaimer: these are quite old and so I can't vouch for their correctness, mathematically of philosophically])\n\nThe general motivation\nThe basic idea stems from the following credo which one encounters ad nauseam in their mathematical education. For a 'geometric space' $X$\n$$\\left\\{\\text{Problems on }X\\right\\}=\\left\\{\\left(\\begin{matrix}\\text{Problems locally}\\\\ \\text{on }X\\end{matrix}\n\\,\\,,\\,\\,\\begin{matrix}\\text{Gluing}\\\\ \\text{data}\\end{matrix}\\right)\\right\\}.$$\nThis breaks questions on $X$ into two (hopefully) more managable pieces.\nGood examples of this are:\n\n(Diff) if $X$ is some manifold and I want to solve the differential equation $Df=0$, I can try to find solutions to $Df=0$ locally on $X$, and then glue them together to a global solution,\n(Sect) if $f\\colon Y\\to X$ is a map of 'geometric spaces' and I want to find a section $s\\colon X\\to Y$, I can try to find a section locally on $X$ and then glue these local sections together.\n\nRemark 1:\n\n The second example is really the representative one, and morally the notion of 'moduli spaces' should tell you that all examples should be of this form. For instance, it's a good exercise to think how one might put the first example in this form!\n\nOf course, this idea really only works with the following serious caveat: the local 'geometry' of $X$ is simple(r) enough to make solving the problem locally on $X$ (more) tenable.\nA geometric perspective of solving equations\nSo, how does this relate to solving Diophantine equations or, for that matter, your question?\nThe beautiful idea of Grothendieck and his collaborators (building off of the work of many others) shows that solving equations can, in fact, be put into the context of (Sect). Namely, there is associated to any ring $R$ a geometric object $\\mathrm{Spec}(R)$, a so-called locally ringed space (the category of which we shall denote $\\mathbf{LRS}$), such that\n$$\\mathbf{Ring}\\to \\mathbf{LRS},\\qquad R\\mapsto \\mathrm{Spec}(R)$$\nis a (contravariant) fully faithful embedding. This is a bit abstract, so before we explain how this relates to solving equations, let us spell this out a little more precisely.\nThe object $\\mathrm{Spec}(R)$, being a locally ringed space, means it comes with two bits of structure:\n\nan underlying topological space and,\na 'sheaf of functions' $\\mathcal{O}$ which assigns to any open subset $U$ the ring $\\mathcal{O}(U)$ of 'functions on $U$'.\n\nThe underlying topological space $\\mathrm{Spec}(R)$ can be described very explicitly. As a set we take\n$$\\mathrm{Spec}(R)=\\{\\mathfrak{p}\\subseteq R\\text{ a prime ideal}\\}.$$\nThe topology on $\\mathrm{Spec}(R)$, called the Zariski topology, has (a basis of) open subsets given by the non-vanishing loci\n$$D(f):=\\{\\mathfrak{p}\\in\\mathrm{Spec}(R):f\\notin\\mathfrak{p}\\}$$\nfor an element $f$ of $R$. To help understand the intuition behind the terminology 'non-vanishing locus', it is helpful to think about the defining property of $\\mathrm{Spec}(R)$: $R=\\mathcal{O}(\\mathrm{Spec}(R))$; in other words, the ring of functions on $\\mathrm{Spec}(R)$ is $R$.\nTo understand this, let us first think of the example $R=\\mathbb{C}[x]$. Here we have\n$$ \\{(x-p):p\\in\\mathbb{C}\\}\\cup\\{(0)\\}= \\mathrm{Spec}(\\mathbb{C}[x])\\supseteq \\mathrm{MaxSpec}(\\mathbb{C}[x])=\\{(x-p):p\\in\\mathbb{C}\\},$$\nwhere for a ring $R$ we write $\\mathrm{MaxSpec}(R)$ for the set of maximal ideals of $R$. Note then that any element $f$ in $\\mathbb{C}[x]$ can be thought of as a function\n$$f\\colon\\mathrm{MaxSpec}(\\mathbb{C}[x])\\to \\mathbb{C},\\qquad (x-p)\\mapsto (f\\bmod (x-p))=f(p),$$\nwhere this last identification is using the isomorphism $\\mathbb{C}[x]/(x-p)\\cong \\mathbb{C}$.\nNow for a general ring $R$ (especially those rings that will show up in our study of Diophantine equations), we don't have the luxury of their maximal ideals admitting such a geometric description. In fact, in general, we don't even have the luxury of having enough maximal ideals for $\\mathrm{MaxSpec}(R)$ to be a particularly useful set (e.g. for a local ring there is only one!). That said, we may use the above example as a hint of how to think of an element $f$ of $R$ as a function on $\\mathrm{Spec}(R)$: it is the assignment\n$$f\\colon \\mathrm{Spec}(R)\\longrightarrow \\bigsqcup_{\\mathfrak{p}\\in\\mathrm{Spec}(R)}k(\\mathfrak{p}),\\qquad \\mathfrak{p}\\mapsto f(\\mathfrak{p}):=(f\\bmod \\mathfrak{p})\\qquad (\\ast).$$\nHere $k(\\mathfrak{p}):=\\mathrm{Frac}(R/\\mathfrak{p})$ is the 'residue field' of $R$ at $\\mathfrak{p}$.\nThis might look quite bizarre, especially the codomain being a big disjoint union of different fields. But, the fact that this didn't occur in the case $R=\\mathbb{C}[x]$ is semi-coincidental: for every $\\mathfrak{p}$ in $\\mathrm{MaxSpec}(\\mathbb{C}[x])$ there is an isomorphism $k(\\mathfrak{p})\\cong\\mathbb{C}$ and so we (sort of carelessly) identified them all with each other. As intimated before, such happy coincidences don't happen in general, and so we're stuck with the above. But, if you buy this interpretation, then the phrase 'non-vanishing locus' makes sense, as $D(f)$ is exactly those $\\mathfrak{p}$ for which $f(\\mathfrak{p})\\ne 0$.\nBut, I told you that a locally ringed space also came with a sheaf $\\mathcal{O}$ of functions -- what is it for $\\mathrm{Spec}(R)$? For formal reasons (because they form a basis) it suffices to describe the value of $\\mathcal{O}(D(f))$, and the guess is now clear: the only new functions that should be introduced by considering the non-vanishing locus of $f$ is the function $f^{-1}$. Thus, one sets $\\mathcal{O}(D(f))=R[\\tfrac{1}{f}]$.\nThen, at least in very broad terms, the promised (contravariant) fully faithful embedding is manifested as a natural bijection\n$$\\mathrm{Hom}_\\mathbf{Ring}(R,S)=\\mathrm{Hom}_\\mathbf{LRS}(\\mathrm{Spec}(S),\\mathrm{Spec}(R)) \\quad (\\ast\\ast).$$\nThis bijection can be made very explicit, but since I am not even telling you what a morphism of locally ringed spaces is, let's just take it as a black box.\nTwo important examples to keep in mind for the discussion below are:\n\nthe natural map $R\\to R[\\tfrac{1}{f}]$ corresponds to a map $\\mathrm{Spec}(R[\\tfrac{1}{f}])\\to \\mathrm{Spec}(R)$ which is an isomorphism onto the open subset $D(f)$ (an 'open immersion'),\nfor a prime $\\mathfrak{p}$ the composition $R\\to\n   R/\\mathfrak{p}\\hookrightarrow k(\\mathfrak{p})$ corresponds to a map\n$\\mathrm{Spec}(k(\\mathfrak{p}))\\to \\mathrm{Spec}(R)$ which one should think of as picking out the point $\\mathfrak{p}$,\n\nOK, fine, how does this help us solve equations? Well, let us fix an $n$-tuple of polynomials\n$$\\mathbf{f}=(f_1,\\ldots,f_m)\\in R[x_1,\\ldots,x_n].$$\nWe may consider the solution set functor for $\\mathbf{f}$ over $S$:\n$$X_\\mathbf{f}\\colon \\{R\\text{-algebras}\\}\\to \\mathbf{Set},$$\ngiven by\n$$X_\\mathbf{f}(S):=\\{(y_1,\\ldots,y_n)\\in S^n: f_j(y_1,\\ldots,y_n)=0\\text{ for }j=1,\\ldots,m\\}.$$\nIn other words, $X_\\mathbf{f}(S)$ just spits out the solution set to $\\mathbf{f}=0$ over $S$. On the other hand, note that there is a natural bijection between $X_\\mathbf{f}(S)$ and the $S$-algebra maps\n$$\\alpha\\colon S[x_1,\\ldots,x_n]/(f_1,\\ldots,f_m)\\to S$$\ngiven by sending $\\alpha$ to the $n$-tuple $(\\alpha(x_1),\\ldots,\\alpha(x_n))$.\nSo, formally studying $(\\ast\\ast)$ shows the following beautiful fact: there is a natural bijection\n$$X_\\mathbf{f}(S)\\longleftrightarrow \\left\\{\\begin{matrix}\\text{Sections of the map}\\\\ \\mathrm{Spec}(S[x_i]/(f_j))\\to \\mathrm{Spec}(S)\\end{matrix}\\right\\}.$$\nIf you buy the above, then the upshot is that by taking $R=S=\\mathbb{Z}$, then we can squarely (if surprisingly) place solving Diophantine equations in the setting of the archetypal problem (Sect) from above with $X=\\mathrm{Spec}(\\mathbb{Z})$.\nThe ring $\\mathbb{Z}_{(p)}^h$\nBefore we get too excited though, we need to recall the aforementioned caveat: all of this formalism is useful if the local geometry of $\\mathrm{Spec}(\\mathbb{Z})$ is simple enough. Well...what is the local geometry of $\\mathrm{Spec}(\\mathbb{Z})$. Or, an even more prescient question...what does local even mean?\nWell, there is a natural guess. After all $\\mathrm{Spec}(\\mathbb{Z})$ is a topological space, so 'local' should mean studying a 'sufficiently small' open neighborhood. Now, we know what the set $\\mathrm{Spec}(\\mathbb{Z})$ looks like:\n$$\\mathrm{Spec}(\\mathbb{Z})=\\{(p):p\\text{ is a prime}\\}\\cup\\{(0)\\}.$$\nA little thought even allows us to describe the Zariski open sets. Namely, the proper non-empty open subsets of $\\mathrm{Spec}(\\mathbb{Z})$ are of the form\n$$\\mathrm{Spec}(\\mathbb{Z})-\\{p_1,\\ldots,p_n\\}$$\nfor primes $p_1,\\ldots,p_n$.\nThe Zariski topology is strange. It's quite feeble (coarse). These open subsets are all just too big to somehow serve as a 'sufficiently small' neighborhood of a point $(p)$. As an analogy, for the space $\\mathbb{C}$ (with its usual topology) and a point $p\\in\\mathbb{C}$, we think of the 'sufficiently small' open neighborhoods of $p$ to be of the form $p\\in\\mathbb{D}\\subseteq\\mathbb{C}$ where $\\mathbb{D}$ is an open disc. In particular, we very much do not think of the open subsets $\\mathbb{C}-\\{p_1,\\ldots,p_m\\}$ for points $p_1,\\ldots,p_m\\in\\mathbb{C}$ as being small.\nAgain, let me emphasize, while $\\mathrm{Spec}(\\mathbb{Z})$ is a powerful object, its topology (the Zariski topology) is somewhat feeble(=coarse).\nRemark 2:\n\nOne equational justification for why no neighborhood $D(f)$ of $(p)$ is sufficiently small: for any prime $p\\ne q\\nmid f$, the equation $qx=1$ does NOT have a solution in $\\mathbb{Z}[\\tfrac{1}{f}]$. A 'sufficiently small' neighborhood of $(p)$ should not have equational obstructions coming from other points!\n\nWell, if no one Zariski open neighborhood of $(p)$ is small enough, you can do something even more drastic: what if we intersect all the neighborhoods. In normal topology land this is not a very reasonable thing to do -- for a Hausdorff topological space you just get the point back! But, the upside of the feebleness of the Zariski topology of $\\mathrm{Spec}(\\mathbb{Z})$ is that this is a reasonable operation here. In fact, inspecting $(\\ast\\ast)$, and the fact that it reverses the direction of maps, we might guess that\n$$\\bigcap_{p\\in D(f)}D(f)=\\varprojlim_{p\\in D(f)}D(f),$$\nshould be a geometric space with ring of functions\n$$\\varinjlim_{f\\notin (p)}\\mathbb{Z}[\\tfrac{1}{f}]=\\mathbb{Z}_{(p)}.$$\nAnd so, in fact, a good model for the 'intersection of all neighborhoods of $(p)$' is $\\mathrm{Spec}(\\mathbb{Z}_{(p)})$. Maybe this is a \"neighborhood\" which is 'sufficiently small'.\nUnfortunately, this is still not small enough (a true indictment of the Zariski topology). The reason for this is more subtle, and takes a bit more of a leap-of-faith in terms of geometric philosophy. Namely, if $U$ is to be a 'sufficiently small neighborhood' of $(p)$ then we would imagine that the map\n$$\\{(p)\\}=\\mathrm{Spec}(\\mathbb{F}_p)\\to \\mathrm{Spec}(\\mathbb{Z}_{(p)}),$$\nshould be something like a (weak) homotopy equivalence. One of the main reasons that a small open disk around $p$ in $\\mathbb{C}$ is often 'sufficiently small' in complex analysis is because it's contractible. Unfortunately, this is not the case. In fact, in a way that one can make precise the map on fundamental groups isn't even an isomorphism.\nRemark 3:\n\nIt takes a lot of faith that this topological wording can be made precise and...wait a minute...even if you can, what in the world does this have to do with solving Diophantine equations? Let me try to give an example which hopefully (mildly) addresses both of these points.  Let us consider the Diophantine equation $$\\{y\\in\\mathbb{Z}:y^2+y+1=0.\\}$$ From our above discussion we know that this should correspond to studying sections of the map $$\\mathrm{Spec}(\\mathbb{Z}[x]/(x^2+x+1))\\to\\mathrm{Spec}(\\mathbb{Z}).$$ Moreover, what it should mean to study this 'locally at $(13)$', with $\\mathbb{Z}_{(13)}$ as our meaning of local, is that we want to study sections of the map $$\\mathrm{Spec}(\\mathbb{Z}_{(13)}[x]/(x^2+x+1))\\to\\mathrm{Spec}(\\mathbb{Z}_{(13)}).$$ That said, while this map does not have a section it does have a section over the point $\\mathbb{F}_{13}$: $$y^2+y+1=0\\bmod 13$$ has two distinct solutions.  Why is this indictative of the fact that $$\\{(13)\\}=\\mathrm{Spec}(\\mathbb{F}_{13})\\to \\mathrm{Spec}(\\mathbb{Z}_{(13)})$$ is not a homotopy equivalence? Well, let us note that the map $$\\mathrm{Spec}(\\mathbb{Z}_{(13)}[x]/(x^2+x+1))\\to\\mathrm{Spec}(\\mathbb{Z}_{(13)}).$$ can/should be thought of as being smooth and proper: smooth because its derivative $2x+1$ vanishes nowhere on $\\mathrm{Spec}(\\mathbb{Z}_{(13)}[x]/(x^2+x+1))$, and proper because it's finite. That said, Ehresmann's theorem says that a smooth proper map is locally trivial fibration. If $X'\\to X$ is a (weak) homotopy equivalence and $Y\\to X$ is a locally trivial fibration, then it should have a section if and only if $Y\\times_X X'\\to X'$ has a section. Thus, this indicates that $$\\{(13)\\}=\\mathrm{Spec}(\\mathbb{F}_{13})\\to \\mathrm{Spec}(\\mathbb{Z}_{(13)})$$ is indeed not a homotopy equivalence.  As mentioned above this remark, for those in the know, what I am really pointing out here is that the map $$\\pi_1^\\mathrm{et}(\\mathrm{Spec}(\\mathbb{F}_p))\\to\\pi_1(\\mathrm{Spec}(\\mathbb{Z}_{(p)})$$ is not a bijection.\n\nThe idea of Grothendieck and his collaborators/descendants is the following: because the Zariski topology on $\\mathrm{Spec}(\\mathbb{Z})$ is too feeble, one should replace it with a sort of \"generalized topology\" where one can take 'sufficiently small neighborhoods' of points. I won't attempt to define \"generalized topology\", but in our case what pops out is the Nisnevich topology on $\\mathrm{Spec}(\\mathbb{Z})$, which acts in a manner much more similar to the usual topology on $\\mathbb{C}$.\nRemark 4:\n\n For those more in the know, you might be surprised that what is showing up here is the Nisnevich topology, and not the more common etale topology. The reason is that unlike the case of $\\mathbb{C}$, our points themselves have non-trivial topology. In fact, recall that $$\\pi_1^\\mathrm{et}(\\mathrm{Spec}(\\mathbb{F}_p))=\\mathrm{Gal}(\\overline{\\mathbb{F}}_p/\\mathbb{F}_p)\\cong\\widehat{\\mathbb{Z}}.$$ The Nisnevich topology respects that, and allows you to zoom in on $(p)$ without altering the topology of the point. The etale neighborhood, on the other hand, pursues the logical conclusion by zooming in so far as to obtain a 'contractible' neighborhood, but at the cost of altering the topology of the point. Both are useful, but for our purposes here it is the Nisnevich topology playing the more crucial role.\n\nIn particular, one can again 'intersect all neighborhoods' of $(p)$ in this more robust Nisnevich topology, arriving at the Henselization of $\\mathbb{Z}_{(p)}$, denoted $\\mathbb{Z}_{(p)}^h$. One may roughly think of this as adding in the minimal number of things to $\\mathbb{Z}_{(p)}$ so that Hensel's lemma works (thus the name!). The map\n$$\\{(p)\\}=\\mathrm{Spec}(\\mathbb{F}_p)\\to \\mathrm{Spec}(\\mathbb{Z}_{(p)}^h),$$\nis like a (weak) homotopy equivalence, and so from a 'topological perspective' this does serve as a 'sufficiently small' neighborhood of $(p)$.\nRemark 5:\n\n My claim that $$\\{(p)\\}=\\mathrm{Spec}(\\mathbb{F}_p)\\to \\mathrm{Spec}(\\mathbb{Z}_{(p)}^h),$$ is like a (weak) homotopy equivalence is meant to be mostly intuitive, and probably only should be taken seriously at the pro-finite level. That said, see this, this, and Theorem 2.1.6 of this for why $\\mathrm{Spec}(A/I)$ and $\\mathrm{Spec}(A)$ are quite 'topologically similar' for a Henselian pair $(A,I)$. It is also helpful to compare this with the strict Henselization $\\mathbb{Z}_{(p)}^\\mathrm{sh}$, the local ring in the etale topology, where the results are more clear-cut: see this and Proposition 8.6 of Friedlander's Etale homotopy of schemes.\n\nOne 'equational' way this manifests itself is that if $\\mathbf{f}$ defines a 'smooth family at $p$' (i.e. that $\\mathrm{Spec}(\\mathbb{Z}[x_i]/(f_j))\\to \\mathrm{Spec}(\\mathbb{Z})$ is smooth at $p$), and so topologically well-behaved, then for any $n\\geqslant 1$ the map\n$$X_\\mathbf{f}(\\mathbb{Z}_{(p)}^h)\\to X(\\mathbb{Z}/p^n\\mathbb{Z}),$$\n(note that $\\mathbb{Z}_{(p)}^h/p^n\\mathbb{Z}^h_{(p)}=\\mathbb{Z}_{(p)}/p^n\\mathbb{Z}_{(p)}=\\mathbb{Z}/p^n\\mathbb{Z}$ by this) is surjective. In other words, for topologically well-behaved families, the only obstruction to having solutions is coming from the point $\\mathrm{Spec}(\\mathbb{F}_p)$ itself.\nThe ring $\\mathbb{Z}_p$\nNow, this Henselization step does not occur in the situation of $\\mathbb{C}$, essentially for the reason that its topology is much richer. More precisely, for a point $p$ in $\\mathbb{C}$, one has the ring\n$$\\mathbb{C}\\langle x-p\\rangle :=\\varinjlim_{p\\in U}\\left\\{\\begin{matrix}\\text{holomorphic functions}\\\\ U\\to \\mathbb{C}\\end{matrix}\\right\\}$$\nof power series $\\sum_{i=0}^\\infty a_i(x-p)^i$ which converge in a neighborhood of $p$. This is a Henselian local ring, with maximal ideal $(x-p)$! Again, we already knew the topology was rich enough to get 'sufficiently small' topological neighborhoods in this case, in the form of small open disks. But, there is one way in which these small open disks are not sufficient, and this will foretell how we will finally alter $\\mathbb{Z}_{(p)}^h$.\nTo understand this, let us go back to (Diff). Solving a differential equation $Df=0$ where $f$ is an entire function on $\\mathbb{C}$, can be studied by first trying to solve this equation locally at $p$. One way of interpreting this is as solving $Df=0$ for $f$ in $\\mathbb{C}\\langle x-p\\rangle$. But, in practice, we quite often actually break this step further into two easier pieces:\n\nfirst solve $Df=0$ for $f\\in\\mathbb{C}[\\![x-p]\\!]$ (i.e. arbitrary power series at $p$ with no convergence conditions),\nthen show that this solution has reasonable convergence properties (i.e. that the solution can be taken to lie in $\\mathbb{C}\\langle x-p\\rangle$).\n\nThe hallmark advantage of solving differential equations in $\\mathbb{C}[\\![x-p]\\!]$ is that it has a 'formal solvability criterion': a solution exists if and only if one can find a polynomial solution which approximates it arbitrarily well. We can represent this equationally as\n$$\\{f\\in\\mathbb{C}[\\![x-p]\\!] : Df=0\\}=\\varprojlim_n \\{g_n\\in\\mathbb{C}[x]_n: Dg_n=0\\},$$\nwhere $\\mathbb{C}[x]_n$ is the set of polynomials of degree at most $n-1$. This reduces us to the purely algebraic study of 'finite-like' objects $\\mathbb{C}[x]_n$. Again, the motto here is that while $\\mathbb{C}\\langle x-p\\rangle$ has no 'topological obstructions' to reducing study to $\\mathbb{C}[x]_n$, it does have analytic obstructions and $\\mathbb{C}[\\![x]\\!]$ does away with those.\nTo understand what the analagous picture is for $\\mathbb{Z}_{(p)}^h$, we observe that $\\mathbb{C}[x]_n$ is nothing but $\\mathbb{C}\\langle x-p\\rangle /(x-p)^n$: the quotient of this Henselian local ring by the $n^\\text{th}$ power of its maximal ideal. As, essentially by definition,\n$$\\mathbb{C}[\\![x-p]\\!]=\\varprojlim_n \\mathbb{C}\\langle x-p\\rangle/(x-p)^n,$$\n(mirroring what showed up in formal solvability criterion). Thus, the correct analogue for $\\mathbb{Z}_{(p)}^h$, with maximal ideal $(p)$, is\n$$\\mathbb{Z}_p=\\varprojlim_n \\mathbb{Z}_{(p)}^h/p^n\\mathbb{Z}^h_{(p)}=\\varprojlim_n \\mathbb{Z}_{(p)}/p^n\\mathbb{Z}_{(p)}=\\varprojlim_n \\mathbb{Z}/p^n\\mathbb{Z}.$$\nThe $p$-adic integers have appeared.\nWhat is the analogue of the formal solvability criterion here? Well, it is simply that one has the equality\n$$X_\\mathbf{f}(\\mathbb{Z}_p)=\\varprojlim_n X_\\mathbf{f}(\\mathbb{Z}/p^n\\mathbb{Z}),$$\nsomething that fails with $\\mathbb{Z}_p$ replaced by $\\mathbb{Z}_{(p)}^h$, essentially for 'convergence' reasons. Thus, our 'formal solvability criterion' again reduces us to studying equations in the 'finite-like' rings $\\mathbb{Z}/p^n\\mathbb{Z}$.\nSummary and original question\nTo summarize the above, we see that by using algebraic geometry to view Diophantine equations geometrically, the guiding principle given in (Sect) led us to\n\nconsider $\\mathbb{Z}_{(p)}$ as a possible 'sufficiently small neighborhood' of $(p)$, and to realize it's woefully 'too big',\nto consider the Henselization $\\mathbb{Z}_{(p)}^h$ to eliminate 'topological obstructions' (not coming from $\\mathrm{Spec}(\\mathbb{F}_p)$ itself) to finding solutions of equations and realizing that it does that job quite well,\nbut to realize that 'analytic' obstructions remain and so we can further replace it with $\\mathbb{Z}_p$ which is a more 'formal' object  (analogous to replacing convergent power series with all power series).\n\nHopefully this is satisfying, and convinces you why rings like $\\mathbb{Z}_{(p)}$ and $\\mathbb{Z}_p$ are ubiquitious as studying the 'local geometry' at $p$ of $\\mathbb{Z}$ (in different perspectives of 'local').\nI think four questions remain though:\n\nCan one actually work backward from $\\mathbb{Z}_p$ to $\\mathbb{Z}_{(p)}^h$? The answer, astoundingly, is almost always yes. This is the theory of Artin approximation.\nCan one actually work backward from $\\mathbb{Z}_{(p)}^h$ to an honest Nisnevich neighborhood? The answer again is essentially yes -- see this.\nOK, but can you finally profitably glue these solutions together to an actual Diophantine solution? This is the million dollar question, and the hardest. The answer is a resounding yes...sometimes. As the gluing data can be quite abstract/complicated, it's hard to enact this in practice (although it theoretically always works). Instead people often times consider 'local solutions' with some sort of 'weakened gluing data' (sometimes no gluing data at all!). These usually go under the heading of 'local-to-global principles', and generally have reasonable success.\nWhy use $\\mathbb{Z}_p$ instead of $\\mathbb{Z}_{(p)}^h$? This is fair question. Hopefully the above gives you some sort of idea of what the advantages of $\\mathbb{Z}_p$ are, but from a more literal gluing perspective $\\mathbb{Z}_{(p)}^h$ is superior. I can imagine a world, not so far from our own (maybe a future world?) where we do use $\\mathbb{Z}_{(p)}^h$ instead. In fact, I think that some of the work of Fujiwara and Kato (on 'Henselian rigid geometry') can be seen to point in that direction.\n\nAppendix: a description of $\\mathbb{Z}_{(p)}^h$\nA natural question you might have while reading the above: what does $\\mathbb{Z}_{(p)}^h$ even look like.\nLet me give a description of $R^h$ in a much more general setting. Suppose that $R$ is a local, Noetherian, excellent integral domain with maximal ideal $\\mathfrak{m}$. This includes many examples, but in particular includes the case when $R$ is a DVR with fraction field of characteristic $0$ (e.g. $\\mathbb{Z}_{(p)}$).\nFor an $R$-algebra $S$, let us define the algebraic closure of $R$ in $S$ to be those $s$ in $S$ which satisfy a non-zero polynomial $p(x)$ in $R[x]$ (NB: there is no assumption that $p(x)$ is monic!).\n\nFact: The Henselization of $R$ is the algebraic closure of $R$ in $\\widehat{R}$.\n\nProof: The map $R\\to R^h$ is local, and in fact $\\mathfrak{m}R^h$ is the maximal ideal of $R^h$ and the map $R/\\mathfrak{m}^nR \\to R^h/\\mathfrak{m}^n R^h$ is an isomorphism for all $n$. Thus, whe natural map $\\widehat{R}\\to \\widehat{R^h}$ is an isomorphism, and so $R^h\\to \\widehat{R^h}=\\widehat{R}$ is faithfully flat, and so injective. We claim that the image of $R^h\\to \\widehat{R}$ consists precisely of those elements of $\\widehat{R}$ which are algebraic over $R$. By definition, if $r$ is an element of $R^h$, then there exists a factorization $R\\to S\\to R^h$, with $R\\to S$ an etale map of local rings, and with $r$ in the image of $S$. Note that $S$ is a domain (combine this and this), and as $R\\to S$ is faithfully flat and so injective, the generic point of $S$ maps to that of $R$ under $\\mathrm{Spec}(S)\\to\\mathrm{Spec}(R)$. So the extension $\\mathrm{Frac}(S)/\\mathrm{Frac}(R)$ is finite, and so $r$ is algebraic. Conversely, suppose that $r$ in $\\widehat{R}$ is algebraic and let $p(x)\\in R[x]$ be a non-zero polynomial annihilating $r$. Since $\\widehat{R}$ is a domain, the polynomial can have at most $\\deg(p)$ many roots in $\\widehat{R}$. Write this finite set of roots as $r=x_1,x_2,\\ldots,x_m$. There exists some $N\\gg 0$ such that $r\\ne x_i\\bmod \\mathfrak{m}^N \\widehat{R}$ for $i\\ne 1$. By Artin approximation there exists some $s$ in $R^h$ which is also a root of $p(x)$ and such that $s=r\\bmod \\mathfrak{m}^M\\widehat{R}$ for some $M\\geqslant N$. But, by set up, we then have that $s=r$. $\\blacksquare$\nRemark 6:\n\n Let us give two applications of this.  First, it lets us give a simple example of an element of $\\mathbb{Z}_p$ not in $\\mathbb{Z}_{(p)}^h$. In particular, the element $\\sum_{i=0}^\\infty p^{i!}$. It suffices to show that it's not algebraic over $\\mathbb{Z}_{(p)}$ and this is fairly elementary (e.g. see An elementary example of a transcendental $p$-adic number by Suter). Second, it lets observe that although $\\mathbb{C}\\langle x-p\\rangle$ is Henselian, and contains $\\mathbb{C}[x]_{(x-p)}$, and the two have the same completion, that $\\mathbb{C}\\langle x-p\\rangle$ is not $\\mathbb{C}[x]_{(x-p)}^h$. Indeed, it suffices to observe that $\\mathbb{C}\\langle x-p\\rangle$ contains $\\exp(x)$ which is not algebraic over $\\mathbb{C}[x]_{(x-p)}$.", "meta": {"post_id": 4652524, "input_score": 29, "output_score": 42, "post_title": "Why are $p$-adic numbers ubiquitous in modern number theory?"}}
{"input": "I have three variables $x_1, x_2, x_3$ that can only be non-negative integers, i.e., $x_1 \\in \\mathbb{N}_0$, $x_2 \\in \\mathbb{N}_0$, and $x_3 \\in \\mathbb{N}_0$. Is it correct to write this in a tuple like $(x_1,x_2,x_3) \\in \\mathbb{N}_0^3$? If not, what is the correct way?", "output": "Unless you're actually considering them as a tuple elsewhere, you can also just write this as\n$$x_1, x_2, x_3 \\in \\mathbb N_0$$\nAnd I would argue that introducing the tuple and the product space solely for the purpose of specifying this constraint makes it more complicated for no reason.\nSo if you need the tuple, yes, use that notation. But if you don't need the tuple, use this one.", "meta": {"post_id": 4669178, "input_score": 15, "output_score": 38, "post_title": "Tuple notation for non-negative integers"}}
{"input": "There is a similar question here, but it's asking that if two matrices have the same\ncharacteristic polynomial then are they similar. If the answer were positive then the answer to my question will also be positive, but it's not.\nNow, if two matrices have the same characteristic polynomial then they are of the same order. Suppose $A$ and $B$ are matrices of order 2 and they both have a characteristic polynomial of $t^2+at+b$. We know that $t^2+at+b=t^2-\\text{trace}(A)t+\\text{det}(A)=t^2-\\text{trace}(B)t+\\text{det}(B)$, so in this case the answer to my question is affirmative.\nHowever, I don't think the answer is \"yes\" in general because then I probably will have a theorem which says this in my book. I looked for counterexamples but couldn't find any. Basically, I made up random matrices and never got a counterexample. So if the answer to the question is \"no\", can you please explain how you arrived at the counterexample.", "output": "If $\\chi_A$ denotes the characteristic polynomial of $A$, then $\\chi_A(0)=\\det(A)$, so the answer is yes.", "meta": {"post_id": 4724879, "input_score": 15, "output_score": 38, "post_title": "If two matrices have the same characteristic polynomial then do they have the same determinant?"}}
{"input": "I attempted the following integral from the 2022 MIT Integration Bee Qualifying Round:\n$$\\int\\frac{1}{1+\\sin x} + \\frac{1}{1+\\cos x}+ \\frac{1}{1+\\tan x} + \\frac{1}{1+\\cot x} + \\frac{1}{1+\\sec x} + \\frac{1}{1+\\csc x}dx$$\n\n$$\\int\\frac{1-\\sin x}{\\cos^2 x} + \\frac{1-\\cos x}{\\sin^2 x} + \\frac{\\cos x}{\\cos x + \\sin x} + \\frac{\\sin x}{\\cos x + \\sin x} + \\frac{\\cos x}{\\cos x + 1} + \\frac{\\sin x}{\\sin x + 1}dx$$\n\n$$x\\,+\\,\\int(\\sec^2 x -\\sec x\\tan x)\\,+\\,(\\csc^2 x - \\csc x\\cot x)dx + \\int\\frac{\\cos x(1 - \\cos x)}{\\sin^2 x}+ \\frac{\\sin x(1 - \\sin x)}{\\cos^2 x}dx $$\n\n$$x\\,+\\,\\tan x - \\sec x - \\cot x + \\csc x + \\int\\cot x\\csc x-\\cot^2 x + \\tan x\\sec x-\\tan^2 x dx $$\n\n$$x\\,+\\, \\tan x - \\sec x - \\cot x + \\csc x-\\csc x+\\sec x - \\tan x + x + \\cot x + x = \\fbox{3x} $$\n\nThis is the correct answer, but I have been trying to find a shorter way to compute this integral. Is there maybe a way to combine some of the terms in the original integral to make it shorter, or this essentially the best way to do it? Thank you.", "output": "The only fact from trigonometry we need are the reciprocal identities:\n$$\\sec x = 1/\\cos x, \\quad \\csc x = 1/\\sin x, \\quad \\cot x = 1/\\tan x$$\nUsing this, the integrand is recognized as the sum of three expressions of the form $$\\frac1{1+u}+\\frac1{1+u^{-1}} = \\frac1{1+u}+ \\frac{u}{u+1} = \\frac{1+u}{1+u} = 1$$\nThat is, the integral is $$\\int (1+1+1)\\,dx = \\int 3\\, dx = 3x+C$$", "meta": {"post_id": 4735687, "input_score": 15, "output_score": 43, "post_title": "2022 MIT Integration Bee, Qualifying Round, Question 17"}}
{"input": "We recently started with exponential functions, and I did this task for fun, but I apparently did everything wrong. I just don't get why it is wrong. I am aware of some logarithmic properties like $\\log(\\frac{x}{y})= \\log(x)-\\log(y)$ and $\\log(x\\cdot y)=\\log(x)+\\log(y)$ (though i don't really understand why that works and if $x$ and $y$ have to be variables or parameters. And I also don't know when to use these properties.)\nThe task:\nFind the intersection point of $g(x)=3\\cdot0.4^x$ and $f(x)=0.5\\cdot1.5^x$\nThis is how i did it (and the solution is wrong):\n$0.5\\cdot1.5^x=3\\cdot0.4^x$\n$(0.5\\cdot1.5^x)\\cdot\\frac{1}{3}\\cdot\\frac{1}{1.5^x}=(3\\cdot0.4^x)\\cdot\\frac{1}{3}\\cdot\\frac{1}{1.5^x}$\n$\\frac{1}{6}=\\frac{0.4^x}{1.5^x}$\n$\\frac{1}{6}=(\\frac{0.4}{1.5})^x$\n$\\frac{1}{6}=x\\cdot \\log(\\frac{4}{15})$\n$\\frac{1}{6\\cdot \\log(\\frac{4}{15})}=x$\n$-0,290$, which is terribly wrong.\n\nThe correct solution/approach:\n$0.5\\cdot1.5^x=3\\cdot 0.4^x$\n$\\log(0.5\\cdot 1.5^x)=\\log(3\\cdot 0.4^x)$\n$\\log(0.5)+\\log(1.5^x)=\\log(3)+\\log(0.4^x)$\n$\\log(0.5)+x\\cdot \\log(1.5)=\\log(3)+x\\cdot \\log(0.4)$\n$\\log(0.5)-\\log(3)=x\\cdot \\log(0.4)-x\\cdot \\log(1.5)$\n$\\log(0.5)-\\log(3)=x\\cdot (\\log(0.4)-\\log(1.5))$\n$\\frac{\\log(0.5)-\\log(3)}{\\log(0.4)-\\log(1.5)}=x=1,36$\nI would appreciate if someone could tell me why I wasn't able to divide and basically do the first approach.", "output": "The issue happens when you jump from\n$$\\frac{1}{6} = \\left (\\frac{4}{15} \\right )^{x}$$\nto\n$$\\frac{1}{6} = x\\ln\\left (\\frac{4}{15} \\right ).$$\nIn this step you have taken the logarithm of the right hand side and only the right hand side. What you need to do is take the logarithm of both sides\n$$\\ln \\left (\\frac{1}{6} \\right ) = x\\ln\\left (\\frac{4}{15} \\right ).$$\nand then divide to obtain\n$$x = \\frac{\\ln(1/6)}{\\ln(4/15)} \\approx 1.36$$", "meta": {"post_id": 4737223, "input_score": 11, "output_score": 39, "post_title": "Why can you not divide both sides of the equation, when working with exponential functions?"}}
{"input": "I am trying to evaluate the following integral using integration by parts:\n$$\\int\\frac{x}{1+e^x}dx$$\nHowever, using $u = x$, $du = 1$, $dv = \\frac{1}{1+e^x}$, $v = x-\\log(e^x+1)$, I keep getting that the  integral is $-\\text{Li}_2(-e^x)+\\frac{x^2}{2}-x\\log(1+e^x)$, but Wolfram Alpha says that the integral is $\\text{Li}_2(-e^{-x})-x\\log(e^{-x}+1)$.\nCan anyone tell me what I'm doing wrong here?", "output": "Who says you went wrong? Subtract the two answers\n$$-\\operatorname{Li}_2(-e^x) - \\operatorname{Li}_2(-e^{-x})+\\frac{x^2}{2}-x\\log(1+e^x) +x\\log(e^{-x}+1)$$\n$$=\\frac{\\pi^2}{6} +x^2 -x\\log(1+e^x) +x\\log(e^{-x}+1)$$\nAnd since $x^2 = x\\log e^x$, the rest of the terms collapse and we get that difference between the two answers is $\\frac{\\pi^2}{6}$, a constant.", "meta": {"post_id": 4742499, "input_score": 17, "output_score": 52, "post_title": "Why is integration by parts not working on this problem?"}}
{"input": "Given the null graph with no edges or vertices, we have a connected planar graph as no edges cross when this graph is drawn in the plane, and the fact that any two distinct vertices have a path between them is vacuously true. However, Euler's formula doesn't work: plugging into $v+f= e+2$, we have $1=2$. Why is this the case? Can we not apply Euler's formula here?", "output": "We can slightly generalize Euler's formula as: $V-E+F-C = 1$, where $C$ is the number of components. Most of the time $C=1$, which gives us the familiar formula. But it works great if $C>1$. And the question is about the \"trivial\" case where $C=0$. Now of course $V=E=0$ and $F=1$, giving us the right answer.\nThere is a standard inductive proof of Euler's formula which involves either removing an edge and a face, or removing an edge and a vertex, and observing that the invariant remains the same. See here. But these proofs are quite complicated, because they are trying to preserve connectedness.\nWith the generalized formula, the proof can be much cleaner (still skating over elementary topology). Remove any edge. There are two disjoint possibilities:\n(1) We merge two distinct faces, so: $E-1; F-1$\n(2) The edge always had the same face on both sides, so removing the edge instead splits a component into two: $E-1; C+1$\nIn either case the invariant $V-E+F-C$ remains the same. So we remove all the edges, and then have a bunch of isolated vertex/components in a single face, so $V=C$ and hence $V-0+1-C = 1$.\nAlways nice when a formula can apply all the way down to $0$.", "meta": {"post_id": 4749400, "input_score": 36, "output_score": 76, "post_title": "Euler's formula doesn't work for null graph?"}}
{"input": "In a first course in linear algebra, it is common for instructors to mostly restrict their attention to finite-dimensional vector spaces. These vector spaces are usually not assumed to be equipped with any additional structure, such as an inner product, norm, or a topology. On the other hand, it seems that when infinite-dimensional vector spaces are encountered in later courses, it is much more common to equip with them additional structure. Why is this? A partial answer might be that infinite-dimensional vector spaces are often studied in functional analysis, where extra structure is needed to properly define analytical concepts such as infinite series. However, I would have expected that \"pure\" infinite-dimensional vector spaces have a use in some area of mathematics.\nI have now also asked this on MathOverflow.", "output": "Consider the following example.\nVectors over, say, the field $\\mathbb{R}$, with finite dimension $n$, we'd like to prototypically understand as lists, or tuples:\n$$\\mathbf{v} = (a_1, a_2, \\cdots, a_n).$$\nNote here that the right hand is literally a tuple of reals; it is the \"meat\" of what the objects in the space \"$\\mathbb{R}^n$\" \"really are\". The reals $a_j$ are the components of the vector, which are not the same concept as coordinates, which refer to the values $b_j$ when $\\mathbf{v}$ is expressed in a basis $\\mathcal{B}$:\n$$\\mathbf{v} = b_1 \\mathbf{b}_1 + b_2 \\mathbf{b}_2 + \\cdots + b_n \\mathbf{b}_n$$\nwhere $\\mathcal{B} = \\{ \\mathbf{b}_1, \\mathbf{b}_2, \\cdots, \\mathbf{b}_n \\}$. Given a basis, any vector has coordinates, but not all vectors have components, because the elements of the actual vector space set need not be just tuples.\nBut here's the thing: it's quite obvious that, in this case, we have a basis where that the coordinates and components are identical, namely:\n$$\\begin{matrix}\n\\mathbf{e}_1 := (1, 0, 0, \\cdots, 0)\\\\\n\\mathbf{e}_2 := (0, 1, 0, \\cdots, 0)\\\\\n\\cdots\\\\\n\\mathbf{e}_n := (0 ,0, 0, \\cdots, 1)\\end{matrix}$$\nThen the vector $(a_1, a_2, \\cdots, a_n) = \\sum_{j=1}^{n} a_j \\mathbf{e}_j$ and the coordinates and components are identical. We call this the standard basis for $\\mathbb{R}^n$.\nNow, consider infinite dimension. It's quite natural to extend vectors with a finite tuple of $n$ reals, to an infinite tuple or perhaps as some might be more comfortable saying, a \"sequence\" (they are \"equivalent\" though one might call them different \"data types\"):\n$$\\mathbf{v} = (a_1, a_2, a_3, \\cdots)$$\nwhere there are now infinitely many components $a_j$. We'd like, then, by analogy with before, to write down a basis in the form\n$$\\begin{align}\n\\mathbf{e}_1 := (1, 0, 0, 0, \\cdots)\\\\\n\\mathbf{e}_2 := (0, 1, 0, 0, \\cdots)\\\\\n\\mathbf{e}_3 := (0, 0, 1, 0, \\cdots)\\\\\n\\cdots\\end{align}$$\nso that we could say\n$$\\mathbf{v} = \\sum_{i=1}^{\\infty} a_i \\mathbf{e}_i.$$\nThe problem though, is that by definition, an infinite summation like this requires us to take the following limit:\n$$\\mathbf{v} = \\lim_{n \\rightarrow \\infty} \\left(\\sum_{i=1}^{n} a_i \\mathbf{e}_i\\right).$$\nAnd the thing in the brackets is a vector, so what we have is a limit of vectors.\nBut limits don't make sense unless you have some way to compare vectors for notions like proximity, or \"approximate-ness\", or what have you!\nThus, if we want to be able to use the \"basis\" we have given above, we must add some extra structure to the space, which specifies how that limits or approximations are to work, i.e. \"who is within some tolerance of who\", so that the infinite sum is both defined at all, and so that it generates the result we'd like it to generate.\nSo no, the vector space itself doesn't \"need\" a topology, but suitably building one on it lets us do a lot of stuff we'd \"like\" to be able to do but otherwise wouldn't.", "meta": {"post_id": 4751895, "input_score": 24, "output_score": 48, "post_title": "Why are infinite-dimensional vector spaces usually equipped with additional structure?"}}
{"input": "This question/observation is inspired by the integral:\n$$\\int_0^{\\sqrt{\\pi}}x\\sin(x^2)\\cos(x^2)dx$$\nThe $u$-substitution $u=\\sin(x^2)$ yields $du=2x\\cos(x^2)dx$ and\n$$\\int_0^{\\sqrt{\\pi}}x\\sin(x^2)\\cos(x^2)dx=\\frac{1}{2}\\int_0^0 u du=0,$$\nright? Wolframalpha certainly agrees.\nGreat, now consider the much harder integral $$\\int_0^1 dx.$$\nAfter banging our heads against the wall for hours we take $u=x^2-x$ so $du=(2x-1)dx=\\pm\\sqrt{1+4u}*dx$ by the quadratic formula, so\n$$\\int_0^1 dx=\\int_0^0\\frac{du}{\\pm\\sqrt{1+4u}}=0$$\nThe problem is wolframalpha says this integral should be $1$. Well, I guess it is a unit square.\n\nSince $0\\neq 1$ there is something fishy going on - I'm just trying to fully nail down the issue here. I think it boils down to hidden division by $0$ like most false proofs. In particular we can't solve the equation $du=(2x-1)dx$ for $dx$ if $x=\\frac{1}{2}$, which happens inside the domain of integration. This isn't a problem for the original integral because even though there is a place where $\\frac{du}{dx}=0$ inside the domain of integration $(x=\\sqrt{\\frac{\\pi}{2}})$ there is no issue because we don't have to divide by this expression to make all the $x$s cancel. Anyway, I'm curious for further explanation and to know if there are any references which carefully explain subtleties such as this for integration by substitution.", "output": "Formally, substitution says that if $g$ is differentiable on an interval, and $f$ is continuous on the image interval of $g$, then\n$$ \\int_a^b f(g(x)) g'(x) \\, dx = \\int_{g(a)}^{g(b)} f(u) \\, du.$$\nIn our case, when we try to write the simple yet problematic integral in the form required by the left-hand side, we get\n$$ \\int_0^1 \\, dx = \\int_0^1 \\frac1{2x-1}(2x-1)\\, dx = \\int_0^1 \\, \\frac1{\\pm\\sqrt{1+4(x^2 -x)}}(2x-1) \\, dx, $$\nbut the sign ambiguity tells us that we must proceed more carefully (or else what single $f$ do we choose?). Indeed $2x - 1 = \\sqrt{1+4(x^2-x)}$ when $x > 1/2$, whereas $2x - 1 = -\\sqrt{1+4(x^2-x)}$ when $x < 1/2$. So really we should have written\n$$ \\int_0^{1/2} \\, \\frac1{-\\sqrt{1+4(x^2 -x)}}(2x-1) \\, dx + \\int_{1/2}^{1} \\, \\frac1{\\sqrt{1+4(x^2 -x)}}(2x-1) \\, dx,$$ or via $u = g(x)$\n$$ \\int_0^{-1/4} \\, \\frac1{-\\sqrt{1+4u}} \\, du + \\int_{-1/4}^{0} \\, \\frac1{\\sqrt{1+4u}} \\, du = \\frac12 + \\frac12 = 1,$$\nwhich is the correct answer.", "meta": {"post_id": 4764847, "input_score": 27, "output_score": 54, "post_title": "u-substitution shows 0=1"}}
{"input": "Find $x$:\n$$9^x = 4^x + 6^x$$\n\nThis was in my exam today, and I have no idea, would really help if someone taught me, I would love to know how to write math on this website and is there a way to like search a topic for a 10th grader where i can find many questions and solve them?", "output": "$$9^x = 4^x + 6^x$$\nDividing by $4^x$ , we get :\n$$\\left(\\frac{9}{4}\\right)^x =  1 + \\left(\\frac{3}{2}\\right)^x$$\n$$\\left(\\frac{3}{2}\\right)^{2x} =  1 + \\left(\\frac{3}{2}\\right)^{x}$$\nSubstituting $\\left(\\frac{3}{2}\\right)^x = t $\n, you get a quadratic equation in $t$,\n$$t^2 =  1 + t$$\nNow, solve for $t$ and then solve for $x$.", "meta": {"post_id": 4771748, "input_score": 0, "output_score": 37, "post_title": "Answer $9^x = 4^x + 6^x$ to a 10th grader, who knows math until the equation of a straight line (just before calculus)"}}
{"input": "Consider all the integers from 1-13. Without reusing any integers, find a set of three integers for which all numbers 1-13 may be created using only addition and subtraction. Prove that for all such sets, it is not possible to create a sum of 14.\nFor this question, I've identified that 1,3,9 is a possible set of integers that fulfill the requirements as follows:\n$1 = 1 \\\\ 2 = 3 - 1 \\\\ 3 = 3 \\\\ 4 = 1 + 3 \\\\ 5 = 9 - 1 - 3 \\\\ 6 = 9 - 3 \\\\ 7 = 9 + 1 - 3 \\\\ 10 = 9 + 1 \\\\ 11 = 9 + 3 - 1 \\\\ 12 = 9 + 3 \\\\ 13 = 1 + 3 + 9$\nThe obvious reason that this set of integers will not add up to 14 is that their sum is 13, smaller than 14. Hence, we cannot get a sum of 14. But I'm not sure if this is the only set of integers that fulfill the requirements and therefore cannot prove that all of these sets of integers will not create a sum of 14. Is there a rule or property I may be overlooking?\nAny assistance on this would be much appreciated!", "output": "Suppose your numbers are $\\{a_1,a_2,a_3\\}$.  Then the set of combinations you consider is $$\\left\\{\\sum \\delta_ia_i\\quad \\text {where}\\quad \\delta_i\\in \\{0, \\pm 1\\}\\right\\}$$\nA priori, there are $3^3=27$ such elements, but of course many of these are $\u22640$.  Indeed, you can always get $0$ (by taking $\\delta_i=0$ for all $i$) and if $n$ is in the set, so is $-n$.  Thus of the non-zero elements in the set, at most $\\frac {27-1}2=13$ of them are positive.  So if you can get all the thirteen numbers from  $1-13$ you can't get anything else.", "meta": {"post_id": 4788518, "input_score": 15, "output_score": 48, "post_title": "How to prove the following number theory puzzle"}}
{"input": "A couple of friends of mine were discussing a problem concerning this shape:\n\nIs it possible to assemble enough of these to form a cube?\nI have discovered a lot of impossible positions but was not successful in creating something useful. We have managed to build a 12x12x4 tower with leftover blocks at the top, however.\nMaybe someone here has any ideas on how to tackle this problem? My next steps would be to try and extend our 12x12x4 tower, and if that doesn't work, to write a program to search for solutions.", "output": "Assemble four copies of the shape as shown.\nFour copies of the assembly create a 6x6x4 cuboid, which can be used to create a 12x12x12 cube.", "meta": {"post_id": 4791428, "input_score": 39, "output_score": 83, "post_title": "Is it possible to assemble copies of this shape into a cube?"}}
